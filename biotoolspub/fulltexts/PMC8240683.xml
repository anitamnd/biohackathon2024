<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?noissn?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">bioRxiv</journal-id>
    <journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
    <journal-title-group>
      <journal-title>bioRxiv</journal-title>
    </journal-title-group>
    <publisher>
      <publisher-name>Cold Spring Harbor Laboratory</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8240683</article-id>
    <article-id pub-id-type="pmid">34189530</article-id>
    <article-id pub-id-type="doi">10.1101/2021.06.22.449395</article-id>
    <article-version-alternatives>
      <article-version article-version-type="status">preprint</article-version>
      <article-version article-version-type="number">2</article-version>
    </article-version-alternatives>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Super.Complex: A supervised machine learning pipeline for molecular complex detection in protein-interaction networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Palukuri</surname>
          <given-names>Meghana V.</given-names>
        </name>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6529-6127</contrib-id>
        <xref rid="A1" ref-type="aff">1</xref>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Marcotte</surname>
          <given-names>Edward M.</given-names>
        </name>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8808-180X</contrib-id>
        <xref rid="A2" ref-type="aff">2</xref>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>1</label>Oden Institute for Computational Engineering and Sciences, The University of Texas at Austin, Austin, Texas, USA</aff>
    <aff id="A2"><label>2</label>Department of Molecular Biosciences, Center for Systems and Synthetic Biology, The University of Texas at Austin, Austin, Texas, USA</aff>
    <author-notes>
      <corresp id="CR1"><label>*</label>Correspondence to <email>meghana.palukuri@utexas.edu</email> (M.V.P.), <email>marcotte@utexas.edu</email> (E.M.M.)</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <elocation-id>2021.06.22.449395</elocation-id>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</ext-link>, which allows reusers to distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator. The license allows for commercial use.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf">nihpp-2021.06.22.449395.pdf</self-uri>
    <abstract id="ABS1">
      <p id="P1">Characterization of protein complexes, <italic toggle="yes">i.e</italic>. sets of proteins assembling into a single larger physical entity, is important, as such assemblies play many essential roles in cells such as gene regulation. From networks of protein-protein interactions, potential protein complexes can be identified computationally through the application of community detection methods, which flag groups of entities interacting with each other in certain patterns. Most community detection algorithms tend to be unsupervised and assume that communities are dense network subgraphs, which is not always true, as protein complexes can exhibit diverse network topologies. The few existing supervised machine learning methods are serial and can potentially be improved in terms of accuracy and scalability by using better-suited machine learning models and parallel algorithms. Here, we present Super.Complex, a distributed, supervised AutoML-based pipeline for overlapping community detection in weighted networks. We also propose three new evaluation measures for the outstanding issue of comparing sets of learned and known communities satisfactorily. Super.Complex learns a community fitness function from known communities using an AutoML method and applies this fitness function to detect new communities. A heuristic local search algorithm finds maximally scoring communities, and a parallel implementation can be run on a computer cluster for scaling to large networks. On a yeast protein-interaction network, Super.Complex outperforms 6 other supervised and 4 unsupervised methods. Application of Super.Complex to a human protein-interaction network with ~8k nodes and ~60k edges yields 1,028 protein complexes, with 234 complexes linked to SARS-CoV-2, the COVID-19 virus, with 111 uncharacterized proteins present in 103 learned complexes. Super.Complex is generalizable with the ability to improve results by incorporating domain-specific features. Learned community characteristics can also be transferred from existing applications to detect communities in a new application with no known communities. Code and interactive visualizations of learned human protein complexes are freely available at: <ext-link xlink:href="https://sites.google.com/view/supercomplex/super-complex-v3-0" ext-link-type="uri">https://sites.google.com/view/supercomplex/super-complex-v3-0</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>protein complex</kwd>
      <kwd>overlapping community detection</kwd>
      <kwd>supervised machine learning</kwd>
      <kwd>protein-interaction network</kwd>
      <kwd>graph mining</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <title>Introduction</title>
    <p id="P2">A protein complex is a group of proteins that interact with each other to perform a particular function in a cell, the basic biological unit of all living organisms. Some examples include the elaborate multiprotein complexes of mRNA transcription and elongation helping with gene regulation and key cytoskeletal protein complexes, such as microtubules with their trafficking proteins which help establish major structural elements of cells. Extensive biological experiments have investigated the physical interactions between proteins, and these have been modeled via weighted protein-protein interaction (PPI) networks, where a protein-protein edge weight corresponds to the strength of evidence for the protein-protein interaction. Disruption of protein-protein interactions often leads to disease, therefore identifying a complete list of protein complexes allows us to better understand the association of protein and disease. All experimental protocols for detecting complexes (such as AP/MS, affinity purification with mass spectrometry, and CF/MS, co-fractionation with mass spectrometry) have a tendency to miss interactions (false negatives) and may also predict extra interactions (false positives). Proteins may also participate in more than one complex, potentially blurring the boundaries of otherwise unrelated protein communities. Computational analysis of protein-protein interaction networks can therefore be very useful in identifying accurate protein complexes and will help augment and direct experimental methods.</p>
    <p id="P3">The weighted PPI network or graph G can be represented as pairs of nodes and edges (<italic toggle="yes">V</italic>, <italic toggle="yes">E)</italic>, where the set of nodes or vertices <italic toggle="yes">V</italic> represents the proteins, and the set of weighted edges <italic toggle="yes">E</italic> represents the strengths of evidence for interactions between proteins. Any group of nodes and edges that can be characterized as a protein complex can be referred to as a community; community detection methods can be used in turn to identify protein complexes.</p>
    <p id="P4">A standard guideline for defining communities [<xref rid="R1" ref-type="bibr">1</xref>] is that a community should have more interactions or connectivity among the community than with the rest of the network. This can be modeled for example by the community fitness function in <xref rid="FD1" ref-type="disp-formula">Equation 1</xref>, mapping a subgraph, <italic toggle="yes">C</italic>, <italic toggle="yes">i.e</italic>. a group of nodes and edges from the full graph, to a scalar value representing a score, where a higher score indicates more community resemblance.
<disp-formula id="FD1"><label>(1)</label><mml:math id="M1" display="block"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
The intra-cluster density <italic toggle="yes">δint</italic> (<italic toggle="yes">C</italic>) and inter-cluster density <italic toggle="yes">δext</italic> (<italic toggle="yes">C</italic>) are given by
<disp-formula id="FD2"><label>(2)</label><mml:math id="M2" display="block"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext> </mml:mtext><mml:mo>#</mml:mo><mml:mtext mathvariant="italic">intra</mml:mtext><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">cluster edges</mml:mtext></mml:mrow><mml:mrow><mml:mo>#</mml:mo><mml:mtext mathvariant="italic">of all possible edges in the cluster</mml:mtext></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD3"><label>(3)</label><mml:math id="M3" display="block"><mml:mrow><mml:mi>δ</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext> </mml:mtext><mml:mo>#</mml:mo><mml:mtext mathvariant="italic">inter</mml:mtext><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">cluster edges</mml:mtext></mml:mrow><mml:mrow><mml:mo>#</mml:mo><mml:mtext mathvariant="italic">of all possible inter</mml:mtext><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">cluster edges</mml:mtext></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>#</mml:mo><mml:mtext mathvariant="italic">inter</mml:mtext><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">cluster edges</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
Here, <italic toggle="yes">n</italic><sub><italic toggle="yes">c</italic></sub> and <italic toggle="yes">m</italic><sub><italic toggle="yes">c</italic></sub> are the numbers of nodes and edges in subgraph <italic toggle="yes">C</italic>, respectively, and <italic toggle="yes">n</italic> is the number of nodes in graph <italic toggle="yes">G</italic>.</p>
    <p id="P7">However, there exist many communities that do not follow this criterion but can be identified by different properties they exhibit. One such example is a star-like topology, where one central node interacts with several nodes in yeast protein-interaction networks [<xref rid="R2" ref-type="bibr">2</xref>], as, for example, in the case of a molecular chaperone that acts on a number of separate protein clients. In the case of human protein complexes, we also observe different topologies such as clique, linear, and hybrid between linear and clique, as shown <xref rid="F1" ref-type="fig">Fig 1</xref>. These human protein complexes represent proteins known to belong to experimentally characterized gold-standard protein complexes from CORUM 3.0 (the comprehensive resource of mammalian protein complexes) [<xref rid="R3" ref-type="bibr">3</xref>] with edge weights taken from hu.MAP [<xref rid="R4" ref-type="bibr">4</xref>], a human protein interaction network with interactions derived from over 9,000 published mass spectrometry experiments.</p>
    <p id="P8">Existing community detection methods have primarily tried to optimize for high scores of community fitness functions, such as that of <xref rid="FD1" ref-type="disp-formula">equation 1</xref> [<xref rid="R5" ref-type="bibr">5</xref>]. These include unsupervised methods, such as implemented by MCL- Markov Clustering [<xref rid="R6" ref-type="bibr">6</xref>], MCODE - Molecular COmplex DEtection [<xref rid="R7" ref-type="bibr">7</xref>], CFinder [<xref rid="R8" ref-type="bibr">8</xref>], SCAN- Structural Clustering Algorithm for Networks [<xref rid="R9" ref-type="bibr">9</xref>], CMC - Clustering based on Maximal Cliques [<xref rid="R10" ref-type="bibr">10</xref>], COACH - COre- AttaCHment based method [<xref rid="R11" ref-type="bibr">11</xref>], GCE - Greedy Clique Expansion [<xref rid="R5" ref-type="bibr">5</xref>], and ClusterONE - clustering with overlapping neighborhood expansion [<xref rid="R12" ref-type="bibr">12</xref>], as well as semi-supervised machine learning algorithms such as COCDM - Constrained Overlapping Complex Detection Model [<xref rid="R13" ref-type="bibr">13</xref>].</p>
    <p id="P9">When there are sufficient data available on known communities, rather than applying a generic community fitness function to the problem, it can be more accurate to learn a community fitness function directly from known communities. Then, new communities detected with the learned community fitness function can be expected to better resemble known communities in the field. Supervised machine learning methods are well suited for this purpose, and a few methods have been used to learn a community fitness function from constructed community embeddings, <italic toggle="yes">i.e</italic>., community representations in vector space, obtained by extracting topological and domain-specific features from communities. The community fitness function learned can then be used to select candidate communities from the network and evaluate them. Since finding maximally scoring communities in a network is an NP-hard (non-deterministic polynomial-time hard) problem [<xref rid="R2" ref-type="bibr">2</xref>], heuristic algorithms have been used to find candidate communities. A common strategy is to select a seed (such as a node or a clique) and grow it into a candidate community by iteratively selecting neighbors to add to the current subgraph using heuristics such as iterative simulated annealing until a defined stopping criterion is met for the growth process. This process is repeated with different seeds to generate a set of candidate communities.</p>
    <p id="P10">Existing supervised methods use different machine learning methods to learn the community fitness function after extracting different features and use different heuristic algorithms to select candidate communities. The first supervised method [<xref rid="R2" ref-type="bibr">2</xref>] used a support vector machine (SCI-SVM) and a Bayesian network (SCI-BN) with 33 features with a greedy heuristic, followed by iterative simulated annealing. Stopping criteria for the growth of a seed include limiting the rounds of growth, checking for score improvement over multiple iterations, and checking for overlap with learned candidate communities so far. A second approach [<xref rid="R14" ref-type="bibr">14</xref>] recursively trained a two-layer feed-forward neural network model, NN for the classifier using 43 features. This greedy heuristic sequentially grows seeds of the highest degree with similar stopping criteria as [<xref rid="R2" ref-type="bibr">2</xref>]. Supervised learning protein complex detection SLPC [<xref rid="R15" ref-type="bibr">15</xref>] uses a regression model (RM) with 10 topological features, solved by gradient descent. A modified cliques algorithm finds and grows maximal cliques using a random but exhaustive neighbor selection followed by a greedy growth heuristic. The algorithm stops when no node addition can yield a higher score, after which they merge some pairs of overlapping complexes with an overlap greater than a threshold. ClusterEPs, short for cluster emerging patterns [<xref rid="R10" ref-type="bibr">10</xref>] uses a score function based on noise-tolerant emerging patterns (NEPs) which are minimal discriminatory feature sets using 22 features, along with an average node degree term. Like [<xref rid="R14" ref-type="bibr">14</xref>], the heuristic for this method also grows the highest degree seed nodes sequentially. The neighboring node that shares the maximum number of edges with the current subgraph is selected as a candidate for growth in each iteration and a greedy growth heuristic is used, stopping when the score is greater than 0.5. ClusterSS, short for clustering with supervised and structural information [<xref rid="R16" ref-type="bibr">16</xref>] uses a neural network with one hidden layer and 17 features, along with a traditional structural score function from [<xref rid="R12" ref-type="bibr">12</xref>]. A greedy heuristic grows seed nodes, also considering deletion of any existing subgraph nodes, with an optimization step of considering only the top k nodes by degree. The stopping criterion is when the new score is less than a factor times the old score. Both ClusterEPs and ClusterSS merge pairs of communities with overlap greater than a threshold at the end.</p>
    <p id="P11">Regarding scalability, the above methods have generally only been implemented on small yeast protein complex datasets, except for ClusterEPs, which trains on yeast data and tests on human PPIs. [<xref rid="R17" ref-type="bibr">17</xref>] implement the regression model of [<xref rid="R15" ref-type="bibr">15</xref>] on a human PPI network re-weighted by breast-cancer specific PPIs extracted from biomedical literature to detect disease-specific complexes. However, these methods employ serial candidate community sampling, negatively impacting their scalability to large networks such as hu.MAP [<xref rid="R4" ref-type="bibr">4</xref>], a human protein-interaction network with ~8k nodes and ~60k edges.</p>
    <p id="P12">In this work, we present Super.Complex (short for Supervised Complex detection algorithm), an end-to-end highly scalable (to large networks that fit on a disk), distributed, and efficient community detection pipeline that explores multiple supervised learning methods with AutoML (Automated Machine Learning) to learn the most accurate community fitness function from known communities. Super.Complex then samples candidate subgraphs in parallel by seeding nodes or starting with maximal cliques and growing them with an epsilon-greedy heuristic, followed by an additional heuristic such as iterative simulated annealing or pseudo-metropolis using the learned community fitness function. On a yeast PPI network, Super.Complex outperforms all 6 existing supervised methods, as well as 4 unsupervised methods. Three novel evaluation measures are proposed to overcome certain shortcomings of existing metrics. We apply Super.Complex to hu.MAP, a human protein-protein interaction network with ~8k nodes and ~60k edges to yield 1028 protein complexes, including high-scoring previously unknown protein complexes, potentially contributing to new biology, and make all data, code, and interactive visualizations openly and freely available at <ext-link xlink:href="https://sites.google.com/view/supercomplex/super-complex-v3-0" ext-link-type="uri">https://sites.google.com/view/supercomplex/super-complex-v3-0</ext-link>.</p>
  </sec>
  <sec id="S2">
    <title>Materials and Methods</title>
    <sec id="S3">
      <title>Overview of Super.Complex</title>
      <p id="P13">The pipeline Super.Complex comprises two main tasks, first, learning a community fitness function with AutoML methods, and second, using the community fitness function to intelligently sample overlapping communities from a network in parallel. As shown in <xref rid="F2" ref-type="fig">Fig 2</xref>, each task is subdivided into different steps, described in brief in this section, with all details in the following sections of Materials and Methods. For the first task, we perform a pre-processing step, Data Preparation, where known communities are cleaned and split into non-overlapping training and testing sets, followed by construction of training and testing negative community data. In (i) Topological Feature Extraction, topological characteristics for all communities are computed to construct training and testing feature matrices. AutoML (ii) then compares different ML (Machine Learning) pipelines to select the best one, followed by training and testing the best ML pipeline, thus learning the community fitness function as the binary classifier distinguishing positive communities from negatives. Having learned the community fitness function, Super.Complex then uses it in its heuristic algorithm for the second task of searching for candidate communities in the network in parallel. For (iii) intelligent sampling, the algorithm can start with either single nodes or maximal cliques as seeds. We note that all nodes of the network were used as seeds in our experiments (this is quite fast due to Super.Complex’s parallel implementation), allowing us to work without any estimate of the number of expected communities. These seeds are grown using a 2-stage heuristic, <italic toggle="yes">e.g. ϵ ϵ ϵ-</italic>greedy + iterative simulated annealing. This is followed by a (iv) post-processing step of merging highly overlapping communities. Finally, in the last step, evaluation, the learned communities are compared with known communities. The steps of the pipeline are fairly independent and can be improved on their own with methods to test the accuracy/performance of each of the steps.</p>
    </sec>
    <sec id="S4">
      <title>Data Preparation</title>
      <p id="P14">First, the weighted network under consideration is cleaned by removing self-loops, as we do not consider interactions with oneself as a feature of communities. For scalability, the graph is stored on disk as a set of files, each corresponding to a node and containing a list of the node’s neighbors via weighted edges.</p>
      <sec id="S5">
        <title>Positive communities.</title>
        <p id="P15">Super.Complex takes sets of nodes comprising known communities and obtains their edge information from the induced subgraph of these nodes on the weighted network. Nodes in communities that are absent from the network are removed. Communities with fewer than 3 nodes, communities that are internally disconnected, and duplicate communities are also removed. Constructing the final set of positive communities involves 2 main steps: (i) merging similar communities, and (ii) splitting them into non-overlapping train and test sets. Note that if independent train and test sets of communities are known in advance, these steps can be skipped.</p>
        <p id="P16">In the first step, using a merging algorithm we devised, we merge highly similar communities to yield a final list of communities, where no pair of communities have a Jaccard score (<xref rid="SD1" ref-type="supplementary-material">S1 File equation 3</xref>) greater than or equal to <italic toggle="yes">j</italic>. We recommend users to set this value based on domain knowledge of observed redundancy in the set of known communities.</p>
        <p id="P17">Multiple solutions exist that achieve this goal, however, we want a solution with a large number of communities, <italic toggle="yes">i.e</italic>., with only a small number of merges performed on the original set of known communities. This is especially important in applications with limited data, such as the human and yeast protein complex experiments in this work. Our algorithm was designed with this objective in mind, and works as follows. The iterative algorithm makes multiple passes through the list of communities performing the merging operation until the specified criterion is achieved. In a single pass of the list of communities, each community is considered in order and merged with the community with which it has the highest overlap (if greater than or equal to <italic toggle="yes">j</italic>) and the list is updated immediately by removing the original 2 communities and adding the merged community to the end of the list, so that the updated list is available for the next community in consideration. This merging algorithm achieves a lesser number of merges than a trivial merging solution which would merge random pairs of communities that do not satisfy the required criteria until convergence. In practice, the proposed algorithm quickly converges to a solution (<italic toggle="yes">i.e</italic>. the final set has no communities that overlap more than the specified value <italic toggle="yes">j</italic>).</p>
        <p id="P18">In the second step, the communities are split into non-overlapping training and testing datasets, to emphasize their independence. We obtain sets with equal size distributions and a 70–30 train-test split, as recommended for machine learning algorithms with a small amount of data. Previous algorithms such as [<xref rid="R4" ref-type="bibr">4</xref>] and Super.Complex v2.0 [<xref rid="R31" ref-type="bibr">31</xref>] discard test communities with sizes greater than a threshold, thus losing out on information from some known communities and which also, in practice, do not yield train-test splits that are close to the recommended 70–30 split. Therefore, we propose the following algorithm. Here, we first make the recommended 70–30 random split into train and test communities. Then we perform iterations of transfers between the two sets until they become independent. In each iteration, we perform two directions of transfers, from train to test and vice-versa, and if the 70–30 split is disturbed, we remove the communities at the end of the list which have extra communities and add them to the other list. In each direction of transfer, for instance, from train to test, we go through the training communities in one pass and if a training community has an overlap (at least one edge) with any of the test communities, it is immediately transferred to the test set, making the updated test set available for comparison with subsequent training set communities. In practice, for many random splits, the algorithm converges fast enough to a solution that is non-overlapping. If for an initial random split, convergence is not achieved after a few iterations, we recommend restarting the algorithm with a different random split.</p>
      </sec>
      <sec id="S6">
        <title>Negative communities.</title>
        <p id="P19">Negative communities, or non-communities are represented by random walks sampled from the network by growing random seeds, adding a random neighbor at each step. The number of steps ranges from the minimum size to the maximum size of positive communities, with a total number of random walks equal to the number of positive communities multiplied by a scale factor &gt; 1. The random walks are split almost equally across all the sizes, by splitting equally across the different number of steps to be taken for a random walk, to yield an almost uniform size distribution for negative communities. We say almost uniform size distribution, as random walks with the same number of steps need not yield the same sizes, given that the random walk as defined here can revisit edges it has already visited. To achieve random walks of the same size, the algorithm attempts an extra number of random walks and an extra number of steps to achieve the desired random walk size.</p>
        <p id="P20">The size distribution of positive communities is taken into consideration while training the machine learning model when using a uniform distribution for negatives. We also explore using almost the same size distribution as the positive communities to construct the negative communities. For this, for each size of the positive communities, we construct the negatives by sampling a number of random walks equal to the scale factor times the number of positive communities of this size. However, in this case, we find that there are quite a few missing sizes due to limited positives which may affect the scoring of subgraphs of the missing sizes. Using a uniform distribution would provide more information to learn a more accurate community fitness function that can recognize negatives at sizes missing for positives. In the following feature extraction step, random walks resembling communities are removed. The final number of negative communities is close to the number of positive communities, as we have sampled a slightly higher number of random walks via the scale factor.</p>
      </sec>
    </sec>
    <sec id="S7">
      <title>Topological Feature Extraction</title>
      <p id="P21">As communities exhibit different topological structures on the graph, these can be learned by considering useful topological features of communities. Based on graph theory, we extract 18 topological features, detailed in <xref rid="SD1" ref-type="supplementary-material">S1 File Methods</xref> (Topological features) for each of the positive/negative communities to construct the final train and test data feature matrices, <italic toggle="yes">i.e</italic>. the positive and negative community embeddings.</p>
    </sec>
    <sec id="S8">
      <title>Learning the community fitness function with AutoML</title>
      <p id="P22">A community fitness function is learned as the decision function of a binary machine learning classifier trained to distinguish the community and non-community embeddings constructed in the previous feature extraction step. For this, we use an AutoML algorithm, TPOT [<xref rid="R18" ref-type="bibr">18</xref>], a genetic algorithm that yields the best model and parameters. It evaluates several preprocessors along with ML models and yields cross-validation scores on the training dataset for each pipeline, which itself is usually a combination of several preprocessors followed by the machine learning model. We configure the algorithm to run in a distributed setting, exploring several combinations of several preprocessors and ML models.</p>
      <p id="P23">We specify 6 pre-processors that scale the feature matrix. These are - (i) Binarizer, which sets a feature to 0 or 1 based on a threshold, (ii) MaxAbsScaler, which divides the feature by the maximum absolute value of the feature, (iii) MinMaxScaler, which subtracts the minimum of the feature from the feature vector and divides by the range of the feature, (iv) Normalizer, which divides the feature vector by its norm to get a unit norm, (v) RobustScaler, which makes a feature robust to outliers by scaling using the interquartile range and (vi) StandardScaler, which standardizes to the Z-score by subtracting the mean and dividing by the standard deviation of the feature.</p>
      <p id="P24">We include four feature selecting pre-processors, which are additionally important as we incorporate 6 additional preprocessors that construct combined features. The additional preprocessors include - (i) Decomposition: PCA (Principal Component Analysis), FastICA (Independent Component Analysis), (ii) Feature Agglomeration, (iii) Kernel Approximation methods: Nystroem, Radial Basis Function RBFSampler, (iv) Adding Polynomial Features, (v) Zero counts: Adds the count of zeros and non-zeros per sample as features and (vi) OneHotEncoder for numeric categorical variables. The feature selecting preprocessors include - (i) SelectPercentile, which selects the highest-scoring percentage of features based on 3 univariate statistical tests, FPR - False Positive Rate, FDR - False Discovery Rate and FWE - Family-wise error rate; (ii) VarianceThreshold which removes low variance features, (iii) RFE (recursive feature elimination) using ExtraTrees and (iv) SelectFromModel using ExtraTrees based on importance weights. The ML models included are - (i) Naive Bayes methods using Gaussian, Bernoulli, and Multinomial distributions (ii) Decision Trees, (iii) Ensemble methods of ExtraTrees, Random Forest, Gradient Boosting and XGB (XGBoost), (iv) K-nearest neighbors, (v) Linear SVMs and (vi) Linear models for Logistic Regression.</p>
      <p id="P25">The population size and number of generations are provided as parameters for the genetic algorithm of the AutoML pipeline. In practice for our application, giving a value of 50 for each yielded good results. There is an option for a warm start, where you can run additional generations and with additional population sizes starting from the latest results, if the results are unsatisfactory. Additionally, several other machine learning models and preprocessors can also be incorporated into this pipeline, including neural networks. Note that in our experiments, we also obtained pipelines that stack different ML models. We run the pipeline in a distributed manner, setting the number of jobs as the number of processes that run in parallel on a single computer. All the processes on the computer can be used for maximum utilization, however, the documentation notes that memory issues may arise for large datasets. In practice, we set the number of jobs as 20 on a Skylake compute node (Intel Xeon Platinum 8160 with 48 cores @2GHz clock rate).</p>
      <sec id="S9">
        <title>Evaluation.</title>
        <p id="P26">By default, 5-fold cross-validation is performed, although this can be modified by a parameter. The pipelines with high cross-validation average precision scores (area under the PR curve) are evaluated on the test dataset to find the best pipeline for our data, to use this for the community fitness function. A one hidden-layer perceptron is also available for training, and comparison with the AutoML output to select the best model. We evaluate the performance of the ML binary classifier using accuracies, precision-recall-f1 score measures, average precision score, and PR curves for the test sets while also evaluating these measures for the training set to compare with the test measures and check the bias and variance of the algorithm to make sure it is not underfitting or overfitting the data. We also plot the size-wise accuracies of the model to understand how a model performs w.r.t to the size of the subgraph it is evaluating.</p>
      </sec>
    </sec>
    <sec id="S10">
      <title>Candidate community search</title>
      <p id="P27">Finding a set of maximally scoring candidate communities in a network is an NP-hard problem, as proved by [<xref rid="R2" ref-type="bibr">2</xref>] by reducing it to the problem of finding maximal cliques. Since this is an NP-hard problem, algorithms based on heuristics are required to solve it. We explore seeding and growth strategies.</p>
      <sec id="S11">
        <title>Design and distributed architecture.</title>
        <p id="P28">First, we need to select seeds. Options for seeds include specifying all the nodes of the graph (recommended for best accuracy), all the nodes of the graph present in known communities, a specified number of nodes that will be selected randomly from the graph, or maximal cliques. In the distributed setting using multiple compute nodes, the specified seeds are partitioned equally across compute nodes, and each compute node deals only with the task of growing the seeds assigned to it. In practice, the partitioning is done by a main compute node which partitions the list of seeds and stores the partitioned lists as separate files on the file server. Then it launches one task per compute node (including itself) using the launcher module [<xref rid="R32" ref-type="bibr">32</xref>], where a task instructs a compute node to read its respective file containing the seed nodes and run the sampling algorithm starting with each of the seed nodes. On each compute node, we take advantage of all the cores by employing multiprocessing with the <italic toggle="yes">joblib</italic> python library. Each process intelligently grows a single seed node into a candidate community and writes it to the compute node’s temporary storage. For this, we need the graph and parameters of the community fitness function, which we store on temporary disk space of each compute node to optimize RAM as it is impractical to store large networks and machine learning models in memory. Each process reads the model into its memory and uses it to evaluate the neighbors, to pick the neighbor to add to the current subgraph in the growth process from the seed node. The neighbors of the subgraph under consideration at each step of the growth are read from disk on-demand and stored in memory only until they have been evaluated by the fitness function. In this way, we ensure that the processes have a low memory footprint, which can otherwise quickly become a bottleneck for large graphs. We also minimize disk storage by storing each resulting candidate community compactly using only its nodes, as its edges can be inferred if/when necessary by inducing the nodes on the graph. After all the child processes of growing seeds complete on a compute node, the compute node reads the set of learned community files it had stored on its disk and compiles them into a list of candidate communities before writing the list to the file server. The same code also runs in a distributed setting with only one multi-core compute node. There also exists a serial option to run the code without invoking parallel constructs, useful for running on a single core.</p>
      </sec>
      <sec id="S12">
        <title>Intelligent sampling - Heuristics.</title>
        <p id="P29">Only for the first step of growth, we add the neighbor connected with the highest edge-weight. We provide 2 options for growing the subgraph at each step- an exhaustive neighbor search that is suitable for graphs that are not very large, and an option that optimizes performance by evaluating only a subset of neighbors. In the latter, using a large user-defined threshold <italic toggle="yes">t</italic><sub>1</sub>, if the number of neighbors of the current subgraph is greater than the threshold, a random sample of the neighbors equal to the provided threshold is chosen for evaluation. Now, of the neighbors, first, an <italic toggle="yes">ϵ</italic>-greedy heuristic is used to select the neighbor to add to the subgraph. In an <italic toggle="yes">ϵ</italic>- greedy heuristic, with <italic toggle="yes">ϵ</italic> probability, a random neighbor is added instead of the maximum scoring neighbor.</p>
        <p id="P30">In the non-exhaustive search case, in the event of 1-<italic toggle="yes">ϵ</italic> probability, if the number of neighbors is greater than a 2nd user-defined threshold <italic toggle="yes">t</italic><sub>2</sub>, a 2nd optimization of cutting down the number of neighbors is applied before evaluating each of the neighbors for choosing the greedy neighbor, as follows. Here, the <italic toggle="yes">t</italic><sub>2</sub> highest neighbors are chosen for evaluation, where the order is decided by sorting the neighbors in descending order based on their maximum edge weight (<italic toggle="yes">i.e</italic>. the highest edge weight among all the edges connecting a neighbor to the subgraph). Note how the first threshold <italic toggle="yes">t</italic><sub>1</sub> ensures that the sorting complexity <italic toggle="yes">O</italic>(<italic toggle="yes">t</italic><sub>1</sub><italic toggle="yes">log</italic>(<italic toggle="yes">t</italic><sub>1</sub>))does not blow up.</p>
        <p id="P31">Note that for efficient constant-time <italic toggle="yes">O</italic>(1) lookup of the maximum edge weight of a neighbor, we store the neighbors of the subgraph as a hash map, where looking up a neighbor yields its maximum edge weight. This hash map also stores, for each neighbor, a list of edges connecting it to the subgraph and was constructed efficiently when the neighbors of each of the subgraph nodes were read from the corresponding file. After selecting the neighbor to add to the subgraph in the current iteration, this hash map is also used to efficiently add the neighbor to the subgraph by providing constant-time lookup to the edges that need to be added.</p>
        <p id="P32">Instead of the base <italic toggle="yes">ϵ</italic>-greedy heuristic, we also have a simple base heuristic option, termed greedy edge weight, where we add the neighbor with the highest maximum weight edge at each step of the iteration. Note that since the ML model is not used at each stage of the growth, this is fast enough and does not require the optimization steps used in the <italic toggle="yes">ϵ</italic>- greedy approach where subsets of neighbors were selected for evaluation by the community fitness function.</p>
        <p id="P33">For both base heuristics, in any iteration, if no neighbors for the subgraph exist, the growth process terminates. If the community score of the subgraph in any iteration is less than 0.5, the node last added is removed and the growth process terminates. We provide additional heuristics that can be applied on top of the base <italic toggle="yes">ϵ</italic>-greedy heuristic. Based on the scores of the current and previous iterations of the subgraph, we accept or reject the latest node addition using the user-defined heuristic - iterative simulated annealing (ISA), or a variant of ISA, termed pseudo-metropolis in which the acceptance probability (<xref rid="FD4" ref-type="disp-formula">equation 9</xref>) is a constant, <italic toggle="yes">i.e. P</italic>(<italic toggle="yes">S</italic><sub><italic toggle="yes">new</italic></sub>, <italic toggle="yes">S</italic><sub><italic toggle="yes">old</italic></sub>) = <italic toggle="yes">k</italic>. In ISA, at each stage of growth of the current subgraph, its maximum scoring neighbor is added, except in the case when the new community score of the subgraph <italic toggle="yes">S</italic><sub><italic toggle="yes">new</italic></sub> is lesser than <italic toggle="yes">S</italic><sub><italic toggle="yes">old</italic></sub>, the value before adding the new node (<italic toggle="yes">i.e. S</italic><sub><italic toggle="yes">new</italic></sub> &lt; <italic toggle="yes">S</italic><sub><italic toggle="yes">old</italic></sub>). In this case, the new node addition is accepted with a probability of,
<disp-formula id="FD4"><label>(9)</label><mml:math id="M4" display="block"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mtext>T</mml:mtext></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula>
here, starting with hyperparameters <italic toggle="yes">T</italic><sub><italic toggle="yes">0</italic></sub> and <italic toggle="yes">α</italic>, we update the temperature as <italic toggle="yes">T</italic> ← <italic toggle="yes">αT</italic> after every iteration.</p>
        <p id="P34">When ISA or pseudo-metropolis heuristics are applied, we also evaluate an additional heuristic where the algorithm terminates if it has been 10 (or can be user-defined) number of iterations since the score of the subgraph has increased.</p>
        <p id="P35">In the implementation, we provide four options to the user - greedy edge weight,<italic toggle="yes">ϵ</italic>-greedy, <italic toggle="yes">ϵ</italic>-greedy + ISA and <italic toggle="yes">ϵ</italic>-greedy + pseudo-metropolis. In all options, the algorithm terminates after a number of steps equal to a user-specified threshold. The default threshold provided is the maximum size of the known communities, and we also provide a smart option for when a few communities have a large number of nodes, where it is set to choose the maximum size after ignoring outliers. This number can also be improved by visual inspection of a boxplot of community sizes that is generated. Future work can also explore greedy edge weight + ISA and greedy edge weight + pseudo-metropolis heuristic algorithms and observe their performance. Note how there are 2 possibilities for exploration in the 3 algorithms other than the greedy edge weight heuristic algorithm. In the 1st stage, we pick a neighbor at random with low probability. In the 2nd stage, we accept the neighbor we picked in the 1st stage with low probability, if it yields a lower score than the original subgraph.</p>
      </sec>
      <sec id="S13">
        <title>Post-processing (merging overlaps) and cross-validation.</title>
        <p id="P36">Communities with only 2 nodes are removed. Note that communities with 2 nodes are rarely found, and while dimers are biologically valid, since they do not have topological variation, we do not consider them in this work focused on higher order assemblies with different topologies as a key feature. We then merge communities that have a Jaccard similarity greater than a specified overlap threshold employing the merging algorithm discussed in the data cleaning section. The only difference is while merging, for two overlapping communities, the final community retained out of the 2 communities or the merged variant is the one that obtains the highest score with the community fitness function. In another variant of the merging algorithm, instead of the Jaccard similarity threshold (<xref rid="SD1" ref-type="supplementary-material">S1 File equation 3</xref>), we use Qi’s overlap measure (<xref rid="SD1" ref-type="supplementary-material">S1 File equation 10</xref>).</p>
        <p id="P37">The parameters <italic toggle="yes">ϵ</italic> in the <italic toggle="yes">ϵ</italic>-greedy heuristic, k in pseudo-metropolis, <italic toggle="yes">T</italic><sub><italic toggle="yes">0</italic></sub> and <italic toggle="yes">α</italic> in iterative simulated annealing, and the overlapping threshold in the post-processing step are varied in parameter sweeps to select the best ones that work using the Qi et al F1 score (<xref rid="SD1" ref-type="supplementary-material">S1 File equation 8</xref>).</p>
        <p id="P38">After parameter sweeps, the results of different heuristics are examined and the one that yields the best F1 score is chosen. Additional details regarding evaluation are outlined in <xref rid="SD1" ref-type="supplementary-material">S1 File Methods</xref> (Evaluation with existing measures).</p>
      </sec>
    </sec>
  </sec>
  <sec id="S14">
    <title>Results and Discussion</title>
    <sec id="S15">
      <title>Contributions of Super.Complex - a scalable, distributed supervised AutoML-based community detection method</title>
      <p id="P39">Super.Complex implements an original distributed architecture and an efficient pipeline, scaling to large networks such as hu.MAP with ~8k nodes and ~60k edges. With an AutoML method, which also includes automated feature selection, and four 2-stage heuristic options for candidate community search, the pipeline finds accurate community fitness functions and high quality communities. Unlike some existing methods that remove nodes in the process of growth (e.g. such as Louvain [<xref rid="R19" ref-type="bibr">19</xref>] and ClusterSS), we note in <xref rid="SD1" ref-type="supplementary-material">S1 File Results</xref> (Algorithm guarantees) that our method guarantees properties such as internal connectivity of communities. Further, the merging algorithm we employ guarantees that no two communities overlap more than a specified threshold. In the case of non-overlapping communities (obtained by specifying a merging threshold of 0 overlap), there is an additional guarantee that no two communities can be merged to yield a higher scoring community. To our knowledge, epsilon-greedy heuristics in conjunction with other heuristics such as iterative simulated annealing have not been applied in the past for community detection. This allows the pipeline to leverage advantages of both heuristics by adding an additional layer of stochasticity allowing better exploration in the candidate community search stage. Super.Complex has a cross-validation pipeline to select the heuristic and parameters that work best for the application at hand. Minimal hyper-parameter selection is required in our algorithm with default parameters provided when smart hyperparameters cannot be inferred.</p>
      <p id="P40">Since the number of known communities can be limited, we emphasize the preservation of known communities when splitting them into train-test sets while also ensuring (i) independence - <italic toggle="yes">i.e</italic>., no edge overlap between a train and test community on the network, (ii) similar size distributions for both sets, and (iii) 70–30 ratios in train-test sets. Similarly, a minimal number of merges is attempted in the merging algorithms devised to maintain a high number of learned protein complexes. Further, unlike existing supervised methods, which evaluate the performance of their algorithms on a reduced network with only nodes present in known communities, we evaluate our algorithm on the full network for more accurate evaluation. Finally, we note that Super.Complex uses only topological features of networks, and can be applied to community detection on networks from various fields, with the possibility of including domain-specific features to learn more accurate domain-specific community fitness functions. Our methods are also applicable in domains with limited or no knowledge by transferring community fitness functions from other domains, such as the defaults we provide for human protein complex detection.</p>
    </sec>
    <sec id="S16">
      <title>Three novel evaluation measures to compare learned communities with known communities</title>
      <p id="P41">Comparing sets of learned and known communities accurately is an outstanding issue. Poor evaluation measures do not satisfactorily identify the quality of learned communities and make it difficult to evaluate a community detection algorithm. Sets of learned communities achieving high scores with existing evaluation measures have been observed to have a lot of redundancies, e.g. multiple learned communities are very similar with high overlaps [<xref rid="R20" ref-type="bibr">20</xref>]. Known big communities were also observed to be split into several learned communities while still achieving good scores on evaluation measures. While it is undesirable to have many false negatives, having many false positives is more hurtful, as wet-lab experiments for biological validation tend to be quite expensive and time-consuming to perform. Therefore, we concentrate on including precision-like measures that compute false positives. Further, evaluation measures that are not sensitive to changes in the sets of learned communities limit our abilities to iterate successfully over algorithm modifications to improve algorithms. We examine the specific shortcomings of different evaluation measures and propose new measures to help overcome the issues discussed and construct robust yet sensitive measures.</p>
      <sec id="S17">
        <title>F-similarity-based Maximal Matching F-score (FMMF).</title>
        <p id="P42">An issue with many measures such as Qi et al F1 score (<xref rid="SD1" ref-type="supplementary-material">S1 File equation 8</xref>) and SPA (<xref rid="SD1" ref-type="supplementary-material">S1 File equation 9</xref>) is that they don’t penalize redundancy, <italic toggle="yes">i.e</italic>. if we learn multiple same or very similar communities which are each individually high scoring, we will get a high value of precision-like measures. This is because in many cases, many to one matches are being made between learned communities and known communities. To deal with such issues, it is best to make one-to-one matches. The MMR (Maximal Matching Ratio) is one such good measure, however, it only calculates a recall-like measure by dividing the sum of the weights of edges (in a maximal sum of one-to-one edge weights) by the total number of known communities. Taken alone this cannot account for precision, for instance, if we learn a series of random subgraphs, these have low weights and will be ignored, while a high MMR score can be obtained from only a small number of high quality learned communities. Therefore, we define the precision equivalent for MMR, <italic toggle="yes">P</italic><sub><italic toggle="yes">FFM</italic></sub> in <xref rid="F3" ref-type="fig">Fig 3c</xref>.</p>
        <p id="P43">In <xref rid="F3" ref-type="fig">Fig 3c</xref>, <italic toggle="yes">M</italic> is a set of weights of a set of maximal one-to-one matches, found using Karp’s algorithm [<xref rid="R21" ref-type="bibr">21</xref>]. The weight w that we use is the F-similarity score (<xref rid="F3" ref-type="fig">Fig 3b</xref>), also described in the next section, Community-wise Maximum F-similarity based F-score (CMFF), unlike the neighborhood affinity used in the original MMR. Correspondingly we can define an F-score, FMMF, as the harmonic mean of the precision <italic toggle="yes">P</italic><sub><italic toggle="yes">FFM</italic></sub> and recall <italic toggle="yes">R</italic><sub><italic toggle="yes">FFM</italic></sub>, also shown in <xref rid="F3" ref-type="fig">Fig 3c</xref>.</p>
        <p id="P44">By doing a one-to-one match, we are also indirectly penalizing cases where the benchmark community is split into multiple smaller communities in the learned set of communities, since the measure considers the weight of only one of the smaller learned communities that comprise the known community, ignoring the rest. Thus only the small weight of the matched community is considered, penalizing this case, unlike one-to-many measures that aggregate the contributions from each of the smaller communities to finally achieve a high score.</p>
      </sec>
      <sec id="S18">
        <title>Community-wise Maximum F-similarity-based F-score (CMFF).</title>
        <p id="P45">[<xref rid="R5" ref-type="bibr">5</xref>] compute F1 scores at the individual known community-learned community match level and look at the histograms of these scores for all known communities. While their work does not state the exact formulation of their F1 score, we are inspired by them to define an F1 score at the match level, <italic toggle="yes">i.e</italic>. an F-similarity score, by comparing the nodes of a learned and a known community. Our F-similarity score is a combination of the recall (of the nodes of the known community) and the precision (of the nodes of the learned community), as shown in <xref rid="F3" ref-type="fig">Fig 3b</xref>.</p>
        <p id="P46">Our F-similarity score can be compared with a threshold to determine a match, and then the overall precision, recall, and F1 scores for the set of predictions can be computed as in <xref rid="SD1" ref-type="supplementary-material">S1 File equations 6</xref>–<xref rid="SD1" ref-type="supplementary-material">8</xref>. Alternatively, our F-similarity score can be used to determine the best matches for communities and overall measures can be defined that can be investigated to reveal the contributions at the individual match level as well. For interpretability at the match level, similar to the unbiased sensitivity and PPV metrics (as discussed in the next section, Unbiased Sn-PPV Accuracy (UnSPA)), we can define precision and recall measures that evaluate, for each community, the closest matching community in the other set using a similarity metric. Using the F-similarity score as the similarity metric here, we define precision and recall-like measures, and combine them into the F1-like measure, CMFF - Community-wise Maximum F-similarity based F-score, as shown in <xref rid="F3" ref-type="fig">Fig 3d</xref>. We detail a general framework to construct similar measures in the next paragraph, drawing inspiration from modifications to the Qi et al F1 score. This framework also gives another method of constructing the CMFF.</p>
        <p id="P47">In the Qi et al measures from [<xref rid="R2" ref-type="bibr">2</xref>], (<xref rid="SD1" ref-type="supplementary-material">S1 File equations 6</xref>–<xref rid="SD1" ref-type="supplementary-material">8</xref>), a binary indication of a possible match is used, <italic toggle="yes">i.e</italic>. as long as there exists a possible match, it is used as a 1 or 0 count towards the aggregate precision or recall measures. Having a measure that provides matches between learned and known communities allows easy identification of previously unknown communities. One to many matches such as Qi et al precision-recall (PR) measures that do not use an explicit matching between learned and known communities can be modified to obtain a matching. In the modified measure, for each community, we choose the most similar community in the other set in order to give the matching. While measures that use a threshold such as Qi et al F1 score (<xref rid="SD1" ref-type="supplementary-material">S1 File equation 8</xref>) have the advantage of being robust, until a match crosses a threshold, the measure will not change, making it insensitive to small variations in predictions. Measures with low sensitivity make it difficult to compare algorithms and select parameters. Weighted measures are more sensitive, giving different values based on the quality of matches, and are more precise when compared to summing binary values of match existence. Accordingly, a more sensitive and precise version of the Qi et al F1 score can be obtained by summing up weights indicating the similarity scores. For instance, instead of the Qi overlap measure (<xref rid="SD1" ref-type="supplementary-material">S1 File equation 8</xref>), the neighborhood affinity similarity measure (<xref rid="SD1" ref-type="supplementary-material">S1 File equation 4</xref>) can be used to construct a more precise and sensitive measure.
<disp-formula id="FD5"><label>(5)</label><mml:math id="M5" display="block"><mml:mrow><mml:mtext mathvariant="italic">Recall </mml:mtext><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mtext>m</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>K</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>C</mml:mi><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:munder><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mtext>m</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>L</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
Here, <italic toggle="yes">sim</italic>(<italic toggle="yes">C</italic><sub>1</sub>, <italic toggle="yes">C</italic><sub>2</sub>) is a similarity measure between communities <italic toggle="yes">C</italic><sub>1</sub> and <italic toggle="yes">C</italic><sub>2</sub>, with |<italic toggle="yes">C</italic><sub>1</sub>| is the number of nodes in <italic toggle="yes">C</italic><sub>1</sub>. <italic toggle="yes">C</italic><sub><italic toggle="yes">k</italic></sub> is a known community from <italic toggle="yes">K</italic>, the set of known communities and <italic toggle="yes">C</italic><sub><italic toggle="yes">l</italic></sub> is a learned community from <italic toggle="yes">L</italic>, the set of learned communities.</p>
        <p id="P49">Different similarity measures (<xref rid="SD1" ref-type="supplementary-material">S1 File equations 3</xref>–<xref rid="SD1" ref-type="supplementary-material">5</xref>), such as the Jaccard coefficient can be used to construct different F1 measures. We recommend the F-similarity measure in <xref rid="F3" ref-type="fig">Fig 3b</xref>, as it can be broken down into a precision-based and recall-based measure at the level of comparing a known and learned community, and use it to construct the CMFF score.</p>
      </sec>
      <sec id="S19">
        <title>Unbiased Sn-PPV Accuracy (UnSPA).</title>
        <p id="P50">Consider the precision-like positive-predictive value (PPV), recall-like Sensitivity (Sn), and their combined Sn-PPV accuracy (SPA) [<xref rid="R22" ref-type="bibr">22</xref>], also given in <xref rid="SD1" ref-type="supplementary-material">S1 File equation 9</xref>. In Sensitivity, the numerator is a sum of the maximal number of recalled nodes for each community and the denominator is a sum of the number of nodes in each community. Measures like these do not give equal importance to each of the known communities and assign higher values for recalling larger communities when compared to recalling smaller communities. For instance, an algorithm that perfectly recalls numerous smaller communities and does not recall much of a few bigger communities can get a worse sensitivity score when compared to an algorithm that does the opposite, <italic toggle="yes">i.e</italic>. recalls most of the big community and does not recall much of any of the smaller communities. Rather than inducing bias into a measure that decides which communities should be weighted higher, it may be a better idea to have a measure that gives equal weights to all communities. We define an unbiased sensitivity <italic toggle="yes">Sn</italic><sub><italic toggle="yes">u</italic></sub> in <xref rid="F3" ref-type="fig">Fig 3e</xref>, by dividing by the total number of known communities.</p>
        <p id="P51">In PPV, the denominator sums, for each learned community, the sum of the subset of nodes in the learned community shared by all known communities. This does not contribute accurately to a precision-like measure, as nodes that are absent in known communities are ignored. For instance, a learned community that has all the nodes in a known community, but also includes a lot of possibly spurious nodes will be scored in the same way as a learned community which is an exact match to the known community. Further, in PPV, nodes in a learned community shared by multiple known communities get counted an extra number of times in the denominator. So if we share a set of nodes with multiple known communities we get penalized more than (i) if we share the set with only a few known communities, or (ii) if nodes of our community are shared with different known communities in a disjoint manner. The reasoning for allowing such behavior is again biased and does not support the detection of overlapping known communities. For example, a learned community that has a high overlap with 2 known communities (ex: a learned community with 10 nodes that shares all of its nodes with each of the known communities) will contribute lesser (0.5) to a PPV score than a learned community which overlaps lesser with one known community (ex: 6 nodes in a learned community with 10 nodes overlapping with only one known community, giving a 0.6 contribution to the PPV). To overcome these issues, we propose an unbiased PPV, <italic toggle="yes">PPV</italic><sub><italic toggle="yes">u</italic></sub> in <xref rid="F3" ref-type="fig">Fig 3e</xref>, where we divide by the total number of learned communities. The corresponding unbiased accuracy is obtained by taking the geometric mean of the <italic toggle="yes">PPV</italic><sub><italic toggle="yes">u</italic></sub> and <italic toggle="yes">Sn</italic><sub><italic toggle="yes">u</italic></sub> as shown in <xref rid="F3" ref-type="fig">Fig 3e</xref>.</p>
        <p id="P52">From the sensitivity of measures plot in <xref rid="F3" ref-type="fig">Fig 3f</xref>, we find that the FMMF score, the Qi et al F1 score (<xref rid="SD1" ref-type="supplementary-material">S1 File equation 8</xref>), and CMFF score are most sensitive to the pairwise overlap between communities, giving high values at the overlap coefficient yielding the best results, determined via visual inspection of the learned results, as follows. We observed highly overlapping, repetitive, and large numbers of similar learned protein complexes in our experiment on hu.MAP, such as several resembling the ribosome complexes at the high overlap threshold of 0.5 Jaccard coefficient, whereas, at low overlaps, we obtain a total small number of learned complexes, 84 learned complexes after removing proteins absent from known complexes. As we would like a high number of good quality complexes, we find that intermediate values of overlap Jaccard coefficient yield satisfactory results, for instance, at 0.25 Jaccard coefficient, we obtain 121 complexes after removing proteins absent from known complexes, with a high recall of known complexes and good observed quality, <italic toggle="yes">i.e</italic>. low numbers of very similar overlapping learned complexes. The clique-based measures from [<xref rid="R4" ref-type="bibr">4</xref>] - F-grand K-clique and F-weighted K-clique do not vary much with overlap, and the UnSPA, like the SPA, increases with increasing overlap threshold. However, the rate of increase of SPA w.r.t increasing overlap values is greater than UnSPA, yielding comparatively higher scores at undesirable high overlaps. In other words, instead of the desired decreasing trend from 0.25 to 0.5 Jaccard coefficient overlap, we have a highly increasing trend for SPA, compared to the almost constant trend for UnSPA - an improvement over SPA that can possibly be attributed to the unbiasing modification we have introduced. Therefore, for accurate evaluation in which redundancy (high overlap) is penalized, we recommend UnSPA over SPA, and primarily recommend the FMMF score, CMFF score, and the existing Qi et al F1 score (<xref rid="SD1" ref-type="supplementary-material">S1 File equation 8</xref>).</p>
      </sec>
    </sec>
    <sec id="S20">
      <title>Super.Complex applied to a human protein interaction network to detect protein complexes</title>
      <sec id="S21">
        <title>Experiment details.</title>
        <p id="P53">We first test and ensure that the pipeline achieves perfect results on a toy dataset we construct comprising disconnected cliques of varying sizes, each corresponding to a known community, where we use all nodes as seeds for growth during the prediction step.</p>
        <p id="P54">To learn potentially new human protein complexes, we apply Super.Complex on the human PPI network hu.MAP [<xref rid="R4" ref-type="bibr">4</xref>] using a community fitness function that is learned from known complexes in CORUM [<xref rid="R3" ref-type="bibr">3</xref>]. The network available on the website (<ext-link xlink:href="http://hu1.proteincomplexes.org/static/downloads/pairsWprob.txt" ext-link-type="uri">http://hu1.proteincomplexes.org/static/downloads/pairsWprob.txt</ext-link>) has 7778 nodes and 56,712 edges, after an edge weight cutoff of 0.0025 was applied to the original 64,048 edges. There are 188 complexes after data cleaning, a set we term as ‘refined CORUM’, out of the original 2916 human CORUM complexes, which underscores the importance of minimizing any losses in the merging and splitting steps of the pipeline. In the data cleaning process, overlapping complexes with a Jaccard coefficient <italic toggle="yes">j</italic> greater than 0.6 are removed, as this value was used in the experiments of hu.MAP 2-stage clustering. Note that of the complexes from CORUM that were removed, there were over 1000 complexes that had fewer than 3 members, and the remaining removed complexes consisted of duplicates and disconnected complexes with edges from hu.MAP. Note, however, that hu.MAP was the highest confidence human protein interaction network integrating 3 large previous human protein-interaction networks, all built using high confidence data from large-scale (~9000) laboratory experiments. The edge weights of hu.MAP were trained using an SVM based on features obtained from experiments.</p>
      </sec>
      <sec id="S22">
        <title>Experiment results.</title>
        <p id="P55">The best results, following different parameter sweeps from the experiment on hu.MAP are given in <xref rid="F4" ref-type="fig">Fig 4</xref> with the best parameter values given in <xref rid="T1" ref-type="table">Table 1</xref>. From <xref rid="F4" ref-type="fig">Fig 4e</xref>, we verify that the size distributions of the train and test sets are similar. In <xref rid="F4" ref-type="fig">Fig 4a</xref>, we can see that we get a good precision-recall curve on the test set for the subgraph classification task as a positive or negative community, achieving an average precision score of 0.88 with a logistic regression model (which is the final ML model stacked on a set of other ML models and processors, output as the best model trained on the training set with 5-fold cross-validation and achieving a cross-validation score of 0.978).</p>
        <p id="P56">We use <xref rid="F4" ref-type="fig">Fig 4e</xref> to set the maximum number of steps taken in the candidate complex growth stage as 20 and learn a total of 1028 complexes. On removal of non-gold standard proteins from these complexes for evaluation purposes, we obtain 131 complexes We get a good PR curve for the prediction of co-complex edges in comparison with known complex edges, as shown in <xref rid="F4" ref-type="fig">Fig 4b</xref>. From <xref rid="F4" ref-type="fig">Fig 4c</xref>, we can see that the best learned complex matches for known complexes have high F-similarity scores. Also, from <xref rid="F4" ref-type="fig">Fig 4d</xref>, we can see that the best known complex matches for learned complexes have high F-similarity scores. Note that there may be unknown but true complexes that are learned by the algorithm that contribute to false positives.</p>
        <p id="P57">In <xref rid="F4" ref-type="fig">Fig 4e</xref>, we can see that learned complexes have a similar size distribution as known complexes. The small peak at size 20 is an artifact of our threshold on the maximum number of steps that can be taken in growing the complex. This means that either of our stopping criteria was not reached for these complexes, <italic toggle="yes">i.e</italic>. the criteria of a score less than 0.5 or no observed score improvement over a specified number of steps (here, 10).</p>
        <p id="P58">Evaluation measures comparing learned complexes on hu.MAP by Super.Complex w.r.t known complexes from CORUM are given in <xref rid="T2" ref-type="table">Tables 2</xref> and <xref rid="SD1" ref-type="supplementary-material">S1 File Table 1</xref>, along with the measures computed on the protein complexes comprising hu.MAP obtained from a 2 stage clustering method with the unsupervised ClusterONE algorithm applied first, followed by the unsupervised MCL algorithm. We observe that Super.Complex does better in terms of precision, as can be seen with the higher FMM precision value, while ClusterONE+MCL does better in terms of recall. This can be attributed to more number of complexes learned by ClusterONE+MCL (~4000 compared to ~1000 by Super.Complex) including a few highly overlapping complexes (the maximum pairwise overlap observed was 0.97 Jaccard coefficient), compared to the strict low overlap among complexes learned by Super.Complex (the maximum pairwise overlap observed was 0.36 Jaccard coefficient). We observe 4152 pairs of complexes learned by ClusterONE + MCL having an overlap greater than 0.36 Jaccard coefficient, the maximum pairwise overlap observed in learned complexes from Super.Complex. Note that while the values of F1 evaluation measures are similar, the results from ClusterONE+MCL were achieved by the authors after significant cross-validation, while Super.Complex was faster as detailed in <xref rid="SD1" ref-type="supplementary-material">S1 File Results</xref> (Performance).</p>
      </sec>
    </sec>
    <sec id="S23">
      <title>State of the art comparison: Super.Complex achieves good evaluation measures and performance</title>
      <p id="P59">To compare our method with published results from existing methods, we perform experiments on the data used by these methods in their experiments - a yeast PPI network, DIP - Database of Interacting Proteins [<xref rid="R23" ref-type="bibr">23</xref>] with known protein complexes from MIPS - Munich Information Center for Protein Sequence [<xref rid="R24" ref-type="bibr">24</xref>] and TAP- Tandem Affinity Purification [<xref rid="R25" ref-type="bibr">25</xref>]. Specifically, for an accurate comparison, we use the same PPI network (projection of DIP yeast PPI network on MIPS + TAP proteins) and known protein complexes, available from the ClusterEPs software website. The results from <xref rid="T3" ref-type="table">Table 3</xref> show that our method outperforms all 6 supervised as well as 4 unsupervised methods (by achieving the highest F1 score and precision values) in the yeast experiments. Specifically, Super.Complex achieves the highest F1 score value (87% higher on average, 63% higher by median) when compared to the 10 other methods, highest precision value (110% higher on average, 72% higher by median) when compared to the 10 other methods, higher recall (92 % higher on average, 45% higher by median) when compared to 8 other algorithms with lower recall values (30% lower on average and by median) when compared to only 2 methods (ClusterSS and ClusterEPs, considered next best as per the F1 score, a metric which gives a better notion of the performance of an algorithm than just the recall or precision measure taken alone). When comparing with the 2 algorithms where Super.Complex has lower recall, it makes up for this by significantly outperforming the precision measure (55% higher on average and by median) to achieve higher F-1 scores (12% higher on average and 14% higher by median). Also, as we have noted earlier, for this application of detecting protein complexes, validation of results usually involves time-taking and expensive biological experiments, therefore, an algorithm like Super.Complex yielding a low number of false positives (translating to high precision) is more desirable (even with lower recall) than an algorithm that is able to identify many existing communities but with high false positive rates (translating to higher recall but low precision). From <xref rid="T3" ref-type="table">Table 3</xref>, similar to observations of metrics from the experiments on hu.MAP in <xref rid="T2" ref-type="table">Tables 2</xref> and <xref rid="SD1" ref-type="supplementary-material">S1 File Table 1</xref>, we obtain high precision values with Super.Complex, suggesting that many of the learned protein complexes are of high quality. On performance, we discuss the time complexity of Super.Complex in the <xref rid="SD1" ref-type="supplementary-material">S1 File Methods</xref> (Time complexity). The whole pipeline was completed in an order of minutes with Super.Complex (including the AutoML step executed on a single Skylake compute node, along with parameter-sweeps for the candidate community sampling step executed on 4 Skylake compute nodes - each with 48 cores @2GHz clock rate). We attempted to run other algorithms on hu.MAP as well, but were unsuccessful due to unavailability of code or limited scalability, as detailed in <xref rid="SD1" ref-type="supplementary-material">S1 File Results</xref> (SOTA availability).</p>
    </sec>
    <sec id="S24">
      <title>Learned human protein complexes from Super.Complex, and applications to COVID-19 and characterizing unknown proteins</title>
      <p id="P60">We provide interactive lists and visualizations of the <ext-link xlink:href="https://meghanapalukuri.github.io/Complexes/Complex2proteins.html" ext-link-type="uri">1028 learned human protein complexes</ext-link> by Super.Complex, along with <ext-link xlink:href="https://meghanapalukuri.github.io/Complexes/CORUM_Complex2proteins.html" ext-link-type="uri">refined</ext-link> and <ext-link xlink:href="https://meghanapalukuri.github.io/Complexes/originalCORUM_Complex2proteins.html" ext-link-type="uri">original</ext-link> CORUM complexes as a resource on <ext-link xlink:href="https://sites.google.com/view/supercomplex/super-complex-v3-0" ext-link-type="uri">https://sites.google.com/view/supercomplex/super-complex-v3-0</ext-link>. The high precision values obtained by Super.Complex in <xref rid="T2" ref-type="table">Table 2</xref> suggest that many of the learned complexes are of high quality, since the ones with proteins from known complexes match individual known complexes closely. We provide individual community fitness function scores for each of the learned complexes, and rank the list of learned complexes by this score to help identify good candidates for investigation for various applications. In this section, we analyze learned human protein complexes by Super.Complex, aiming to provide easily accessible resources for two biological applications that can be investigated further by researchers in the future. We highlight learned complexes with uncharacterized proteins to provide experimental candidates for functional characterization. In the second application, we construct an interactive map of SARS-CoV-2 protein interactions with 234 learned human protein complexes from Super.Complex using protein-interaction information between SARS-CoV-2 proteins and human proteins [<xref rid="R26" ref-type="bibr">26</xref>]. We also provide a list of complexes interacting with SARS-CoV-2 proteins ranked by their possible importance, which can be used to determine potential COVID-19 drug targets (<xref rid="SD1" ref-type="supplementary-material">S1 Fig</xref> &amp; <xref rid="SD1" ref-type="supplementary-material">S1 File Results</xref> (SARS-CoV-2 affected protein complexes)).</p>
      <sec id="S25">
        <title>Uncharacterized proteins and their complexes.</title>
        <p id="P61">111 uncharacterized proteins (Uniprot [<xref rid="R27" ref-type="bibr">27</xref>] annotation score unknown or less than 3) and their corresponding learned 103 complexes are presented on the website (<ext-link xlink:href="https://meghanapalukuri.github.io/Complexes/*" ext-link-type="uri">https://meghanapalukuri.github.io/Complexes/*</ext-link> where * is <ext-link xlink:href="https://meghanapalukuri.github.io/Complexes/Protein2complex_annotated.html" ext-link-type="uri">Protein2complex_annotated.html</ext-link> and <ext-link xlink:href="https://meghanapalukuri.github.io/Complexes/Complex2proteins_annotated.html" ext-link-type="uri">Complex2proteins_annotated.html</ext-link>). Three examples of uncharacterized proteins (C11orf42, C18orf21, and C16orf91) along with their corresponding complexes are highlighted in <bold>Fig 6</bold>. C11orf42 could potentially be related to trafficking, as it is a part of a complex with 30% similarity to the retromer complex, (<italic toggle="yes">i.e</italic>. with 0.3 Jaccard similarity to the known CORUM retromer complex), with additional evidence available from the Human Protein Atlas (HPA) [<xref rid="R28" ref-type="bibr">28</xref>] (available from <ext-link xlink:href="http://www.proteinatlas.org" ext-link-type="uri">http://www.proteinatlas.org</ext-link>) showing subcellular localization to vesicles, similar to other proteins of the complex. C18orf21 also has evidence from HPA, localized to the nucleoli and interacting with other proteins of a complex with 50% similarity to the Rnase/Mrp complex with most members in the nucleoli/nucleoplasm. Further evidence from [<xref rid="R29" ref-type="bibr">29</xref>] also independently supports C18orf21 as a cellular component of the ribonuclease MRP complex and a participant in ribonuclease P RNA binding as it exhibits significant co-essentiality across cancer cell lines with the POP4, POP5, POP7, RPP30, RPP38, and RPP40 proteins. C16orf91 could potentially be localized to mitochondria like other proteins of the COX 20-C16orf91-UQCC1 complex, with independent experimental evidence from [<xref rid="R30" ref-type="bibr">30</xref>].</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material id="SD1" position="float" content-type="local-data">
      <label>Supplement 1</label>
      <caption>
        <p id="P63"><bold>S1 File. Document containing supplementary tables, results and methods</bold>. a. <bold>Table 1</bold>. Comparing Super.Complex with 2-stage clustering on the hu.MAP dataset using 6 existing and 3 new evaluation metrics shows comparable performance for both algorithms.</p>
        <p id="P171">b. <bold>Results</bold>: Algorithm Guarantees, Robustness of the Super.Complex algorithm, Performance, SOTA Availability, and SARS-CoV-2 affected protein complexes.</p>
        <p id="P172">c. <bold>Methods</bold>: Topological Features, Similarity measures for evaluation, Evaluation with existing measures, Time complexity</p>
        <p id="P64"><bold>S1 Fig. SARS-CoV-2 - human protein complex map showing complexes identified by Super.Complex. a</bold>. A section of the full map, featuring SARS-CoV-2 nsp4 and orf6 and their interacting human protein complexes <bold>b</bold>. A protein complex with a 30% match to the Nup 107–160 subcomplex interacts with both SARS-CoV-2 nsp4 and orf6 <bold>c</bold>. Map of SARS-CoV-2 nsp2 interactions with human proteins and their corresponding complexes <bold>d</bold>. A complex with a 20% match to the Endosomal targeting complex, and <bold>e</bold>. A complex with a 40% match to the retromer complex, both of which interact with SARS-CoV-2 nsp2. An interactive map is available at <ext-link xlink:href="https://meghanapalukuri.github.io/Complexes/SARS_COV2_Map_only_mapped_complexes_names.html" ext-link-type="uri">https://meghanapalukuri.github.io/Complexes/SARS_COV2_Map_only_mapped_complexes_names.html</ext-link>.</p>
      </caption>
      <media xlink:href="media-1.pdf" id="d64e1489" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S27">
    <title>Acknowledgments</title>
    <p id="P65">The authors gratefully acknowledge Benjamin Liebeskind for a TPOT code wrapper, Kevin Drew for computing some previous evaluation measures, Claire McWhite for critical reading of the manuscript, and funding from the Welch Foundation (F-1515) and National Institutes of Health (R35 GM122480, R01 HD085901, R21 HD103588) to E.M.M. The funders’ websites are <ext-link xlink:href="https://www.welch1.org/" ext-link-type="uri">https://www.welch1.org/</ext-link> and <ext-link xlink:href="https://www.nih.gov/" ext-link-type="uri">https://www.nih.gov/</ext-link> respectively. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
  </ack>
  <sec sec-type="data-availability" id="S26">
    <title>Data and Code Availability</title>
    <p id="P62">We make interactive visualizations of our learned protein complexes freely available as a resource at <ext-link xlink:href="https://sites.google.com/view/supercomplex/super-complex-v3-0" ext-link-type="uri">https://sites.google.com/view/supercomplex/super-complex-v3-0</ext-link>, which includes downloadable sets of interactions and complexes, including the 234 complexes that are potentially linked to COVID-19 and SARS-CoV-2 infection, and the set of 111 uncharacterized proteins implicated in 103 complexes. Our code is available at <ext-link xlink:href="https://github.com/marcottelab/super.complex" ext-link-type="uri">https://github.com/marcottelab/super.complex</ext-link>. To simplify reanalysis, the full interactome datasets are additionally deposited in Zenodo, DOI: <ext-link xlink:href="10.5281/zenodo.4814944" ext-link-type="doi">http://doi.org/10.5281/zenodo.4814944</ext-link></p>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><name><surname>Fortunato</surname><given-names>S.</given-names></name><article-title>Community detection in graphs</article-title>. <source>Physics reports</source>. <year>2010</year><month>Feb</month><day>1</day>;<volume>486</volume>(<issue>3</issue>):<fpage>75</fpage>–<lpage>174</lpage>.</mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><name><surname>Qi</surname><given-names>Y</given-names></name>, <name><surname>Balem</surname><given-names>F</given-names></name>, <name><surname>Faloutsos</surname><given-names>C</given-names></name>, <name><surname>Klein-Seetharaman</surname><given-names>J</given-names></name>, <name><surname>Bar-Joseph</surname><given-names>Z</given-names></name>. <article-title>Protein complex identification by supervised graph local clustering</article-title>. <source>Bioinformatics</source>. <year>2008</year><month>Jul</month><day>1</day>;<volume>24</volume>(<issue>13</issue>):<fpage>i250</fpage>–<lpage>68</lpage>.<pub-id pub-id-type="pmid">18586722</pub-id></mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><name><surname>Giurgiu</surname><given-names>M</given-names></name>, <name><surname>Reinhard</surname><given-names>J</given-names></name>, <name><surname>Brauner</surname><given-names>B</given-names></name>, <name><surname>Dunger-Kaltenbach</surname><given-names>I</given-names></name>, <name><surname>Fobo</surname><given-names>G</given-names></name>, <name><surname>Frishman</surname><given-names>G</given-names></name>, <etal/><article-title>CORUM: the comprehensive resource of mammalian protein complexes-2019</article-title>. <source>Nucleic Acids Research</source>. <year>2019</year><month>Jan</month><day>8</day>;<volume>47</volume>(<issue>D1</issue>):<fpage>D559</fpage>–<lpage>63</lpage>.<pub-id pub-id-type="pmid">30357367</pub-id></mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><name><surname>Drew</surname><given-names>K</given-names></name>, <name><surname>Lee</surname><given-names>C</given-names></name>, <name><surname>Huizar</surname><given-names>RL</given-names></name>, <name><surname>Tu</surname><given-names>F</given-names></name>, <name><surname>Borgeson</surname><given-names>B</given-names></name>, <name><surname>McWhite</surname><given-names>CD</given-names></name>, <etal/><article-title>Integration of over 9,000 mass spectrometry experiments builds a global map of human protein complexes</article-title>. <source>Molecular Systems Biology</source>. <year>2017</year><month>08</month>;<volume>13</volume>(<issue>6</issue>):<fpage>932</fpage>.<pub-id pub-id-type="pmid">28596423</pub-id></mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>C</given-names></name>, <name><surname>Reid</surname><given-names>F</given-names></name>, <name><surname>McDaid</surname><given-names>A</given-names></name>, <name><surname>Hurley</surname><given-names>N</given-names></name>. <article-title>Detecting highly overlapping community structure by greedy clique expansion</article-title>. <source>ArXiv10021827 Phys [Internet]</source>. <year>2010</year><month>Jun</month><day>15</day> [<comment>cited</comment>
<date-in-citation>2020 Oct 28</date-in-citation>]; <comment>Available from: <ext-link xlink:href="http://arxiv.org/abs/1002.1827" ext-link-type="uri">http://arxiv.org/abs/1002.1827</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="webpage"><name><surname>van Dongen</surname><given-names>SM</given-names></name>. <source>Graph clustering by flow simulation [Internet]</source>. <year>2000</year> [<comment>cited</comment>
<date-in-citation>2019 Dec 9</date-in-citation>]. <comment>Available from: <ext-link xlink:href="http://dspace.library.uu.nl/handle/1874/848" ext-link-type="uri">http://dspace.library.uu.nl/handle/1874/848</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><name><surname>Bader</surname><given-names>GD</given-names></name>, <name><surname>Hogue</surname><given-names>CW</given-names></name>. <article-title>An automated method for finding molecular complexes in large protein interaction networks</article-title>. <source>BMC Bioinformatics</source>. <year>2003</year><month>Jan</month><day>13</day>;<volume>4</volume>(<issue>1</issue>):<fpage>2</fpage>.<pub-id pub-id-type="pmid">12525261</pub-id></mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><name><surname>Adamcsek</surname><given-names>B</given-names></name>, <name><surname>Palla</surname><given-names>G</given-names></name>, <name><surname>Farkas</surname><given-names>IJ</given-names></name>, <name><surname>Derényi</surname><given-names>I</given-names></name>, <name><surname>Vicsek</surname><given-names>T</given-names></name>. <article-title>CFinder: locating cliques and overlapping modules in biological networks</article-title>. <source>Bioinformatics</source>. <year>2006</year><month>Apr</month><day>15</day>;<volume>22</volume>(<issue>8</issue>):<fpage>1021</fpage>–<lpage>3</lpage>.<pub-id pub-id-type="pmid">16473872</pub-id></mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><name><surname>Mete</surname><given-names>M</given-names></name>, <name><surname>Tang</surname><given-names>F</given-names></name>, <name><surname>Xu</surname><given-names>X</given-names></name>, <name><surname>Yuruk</surname><given-names>N</given-names></name>. <article-title>A structural approach for finding functional modules from large biological networks</article-title>. <source>BMC Bioinformatics</source>. <year>2008</year><month>Aug</month><day>12</day>;<volume>9</volume>(<issue>Suppl 9</issue>):<fpage>S19</fpage>.</mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>G</given-names></name>, <name><surname>Wong</surname><given-names>L</given-names></name>, <name><surname>Chua</surname><given-names>HN</given-names></name>. <article-title>Complex discovery from weighted PPI networks</article-title>. <source>Bioinformatics</source>. <year>2009</year><month>Aug</month><day>1</day>;<volume>25</volume>(<issue>15</issue>):<fpage>1891</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">19435747</pub-id></mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><name><surname>Wu</surname><given-names>M</given-names></name>, <name><surname>Li</surname><given-names>X</given-names></name>, <name><surname>Kwoh</surname><given-names>C-K</given-names></name>, <name><surname>Ng</surname><given-names>S-K</given-names></name>. <article-title>A core-attachment based method to detect protein complexes in PPI networks</article-title>. <source>BMC Bioinformatics</source>. <year>2009</year><month>Jun</month><day>2</day>;<volume>10</volume>(<issue>1</issue>):<fpage>169</fpage>.<pub-id pub-id-type="pmid">19486541</pub-id></mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><name><surname>Nepusz</surname><given-names>T</given-names></name>, <name><surname>Yu</surname><given-names>H</given-names></name>, <name><surname>Paccanaro</surname><given-names>A</given-names></name>. <article-title>Detecting overlapping protein complexes in protein-protein interaction networks</article-title>. <source>Nature Methods</source>. <year>2012</year><month>Mar</month><day>18</day>;<volume>9</volume>(<issue>5</issue>):<fpage>471</fpage>–<lpage>2</lpage>.<pub-id pub-id-type="pmid">22426491</pub-id></mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="book"><name><surname>Wu</surname><given-names>J</given-names></name>, <name><surname>Lin</surname><given-names>M</given-names></name>. <part-title>Protein Complex Detection Based on Semi-Supervised Matrix Factorization</part-title>. In: <source>2018 37th Chinese Control Conference (CCC) [Internet]</source>. <publisher-loc>Wuhan</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2018</year> [<comment>cited</comment>
<date-in-citation>2020 Dec 9</date-in-citation>]. p. <fpage>8205</fpage>–<lpage>8</lpage>. <comment>Available from: <ext-link xlink:href="https://ieeexplore.ieee.org/document/8484055/" ext-link-type="uri">https://ieeexplore.ieee.org/document/8484055/</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><name><surname>Shi</surname><given-names>L</given-names></name>, <name><surname>Lei</surname><given-names>X</given-names></name>, <name><surname>Zhang</surname><given-names>A</given-names></name>. <article-title>Protein complex detection with semi-supervised learning in protein interaction networks</article-title>. <source>Proteome Science</source>. <year>2011</year>;<volume>9</volume>(<issue>Suppl 1</issue>):<fpage>S5</fpage>.<pub-id pub-id-type="pmid">22165896</pub-id></mixed-citation>
    </ref>
    <ref id="R15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><name><surname>Yu</surname><given-names>F</given-names></name>, <name><surname>Yang</surname><given-names>Z</given-names></name>, <name><surname>Tang</surname><given-names>N</given-names></name>, <name><surname>Lin</surname><given-names>H</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Yang</surname><given-names>Z</given-names></name>. <article-title>Predicting protein complex in protein interaction network - a supervised learning based method</article-title>. <source>BMC Systems Biology</source>. <year>2014</year>;<volume>8</volume>(<issue>Suppl 3</issue>):<fpage>S4</fpage>.</mixed-citation>
    </ref>
    <ref id="R16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><name><surname>Dong</surname><given-names>Y</given-names></name>, <name><surname>Sun</surname><given-names>Y</given-names></name>, <name><surname>Qin</surname><given-names>C</given-names></name>. <article-title>Predicting protein complexes using a supervised learning method combined with local structural information</article-title>. <name><surname>Keskin</surname><given-names>O</given-names></name>, editor. <source>PLOS ONE</source>. <year>2018</year><month>Mar</month><day>19</day>;<volume>13</volume>(<issue>3</issue>):<fpage>e0194124</fpage>.<pub-id pub-id-type="pmid">29554120</pub-id></mixed-citation>
    </ref>
    <ref id="R17">
      <label>17.</label>
      <mixed-citation publication-type="book"><name><surname>Zhou</surname><given-names>Ziwei</given-names></name>, <name><surname>Gui</surname><given-names>Yingyi</given-names></name>, <name><surname>Yang</surname><given-names>Z</given-names></name>, <name><surname>Xiaoxia</surname><given-names>Liu</given-names></name>, <name><surname>Wang</surname><given-names>Lei</given-names></name>, <name><surname>Zhang</surname><given-names>Yin</given-names></name>, <etal/><part-title>Disease-specific protein complex detection in the human protein interaction network with a supervised learning method</part-title>. In: <source>2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) [Internet]</source>. <publisher-loc>Shenzhen, China</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2016</year> [<comment>cited</comment>
<date-in-citation>2019 Dec 8</date-in-citation>]. p. <fpage>1296</fpage>–<lpage>301</lpage>. <comment>Available from: <ext-link xlink:href="http://ieeexplore.ieee.org/document/7822705/" ext-link-type="uri">http://ieeexplore.ieee.org/document/7822705/</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R18">
      <label>18.</label>
      <mixed-citation publication-type="book"><name><surname>Olson</surname><given-names>Randy</given-names></name>, <name><surname>Fu</surname><given-names>Weixuan</given-names></name>, <name><surname>Nathan</surname><given-names>PGijsbers</given-names></name>, <name><surname>Jena</surname><given-names>Grishma</given-names></name>, <name><surname>Augspurger</surname><given-names>Tom</given-names></name>, <etal/><source>EpistasisLab/tpot: v0.10.1 minor release [Internet].</source><publisher-name>Zenodo</publisher-name>; <year>2019</year> [<comment>cited</comment>
<date-in-citation>2019 Dec 9</date-in-citation>]. <comment>Available from: <ext-link xlink:href="https://zenodo.org/record/2647523#.Xe7Q5Px7nv9" ext-link-type="uri">https://zenodo.org/record/2647523#.Xe7Q5Px7nv9</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><name><surname>Traag</surname><given-names>VA</given-names></name>, <name><surname>Waltman</surname><given-names>L</given-names></name>, <name><surname>van Eck</surname><given-names>NJ</given-names></name>. <article-title>From Louvain to Leiden: guaranteeing well-connected communities</article-title>. <source>Scientific Reports</source>. <year>2019</year><month>Mar</month><day>26</day>;<volume>9</volume>(<issue>1</issue>):<fpage>5233</fpage>.<pub-id pub-id-type="pmid">30914743</pub-id></mixed-citation>
    </ref>
    <ref id="R20">
      <label>20.</label>
      <mixed-citation publication-type="webpage"><name><surname>Borgeson</surname><given-names>BC</given-names></name>. <source>All-by-all discovery of conserved protein complexes by deep proteome fractionation [Internet] [Thesis]</source>. <year>2016</year> [<comment>cited</comment>
<date-in-citation>2020 Dec 9</date-in-citation>]. <comment>Available from: <ext-link xlink:href="https://repositories.lib.utexas.edu/handle/2152/46875" ext-link-type="uri">https://repositories.lib.utexas.edu/handle/2152/46875</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><name><surname>Karp</surname><given-names>RM</given-names></name>. <article-title>An algorithm to solve the m × n assignment problem in expected time O(mn log n)</article-title>. <source>Networks</source>. <year>1980</year>;<volume>10</volume>(<issue>2</issue>):<fpage>143</fpage>–<lpage>52</lpage>.</mixed-citation>
    </ref>
    <ref id="R22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><name><surname>Brohée</surname><given-names>S</given-names></name>, <name><surname>van Helden</surname><given-names>J</given-names></name>. <article-title>Evaluation of clustering algorithms for protein-protein interaction networks</article-title>. <source>BMC Bioinformatics</source>. <year>2006</year><month>Dec</month>;<volume>7</volume>(<issue>1</issue>):<fpage>488</fpage>.<pub-id pub-id-type="pmid">17087821</pub-id></mixed-citation>
    </ref>
    <ref id="R23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><name><surname>Xenarios</surname><given-names>I</given-names></name>, <name><surname>Salwínski</surname><given-names>L</given-names></name>, <name><surname>Duan</surname><given-names>XJ</given-names></name>, <name><surname>Higney</surname><given-names>P</given-names></name>, <name><surname>Kim</surname><given-names>S-M</given-names></name>, <name><surname>Eisenberg</surname><given-names>D</given-names></name>. <article-title>DIP, the Database of Interacting Proteins: a research tool for studying cellular networks of protein interactions</article-title>. <source>Nucleic Acids Research</source>. <year>2002</year><month>Jan</month><day>1</day>;<volume>30</volume>(<issue>1</issue>):<fpage>303</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">11752321</pub-id></mixed-citation>
    </ref>
    <ref id="R24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><name><surname>Mewes</surname><given-names>HW</given-names></name>, <name><surname>Amid</surname><given-names>C</given-names></name>, <name><surname>Arnold</surname><given-names>R</given-names></name>, <name><surname>Frishman</surname><given-names>D</given-names></name>, <name><surname>Güldener</surname><given-names>U</given-names></name>, <name><surname>Mannhaupt</surname><given-names>G</given-names></name>, <etal/><article-title>MIPS: analysis and annotation of proteins from whole genomes</article-title>. <source>Nucleic Acids Research</source>. <year>2004</year><month>Jan</month><day>1</day>;<volume>32</volume>(<issue>Database issue</issue>):<fpage>D41</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">14681354</pub-id></mixed-citation>
    </ref>
    <ref id="R25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><name><surname>Gavin</surname><given-names>A-C</given-names></name>, <name><surname>Aloy</surname><given-names>P</given-names></name>, <name><surname>Grandi</surname><given-names>P</given-names></name>, <name><surname>Krause</surname><given-names>R</given-names></name>, <name><surname>Boesche</surname><given-names>M</given-names></name>, <name><surname>Marzioch</surname><given-names>M</given-names></name>, <etal/><article-title>Proteome survey reveals modularity of the yeast cell machinery</article-title>. <source>Nature</source>. <year>2006</year><month>Mar</month><day>30</day>;<volume>440</volume>(<issue>7084</issue>):<fpage>631</fpage>–<lpage>6</lpage>.<pub-id pub-id-type="pmid">16429126</pub-id></mixed-citation>
    </ref>
    <ref id="R26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><name><surname>Gordon</surname><given-names>DE</given-names></name>, <name><surname>Jang</surname><given-names>GM</given-names></name>, <name><surname>Bouhaddou</surname><given-names>M</given-names></name>, <name><surname>Xu</surname><given-names>J</given-names></name>, <name><surname>Obernier</surname><given-names>K</given-names></name>, <name><surname>White</surname><given-names>KM</given-names></name>, <etal/><article-title>A SARS-CoV-2 protein interaction map reveals targets for drug repurposing</article-title>. <source>Nature</source>. <year>2020</year><month>Jul</month>;<volume>583</volume>(<issue>7816</issue>):<fpage>459</fpage>–<lpage>68</lpage>.<pub-id pub-id-type="pmid">32353859</pub-id></mixed-citation>
    </ref>
    <ref id="R27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><collab>The UniProt Consortium</collab>. <article-title>UniProt: the universal protein knowledgebase in 2021</article-title>. <source>Nucleic Acids Research</source>. <year>2021</year><month>Jan</month><day>8</day>;<volume>49</volume>(<issue>D1</issue>):<fpage>D480</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">33237286</pub-id></mixed-citation>
    </ref>
    <ref id="R28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><name><surname>Thul</surname><given-names>PJ</given-names></name>, <name><surname>Åkesson</surname><given-names>L</given-names></name>, <name><surname>Wiking</surname><given-names>M</given-names></name>, <name><surname>Mahdessian</surname><given-names>D</given-names></name>, <name><surname>Geladaki</surname><given-names>A</given-names></name>, <name><surname>Blal</surname><given-names>HA</given-names></name>, <etal/><article-title>A subcellular map of the human proteome</article-title>. <source>Science [Internet]</source>. <year>2017</year><month>May</month><day>26</day> [<comment>cited</comment>
<date-in-citation>2021 Apr 16</date-in-citation>];<volume>356</volume>(<issue>6340</issue>). <comment>Available from: <ext-link xlink:href="https://science.sciencemag.org/content/356/6340/eaal3321" ext-link-type="uri">https://science.sciencemag.org/content/356/6340/eaal3321</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><name><surname>Wainberg</surname><given-names>M</given-names></name>, <name><surname>Kamber</surname><given-names>RA</given-names></name>, <name><surname>Balsubramani</surname><given-names>A</given-names></name>, <name><surname>Meyers</surname><given-names>RM</given-names></name>, <name><surname>Sinnott-Armstrong</surname><given-names>N</given-names></name>, <name><surname>Hornburg</surname><given-names>D</given-names></name>, <etal/><article-title>A genome-wide atlas of co-essential modules assigns function to uncharacterized genes</article-title>. <source>Nature Genetics</source>. <year>2021</year><month>Apr</month><day>15</day>;<fpage>1</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">33414547</pub-id></mixed-citation>
    </ref>
    <ref id="R30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>H</given-names></name>, <name><surname>Rukina</surname><given-names>D</given-names></name>, <name><surname>David</surname><given-names>FPA</given-names></name>, <name><surname>Li</surname><given-names>TY</given-names></name>, <name><surname>Oh</surname><given-names>C-M</given-names></name>, <name><surname>Gao</surname><given-names>AW</given-names></name>, <etal/><article-title>Identifying gene function and module connections by the integration of multispecies expression compendia</article-title>. <source>Genome Research</source>. <year>2019</year><month>Dec</month><day>1</day>;<volume>29</volume>(<issue>12</issue>):<fpage>2034</fpage>–<lpage>45</lpage>.<pub-id pub-id-type="pmid">31754022</pub-id></mixed-citation>
    </ref>
    <ref id="R31">
      <label>31.</label>
      <mixed-citation publication-type="webpage"><name><surname>Palukuri</surname><given-names>M</given-names></name>, <name><surname>Marcotte</surname><given-names>E</given-names></name>. <source>Supervised Community Detection in Protein-interaction Networks. TACCSTER 2019 Proceedings [Internet].</source><year>2019</year> [<comment>cited</comment>
<date-in-citation>2020 Oct 29</date-in-citation>]; <comment>Available from: <ext-link xlink:href="https://repositories.lib.utexas.edu/handle/2152/79826" ext-link-type="uri">https://repositories.lib.utexas.edu/handle/2152/79826</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R32">
      <label>32.</label>
      <mixed-citation publication-type="book"><name><surname>Wilson</surname><given-names>LA</given-names></name>, <name><surname>Fonner</surname><given-names>JM</given-names></name>. <part-title>Launcher: A Shell-based Framework for Rapid Development of Parallel Parametric Studies</part-title>. In: <source>Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment [Internet]</source>. <publisher-loc>New York, NY, USA</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>; <year>2014</year> [<comment>cited</comment>
<date-in-citation>2020 Dec 5</date-in-citation>]. p. <fpage>1</fpage>–<lpage>8</lpage>. <comment>(XSEDE ‘14). Available from: </comment><pub-id pub-id-type="doi">10.1145/2616498.2616534</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Fig 1.</label>
    <caption>
      <title>Different topologies are exhibited by human protein complexes.</title>
      <p id="P66"><bold>a.</bold> Clique (<ext-link xlink:href="https://meghanapalukuri.github.io/Complexes/CORUM/Complex112.html" ext-link-type="uri">Commander/CCC complex</ext-link>), <bold>b.</bold> Hybrid with different edge-weights (<ext-link xlink:href="https://meghanapalukuri.github.io/Complexes/CORUM/Complex22.html" ext-link-type="uri">BLOC-1</ext-link> (biogenesis of lysosome-related organelles complex 1)), c. Hybrid (<ext-link xlink:href="https://meghanapalukuri.github.io/Complexes/CORUM/Complex119.html" ext-link-type="uri">NRD complex</ext-link> (Nucleosome remodeling and deacetylation complex), <bold>d.</bold> Linear (<ext-link xlink:href="https://meghanapalukuri.github.io/Complexes/CORUM/Complex96.html" ext-link-type="uri">Ubiquitin E3 ligase</ext-link> (CUL3, KLHL9, KLHL13, RBX1)). These are experimentally characterized complexes from CORUM [<xref rid="R3" ref-type="bibr">3</xref>] with protein interaction evidence obtained from hu.MAP [<xref rid="R4" ref-type="bibr">4</xref>].</p>
    </caption>
    <graphic xlink:href="nihpp-2021.06.22.449395v2-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Fig 2.</label>
    <caption>
      <title>Super.Complex identifies likely protein complexes within a PPI network using a distributed supervised AutoML method.</title>
      <p id="P67">Task 1: Learning a community fitness function:</p>
      <p id="P68">(i) Topological feature extraction: Topological features are extracted from known communities to build community embeddings (feature vectors, which are representations of communities in vector space)</p>
      <p id="P69">(ii) Supervised learning with AutoML: A score function for communities, the community fitness function, is learned from the community embeddings as the decision function for binary classification of a network subgraph as a community or a random walk (illustration on the right). The best score function is selected after training multiple machine learning models with TPOT [<xref rid="R18" ref-type="bibr">18</xref>], an AutoML pipeline.</p>
      <p id="P70">Task 2: Searching for candidate communities in the network:</p>
      <p id="P71">(iii) Intelligent sampling: Multiple communities are sampled in parallel from the network. To build each candidate community, a seed edge is selected and grown using a 2-stage heuristic. First, we use an epsilon-greedy heuristic to select a candidate neighbor, and then we use a pseudo-metropolis (constant probability) or iterative simulated annealing heuristic to accept or reject the candidate neighbor for growing the current community. An iteration of neighbor selection using a greedy heuristic is shown (illustration on the left), starting from a seed edge {F, I}. The edge is grown to the subgraph {F, I, E} as adding node E yields a higher community fitness function than adding any other neighbor of F and I. The seed edge {B, C} is grown in parallel (not shown)</p>
      <p id="P72">(iv) Merging overlaps: The candidate communities are merged such that the maximum overlap between any 2 communities is not greater than a specified threshold.</p>
    </caption>
    <graphic xlink:href="nihpp-2021.06.22.449395v2-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Fig 3.</label>
    <caption>
      <title>Proposed evaluation measures - FMMF, CMFF, and UnSPA are sensitive metrics.</title>
      <p id="P73"><bold>a.</bold> Bipartite graph, where each edge weight corresponds to the F-similarity (<italic toggle="yes">sim</italic><sub><italic toggle="yes">F</italic></sub>(<italic toggle="yes">C</italic><sub><italic toggle="yes">k</italic></sub>, <italic toggle="yes">C</italic><sub><italic toggle="yes">l</italic></sub>)) between <italic toggle="yes">C</italic><sub><italic toggle="yes">k</italic></sub>, a known community from <italic toggle="yes">K</italic>, the set of known communities and <italic toggle="yes">C</italic><sub><italic toggle="yes">l</italic></sub>, a learned community from <italic toggle="yes">L</italic>, the set of learned communities. <bold>b.</bold> The F-similarity score combines precision (<italic toggle="yes">P</italic>(<italic toggle="yes">C</italic><sub><italic toggle="yes">k</italic></sub>, <italic toggle="yes">C</italic><sub><italic toggle="yes">l</italic></sub>)) and recall (<italic toggle="yes">R</italic>(<italic toggle="yes">C</italic><sub><italic toggle="yes">k</italic></sub>, <italic toggle="yes">C</italic><sub><italic toggle="yes">l</italic></sub>)) measures, computed as fractions of the number of common nodes w.r.t the number of nodes in a community. |<italic toggle="yes">C</italic>| is the number of nodes in community <italic toggle="yes">C</italic> and | <italic toggle="yes">C</italic><sub>1</sub> ∩ <italic toggle="yes">C</italic><sub>2</sub> | is the number of nodes common to both communities. <bold>c.</bold> F-similarity-based Maximal Matching F-score (FMMF) combines precision (<italic toggle="yes">P</italic><sub><italic toggle="yes">FFM</italic></sub>) and recall (<italic toggle="yes">R</italic><sub><italic toggle="yes">FFM</italic></sub>) measures computed for a maximal matching, <italic toggle="yes">M</italic> of the bipartite graph in <xref rid="F3" ref-type="fig">Fig 3a</xref>
<bold>d.</bold> Community-wise Maximum F-similarity based F-score (CMFF) combines precision (<italic toggle="yes">P</italic><sub><italic toggle="yes">CMF</italic></sub>) and recall (<italic toggle="yes">R</italic><sub><italic toggle="yes">CMF</italic></sub>) measures, averaging over the maximum F-similarity score for a community in a particular set (e.g. known communities) w.r.t to a community of the other set (e.g. learned communities) <bold>e.</bold> UnSPA is an unbiased version of Sn-PPV accuracy (SPA), computed as the geometric mean of unbiased PPV (<italic toggle="yes">PPV</italic><sub><italic toggle="yes">u</italic></sub>) and unbiased Sensitivity (<italic toggle="yes">Sn</italic><sub><italic toggle="yes">u</italic></sub>), computed similar to precision and recall measures in CMFF, only, instead of the F-similarity score, precision and recall similarity scores are used respectively <bold>f.</bold> Sensitivity of different evaluation measures w.r.t. (maximum pairwise Jaccard coefficient) overlap between communities shows that FMMF, CMFF, UnSPA, and existing measures Qi et al F1 score (<xref rid="SD1" ref-type="supplementary-material">S1 File equation 8</xref>), and SPA (<xref rid="SD1" ref-type="supplementary-material">S1 File equation 9</xref>) are sensitive metrics, with FMMF, CMFF, and Qi et al F1 score following the desired trend. Here, each data point on the plot corresponds to a measure evaluating an individual run of Super.Complex’s merging algorithm with a maximum Jaccard overlap threshold set to the x-axis value.</p>
    </caption>
    <graphic xlink:href="nihpp-2021.06.22.449395v2-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Fig 4.</label>
    <caption>
      <title>Learned human protein complexes with Super.Complex achieve good PR curves and follow similar size distributions as known complexes.</title>
      <p id="P74"><bold>a.</bold> PR curve for the best model (community fitness function) from the AutoML pipeline on the test dataset, for the task of classifying a subgraph as a community or not. <bold>b.</bold> Co-complex edge classification PR curve for final learned complexes. <bold>c &amp; d.</bold> Best F-similarity score distributions per known complex and per learned complex. <bold>e.</bold> The size distributions of train, test, and all known complexes, learned complexes, and learned complexes after removing known complex proteins.</p>
    </caption>
    <graphic xlink:href="nihpp-2021.06.22.449395v2-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Fig 5.</label>
    <caption>
      <title>Examples of complexes with proteins having low annotation scores.</title>
      <p id="P75"><bold>a.</bold> C11orf42 constitutes the <ext-link xlink:href="https://meghanapalukuri.github.io/Complexes/Predicted/Complex692.html" ext-link-type="uri">Retromer complex</ext-link> (SNX1, SNX2, VPS35, VPS29, VPS26A), potentially related to trafficking, with <ext-link xlink:href="https://www.proteinatlas.org/ENSG00000180878-C11orf42/cell" ext-link-type="uri">C11orf42</ext-link> localized in cells to vesicles, similar to the other proteins of the complex (<ext-link xlink:href="https://www.proteinatlas.org/ENSG00000028528-SNX1/cell" ext-link-type="uri">SNX1</ext-link>, <ext-link xlink:href="https://www.proteinatlas.org/ENSG00000089006-SNX5/cell" ext-link-type="uri">SNX5</ext-link>, and <ext-link xlink:href="https://www.proteinatlas.org/ENSG00000111237-VPS29/cell" ext-link-type="uri">VPS29</ext-link>) <bold>b.</bold> C16orf91 constitutes the <ext-link xlink:href="https://meghanapalukuri.github.io/Complexes/Predicted/Complex334.html" ext-link-type="uri">COX 20-C16orf91-UQCC1 complex</ext-link>, potentially localized to mitochondria like <ext-link xlink:href="https://www.proteinatlas.org/ENSG00000203667-COX20/cell" ext-link-type="uri">COX20</ext-link>. <bold>c.</bold> C18orf21 constitutes the <ext-link xlink:href="https://meghanapalukuri.github.io/Complexes/Predicted/Complex1012.html" ext-link-type="uri">Rnase/Mrp complex</ext-link>, with <ext-link xlink:href="https://www.proteinatlas.org/ENSG00000141428-C18orf21/cell" ext-link-type="uri">C18orf21</ext-link>, localized to nucleoli, closely interacting with nucleoplasm proteins of the complex such as <ext-link xlink:href="https://www.proteinatlas.org/ENSG00000178718-RPP25/cell" ext-link-type="uri">RPP25</ext-link>, <ext-link xlink:href="https://www.proteinatlas.org/ENSG00000167272-POP5/cell" ext-link-type="uri">POP5</ext-link>, <ext-link xlink:href="https://www.proteinatlas.org/ENSG00000163684-RPP14/cell" ext-link-type="uri">RPP14</ext-link>, <ext-link xlink:href="https://www.proteinatlas.org/ENSG00000163608-NEPRO/cell" ext-link-type="uri">NEPRO</ext-link>, <ext-link xlink:href="https://www.proteinatlas.org/ENSG00000148688-RPP30/cell" ext-link-type="uri">RPP30</ext-link>, <ext-link xlink:href="https://www.proteinatlas.org/ENSG00000005700-IBTK/cell" ext-link-type="uri">IBTK</ext-link>, <ext-link xlink:href="https://www.proteinatlas.org/ENSG00000164967-RPP25L/cell" ext-link-type="uri">RPP25L</ext-link>, and <ext-link xlink:href="https://www.proteinatlas.org/ENSG00000181163-NPM1/cell" ext-link-type="uri">NPM1</ext-link>. The images of subcellular localization are available from v20.1 of <ext-link xlink:href="http://proteinatlas.org" ext-link-type="uri">proteinatlas.org</ext-link>, as <ext-link xlink:href="https://v20.proteinatlas.org/ENSG00000*/cell" ext-link-type="uri">https://v20.proteinatlas.org/ENSG00000*/cell</ext-link>, where * is 180878-C11orf42, 028528-SNX1, 089006-SNX5, 111237-VPS29, 167272-POP5, 163608-NEPRO, 148688-RPP30, and 181163-NPM1. Note that localizations were measured in varying cell types, including HeLa, HEL, U2OS, and U-251 MG cells, across the highlighted proteins.</p>
    </caption>
    <graphic xlink:href="nihpp-2021.06.22.449395v2-f0005" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>Table 1.</label>
    <caption>
      <p id="P76">Best parameters found and used in each of the experiments.</p>
    </caption>
    <table frame="box" rules="all">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>PPI Network</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">Hu.MAP</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Yeast</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Yeast</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Yeast</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>Experiment</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">train: CORUM, test: CORUM (independent)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1. train: TAP, test: MIPS</td>
          <td align="left" valign="top" rowspan="1" colspan="1">2. train: MIPS, test: TAP</td>
          <td align="left" valign="top" rowspan="1" colspan="1">3. train: MIPS, test: MIPS</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>Seeds</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">All nodes</td>
          <td align="left" valign="top" rowspan="1" colspan="1">All nodes</td>
          <td align="left" valign="top" rowspan="1" colspan="1">All nodes</td>
          <td align="left" valign="top" rowspan="1" colspan="1">All nodes</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>No. of negatives sampled</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">10x positives</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1.1x positives</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1.1x positives</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1.1x positives</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>Size (no. of nodes) distribution for negatives</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">Uniform</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Uniform</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Uniform</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Uniform</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>Candidate sampling Method</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1"><italic toggle="yes">ϵ-</italic> greedy + iterative simulated annealing</td>
          <td align="left" valign="top" rowspan="1" colspan="1"><italic toggle="yes">ϵ-</italic> greedy + iterative simulated annealing</td>
          <td align="left" valign="top" rowspan="1" colspan="1"><italic toggle="yes">ϵ-</italic> greedy + pseudo-metropolis</td>
          <td align="left" valign="top" rowspan="1" colspan="1"><italic toggle="yes">ϵ-</italic> greedy + iterative simulated annealing</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>
              <italic toggle="yes">ϵ</italic>
            </bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.01</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.01</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.01</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.01</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>Sampling method parameters</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">T0 = 1.75 and <italic toggle="yes">α</italic> = 0.005</td>
          <td align="left" valign="top" rowspan="1" colspan="1">T0 = 0.88 and <italic toggle="yes">α</italic> = 1.8</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Probability p = 0.1</td>
          <td align="left" valign="top" rowspan="1" colspan="1">T0 = 0.88 and <italic toggle="yes">α</italic> = 1.8</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>No. of steps (specified or inferred from known complexes)</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">20</td>
          <td align="left" valign="top" rowspan="1" colspan="1">4</td>
          <td align="left" valign="top" rowspan="1" colspan="1">9</td>
          <td align="left" valign="top" rowspan="1" colspan="1">10</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>Neighbors considered for growth</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">All neighbors</td>
          <td align="left" valign="top" rowspan="1" colspan="1">All neighbors</td>
          <td align="left" valign="top" rowspan="1" colspan="1">All neighbors</td>
          <td align="left" valign="top" rowspan="1" colspan="1">All neighbors</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">
            <bold>Merging method and parameter</bold>
          </td>
          <td align="left" valign="top" rowspan="1" colspan="1">Qi overlap measure = 0.375</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Qi overlap measure = 0.1</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Qi overlap measure = 0.3</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Qi overlap measure = 0.9</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T2">
    <label>Table 2.</label>
    <caption>
      <title>Evaluating learned complexes on hu.MAP w.r.t ‘refined CORUM’ complexes.</title>
      <p id="P77">Refined CORUM comprises 188 complexes after cleaning original CORUM complexes.</p>
    </caption>
    <table frame="box" rules="all">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th rowspan="2" align="center" valign="bottom" colspan="1">Method</th>
          <th colspan="3" align="center" valign="bottom" rowspan="1">FMM</th>
          <th rowspan="2" align="center" valign="bottom" colspan="1">CMF F1 score</th>
          <th rowspan="2" align="center" valign="bottom" colspan="1">Unbiased Sn-PPV accuracy</th>
          <th rowspan="2" align="center" valign="bottom" colspan="1">Qi et al F1 Score (t=0.5)</th>
          <th rowspan="2" align="center" valign="bottom" colspan="1">F-Grand k-Clique</th>
          <th rowspan="2" align="center" valign="bottom" colspan="1">F-weighted k-Clique</th>
        </tr>
        <tr>
          <th align="center" valign="bottom" rowspan="1" colspan="1">Precision</th>
          <th align="center" valign="bottom" rowspan="1" colspan="1">Recall</th>
          <th align="center" valign="bottom" rowspan="1" colspan="1">F1 score</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="bottom" rowspan="1" colspan="1">Super.Complex</td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">
            <bold>0.767</bold>
          </td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">0.534</td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">
            <bold>0.63</bold>
          </td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">0.783</td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">0.888</td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">0.739</td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">
            <bold>0.785</bold>
          </td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">
            <bold>0.972</bold>
          </td>
        </tr>
        <tr>
          <td align="center" valign="bottom" rowspan="1" colspan="1">hu.MAP (ClusterONE + MCL)</td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">0.471</td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">
            <bold>0.686</bold>
          </td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">0.559</td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">
            <bold>0.797</bold>
          </td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">
            <bold>0.911</bold>
          </td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">
            <bold>0.764</bold>
          </td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">0.77</td>
          <td align="center" valign="bottom" rowspan="1" colspan="1">0.967</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T3">
    <label>Table 3.</label>
    <caption>
      <title>Comparing our method with 6 supervised and 4 unsupervised methods on a yeast PPI network.</title>
      <p id="P78">Precision, recall, and F-measures are from Qi et al. Parameters for each of the Super.Complex experiments are given in <xref rid="T1" ref-type="table">Table 1</xref>.</p>
    </caption>
    <table frame="box" rules="all">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1"/>
          <th align="left" valign="middle" rowspan="1" colspan="1">Train</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Test</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Precision</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Recall</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">F-measure</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>Super.Complex</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>0.841</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.629</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>0.72</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">ClusterSS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.526</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.807</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.636</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">ClusterEPs</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.606</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.664</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.633</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">RM</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.489</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.525</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.506</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SCI-BN</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.219</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.537</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.312</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SCI-SVM</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.176</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.379</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.240</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">ClusterONE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.428</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.435</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.431</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">COACH</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.364</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.495</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.419</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">CMC</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.46</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.38</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.416</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">MCODE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.1</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.16</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>Super.Complex</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>0.718</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.581</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>0.642</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">ClusterSS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.477</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.864</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.614</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">ClusterEPs</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.424</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.782</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.548</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">RM</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.424</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.433</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.429</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SCI-BN</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.312</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.489</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.381</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SCI-SVM</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.247</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.377</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.298</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">ClusterONE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.480</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.46</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.47</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">COACH</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.387</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.533</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.449</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">CMC</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.447</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.353</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.395</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">MCODE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1"/>
          <td align="left" valign="middle" rowspan="1" colspan="1">TAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.422</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.127</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.195</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>Super.Complex</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>0.552</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>0.733</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>0.63</bold>
          </td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">NN</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIPS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.333</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.491</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.397</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
