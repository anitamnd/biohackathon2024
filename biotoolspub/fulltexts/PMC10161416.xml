<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Genome Biol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Genome Biol</journal-id>
    <journal-title-group>
      <journal-title>Genome Biology</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1474-7596</issn>
    <issn pub-type="epub">1474-760X</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10161416</article-id>
    <article-id pub-id-type="pmid">37143118</article-id>
    <article-id pub-id-type="publisher-id">2941</article-id>
    <article-id pub-id-type="doi">10.1186/s13059-023-02941-w</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Short Report</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>EvoAug: improving generalization and interpretability of genomic deep neural networks with evolution-inspired data augmentations</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Lee</surname>
          <given-names>Nicholas Keone</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tang</surname>
          <given-names>Ziqi</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Toneyan</surname>
          <given-names>Shushan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8722-0038</contrib-id>
        <name>
          <surname>Koo</surname>
          <given-names>Peter K.</given-names>
        </name>
        <address>
          <email>koo@cshl.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.225279.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 0387 3667</institution-id><institution>Simons Center for Quantitative Biology, Cold Spring Harbor Laboratory, </institution></institution-wrap>1 Bungtown Road, Cold Spring Harbor, NY USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>5</day>
      <month>5</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>5</day>
      <month>5</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>24</volume>
    <elocation-id>105</elocation-id>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>4</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Deep neural networks (DNNs) hold promise for functional genomics prediction, but their generalization capability may be limited by the amount of available data. To address this, we propose EvoAug, a suite of evolution-inspired augmentations that enhance the training of genomic DNNs by increasing genetic variation. Random transformation of DNA sequences can potentially alter their function in unknown ways, so we employ a fine-tuning procedure using the original non-transformed data to preserve functional integrity. Our results demonstrate that EvoAug substantially improves the generalization and interpretability of established DNNs across prominent regulatory genomics prediction tasks, offering a robust solution for genomic DNNs.</p>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1186/s13059-023-02941-w.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Deep learning</kwd>
      <kwd>Regulatory genomics</kwd>
      <kwd>Data augmentations</kwd>
      <kwd>Model interpretability</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000051</institution-id>
            <institution>National Human Genome Research Institute</institution>
          </institution-wrap>
        </funding-source>
        <award-id>R01HG012131</award-id>
        <principal-award-recipient>
          <name>
            <surname>Lee</surname>
            <given-names>Nicholas Keone</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par2">Uncovering <italic>cis</italic>-regulatory elements and their coordinated interactions is a major goal of regulatory genomics. Deep neural networks (DNNs) offer a promising avenue to learn these genomic features de novo through being trained to take DNA sequences as input and predict their regulatory functions as output [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. Following training, these DNNs have been employed to score the functional effect of disease-associated variants [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>]. Moreover, post hoc model interpretability methods have revealed that DNNs base their decisions on learning sequence motifs of transcription factor (TF) binding sites and dependencies with other TFs and sequence context [<xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR10">10</xref>].</p>
    <p id="Par3">For DNNs, generalization typically improves with more training data. However, the amount of data generated in a high-throughput functional genomics experiment is fundamentally limited by the underlying biology. For example, the extent to which certain TFs bind to DNA is constrained by the availability of high-affinity binding sites in accessible chromatin.</p>
    <p id="Par4">To expand a finite dataset, data augmentations can provide additional variations on existing training data [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>]. Data augmentations act as a form of regularization, guiding the learned function to be invariant to symmetries created by the data transformations [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>]. This approach can help prevent a DNN from overfitting to spurious features and improve generalization [<xref ref-type="bibr" rid="CR15">15</xref>]. The main challenge with data augmentations in genomics is quantifying how the regulatory function changes for a given transformation. With image data, basic affine transformations can translate, magnify, or rotate an image without changing its label. However, in genomics, the available neutral augmentations are reverse-complement transformation [<xref ref-type="bibr" rid="CR16">16</xref>] and small random translations of the input sequence [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. With the finite size of experimental data and a paucity of augmentation methods, strategies to promote generalization for genomic DNNs are limited.</p>
    <p id="Par5">Here, we introduce EvoAug, an open-source PyTorch package that provides a suite of evolution-inspired data augmentations. We show that training DNNs with EvoAug leads to better generalization performance and improves efficacy with standard post hoc explanation methods, including filter interpretability and attribution analysis, across prominent regulatory genomics prediction tasks for well-established DNNs.</p>
  </sec>
  <sec id="Sec2">
    <title>Results and discussion</title>
    <sec id="Sec3">
      <title>Evolution-inspired data augmentations for sequence-based genomic DNNs</title>
      <p id="Par6">To enhance the effectiveness of sequence-based models, data augmentations should aim to increase genetic diversity while maintaining the same biological functionality. Evolution provides a natural process to generate genetic variability, including random mutations, deletions, insertions, inversions, and translocations, among others [<xref ref-type="bibr" rid="CR19">19</xref>]. However, these genetic changes often have functional consequences that expand phenotypic diversity and aid in natural selection. While the addition of homologous sequences to a dataset could achieve the goal of increasing sequence diversity while preserving biological function, identifying regulatory regions with similar functions throughout the genomes across species is difficult. Alternatively, synthetic perturbations that do not alter the function can be applied, but it is crucial to have prior knowledge to ensure that features such as motifs and their dependencies are not affected. Therefore, formulating new data augmentation strategies for genomics remains a significant challenge.</p>
      <p id="Par7">In this study, we present a suite of evolution-based data augmentations and a two-stage training curriculum to preserve functional integrity (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a). In the first stage, a DNN is trained on sequences with EvoAug augmentations applied stochastically online during training, using the same training labels as the wild-type sequence. The goal is to enhance the model’s ability to learn robust representations of features, such as motifs, by exposing it to expanded (albeit synthetically generated) genetic variation. While each augmentation has the potential to disrupt core motifs in any given perturbation, we expect the overall effect to preserve motifs on average. However, the specific data augmentations employed may introduce a bias in how these motif grammars are structured. Thus, in the second stage, the DNN is fine-tuned on the original, unperturbed data to refine these features and guide the function towards the observed biology, thereby removing any bias introduced by the data augmentations (see Methods).<fig id="Fig1"><label>Fig. 1</label><caption><p>EvoAug improves generalization and interpretability of Basset models. <bold>a</bold> Schematic of evolution-inspired data augmentations (left) and the two-stage training curriculum (right). <bold>b</bold> Generalization performance (area under the precision-recall curve) for Basset models pretrained with individual and combinations of augmentations, i.e., Noise+Ins+RC (Gaussian noise, insertion, reverse-complement) and all augmentations (Gaussian noise, reverse-complement, mutation, translocation, deletion, insertion), and fine-tuned on Basset dataset. Standard represents no augmentations during training. <bold>c</bold> Comparison of the average hit rate of first-layer filters to known motifs in the JASPAR database (top) and the average <italic>q</italic>-value of the filters with matches (bottom). <bold>d</bold> Comparison of the average Pearson correlation between model predictions and experimental data from CAGI5 Challenge. <bold>b</bold>–<bold>d</bold> Each box-plot represents 5 trials with random initializations</p></caption><graphic xlink:href="13059_2023_2941_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par8">EvoAug data augmentations introduce a modeling bias to learn invariances of the (un)natural symmetries generated by the augmentations. For instance, random insertions and deletions assume that the distance between motifs is not critical, whereas random inversions and translocations promote invariances to motif strand orientation and the order of motifs, respectively. Nevertheless, the bias created by the augmentations can lead to poor generalization when the introduced bias does not accurately reflect the underlying biology. Therefore, the fine-tuning stage is critical as it provides an avenue to unlearn any biases not supported by the observed data.</p>
    </sec>
    <sec id="Sec4">
      <title>EvoAug improves generalization and interpretability of genomic DNNs</title>
      <p id="Par9">To demonstrate the utility of EvoAug, we analyzed several established DNNs across three prominent types of regulatory genomic prediction tasks that span a range of complexity.</p>
      <p id="Par10">First, we applied Evoaug to the Basset model and dataset [<xref ref-type="bibr" rid="CR20">20</xref>], which consists of a multi-task binary classification of chromatin accessibility sites across 161 cell types/tissues. We trained the Basset model with each augmentation applied independently and in various combinations. We conducted a hyperparameter sweep to determine the optimal settings for each augmentation (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figs. S1-S5). From hyperparameter sweeps, we observed that the inversion augmentation improved performance up to the sequence length, which is essentially a reverse-complement transformation (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figs. S1, S3, and S4). Hence, inversions were excluded to reduce redundancy.</p>
      <p id="Par11">Remarkably, EvoAug-trained DNNs outperformed standard training with no augmentations (Fig. <xref rid="Fig1" ref-type="fig">1</xref>b). The best results were achieved when multiple augmentations were used together. Additionally, we found that fine-tuning on the original data further improved performance, even when augmentation hyperparameters were poorly specified (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S1). Notably, specific EvoAug augmentations, such as random mutations and combinations of data augmentations, had a profound impact on improving the motif representations learned by the first-layer convolutional filters (Fig. <xref rid="Fig1" ref-type="fig">1</xref>c). The convolutional filters capture a wider repertoire of motifs and their representations better reflect known motifs, both quantitatively and qualitatively, when compared with convolutional filters of models trained without augmentations. This suggests that EvoAug augmentations can help DNNs learn more accurate and informative representations of the sequence motifs.</p>
      <p id="Par12">A major downstream application of genomic DNNs is to score the functional consequences of non-coding mutations. By evaluating the zero-shot prediction capabilities of each DNN on saturation mutagenesis data of 15 <italic>cis</italic>-regulatory elements from the CAGI5 Challenge [<xref ref-type="bibr" rid="CR21">21</xref>], we found that models trained with EvoAug outperformed their standard training counterpart (Fig. <xref rid="Fig1" ref-type="fig">1</xref>d). Notably, Basset’s performance was comparable to other DNNs based on binary predictions [<xref ref-type="bibr" rid="CR17">17</xref>]; however, its overall performance was lower than more sophisticated DNNs and top competitors in the CAGI5 challenge [<xref ref-type="bibr" rid="CR2">2</xref>]. Interestingly, we observed that DNNs pretrained with Gaussian noise or random mutagenesis augmentations did not perform well. These augmentations impose flatness locally in sequence-function space, effectively reducing the effect size of nucleotide variants. However, fine-tuning these models improved their variant effect predictions beyond what was achieved with standard training, thus demonstrating the effectiveness of the two-stage training curriculum.</p>
      <p id="Par13">To further demonstrate the benefits of EvoAug, we trained DeepSTARR models as a multi-task quantitative regression to predict enhancer activity from self-transcribing active regulatory region sequencing (STARR-seq) data [<xref ref-type="bibr" rid="CR9">9</xref>], where each task represents a different promoter from a developmental or housekeeping gene in <italic>Drosophila</italic> S2 cells. Most EvoAug augmentations resulted in improved performance, except for reverse-complement and random mutations (Fig. <xref rid="Fig2" ref-type="fig">2</xref>a and Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figs. S3-S5). As before, we observed additional performance gains when augmentations were used in combination. Furthermore, the attribution maps generated by EvoAug-trained models were more interpretable, with identifiable motifs and less spurious noise (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S6).</p>
      <p id="Par14">In addition, we found that the EvoAug-trained DNNs consistently outperformed DNNs with standard training on various single-task binary classification tasks for TF binding across multiple chromatin immunoprecipitation sequencing (ChIP-seq) datasets (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b). Interestingly, we did not observe any significant improvement in performance after fine-tuning, suggesting that the implicit prior imposed by EvoAug augmentations was appropriate for these tasks; the underlying regulatory grammars for these TFs are not complex.<fig id="Fig2"><label>Fig. 2</label><caption><p>Generalization of EvoAug on additional models and datasets. <bold>a</bold> Box-plot of regression performance for DeepSTARR models pretrained with individual or combination of augmentations (i.e., insertion + translocation + deletion; all augmentations) and fine-tuned on original STARR-seq data for two promoters: developmental (top) and housekeeping (bottom). Standard represents no augmentations during training. <bold>b</bold> Box-plot of classification performance (area under the receiver-operating-characteristic curve) for DNNs trained on ChIP-seq datas. <bold>c</bold> Average classification performance for ChIP-seq experiments downsampled to different dataset sizes. Shaded region represents the standard deviation of the mean. <bold>a</bold>, <bold>b</bold> Each box-plot represents 5 trials with random initializations</p></caption><graphic xlink:href="13059_2023_2941_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par15">To further investigate the impact of EvoAug on small datasets, we retrained each DNN on down-sampled versions of two abundant ChIP-seq datasets. We found that EvoAug-trained DNNs exhibit a greater improvement in performance for smaller datasets compared to standard training (Fig. <xref rid="Fig2" ref-type="fig">2</xref>c). This result suggests that EvoAug can be particularly useful in scenarios where the available training data is limited.</p>
      <p id="Par16">Training with EvoAug adds a computational cost, depending on the augmentations chosen and their settings (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Tables S2 and S3). Nevertheless, EvoAug stabilized training (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S7), leading to smoother convergence and improved generalization overall.</p>
    </sec>
  </sec>
  <sec id="Sec5">
    <title>Conclusion</title>
    <p id="Par17">EvoAug greatly expands the set of available data augmentations for genomic DNNs. Our study demonstrated that EvoAug’s two-stage training curriculum is effective in improving generalization performance. Moreover, EvoAug-trained models learned better representations of consensus motifs, as evidenced by filter visualization and attribution analysis.</p>
    <p id="Par18">Our findings support previous arguments for using evolution as a natural source of data augmentation [<xref ref-type="bibr" rid="CR22">22</xref>]. Interestingly, the impact of synthetic evolutionary perturbations was not excessively disruptive, and performance even improved before fine-tuning in most cases. This functional robustness appears to be a characteristic of the non-coding genome [<xref ref-type="bibr" rid="CR23">23</xref>].</p>
    <p id="Par19">Data augmentations are a commonly used technique to balance bias and variance in machine learning models. However, their effectiveness is expected to decrease as the dataset size increases. Nevertheless, EvoAug still improved performance on the already large Basset dataset. Other methods that can enhance generalization include multitask learning [<xref ref-type="bibr" rid="CR24">24</xref>], contrastive learning [<xref ref-type="bibr" rid="CR25">25</xref>, <xref ref-type="bibr" rid="CR26">26</xref>], and language modeling [<xref ref-type="bibr" rid="CR27">27</xref>]. Even though Basset and DeepSTARR are already trained in a multitask framework, EvoAug improved their performance. Multitasking can introduce class imbalance, but EvoAug provides additional examples with pseudo-positive labels, which can mitigate this issue. EvoAug also provides different views of the data, which can be useful for contrastive learning. Importantly, EvoAug is a lightweight and effective strategy that only requires the original data.</p>
    <p id="Par20">The optimal combination of augmentations and their hyperparameter choices depends on the model and dataset. While we performed hyperparameter grid searches in this study, more advanced search strategies such as population-based training [<xref ref-type="bibr" rid="CR28">28</xref>] using Ray Tune [<xref ref-type="bibr" rid="CR29">29</xref>] could improve efficiency. In the future, we plan to investigate EvoAug’s potential in cross-dataset generalization and variant effect predictions, including expression quantitative trait loci.</p>
    <p id="Par21">EvoAug is a PyTorch package that is open-source [<xref ref-type="bibr" rid="CR30">30</xref>], easy to use, extensible, and accessible via pip (<ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/evoaug">https://pypi.org/project/evoaug</ext-link>) and GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/p-koo/evoaug">https://github.com/p-koo/evoaug</ext-link>), with full documentation provided on ReadtheDocs.org (<ext-link ext-link-type="uri" xlink:href="https://evoaug.readthedocs.io">https://evoaug.readthedocs.io</ext-link>). In time, we plan to extend EvoAug functionality to TensorFlow [<xref ref-type="bibr" rid="CR30">30</xref>] and JAX [<xref ref-type="bibr" rid="CR31">31</xref>]. We anticipate that EvoAug will have broad utility in improving the efficacy of sequence-based DNNs for regulatory genomics.</p>
  </sec>
  <sec id="Sec6">
    <title>Methods</title>
    <sec id="Sec7">
      <title>Models and datasets</title>
      <sec id="Sec8">
        <title>Basset</title>
        <p id="Par22">The Basset dataset [<xref ref-type="bibr" rid="CR20">20</xref>] consists of a multi-task binary classification of chromatin accessibility sites across 161 cell types/tissues. The inputs are genomic sequences of length 600 nt and the output are binary labels (representing accessible or not accessible) for 161 cell types measured experimentally using DNase I hypersensitive sites sequencing (DNase-seq). We filtered sequences that contained at least one N character and the data splits (training; validation; test) reduced from (1,879,982; 70,000; 71,886) to (437,478; 16,410; 16,703). This “cleaned” dataset [<xref ref-type="bibr" rid="CR32">32</xref>] was analyzed using a Basset-inspired model, which is given according to:</p>
        <p id="Par23"><list list-type="bullet"><list-item><p id="Par24">Input <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x \in \{0,1\}^{600 \times 4}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mn>600</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq1.gif"/></alternatives></inline-formula> (one-hot encoding of 600 nt sequence)</p></list-item><list-item><p id="Par25">1D convolution (300 filters, size 19, stride 1)</p></list-item><list-item><p id="Par26">BatchNorm + ReLU</p></list-item><list-item><p id="Par27">Max-pooling (size 3, stride 3)</p></list-item><list-item><p id="Par28">1D convolution (200 filters, size 11, stride 1)</p></list-item><list-item><p id="Par29">BatchNorm + ReLU</p></list-item><list-item><p id="Par30">Max-pooling (size 4, stride 4)</p></list-item><list-item><p id="Par31">1D convolution (200 filters, size 7, stride 1)</p></list-item><list-item><p id="Par32">BatchNorm + ReLU</p></list-item><list-item><p id="Par33">Max-pooling (size 2, stride 2)</p></list-item><list-item><p id="Par34">Fully-connected (1000 units)</p></list-item><list-item><p id="Par35">BatchNorm + ReLU</p></list-item><list-item><p id="Par36">Dropout (0.3)</p></list-item><list-item><p id="Par37">Fully-connected (1000 units)</p></list-item><list-item><p id="Par38">BatchNorm + ReLU</p></list-item><list-item><p id="Par39">Dropout (0.3)</p></list-item><list-item><p id="Par40">Fully-connected output (161 units, sigmoid)</p></list-item></list>BatchNorm represents batch normalization [<xref ref-type="bibr" rid="CR33">33</xref>], and dropout [<xref ref-type="bibr" rid="CR34">34</xref>] rates set the probability that neurons in a given layer are temporarily removed during each mini-batch of training.</p>
      </sec>
      <sec id="Sec9">
        <title>DeepSTARR</title>
        <p id="Par41">The DeepSTARR dataset [<xref ref-type="bibr" rid="CR9">9</xref>] consists of a multi-task regression of enhancer activity for two promoters, well-known developmental and housekeeping transcriptional programs in <italic>D. melanogaster</italic> S2 cells. The inputs are genomic sequences of length 249 nt and the output is 2 scalar values representing the activity of developmental enhancers and housekeeping enhancers measured experimentally using STARR-seq. Sequences with N characters were also removed, but this minimally affected the size of the dataset (i.e., reduced it by approximately 0.005%). This dataset [<xref ref-type="bibr" rid="CR32">32</xref>] was analyzed using the original DeepSTARR model, given according to:</p>
        <p id="Par42">
          <list list-type="bullet">
            <list-item>
              <p id="Par43">Input <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x \in \{0,1\}^{249 \times 4}$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mn>249</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq2.gif"/></alternatives></inline-formula></p>
            </list-item>
            <list-item>
              <p id="Par44">1D convolution (256 filters, size 7, stride 1)</p>
            </list-item>
            <list-item>
              <p id="Par45">BatchNorm + ReLU</p>
            </list-item>
            <list-item>
              <p id="Par46">Max-pooling (size 2, stride 2)</p>
            </list-item>
            <list-item>
              <p id="Par47">1D convolution (60 filters, size 3, stride 1)</p>
            </list-item>
            <list-item>
              <p id="Par48">BatchNorm + ReLU</p>
            </list-item>
            <list-item>
              <p id="Par49">Max-pooling (size 2, stride 2)</p>
            </list-item>
            <list-item>
              <p id="Par50">1D convolution (60 filters, size 5, stride 1)</p>
            </list-item>
            <list-item>
              <p id="Par51">BatchNorm + ReLU</p>
            </list-item>
            <list-item>
              <p id="Par52">Max-pooling (size 2, stride 2)</p>
            </list-item>
            <list-item>
              <p id="Par53">1D convolution (120 filters, size 3, stride 1)</p>
            </list-item>
            <list-item>
              <p id="Par54">BatchNorm + ReLU</p>
            </list-item>
            <list-item>
              <p id="Par55">Max-pooling (size 2, stride 2)</p>
            </list-item>
            <list-item>
              <p id="Par56">Fully-connected (256 units)</p>
            </list-item>
            <list-item>
              <p id="Par57">BatchNorm + ReLU</p>
            </list-item>
            <list-item>
              <p id="Par58">Dropout (0.4)</p>
            </list-item>
            <list-item>
              <p id="Par59">Fully-connected (256 units)</p>
            </list-item>
            <list-item>
              <p id="Par60">BatchNorm + ReLU</p>
            </list-item>
            <list-item>
              <p id="Par61">Dropout (0.4)</p>
            </list-item>
            <list-item>
              <p id="Par62">Fully-connected output (2 units, linear)</p>
            </list-item>
          </list>
        </p>
      </sec>
      <sec id="Sec10">
        <title>ChIP-seq</title>
        <p id="Par63">Transcription factor (TF) chromatin immunoprecipitation sequencing (ChIP-seq) data was processed and framed as a binary classification task. The inputs are genomic sequences of length 200 nt and the output is a single binary label representing TF binding activity, with positive-label sequences indicating the presence of a ChIP-seq peak and negative-label sequences indicating a peak for a DNase I hypersensitive site from the same cell type but one that does not overlap with any ChIP-seq peaks. Nine representative TF ChIP-seq experiments in a GM12878 cell line and a DNase-seq experiment for the same cell line were downloaded from ENCODE [<xref ref-type="bibr" rid="CR35">35</xref>]; for details, see Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S1. Negative sequences (i.e., DNase-seq peaks that do not overlap with any positive peaks) were randomly down-sampled to match the number of positive sequences, keeping the classes balanced. The dataset was split randomly into training, validation, and test set according to the fractions 0.7, 0.1, and 0.2, respectively [<xref ref-type="bibr" rid="CR32">32</xref>].</p>
        <p id="Par64">A custom convolutional neural network was employed to analyze these datasets, given according to:<list list-type="bullet"><list-item><p id="Par65">Input <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x \in \{0,1\}^{200 \times 4}$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mn>200</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq3.gif"/></alternatives></inline-formula></p></list-item><list-item><p id="Par66">1D convolution (64 filters, size 7, stride 1)</p></list-item><list-item><p id="Par67">BatchNorm + ReLU</p></list-item><list-item><p id="Par68">Dropout (0.2)</p></list-item><list-item><p id="Par69">Max-pooling (size 4, stride 4)</p></list-item><list-item><p id="Par70">1D convolution (96 filters, size 5, stride 1)</p></list-item><list-item><p id="Par71">BatchNorm + ReLU</p></list-item><list-item><p id="Par72">Dropout (0.2)</p></list-item><list-item><p id="Par73">Max-pooling (size 4, stride 4)</p></list-item><list-item><p id="Par74">1D convolution (128 filters, size 5, stride 1)</p></list-item><list-item><p id="Par75">BatchNorm + ReLU</p></list-item><list-item><p id="Par76">Dropout (0.2)</p></list-item><list-item><p id="Par77">Max-pooling (size 2, stride 2)</p></list-item><list-item><p id="Par78">Fully-connected layer (256 units)</p></list-item><list-item><p id="Par79">BatchNorm + ReLU</p></list-item><list-item><p id="Par80">Dropout (0.5)</p></list-item><list-item><p id="Par81">Fully-connected output layer (1 unit, sigmoid)</p></list-item></list></p>
      </sec>
    </sec>
    <sec id="Sec11">
      <title>Evolution-inspired data augmentations</title>
      <p id="Par82">EvoAug is comprised of a set of data augmentations given by the following:<list list-type="bullet"><list-item><p id="Par83">Mutation: a transformation where single nucleotide mutations are randomly applied to a given wild-type sequence. This is implemented as follows: (1) given the hyperparameter of the fraction of nucleotides in each sequence to mutate (mutate_frac), the number of mutations for a given sequence length is calculated; (2) a position along the sequence is randomly sampled (with replacement) for each number of mutations; and (3) the selected positions are mutagenized to a random nucleotide. Since our implementation does not guarantee that a nucleotide selected will be mutated to a different nucleotide than it originally was, we take approximate account for silent mutations by dividing the user-defined mutate_frac by 0.75 so that on average the fraction of nucleotides in each sequence mutated to a different nucleotide is equal to mutate_frac.</p></list-item><list-item><p id="Par84">Translocation: a transformation that randomly selects a break point in the sequence (thereby creating two segments) and then swaps the order of the two sequence segments. An equivalent statement of this transformation is a “roll”—shifting the sequence forward along its length a randomly specified distance and then reintroducing the part of the sequence shifted beyond the last position back at the first position. This is implemented as follows: (1) given the hyperparameters of the minimum distance (shift_min, default 0) and maximum distance (shift_max) of the shift, the integer-valued shift length is chosen randomly from the interval <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\left[ -\texttt {shift\_max}, -\texttt {shift\_min}\right] \cup \left[ \texttt {shift\_min}, \texttt {shift\_max}\right] }$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mfenced close="]" open="["><mml:mo>-</mml:mo><mml:mi mathvariant="monospace">shift</mml:mi><mml:mi>_</mml:mi><mml:mi mathvariant="monospace">max</mml:mi><mml:mo>,</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant="monospace">shift</mml:mi><mml:mi>_</mml:mi><mml:mi mathvariant="monospace">min</mml:mi></mml:mfenced><mml:mo>∪</mml:mo><mml:mfenced close="]" open="["><mml:mi mathvariant="monospace">shift</mml:mi><mml:mi>_</mml:mi><mml:mi mathvariant="monospace">min</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="monospace">shift</mml:mi><mml:mi>_</mml:mi><mml:mi mathvariant="monospace">max</mml:mi></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq4.gif"/></alternatives></inline-formula>, where a negative value simply denotes a backward shift rather than a forward shift, and (2) the shift is applied to the sequence with a roll() function in PyTorch.</p></list-item><list-item><p id="Par85">Insertion: a transformation where a random DNA sequence (of random length) is inserted randomly into a wild-type sequence. This is implemented as follows: (1) given the hyperparameters of the minimum length (insert_min, default 0) and maximum length (insert_max) of the insertion, the integer-valued insertion length is chosen randomly from the interval between insert_min and insert_max (inclusive), and (2) the insertion is inserted at a random position within the original sequence. Importantly, to maintain a constant input sequence length to the model (i.e., original length plus insert_max), the remaining amount of length between the insertion length and insert_max is split evenly and placed on the 5’ and 3’ flanks of the sequence, with the remainder from odd lengths going to the 3′ end. Whenever an insertion augmentation is employed in combination with other augmentations, all sequences without an insertion are padded with a stretch of random DNA of length insert_max at the 3′ end to ensure that the model processes sequences with a constant length for both training and inference time.</p></list-item><list-item><p id="Par86">Deletion: a transformation where a random, contiguous segment of a wild-type sequence is removed, and the shortened sequence is then padded with random DNA sequence to maintain the same length as wild-type. This is implemented as follows: (1) given the hyperparameters of the minimum length (delete_min, default 0) and maximum length (delete_max) of the deletion, the integer-valued deletion length is chosen randomly from the interval between delete_min and delete_max (inclusive); (2) the starting position of the deletion is chosen randomly from the valid positions in the sequence that can encapsulate the deletion; (3) the deletion is performed on the designated stretch of the sequence; (4) the remaining portions of the sequence are concatenated together; and (5) random DNA is used to pad the 5′ and 3′ flanks to maintain a constant input sequence length, similar to the procedure for insertions.</p></list-item><list-item><p id="Par87">Inversion: a transformation where a random subsequence is replaced by its reverse-complement. This is implemented as follows: (1) given the hyperparameters of the minimum length (invert_min, default 0) and maximum length (invert_max) of the inversion, the integer-valued inversion length is chosen randomly from the interval between invert_min and invert_max (inclusive); (2) the starting position of the inversion is chosen randomly from the valid position indices in the sequence; and (3) the inversion (i.e., a reverse-complement transformation) is performed on the designated subsequence while the remaining portions of the sequence remain untouched.</p></list-item><list-item><p id="Par88">Reverse-complement: a transformation where a full sequence is replaced with some probability rc_prob by its reverse-complement.</p></list-item><list-item><p id="Par89">Gaussian noise: a transformation where Gaussian noise (with distribution parameters <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\texttt {noise\_mean} = 0$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi mathvariant="monospace">noise</mml:mi><mml:mi>_</mml:mi><mml:mi mathvariant="monospace">mean</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq5.gif"/></alternatives></inline-formula> and noise_std) is added to the input sequence; a random value drawn independently and identically from the specified distribution is added to each element of the one-hot input matrix.</p></list-item></list></p>
      <sec id="Sec12">
        <title>Pretraining with data augmentations</title>
        <p id="Par90">Training with augmentations requires two main hyperparameters: first, a set of augmentations to sample from; and second, the maximum number of augmentations to be applied to a sequence. For each mini-batch during training, each sequence is randomly augmented independently. The number of augmentations to be applied to a given sequence has two possible settings in EvoAug: (1) hard, always equal to the maximum number of augmentations, or (2) soft, randomly select the number of augmentations for a sequence from 1 to the maximum number. Our experiments with Basset and DeepSTARR use the former setting, while our experiments with ChIP-seq datasets use the latter setting. Then, the subset of augmentations to be applied to the sequence is sampled randomly without replacement from the user-defined set of augmentations. After a subset of augmentations is chosen, the order in which multiple augmentations are applied to a single sequence is given by the following priority: inversion, deletion, translocation, insertion, reverse-complement, mutation, noise addition. Each augmentation is then applied stochastically for each sequence.</p>
        <p id="Par91">For the Basset and DeepSTARR models, each augmentation has an optimal setting that was determined from a hyperparameter search independently using the validation set (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figs. S1, S3, and S4). For the Basset models, the hyperparameters were set to:<list list-type="bullet"><list-item><p id="Par92">mutation: mutate_frac = 0.15</p></list-item><list-item><p id="Par93">translocation: shift_min = 0, shift_max = 30</p></list-item><list-item><p id="Par94">insertion: insert_min = 0, insert_max = 30</p></list-item><list-item><p id="Par95">deletion: delete_min = 0, delete_max = 30</p></list-item><list-item><p id="Par96">reverse-complement: rc_prob = 0.5</p></list-item><list-item><p id="Par97">noise: noise_mean = 0, noise_std (standard deviation) = 0.3</p></list-item></list>For the DeepSTARR models, the hyperparameters were set to:<list list-type="bullet"><list-item><p id="Par98">mutation: mutate_frac = 0.05</p></list-item><list-item><p id="Par99">translocation: shift_min = 0, shift_max = 20</p></list-item><list-item><p id="Par100">insertion: insert_min = 0, insert_max = 20</p></list-item><list-item><p id="Par101">deletion: delete_min = 0, delete_max = 30</p></list-item><list-item><p id="Par102">reverse-complement: rc_prob = 0</p></list-item><list-item><p id="Par103">noise: noise_mean = 0, noise_std = 0.3</p></list-item></list> When augmentations were used in combinations, the maximum number of augmentations was set to 3 for Basset and 2 for DeepSTARR. The same hyperparameter settings used in DeepSTARR analyses with all augmentations were used for the ChIP-seq analysis. For models trained with combinations of augmentations, the hyperparameters intrinsic to augmentations were set at the values identified above and the maximum number of augmentations per sequence was also determined through a hyperparameter sweep for each dataset (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figs. S2 and S5).</p>
        <p id="Par104">Unless otherwise specified, all models were trained (with or without data augmentations) for 100 epochs using the Adam optimizer [<xref ref-type="bibr" rid="CR36">36</xref>] with an initial learning rate of <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1 \times 10^{-3}$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq6.gif"/></alternatives></inline-formula> and a weight decay (<inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq7.gif"/></alternatives></inline-formula> penalty) term of <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1 \times 10^{-6}$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq8.gif"/></alternatives></inline-formula>; additionally, we employed early stopping with a patience of 10 epochs and a learning rate decay that decreased the learning rate by a factor of 0.1 when the validation loss did not improve for 5 epochs. For each model trained, the version of the model with the highest-performing weights during its training, as measured by validation loss, is the version of the model whose performance is reported here.</p>
      </sec>
      <sec id="Sec13">
        <title>Fine-tuning</title>
        <p id="Par105">Models that completed training with data augmentations were subsequently fine-tuned on the original dataset without augmentations. Fine-tuning employs the Adam optimizer with a learning rate of <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1 \times 10^{-4}$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq9.gif"/></alternatives></inline-formula> and a weight decay (<inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq10.gif"/></alternatives></inline-formula> penalty) term of <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1 \times 10^{-6}$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq11.gif"/></alternatives></inline-formula> for 5 epochs. The model that yields the lowest validation loss was used for test time evaluation.</p>
      </sec>
      <sec id="Sec14">
        <title>Evaluation</title>
        <p id="Par106">When evaluating models on validation or test sets, no data augmentations were used on input sequences. For models trained with an insertion augmentation (alone or in combination with other augmentations), each sequence is padded at the 3′ end with a stretch of random DNA of length insert_max.</p>
      </sec>
    </sec>
    <sec id="Sec15">
      <title>Interpretability analysis</title>
      <sec id="Sec16">
        <title>Filter interpretability</title>
        <p id="Par107">We visualized the first-layer filters of various Basset models according to activation-based alignments [<xref ref-type="bibr" rid="CR37">37</xref>] and compared how well they match motifs in the 2022 JASPAR nonredundant vertebrates database [<xref ref-type="bibr" rid="CR38">38</xref>] using Tomtom [<xref ref-type="bibr" rid="CR39">39</xref>], a motif search comparison tool. Matrix profiles MA1929.1 and MA0615.1 were excluded from filter matching to remove poor quality hits; low information content filters tend to have a high hit rate with these two matrix profiles. Hit rate is calculated by measuring how many filters matched to at least one JASPAR motif. Average <italic>q</italic>-value is calculated by taking the average of the smallest <italic>q</italic>-values for each filter among its matches.</p>
      </sec>
      <sec id="Sec17">
        <title>Attribution analysis</title>
        <p id="Par108">SHAP-based [<xref ref-type="bibr" rid="CR40">40</xref>] attribution maps (implemented with GradientShap from the Captum package [<xref ref-type="bibr" rid="CR41">41</xref>]) were used to generate sequence logos (visualized by Logomaker [<xref ref-type="bibr" rid="CR42">42</xref>]) for sequences that exhibited high experimental enhancer activity for the Developmental promoter (i.e., task 0 in the DeepSTARR dataset). One thousand random DNA sequences were synthesized to serve as references for each GradientShap-based attribution map. A gradient correction [<xref ref-type="bibr" rid="CR43">43</xref>] was applied to each attribution map. For comparison, this analysis was repeated for a DeepSTARR model that was trained without any augmentations and a fine-tuned DeepSTARR model that was pretrained with all augmentations (excluding inversions) with two augmentations per sequence.</p>
      </sec>
    </sec>
    <sec id="Sec18">
      <title>CAGI5 challenge analysis</title>
      <p id="Par109">The CAGI5 challenge dataset [<xref ref-type="bibr" rid="CR21">21</xref>] was used to benchmark model performance on variant effect predictions. This dataset contains massively parallel reporter assays (MPRAs) that measure the effect size of single-nucleotide variants through saturation mutagenesis of 15 different regulatory elements ranging from 187 nt to 600 nt in length. We extracted 600 nt sequences from the reference genome centered on each regulatory region of interest and converted it into a one-hot representation. Alternative alleles were then substituted correspondingly to construct the CAGI test sequences.</p>
      <p id="Par110">For a given Basset model, the output predictions of two input sequences, one with a centered reference allele and the other with an alternative allele, are made. The cell type-agnostic approach employed in this study uses the mean across these values to calculate a single scalar value, functional activity across cell types. The effect size is then calculated with the log-ratio of this single value for the alternative allele and reference allele, according to: <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\log$$\end{document}</tex-math><mml:math id="M24"><mml:mo>log</mml:mo></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq12.gif"/></alternatives></inline-formula>(alternative value/reference value).</p>
      <p id="Par111">To evaluate the variant effect prediction performance, Pearson correlation was calculated within each CAGI5 experiment between the experimentally measured and predicted effect sizes. The average of the Pearson correlation across all 15 experiments represents the overall performance of the model.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec19">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="13059_2023_2941_MOESM1_ESM.pdf">
            <caption>
              <p><bold>Additional file 1.</bold> Supplementary Tables S1-S3 and Figures S1-S7.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="13059_2023_2941_MOESM2_ESM.docx">
            <caption>
              <p><bold>Additional file 2.</bold> Review history.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>This work was supported in part by funding from the NIH grant R01HG012131 and the Simons Center for Quantitative Biology at Cold Spring Harbor Laboratory. This work was performed with assistance from the US National Institutes of Health Grant S10OD028632-01.</p>
    <sec id="FPar9">
      <title>Peer review information</title>
      <p id="Par112">Andrew Cosgrove was the primary editor of this article and managed its editorial process and peer review in collaboration with the rest of the editorial team.</p>
    </sec>
    <sec id="FPar10">
      <title>Review history</title>
      <p id="Par113">The review history is available as Additional file <xref rid="MOESM2" ref-type="media">2</xref>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>NKL and PKK conceived of the method, designed the experiments, and wrote the majority of the code base. NKL, ST, and ZT conducted experiments and analyzed the data. PKK oversaw the project. All authors interpreted the results and contributed to the manuscript. The authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>EvoAug Python package is deposited on the Python Package Index (PyPI) repository with documentation hosted on <ext-link ext-link-type="uri" xlink:href="https://evoaug.readthedocs.io">https://evoaug.readthedocs.io</ext-link>. The open-source project repository is available under the MIT license at GitHub [<xref ref-type="bibr" rid="CR30">30</xref>] ( <ext-link ext-link-type="uri" xlink:href="https://github.com/p-koo/evoaug">https://github.com/p-koo/evoaug</ext-link>). The code to reproduce analyses in this paper is available under the MIT license at GitHub, <ext-link ext-link-type="uri" xlink:href="https://github.com/p-koo/evoaug_analysis">https://github.com/p-koo/evoaug_analysis</ext-link>. Processed data, including DeepSTARR [<xref ref-type="bibr" rid="CR9">9</xref>], Basset [<xref ref-type="bibr" rid="CR20">20</xref>] and ChIP-seq analysis, are available at Zenodo [<xref ref-type="bibr" rid="CR32">32</xref>] (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7265991">doi.org/10.5281/zenodo.7265991</ext-link>). Model weights and code [<xref ref-type="bibr" rid="CR44">44</xref>] are also available at Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7767325">doi.org/10.5281/zenodo.7767325</ext-link>).</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar11">
      <title>Ethics approval and consent to participate</title>
      <p id="Par114">Not applicable.</p>
    </notes>
    <notes id="FPar12" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par115">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Troyanskaya</surname>
            <given-names>OG</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A sequence-based global map of regulatory activity for deciphering human genetics</article-title>
        <source>Nat Genet.</source>
        <year>2022</year>
        <volume>54</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1038/s41588-022-01102-2</pub-id>
        <pub-id pub-id-type="pmid">35022602</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Avsec</surname>
            <given-names>Ž</given-names>
          </name>
          <name>
            <surname>Agarwal</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Visentin</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ledsam</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Grabska-Barwinska</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>KR</given-names>
          </name>
          <name>
            <surname>Assael</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Jumper</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kohli</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kelley</surname>
            <given-names>DR</given-names>
          </name>
        </person-group>
        <article-title>Effective gene expression prediction from sequence by integrating long-range interactions</article-title>
        <source>Nat Methods.</source>
        <year>2021</year>
        <volume>18</volume>
        <issue>10</issue>
        <fpage>1196</fpage>
        <lpage>1203</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-021-01252-x</pub-id>
        <?supplied-pmid 34608324?>
        <pub-id pub-id-type="pmid">34608324</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Sequence-based modeling of three-dimensional genome architecture from kilobase to chromosome scale</article-title>
        <source>Nat Genet.</source>
        <year>2022</year>
        <volume>54</volume>
        <issue>5</issue>
        <fpage>725</fpage>
        <lpage>734</lpage>
        <pub-id pub-id-type="doi">10.1038/s41588-022-01065-4</pub-id>
        <?supplied-pmid 35551308?>
        <pub-id pub-id-type="pmid">35551308</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hoffman</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Bendl</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Girdhar</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Schadt</surname>
            <given-names>EE</given-names>
          </name>
          <name>
            <surname>Roussos</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Functional interpretation of genetic variants using deep learning predicts impact on chromatin accessibility and histone modification</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2019</year>
        <volume>47</volume>
        <issue>20</issue>
        <fpage>10597</fpage>
        <lpage>10611</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkz808</pub-id>
        <?supplied-pmid 31544924?>
        <pub-id pub-id-type="pmid">31544924</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dey</surname>
            <given-names>KK</given-names>
          </name>
          <name>
            <surname>Van de Geijn</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Hormozdiari</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Kelley</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Price</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>Evaluating the informativeness of deep learning annotations for human complex diseases</article-title>
        <source>Nat Commun.</source>
        <year>2020</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1038/s41467-020-18515-4</pub-id>
        <pub-id pub-id-type="pmid">31911652</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koo</surname>
            <given-names>PK</given-names>
          </name>
          <name>
            <surname>Ploenzke</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Improving representations of genomic sequence motifs in convolutional networks with exponential activations</article-title>
        <source>Nat Mach Intell.</source>
        <year>2021</year>
        <volume>3</volume>
        <issue>3</issue>
        <fpage>258</fpage>
        <lpage>266</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-020-00291-x</pub-id>
        <?supplied-pmid 34322657?>
        <pub-id pub-id-type="pmid">34322657</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Avsec</surname>
            <given-names>Ž</given-names>
          </name>
          <name>
            <surname>Weilert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Shrikumar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Krueger</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Alexandari</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dalal</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Fropf</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>McAnany</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Gagneur</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kundaje</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Base-resolution models of transcription-factor binding reveal soft motif syntax</article-title>
        <source>Nat Genet.</source>
        <year>2021</year>
        <volume>53</volume>
        <issue>3</issue>
        <fpage>354</fpage>
        <lpage>366</lpage>
        <pub-id pub-id-type="doi">10.1038/s41588-021-00782-6</pub-id>
        <?supplied-pmid 33603233?>
        <pub-id pub-id-type="pmid">33603233</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koo</surname>
            <given-names>PK</given-names>
          </name>
          <name>
            <surname>Majdandzic</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ploenzke</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Anand</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Paul</surname>
            <given-names>SB</given-names>
          </name>
        </person-group>
        <article-title>Global importance analysis: An interpretability method to quantify importance of genomic features in deep neural networks</article-title>
        <source>PLoS Comput Biol.</source>
        <year>2021</year>
        <volume>17</volume>
        <issue>5</issue>
        <fpage>1008925</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008925</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>de Almeida</surname>
            <given-names>BP</given-names>
          </name>
          <name>
            <surname>Reiter</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Pagani</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Stark</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Deepstarr predicts enhancer activity from DNA sequence and enables the de novo design of synthetic enhancers</article-title>
        <source>Nat Genet.</source>
        <year>2022</year>
        <volume>54</volume>
        <issue>5</issue>
        <fpage>613</fpage>
        <lpage>624</lpage>
        <pub-id pub-id-type="doi">10.1038/s41588-022-01048-5</pub-id>
        <?supplied-pmid 35551305?>
        <pub-id pub-id-type="pmid">35551305</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Horton</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Alexandari</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Hayes</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Schaepe</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Marklund</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Aditham</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Shrikumar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Afek</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Greenleaf</surname>
            <given-names>WJ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Short tandem repeats recruit transcription factors to tune eukaryotic gene expression</article-title>
        <source>Biophys J.</source>
        <year>2022</year>
        <volume>121</volume>
        <issue>3</issue>
        <fpage>287</fpage>
        <lpage>288</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bpj.2021.11.1305</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shorten</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Khoshgoftaar</surname>
            <given-names>TM</given-names>
          </name>
        </person-group>
        <article-title>A survey on image data augmentation for deep learning</article-title>
        <source>J Big Data.</source>
        <year>2019</year>
        <volume>6</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>48</lpage>
        <pub-id pub-id-type="doi">10.1186/s40537-019-0197-0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Fort S, Brock A, Pascanu R, De S, Smith SL. Drawing multiple augmentation samples per image during training efficiently decreases test error. 2021. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2105.13343">arXiv:2105.13343</ext-link></mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>An</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Understanding the generalization benefit of model invariance from a data perspective</article-title>
        <source>Adv Neural Inf Process Syst.</source>
        <year>2021</year>
        <volume>34</volume>
        <fpage>4328</fpage>
        <lpage>4341</lpage>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Geiping J, Goldblum M, Somepalli G, Shwartz-Ziv R, Goldstein T, Wilson AG. How much data are augmentations worth? An investigation into scaling laws, invariance, and implicit regularization. 2022. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2210.06441">arXiv:2210.06441</ext-link></mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Puli A, Zhang LH, Oermann EK, Ranganath R. Out-of-distribution generalization in the presence of nuisance-induced spurious correlations. 2021. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2107.00520">arXiv:2107.00520</ext-link></mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Zhou H, Shrikumar A, Kundaje A. Towards a better understanding of reverse-complement equivariance for deep learning models in genomics. In: Machine Learning in Computational Biology, PMLR; 2022. p. 1–33</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Toneyan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Koo</surname>
            <given-names>PK</given-names>
          </name>
        </person-group>
        <article-title>Evaluating deep learning for predicting epigenomic profiles</article-title>
        <source>Nat Mach Intell.</source>
        <year>2022</year>
        <volume>4</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-022-00570-9</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kelley</surname>
            <given-names>DR</given-names>
          </name>
        </person-group>
        <article-title>Cross-species regulatory sequence activity prediction</article-title>
        <source>PLoS Comput Biol</source>
        <year>2020</year>
        <volume>16</volume>
        <issue>7</issue>
        <fpage>1008050</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008050</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Frazer</surname>
            <given-names>KA</given-names>
          </name>
          <name>
            <surname>Murray</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Schork</surname>
            <given-names>NJ</given-names>
          </name>
          <name>
            <surname>Topol</surname>
            <given-names>EJ</given-names>
          </name>
        </person-group>
        <article-title>Human genetic variation and its contribution to complex traits</article-title>
        <source>Nat Rev Genet</source>
        <year>2009</year>
        <volume>10</volume>
        <issue>4</issue>
        <fpage>241</fpage>
        <lpage>251</lpage>
        <pub-id pub-id-type="doi">10.1038/nrg2554</pub-id>
        <?supplied-pmid 19293820?>
        <pub-id pub-id-type="pmid">19293820</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kelley</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Snoek</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rinn</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks</article-title>
        <source>Genome Res</source>
        <year>2016</year>
        <volume>26</volume>
        <issue>7</issue>
        <fpage>990</fpage>
        <lpage>999</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.200535.115</pub-id>
        <?supplied-pmid 27197224?>
        <pub-id pub-id-type="pmid">27197224</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shigaki</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Adato</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Adhikari</surname>
            <given-names>AN</given-names>
          </name>
          <name>
            <surname>Dong</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Hawkins-Hooker</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Inoue</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Juven-Gershon</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kenlay</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Patra</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Penzar</surname>
            <given-names>DD</given-names>
          </name>
          <name>
            <surname>Schubach</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Boyle</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Kreimer</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kulakovskiy</surname>
            <given-names>IV</given-names>
          </name>
          <name>
            <surname>Reid</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Unger</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yosef</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Shendure</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ahituv</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Kircher</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Beer</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Integration of multiple epigenomic marks improves prediction of variant impact in saturation mutagenesis reporter assay</article-title>
        <source>Hum Mutat</source>
        <year>2019</year>
        <volume>40</volume>
        <issue>9</issue>
        <fpage>1280</fpage>
        <lpage>1291</lpage>
        <pub-id pub-id-type="doi">10.1002/humu.23797</pub-id>
        <?supplied-pmid 31106481?>
        <pub-id pub-id-type="pmid">31106481</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Lu, A.X, Lu, A.X, Moses, A. Evolution is all you need: phylogenetic augmentation for contrastive learning. 2020. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2012.13475">arXiv:2012.13475</ext-link></mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kryukov</surname>
            <given-names>GV</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sunyaev</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Small fitness effect of mutations in highly conserved non-coding regions</article-title>
        <source>Hum Mol Genet</source>
        <year>2005</year>
        <volume>14</volume>
        <issue>15</issue>
        <fpage>2221</fpage>
        <lpage>2229</lpage>
        <pub-id pub-id-type="doi">10.1093/hmg/ddi226</pub-id>
        <?supplied-pmid 15994173?>
        <pub-id pub-id-type="pmid">15994173</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Crawshaw, M. Multi-task learning with deep neural networks: a survey. 2020. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2009.09796">arXiv:2009.09796</ext-link></mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Zbontar J, Jing L, Misra I, LeCun Y, Deny S. Barlow twins: Self-supervised learning via redundancy reduction. In: International Conference on Machine Learning, PMLR; 2021. p. 12310–12320</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Hjelm RD, Fedorov A, Lavoie-Marchildon S, Grewal K, Bachman P, Trischler A, Bengio Y. Learning deep representations by mutual information estimation and maximization. 2018. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1808.06670">arXiv:1808.06670</ext-link></mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Devlin J, Chang M-W, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding. 2018. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1810.04805">arXiv:1810.04805</ext-link></mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Jaderberg M, Dalibard V, Osindero S, Czarnecki WM, Donahue J, Razavi A, Vinyals O, Green T, Dunning I, Simonyan K, et al. Population based training of neural networks. 2017. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1711.09846">arXiv:1711.09846</ext-link></mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Liaw R, Liang E, Nishihara R, Moritz P, Gonzalez JE, Stoica I. Tune: a research platform for distributed model selection and training. 2018. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1807.05118">arXiv:1807.05118</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, Corrado GS, Davis A, Dean J, Devin M, Ghemawat S, Goodfellow I, Harp A, Irving G, Isard M, Jia Y, Jozefowicz R, Kaiser L, Kudlur M, Levenberg J, Mané D, Monga R, Moore S, Murray D, Olah C, Schuster M, Shlens J, Steiner B, Sutskever I, Talwar K, Tucker P, Vanhoucke V, Vasudevan V, Viégas F, Vinyals O, Warden P, Wattenberg M, Wicke M, Yu Y, Zheng X. TensorFlow: Large-scale machine learning on heterogeneous systems. 2015. <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/">https://www.tensorflow.org/</ext-link>. Accessed 31 Oct 2022.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Bradbury J, Frostig R, Hawkins P, Johnson MJ, Leary C, Maclaurin D, Necula G, Paszke A, VanderPlas J, Wanderman-Milne S, Zhang Q. JAX: Composable transformations of Python+NumPy programs. <ext-link ext-link-type="uri" xlink:href="https://github.com/google/jax">http://github.com/google/jax</ext-link>. Accessed 31 Oct 2022.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Lee NK, Toneyan S, Tang Z, Koo PK. EvoAug Data [Data set]. Zenodo. 2022. 10.5281/zenodo.7265991.  Accessed 31 Oct 2022. </mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift. International Conference on Machine Learning, PMLR; 2015. p. 448–456</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>
        <source>J Mach Learn Res.</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>1929</fpage>
        <lpage>1958</lpage>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Luo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hitz</surname>
            <given-names>BC</given-names>
          </name>
          <name>
            <surname>Gabdank</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Hilton</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Kagda</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Lam</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Myers</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Sud</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Jou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>New developments on the encyclopedia of DNA elements (encode) data portal</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2020</year>
        <volume>48</volume>
        <issue>D1</issue>
        <fpage>882</fpage>
        <lpage>889</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkz1062</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Kingma D, Ba J. Adam: A method for stochastic optimization. 2014. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">arXiv:1412.6980</ext-link></mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koo</surname>
            <given-names>PK</given-names>
          </name>
          <name>
            <surname>Ploenzke</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Deep learning for inferring transcription factor binding sites</article-title>
        <source>Curr Opin Syst Biol</source>
        <year>2020</year>
        <volume>19</volume>
        <fpage>16</fpage>
        <lpage>23</lpage>
        <pub-id pub-id-type="doi">10.1016/j.coisb.2020.04.001</pub-id>
        <?supplied-pmid 32905524?>
        <pub-id pub-id-type="pmid">32905524</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Castro-Mondragon</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Riudavets-Puig</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Rauluseviciute</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Lemma</surname>
            <given-names>RB</given-names>
          </name>
          <name>
            <surname>Turchi</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Blanc-Mathieu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lucas</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Boddie</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pérez</surname>
            <given-names>NM</given-names>
          </name>
          <name>
            <surname>Fornes</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Leung</surname>
            <given-names>TY</given-names>
          </name>
          <name>
            <surname>Aguirre</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hammal</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Schmelter</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Baranasic</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ballester</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sandelin</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lenhard</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Vandepoele</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Wasserman</surname>
            <given-names>WW</given-names>
          </name>
          <name>
            <surname>Parcy</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Mathelier</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>JASPAR 2022: the 9th release of the open-access database of transcription factor binding profiles</article-title>
        <source>Nucleic Acids Res</source>
        <year>2021</year>
        <volume>50</volume>
        <issue>D1</issue>
        <fpage>165</fpage>
        <lpage>173</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkab1113</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gupta</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Stamatoyannopoulos</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Bailey</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Noble</surname>
            <given-names>WS</given-names>
          </name>
        </person-group>
        <article-title>Quantifying similarity between motifs</article-title>
        <source>Genome Biol</source>
        <year>2007</year>
        <volume>8</volume>
        <issue>2</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1186/gb-2007-8-2-r24</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Lundberg SM, Lee S-I. A unified approach to interpreting model predictions. Adv Neural Inf Process Syst. 2017;30. <ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html">https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Kokhlikyan N, Miglani V, Martin M, Wang E, Alsallakh B, Reynolds J, Melnikov A, Kliushkina N, Araya C, Yan S, Reblitz-Richardson O. Captum: a unified and generic model interpretability library for pytorch. 2020. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2009.07896">arXiv:2009.07896</ext-link></mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tareen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kinney</surname>
            <given-names>JB</given-names>
          </name>
        </person-group>
        <article-title>Logomaker: beautiful sequence logos in python</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <issue>7</issue>
        <fpage>2272</fpage>
        <lpage>2274</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz921</pub-id>
        <?supplied-pmid 31821414?>
        <pub-id pub-id-type="pmid">31821414</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Majdandzic A, Rajesh C, Koo PK. Statistical correction of input gradients for black box models trained with categorical input features. 2022. bioRxiv preprint. <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.04.29.490102v2">biorxiv.org/content/10.1101/2022.04.29.490102v2</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Lee NK, Toneyan S, Tang Z, Koo PK. EvoAug reproducibility code. Github. 2022. <ext-link ext-link-type="uri" xlink:href="https://github.com/p-koo/evoaug_analysis">https://github.com/p-koo/evoaug_analysis</ext-link>. Accessed 31 Oct 2022. </mixed-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Genome Biol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Genome Biol</journal-id>
    <journal-title-group>
      <journal-title>Genome Biology</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1474-7596</issn>
    <issn pub-type="epub">1474-760X</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10161416</article-id>
    <article-id pub-id-type="pmid">37143118</article-id>
    <article-id pub-id-type="publisher-id">2941</article-id>
    <article-id pub-id-type="doi">10.1186/s13059-023-02941-w</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Short Report</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>EvoAug: improving generalization and interpretability of genomic deep neural networks with evolution-inspired data augmentations</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Lee</surname>
          <given-names>Nicholas Keone</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tang</surname>
          <given-names>Ziqi</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Toneyan</surname>
          <given-names>Shushan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8722-0038</contrib-id>
        <name>
          <surname>Koo</surname>
          <given-names>Peter K.</given-names>
        </name>
        <address>
          <email>koo@cshl.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.225279.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 0387 3667</institution-id><institution>Simons Center for Quantitative Biology, Cold Spring Harbor Laboratory, </institution></institution-wrap>1 Bungtown Road, Cold Spring Harbor, NY USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>5</day>
      <month>5</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>5</day>
      <month>5</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>24</volume>
    <elocation-id>105</elocation-id>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>4</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Deep neural networks (DNNs) hold promise for functional genomics prediction, but their generalization capability may be limited by the amount of available data. To address this, we propose EvoAug, a suite of evolution-inspired augmentations that enhance the training of genomic DNNs by increasing genetic variation. Random transformation of DNA sequences can potentially alter their function in unknown ways, so we employ a fine-tuning procedure using the original non-transformed data to preserve functional integrity. Our results demonstrate that EvoAug substantially improves the generalization and interpretability of established DNNs across prominent regulatory genomics prediction tasks, offering a robust solution for genomic DNNs.</p>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1186/s13059-023-02941-w.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Deep learning</kwd>
      <kwd>Regulatory genomics</kwd>
      <kwd>Data augmentations</kwd>
      <kwd>Model interpretability</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000051</institution-id>
            <institution>National Human Genome Research Institute</institution>
          </institution-wrap>
        </funding-source>
        <award-id>R01HG012131</award-id>
        <principal-award-recipient>
          <name>
            <surname>Lee</surname>
            <given-names>Nicholas Keone</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par2">Uncovering <italic>cis</italic>-regulatory elements and their coordinated interactions is a major goal of regulatory genomics. Deep neural networks (DNNs) offer a promising avenue to learn these genomic features de novo through being trained to take DNA sequences as input and predict their regulatory functions as output [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. Following training, these DNNs have been employed to score the functional effect of disease-associated variants [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>]. Moreover, post hoc model interpretability methods have revealed that DNNs base their decisions on learning sequence motifs of transcription factor (TF) binding sites and dependencies with other TFs and sequence context [<xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR10">10</xref>].</p>
    <p id="Par3">For DNNs, generalization typically improves with more training data. However, the amount of data generated in a high-throughput functional genomics experiment is fundamentally limited by the underlying biology. For example, the extent to which certain TFs bind to DNA is constrained by the availability of high-affinity binding sites in accessible chromatin.</p>
    <p id="Par4">To expand a finite dataset, data augmentations can provide additional variations on existing training data [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>]. Data augmentations act as a form of regularization, guiding the learned function to be invariant to symmetries created by the data transformations [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>]. This approach can help prevent a DNN from overfitting to spurious features and improve generalization [<xref ref-type="bibr" rid="CR15">15</xref>]. The main challenge with data augmentations in genomics is quantifying how the regulatory function changes for a given transformation. With image data, basic affine transformations can translate, magnify, or rotate an image without changing its label. However, in genomics, the available neutral augmentations are reverse-complement transformation [<xref ref-type="bibr" rid="CR16">16</xref>] and small random translations of the input sequence [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. With the finite size of experimental data and a paucity of augmentation methods, strategies to promote generalization for genomic DNNs are limited.</p>
    <p id="Par5">Here, we introduce EvoAug, an open-source PyTorch package that provides a suite of evolution-inspired data augmentations. We show that training DNNs with EvoAug leads to better generalization performance and improves efficacy with standard post hoc explanation methods, including filter interpretability and attribution analysis, across prominent regulatory genomics prediction tasks for well-established DNNs.</p>
  </sec>
  <sec id="Sec2">
    <title>Results and discussion</title>
    <sec id="Sec3">
      <title>Evolution-inspired data augmentations for sequence-based genomic DNNs</title>
      <p id="Par6">To enhance the effectiveness of sequence-based models, data augmentations should aim to increase genetic diversity while maintaining the same biological functionality. Evolution provides a natural process to generate genetic variability, including random mutations, deletions, insertions, inversions, and translocations, among others [<xref ref-type="bibr" rid="CR19">19</xref>]. However, these genetic changes often have functional consequences that expand phenotypic diversity and aid in natural selection. While the addition of homologous sequences to a dataset could achieve the goal of increasing sequence diversity while preserving biological function, identifying regulatory regions with similar functions throughout the genomes across species is difficult. Alternatively, synthetic perturbations that do not alter the function can be applied, but it is crucial to have prior knowledge to ensure that features such as motifs and their dependencies are not affected. Therefore, formulating new data augmentation strategies for genomics remains a significant challenge.</p>
      <p id="Par7">In this study, we present a suite of evolution-based data augmentations and a two-stage training curriculum to preserve functional integrity (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a). In the first stage, a DNN is trained on sequences with EvoAug augmentations applied stochastically online during training, using the same training labels as the wild-type sequence. The goal is to enhance the model’s ability to learn robust representations of features, such as motifs, by exposing it to expanded (albeit synthetically generated) genetic variation. While each augmentation has the potential to disrupt core motifs in any given perturbation, we expect the overall effect to preserve motifs on average. However, the specific data augmentations employed may introduce a bias in how these motif grammars are structured. Thus, in the second stage, the DNN is fine-tuned on the original, unperturbed data to refine these features and guide the function towards the observed biology, thereby removing any bias introduced by the data augmentations (see Methods).<fig id="Fig1"><label>Fig. 1</label><caption><p>EvoAug improves generalization and interpretability of Basset models. <bold>a</bold> Schematic of evolution-inspired data augmentations (left) and the two-stage training curriculum (right). <bold>b</bold> Generalization performance (area under the precision-recall curve) for Basset models pretrained with individual and combinations of augmentations, i.e., Noise+Ins+RC (Gaussian noise, insertion, reverse-complement) and all augmentations (Gaussian noise, reverse-complement, mutation, translocation, deletion, insertion), and fine-tuned on Basset dataset. Standard represents no augmentations during training. <bold>c</bold> Comparison of the average hit rate of first-layer filters to known motifs in the JASPAR database (top) and the average <italic>q</italic>-value of the filters with matches (bottom). <bold>d</bold> Comparison of the average Pearson correlation between model predictions and experimental data from CAGI5 Challenge. <bold>b</bold>–<bold>d</bold> Each box-plot represents 5 trials with random initializations</p></caption><graphic xlink:href="13059_2023_2941_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par8">EvoAug data augmentations introduce a modeling bias to learn invariances of the (un)natural symmetries generated by the augmentations. For instance, random insertions and deletions assume that the distance between motifs is not critical, whereas random inversions and translocations promote invariances to motif strand orientation and the order of motifs, respectively. Nevertheless, the bias created by the augmentations can lead to poor generalization when the introduced bias does not accurately reflect the underlying biology. Therefore, the fine-tuning stage is critical as it provides an avenue to unlearn any biases not supported by the observed data.</p>
    </sec>
    <sec id="Sec4">
      <title>EvoAug improves generalization and interpretability of genomic DNNs</title>
      <p id="Par9">To demonstrate the utility of EvoAug, we analyzed several established DNNs across three prominent types of regulatory genomic prediction tasks that span a range of complexity.</p>
      <p id="Par10">First, we applied Evoaug to the Basset model and dataset [<xref ref-type="bibr" rid="CR20">20</xref>], which consists of a multi-task binary classification of chromatin accessibility sites across 161 cell types/tissues. We trained the Basset model with each augmentation applied independently and in various combinations. We conducted a hyperparameter sweep to determine the optimal settings for each augmentation (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figs. S1-S5). From hyperparameter sweeps, we observed that the inversion augmentation improved performance up to the sequence length, which is essentially a reverse-complement transformation (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figs. S1, S3, and S4). Hence, inversions were excluded to reduce redundancy.</p>
      <p id="Par11">Remarkably, EvoAug-trained DNNs outperformed standard training with no augmentations (Fig. <xref rid="Fig1" ref-type="fig">1</xref>b). The best results were achieved when multiple augmentations were used together. Additionally, we found that fine-tuning on the original data further improved performance, even when augmentation hyperparameters were poorly specified (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S1). Notably, specific EvoAug augmentations, such as random mutations and combinations of data augmentations, had a profound impact on improving the motif representations learned by the first-layer convolutional filters (Fig. <xref rid="Fig1" ref-type="fig">1</xref>c). The convolutional filters capture a wider repertoire of motifs and their representations better reflect known motifs, both quantitatively and qualitatively, when compared with convolutional filters of models trained without augmentations. This suggests that EvoAug augmentations can help DNNs learn more accurate and informative representations of the sequence motifs.</p>
      <p id="Par12">A major downstream application of genomic DNNs is to score the functional consequences of non-coding mutations. By evaluating the zero-shot prediction capabilities of each DNN on saturation mutagenesis data of 15 <italic>cis</italic>-regulatory elements from the CAGI5 Challenge [<xref ref-type="bibr" rid="CR21">21</xref>], we found that models trained with EvoAug outperformed their standard training counterpart (Fig. <xref rid="Fig1" ref-type="fig">1</xref>d). Notably, Basset’s performance was comparable to other DNNs based on binary predictions [<xref ref-type="bibr" rid="CR17">17</xref>]; however, its overall performance was lower than more sophisticated DNNs and top competitors in the CAGI5 challenge [<xref ref-type="bibr" rid="CR2">2</xref>]. Interestingly, we observed that DNNs pretrained with Gaussian noise or random mutagenesis augmentations did not perform well. These augmentations impose flatness locally in sequence-function space, effectively reducing the effect size of nucleotide variants. However, fine-tuning these models improved their variant effect predictions beyond what was achieved with standard training, thus demonstrating the effectiveness of the two-stage training curriculum.</p>
      <p id="Par13">To further demonstrate the benefits of EvoAug, we trained DeepSTARR models as a multi-task quantitative regression to predict enhancer activity from self-transcribing active regulatory region sequencing (STARR-seq) data [<xref ref-type="bibr" rid="CR9">9</xref>], where each task represents a different promoter from a developmental or housekeeping gene in <italic>Drosophila</italic> S2 cells. Most EvoAug augmentations resulted in improved performance, except for reverse-complement and random mutations (Fig. <xref rid="Fig2" ref-type="fig">2</xref>a and Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figs. S3-S5). As before, we observed additional performance gains when augmentations were used in combination. Furthermore, the attribution maps generated by EvoAug-trained models were more interpretable, with identifiable motifs and less spurious noise (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S6).</p>
      <p id="Par14">In addition, we found that the EvoAug-trained DNNs consistently outperformed DNNs with standard training on various single-task binary classification tasks for TF binding across multiple chromatin immunoprecipitation sequencing (ChIP-seq) datasets (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b). Interestingly, we did not observe any significant improvement in performance after fine-tuning, suggesting that the implicit prior imposed by EvoAug augmentations was appropriate for these tasks; the underlying regulatory grammars for these TFs are not complex.<fig id="Fig2"><label>Fig. 2</label><caption><p>Generalization of EvoAug on additional models and datasets. <bold>a</bold> Box-plot of regression performance for DeepSTARR models pretrained with individual or combination of augmentations (i.e., insertion + translocation + deletion; all augmentations) and fine-tuned on original STARR-seq data for two promoters: developmental (top) and housekeeping (bottom). Standard represents no augmentations during training. <bold>b</bold> Box-plot of classification performance (area under the receiver-operating-characteristic curve) for DNNs trained on ChIP-seq datas. <bold>c</bold> Average classification performance for ChIP-seq experiments downsampled to different dataset sizes. Shaded region represents the standard deviation of the mean. <bold>a</bold>, <bold>b</bold> Each box-plot represents 5 trials with random initializations</p></caption><graphic xlink:href="13059_2023_2941_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par15">To further investigate the impact of EvoAug on small datasets, we retrained each DNN on down-sampled versions of two abundant ChIP-seq datasets. We found that EvoAug-trained DNNs exhibit a greater improvement in performance for smaller datasets compared to standard training (Fig. <xref rid="Fig2" ref-type="fig">2</xref>c). This result suggests that EvoAug can be particularly useful in scenarios where the available training data is limited.</p>
      <p id="Par16">Training with EvoAug adds a computational cost, depending on the augmentations chosen and their settings (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Tables S2 and S3). Nevertheless, EvoAug stabilized training (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S7), leading to smoother convergence and improved generalization overall.</p>
    </sec>
  </sec>
  <sec id="Sec5">
    <title>Conclusion</title>
    <p id="Par17">EvoAug greatly expands the set of available data augmentations for genomic DNNs. Our study demonstrated that EvoAug’s two-stage training curriculum is effective in improving generalization performance. Moreover, EvoAug-trained models learned better representations of consensus motifs, as evidenced by filter visualization and attribution analysis.</p>
    <p id="Par18">Our findings support previous arguments for using evolution as a natural source of data augmentation [<xref ref-type="bibr" rid="CR22">22</xref>]. Interestingly, the impact of synthetic evolutionary perturbations was not excessively disruptive, and performance even improved before fine-tuning in most cases. This functional robustness appears to be a characteristic of the non-coding genome [<xref ref-type="bibr" rid="CR23">23</xref>].</p>
    <p id="Par19">Data augmentations are a commonly used technique to balance bias and variance in machine learning models. However, their effectiveness is expected to decrease as the dataset size increases. Nevertheless, EvoAug still improved performance on the already large Basset dataset. Other methods that can enhance generalization include multitask learning [<xref ref-type="bibr" rid="CR24">24</xref>], contrastive learning [<xref ref-type="bibr" rid="CR25">25</xref>, <xref ref-type="bibr" rid="CR26">26</xref>], and language modeling [<xref ref-type="bibr" rid="CR27">27</xref>]. Even though Basset and DeepSTARR are already trained in a multitask framework, EvoAug improved their performance. Multitasking can introduce class imbalance, but EvoAug provides additional examples with pseudo-positive labels, which can mitigate this issue. EvoAug also provides different views of the data, which can be useful for contrastive learning. Importantly, EvoAug is a lightweight and effective strategy that only requires the original data.</p>
    <p id="Par20">The optimal combination of augmentations and their hyperparameter choices depends on the model and dataset. While we performed hyperparameter grid searches in this study, more advanced search strategies such as population-based training [<xref ref-type="bibr" rid="CR28">28</xref>] using Ray Tune [<xref ref-type="bibr" rid="CR29">29</xref>] could improve efficiency. In the future, we plan to investigate EvoAug’s potential in cross-dataset generalization and variant effect predictions, including expression quantitative trait loci.</p>
    <p id="Par21">EvoAug is a PyTorch package that is open-source [<xref ref-type="bibr" rid="CR30">30</xref>], easy to use, extensible, and accessible via pip (<ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/evoaug">https://pypi.org/project/evoaug</ext-link>) and GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/p-koo/evoaug">https://github.com/p-koo/evoaug</ext-link>), with full documentation provided on ReadtheDocs.org (<ext-link ext-link-type="uri" xlink:href="https://evoaug.readthedocs.io">https://evoaug.readthedocs.io</ext-link>). In time, we plan to extend EvoAug functionality to TensorFlow [<xref ref-type="bibr" rid="CR30">30</xref>] and JAX [<xref ref-type="bibr" rid="CR31">31</xref>]. We anticipate that EvoAug will have broad utility in improving the efficacy of sequence-based DNNs for regulatory genomics.</p>
  </sec>
  <sec id="Sec6">
    <title>Methods</title>
    <sec id="Sec7">
      <title>Models and datasets</title>
      <sec id="Sec8">
        <title>Basset</title>
        <p id="Par22">The Basset dataset [<xref ref-type="bibr" rid="CR20">20</xref>] consists of a multi-task binary classification of chromatin accessibility sites across 161 cell types/tissues. The inputs are genomic sequences of length 600 nt and the output are binary labels (representing accessible or not accessible) for 161 cell types measured experimentally using DNase I hypersensitive sites sequencing (DNase-seq). We filtered sequences that contained at least one N character and the data splits (training; validation; test) reduced from (1,879,982; 70,000; 71,886) to (437,478; 16,410; 16,703). This “cleaned” dataset [<xref ref-type="bibr" rid="CR32">32</xref>] was analyzed using a Basset-inspired model, which is given according to:</p>
        <p id="Par23"><list list-type="bullet"><list-item><p id="Par24">Input <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x \in \{0,1\}^{600 \times 4}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mn>600</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq1.gif"/></alternatives></inline-formula> (one-hot encoding of 600 nt sequence)</p></list-item><list-item><p id="Par25">1D convolution (300 filters, size 19, stride 1)</p></list-item><list-item><p id="Par26">BatchNorm + ReLU</p></list-item><list-item><p id="Par27">Max-pooling (size 3, stride 3)</p></list-item><list-item><p id="Par28">1D convolution (200 filters, size 11, stride 1)</p></list-item><list-item><p id="Par29">BatchNorm + ReLU</p></list-item><list-item><p id="Par30">Max-pooling (size 4, stride 4)</p></list-item><list-item><p id="Par31">1D convolution (200 filters, size 7, stride 1)</p></list-item><list-item><p id="Par32">BatchNorm + ReLU</p></list-item><list-item><p id="Par33">Max-pooling (size 2, stride 2)</p></list-item><list-item><p id="Par34">Fully-connected (1000 units)</p></list-item><list-item><p id="Par35">BatchNorm + ReLU</p></list-item><list-item><p id="Par36">Dropout (0.3)</p></list-item><list-item><p id="Par37">Fully-connected (1000 units)</p></list-item><list-item><p id="Par38">BatchNorm + ReLU</p></list-item><list-item><p id="Par39">Dropout (0.3)</p></list-item><list-item><p id="Par40">Fully-connected output (161 units, sigmoid)</p></list-item></list>BatchNorm represents batch normalization [<xref ref-type="bibr" rid="CR33">33</xref>], and dropout [<xref ref-type="bibr" rid="CR34">34</xref>] rates set the probability that neurons in a given layer are temporarily removed during each mini-batch of training.</p>
      </sec>
      <sec id="Sec9">
        <title>DeepSTARR</title>
        <p id="Par41">The DeepSTARR dataset [<xref ref-type="bibr" rid="CR9">9</xref>] consists of a multi-task regression of enhancer activity for two promoters, well-known developmental and housekeeping transcriptional programs in <italic>D. melanogaster</italic> S2 cells. The inputs are genomic sequences of length 249 nt and the output is 2 scalar values representing the activity of developmental enhancers and housekeeping enhancers measured experimentally using STARR-seq. Sequences with N characters were also removed, but this minimally affected the size of the dataset (i.e., reduced it by approximately 0.005%). This dataset [<xref ref-type="bibr" rid="CR32">32</xref>] was analyzed using the original DeepSTARR model, given according to:</p>
        <p id="Par42">
          <list list-type="bullet">
            <list-item>
              <p id="Par43">Input <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x \in \{0,1\}^{249 \times 4}$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mn>249</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq2.gif"/></alternatives></inline-formula></p>
            </list-item>
            <list-item>
              <p id="Par44">1D convolution (256 filters, size 7, stride 1)</p>
            </list-item>
            <list-item>
              <p id="Par45">BatchNorm + ReLU</p>
            </list-item>
            <list-item>
              <p id="Par46">Max-pooling (size 2, stride 2)</p>
            </list-item>
            <list-item>
              <p id="Par47">1D convolution (60 filters, size 3, stride 1)</p>
            </list-item>
            <list-item>
              <p id="Par48">BatchNorm + ReLU</p>
            </list-item>
            <list-item>
              <p id="Par49">Max-pooling (size 2, stride 2)</p>
            </list-item>
            <list-item>
              <p id="Par50">1D convolution (60 filters, size 5, stride 1)</p>
            </list-item>
            <list-item>
              <p id="Par51">BatchNorm + ReLU</p>
            </list-item>
            <list-item>
              <p id="Par52">Max-pooling (size 2, stride 2)</p>
            </list-item>
            <list-item>
              <p id="Par53">1D convolution (120 filters, size 3, stride 1)</p>
            </list-item>
            <list-item>
              <p id="Par54">BatchNorm + ReLU</p>
            </list-item>
            <list-item>
              <p id="Par55">Max-pooling (size 2, stride 2)</p>
            </list-item>
            <list-item>
              <p id="Par56">Fully-connected (256 units)</p>
            </list-item>
            <list-item>
              <p id="Par57">BatchNorm + ReLU</p>
            </list-item>
            <list-item>
              <p id="Par58">Dropout (0.4)</p>
            </list-item>
            <list-item>
              <p id="Par59">Fully-connected (256 units)</p>
            </list-item>
            <list-item>
              <p id="Par60">BatchNorm + ReLU</p>
            </list-item>
            <list-item>
              <p id="Par61">Dropout (0.4)</p>
            </list-item>
            <list-item>
              <p id="Par62">Fully-connected output (2 units, linear)</p>
            </list-item>
          </list>
        </p>
      </sec>
      <sec id="Sec10">
        <title>ChIP-seq</title>
        <p id="Par63">Transcription factor (TF) chromatin immunoprecipitation sequencing (ChIP-seq) data was processed and framed as a binary classification task. The inputs are genomic sequences of length 200 nt and the output is a single binary label representing TF binding activity, with positive-label sequences indicating the presence of a ChIP-seq peak and negative-label sequences indicating a peak for a DNase I hypersensitive site from the same cell type but one that does not overlap with any ChIP-seq peaks. Nine representative TF ChIP-seq experiments in a GM12878 cell line and a DNase-seq experiment for the same cell line were downloaded from ENCODE [<xref ref-type="bibr" rid="CR35">35</xref>]; for details, see Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S1. Negative sequences (i.e., DNase-seq peaks that do not overlap with any positive peaks) were randomly down-sampled to match the number of positive sequences, keeping the classes balanced. The dataset was split randomly into training, validation, and test set according to the fractions 0.7, 0.1, and 0.2, respectively [<xref ref-type="bibr" rid="CR32">32</xref>].</p>
        <p id="Par64">A custom convolutional neural network was employed to analyze these datasets, given according to:<list list-type="bullet"><list-item><p id="Par65">Input <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x \in \{0,1\}^{200 \times 4}$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mn>200</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq3.gif"/></alternatives></inline-formula></p></list-item><list-item><p id="Par66">1D convolution (64 filters, size 7, stride 1)</p></list-item><list-item><p id="Par67">BatchNorm + ReLU</p></list-item><list-item><p id="Par68">Dropout (0.2)</p></list-item><list-item><p id="Par69">Max-pooling (size 4, stride 4)</p></list-item><list-item><p id="Par70">1D convolution (96 filters, size 5, stride 1)</p></list-item><list-item><p id="Par71">BatchNorm + ReLU</p></list-item><list-item><p id="Par72">Dropout (0.2)</p></list-item><list-item><p id="Par73">Max-pooling (size 4, stride 4)</p></list-item><list-item><p id="Par74">1D convolution (128 filters, size 5, stride 1)</p></list-item><list-item><p id="Par75">BatchNorm + ReLU</p></list-item><list-item><p id="Par76">Dropout (0.2)</p></list-item><list-item><p id="Par77">Max-pooling (size 2, stride 2)</p></list-item><list-item><p id="Par78">Fully-connected layer (256 units)</p></list-item><list-item><p id="Par79">BatchNorm + ReLU</p></list-item><list-item><p id="Par80">Dropout (0.5)</p></list-item><list-item><p id="Par81">Fully-connected output layer (1 unit, sigmoid)</p></list-item></list></p>
      </sec>
    </sec>
    <sec id="Sec11">
      <title>Evolution-inspired data augmentations</title>
      <p id="Par82">EvoAug is comprised of a set of data augmentations given by the following:<list list-type="bullet"><list-item><p id="Par83">Mutation: a transformation where single nucleotide mutations are randomly applied to a given wild-type sequence. This is implemented as follows: (1) given the hyperparameter of the fraction of nucleotides in each sequence to mutate (mutate_frac), the number of mutations for a given sequence length is calculated; (2) a position along the sequence is randomly sampled (with replacement) for each number of mutations; and (3) the selected positions are mutagenized to a random nucleotide. Since our implementation does not guarantee that a nucleotide selected will be mutated to a different nucleotide than it originally was, we take approximate account for silent mutations by dividing the user-defined mutate_frac by 0.75 so that on average the fraction of nucleotides in each sequence mutated to a different nucleotide is equal to mutate_frac.</p></list-item><list-item><p id="Par84">Translocation: a transformation that randomly selects a break point in the sequence (thereby creating two segments) and then swaps the order of the two sequence segments. An equivalent statement of this transformation is a “roll”—shifting the sequence forward along its length a randomly specified distance and then reintroducing the part of the sequence shifted beyond the last position back at the first position. This is implemented as follows: (1) given the hyperparameters of the minimum distance (shift_min, default 0) and maximum distance (shift_max) of the shift, the integer-valued shift length is chosen randomly from the interval <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\left[ -\texttt {shift\_max}, -\texttt {shift\_min}\right] \cup \left[ \texttt {shift\_min}, \texttt {shift\_max}\right] }$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mfenced close="]" open="["><mml:mo>-</mml:mo><mml:mi mathvariant="monospace">shift</mml:mi><mml:mi>_</mml:mi><mml:mi mathvariant="monospace">max</mml:mi><mml:mo>,</mml:mo><mml:mo>-</mml:mo><mml:mi mathvariant="monospace">shift</mml:mi><mml:mi>_</mml:mi><mml:mi mathvariant="monospace">min</mml:mi></mml:mfenced><mml:mo>∪</mml:mo><mml:mfenced close="]" open="["><mml:mi mathvariant="monospace">shift</mml:mi><mml:mi>_</mml:mi><mml:mi mathvariant="monospace">min</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="monospace">shift</mml:mi><mml:mi>_</mml:mi><mml:mi mathvariant="monospace">max</mml:mi></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq4.gif"/></alternatives></inline-formula>, where a negative value simply denotes a backward shift rather than a forward shift, and (2) the shift is applied to the sequence with a roll() function in PyTorch.</p></list-item><list-item><p id="Par85">Insertion: a transformation where a random DNA sequence (of random length) is inserted randomly into a wild-type sequence. This is implemented as follows: (1) given the hyperparameters of the minimum length (insert_min, default 0) and maximum length (insert_max) of the insertion, the integer-valued insertion length is chosen randomly from the interval between insert_min and insert_max (inclusive), and (2) the insertion is inserted at a random position within the original sequence. Importantly, to maintain a constant input sequence length to the model (i.e., original length plus insert_max), the remaining amount of length between the insertion length and insert_max is split evenly and placed on the 5’ and 3’ flanks of the sequence, with the remainder from odd lengths going to the 3′ end. Whenever an insertion augmentation is employed in combination with other augmentations, all sequences without an insertion are padded with a stretch of random DNA of length insert_max at the 3′ end to ensure that the model processes sequences with a constant length for both training and inference time.</p></list-item><list-item><p id="Par86">Deletion: a transformation where a random, contiguous segment of a wild-type sequence is removed, and the shortened sequence is then padded with random DNA sequence to maintain the same length as wild-type. This is implemented as follows: (1) given the hyperparameters of the minimum length (delete_min, default 0) and maximum length (delete_max) of the deletion, the integer-valued deletion length is chosen randomly from the interval between delete_min and delete_max (inclusive); (2) the starting position of the deletion is chosen randomly from the valid positions in the sequence that can encapsulate the deletion; (3) the deletion is performed on the designated stretch of the sequence; (4) the remaining portions of the sequence are concatenated together; and (5) random DNA is used to pad the 5′ and 3′ flanks to maintain a constant input sequence length, similar to the procedure for insertions.</p></list-item><list-item><p id="Par87">Inversion: a transformation where a random subsequence is replaced by its reverse-complement. This is implemented as follows: (1) given the hyperparameters of the minimum length (invert_min, default 0) and maximum length (invert_max) of the inversion, the integer-valued inversion length is chosen randomly from the interval between invert_min and invert_max (inclusive); (2) the starting position of the inversion is chosen randomly from the valid position indices in the sequence; and (3) the inversion (i.e., a reverse-complement transformation) is performed on the designated subsequence while the remaining portions of the sequence remain untouched.</p></list-item><list-item><p id="Par88">Reverse-complement: a transformation where a full sequence is replaced with some probability rc_prob by its reverse-complement.</p></list-item><list-item><p id="Par89">Gaussian noise: a transformation where Gaussian noise (with distribution parameters <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\texttt {noise\_mean} = 0$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi mathvariant="monospace">noise</mml:mi><mml:mi>_</mml:mi><mml:mi mathvariant="monospace">mean</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq5.gif"/></alternatives></inline-formula> and noise_std) is added to the input sequence; a random value drawn independently and identically from the specified distribution is added to each element of the one-hot input matrix.</p></list-item></list></p>
      <sec id="Sec12">
        <title>Pretraining with data augmentations</title>
        <p id="Par90">Training with augmentations requires two main hyperparameters: first, a set of augmentations to sample from; and second, the maximum number of augmentations to be applied to a sequence. For each mini-batch during training, each sequence is randomly augmented independently. The number of augmentations to be applied to a given sequence has two possible settings in EvoAug: (1) hard, always equal to the maximum number of augmentations, or (2) soft, randomly select the number of augmentations for a sequence from 1 to the maximum number. Our experiments with Basset and DeepSTARR use the former setting, while our experiments with ChIP-seq datasets use the latter setting. Then, the subset of augmentations to be applied to the sequence is sampled randomly without replacement from the user-defined set of augmentations. After a subset of augmentations is chosen, the order in which multiple augmentations are applied to a single sequence is given by the following priority: inversion, deletion, translocation, insertion, reverse-complement, mutation, noise addition. Each augmentation is then applied stochastically for each sequence.</p>
        <p id="Par91">For the Basset and DeepSTARR models, each augmentation has an optimal setting that was determined from a hyperparameter search independently using the validation set (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figs. S1, S3, and S4). For the Basset models, the hyperparameters were set to:<list list-type="bullet"><list-item><p id="Par92">mutation: mutate_frac = 0.15</p></list-item><list-item><p id="Par93">translocation: shift_min = 0, shift_max = 30</p></list-item><list-item><p id="Par94">insertion: insert_min = 0, insert_max = 30</p></list-item><list-item><p id="Par95">deletion: delete_min = 0, delete_max = 30</p></list-item><list-item><p id="Par96">reverse-complement: rc_prob = 0.5</p></list-item><list-item><p id="Par97">noise: noise_mean = 0, noise_std (standard deviation) = 0.3</p></list-item></list>For the DeepSTARR models, the hyperparameters were set to:<list list-type="bullet"><list-item><p id="Par98">mutation: mutate_frac = 0.05</p></list-item><list-item><p id="Par99">translocation: shift_min = 0, shift_max = 20</p></list-item><list-item><p id="Par100">insertion: insert_min = 0, insert_max = 20</p></list-item><list-item><p id="Par101">deletion: delete_min = 0, delete_max = 30</p></list-item><list-item><p id="Par102">reverse-complement: rc_prob = 0</p></list-item><list-item><p id="Par103">noise: noise_mean = 0, noise_std = 0.3</p></list-item></list> When augmentations were used in combinations, the maximum number of augmentations was set to 3 for Basset and 2 for DeepSTARR. The same hyperparameter settings used in DeepSTARR analyses with all augmentations were used for the ChIP-seq analysis. For models trained with combinations of augmentations, the hyperparameters intrinsic to augmentations were set at the values identified above and the maximum number of augmentations per sequence was also determined through a hyperparameter sweep for each dataset (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figs. S2 and S5).</p>
        <p id="Par104">Unless otherwise specified, all models were trained (with or without data augmentations) for 100 epochs using the Adam optimizer [<xref ref-type="bibr" rid="CR36">36</xref>] with an initial learning rate of <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1 \times 10^{-3}$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq6.gif"/></alternatives></inline-formula> and a weight decay (<inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq7.gif"/></alternatives></inline-formula> penalty) term of <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1 \times 10^{-6}$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq8.gif"/></alternatives></inline-formula>; additionally, we employed early stopping with a patience of 10 epochs and a learning rate decay that decreased the learning rate by a factor of 0.1 when the validation loss did not improve for 5 epochs. For each model trained, the version of the model with the highest-performing weights during its training, as measured by validation loss, is the version of the model whose performance is reported here.</p>
      </sec>
      <sec id="Sec13">
        <title>Fine-tuning</title>
        <p id="Par105">Models that completed training with data augmentations were subsequently fine-tuned on the original dataset without augmentations. Fine-tuning employs the Adam optimizer with a learning rate of <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1 \times 10^{-4}$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq9.gif"/></alternatives></inline-formula> and a weight decay (<inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq10.gif"/></alternatives></inline-formula> penalty) term of <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1 \times 10^{-6}$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq11.gif"/></alternatives></inline-formula> for 5 epochs. The model that yields the lowest validation loss was used for test time evaluation.</p>
      </sec>
      <sec id="Sec14">
        <title>Evaluation</title>
        <p id="Par106">When evaluating models on validation or test sets, no data augmentations were used on input sequences. For models trained with an insertion augmentation (alone or in combination with other augmentations), each sequence is padded at the 3′ end with a stretch of random DNA of length insert_max.</p>
      </sec>
    </sec>
    <sec id="Sec15">
      <title>Interpretability analysis</title>
      <sec id="Sec16">
        <title>Filter interpretability</title>
        <p id="Par107">We visualized the first-layer filters of various Basset models according to activation-based alignments [<xref ref-type="bibr" rid="CR37">37</xref>] and compared how well they match motifs in the 2022 JASPAR nonredundant vertebrates database [<xref ref-type="bibr" rid="CR38">38</xref>] using Tomtom [<xref ref-type="bibr" rid="CR39">39</xref>], a motif search comparison tool. Matrix profiles MA1929.1 and MA0615.1 were excluded from filter matching to remove poor quality hits; low information content filters tend to have a high hit rate with these two matrix profiles. Hit rate is calculated by measuring how many filters matched to at least one JASPAR motif. Average <italic>q</italic>-value is calculated by taking the average of the smallest <italic>q</italic>-values for each filter among its matches.</p>
      </sec>
      <sec id="Sec17">
        <title>Attribution analysis</title>
        <p id="Par108">SHAP-based [<xref ref-type="bibr" rid="CR40">40</xref>] attribution maps (implemented with GradientShap from the Captum package [<xref ref-type="bibr" rid="CR41">41</xref>]) were used to generate sequence logos (visualized by Logomaker [<xref ref-type="bibr" rid="CR42">42</xref>]) for sequences that exhibited high experimental enhancer activity for the Developmental promoter (i.e., task 0 in the DeepSTARR dataset). One thousand random DNA sequences were synthesized to serve as references for each GradientShap-based attribution map. A gradient correction [<xref ref-type="bibr" rid="CR43">43</xref>] was applied to each attribution map. For comparison, this analysis was repeated for a DeepSTARR model that was trained without any augmentations and a fine-tuned DeepSTARR model that was pretrained with all augmentations (excluding inversions) with two augmentations per sequence.</p>
      </sec>
    </sec>
    <sec id="Sec18">
      <title>CAGI5 challenge analysis</title>
      <p id="Par109">The CAGI5 challenge dataset [<xref ref-type="bibr" rid="CR21">21</xref>] was used to benchmark model performance on variant effect predictions. This dataset contains massively parallel reporter assays (MPRAs) that measure the effect size of single-nucleotide variants through saturation mutagenesis of 15 different regulatory elements ranging from 187 nt to 600 nt in length. We extracted 600 nt sequences from the reference genome centered on each regulatory region of interest and converted it into a one-hot representation. Alternative alleles were then substituted correspondingly to construct the CAGI test sequences.</p>
      <p id="Par110">For a given Basset model, the output predictions of two input sequences, one with a centered reference allele and the other with an alternative allele, are made. The cell type-agnostic approach employed in this study uses the mean across these values to calculate a single scalar value, functional activity across cell types. The effect size is then calculated with the log-ratio of this single value for the alternative allele and reference allele, according to: <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\log$$\end{document}</tex-math><mml:math id="M24"><mml:mo>log</mml:mo></mml:math><inline-graphic xlink:href="13059_2023_2941_Article_IEq12.gif"/></alternatives></inline-formula>(alternative value/reference value).</p>
      <p id="Par111">To evaluate the variant effect prediction performance, Pearson correlation was calculated within each CAGI5 experiment between the experimentally measured and predicted effect sizes. The average of the Pearson correlation across all 15 experiments represents the overall performance of the model.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec19">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="13059_2023_2941_MOESM1_ESM.pdf">
            <caption>
              <p><bold>Additional file 1.</bold> Supplementary Tables S1-S3 and Figures S1-S7.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="13059_2023_2941_MOESM2_ESM.docx">
            <caption>
              <p><bold>Additional file 2.</bold> Review history.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>This work was supported in part by funding from the NIH grant R01HG012131 and the Simons Center for Quantitative Biology at Cold Spring Harbor Laboratory. This work was performed with assistance from the US National Institutes of Health Grant S10OD028632-01.</p>
    <sec id="FPar9">
      <title>Peer review information</title>
      <p id="Par112">Andrew Cosgrove was the primary editor of this article and managed its editorial process and peer review in collaboration with the rest of the editorial team.</p>
    </sec>
    <sec id="FPar10">
      <title>Review history</title>
      <p id="Par113">The review history is available as Additional file <xref rid="MOESM2" ref-type="media">2</xref>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>NKL and PKK conceived of the method, designed the experiments, and wrote the majority of the code base. NKL, ST, and ZT conducted experiments and analyzed the data. PKK oversaw the project. All authors interpreted the results and contributed to the manuscript. The authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>EvoAug Python package is deposited on the Python Package Index (PyPI) repository with documentation hosted on <ext-link ext-link-type="uri" xlink:href="https://evoaug.readthedocs.io">https://evoaug.readthedocs.io</ext-link>. The open-source project repository is available under the MIT license at GitHub [<xref ref-type="bibr" rid="CR30">30</xref>] ( <ext-link ext-link-type="uri" xlink:href="https://github.com/p-koo/evoaug">https://github.com/p-koo/evoaug</ext-link>). The code to reproduce analyses in this paper is available under the MIT license at GitHub, <ext-link ext-link-type="uri" xlink:href="https://github.com/p-koo/evoaug_analysis">https://github.com/p-koo/evoaug_analysis</ext-link>. Processed data, including DeepSTARR [<xref ref-type="bibr" rid="CR9">9</xref>], Basset [<xref ref-type="bibr" rid="CR20">20</xref>] and ChIP-seq analysis, are available at Zenodo [<xref ref-type="bibr" rid="CR32">32</xref>] (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7265991">doi.org/10.5281/zenodo.7265991</ext-link>). Model weights and code [<xref ref-type="bibr" rid="CR44">44</xref>] are also available at Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7767325">doi.org/10.5281/zenodo.7767325</ext-link>).</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar11">
      <title>Ethics approval and consent to participate</title>
      <p id="Par114">Not applicable.</p>
    </notes>
    <notes id="FPar12" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par115">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Troyanskaya</surname>
            <given-names>OG</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A sequence-based global map of regulatory activity for deciphering human genetics</article-title>
        <source>Nat Genet.</source>
        <year>2022</year>
        <volume>54</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1038/s41588-022-01102-2</pub-id>
        <pub-id pub-id-type="pmid">35022602</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Avsec</surname>
            <given-names>Ž</given-names>
          </name>
          <name>
            <surname>Agarwal</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Visentin</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ledsam</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Grabska-Barwinska</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>KR</given-names>
          </name>
          <name>
            <surname>Assael</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Jumper</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kohli</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kelley</surname>
            <given-names>DR</given-names>
          </name>
        </person-group>
        <article-title>Effective gene expression prediction from sequence by integrating long-range interactions</article-title>
        <source>Nat Methods.</source>
        <year>2021</year>
        <volume>18</volume>
        <issue>10</issue>
        <fpage>1196</fpage>
        <lpage>1203</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-021-01252-x</pub-id>
        <?supplied-pmid 34608324?>
        <pub-id pub-id-type="pmid">34608324</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Sequence-based modeling of three-dimensional genome architecture from kilobase to chromosome scale</article-title>
        <source>Nat Genet.</source>
        <year>2022</year>
        <volume>54</volume>
        <issue>5</issue>
        <fpage>725</fpage>
        <lpage>734</lpage>
        <pub-id pub-id-type="doi">10.1038/s41588-022-01065-4</pub-id>
        <?supplied-pmid 35551308?>
        <pub-id pub-id-type="pmid">35551308</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hoffman</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Bendl</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Girdhar</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Schadt</surname>
            <given-names>EE</given-names>
          </name>
          <name>
            <surname>Roussos</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Functional interpretation of genetic variants using deep learning predicts impact on chromatin accessibility and histone modification</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2019</year>
        <volume>47</volume>
        <issue>20</issue>
        <fpage>10597</fpage>
        <lpage>10611</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkz808</pub-id>
        <?supplied-pmid 31544924?>
        <pub-id pub-id-type="pmid">31544924</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dey</surname>
            <given-names>KK</given-names>
          </name>
          <name>
            <surname>Van de Geijn</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Hormozdiari</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Kelley</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Price</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>Evaluating the informativeness of deep learning annotations for human complex diseases</article-title>
        <source>Nat Commun.</source>
        <year>2020</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1038/s41467-020-18515-4</pub-id>
        <pub-id pub-id-type="pmid">31911652</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koo</surname>
            <given-names>PK</given-names>
          </name>
          <name>
            <surname>Ploenzke</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Improving representations of genomic sequence motifs in convolutional networks with exponential activations</article-title>
        <source>Nat Mach Intell.</source>
        <year>2021</year>
        <volume>3</volume>
        <issue>3</issue>
        <fpage>258</fpage>
        <lpage>266</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-020-00291-x</pub-id>
        <?supplied-pmid 34322657?>
        <pub-id pub-id-type="pmid">34322657</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Avsec</surname>
            <given-names>Ž</given-names>
          </name>
          <name>
            <surname>Weilert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Shrikumar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Krueger</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Alexandari</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dalal</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Fropf</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>McAnany</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Gagneur</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kundaje</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Base-resolution models of transcription-factor binding reveal soft motif syntax</article-title>
        <source>Nat Genet.</source>
        <year>2021</year>
        <volume>53</volume>
        <issue>3</issue>
        <fpage>354</fpage>
        <lpage>366</lpage>
        <pub-id pub-id-type="doi">10.1038/s41588-021-00782-6</pub-id>
        <?supplied-pmid 33603233?>
        <pub-id pub-id-type="pmid">33603233</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koo</surname>
            <given-names>PK</given-names>
          </name>
          <name>
            <surname>Majdandzic</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ploenzke</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Anand</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Paul</surname>
            <given-names>SB</given-names>
          </name>
        </person-group>
        <article-title>Global importance analysis: An interpretability method to quantify importance of genomic features in deep neural networks</article-title>
        <source>PLoS Comput Biol.</source>
        <year>2021</year>
        <volume>17</volume>
        <issue>5</issue>
        <fpage>1008925</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008925</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>de Almeida</surname>
            <given-names>BP</given-names>
          </name>
          <name>
            <surname>Reiter</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Pagani</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Stark</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Deepstarr predicts enhancer activity from DNA sequence and enables the de novo design of synthetic enhancers</article-title>
        <source>Nat Genet.</source>
        <year>2022</year>
        <volume>54</volume>
        <issue>5</issue>
        <fpage>613</fpage>
        <lpage>624</lpage>
        <pub-id pub-id-type="doi">10.1038/s41588-022-01048-5</pub-id>
        <?supplied-pmid 35551305?>
        <pub-id pub-id-type="pmid">35551305</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Horton</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Alexandari</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Hayes</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Schaepe</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Marklund</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Aditham</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Shrikumar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Afek</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Greenleaf</surname>
            <given-names>WJ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Short tandem repeats recruit transcription factors to tune eukaryotic gene expression</article-title>
        <source>Biophys J.</source>
        <year>2022</year>
        <volume>121</volume>
        <issue>3</issue>
        <fpage>287</fpage>
        <lpage>288</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bpj.2021.11.1305</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shorten</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Khoshgoftaar</surname>
            <given-names>TM</given-names>
          </name>
        </person-group>
        <article-title>A survey on image data augmentation for deep learning</article-title>
        <source>J Big Data.</source>
        <year>2019</year>
        <volume>6</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>48</lpage>
        <pub-id pub-id-type="doi">10.1186/s40537-019-0197-0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Fort S, Brock A, Pascanu R, De S, Smith SL. Drawing multiple augmentation samples per image during training efficiently decreases test error. 2021. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2105.13343">arXiv:2105.13343</ext-link></mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>An</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Understanding the generalization benefit of model invariance from a data perspective</article-title>
        <source>Adv Neural Inf Process Syst.</source>
        <year>2021</year>
        <volume>34</volume>
        <fpage>4328</fpage>
        <lpage>4341</lpage>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Geiping J, Goldblum M, Somepalli G, Shwartz-Ziv R, Goldstein T, Wilson AG. How much data are augmentations worth? An investigation into scaling laws, invariance, and implicit regularization. 2022. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2210.06441">arXiv:2210.06441</ext-link></mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Puli A, Zhang LH, Oermann EK, Ranganath R. Out-of-distribution generalization in the presence of nuisance-induced spurious correlations. 2021. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2107.00520">arXiv:2107.00520</ext-link></mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Zhou H, Shrikumar A, Kundaje A. Towards a better understanding of reverse-complement equivariance for deep learning models in genomics. In: Machine Learning in Computational Biology, PMLR; 2022. p. 1–33</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Toneyan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Koo</surname>
            <given-names>PK</given-names>
          </name>
        </person-group>
        <article-title>Evaluating deep learning for predicting epigenomic profiles</article-title>
        <source>Nat Mach Intell.</source>
        <year>2022</year>
        <volume>4</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-022-00570-9</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kelley</surname>
            <given-names>DR</given-names>
          </name>
        </person-group>
        <article-title>Cross-species regulatory sequence activity prediction</article-title>
        <source>PLoS Comput Biol</source>
        <year>2020</year>
        <volume>16</volume>
        <issue>7</issue>
        <fpage>1008050</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008050</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Frazer</surname>
            <given-names>KA</given-names>
          </name>
          <name>
            <surname>Murray</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Schork</surname>
            <given-names>NJ</given-names>
          </name>
          <name>
            <surname>Topol</surname>
            <given-names>EJ</given-names>
          </name>
        </person-group>
        <article-title>Human genetic variation and its contribution to complex traits</article-title>
        <source>Nat Rev Genet</source>
        <year>2009</year>
        <volume>10</volume>
        <issue>4</issue>
        <fpage>241</fpage>
        <lpage>251</lpage>
        <pub-id pub-id-type="doi">10.1038/nrg2554</pub-id>
        <?supplied-pmid 19293820?>
        <pub-id pub-id-type="pmid">19293820</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kelley</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Snoek</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rinn</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks</article-title>
        <source>Genome Res</source>
        <year>2016</year>
        <volume>26</volume>
        <issue>7</issue>
        <fpage>990</fpage>
        <lpage>999</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.200535.115</pub-id>
        <?supplied-pmid 27197224?>
        <pub-id pub-id-type="pmid">27197224</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shigaki</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Adato</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Adhikari</surname>
            <given-names>AN</given-names>
          </name>
          <name>
            <surname>Dong</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Hawkins-Hooker</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Inoue</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Juven-Gershon</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kenlay</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Patra</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Penzar</surname>
            <given-names>DD</given-names>
          </name>
          <name>
            <surname>Schubach</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Boyle</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Kreimer</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kulakovskiy</surname>
            <given-names>IV</given-names>
          </name>
          <name>
            <surname>Reid</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Unger</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yosef</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Shendure</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ahituv</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Kircher</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Beer</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Integration of multiple epigenomic marks improves prediction of variant impact in saturation mutagenesis reporter assay</article-title>
        <source>Hum Mutat</source>
        <year>2019</year>
        <volume>40</volume>
        <issue>9</issue>
        <fpage>1280</fpage>
        <lpage>1291</lpage>
        <pub-id pub-id-type="doi">10.1002/humu.23797</pub-id>
        <?supplied-pmid 31106481?>
        <pub-id pub-id-type="pmid">31106481</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Lu, A.X, Lu, A.X, Moses, A. Evolution is all you need: phylogenetic augmentation for contrastive learning. 2020. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2012.13475">arXiv:2012.13475</ext-link></mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kryukov</surname>
            <given-names>GV</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sunyaev</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Small fitness effect of mutations in highly conserved non-coding regions</article-title>
        <source>Hum Mol Genet</source>
        <year>2005</year>
        <volume>14</volume>
        <issue>15</issue>
        <fpage>2221</fpage>
        <lpage>2229</lpage>
        <pub-id pub-id-type="doi">10.1093/hmg/ddi226</pub-id>
        <?supplied-pmid 15994173?>
        <pub-id pub-id-type="pmid">15994173</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Crawshaw, M. Multi-task learning with deep neural networks: a survey. 2020. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2009.09796">arXiv:2009.09796</ext-link></mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Zbontar J, Jing L, Misra I, LeCun Y, Deny S. Barlow twins: Self-supervised learning via redundancy reduction. In: International Conference on Machine Learning, PMLR; 2021. p. 12310–12320</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Hjelm RD, Fedorov A, Lavoie-Marchildon S, Grewal K, Bachman P, Trischler A, Bengio Y. Learning deep representations by mutual information estimation and maximization. 2018. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1808.06670">arXiv:1808.06670</ext-link></mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Devlin J, Chang M-W, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding. 2018. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1810.04805">arXiv:1810.04805</ext-link></mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Jaderberg M, Dalibard V, Osindero S, Czarnecki WM, Donahue J, Razavi A, Vinyals O, Green T, Dunning I, Simonyan K, et al. Population based training of neural networks. 2017. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1711.09846">arXiv:1711.09846</ext-link></mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Liaw R, Liang E, Nishihara R, Moritz P, Gonzalez JE, Stoica I. Tune: a research platform for distributed model selection and training. 2018. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1807.05118">arXiv:1807.05118</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, Corrado GS, Davis A, Dean J, Devin M, Ghemawat S, Goodfellow I, Harp A, Irving G, Isard M, Jia Y, Jozefowicz R, Kaiser L, Kudlur M, Levenberg J, Mané D, Monga R, Moore S, Murray D, Olah C, Schuster M, Shlens J, Steiner B, Sutskever I, Talwar K, Tucker P, Vanhoucke V, Vasudevan V, Viégas F, Vinyals O, Warden P, Wattenberg M, Wicke M, Yu Y, Zheng X. TensorFlow: Large-scale machine learning on heterogeneous systems. 2015. <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/">https://www.tensorflow.org/</ext-link>. Accessed 31 Oct 2022.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Bradbury J, Frostig R, Hawkins P, Johnson MJ, Leary C, Maclaurin D, Necula G, Paszke A, VanderPlas J, Wanderman-Milne S, Zhang Q. JAX: Composable transformations of Python+NumPy programs. <ext-link ext-link-type="uri" xlink:href="https://github.com/google/jax">http://github.com/google/jax</ext-link>. Accessed 31 Oct 2022.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Lee NK, Toneyan S, Tang Z, Koo PK. EvoAug Data [Data set]. Zenodo. 2022. 10.5281/zenodo.7265991.  Accessed 31 Oct 2022. </mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift. International Conference on Machine Learning, PMLR; 2015. p. 448–456</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>
        <source>J Mach Learn Res.</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>1929</fpage>
        <lpage>1958</lpage>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Luo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hitz</surname>
            <given-names>BC</given-names>
          </name>
          <name>
            <surname>Gabdank</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Hilton</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Kagda</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Lam</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Myers</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Sud</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Jou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>New developments on the encyclopedia of DNA elements (encode) data portal</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2020</year>
        <volume>48</volume>
        <issue>D1</issue>
        <fpage>882</fpage>
        <lpage>889</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkz1062</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Kingma D, Ba J. Adam: A method for stochastic optimization. 2014. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">arXiv:1412.6980</ext-link></mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koo</surname>
            <given-names>PK</given-names>
          </name>
          <name>
            <surname>Ploenzke</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Deep learning for inferring transcription factor binding sites</article-title>
        <source>Curr Opin Syst Biol</source>
        <year>2020</year>
        <volume>19</volume>
        <fpage>16</fpage>
        <lpage>23</lpage>
        <pub-id pub-id-type="doi">10.1016/j.coisb.2020.04.001</pub-id>
        <?supplied-pmid 32905524?>
        <pub-id pub-id-type="pmid">32905524</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Castro-Mondragon</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Riudavets-Puig</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Rauluseviciute</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Lemma</surname>
            <given-names>RB</given-names>
          </name>
          <name>
            <surname>Turchi</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Blanc-Mathieu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lucas</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Boddie</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pérez</surname>
            <given-names>NM</given-names>
          </name>
          <name>
            <surname>Fornes</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Leung</surname>
            <given-names>TY</given-names>
          </name>
          <name>
            <surname>Aguirre</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hammal</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Schmelter</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Baranasic</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ballester</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sandelin</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lenhard</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Vandepoele</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Wasserman</surname>
            <given-names>WW</given-names>
          </name>
          <name>
            <surname>Parcy</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Mathelier</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>JASPAR 2022: the 9th release of the open-access database of transcription factor binding profiles</article-title>
        <source>Nucleic Acids Res</source>
        <year>2021</year>
        <volume>50</volume>
        <issue>D1</issue>
        <fpage>165</fpage>
        <lpage>173</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkab1113</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gupta</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Stamatoyannopoulos</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Bailey</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Noble</surname>
            <given-names>WS</given-names>
          </name>
        </person-group>
        <article-title>Quantifying similarity between motifs</article-title>
        <source>Genome Biol</source>
        <year>2007</year>
        <volume>8</volume>
        <issue>2</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1186/gb-2007-8-2-r24</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Lundberg SM, Lee S-I. A unified approach to interpreting model predictions. Adv Neural Inf Process Syst. 2017;30. <ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html">https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Kokhlikyan N, Miglani V, Martin M, Wang E, Alsallakh B, Reynolds J, Melnikov A, Kliushkina N, Araya C, Yan S, Reblitz-Richardson O. Captum: a unified and generic model interpretability library for pytorch. 2020. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2009.07896">arXiv:2009.07896</ext-link></mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tareen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kinney</surname>
            <given-names>JB</given-names>
          </name>
        </person-group>
        <article-title>Logomaker: beautiful sequence logos in python</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <issue>7</issue>
        <fpage>2272</fpage>
        <lpage>2274</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz921</pub-id>
        <?supplied-pmid 31821414?>
        <pub-id pub-id-type="pmid">31821414</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Majdandzic A, Rajesh C, Koo PK. Statistical correction of input gradients for black box models trained with categorical input features. 2022. bioRxiv preprint. <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.04.29.490102v2">biorxiv.org/content/10.1101/2022.04.29.490102v2</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Lee NK, Toneyan S, Tang Z, Koo PK. EvoAug reproducibility code. Github. 2022. <ext-link ext-link-type="uri" xlink:href="https://github.com/p-koo/evoaug_analysis">https://github.com/p-koo/evoaug_analysis</ext-link>. Accessed 31 Oct 2022. </mixed-citation>
    </ref>
  </ref-list>
</back>
