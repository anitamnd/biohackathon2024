<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Brief Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Brief Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">bib</journal-id>
    <journal-title-group>
      <journal-title>Briefings in Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1467-5463</issn>
    <issn pub-type="epub">1477-4054</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10938904</article-id>
    <article-id pub-id-type="doi">10.1093/bib/bbae083</article-id>
    <article-id pub-id-type="publisher-id">bbae083</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Problem Solving Protocol</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Enhancer-MDLF: a novel deep learning framework for identifying cell-specific enhancers</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0009-0007-4698-7786</contrib-id>
        <name>
          <surname>Zhang</surname>
          <given-names>Yao</given-names>
        </name>
        <aff><institution>School of Software, Shandong University</institution>, <addr-line>Jinan, 250100, Shandong</addr-line>, <country country="CN">China</country></aff>
        <xref rid="afn1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-8696-4983</contrib-id>
        <name>
          <surname>Zhang</surname>
          <given-names>Pengyu</given-names>
        </name>
        <aff><institution>College of Information Engineering, Northwest A&amp;F University</institution>, <addr-line>Yangling, 712100, Shaanxi</addr-line>, <country country="CN">China</country></aff>
        <xref rid="afn1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-2340-9258</contrib-id>
        <name>
          <surname>Wu</surname>
          <given-names>Hao</given-names>
        </name>
        <!--haowu@sdu.edu.cn-->
        <aff><institution>School of Software, Shandong University</institution>, <addr-line>Jinan, 250100, Shandong</addr-line>, <country country="CN">China</country></aff>
        <xref rid="cor1" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="cor1">Corresponding author. Hao Wu, School of Software, Shandong University, Jinan, 250100, Shandong, China. Tel.:+86-18254105536; Fax:+86-0531-88391686; E-mail: <email>haowu@sdu.edu.cn</email></corresp>
      <fn id="afn1">
        <p>Yao Zhang and Pengyu Zhang contributed equally to this work.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2024-03-13">
      <day>13</day>
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <volume>25</volume>
    <issue>2</issue>
    <elocation-id>bbae083</elocation-id>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>8</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>27</day>
        <month>1</month>
        <year>2024</year>
      </date>
      <date date-type="accepted">
        <day>7</day>
        <month>2</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2024</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="bbae083.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>Enhancers, noncoding DNA fragments, play a pivotal role in gene regulation, facilitating gene transcription. Identifying enhancers is crucial for understanding genomic regulatory mechanisms, pinpointing key elements and investigating networks governing gene expression and disease-related mechanisms. Existing enhancer identification methods exhibit limitations, prompting the development of our novel multi-input deep learning framework, termed Enhancer-MDLF. Experimental results illustrate that Enhancer-MDLF outperforms the previous method, Enhancer-IF, across eight distinct human cell lines and exhibits superior performance on generic enhancer datasets and enhancer–promoter datasets, affirming the robustness of Enhancer-MDLF. Additionally, we introduce transfer learning to provide an effective and potential solution to address the prediction challenges posed by enhancer specificity. Furthermore, we utilize model interpretation to identify transcription factor binding site motifs that may be associated with enhancer regions, with important implications for facilitating the study of enhancer regulatory mechanisms. The source code is openly accessible at <ext-link xlink:href="https://github.com/HaoWuLab-Bioinformatics/Enhancer-MDLF" ext-link-type="uri">https://github.com/HaoWuLab-Bioinformatics/Enhancer-MDLF</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>DNA sequence</kwd>
      <kwd>cell-specific enhancers</kwd>
      <kwd>deep learning</kwd>
      <kwd>transfer learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>62272278</award-id>
        <award-id>61972322</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Key Research and Development Program</institution>
            <institution-id institution-id-type="DOI">10.13039/501100012166</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>2021YFF0704103</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Fundamental Research Funds of Shandong University</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="12"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>INTRODUCTION</title>
    <p>Enhancers, noncoding fragments within DNA sequences, play a pivotal role in regulating gene transcription [<xref rid="ref1" ref-type="bibr">1</xref>, <xref rid="ref2" ref-type="bibr">2</xref>]. As a class of regulatory elements, enhancers exert control over diverse cellular activities, including tissue-specific gene expression [<xref rid="ref3" ref-type="bibr">3</xref>], cell growth and differentiation [<xref rid="ref4" ref-type="bibr">4</xref>] and cell carcinogenesis [<xref rid="ref5" ref-type="bibr">5</xref>]. Mutations or abnormal expression of enhancers can disrupt gene regulatory networks, thereby affecting cellular function, tissue development and disease progression [<xref rid="ref6" ref-type="bibr">6</xref>]. Many recent studies have found that the genetic mechanisms of complex diseases can be better revealed by understanding the role of enhancers in gene expression [<xref rid="ref7" ref-type="bibr">7–9</xref>]. Therefore, the identification of enhancers is crucial for advancing the comprehension of gene expression and regulation.</p>
    <p>High-throughput computational and experimental methods have been employed to predict enhancers. Several methods for identifying enhancers are as follows: (i) Computational Analysis Using Conserved Sequences and Transcription Factor Binding Site Data [<xref rid="ref10" ref-type="bibr">10</xref>, <xref rid="ref11" ref-type="bibr">11</xref>]. This method effectively predicts the genomic locations where known transcription factors (TFs) with binding sequence motifs are likely to interact. However, it may yield false positives by encompassing regulatory element sequences that bind TFs but do not serve as enhancers. (ii) Utilizing ChIP-seq Data for Transcription Factors and P300. ChIP-seq data for TFs can identify enhancers bound by known TFs [<xref rid="ref12" ref-type="bibr">12</xref>]. However, it cannot distinguish between enhancer and promoter regions because both can bind TFs. Furthermore, not all enhancers necessarily bind TFs. ChIP-seq data for p300 [<xref rid="ref13" ref-type="bibr">13</xref>], commonly used for enhancer prediction, faces limitations in distinguishing between active and inactive enhancers. (iii) Chromatin Accessibility-Related Data (e.g. DNase-seq [<xref rid="ref14" ref-type="bibr">14</xref>], FAIRE-seq [<xref rid="ref15" ref-type="bibr">15</xref>], ATAC-seq [<xref rid="ref16" ref-type="bibr">16</xref>]). This method relies on data related to chromatin accessibility, However, it may yield false positives by including other transcriptional regulatory elements, such as promoters, insulators and silencers. (iv) Histone Modification Data [<xref rid="ref17" ref-type="bibr">17</xref>] (e.g. H3K4me1 and H3K27ac). Using both H3K4me1, which marks active and poised enhancers, and H3K27ac, which marks associated with active regulatory regions from both promoters and enhancers to identify activated enhancer regions, this method benefits from the widespread availability of histone modification data across different species, effectively supporting various research needs. However, its drawback lies in the broad nature of histone modification data across the entire genome, hindering precise enhancer prediction. (v) Prediction based on enhancer RNA (eRNA) data [<xref rid="ref18" ref-type="bibr">18</xref>] (e.g. RNA-seq, ChAR-seq, GRO-seq and NET-seq). Enhancers transcribe eRNAs, and their locations can be predicted using eRNA data obtained through sequencing techniques. However, this method is limited in predicting enhancers that are not actively transcribed. Despite their individual merits, these experimental methods have inherent limitations, and they are both time-consuming and expensive. Therefore, it is essential to develop reliable computational tools for enhancer identification.</p>
    <p>In recent times, several computational methods for enhancer identification have been proposed, including ienhancer-2L [<xref rid="ref19" ref-type="bibr">19</xref>], Enhancerpred [<xref rid="ref20" ref-type="bibr">20</xref>], ienhancer-EL [<xref rid="ref21" ref-type="bibr">21</xref>], ienhancer-ECNN [<xref rid="ref22" ref-type="bibr">22</xref>], BERT-enhancer [<xref rid="ref23" ref-type="bibr">23</xref>], ienhancer-EBLSTM [<xref rid="ref24" ref-type="bibr">24</xref>] and ienhancer-XG [<xref rid="ref25" ref-type="bibr">25</xref>]. Notably, these methods are based on the dataset created by Liu <italic toggle="yes">et al</italic>. [<xref rid="ref19" ref-type="bibr">19</xref>]. However, two notable issues arise with this dataset. Firstly, the enhancers within it are extracted as short sequences of a fixed length (200bp), raising questions about the adaptability of these methods to unequal-length sequences and their ability to maintain optimal performance under such circumstances. Secondly, the dataset is a mixed general dataset encompassing nine cell lines, notwithstanding the established understanding that enhancers exhibit cell-specificity [<xref rid="ref26" ref-type="bibr">26</xref>, <xref rid="ref27" ref-type="bibr">27</xref>].</p>
    <p>To further investigate the cell-specific nature of enhancers, Enhancer-IF [<xref rid="ref28" ref-type="bibr">28</xref>], a framework based on integrated machine learning (ML), was proposed to identify enhancers. This framework utilizes eight cell lines with known cell-specific enhancers. Cross-cell line validation results demonstrate that a significant majority of enhancers indeed exhibit cell-type specificity. This underscores the importance of considering cell specificity in enhancer identification, a facet not fully addressed by the earlier methods relying on the general dataset approach.</p>
    <p>Despite notable advancements in enhancer identification, there exist notable limitations. Firstly, the predictive performance of Enhancer-IF for cell-specific enhancers is not ideal, which may be due to the conventional feature encoding scheme and the relatively simplistic design of the model framework. Secondly, Enhancer-IF integrates five commonly used classifiers (Random Forest, Extremely Randomized Tree, MultiLayer Perceptron, Support Vector Machine and Extreme Gradient Boosting) and employs a grid search algorithm to optimize parameters for each classifier across every cell line. Undoubtedly, this approach is time-consuming when applied to new cell lines. Furthermore, there is a lack of in-depth exploration of potential strategies to mitigate the impact of cell specificity on the overall performance of enhancer prediction. Lastly, Enhancer-IF lacks explanations for its prediction models, which is crucial for exploring transcription factor binding sites (TFBSs) motifs in enhancer regions. The absence of such interpretability hinders a comprehensive understanding of the biological insights derived from the predictions. Addressing these limitations is crucial for advancing the accuracy, efficiency and biological interpretability of enhancer prediction models.</p>
    <p>Therefore, we propose Enhancer-MDLF, a Multi-input Deep Learning Framework designed to predict cell-specific enhancers across multiple human cell lines. Our approach amalgamates word vector features derived from the human genome sequence and motif features extracted from the position weight matrix (PWM) of motifs. Through comprehensive evaluation on cell-specific datasets and other pertinent datasets, we demonstrate the superior performance of enhancer-MDLF. The principal contributions of our work include (i) the introduction of a novel deep learning framework, employing multi-module inputs for the identification of cell-specific enhancers; (ii) the substantiation of Enhancer-MDLF’s substantial outperformance relative to state-of-the-art predictors, accomplished without the need of parameter tuning; (iii) the incorporation of transfer learning into our model to address the challenges in cross-cell line predictions stemming from enhancer specificity and (iv) a meticulous analysis of the conservation and specificity of enhancers at the motif level, culminating in the identification of the most important TFBS motifs within enhancer regions. The overall framework of our study is depicted in <xref rid="f1" ref-type="fig">Figure 1</xref>.</p>
    <fig position="float" id="f1">
      <label>Figure 1</label>
      <caption>
        <p><bold>The overall flowchart of Enhancer-MDLF.</bold> (<bold>A</bold>) Sequence encoding: Enhancer-MDLF utilizes two distinct sequence encoding schemes, namely dna2vec and Motif frequency. (<bold>B</bold>) Model construction: Enhancer-MDLF is structured by integrating two pivotal modules: the dna2vec module and the motif module. (<bold>C</bold>) Model evaluation. Enhancer-MDLF undergoes thorough evaluation across multiple dimensions to assess its performance. (<bold>D</bold>) Motif analysis: Utilizing the SHAP framework, we conduct an in-depth analysis of the important features identified by Enhancer-MDLF.</p>
      </caption>
      <graphic xlink:href="bbae083f1" position="float"/>
    </fig>
  </sec>
  <sec id="sec2">
    <title>MATERIALS AND METHODS</title>
    <sec id="sec2a">
      <title>Dataset</title>
      <p>In this study, we utilize the benchmark dataset derived from Enhancer-IF [<xref rid="ref28" ref-type="bibr">28</xref>]. The dataset encompasses eight distinct cell lines, namely GM12878, HEK293, HMEC, HSMM, HUVEC, K562, NHEK and NHLF. They utilize Enhancer Atlas 2.0 [<xref rid="ref29" ref-type="bibr">29</xref>] (<ext-link xlink:href="http://www.enhanceratlas.org/indexv2.php" ext-link-type="uri">http://www.enhanceratlas.org/indexv2.php</ext-link>) to extract enhancer locations for each cell line, and their corresponding sequences are obtained through Retailor [<xref rid="ref30" ref-type="bibr">30</xref>] (<ext-link xlink:href="http://shiva.rockefeller.edu/SeqTailor/" ext-link-type="uri">http://shiva.rockefeller.edu/SeqTailor/</ext-link>). To ensure diversity, the CD-HIT software is applied to eliminate paired sequences with a similarity exceeding 60%. The construction of negative samples follows the methodology introduced by Dao <italic toggle="yes">et al</italic>. [<xref rid="ref31" ref-type="bibr">31</xref>]. Finally, a training set and an independent test set are obtained for each of the eight cell lines. For more details regarding the dataset employed in this study, please refer to <xref rid="TB1" ref-type="table">Table 1</xref>.</p>
      <table-wrap position="float" id="TB1">
        <label>Table 1</label>
        <caption>
          <p>Statistical summary of training and independent datasets for different cell lines</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" span="1"/>
            <col span="2" align="left"/>
            <col span="3" align="left"/>
            <col span="4" align="left"/>
            <col span="5" align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th align="left" rowspan="1" colspan="1">Cell lines</th>
              <th colspan="2" align="left" rowspan="1">Training</th>
              <th colspan="2" align="left" rowspan="1">Independent</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th align="left" rowspan="1" colspan="1">Positives</th>
              <th align="left" rowspan="1" colspan="1">Negatives</th>
              <th align="left" rowspan="1" colspan="1">Positives</th>
              <th align="left" rowspan="1" colspan="1">Negatives</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">GM12878</td>
              <td rowspan="1" colspan="1">2187</td>
              <td rowspan="1" colspan="1">2187</td>
              <td rowspan="1" colspan="1">1187</td>
              <td rowspan="1" colspan="1">2356</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HEK293</td>
              <td rowspan="1" colspan="1">3756</td>
              <td rowspan="1" colspan="1">3756</td>
              <td rowspan="1" colspan="1">2662</td>
              <td rowspan="1" colspan="1">5324</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HMEC</td>
              <td rowspan="1" colspan="1">3333</td>
              <td rowspan="1" colspan="1">3333</td>
              <td rowspan="1" colspan="1">1795</td>
              <td rowspan="1" colspan="1">3590</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HSMM</td>
              <td rowspan="1" colspan="1">2821</td>
              <td rowspan="1" colspan="1">2821</td>
              <td rowspan="1" colspan="1">1520</td>
              <td rowspan="1" colspan="1">3040</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HUVEC</td>
              <td rowspan="1" colspan="1">4750</td>
              <td rowspan="1" colspan="1">4750</td>
              <td rowspan="1" colspan="1">2559</td>
              <td rowspan="1" colspan="1">5118</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">K562</td>
              <td rowspan="1" colspan="1">3318</td>
              <td rowspan="1" colspan="1">3318</td>
              <td rowspan="1" colspan="1">1787</td>
              <td rowspan="1" colspan="1">3754</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NHEK</td>
              <td rowspan="1" colspan="1">2896</td>
              <td rowspan="1" colspan="1">2896</td>
              <td rowspan="1" colspan="1">1559</td>
              <td rowspan="1" colspan="1">3118</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NHLF</td>
              <td rowspan="1" colspan="1">1462</td>
              <td rowspan="1" colspan="1">1462</td>
              <td rowspan="1" colspan="1">788</td>
              <td rowspan="1" colspan="1">1576</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec2b">
      <title>DNA sequence encoding schemes</title>
      <p>Enhancer-MDLF is a deep learning model, including a dna2vec module and a motif module. The two modules utilize different sequence encoding schemes as input, as depicted in the following sections.</p>
      <sec id="sec2b1">
        <title>dna2vec</title>
        <p>In recent times, word embedding techniques have gained significant popularity within the bioinformatics community, offering a promising solution to address the challenge posed by the similarity of kmer-based features in different sequences, even when their orders are reversed [<xref rid="ref32" ref-type="bibr">32</xref>, <xref rid="ref33" ref-type="bibr">33</xref>]. However, a notable limitation arises when employing word vector techniques for sequence coding. Typically, the training corpus for learning word vectors encompasses only one cell line dataset. This constrained corpus imparts a limited amount of information to the learned word vectors, thereby restricting their representational capacity.</p>
        <p>To address the above limitation, we utilize the pre-trained DNA vectors provided in dna2vec as a sequence encoding index. dna2vec, a method grounded in the word2vec word embedding model, is employed to compute distributed representations of variable-length k-mers within DNA sequences. Earlier investigations have substantiated that the mathematical operations applied to dna2vec vectors exhibit similarity to nucleotide concatenation [<xref rid="ref34" ref-type="bibr">34</xref>]. dna2vec employs the human genome sequence as a learning corpus for unsupervised training, employing the continuous skip-gram (Skip-gram) model in word2vec. This process results in embedding k-mers into a continuous vector space with 100 dimensions.</p>
        <p>In this study, we conducted experiments within the range of [3,8] (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>) to determine that the optimal value for k is 3. Subsequently, we utilize the pre-trained dna2vec model to obtain a 100-dimensional feature vector for each word. These feature vectors are obtained by concatenating the vectors of short sequences with a step size of 1 in the overall sequence. They are then input to the dna2vec module as sequence features. Assuming that the sequence length is denoted as L, the input dimension of the dna2vec module becomes 100<inline-formula><tex-math notation="LaTeX" id="ImEquation1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\times $\end{document}</tex-math></inline-formula> (L - k + 1). It is noteworthy that due to varying feature dimensions for unequal sequences in the dataset, adaptive pooling operations are employed to flatten the feature dimensions to 10 000 dimensions. This pre-processing step is essential for subsequent input into the deep learning model.</p>
      </sec>
      <sec id="sec2b2">
        <title>Motif frequency</title>
        <p>In dna2vec, the parameter k in k-mer is set to 3, signifying that DNA fragments are divided into short sequences with a nucleotide length of 3. Given the relatively brief nature of these sequences, addressing this limitation necessitates the utilization of longer features for capturing intricate sequence patterns. Notably, TFs play an important role in gene transcription by directly binding motifs in the genome. A previous study has successfully identified some potential TF binding within DNA sequences, particularly those inclined to bind in the enhancer regions [<xref rid="ref35" ref-type="bibr">35</xref>]. Leveraging this biological characteristic, we extract the count of TFBS motifs within each DNA sequence and convert it into a frequency representation for input to the motif module.</p>
        <p>We extract the PWM of motifs from the HOCOMOCO Human v11 database [<xref rid="ref36" ref-type="bibr">36</xref>] for sliding-scale matching to the sequence data in this dataset. Assuming that the length of the motif is denoted as <inline-formula><tex-math notation="LaTeX" id="ImEquation2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula>, the PWM is structured as a matrix with <inline-formula><tex-math notation="LaTeX" id="ImEquation3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula> rows and four columns, representing the values corresponding to each base (A, C, G and T). The process involves dividing each sequence of length <inline-formula><tex-math notation="LaTeX" id="ImEquation4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{s}$\end{document}</tex-math></inline-formula> into subsequences of length <inline-formula><tex-math notation="LaTeX" id="ImEquation5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula> with a stride of 1. Consequently, we obtain <inline-formula><tex-math notation="LaTeX" id="ImEquation6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{s}$\end{document}</tex-math></inline-formula> - <inline-formula><tex-math notation="LaTeX" id="ImEquation7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula> +1 subsequence segments, each of length <inline-formula><tex-math notation="LaTeX" id="ImEquation8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula>. For each subsequence segment, we calculate the sum of the corresponding value for each base, and this sum serves as the final matching score. This score is then compared with the predetermined threshold score to determine whether this subsequence segment matches a motif. The criteria for subsequence segment comparison are defined by the following equation: </p>
        <disp-formula id="deqn01">
          <label>(1)</label>
          <tex-math notation="LaTeX" id="DmEquation1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*}&amp; Q = \sum_{i=0}^{l-1} PWM_{ij},\end{align*}\end{document}</tex-math>
        </disp-formula>
        <p>where the variable j takes on values 0, 1, 2 and 3, corresponding to the nucleotides A, C, G and T in the subsequence segments, and Q represents the matching score. Assuming P represents the <italic toggle="yes">P</italic>-value threshold score (set at <inline-formula><tex-math notation="LaTeX" id="ImEquation9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$10^{-4}$\end{document}</tex-math></inline-formula>) for respective motifs, the subsequence segment is deemed to match this motif when <inline-formula><tex-math notation="LaTeX" id="ImEquation10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$Q&gt; P$\end{document}</tex-math></inline-formula>.</p>
        <p>Upon traversing each sequence, we gather information on the count of each TFBS motif, resulting in a 401-dimensional feature vector denoted as <inline-formula><tex-math notation="LaTeX" id="ImEquation11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$V_{count}$\end{document}</tex-math></inline-formula>. Additionally, to accommodate varying sequence lengths, we utilize the feature vector <inline-formula><tex-math notation="LaTeX" id="ImEquation12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$V_{frequency}$\end{document}</tex-math></inline-formula> as the final model input, calculated through the following equation: </p>
        <disp-formula id="deqn02">
          <label>(2)</label>
          <tex-math notation="LaTeX" id="DmEquation2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*}&amp; V_{frequency} = \frac{V_{count}}{L_{s}},\end{align*}\end{document}</tex-math>
        </disp-formula>
        <p>where <inline-formula><tex-math notation="LaTeX" id="ImEquation13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{s}$\end{document}</tex-math></inline-formula> represents the length of each sequence.</p>
      </sec>
    </sec>
    <sec id="sec2e">
      <title>The framework of Enhancer-MDLF</title>
      <sec id="sec2e1">
        <title>Model architecture</title>
        <p>The DNA fragments extracted from the dataset undergo encoding through two schemes, namely dna2vec and motif. Subsequently, these encoded fragments are input into the model to predict whether the fragment contains enhancer regions. Through systematic experimentation involving various combinations of convolutional layers, max-pooling layers and dense layers, along with meticulous parameter tuning for optimal balance between accuracy, efficiency and generalization capabilities, we have defined the comprehensive framework of Enhancer-MDLF, as illustrated in <xref rid="f2" ref-type="fig">Figure 2A</xref>. The details are elaborated as follows.</p>
        <fig position="float" id="f2">
          <label>Figure 2</label>
          <caption>
            <p><bold>The model architecture and training procedure of Enhancer-MDLF.</bold> (<bold>A</bold>) Enhancer-MDLF comprises a dna2vec module predominantly composed of convolutional layers and a motif module primarily consisting of Dense layers. It takes dna2vec and motif frequency, the two sequence encoding results, as inputs. The fused features are derived by concatenating after feature extraction and subsequently passed through a sigmoid function for enhancer detection. (<bold>B</bold>) The training procedure of the Enhancer-MDLF framework. The process involves iterations through the training set five times to iteratively refine the predictive model. Subsequently, this model undergoes testing on the test set to yield the final prediction results.</p>
          </caption>
          <graphic xlink:href="bbae083f2" position="float"/>
        </fig>
        <p><bold>Feature Extraction:</bold> We employ a combination of three 1D convolutional layers with corresponding 1D max-pooling layers in the dna2vec module, and three dense layers in the motif module. The convolutional layers are instrumental in capturing complex features from the inputs through convolutional computations. Simultaneously, the max-pooling layers implement a down-sampling approach, selecting the maximum value for each sub-region. This not only improves the robustness of the model but also mitigates the risk of overfitting. Specifically, we define three convolutional layers with 64 filters, a kernel size of 7 and a stride of 3. Additionally, three max-pooling layers are constructed with a pool size of 2, respectively.</p>
        <p>To further enhance the model’s generalization and prevent overfitting, we introduce a dropout layer with a probability of 0.6. This layer randomly removes certain neural network units during the training phase, contributing to the model’s overall robustness and preventing excessive adaptation to the training data.</p>
        <p><bold>Prediction:</bold> The dna2vec module undergoes computation through a dense layer comprising 500 neurons after flattening. Simultaneously, the motif module is designed to culminate in 16 neurons, with a choice of the ‘relu’ activation function for both modules. Within the motif module, we incorporate a dropout layer with a probability of 0.6 following the three dense layers. Finally, Enhancer-MDLF combines the results from the dna2vec and motif modules and generates predictive values using a dense layer with a single neuron and a ‘sigmoid’ activation function. To further enhance model generalization, a dropout layer with a probability of 0.5 is applied after concatenating the two sets of results. The classification criterion is set such that a sample is considered positive if the predicted value exceeds 0.5; otherwise, it is deemed negative.</p>
        <p><bold>Hyperparameters:</bold> The hyperparameters of Enhancer-MDLF include learning rate, batch size and maximum epoch. Following a comprehensive comparison of performance across multiple hyperparameter combinations through the grid search method, we establish the optimal settings as follows: learning rate = 0.0001, batch size = 100 and max epoch = 200. The specific details of the grid search method and the details of the search ranges for hyperparameters can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>.</p>
      </sec>
      <sec id="sec2e2">
        <title>Loss function</title>
        <p>The dataset we utilize exhibits an imbalance, a characteristic that poses challenges for traditional loss functions. Traditional approaches tend to disproportionately penalize dominant classes, often neglecting the informative contributions of minority classes in imbalanced datasets, thereby resulting in suboptimal prediction performance. Recognizing this, the focal loss [<xref rid="ref37" ref-type="bibr">37</xref>] initially employed in computer vision [<xref rid="ref38" ref-type="bibr">38</xref>, <xref rid="ref39" ref-type="bibr">39</xref>] has emerged as a solution. Consequently, we use the focal loss as the primary loss function for Enhancer-MDLF, aiming to mitigate the drawbacks associated with the traditional cross-entropy loss function in managing imbalanced datasets. The focal loss is formally defined as follows: </p>
        <disp-formula id="deqn03">
          <label>(3)</label>
          <tex-math notation="LaTeX" id="DmEquation3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*}&amp; FL(P_{t})=-\alpha(1-P_{t})^{\gamma}\log(P_{t}),\end{align*}\end{document}</tex-math>
        </disp-formula>
        <p>where <inline-formula><tex-math notation="LaTeX" id="ImEquation14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$P_{t}$\end{document}</tex-math></inline-formula> represents the predicted probability, <inline-formula><tex-math notation="LaTeX" id="ImEquation15">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula> denotes the balance parameter and <inline-formula><tex-math notation="LaTeX" id="ImEquation16">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\gamma $\end{document}</tex-math></inline-formula> is the focus parameter. To determine the optimal configuration for Enhancer-MDLF, we conduct a thorough performance evaluation using the grid search method over a range of <inline-formula><tex-math notation="LaTeX" id="ImEquation17">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula> values [0.25, 0.5, 0.75] and <inline-formula><tex-math notation="LaTeX" id="ImEquation18">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\gamma $\end{document}</tex-math></inline-formula> values [1, 2, 3]. Our findings reveal that the model achieves optimal performance when <inline-formula><tex-math notation="LaTeX" id="ImEquation19">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula>=0.5 and <inline-formula><tex-math notation="LaTeX" id="ImEquation20">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\gamma $\end{document}</tex-math></inline-formula>=3.</p>
      </sec>
      <sec id="sec2e3">
        <title>Training procedure</title>
        <p>We utilize a novel training strategy for Enhancer-MDLF to enhance its capacity to effectively learn information from the input feature vectors of the dna2vec and motif modules. First, we implement a 5-fold cross-validation algorithm, randomly partitioning the training set into five folds with a 4:1 ratio between the training and validation sets. To prevent overfitting, an early stopping mechanism is applied to the validation set. The model undergoes training five times, with the initial training parameters being the model’s starting parameters, determined by TensorFlow’s default network initialization methods [<xref rid="ref40" ref-type="bibr">40</xref>]. Xavier initialization is frequently used in TensorFlow for a variety of neural network layers, such as dense and convolutional layers. This method initializes weights randomly from a uniform or normal distribution, and the calculation of the standard deviation depends on the number of input and output units in the layer. Subsequent trainings build upon the most recent model, utilizing different folds of the training and validation sets. Following the completion of the five training sessions, the model is applied to the testing set for prediction.</p>
        <p>As shown in <xref rid="f2" ref-type="fig">Figure 2B</xref>, the workflow of Enhancer-MDLF is as follows:</p>
        <p>(i) Divide the training dataset <inline-formula><tex-math notation="LaTeX" id="ImEquation21">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{train}$\end{document}</tex-math></inline-formula> into five folds: D1, D2, D3, D4 and D5 utilizing a 5-fold cross-validation strategy. For the first training iteration, choose <inline-formula><tex-math notation="LaTeX" id="ImEquation22">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{train}$\end{document}</tex-math></inline-formula> - D1 (The remaining samples after removing D1 in <inline-formula><tex-math notation="LaTeX" id="ImEquation23">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{train}$\end{document}</tex-math></inline-formula>) as the current training set, utilizing ‘D1’ as the validation set to train the model and obtain <inline-formula><tex-math notation="LaTeX" id="ImEquation24">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$model_{1}$\end{document}</tex-math></inline-formula>.</p>
        <p>(ii) Subsequent to model1, for the second training iteration, employ <inline-formula><tex-math notation="LaTeX" id="ImEquation25">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{train}$\end{document}</tex-math></inline-formula> - D2 as the current training set and D2 as the validation set to continue training <inline-formula><tex-math notation="LaTeX" id="ImEquation26">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$model_{1}$\end{document}</tex-math></inline-formula>, obtaining <inline-formula><tex-math notation="LaTeX" id="ImEquation27">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$model_{2}$\end{document}</tex-math></inline-formula>. By analogy, repeat the training process five times to obtain the final model, denoted as <inline-formula><tex-math notation="LaTeX" id="ImEquation28">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$model_{5}$\end{document}</tex-math></inline-formula>.</p>
        <p>(iii) Evaluate the performance of the final model on the test set <inline-formula><tex-math notation="LaTeX" id="ImEquation29">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{test}$\end{document}</tex-math></inline-formula>.</p>
      </sec>
    </sec>
    <sec id="sec2i">
      <title>Evaluation metrics</title>
      <p>We utilize seven evaluation metrics to assess the performance of Enhancer-MDLF and compare it with that of other methods. These metrics encompass accuracy (ACC), balanced accuracy (BACC), area under the receiver operating characteristics (AUROC), Matthews correlation coefficient (MCC), sensitivity (Sn), specificity (Sp) and F1 score. The details of these evaluation metrics are provided in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>. In general, a higher value for these metrics indicates superior model performance.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>RESULTS</title>
    <sec id="sec3a">
      <title>Performance evaluation of combinatorial module in comparison with individual modules</title>
      <p>Recent studies have underscored the superiority of leveraging multiple features over individual ones in sequence-based prediction tasks [<xref rid="ref41" ref-type="bibr">41–44</xref>]. Consequently, Enhancer-MDLF, proposed in this study, incorporates a dna2vec module and a motif module to comprehensively extract information on enhancer sequences. To assess the effectiveness of this combinatorial module in improving the model’s performance for predicting cell-specific enhancers, we individually train the dna2vec module and the motif module utilizing training sets for each cell line. Subsequently, we evaluate the performance of these modules on independent test sets and draw comparisons with the performance of the combinatorial module.</p>
      <p>Given that the datasets used in this study are imbalanced, we utilize the BACC metric rather than ACC to evaluate the performance of models. What surprised us is that Enhancer-MDLF can effectively improve the performance of the model by fusing the two modules while maintaining a reasonable computational cost (refer to <xref rid="f3" ref-type="fig">Figure 3</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S2</xref> and <xref rid="sup1" ref-type="supplementary-material">S3</xref>). All six metrics exhibit improvement across the eight cell lines. Especially, the performance of two individual modules on the HMEC cell line is unsatisfactory, while the combinatorial module greatly improves the prediction performance. Furthermore, the training time for the combined module is shorter than that for the dna2vec module alone in the HSMM and NHEK cell lines (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>), potentially due to an accelerated convergence speed facilitated by our early stopping strategy. Overall, these results indicate that leveraging multiple features indeed improves prediction performance, establishing Enhancer-MDLF as a powerful and robust tool for predicting cell-specific enhancers.</p>
      <fig position="float" id="f3">
        <label>Figure 3</label>
        <caption>
          <p><bold>Performance evaluation of combinatorial module in comparison with individual modules.</bold> Panels (<bold>A–F</bold>) depict the evaluation of model performance across eight cell lines using metrics such as AUROC, BACC, F1 score, MCC, SN and SP.</p>
        </caption>
        <graphic xlink:href="bbae083f3" position="float"/>
      </fig>
    </sec>
    <sec id="sec3b">
      <title>Performance comparison with state-of-the-art method</title>
      <p>To comprehensively assess the predictive capabilities of Enhancer-MDLF for cell-specific enhancers, we conduct a thorough comparison with Enhancer-IF, a model dedicated to cell-specific enhancer prediction, across eight cell lines. As mentioned in the introduction section, existing enhancer prediction methods predominantly rely on generic datasets, neglecting the crucial aspect of enhancer’s cell specificity. To validate the robustness of our model, we conduct performance comparisons for both methods on independent test sets and through 10-fold cross-validation (refer to <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>).</p>
      <p>As can be seen in <xref rid="f4" ref-type="fig">Figure 4</xref> and detailed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref>, Enhancer-MDLF consistently outperforms Enhancer-IF across all five metrics and all cell lines, demonstrating its significant superiority. The robust predictive performance of Enhancer-MDLF could be attributed to its unique multi-module fusion approach and a novel training strategy that incorporates comprehensive and in-depth sequence information. It is crucial to highlight that, in contrast to the training strategy of Enhancer-IF, we design a unique training strategy for Enhancer-MDLF. This strategy involves learning more distributions iteratively within a limited dataset. Our approach and Enhancer-IF employ identical input data and generate results on the same test dataset, ensuring a fair and meaningful comparison within the training process from an end-to-end perspective.</p>
      <fig position="float" id="f4">
        <label>Figure 4</label>
        <caption>
          <p><bold>Performance comparison between Enhance-MDLF and Enhancer-IF on independent test sets.</bold> Panels (<bold>A–D</bold>) present the evaluation of model performance across eight cell lines using metrics such as AUROC, BACC, MCC and SN. Panels (<bold>E–H</bold>) present the evaluation of model performance across eight cell lines using metrics such as AUROC, BACC, MCC and SN.</p>
        </caption>
        <graphic xlink:href="bbae083f4" position="float"/>
      </fig>
      <p>Notably, a pronounced performance gap is observed between the 10-fold cross-validation and independent test sets for Enhancer-MDLF on the HMEC cell line. This discrepancy may arise from the distinct data distributions between the training and test sets, posing a challenge for the model to effectively generalize information learned from the training set to the test set. However, even under these challenging conditions, our method outperforms Enhancer-IF by 9.54% on the independent test set in terms of BACC.</p>
      <p>However, to the best of our knowledge, there is no specific pattern observed in Enhancer-IF for the HMEC dataset. This lack of observation could be attributed to Enhancer-IF consistently performing within the range of 70–80% across multiple cell lines, which may imply limited knowledge acquisition, possibly concealing specific characteristics within the HMEC dataset. It is precisely due to the robust predictive capabilities of Enhancer-MDLF that these discrepant results may indicate that the enhancers within the HMEC cell line have more complex gene regulatory mechanisms. This suggests an intriguing direction for further exploration to help uncover the reasons behind enhancer cell specificity. Overall, these results demonstrate the robustness and effectiveness of Enhancer-MDLF in accurately predicting cell-specific enhancers.</p>
    </sec>
    <sec id="sec3c">
      <title>Performance evaluation across cell lines</title>
      <p>Previous studies have established that enhancers exhibit cell-specific functionalities [<xref rid="ref27" ref-type="bibr">27</xref>, <xref rid="ref45" ref-type="bibr">45</xref>]. To explore the potential relationships among enhancers across different cell lines, we conduct a comprehensive cross-cell line performance evaluation to investigate the transferability of cell-specific models. Our approach involves training models on one cell line and evaluating the performance of Enhancer-MDLF and Enhancer-IF on the test sets of seven other cell lines. The results demonstrate that Enhancer-MDLF achieves optimal performance in terms of average BACC across all eight cell lines (<xref rid="f5" ref-type="fig">Figure 5A</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S6</xref> and <xref rid="sup1" ref-type="supplementary-material">S7</xref>). Notably, the HEK293 model achieves an average BACC of 86.18% when predicting outcomes in other cell lines, indicating its capacity to transfer to other cell lines. Despite the prevalence of cell-specific enhancers observed in most instances, such as those identified in NHLF cell lines, it is important to acknowledge the existence of enhancers that exhibit significant similarities across certain cell lines. For instance, Enhancer-MDLF demonstrates satisfactory mutual predictive performance on the HUVEC and K562 cell lines, as well as on the NHEK and HMEC cell lines (<xref rid="f5" ref-type="fig">Figure 5B</xref>). These results highlight a commonality of enhancers between some cell lines. Overall, enhancers generally display specificity among most cell lines, but there exist similarities between enhancers across specific cell lines. Therefore, investigating both cell-specific and non-cell-specific enhancers emerges as a critical avenue for unveiling insights into cell specificity and differentiation.</p>
      <fig position="float" id="f5">
        <label>Figure 5</label>
        <caption>
          <p><bold>Comprehensive performance evaluation across multiple aspects.</bold> (<bold>A</bold>) Average BACC for cross-cell line prediction on eight cell lines. (<bold>B</bold>) Heat map of BACC in cross-cell line evaluation. Columns represent pre-trained models trained on different training sets, and rows represent testing on their own or other cell lines’ test sets. (<bold>C</bold>) Exploration of Transfer Learning Strategies: Three transfer learning strategies are presented to address the challenge of poor direct prediction performance in cross-cell line evaluation. (<bold>D</bold>) Performance comparison of Enhancer-MDLF with other methods on a universal dataset. (<bold>E–H</bold>) Performance comparison of Enhancer-MDLF with other methods on enhancer–promoter datasets.</p>
        </caption>
        <graphic xlink:href="bbae083f5" position="float"/>
      </fig>
    </sec>
    <sec id="sec3d">
      <title>Performance evaluation across cell lines with transfer learning</title>
      <p>The challenge of predicting enhancers across different cell lines, attributed to the inherent cell specificity of enhancers, significantly hinders the exploration of gene regulatory mechanisms. To overcome this obstacle and advance the field, we further explore potential approaches. An emerging ML technique, transfer learning, proves promising as it leverages knowledge acquired from a source domain to enhance learning performance in a target domain. Notably, this technique has demonstrated success in predicting cell-specific enhancer–promoter interactions [<xref rid="ref46" ref-type="bibr">46</xref>, <xref rid="ref47" ref-type="bibr">47</xref>]. To comprehensively evaluate the potential of transfer learning in the context of enhancer prediction, we adopt three strategies: transfer_1, transfer_2 and transfer_3. These strategies align with both traditional transfer learning methods and those previously utilized in a relevant study [<xref rid="ref48" ref-type="bibr">48</xref>]. Detailed descriptions of these strategies are provided in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>, aiming to shed light on their effectiveness in mitigating the challenges posed by the cell specificity of enhancers.</p>
      <p>We apply these three distinct transfer strategies separately to Enhancer-MDLF to validate their effectiveness and explore their practical applicability (<xref rid="f5" ref-type="fig">Figure 5C</xref>). The results, as detailed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S8</xref>, <xref rid="sup1" ref-type="supplementary-material">S9</xref> and <xref rid="sup1" ref-type="supplementary-material">S10</xref> across eight cell lines, illustrate significant insights. Enhancer-MDLF with transfer_1 demonstrates notable improvement in prediction performance, albeit falling short compared with predicting enhancers in the same cell line. However, this outcome underscores the effectiveness of transfer learning. In contrast, Enhancer-MDLF with transfer_2, leveraging additional enhancer information from multiple cell lines during the model pre-training, exhibits exceptional performance. Compared with the model directly trained with random initial weights (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S11</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2</xref>), the results indicate that pre-training the model with enhancers from diverse cell lines yields more effective predictions in new cell lines. This is attributed to the more representative initial weights acquired from the pre-trained model, highlighting its potential for accurately labeling other unannotated data, especially under the constraint of limited annotated data size. Particularly noteworthy is the observation that Enhancer-MDLF with transfer_3 exhibits optimal performance across the eight cell lines.</p>
      <p>These findings present a practical scenario for addressing enhancer-specific prediction challenges. If Enhancer-MDLF is pre-trained using as many (or even all) cell line enhancers following our transfer learning strategies, a comprehensive pre-trained model may be obtained, serving as an effective initial model for enhancer prediction tasks in various cell lines. Even with potential cost constraints in obtaining enhancers from numerous cell lines, Enhancer-MDLF with transfer_2 demonstrates robust performance with only a few cell lines’ enhancers. Overall, the utilization of transfer learning provides an effective and promising solution to overcome prediction challenges arising from enhancer specificity in practical applications. We have provided the pre-trained model, utilizing enhancers from these eight cell lines, available on GitHub for further exploration and utilization.</p>
    </sec>
    <sec id="sec3e">
      <title>Performance evaluation on other datasets</title>
      <p>To demonstrate the superiority of our approach comprehensively, we subject Enhancer-MDLF to evaluation using the generic dataset created by Liu <italic toggle="yes">et al</italic>. [<xref rid="ref21" ref-type="bibr">21</xref>], a widely used benchmark in enhancer identification tasks. To maintain comparability with prior studies, we utilize the same training set, test set and evaluation metrics. The outcomes reveal that Enhancer-MDLF achieves optimal performance on the independent test set, with MCC, SN and ACC of 0.6067, 0.84 and 0.8025, respectively (<xref rid="f5" ref-type="fig">Figure 5D</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S12</xref>). These results highlight the robustness and generality of Enhancer-MDLF, demonstrating its efficacy and versatility as a powerful tool for predicting enhancers in human cell lines.</p>
      <p>Besides, existing studies have demonstrated that enhancers and promoters exhibit similar sequence structures [<xref rid="ref49" ref-type="bibr">49</xref>, <xref rid="ref50" ref-type="bibr">50</xref>]. Leveraging datasets derived from iPro-WAEL [<xref rid="ref33" ref-type="bibr">33</xref>], we conduct an investigation to assess whether Enhancer-MDLF effectively distinguishes between enhancers and promoters. Given the limitations of some enhancer prediction methods for this dataset, as detailed in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>, we restrict our comparison to Enhancer-MDLF, ienhancer-2L and enhancer-IF. The results unequivocally showcase Enhancer-MDLF’s robust capacity to discriminate between promoters and enhancers, effectively capturing the distinct information associated with these two regulatory elements despite their similar sequence structures (<xref rid="f5" ref-type="fig">Figure 5E–H</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S14</xref>).</p>
    </sec>
    <sec id="sec3f">
      <title>Exploration of TFBS motifs in enhancer regions</title>
      <p>Investigating TFBS motifs within enhancer regions is crucial for understanding the regulatory mechanisms of TFs. TFs typically exert their transcriptional influence by recognizing specific TFBS motifs and binding to the regulatory regions of genes. Enhancers, along with the associated TFs, play a significant role in human diseases and biological processes [<xref rid="ref35" ref-type="bibr">35</xref>, <xref rid="ref51" ref-type="bibr">51</xref>, <xref rid="ref52" ref-type="bibr">52</xref>]. To identify key motifs associated with enhancers, we utilize the Shapley Additive exPlanations (SHAP) framework [<xref rid="ref53" ref-type="bibr">53</xref>, <xref rid="ref54" ref-type="bibr">54</xref>] to interpret the input features of the motif module in Enhancer-MDLF. The details of the SHAP framework are provided in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>.</p>
      <p><xref rid="f6" ref-type="fig">Figure 6</xref> displays the top 50 motifs with the most significant impact on the model’s output across eight cell lines. The x-axis represents the SHAP value, where a positive value indicates a positive effect on the model’s output, while a negative value indicates a negative effect. Taking SP2 in <xref rid="f6" ref-type="fig">Figure 6A</xref> as an example, higher feature values predominantly cluster in the region where SHAP values &gt;0, indicating that SP2 has a positive effect on predicting the samples as enhancers. It can be seen from <xref rid="f6" ref-type="fig">Figure 6</xref> that the contributions of different motifs to the model’s output vary across different cell lines, providing further insight into the cell specificity of enhancers at the motif level.</p>
      <fig position="float" id="f6">
        <label>Figure 6</label>
        <caption>
          <p><bold>Analysis of top 50 important motif features across eight cell lines.</bold> (<bold>A–H</bold>) This study analyzes the top 50 motifs on eight cell lines that impact the model’s output utilizing the SHAP framework. The x-axis represents the SHAP values that impact the model’s output. Each point on the graph represents a sample, and the color of each sample point ranges from blue to red, representing the corresponding feature values of each sample. The color transition reflects the variation in feature values from low (blue) to high (red).</p>
        </caption>
        <graphic xlink:href="bbae083f6" position="float"/>
      </fig>
      <p>Besides, we find that certain important motifs identified in our study align well with findings from previous studies. For instance, the BACH1 motif emerges exclusively among the top 50 important features in the HUVEC cell line. Previous studies have shown that the heme-binding factor BACH1 can bind to multiple Maf recognition elements in the heme oxygenase 1 (HO-1) enhancer, thereby inhibiting its activity and participating in gene regulation [<xref rid="ref55" ref-type="bibr">55</xref>]. Similarly, the GATA1 motif is uniquely present among the top 50 important features solely in the K562 cell line. Previous studies have revealed that the GATA1 protein binds to the GATA-A site in the intronic WT1 enhancer <italic toggle="yes">in vitro</italic> in K562 cells, transactivating the enhancer and markedly increasing the CAT reporter activity 10–15-fold [<xref rid="ref56" ref-type="bibr">56</xref>].</p>
      <p>To visualize the impact of these features on the model’s output, force plots of SHAP are employed, and specific details are available in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1</xref>. Additionally, to enhance the user’s interactive experience, we have provided HTML files on GitHub (<ext-link xlink:href="https://github.com/HaoWuLab-Bioinformatics/Enhancer-MDLF" ext-link-type="uri">https://github.com/HaoWuLab-Bioinformatics/Enhancer-MDLF</ext-link>). These files showcase the contributions of each feature to the model’s output on the test sets of eight cell lines. Users can interactively select various features and samples on the graph, facilitating a dynamic exploration of their influence on the model’s output.</p>
      <p>Furthermore, the SP1 motif significantly contributes to enhancer predictions across numerous cell lines, as it has been shown to regulate chromatin loops between enhancers and distal promoters, thereby influencing transcriptional activity [<xref rid="ref57" ref-type="bibr">57</xref>]. Additionally, our analysis reveals certain key motifs, such as RUNX and MAX, which have not been extensively studied but are highlighted in TargetFinder [<xref rid="ref58" ref-type="bibr">58</xref>], These results underscore that enhancers can exert their effects through the involvement of certain proteins in specific cells, emphasizing the complexity of gene regulation involving enhancers.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <title>DISCUSSION AND CONCLUSION</title>
    <p>The identification of cell-specific enhancers is of significant importance for understanding cell-specific gene regulation and deciphering tissue development. In this study, we develop Enhancer-MDLF, a deep learning framework with multi-inputs, designed for accurate identification of cell-specific enhancers by integrating the dna2vec module and motif module. Experimental analyses demonstrate that Enhancer-MDLF outperforms existing methods across multiple cell-specific enhancer datasets, and significantly outperforms previous studies on general datasets and enhancer–promoter datasets, highlighting the robustness and versatility of Enhancer-MDLF. While our evaluations are constrained by the scale of the annotated dataset to specific cell lines, various experimental results instill confidence in the generalization ability of our model to extend to broader datasets. In summary, our proposed Enhancer-MDLF emerges as a superior and efficient tool for identifying enhancers.</p>
    <p>Additionally, in the course of cross-cell-line evaluation, we observe a notable challenge wherein models trained on one cell line often exhibit unsatisfactory performance when applied to predict enhancers on other cell lines. This limitation stems from the inherent cell specificity of enhancers, suggesting that a model trained on a specific cell line achieves optimal prediction performance solely within that context, posing a substantial limitation in practical applications. To overcome this limitation, we endeavor to mitigate it through the implementation of transfer learning. The pre-trained model with transfer_2 achieves remarkably precise predictions with only a modest amount of annotated data (e.g. 292 samples, constituting 10% of the training set, in the NHLF cell line). In practical scenarios, the scarcity of annotated data in private datasets may face challenges for the effective application of supervised learning. While unsupervised learning methods, such as clustering, can partially emulate similar functionality, their accuracy often falls significantly below the standards required for practical applications. Consequently, we assert that this approach substantiates an effective strategy for addressing the challenges posed by the cell-specific nature of enhancers. Among the three transfer learning strategies employed, transfer_3, a pre-trained model trained on all cell lines, demonstrates the most promising performance. This outcome suggests that with a sufficiently extensive dataset encompassing a diverse range of cell lines as input to our model, the development of a universal pre-trained model for identifying human enhancers becomes a plausible avenue of exploration—a prospect that holds significant interest and potential in research endeavors.</p>
    <p>Moreover, we employ a computational approach to analyze potentially important TFBS motifs associated with enhancers. Our analysis reveals several motifs that have undergone extensive study in previous research, such as BACH1, GATA1, SP1, RUNX and MAX. Additionally, we identify motifs like FOXJ3 and SP2, which significantly contribute to enhancer predictions across numerous cell lines but have received limited attention in prior studies. We infer that these less-explored motifs may play roles in gene regulation through intricate and as-yet-unidentified processes. While our current research conditions constrain further validation and exploration of the relationships between these new motifs and enhancer function, we believe these findings open new avenues for future enhancer-related research in the field of biology. Notably, Whalen <italic toggle="yes">et al</italic>. [<xref rid="ref58" ref-type="bibr">58</xref>] extensively explored the critical role of YY1 in enhancer–promoter interactions by interpreting ML models. Later on, it was further confirmed that YY1 regulates enhancer–promoter chromatin loops [<xref rid="ref59" ref-type="bibr">59</xref>].</p>
    <boxed-text id="box01" position="float">
      <sec id="sec28a">
        <title>Key Points</title>
        <list list-type="bullet">
          <list-item>
            <p>We propose a novel deep learning framework, called Enhancer-MDLF, employing multi-module inputs to identify the cell-specific enhancers.</p>
          </list-item>
          <list-item>
            <p>We confirm that Enhancer-MDLF substantially outperforms state-of-the-art predictors without the need for parameter tuning.</p>
          </list-item>
          <list-item>
            <p>We incorporate three transfer learning strategies into our model to address the challenges posed by enhancer specificity for cross-cell lineage prediction.</p>
          </list-item>
          <list-item>
            <p>We analyze the conservation and specificity of enhancers at the motif level, exploring the most important TFBS motifs within enhancer regions utilizing the SHAP framework.</p>
          </list-item>
        </list>
      </sec>
    </boxed-text>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>Supplementary_Information_bbae083</label>
      <media xlink:href="supplementary_information_bbae083.docx"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>ACKNOWLEDGMENTS</title>
    <p>We thank members of the group for their valuable discussions and comments. The scientific calculations in this study have been done on the HPC Cloud Platform of Shandong University.</p>
  </ack>
  <sec id="sec50a">
    <title>FUNDING</title>
    <p>This work is supported by the National Natural Science Foundation of China (Grant No. 62272278 and 61972322), the National Key Research and Development Program (Grant No. 2021YFF0704103) and the Fundamental Research Funds of Shandong University. The funders did not play any role in the design of the study, the collection, analysis and interpretation of data or the writing of the manuscript.</p>
  </sec>
  <sec id="sec50b">
    <title>AUTHOR CONTRIBUTIONS STATEMENT</title>
    <p>H.W., Y.Z. and P.Z. conceived the experiments. Y.Z. and P.Z. conducted and analyzed the experiments. Y.Z. and P.Z. wrote the manuscript. H.W. reviewed the manuscript.</p>
  </sec>
  <notes id="bio1">
    <title>Author Biographies</title>
    <p><bold>Yao Zhang</bold> is currently a graduate student at the School of Software, Shandong University. Her research interests focus on bioinformatics and artificial intelligence.</p>
    <sec sec-type="author-bio" id="sec23b">
      <p><bold>Pengyu Zhang</bold> is currently pursuing a PhD degree at Xi'an Jiaotong University, China, with his research interests focused on computational biology, genomics, bioinformatics and deep learning.</p>
    </sec>
    <sec sec-type="author-bio" id="sec23c">
      <p><bold>Hao Wu</bold> is currently an associate professor at the School of Software, Shandong University. His research interests include artificial intelligence, data mining, and biomedical big data mining.</p>
    </sec>
  </notes>
  <ref-list id="bib1">
    <title>References</title>
    <ref id="ref1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pennacchio</surname><given-names>LA</given-names></string-name>, <string-name><surname>Bickmore</surname><given-names>W</given-names></string-name>, <string-name><surname>Dean</surname><given-names>A</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Enhancers: five essential questions</article-title>. <source>Nat Rev Genet</source>  <year>2013</year>;<volume>14</volume>(<issue>4</issue>):<fpage>288</fpage>–<lpage>95</lpage>.<pub-id pub-id-type="pmid">23503198</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Omar</surname><given-names>N</given-names></string-name>, <string-name><surname>Wong</surname><given-names>YS</given-names></string-name>, <string-name><surname>Li</surname><given-names>X</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Enhancer prediction in proboscis monkey genome:a comparative study</article-title>. <source>J Telecommun Electron Comput Eng</source>  <year>2017</year>;<volume>9</volume>(<issue>2–9</issue>):<fpage>175</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="ref3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ong</surname><given-names>C</given-names></string-name>, <string-name><surname>Corces</surname><given-names>vG.</given-names></string-name></person-group>  <article-title>Enhancer function: new insights into the regulation of tissue-specific gene expression</article-title>. <source>Nat Rev Genet</source>  <year>2011</year>;<volume>12</volume>(<issue>4</issue>):<fpage>283</fpage>–<lpage>93</lpage>.<pub-id pub-id-type="pmid">21358745</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>X</given-names></string-name>, <string-name><surname>Si</surname><given-names>J</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Y</given-names></string-name>, <string-name><surname>DeWille</surname><given-names>JW</given-names></string-name></person-group>. <article-title>Ccaat/enhancer binding protein-delta (c/ebp-delta) regulates cell growth, migration and differentiation</article-title>. <source>Cancer Cell Int</source>  <year>2010</year>;<volume>10</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>11</lpage>.<pub-id pub-id-type="pmid">20142996</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herz</surname><given-names>H</given-names></string-name></person-group>. <article-title>Enhancer deregulation in cancer and other diseases</article-title>. <source>Bioessays</source>  <year>2016</year>;<volume>38</volume>(<issue>10</issue>):<fpage>1003</fpage>–<lpage>15</lpage>.<pub-id pub-id-type="pmid">27570183</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Corradin</surname><given-names>O</given-names></string-name>, <string-name><surname>Scacheri</surname><given-names>PC</given-names></string-name></person-group>. <article-title>Enhancer variants: evaluating functions in common disease</article-title>. <source>Genome Med</source>  <year>2014</year>;<volume>6</volume>(<issue>10</issue>):<fpage>1</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">24433494</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moore</surname><given-names>JE</given-names></string-name>, <string-name><surname>Purcaro</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Pratt</surname><given-names>HE</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Expanded encyclopaedias of DNA elements in the human and mouse genomes</article-title>. <source>Nature</source>  <year>2020</year>;<volume>583</volume>(<issue>7818</issue>):<fpage>699</fpage>–<lpage>710</lpage>.<pub-id pub-id-type="pmid">32728249</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Finucane</surname><given-names>HK</given-names></string-name>, <string-name><surname>Bulik-Sullivan</surname><given-names>B</given-names></string-name>, <string-name><surname>Gusev</surname><given-names>A</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Partitioning heritability by functional annotation using genome-wide association summary statistics</article-title>. <source>Nat Genet</source>  <year>2015</year>;<volume>47</volume>(<issue>11</issue>):<fpage>1228</fpage>–<lpage>35</lpage>.<pub-id pub-id-type="pmid">26414678</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koido</surname><given-names>M</given-names></string-name>, <string-name><surname>Hon</surname><given-names>CC</given-names></string-name>, <string-name><surname>Koyama</surname><given-names>S</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Prediction of the cell-type-specific transcription of non-coding RNAs from genome sequences via machine learning</article-title>. <source>Nat Biomed Eng</source>  <year>2023</year>;<volume>7</volume>(<issue>6</issue>):<fpage>830</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">36411359</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Woolfe</surname><given-names>A</given-names></string-name>, <string-name><surname>Goodson</surname><given-names>M</given-names></string-name>, <string-name><surname>Goode</surname><given-names>DK</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Highly conserved non-coding sequences are associated with vertebrate development</article-title>. <source>PLoS Biol</source>  <year>2005</year>;<volume>3</volume>(<issue>1</issue>):e7.</mixed-citation>
    </ref>
    <ref id="ref11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pennacchio</surname><given-names>LA</given-names></string-name>, <string-name><surname>Ahituv</surname><given-names>N</given-names></string-name>, <string-name><surname>Moses</surname><given-names>AM</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>In vivo enhancer analysis of human conserved non-coding sequences</article-title>. <source>Nature</source>  <year>2006</year>;<volume>444</volume>(<issue>7118</issue>):<fpage>499</fpage>–<lpage>502</lpage>.<pub-id pub-id-type="pmid">17086198</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>X</given-names></string-name>, <string-name><surname>Xu</surname><given-names>H</given-names></string-name>, <string-name><surname>Yuan</surname><given-names>P</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Integration of external signaling pathways with the Core transcriptional network in embryonic stem cells</article-title>. <source>Cell</source>  <year>2008</year>;<volume>133</volume>(<issue>6</issue>):<fpage>1106</fpage>–<lpage>17</lpage>.<pub-id pub-id-type="pmid">18555785</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Visel</surname><given-names>A</given-names></string-name>, <string-name><surname>Blow</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Li</surname><given-names>Z</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Chip-seq accurately predicts tissue-specific activity of enhancers</article-title>. <source>Nature</source>  <year>2009</year>;<volume>457</volume>(<issue>7231</issue>):<fpage>854</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">19212405</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dorschner</surname><given-names>MO</given-names></string-name>, <string-name><surname>Hawrylycz</surname><given-names>M</given-names></string-name>, <string-name><surname>Humbert</surname><given-names>R</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>High-throughput localization of functional elements by quantitative chromatin profiling</article-title>. <source>Nat Methods</source>  <year>2004</year>;<volume>1</volume>(<issue>3</issue>):<fpage>219</fpage>–<lpage>25</lpage>.<pub-id pub-id-type="pmid">15782197</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giresi</surname><given-names>PG</given-names></string-name>, <string-name><surname>Kim</surname><given-names>J</given-names></string-name>, <string-name><surname>McDaniell</surname><given-names>RM</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>FAIRE (formaldehyde-assisted isolation of regulatory elements) isolates active regulatory elements from human chromatin</article-title>. <source>Genome Res</source>  <year>2007</year>;<volume>17</volume>(<issue>6</issue>):<fpage>877</fpage>–<lpage>85</lpage>.<pub-id pub-id-type="pmid">17179217</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buenrostro</surname><given-names>JD</given-names></string-name>, <string-name><surname>Giresi</surname><given-names>PG</given-names></string-name>, <string-name><surname>Zaba</surname><given-names>LC</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Transposition of native chromatin for fast and sensitive epigenomic profiling of open chromatin, DNA-binding proteins and nucleosome position</article-title>. <source>Nat Methods</source>  <year>2013</year>;<volume>10</volume>(<issue>12</issue>):<fpage>1213</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">24097267</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heintzman</surname><given-names>ND</given-names></string-name>, <string-name><surname>Stuart</surname><given-names>RK</given-names></string-name>, <string-name><surname>Hon</surname><given-names>G</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Distinct and predictive chromatin signatures of transcriptional promoters and enhancers in the human genome</article-title>. <source>Nat Genet</source>  <year>2007</year>;<volume>39</volume>(<issue>3</issue>):<fpage>311</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">17277777</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andersson</surname><given-names>R</given-names></string-name>, <string-name><surname>Gebhard</surname><given-names>C</given-names></string-name>, <string-name><surname>Miguel-Escalada</surname><given-names>I</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>An atlas of active enhancers across human cell types and tissues</article-title>. <source>Nature</source>  <year>2014</year>;<volume>507</volume>(<issue>7493</issue>):<fpage>455</fpage>–<lpage>61</lpage>.<pub-id pub-id-type="pmid">24670763</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B</given-names></string-name>, <string-name><surname>Fang</surname><given-names>L</given-names></string-name>, <string-name><surname>Long</surname><given-names>R</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ienhancer-2l: a two-layer predictor for identifying enhancers and their strength by pseudo k-tuple nucleotide composition</article-title>. <source>Bioinformatics</source>  <year>2016</year>;<volume>32</volume>(<issue>3</issue>):<fpage>362</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">26476782</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jia</surname><given-names>C</given-names></string-name>, <string-name><surname>He</surname><given-names>W</given-names></string-name></person-group>. <article-title>Enhancerpred: a predictor for discovering enhancers based on the combination and selection of multiple features</article-title>. <source>Sci Rep</source>  <year>2016</year>;<volume>6</volume>(<issue>1</issue>):<fpage>38741</fpage>.<pub-id pub-id-type="pmid">27941893</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B</given-names></string-name>, <string-name><surname>Li</surname><given-names>K</given-names></string-name>, <string-name><surname>Huang</surname><given-names>D</given-names></string-name>, <string-name><surname>Chou</surname><given-names>KC</given-names></string-name></person-group>. <article-title>Ienhancer-el: identifying enhancers and their strength with ensemble learning approach</article-title>. <source>Bioinformatics</source>  <year>2018</year>;<volume>34</volume>(<issue>22</issue>):<fpage>3835</fpage>–<lpage>42</lpage>.<pub-id pub-id-type="pmid">29878118</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nguyen</surname><given-names>QH</given-names></string-name>, <string-name><surname>Nguyen-Vo</surname><given-names>T</given-names></string-name>, <string-name><surname>Le</surname><given-names>NQK</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ienhancer-ecnn: identifying enhancers and their strength using ensembles of convolutional neural networks</article-title>. <source>BMC Genomics</source>  <year>2019</year>;<volume>20</volume>:<fpage>1</fpage>–<lpage>10</lpage>.<pub-id pub-id-type="pmid">30606130</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Le</surname><given-names>NQK</given-names></string-name>, <string-name><surname>Ho</surname><given-names>Q</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>T</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Transformer architecture based on bert and 2d convolutional neural network to identify dna enhancers from sequence information</article-title>. <source>Brief Bioinform</source>  <year>2021</year>;<volume>22</volume>.</mixed-citation>
    </ref>
    <ref id="ref24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niu</surname><given-names>K</given-names></string-name>, <string-name><surname>Luo</surname><given-names>X</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>S</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ienhancer-eblstm: identifying enhancers and strengths by ensembles of bidirectional long short-term memory</article-title>. <source>Front Genet</source>  <year>2021</year>;<volume>12</volume>:665498.</mixed-citation>
    </ref>
    <ref id="ref25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cai</surname><given-names>L</given-names></string-name>, <string-name><surname>Ren</surname><given-names>X</given-names></string-name>, <string-name><surname>Fu</surname><given-names>X</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ienhancer-xg: interpretable sequence-based enhancers and their strength predictor</article-title>. <source>Bioinformatics</source>  <year>2021</year>;<volume>37</volume>(<issue>8</issue>):<fpage>1060</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">33119044</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bai</surname><given-names>X</given-names></string-name>, <string-name><surname>Shi</surname><given-names>S</given-names></string-name>, <string-name><surname>Ai</surname><given-names>B</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Endb: a manually curated database of experimentally supported enhancers for human and mouse</article-title>. <source>Nucleic Acids Res</source>  <year>2020</year>;<volume>48</volume>(<issue>D1</issue>):<fpage>D51</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">31665430</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heinz</surname><given-names>S</given-names></string-name>, <string-name><surname>Romanoski</surname><given-names>CE</given-names></string-name>, <string-name><surname>Benner</surname><given-names>C</given-names></string-name>, <string-name><surname>Glass</surname><given-names>CK</given-names></string-name></person-group>. <article-title>The selection and function of cell type-specific enhancers</article-title>. <source>Nat Rev Mol Cell Biol</source>  <year>2015</year>;<volume>16</volume>(<issue>3</issue>):<fpage>144</fpage>–<lpage>54</lpage>.<pub-id pub-id-type="pmid">25650801</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Basith</surname><given-names>S</given-names></string-name>, <string-name><surname>Hasan</surname><given-names>MM</given-names></string-name>, <string-name><surname>Lee</surname><given-names>G</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Manavalan, integrative machine learning framework for the identification of cell-specific enhancers from the human genomes</article-title>. <source>Brief Bioinform</source>  <year>2021</year>;<volume>22</volume>.</mixed-citation>
    </ref>
    <ref id="ref29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname><given-names>T</given-names></string-name>, <string-name><surname>Qian</surname><given-names>J</given-names></string-name></person-group>. <article-title>Enhanceratlas 2.0: an updated resource with enhancer annotation in 586 tissue/cell types across nine species</article-title>. <source>Nucleic Acids Res</source>  <year>2020</year>;<volume>48</volume>(<issue>D1</issue>):<fpage>D58</fpage>–<lpage>64</lpage>.<pub-id pub-id-type="pmid">31740966</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>P</given-names></string-name>, <string-name><surname>Boisson</surname><given-names>B</given-names></string-name>, <string-name><surname>Stenson</surname><given-names>PD</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Seqtailor: a user-friendly webserver for the extraction of dna or protein sequences from next- generation sequencing data</article-title>. <source>Nucleic Acids Res</source>  <year>2019</year>;<volume>47</volume>(<issue>W1</issue>):<fpage>W623</fpage>–<lpage>31</lpage>.<pub-id pub-id-type="pmid">31045209</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dao</surname><given-names>F</given-names></string-name>, <string-name><surname>Lv</surname><given-names>H</given-names></string-name>, <string-name><surname>Su</surname><given-names>W</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Idhs-deep: an integrated tool for predicting dnase i hypersensitive sites by deep neural network</article-title>. <source>Brief Bioinform</source>  <year>2021</year>;<volume>22</volume>.</mixed-citation>
    </ref>
    <ref id="ref32">
      <label>32.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Xhafa</surname><given-names>F</given-names></string-name></person-group>. <article-title>Lecture Notes on Data Engineering and Communications Technologies</article-title>. Cham, Germany: Springer, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>P</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>H</given-names></string-name>, <string-name><surname>Wu</surname><given-names>H</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ipro-wael: a comprehensive and robust framework for identifying promoters in multiple species</article-title>. <source>Nucleic Acids Res</source>  <year>2022</year>;<volume>50</volume>(<issue>18</issue>):<fpage>10278</fpage>–<lpage>89</lpage>.<pub-id pub-id-type="pmid">36161334</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref34">
      <label>34.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ng</surname><given-names>P</given-names></string-name></person-group>. <article-title>dna2vec: consistent vector representations of variable-length k-mers</article-title>. <comment>arXiv preprint arXiv 2017;1701.06279</comment>.</mixed-citation>
    </ref>
    <ref id="ref35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Latchman</surname><given-names>DS</given-names></string-name></person-group>. <article-title>Transcription factors: an overview</article-title>. <source>Int J Biochem Cell Biol</source>  <year>1997</year>;<volume>29</volume>(<issue>12</issue>):<fpage>1305</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">9570129</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kulakovskiy</surname><given-names>IV</given-names></string-name>, <string-name><surname>Vorontsov</surname><given-names>IE</given-names></string-name>, <string-name><surname>Yevshin</surname><given-names>IS</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Hocomoco: towards a complete collection of transcription factor binding models for human and mouse via large-scale chip-seq analysis</article-title>. <source>Nucleic Acids Res</source>  <year>2018</year>;<volume>46</volume>(<issue>D1</issue>):<fpage>D252</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">29140464</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref37">
      <label>37.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>T</given-names></string-name>, <string-name><surname>Goyal</surname><given-names>P</given-names></string-name>, <string-name><surname>Girshick</surname><given-names>R</given-names></string-name>, <etal>et al</etal></person-group>  <article-title>Focal loss for dense object detection</article-title>. <italic toggle="yes">in:</italic><source>Proceedings of the IEEE international conference on computer vision</source>. Venice, Italy: IEEE, <year>2017</year>;<fpage>2980</fpage>–<lpage>2988</lpage>.</mixed-citation>
    </ref>
    <ref id="ref38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cai</surname><given-names>J</given-names></string-name>, <string-name><surname>Wang</surname><given-names>S</given-names></string-name>, <string-name><surname>Xu</surname><given-names>C</given-names></string-name>, <string-name><surname>Guo</surname><given-names>W</given-names></string-name></person-group>. <article-title>Unsupervised deep clustering via contractive feature representation and focal loss</article-title>. <source>Pattern Recognit</source>  <year>2022</year>;<volume>123</volume>:<fpage>108386</fpage>.</mixed-citation>
    </ref>
    <ref id="ref39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tran</surname><given-names>GS</given-names></string-name>, <string-name><surname>Nghiem</surname><given-names>TP</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>VT</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Improving accuracy of lung nodule classification using deep learning with focal loss</article-title>. <source>J Healthcare Eng</source>  <year>2019</year>;<volume>2019</volume>:<fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="ref40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pang</surname><given-names>B</given-names></string-name>, <string-name><surname>Nijkamp</surname><given-names>E</given-names></string-name>, <string-name><surname>Wu</surname><given-names>YN</given-names></string-name></person-group>. <article-title>Deep learning with tensorflow: a review[J]</article-title>. <source>JEduc Behav Stat</source>  <year>2020</year>;<volume>45</volume>(<issue>2</issue>):<fpage>227</fpage>–<lpage>48</lpage>.</mixed-citation>
    </ref>
    <ref id="ref41">
      <label>41.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>X</given-names></string-name>, <string-name><surname>Shi</surname><given-names>Z</given-names></string-name>, <string-name><surname>Wu</surname><given-names>Y</given-names></string-name>, <etal>et al</etal></person-group>  <article-title>schicsc: A novel single-cell hi-c clustering framework by contact-weight-based smoothing and feature fusion</article-title>. In: <source>2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM).</source> Las Vegas, NV: IEEE, <year>2022</year>;<fpage>44</fpage>–<lpage>50</lpage>.</mixed-citation>
    </ref>
    <ref id="ref42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname><given-names>L</given-names></string-name>, <string-name><surname>Yuan</surname><given-names>R</given-names></string-name>, <string-name><surname>Han</surname><given-names>C</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Cellenboost: a boosting-based ligand-receptor interaction identification model for cell-to-cell communication inference</article-title>. <source>IEEE Trans Nanobioscience</source>  <year>2023</year>;<volume>22</volume>:<fpage>705</fpage>–<lpage>15</lpage>.<pub-id pub-id-type="pmid">37216267</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>P</given-names></string-name>, <string-name><surname>Wu</surname><given-names>H</given-names></string-name></person-group>. <article-title>Ichrom-deep: an attention-based deep learning model for identifying chromatin interactions</article-title>. <source>IEEE J Biomed Health Inform</source>  <year>2023</year>;<volume>27</volume>:<fpage>4559</fpage>–<lpage>68</lpage>.<pub-id pub-id-type="pmid">37402191</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>H</given-names></string-name>, <string-name><surname>Li</surname><given-names>D</given-names></string-name>, <string-name><surname>Wu</surname><given-names>H</given-names></string-name></person-group>. <article-title>Lnclocator-imb: an imbalance-tolerant ensemble deep learning framework for predicting Long non-coding RNA subcellular localization[J]</article-title>. <source>IEEE J Biomed Health Inform</source>  <year>2023</year>;<volume>28</volume>(<issue>1</issue>):<fpage>538</fpage>–<lpage>47</lpage>.</mixed-citation>
    </ref>
    <ref id="ref45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ong</surname><given-names>C</given-names></string-name>, <string-name><surname>Corces</surname><given-names>VG</given-names></string-name></person-group>. <article-title>Enhancers: emerging roles in cell fate specification</article-title>. <source>EMBO Rep</source>  <year>2012</year>;<volume>13</volume>(<issue>5</issue>):<fpage>423</fpage>–<lpage>30</lpage>.<pub-id pub-id-type="pmid">22491032</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weiss</surname><given-names>K</given-names></string-name>, <string-name><surname>Khoshgoftaar</surname><given-names>TM</given-names></string-name>, <string-name><surname>Wang</surname><given-names>D</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>A survey of transfer learning</article-title>. <source>J Big Data</source>  <year>2016</year>;<volume>3</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="ref47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hu</surname><given-names>J</given-names></string-name>, <string-name><surname>Zhong</surname><given-names>Y</given-names></string-name>, <string-name><surname>Shang</surname><given-names>X</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>A versatile and scalable single-cell data integration algorithm based on domain-adversarial and variational approximation</article-title>. <source>Brief Bioinform</source>  <year>2022</year>;<volume>23</volume>.</mixed-citation>
    </ref>
    <ref id="ref48">
      <label>48.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhuang</surname><given-names>Z</given-names></string-name>, <string-name><surname>Shen</surname><given-names>X</given-names></string-name>, <string-name><surname>Pan</surname><given-names>W</given-names></string-name></person-group>. <article-title>A simple convolutional neural network for prediction of enhancer–promoter interactions with dna sequence data</article-title>. <source>Bioinformatics</source>  <year>2019</year>;<volume>35</volume>(<issue>17</issue>):<fpage>2899</fpage>–<lpage>906</lpage>.<pub-id pub-id-type="pmid">30649185</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koch</surname><given-names>F</given-names></string-name>, <string-name><surname>Fenouil</surname><given-names>R</given-names></string-name>, <string-name><surname>Gut</surname><given-names>M</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Transcription initiation platforms and gtf recruitment at tissue-specific enhancers and promoters</article-title>. <source>Nat Struct Mol Biol</source>  <year>2011</year>;<volume>18</volume>(<issue>8</issue>):<fpage>956</fpage>–<lpage>63</lpage>.<pub-id pub-id-type="pmid">21765417</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Y</given-names></string-name>, <string-name><surname>Pai</surname><given-names>AA</given-names></string-name>, <string-name><surname>Herudek</surname><given-names>J</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Principles for rna metabolism and alternative transcription initiation within closely spaced promoters</article-title>. <source>Nat Genet</source>  <year>2016</year>;<volume>48</volume>(<issue>9</issue>):<fpage>984</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">27455346</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname><given-names>M</given-names></string-name>, <string-name><surname>Bai</surname><given-names>X</given-names></string-name>, <string-name><surname>Ai</surname><given-names>B</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Tf-marker: a comprehensive manually curated database for transcription factors and related markers in specific cell and tissue types in human</article-title>. <source>Nucleic Acids Res</source>  <year>2022</year>;<volume>50</volume>(<issue>D1</issue>):<fpage>D402</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">34986601</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref52">
      <label>52.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname><given-names>T</given-names></string-name>, <string-name><surname>He</surname><given-names>B</given-names></string-name>, <string-name><surname>Liu</surname><given-names>S</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Enhanceratlas: a resource for enhancer annotation and analysis in 105 human cell/tissue types</article-title>. <source>Bioinformatics</source>  <year>2016</year>;<volume>32</volume>(<issue>23</issue>):<fpage>3543</fpage>–<lpage>51</lpage>.<pub-id pub-id-type="pmid">27515742</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref53">
      <label>53.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lundberg</surname><given-names>SM</given-names></string-name>, <string-name><surname>Lee</surname><given-names>S</given-names></string-name></person-group>. <article-title>A unified approach to interpreting model predictions</article-title>. <source>Advances in neural information processing systems</source>. Long Beach, CA, USA: NeurIPS Foundation, <year>2017</year>;<volume>30</volume>.</mixed-citation>
    </ref>
    <ref id="ref54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>P</given-names></string-name>, <string-name><surname>Wu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>H</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Clnn-loop: a deep learning model to predict ctcf-mediated chromatin loops in the different cell lines and ctcf-binding sites (cbs) pair types</article-title>. <source>Bioinformatics</source>  <year>2022</year>;<volume>38</volume>(<issue>19</issue>):<fpage>4497</fpage>–<lpage>504</lpage>.<pub-id pub-id-type="pmid">35997565</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname><given-names>J</given-names></string-name>, <string-name><surname>Hoshino</surname><given-names>H</given-names></string-name>, <string-name><surname>Takaku</surname><given-names>K</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Hemoprotein bach1 regulates enhancer availability of heme oxygenase-1 gene</article-title>. <source>EMBO J</source>  <year>2002</year>;<volume>21</volume>(<issue>19</issue>):<fpage>5216</fpage>–<lpage>24</lpage>.<pub-id pub-id-type="pmid">12356737</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref56">
      <label>56.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>X</given-names></string-name>, <string-name><surname>Xing</surname><given-names>G</given-names></string-name>, <string-name><surname>Fraizer</surname><given-names>GC</given-names></string-name>, <string-name><surname>Saunders</surname><given-names>GF</given-names></string-name></person-group>. <article-title>Transactivation of an intronic hematopoietic-specific enhancer of the human wilms’ tumor 1 gene by Gata-1 and c-myb</article-title>. <source>J Biol Chem</source>  <year>1997</year>;<volume>272</volume>(<issue>46</issue>):<fpage>29272</fpage>–<lpage>80</lpage>.<pub-id pub-id-type="pmid">9361007</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref57">
      <label>57.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nolis</surname><given-names>IK</given-names></string-name>, <string-name><surname>McKay</surname><given-names>DJ</given-names></string-name>, <string-name><surname>Mantouvalou</surname><given-names>E</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Transcription factors mediate long-rang enhancer–promoter interactions</article-title>. <source>Proc Natl Acad Sci</source>  <year>2009</year>;<volume>106</volume>(<issue>48</issue>):<fpage>20222</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">19923429</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref58">
      <label>58.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whalen</surname><given-names>S</given-names></string-name>, <string-name><surname>Truty</surname><given-names>RM</given-names></string-name>, <string-name><surname>Pollard</surname><given-names>KS</given-names></string-name></person-group>. <article-title>Enhancer–promoter interactions are encoded by complex genomic signatures on looping chromatin</article-title>. <source>Nature geneticss</source>  <year>2016</year>;<volume>48</volume>(<issue>5</issue>):<fpage>488</fpage>–<lpage>96</lpage>.</mixed-citation>
    </ref>
    <ref id="ref59">
      <label>59.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weintraub</surname><given-names>AS</given-names></string-name>, <string-name><surname>Li</surname><given-names>CH</given-names></string-name>, <string-name><surname>Zamudio</surname><given-names>AV</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>YY1 is a structural regulator of enhancer-promoter loops[J]</article-title>. <source>Cell</source>  <year>2017</year>;<volume>171</volume>(<issue>7</issue>):<fpage>1573</fpage>–<lpage>1588.e28</lpage>.<pub-id pub-id-type="pmid">29224777</pub-id>
</mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Brief Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Brief Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">bib</journal-id>
    <journal-title-group>
      <journal-title>Briefings in Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1467-5463</issn>
    <issn pub-type="epub">1477-4054</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10938904</article-id>
    <article-id pub-id-type="doi">10.1093/bib/bbae083</article-id>
    <article-id pub-id-type="publisher-id">bbae083</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Problem Solving Protocol</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Enhancer-MDLF: a novel deep learning framework for identifying cell-specific enhancers</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0009-0007-4698-7786</contrib-id>
        <name>
          <surname>Zhang</surname>
          <given-names>Yao</given-names>
        </name>
        <aff><institution>School of Software, Shandong University</institution>, <addr-line>Jinan, 250100, Shandong</addr-line>, <country country="CN">China</country></aff>
        <xref rid="afn1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-8696-4983</contrib-id>
        <name>
          <surname>Zhang</surname>
          <given-names>Pengyu</given-names>
        </name>
        <aff><institution>College of Information Engineering, Northwest A&amp;F University</institution>, <addr-line>Yangling, 712100, Shaanxi</addr-line>, <country country="CN">China</country></aff>
        <xref rid="afn1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-2340-9258</contrib-id>
        <name>
          <surname>Wu</surname>
          <given-names>Hao</given-names>
        </name>
        <!--haowu@sdu.edu.cn-->
        <aff><institution>School of Software, Shandong University</institution>, <addr-line>Jinan, 250100, Shandong</addr-line>, <country country="CN">China</country></aff>
        <xref rid="cor1" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="cor1">Corresponding author. Hao Wu, School of Software, Shandong University, Jinan, 250100, Shandong, China. Tel.:+86-18254105536; Fax:+86-0531-88391686; E-mail: <email>haowu@sdu.edu.cn</email></corresp>
      <fn id="afn1">
        <p>Yao Zhang and Pengyu Zhang contributed equally to this work.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2024-03-13">
      <day>13</day>
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <volume>25</volume>
    <issue>2</issue>
    <elocation-id>bbae083</elocation-id>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>8</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>27</day>
        <month>1</month>
        <year>2024</year>
      </date>
      <date date-type="accepted">
        <day>7</day>
        <month>2</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2024</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="bbae083.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>Enhancers, noncoding DNA fragments, play a pivotal role in gene regulation, facilitating gene transcription. Identifying enhancers is crucial for understanding genomic regulatory mechanisms, pinpointing key elements and investigating networks governing gene expression and disease-related mechanisms. Existing enhancer identification methods exhibit limitations, prompting the development of our novel multi-input deep learning framework, termed Enhancer-MDLF. Experimental results illustrate that Enhancer-MDLF outperforms the previous method, Enhancer-IF, across eight distinct human cell lines and exhibits superior performance on generic enhancer datasets and enhancer–promoter datasets, affirming the robustness of Enhancer-MDLF. Additionally, we introduce transfer learning to provide an effective and potential solution to address the prediction challenges posed by enhancer specificity. Furthermore, we utilize model interpretation to identify transcription factor binding site motifs that may be associated with enhancer regions, with important implications for facilitating the study of enhancer regulatory mechanisms. The source code is openly accessible at <ext-link xlink:href="https://github.com/HaoWuLab-Bioinformatics/Enhancer-MDLF" ext-link-type="uri">https://github.com/HaoWuLab-Bioinformatics/Enhancer-MDLF</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>DNA sequence</kwd>
      <kwd>cell-specific enhancers</kwd>
      <kwd>deep learning</kwd>
      <kwd>transfer learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>62272278</award-id>
        <award-id>61972322</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Key Research and Development Program</institution>
            <institution-id institution-id-type="DOI">10.13039/501100012166</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>2021YFF0704103</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Fundamental Research Funds of Shandong University</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="12"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>INTRODUCTION</title>
    <p>Enhancers, noncoding fragments within DNA sequences, play a pivotal role in regulating gene transcription [<xref rid="ref1" ref-type="bibr">1</xref>, <xref rid="ref2" ref-type="bibr">2</xref>]. As a class of regulatory elements, enhancers exert control over diverse cellular activities, including tissue-specific gene expression [<xref rid="ref3" ref-type="bibr">3</xref>], cell growth and differentiation [<xref rid="ref4" ref-type="bibr">4</xref>] and cell carcinogenesis [<xref rid="ref5" ref-type="bibr">5</xref>]. Mutations or abnormal expression of enhancers can disrupt gene regulatory networks, thereby affecting cellular function, tissue development and disease progression [<xref rid="ref6" ref-type="bibr">6</xref>]. Many recent studies have found that the genetic mechanisms of complex diseases can be better revealed by understanding the role of enhancers in gene expression [<xref rid="ref7" ref-type="bibr">7–9</xref>]. Therefore, the identification of enhancers is crucial for advancing the comprehension of gene expression and regulation.</p>
    <p>High-throughput computational and experimental methods have been employed to predict enhancers. Several methods for identifying enhancers are as follows: (i) Computational Analysis Using Conserved Sequences and Transcription Factor Binding Site Data [<xref rid="ref10" ref-type="bibr">10</xref>, <xref rid="ref11" ref-type="bibr">11</xref>]. This method effectively predicts the genomic locations where known transcription factors (TFs) with binding sequence motifs are likely to interact. However, it may yield false positives by encompassing regulatory element sequences that bind TFs but do not serve as enhancers. (ii) Utilizing ChIP-seq Data for Transcription Factors and P300. ChIP-seq data for TFs can identify enhancers bound by known TFs [<xref rid="ref12" ref-type="bibr">12</xref>]. However, it cannot distinguish between enhancer and promoter regions because both can bind TFs. Furthermore, not all enhancers necessarily bind TFs. ChIP-seq data for p300 [<xref rid="ref13" ref-type="bibr">13</xref>], commonly used for enhancer prediction, faces limitations in distinguishing between active and inactive enhancers. (iii) Chromatin Accessibility-Related Data (e.g. DNase-seq [<xref rid="ref14" ref-type="bibr">14</xref>], FAIRE-seq [<xref rid="ref15" ref-type="bibr">15</xref>], ATAC-seq [<xref rid="ref16" ref-type="bibr">16</xref>]). This method relies on data related to chromatin accessibility, However, it may yield false positives by including other transcriptional regulatory elements, such as promoters, insulators and silencers. (iv) Histone Modification Data [<xref rid="ref17" ref-type="bibr">17</xref>] (e.g. H3K4me1 and H3K27ac). Using both H3K4me1, which marks active and poised enhancers, and H3K27ac, which marks associated with active regulatory regions from both promoters and enhancers to identify activated enhancer regions, this method benefits from the widespread availability of histone modification data across different species, effectively supporting various research needs. However, its drawback lies in the broad nature of histone modification data across the entire genome, hindering precise enhancer prediction. (v) Prediction based on enhancer RNA (eRNA) data [<xref rid="ref18" ref-type="bibr">18</xref>] (e.g. RNA-seq, ChAR-seq, GRO-seq and NET-seq). Enhancers transcribe eRNAs, and their locations can be predicted using eRNA data obtained through sequencing techniques. However, this method is limited in predicting enhancers that are not actively transcribed. Despite their individual merits, these experimental methods have inherent limitations, and they are both time-consuming and expensive. Therefore, it is essential to develop reliable computational tools for enhancer identification.</p>
    <p>In recent times, several computational methods for enhancer identification have been proposed, including ienhancer-2L [<xref rid="ref19" ref-type="bibr">19</xref>], Enhancerpred [<xref rid="ref20" ref-type="bibr">20</xref>], ienhancer-EL [<xref rid="ref21" ref-type="bibr">21</xref>], ienhancer-ECNN [<xref rid="ref22" ref-type="bibr">22</xref>], BERT-enhancer [<xref rid="ref23" ref-type="bibr">23</xref>], ienhancer-EBLSTM [<xref rid="ref24" ref-type="bibr">24</xref>] and ienhancer-XG [<xref rid="ref25" ref-type="bibr">25</xref>]. Notably, these methods are based on the dataset created by Liu <italic toggle="yes">et al</italic>. [<xref rid="ref19" ref-type="bibr">19</xref>]. However, two notable issues arise with this dataset. Firstly, the enhancers within it are extracted as short sequences of a fixed length (200bp), raising questions about the adaptability of these methods to unequal-length sequences and their ability to maintain optimal performance under such circumstances. Secondly, the dataset is a mixed general dataset encompassing nine cell lines, notwithstanding the established understanding that enhancers exhibit cell-specificity [<xref rid="ref26" ref-type="bibr">26</xref>, <xref rid="ref27" ref-type="bibr">27</xref>].</p>
    <p>To further investigate the cell-specific nature of enhancers, Enhancer-IF [<xref rid="ref28" ref-type="bibr">28</xref>], a framework based on integrated machine learning (ML), was proposed to identify enhancers. This framework utilizes eight cell lines with known cell-specific enhancers. Cross-cell line validation results demonstrate that a significant majority of enhancers indeed exhibit cell-type specificity. This underscores the importance of considering cell specificity in enhancer identification, a facet not fully addressed by the earlier methods relying on the general dataset approach.</p>
    <p>Despite notable advancements in enhancer identification, there exist notable limitations. Firstly, the predictive performance of Enhancer-IF for cell-specific enhancers is not ideal, which may be due to the conventional feature encoding scheme and the relatively simplistic design of the model framework. Secondly, Enhancer-IF integrates five commonly used classifiers (Random Forest, Extremely Randomized Tree, MultiLayer Perceptron, Support Vector Machine and Extreme Gradient Boosting) and employs a grid search algorithm to optimize parameters for each classifier across every cell line. Undoubtedly, this approach is time-consuming when applied to new cell lines. Furthermore, there is a lack of in-depth exploration of potential strategies to mitigate the impact of cell specificity on the overall performance of enhancer prediction. Lastly, Enhancer-IF lacks explanations for its prediction models, which is crucial for exploring transcription factor binding sites (TFBSs) motifs in enhancer regions. The absence of such interpretability hinders a comprehensive understanding of the biological insights derived from the predictions. Addressing these limitations is crucial for advancing the accuracy, efficiency and biological interpretability of enhancer prediction models.</p>
    <p>Therefore, we propose Enhancer-MDLF, a Multi-input Deep Learning Framework designed to predict cell-specific enhancers across multiple human cell lines. Our approach amalgamates word vector features derived from the human genome sequence and motif features extracted from the position weight matrix (PWM) of motifs. Through comprehensive evaluation on cell-specific datasets and other pertinent datasets, we demonstrate the superior performance of enhancer-MDLF. The principal contributions of our work include (i) the introduction of a novel deep learning framework, employing multi-module inputs for the identification of cell-specific enhancers; (ii) the substantiation of Enhancer-MDLF’s substantial outperformance relative to state-of-the-art predictors, accomplished without the need of parameter tuning; (iii) the incorporation of transfer learning into our model to address the challenges in cross-cell line predictions stemming from enhancer specificity and (iv) a meticulous analysis of the conservation and specificity of enhancers at the motif level, culminating in the identification of the most important TFBS motifs within enhancer regions. The overall framework of our study is depicted in <xref rid="f1" ref-type="fig">Figure 1</xref>.</p>
    <fig position="float" id="f1">
      <label>Figure 1</label>
      <caption>
        <p><bold>The overall flowchart of Enhancer-MDLF.</bold> (<bold>A</bold>) Sequence encoding: Enhancer-MDLF utilizes two distinct sequence encoding schemes, namely dna2vec and Motif frequency. (<bold>B</bold>) Model construction: Enhancer-MDLF is structured by integrating two pivotal modules: the dna2vec module and the motif module. (<bold>C</bold>) Model evaluation. Enhancer-MDLF undergoes thorough evaluation across multiple dimensions to assess its performance. (<bold>D</bold>) Motif analysis: Utilizing the SHAP framework, we conduct an in-depth analysis of the important features identified by Enhancer-MDLF.</p>
      </caption>
      <graphic xlink:href="bbae083f1" position="float"/>
    </fig>
  </sec>
  <sec id="sec2">
    <title>MATERIALS AND METHODS</title>
    <sec id="sec2a">
      <title>Dataset</title>
      <p>In this study, we utilize the benchmark dataset derived from Enhancer-IF [<xref rid="ref28" ref-type="bibr">28</xref>]. The dataset encompasses eight distinct cell lines, namely GM12878, HEK293, HMEC, HSMM, HUVEC, K562, NHEK and NHLF. They utilize Enhancer Atlas 2.0 [<xref rid="ref29" ref-type="bibr">29</xref>] (<ext-link xlink:href="http://www.enhanceratlas.org/indexv2.php" ext-link-type="uri">http://www.enhanceratlas.org/indexv2.php</ext-link>) to extract enhancer locations for each cell line, and their corresponding sequences are obtained through Retailor [<xref rid="ref30" ref-type="bibr">30</xref>] (<ext-link xlink:href="http://shiva.rockefeller.edu/SeqTailor/" ext-link-type="uri">http://shiva.rockefeller.edu/SeqTailor/</ext-link>). To ensure diversity, the CD-HIT software is applied to eliminate paired sequences with a similarity exceeding 60%. The construction of negative samples follows the methodology introduced by Dao <italic toggle="yes">et al</italic>. [<xref rid="ref31" ref-type="bibr">31</xref>]. Finally, a training set and an independent test set are obtained for each of the eight cell lines. For more details regarding the dataset employed in this study, please refer to <xref rid="TB1" ref-type="table">Table 1</xref>.</p>
      <table-wrap position="float" id="TB1">
        <label>Table 1</label>
        <caption>
          <p>Statistical summary of training and independent datasets for different cell lines</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" span="1"/>
            <col span="2" align="left"/>
            <col span="3" align="left"/>
            <col span="4" align="left"/>
            <col span="5" align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th align="left" rowspan="1" colspan="1">Cell lines</th>
              <th colspan="2" align="left" rowspan="1">Training</th>
              <th colspan="2" align="left" rowspan="1">Independent</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th align="left" rowspan="1" colspan="1">Positives</th>
              <th align="left" rowspan="1" colspan="1">Negatives</th>
              <th align="left" rowspan="1" colspan="1">Positives</th>
              <th align="left" rowspan="1" colspan="1">Negatives</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">GM12878</td>
              <td rowspan="1" colspan="1">2187</td>
              <td rowspan="1" colspan="1">2187</td>
              <td rowspan="1" colspan="1">1187</td>
              <td rowspan="1" colspan="1">2356</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HEK293</td>
              <td rowspan="1" colspan="1">3756</td>
              <td rowspan="1" colspan="1">3756</td>
              <td rowspan="1" colspan="1">2662</td>
              <td rowspan="1" colspan="1">5324</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HMEC</td>
              <td rowspan="1" colspan="1">3333</td>
              <td rowspan="1" colspan="1">3333</td>
              <td rowspan="1" colspan="1">1795</td>
              <td rowspan="1" colspan="1">3590</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HSMM</td>
              <td rowspan="1" colspan="1">2821</td>
              <td rowspan="1" colspan="1">2821</td>
              <td rowspan="1" colspan="1">1520</td>
              <td rowspan="1" colspan="1">3040</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HUVEC</td>
              <td rowspan="1" colspan="1">4750</td>
              <td rowspan="1" colspan="1">4750</td>
              <td rowspan="1" colspan="1">2559</td>
              <td rowspan="1" colspan="1">5118</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">K562</td>
              <td rowspan="1" colspan="1">3318</td>
              <td rowspan="1" colspan="1">3318</td>
              <td rowspan="1" colspan="1">1787</td>
              <td rowspan="1" colspan="1">3754</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NHEK</td>
              <td rowspan="1" colspan="1">2896</td>
              <td rowspan="1" colspan="1">2896</td>
              <td rowspan="1" colspan="1">1559</td>
              <td rowspan="1" colspan="1">3118</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NHLF</td>
              <td rowspan="1" colspan="1">1462</td>
              <td rowspan="1" colspan="1">1462</td>
              <td rowspan="1" colspan="1">788</td>
              <td rowspan="1" colspan="1">1576</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec2b">
      <title>DNA sequence encoding schemes</title>
      <p>Enhancer-MDLF is a deep learning model, including a dna2vec module and a motif module. The two modules utilize different sequence encoding schemes as input, as depicted in the following sections.</p>
      <sec id="sec2b1">
        <title>dna2vec</title>
        <p>In recent times, word embedding techniques have gained significant popularity within the bioinformatics community, offering a promising solution to address the challenge posed by the similarity of kmer-based features in different sequences, even when their orders are reversed [<xref rid="ref32" ref-type="bibr">32</xref>, <xref rid="ref33" ref-type="bibr">33</xref>]. However, a notable limitation arises when employing word vector techniques for sequence coding. Typically, the training corpus for learning word vectors encompasses only one cell line dataset. This constrained corpus imparts a limited amount of information to the learned word vectors, thereby restricting their representational capacity.</p>
        <p>To address the above limitation, we utilize the pre-trained DNA vectors provided in dna2vec as a sequence encoding index. dna2vec, a method grounded in the word2vec word embedding model, is employed to compute distributed representations of variable-length k-mers within DNA sequences. Earlier investigations have substantiated that the mathematical operations applied to dna2vec vectors exhibit similarity to nucleotide concatenation [<xref rid="ref34" ref-type="bibr">34</xref>]. dna2vec employs the human genome sequence as a learning corpus for unsupervised training, employing the continuous skip-gram (Skip-gram) model in word2vec. This process results in embedding k-mers into a continuous vector space with 100 dimensions.</p>
        <p>In this study, we conducted experiments within the range of [3,8] (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>) to determine that the optimal value for k is 3. Subsequently, we utilize the pre-trained dna2vec model to obtain a 100-dimensional feature vector for each word. These feature vectors are obtained by concatenating the vectors of short sequences with a step size of 1 in the overall sequence. They are then input to the dna2vec module as sequence features. Assuming that the sequence length is denoted as L, the input dimension of the dna2vec module becomes 100<inline-formula><tex-math notation="LaTeX" id="ImEquation1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\times $\end{document}</tex-math></inline-formula> (L - k + 1). It is noteworthy that due to varying feature dimensions for unequal sequences in the dataset, adaptive pooling operations are employed to flatten the feature dimensions to 10 000 dimensions. This pre-processing step is essential for subsequent input into the deep learning model.</p>
      </sec>
      <sec id="sec2b2">
        <title>Motif frequency</title>
        <p>In dna2vec, the parameter k in k-mer is set to 3, signifying that DNA fragments are divided into short sequences with a nucleotide length of 3. Given the relatively brief nature of these sequences, addressing this limitation necessitates the utilization of longer features for capturing intricate sequence patterns. Notably, TFs play an important role in gene transcription by directly binding motifs in the genome. A previous study has successfully identified some potential TF binding within DNA sequences, particularly those inclined to bind in the enhancer regions [<xref rid="ref35" ref-type="bibr">35</xref>]. Leveraging this biological characteristic, we extract the count of TFBS motifs within each DNA sequence and convert it into a frequency representation for input to the motif module.</p>
        <p>We extract the PWM of motifs from the HOCOMOCO Human v11 database [<xref rid="ref36" ref-type="bibr">36</xref>] for sliding-scale matching to the sequence data in this dataset. Assuming that the length of the motif is denoted as <inline-formula><tex-math notation="LaTeX" id="ImEquation2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula>, the PWM is structured as a matrix with <inline-formula><tex-math notation="LaTeX" id="ImEquation3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula> rows and four columns, representing the values corresponding to each base (A, C, G and T). The process involves dividing each sequence of length <inline-formula><tex-math notation="LaTeX" id="ImEquation4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{s}$\end{document}</tex-math></inline-formula> into subsequences of length <inline-formula><tex-math notation="LaTeX" id="ImEquation5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula> with a stride of 1. Consequently, we obtain <inline-formula><tex-math notation="LaTeX" id="ImEquation6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{s}$\end{document}</tex-math></inline-formula> - <inline-formula><tex-math notation="LaTeX" id="ImEquation7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula> +1 subsequence segments, each of length <inline-formula><tex-math notation="LaTeX" id="ImEquation8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula>. For each subsequence segment, we calculate the sum of the corresponding value for each base, and this sum serves as the final matching score. This score is then compared with the predetermined threshold score to determine whether this subsequence segment matches a motif. The criteria for subsequence segment comparison are defined by the following equation: </p>
        <disp-formula id="deqn01">
          <label>(1)</label>
          <tex-math notation="LaTeX" id="DmEquation1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*}&amp; Q = \sum_{i=0}^{l-1} PWM_{ij},\end{align*}\end{document}</tex-math>
        </disp-formula>
        <p>where the variable j takes on values 0, 1, 2 and 3, corresponding to the nucleotides A, C, G and T in the subsequence segments, and Q represents the matching score. Assuming P represents the <italic toggle="yes">P</italic>-value threshold score (set at <inline-formula><tex-math notation="LaTeX" id="ImEquation9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$10^{-4}$\end{document}</tex-math></inline-formula>) for respective motifs, the subsequence segment is deemed to match this motif when <inline-formula><tex-math notation="LaTeX" id="ImEquation10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$Q&gt; P$\end{document}</tex-math></inline-formula>.</p>
        <p>Upon traversing each sequence, we gather information on the count of each TFBS motif, resulting in a 401-dimensional feature vector denoted as <inline-formula><tex-math notation="LaTeX" id="ImEquation11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$V_{count}$\end{document}</tex-math></inline-formula>. Additionally, to accommodate varying sequence lengths, we utilize the feature vector <inline-formula><tex-math notation="LaTeX" id="ImEquation12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$V_{frequency}$\end{document}</tex-math></inline-formula> as the final model input, calculated through the following equation: </p>
        <disp-formula id="deqn02">
          <label>(2)</label>
          <tex-math notation="LaTeX" id="DmEquation2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*}&amp; V_{frequency} = \frac{V_{count}}{L_{s}},\end{align*}\end{document}</tex-math>
        </disp-formula>
        <p>where <inline-formula><tex-math notation="LaTeX" id="ImEquation13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{s}$\end{document}</tex-math></inline-formula> represents the length of each sequence.</p>
      </sec>
    </sec>
    <sec id="sec2e">
      <title>The framework of Enhancer-MDLF</title>
      <sec id="sec2e1">
        <title>Model architecture</title>
        <p>The DNA fragments extracted from the dataset undergo encoding through two schemes, namely dna2vec and motif. Subsequently, these encoded fragments are input into the model to predict whether the fragment contains enhancer regions. Through systematic experimentation involving various combinations of convolutional layers, max-pooling layers and dense layers, along with meticulous parameter tuning for optimal balance between accuracy, efficiency and generalization capabilities, we have defined the comprehensive framework of Enhancer-MDLF, as illustrated in <xref rid="f2" ref-type="fig">Figure 2A</xref>. The details are elaborated as follows.</p>
        <fig position="float" id="f2">
          <label>Figure 2</label>
          <caption>
            <p><bold>The model architecture and training procedure of Enhancer-MDLF.</bold> (<bold>A</bold>) Enhancer-MDLF comprises a dna2vec module predominantly composed of convolutional layers and a motif module primarily consisting of Dense layers. It takes dna2vec and motif frequency, the two sequence encoding results, as inputs. The fused features are derived by concatenating after feature extraction and subsequently passed through a sigmoid function for enhancer detection. (<bold>B</bold>) The training procedure of the Enhancer-MDLF framework. The process involves iterations through the training set five times to iteratively refine the predictive model. Subsequently, this model undergoes testing on the test set to yield the final prediction results.</p>
          </caption>
          <graphic xlink:href="bbae083f2" position="float"/>
        </fig>
        <p><bold>Feature Extraction:</bold> We employ a combination of three 1D convolutional layers with corresponding 1D max-pooling layers in the dna2vec module, and three dense layers in the motif module. The convolutional layers are instrumental in capturing complex features from the inputs through convolutional computations. Simultaneously, the max-pooling layers implement a down-sampling approach, selecting the maximum value for each sub-region. This not only improves the robustness of the model but also mitigates the risk of overfitting. Specifically, we define three convolutional layers with 64 filters, a kernel size of 7 and a stride of 3. Additionally, three max-pooling layers are constructed with a pool size of 2, respectively.</p>
        <p>To further enhance the model’s generalization and prevent overfitting, we introduce a dropout layer with a probability of 0.6. This layer randomly removes certain neural network units during the training phase, contributing to the model’s overall robustness and preventing excessive adaptation to the training data.</p>
        <p><bold>Prediction:</bold> The dna2vec module undergoes computation through a dense layer comprising 500 neurons after flattening. Simultaneously, the motif module is designed to culminate in 16 neurons, with a choice of the ‘relu’ activation function for both modules. Within the motif module, we incorporate a dropout layer with a probability of 0.6 following the three dense layers. Finally, Enhancer-MDLF combines the results from the dna2vec and motif modules and generates predictive values using a dense layer with a single neuron and a ‘sigmoid’ activation function. To further enhance model generalization, a dropout layer with a probability of 0.5 is applied after concatenating the two sets of results. The classification criterion is set such that a sample is considered positive if the predicted value exceeds 0.5; otherwise, it is deemed negative.</p>
        <p><bold>Hyperparameters:</bold> The hyperparameters of Enhancer-MDLF include learning rate, batch size and maximum epoch. Following a comprehensive comparison of performance across multiple hyperparameter combinations through the grid search method, we establish the optimal settings as follows: learning rate = 0.0001, batch size = 100 and max epoch = 200. The specific details of the grid search method and the details of the search ranges for hyperparameters can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>.</p>
      </sec>
      <sec id="sec2e2">
        <title>Loss function</title>
        <p>The dataset we utilize exhibits an imbalance, a characteristic that poses challenges for traditional loss functions. Traditional approaches tend to disproportionately penalize dominant classes, often neglecting the informative contributions of minority classes in imbalanced datasets, thereby resulting in suboptimal prediction performance. Recognizing this, the focal loss [<xref rid="ref37" ref-type="bibr">37</xref>] initially employed in computer vision [<xref rid="ref38" ref-type="bibr">38</xref>, <xref rid="ref39" ref-type="bibr">39</xref>] has emerged as a solution. Consequently, we use the focal loss as the primary loss function for Enhancer-MDLF, aiming to mitigate the drawbacks associated with the traditional cross-entropy loss function in managing imbalanced datasets. The focal loss is formally defined as follows: </p>
        <disp-formula id="deqn03">
          <label>(3)</label>
          <tex-math notation="LaTeX" id="DmEquation3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*}&amp; FL(P_{t})=-\alpha(1-P_{t})^{\gamma}\log(P_{t}),\end{align*}\end{document}</tex-math>
        </disp-formula>
        <p>where <inline-formula><tex-math notation="LaTeX" id="ImEquation14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$P_{t}$\end{document}</tex-math></inline-formula> represents the predicted probability, <inline-formula><tex-math notation="LaTeX" id="ImEquation15">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula> denotes the balance parameter and <inline-formula><tex-math notation="LaTeX" id="ImEquation16">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\gamma $\end{document}</tex-math></inline-formula> is the focus parameter. To determine the optimal configuration for Enhancer-MDLF, we conduct a thorough performance evaluation using the grid search method over a range of <inline-formula><tex-math notation="LaTeX" id="ImEquation17">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula> values [0.25, 0.5, 0.75] and <inline-formula><tex-math notation="LaTeX" id="ImEquation18">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\gamma $\end{document}</tex-math></inline-formula> values [1, 2, 3]. Our findings reveal that the model achieves optimal performance when <inline-formula><tex-math notation="LaTeX" id="ImEquation19">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula>=0.5 and <inline-formula><tex-math notation="LaTeX" id="ImEquation20">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\gamma $\end{document}</tex-math></inline-formula>=3.</p>
      </sec>
      <sec id="sec2e3">
        <title>Training procedure</title>
        <p>We utilize a novel training strategy for Enhancer-MDLF to enhance its capacity to effectively learn information from the input feature vectors of the dna2vec and motif modules. First, we implement a 5-fold cross-validation algorithm, randomly partitioning the training set into five folds with a 4:1 ratio between the training and validation sets. To prevent overfitting, an early stopping mechanism is applied to the validation set. The model undergoes training five times, with the initial training parameters being the model’s starting parameters, determined by TensorFlow’s default network initialization methods [<xref rid="ref40" ref-type="bibr">40</xref>]. Xavier initialization is frequently used in TensorFlow for a variety of neural network layers, such as dense and convolutional layers. This method initializes weights randomly from a uniform or normal distribution, and the calculation of the standard deviation depends on the number of input and output units in the layer. Subsequent trainings build upon the most recent model, utilizing different folds of the training and validation sets. Following the completion of the five training sessions, the model is applied to the testing set for prediction.</p>
        <p>As shown in <xref rid="f2" ref-type="fig">Figure 2B</xref>, the workflow of Enhancer-MDLF is as follows:</p>
        <p>(i) Divide the training dataset <inline-formula><tex-math notation="LaTeX" id="ImEquation21">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{train}$\end{document}</tex-math></inline-formula> into five folds: D1, D2, D3, D4 and D5 utilizing a 5-fold cross-validation strategy. For the first training iteration, choose <inline-formula><tex-math notation="LaTeX" id="ImEquation22">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{train}$\end{document}</tex-math></inline-formula> - D1 (The remaining samples after removing D1 in <inline-formula><tex-math notation="LaTeX" id="ImEquation23">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{train}$\end{document}</tex-math></inline-formula>) as the current training set, utilizing ‘D1’ as the validation set to train the model and obtain <inline-formula><tex-math notation="LaTeX" id="ImEquation24">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$model_{1}$\end{document}</tex-math></inline-formula>.</p>
        <p>(ii) Subsequent to model1, for the second training iteration, employ <inline-formula><tex-math notation="LaTeX" id="ImEquation25">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{train}$\end{document}</tex-math></inline-formula> - D2 as the current training set and D2 as the validation set to continue training <inline-formula><tex-math notation="LaTeX" id="ImEquation26">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$model_{1}$\end{document}</tex-math></inline-formula>, obtaining <inline-formula><tex-math notation="LaTeX" id="ImEquation27">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$model_{2}$\end{document}</tex-math></inline-formula>. By analogy, repeat the training process five times to obtain the final model, denoted as <inline-formula><tex-math notation="LaTeX" id="ImEquation28">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$model_{5}$\end{document}</tex-math></inline-formula>.</p>
        <p>(iii) Evaluate the performance of the final model on the test set <inline-formula><tex-math notation="LaTeX" id="ImEquation29">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{test}$\end{document}</tex-math></inline-formula>.</p>
      </sec>
    </sec>
    <sec id="sec2i">
      <title>Evaluation metrics</title>
      <p>We utilize seven evaluation metrics to assess the performance of Enhancer-MDLF and compare it with that of other methods. These metrics encompass accuracy (ACC), balanced accuracy (BACC), area under the receiver operating characteristics (AUROC), Matthews correlation coefficient (MCC), sensitivity (Sn), specificity (Sp) and F1 score. The details of these evaluation metrics are provided in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>. In general, a higher value for these metrics indicates superior model performance.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>RESULTS</title>
    <sec id="sec3a">
      <title>Performance evaluation of combinatorial module in comparison with individual modules</title>
      <p>Recent studies have underscored the superiority of leveraging multiple features over individual ones in sequence-based prediction tasks [<xref rid="ref41" ref-type="bibr">41–44</xref>]. Consequently, Enhancer-MDLF, proposed in this study, incorporates a dna2vec module and a motif module to comprehensively extract information on enhancer sequences. To assess the effectiveness of this combinatorial module in improving the model’s performance for predicting cell-specific enhancers, we individually train the dna2vec module and the motif module utilizing training sets for each cell line. Subsequently, we evaluate the performance of these modules on independent test sets and draw comparisons with the performance of the combinatorial module.</p>
      <p>Given that the datasets used in this study are imbalanced, we utilize the BACC metric rather than ACC to evaluate the performance of models. What surprised us is that Enhancer-MDLF can effectively improve the performance of the model by fusing the two modules while maintaining a reasonable computational cost (refer to <xref rid="f3" ref-type="fig">Figure 3</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S2</xref> and <xref rid="sup1" ref-type="supplementary-material">S3</xref>). All six metrics exhibit improvement across the eight cell lines. Especially, the performance of two individual modules on the HMEC cell line is unsatisfactory, while the combinatorial module greatly improves the prediction performance. Furthermore, the training time for the combined module is shorter than that for the dna2vec module alone in the HSMM and NHEK cell lines (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>), potentially due to an accelerated convergence speed facilitated by our early stopping strategy. Overall, these results indicate that leveraging multiple features indeed improves prediction performance, establishing Enhancer-MDLF as a powerful and robust tool for predicting cell-specific enhancers.</p>
      <fig position="float" id="f3">
        <label>Figure 3</label>
        <caption>
          <p><bold>Performance evaluation of combinatorial module in comparison with individual modules.</bold> Panels (<bold>A–F</bold>) depict the evaluation of model performance across eight cell lines using metrics such as AUROC, BACC, F1 score, MCC, SN and SP.</p>
        </caption>
        <graphic xlink:href="bbae083f3" position="float"/>
      </fig>
    </sec>
    <sec id="sec3b">
      <title>Performance comparison with state-of-the-art method</title>
      <p>To comprehensively assess the predictive capabilities of Enhancer-MDLF for cell-specific enhancers, we conduct a thorough comparison with Enhancer-IF, a model dedicated to cell-specific enhancer prediction, across eight cell lines. As mentioned in the introduction section, existing enhancer prediction methods predominantly rely on generic datasets, neglecting the crucial aspect of enhancer’s cell specificity. To validate the robustness of our model, we conduct performance comparisons for both methods on independent test sets and through 10-fold cross-validation (refer to <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>).</p>
      <p>As can be seen in <xref rid="f4" ref-type="fig">Figure 4</xref> and detailed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref>, Enhancer-MDLF consistently outperforms Enhancer-IF across all five metrics and all cell lines, demonstrating its significant superiority. The robust predictive performance of Enhancer-MDLF could be attributed to its unique multi-module fusion approach and a novel training strategy that incorporates comprehensive and in-depth sequence information. It is crucial to highlight that, in contrast to the training strategy of Enhancer-IF, we design a unique training strategy for Enhancer-MDLF. This strategy involves learning more distributions iteratively within a limited dataset. Our approach and Enhancer-IF employ identical input data and generate results on the same test dataset, ensuring a fair and meaningful comparison within the training process from an end-to-end perspective.</p>
      <fig position="float" id="f4">
        <label>Figure 4</label>
        <caption>
          <p><bold>Performance comparison between Enhance-MDLF and Enhancer-IF on independent test sets.</bold> Panels (<bold>A–D</bold>) present the evaluation of model performance across eight cell lines using metrics such as AUROC, BACC, MCC and SN. Panels (<bold>E–H</bold>) present the evaluation of model performance across eight cell lines using metrics such as AUROC, BACC, MCC and SN.</p>
        </caption>
        <graphic xlink:href="bbae083f4" position="float"/>
      </fig>
      <p>Notably, a pronounced performance gap is observed between the 10-fold cross-validation and independent test sets for Enhancer-MDLF on the HMEC cell line. This discrepancy may arise from the distinct data distributions between the training and test sets, posing a challenge for the model to effectively generalize information learned from the training set to the test set. However, even under these challenging conditions, our method outperforms Enhancer-IF by 9.54% on the independent test set in terms of BACC.</p>
      <p>However, to the best of our knowledge, there is no specific pattern observed in Enhancer-IF for the HMEC dataset. This lack of observation could be attributed to Enhancer-IF consistently performing within the range of 70–80% across multiple cell lines, which may imply limited knowledge acquisition, possibly concealing specific characteristics within the HMEC dataset. It is precisely due to the robust predictive capabilities of Enhancer-MDLF that these discrepant results may indicate that the enhancers within the HMEC cell line have more complex gene regulatory mechanisms. This suggests an intriguing direction for further exploration to help uncover the reasons behind enhancer cell specificity. Overall, these results demonstrate the robustness and effectiveness of Enhancer-MDLF in accurately predicting cell-specific enhancers.</p>
    </sec>
    <sec id="sec3c">
      <title>Performance evaluation across cell lines</title>
      <p>Previous studies have established that enhancers exhibit cell-specific functionalities [<xref rid="ref27" ref-type="bibr">27</xref>, <xref rid="ref45" ref-type="bibr">45</xref>]. To explore the potential relationships among enhancers across different cell lines, we conduct a comprehensive cross-cell line performance evaluation to investigate the transferability of cell-specific models. Our approach involves training models on one cell line and evaluating the performance of Enhancer-MDLF and Enhancer-IF on the test sets of seven other cell lines. The results demonstrate that Enhancer-MDLF achieves optimal performance in terms of average BACC across all eight cell lines (<xref rid="f5" ref-type="fig">Figure 5A</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S6</xref> and <xref rid="sup1" ref-type="supplementary-material">S7</xref>). Notably, the HEK293 model achieves an average BACC of 86.18% when predicting outcomes in other cell lines, indicating its capacity to transfer to other cell lines. Despite the prevalence of cell-specific enhancers observed in most instances, such as those identified in NHLF cell lines, it is important to acknowledge the existence of enhancers that exhibit significant similarities across certain cell lines. For instance, Enhancer-MDLF demonstrates satisfactory mutual predictive performance on the HUVEC and K562 cell lines, as well as on the NHEK and HMEC cell lines (<xref rid="f5" ref-type="fig">Figure 5B</xref>). These results highlight a commonality of enhancers between some cell lines. Overall, enhancers generally display specificity among most cell lines, but there exist similarities between enhancers across specific cell lines. Therefore, investigating both cell-specific and non-cell-specific enhancers emerges as a critical avenue for unveiling insights into cell specificity and differentiation.</p>
      <fig position="float" id="f5">
        <label>Figure 5</label>
        <caption>
          <p><bold>Comprehensive performance evaluation across multiple aspects.</bold> (<bold>A</bold>) Average BACC for cross-cell line prediction on eight cell lines. (<bold>B</bold>) Heat map of BACC in cross-cell line evaluation. Columns represent pre-trained models trained on different training sets, and rows represent testing on their own or other cell lines’ test sets. (<bold>C</bold>) Exploration of Transfer Learning Strategies: Three transfer learning strategies are presented to address the challenge of poor direct prediction performance in cross-cell line evaluation. (<bold>D</bold>) Performance comparison of Enhancer-MDLF with other methods on a universal dataset. (<bold>E–H</bold>) Performance comparison of Enhancer-MDLF with other methods on enhancer–promoter datasets.</p>
        </caption>
        <graphic xlink:href="bbae083f5" position="float"/>
      </fig>
    </sec>
    <sec id="sec3d">
      <title>Performance evaluation across cell lines with transfer learning</title>
      <p>The challenge of predicting enhancers across different cell lines, attributed to the inherent cell specificity of enhancers, significantly hinders the exploration of gene regulatory mechanisms. To overcome this obstacle and advance the field, we further explore potential approaches. An emerging ML technique, transfer learning, proves promising as it leverages knowledge acquired from a source domain to enhance learning performance in a target domain. Notably, this technique has demonstrated success in predicting cell-specific enhancer–promoter interactions [<xref rid="ref46" ref-type="bibr">46</xref>, <xref rid="ref47" ref-type="bibr">47</xref>]. To comprehensively evaluate the potential of transfer learning in the context of enhancer prediction, we adopt three strategies: transfer_1, transfer_2 and transfer_3. These strategies align with both traditional transfer learning methods and those previously utilized in a relevant study [<xref rid="ref48" ref-type="bibr">48</xref>]. Detailed descriptions of these strategies are provided in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>, aiming to shed light on their effectiveness in mitigating the challenges posed by the cell specificity of enhancers.</p>
      <p>We apply these three distinct transfer strategies separately to Enhancer-MDLF to validate their effectiveness and explore their practical applicability (<xref rid="f5" ref-type="fig">Figure 5C</xref>). The results, as detailed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S8</xref>, <xref rid="sup1" ref-type="supplementary-material">S9</xref> and <xref rid="sup1" ref-type="supplementary-material">S10</xref> across eight cell lines, illustrate significant insights. Enhancer-MDLF with transfer_1 demonstrates notable improvement in prediction performance, albeit falling short compared with predicting enhancers in the same cell line. However, this outcome underscores the effectiveness of transfer learning. In contrast, Enhancer-MDLF with transfer_2, leveraging additional enhancer information from multiple cell lines during the model pre-training, exhibits exceptional performance. Compared with the model directly trained with random initial weights (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S11</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2</xref>), the results indicate that pre-training the model with enhancers from diverse cell lines yields more effective predictions in new cell lines. This is attributed to the more representative initial weights acquired from the pre-trained model, highlighting its potential for accurately labeling other unannotated data, especially under the constraint of limited annotated data size. Particularly noteworthy is the observation that Enhancer-MDLF with transfer_3 exhibits optimal performance across the eight cell lines.</p>
      <p>These findings present a practical scenario for addressing enhancer-specific prediction challenges. If Enhancer-MDLF is pre-trained using as many (or even all) cell line enhancers following our transfer learning strategies, a comprehensive pre-trained model may be obtained, serving as an effective initial model for enhancer prediction tasks in various cell lines. Even with potential cost constraints in obtaining enhancers from numerous cell lines, Enhancer-MDLF with transfer_2 demonstrates robust performance with only a few cell lines’ enhancers. Overall, the utilization of transfer learning provides an effective and promising solution to overcome prediction challenges arising from enhancer specificity in practical applications. We have provided the pre-trained model, utilizing enhancers from these eight cell lines, available on GitHub for further exploration and utilization.</p>
    </sec>
    <sec id="sec3e">
      <title>Performance evaluation on other datasets</title>
      <p>To demonstrate the superiority of our approach comprehensively, we subject Enhancer-MDLF to evaluation using the generic dataset created by Liu <italic toggle="yes">et al</italic>. [<xref rid="ref21" ref-type="bibr">21</xref>], a widely used benchmark in enhancer identification tasks. To maintain comparability with prior studies, we utilize the same training set, test set and evaluation metrics. The outcomes reveal that Enhancer-MDLF achieves optimal performance on the independent test set, with MCC, SN and ACC of 0.6067, 0.84 and 0.8025, respectively (<xref rid="f5" ref-type="fig">Figure 5D</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S12</xref>). These results highlight the robustness and generality of Enhancer-MDLF, demonstrating its efficacy and versatility as a powerful tool for predicting enhancers in human cell lines.</p>
      <p>Besides, existing studies have demonstrated that enhancers and promoters exhibit similar sequence structures [<xref rid="ref49" ref-type="bibr">49</xref>, <xref rid="ref50" ref-type="bibr">50</xref>]. Leveraging datasets derived from iPro-WAEL [<xref rid="ref33" ref-type="bibr">33</xref>], we conduct an investigation to assess whether Enhancer-MDLF effectively distinguishes between enhancers and promoters. Given the limitations of some enhancer prediction methods for this dataset, as detailed in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>, we restrict our comparison to Enhancer-MDLF, ienhancer-2L and enhancer-IF. The results unequivocally showcase Enhancer-MDLF’s robust capacity to discriminate between promoters and enhancers, effectively capturing the distinct information associated with these two regulatory elements despite their similar sequence structures (<xref rid="f5" ref-type="fig">Figure 5E–H</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S14</xref>).</p>
    </sec>
    <sec id="sec3f">
      <title>Exploration of TFBS motifs in enhancer regions</title>
      <p>Investigating TFBS motifs within enhancer regions is crucial for understanding the regulatory mechanisms of TFs. TFs typically exert their transcriptional influence by recognizing specific TFBS motifs and binding to the regulatory regions of genes. Enhancers, along with the associated TFs, play a significant role in human diseases and biological processes [<xref rid="ref35" ref-type="bibr">35</xref>, <xref rid="ref51" ref-type="bibr">51</xref>, <xref rid="ref52" ref-type="bibr">52</xref>]. To identify key motifs associated with enhancers, we utilize the Shapley Additive exPlanations (SHAP) framework [<xref rid="ref53" ref-type="bibr">53</xref>, <xref rid="ref54" ref-type="bibr">54</xref>] to interpret the input features of the motif module in Enhancer-MDLF. The details of the SHAP framework are provided in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>.</p>
      <p><xref rid="f6" ref-type="fig">Figure 6</xref> displays the top 50 motifs with the most significant impact on the model’s output across eight cell lines. The x-axis represents the SHAP value, where a positive value indicates a positive effect on the model’s output, while a negative value indicates a negative effect. Taking SP2 in <xref rid="f6" ref-type="fig">Figure 6A</xref> as an example, higher feature values predominantly cluster in the region where SHAP values &gt;0, indicating that SP2 has a positive effect on predicting the samples as enhancers. It can be seen from <xref rid="f6" ref-type="fig">Figure 6</xref> that the contributions of different motifs to the model’s output vary across different cell lines, providing further insight into the cell specificity of enhancers at the motif level.</p>
      <fig position="float" id="f6">
        <label>Figure 6</label>
        <caption>
          <p><bold>Analysis of top 50 important motif features across eight cell lines.</bold> (<bold>A–H</bold>) This study analyzes the top 50 motifs on eight cell lines that impact the model’s output utilizing the SHAP framework. The x-axis represents the SHAP values that impact the model’s output. Each point on the graph represents a sample, and the color of each sample point ranges from blue to red, representing the corresponding feature values of each sample. The color transition reflects the variation in feature values from low (blue) to high (red).</p>
        </caption>
        <graphic xlink:href="bbae083f6" position="float"/>
      </fig>
      <p>Besides, we find that certain important motifs identified in our study align well with findings from previous studies. For instance, the BACH1 motif emerges exclusively among the top 50 important features in the HUVEC cell line. Previous studies have shown that the heme-binding factor BACH1 can bind to multiple Maf recognition elements in the heme oxygenase 1 (HO-1) enhancer, thereby inhibiting its activity and participating in gene regulation [<xref rid="ref55" ref-type="bibr">55</xref>]. Similarly, the GATA1 motif is uniquely present among the top 50 important features solely in the K562 cell line. Previous studies have revealed that the GATA1 protein binds to the GATA-A site in the intronic WT1 enhancer <italic toggle="yes">in vitro</italic> in K562 cells, transactivating the enhancer and markedly increasing the CAT reporter activity 10–15-fold [<xref rid="ref56" ref-type="bibr">56</xref>].</p>
      <p>To visualize the impact of these features on the model’s output, force plots of SHAP are employed, and specific details are available in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1</xref>. Additionally, to enhance the user’s interactive experience, we have provided HTML files on GitHub (<ext-link xlink:href="https://github.com/HaoWuLab-Bioinformatics/Enhancer-MDLF" ext-link-type="uri">https://github.com/HaoWuLab-Bioinformatics/Enhancer-MDLF</ext-link>). These files showcase the contributions of each feature to the model’s output on the test sets of eight cell lines. Users can interactively select various features and samples on the graph, facilitating a dynamic exploration of their influence on the model’s output.</p>
      <p>Furthermore, the SP1 motif significantly contributes to enhancer predictions across numerous cell lines, as it has been shown to regulate chromatin loops between enhancers and distal promoters, thereby influencing transcriptional activity [<xref rid="ref57" ref-type="bibr">57</xref>]. Additionally, our analysis reveals certain key motifs, such as RUNX and MAX, which have not been extensively studied but are highlighted in TargetFinder [<xref rid="ref58" ref-type="bibr">58</xref>], These results underscore that enhancers can exert their effects through the involvement of certain proteins in specific cells, emphasizing the complexity of gene regulation involving enhancers.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <title>DISCUSSION AND CONCLUSION</title>
    <p>The identification of cell-specific enhancers is of significant importance for understanding cell-specific gene regulation and deciphering tissue development. In this study, we develop Enhancer-MDLF, a deep learning framework with multi-inputs, designed for accurate identification of cell-specific enhancers by integrating the dna2vec module and motif module. Experimental analyses demonstrate that Enhancer-MDLF outperforms existing methods across multiple cell-specific enhancer datasets, and significantly outperforms previous studies on general datasets and enhancer–promoter datasets, highlighting the robustness and versatility of Enhancer-MDLF. While our evaluations are constrained by the scale of the annotated dataset to specific cell lines, various experimental results instill confidence in the generalization ability of our model to extend to broader datasets. In summary, our proposed Enhancer-MDLF emerges as a superior and efficient tool for identifying enhancers.</p>
    <p>Additionally, in the course of cross-cell-line evaluation, we observe a notable challenge wherein models trained on one cell line often exhibit unsatisfactory performance when applied to predict enhancers on other cell lines. This limitation stems from the inherent cell specificity of enhancers, suggesting that a model trained on a specific cell line achieves optimal prediction performance solely within that context, posing a substantial limitation in practical applications. To overcome this limitation, we endeavor to mitigate it through the implementation of transfer learning. The pre-trained model with transfer_2 achieves remarkably precise predictions with only a modest amount of annotated data (e.g. 292 samples, constituting 10% of the training set, in the NHLF cell line). In practical scenarios, the scarcity of annotated data in private datasets may face challenges for the effective application of supervised learning. While unsupervised learning methods, such as clustering, can partially emulate similar functionality, their accuracy often falls significantly below the standards required for practical applications. Consequently, we assert that this approach substantiates an effective strategy for addressing the challenges posed by the cell-specific nature of enhancers. Among the three transfer learning strategies employed, transfer_3, a pre-trained model trained on all cell lines, demonstrates the most promising performance. This outcome suggests that with a sufficiently extensive dataset encompassing a diverse range of cell lines as input to our model, the development of a universal pre-trained model for identifying human enhancers becomes a plausible avenue of exploration—a prospect that holds significant interest and potential in research endeavors.</p>
    <p>Moreover, we employ a computational approach to analyze potentially important TFBS motifs associated with enhancers. Our analysis reveals several motifs that have undergone extensive study in previous research, such as BACH1, GATA1, SP1, RUNX and MAX. Additionally, we identify motifs like FOXJ3 and SP2, which significantly contribute to enhancer predictions across numerous cell lines but have received limited attention in prior studies. We infer that these less-explored motifs may play roles in gene regulation through intricate and as-yet-unidentified processes. While our current research conditions constrain further validation and exploration of the relationships between these new motifs and enhancer function, we believe these findings open new avenues for future enhancer-related research in the field of biology. Notably, Whalen <italic toggle="yes">et al</italic>. [<xref rid="ref58" ref-type="bibr">58</xref>] extensively explored the critical role of YY1 in enhancer–promoter interactions by interpreting ML models. Later on, it was further confirmed that YY1 regulates enhancer–promoter chromatin loops [<xref rid="ref59" ref-type="bibr">59</xref>].</p>
    <boxed-text id="box01" position="float">
      <sec id="sec28a">
        <title>Key Points</title>
        <list list-type="bullet">
          <list-item>
            <p>We propose a novel deep learning framework, called Enhancer-MDLF, employing multi-module inputs to identify the cell-specific enhancers.</p>
          </list-item>
          <list-item>
            <p>We confirm that Enhancer-MDLF substantially outperforms state-of-the-art predictors without the need for parameter tuning.</p>
          </list-item>
          <list-item>
            <p>We incorporate three transfer learning strategies into our model to address the challenges posed by enhancer specificity for cross-cell lineage prediction.</p>
          </list-item>
          <list-item>
            <p>We analyze the conservation and specificity of enhancers at the motif level, exploring the most important TFBS motifs within enhancer regions utilizing the SHAP framework.</p>
          </list-item>
        </list>
      </sec>
    </boxed-text>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>Supplementary_Information_bbae083</label>
      <media xlink:href="supplementary_information_bbae083.docx"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>ACKNOWLEDGMENTS</title>
    <p>We thank members of the group for their valuable discussions and comments. The scientific calculations in this study have been done on the HPC Cloud Platform of Shandong University.</p>
  </ack>
  <sec id="sec50a">
    <title>FUNDING</title>
    <p>This work is supported by the National Natural Science Foundation of China (Grant No. 62272278 and 61972322), the National Key Research and Development Program (Grant No. 2021YFF0704103) and the Fundamental Research Funds of Shandong University. The funders did not play any role in the design of the study, the collection, analysis and interpretation of data or the writing of the manuscript.</p>
  </sec>
  <sec id="sec50b">
    <title>AUTHOR CONTRIBUTIONS STATEMENT</title>
    <p>H.W., Y.Z. and P.Z. conceived the experiments. Y.Z. and P.Z. conducted and analyzed the experiments. Y.Z. and P.Z. wrote the manuscript. H.W. reviewed the manuscript.</p>
  </sec>
  <notes id="bio1">
    <title>Author Biographies</title>
    <p><bold>Yao Zhang</bold> is currently a graduate student at the School of Software, Shandong University. Her research interests focus on bioinformatics and artificial intelligence.</p>
    <sec sec-type="author-bio" id="sec23b">
      <p><bold>Pengyu Zhang</bold> is currently pursuing a PhD degree at Xi'an Jiaotong University, China, with his research interests focused on computational biology, genomics, bioinformatics and deep learning.</p>
    </sec>
    <sec sec-type="author-bio" id="sec23c">
      <p><bold>Hao Wu</bold> is currently an associate professor at the School of Software, Shandong University. His research interests include artificial intelligence, data mining, and biomedical big data mining.</p>
    </sec>
  </notes>
  <ref-list id="bib1">
    <title>References</title>
    <ref id="ref1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pennacchio</surname><given-names>LA</given-names></string-name>, <string-name><surname>Bickmore</surname><given-names>W</given-names></string-name>, <string-name><surname>Dean</surname><given-names>A</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Enhancers: five essential questions</article-title>. <source>Nat Rev Genet</source>  <year>2013</year>;<volume>14</volume>(<issue>4</issue>):<fpage>288</fpage>–<lpage>95</lpage>.<pub-id pub-id-type="pmid">23503198</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Omar</surname><given-names>N</given-names></string-name>, <string-name><surname>Wong</surname><given-names>YS</given-names></string-name>, <string-name><surname>Li</surname><given-names>X</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Enhancer prediction in proboscis monkey genome:a comparative study</article-title>. <source>J Telecommun Electron Comput Eng</source>  <year>2017</year>;<volume>9</volume>(<issue>2–9</issue>):<fpage>175</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="ref3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ong</surname><given-names>C</given-names></string-name>, <string-name><surname>Corces</surname><given-names>vG.</given-names></string-name></person-group>  <article-title>Enhancer function: new insights into the regulation of tissue-specific gene expression</article-title>. <source>Nat Rev Genet</source>  <year>2011</year>;<volume>12</volume>(<issue>4</issue>):<fpage>283</fpage>–<lpage>93</lpage>.<pub-id pub-id-type="pmid">21358745</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>X</given-names></string-name>, <string-name><surname>Si</surname><given-names>J</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Y</given-names></string-name>, <string-name><surname>DeWille</surname><given-names>JW</given-names></string-name></person-group>. <article-title>Ccaat/enhancer binding protein-delta (c/ebp-delta) regulates cell growth, migration and differentiation</article-title>. <source>Cancer Cell Int</source>  <year>2010</year>;<volume>10</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>11</lpage>.<pub-id pub-id-type="pmid">20142996</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herz</surname><given-names>H</given-names></string-name></person-group>. <article-title>Enhancer deregulation in cancer and other diseases</article-title>. <source>Bioessays</source>  <year>2016</year>;<volume>38</volume>(<issue>10</issue>):<fpage>1003</fpage>–<lpage>15</lpage>.<pub-id pub-id-type="pmid">27570183</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Corradin</surname><given-names>O</given-names></string-name>, <string-name><surname>Scacheri</surname><given-names>PC</given-names></string-name></person-group>. <article-title>Enhancer variants: evaluating functions in common disease</article-title>. <source>Genome Med</source>  <year>2014</year>;<volume>6</volume>(<issue>10</issue>):<fpage>1</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">24433494</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moore</surname><given-names>JE</given-names></string-name>, <string-name><surname>Purcaro</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Pratt</surname><given-names>HE</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Expanded encyclopaedias of DNA elements in the human and mouse genomes</article-title>. <source>Nature</source>  <year>2020</year>;<volume>583</volume>(<issue>7818</issue>):<fpage>699</fpage>–<lpage>710</lpage>.<pub-id pub-id-type="pmid">32728249</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Finucane</surname><given-names>HK</given-names></string-name>, <string-name><surname>Bulik-Sullivan</surname><given-names>B</given-names></string-name>, <string-name><surname>Gusev</surname><given-names>A</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Partitioning heritability by functional annotation using genome-wide association summary statistics</article-title>. <source>Nat Genet</source>  <year>2015</year>;<volume>47</volume>(<issue>11</issue>):<fpage>1228</fpage>–<lpage>35</lpage>.<pub-id pub-id-type="pmid">26414678</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koido</surname><given-names>M</given-names></string-name>, <string-name><surname>Hon</surname><given-names>CC</given-names></string-name>, <string-name><surname>Koyama</surname><given-names>S</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Prediction of the cell-type-specific transcription of non-coding RNAs from genome sequences via machine learning</article-title>. <source>Nat Biomed Eng</source>  <year>2023</year>;<volume>7</volume>(<issue>6</issue>):<fpage>830</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">36411359</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Woolfe</surname><given-names>A</given-names></string-name>, <string-name><surname>Goodson</surname><given-names>M</given-names></string-name>, <string-name><surname>Goode</surname><given-names>DK</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Highly conserved non-coding sequences are associated with vertebrate development</article-title>. <source>PLoS Biol</source>  <year>2005</year>;<volume>3</volume>(<issue>1</issue>):e7.</mixed-citation>
    </ref>
    <ref id="ref11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pennacchio</surname><given-names>LA</given-names></string-name>, <string-name><surname>Ahituv</surname><given-names>N</given-names></string-name>, <string-name><surname>Moses</surname><given-names>AM</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>In vivo enhancer analysis of human conserved non-coding sequences</article-title>. <source>Nature</source>  <year>2006</year>;<volume>444</volume>(<issue>7118</issue>):<fpage>499</fpage>–<lpage>502</lpage>.<pub-id pub-id-type="pmid">17086198</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>X</given-names></string-name>, <string-name><surname>Xu</surname><given-names>H</given-names></string-name>, <string-name><surname>Yuan</surname><given-names>P</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Integration of external signaling pathways with the Core transcriptional network in embryonic stem cells</article-title>. <source>Cell</source>  <year>2008</year>;<volume>133</volume>(<issue>6</issue>):<fpage>1106</fpage>–<lpage>17</lpage>.<pub-id pub-id-type="pmid">18555785</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Visel</surname><given-names>A</given-names></string-name>, <string-name><surname>Blow</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Li</surname><given-names>Z</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Chip-seq accurately predicts tissue-specific activity of enhancers</article-title>. <source>Nature</source>  <year>2009</year>;<volume>457</volume>(<issue>7231</issue>):<fpage>854</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">19212405</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dorschner</surname><given-names>MO</given-names></string-name>, <string-name><surname>Hawrylycz</surname><given-names>M</given-names></string-name>, <string-name><surname>Humbert</surname><given-names>R</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>High-throughput localization of functional elements by quantitative chromatin profiling</article-title>. <source>Nat Methods</source>  <year>2004</year>;<volume>1</volume>(<issue>3</issue>):<fpage>219</fpage>–<lpage>25</lpage>.<pub-id pub-id-type="pmid">15782197</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giresi</surname><given-names>PG</given-names></string-name>, <string-name><surname>Kim</surname><given-names>J</given-names></string-name>, <string-name><surname>McDaniell</surname><given-names>RM</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>FAIRE (formaldehyde-assisted isolation of regulatory elements) isolates active regulatory elements from human chromatin</article-title>. <source>Genome Res</source>  <year>2007</year>;<volume>17</volume>(<issue>6</issue>):<fpage>877</fpage>–<lpage>85</lpage>.<pub-id pub-id-type="pmid">17179217</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buenrostro</surname><given-names>JD</given-names></string-name>, <string-name><surname>Giresi</surname><given-names>PG</given-names></string-name>, <string-name><surname>Zaba</surname><given-names>LC</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Transposition of native chromatin for fast and sensitive epigenomic profiling of open chromatin, DNA-binding proteins and nucleosome position</article-title>. <source>Nat Methods</source>  <year>2013</year>;<volume>10</volume>(<issue>12</issue>):<fpage>1213</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">24097267</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heintzman</surname><given-names>ND</given-names></string-name>, <string-name><surname>Stuart</surname><given-names>RK</given-names></string-name>, <string-name><surname>Hon</surname><given-names>G</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Distinct and predictive chromatin signatures of transcriptional promoters and enhancers in the human genome</article-title>. <source>Nat Genet</source>  <year>2007</year>;<volume>39</volume>(<issue>3</issue>):<fpage>311</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">17277777</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andersson</surname><given-names>R</given-names></string-name>, <string-name><surname>Gebhard</surname><given-names>C</given-names></string-name>, <string-name><surname>Miguel-Escalada</surname><given-names>I</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>An atlas of active enhancers across human cell types and tissues</article-title>. <source>Nature</source>  <year>2014</year>;<volume>507</volume>(<issue>7493</issue>):<fpage>455</fpage>–<lpage>61</lpage>.<pub-id pub-id-type="pmid">24670763</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B</given-names></string-name>, <string-name><surname>Fang</surname><given-names>L</given-names></string-name>, <string-name><surname>Long</surname><given-names>R</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ienhancer-2l: a two-layer predictor for identifying enhancers and their strength by pseudo k-tuple nucleotide composition</article-title>. <source>Bioinformatics</source>  <year>2016</year>;<volume>32</volume>(<issue>3</issue>):<fpage>362</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">26476782</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jia</surname><given-names>C</given-names></string-name>, <string-name><surname>He</surname><given-names>W</given-names></string-name></person-group>. <article-title>Enhancerpred: a predictor for discovering enhancers based on the combination and selection of multiple features</article-title>. <source>Sci Rep</source>  <year>2016</year>;<volume>6</volume>(<issue>1</issue>):<fpage>38741</fpage>.<pub-id pub-id-type="pmid">27941893</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B</given-names></string-name>, <string-name><surname>Li</surname><given-names>K</given-names></string-name>, <string-name><surname>Huang</surname><given-names>D</given-names></string-name>, <string-name><surname>Chou</surname><given-names>KC</given-names></string-name></person-group>. <article-title>Ienhancer-el: identifying enhancers and their strength with ensemble learning approach</article-title>. <source>Bioinformatics</source>  <year>2018</year>;<volume>34</volume>(<issue>22</issue>):<fpage>3835</fpage>–<lpage>42</lpage>.<pub-id pub-id-type="pmid">29878118</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nguyen</surname><given-names>QH</given-names></string-name>, <string-name><surname>Nguyen-Vo</surname><given-names>T</given-names></string-name>, <string-name><surname>Le</surname><given-names>NQK</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ienhancer-ecnn: identifying enhancers and their strength using ensembles of convolutional neural networks</article-title>. <source>BMC Genomics</source>  <year>2019</year>;<volume>20</volume>:<fpage>1</fpage>–<lpage>10</lpage>.<pub-id pub-id-type="pmid">30606130</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Le</surname><given-names>NQK</given-names></string-name>, <string-name><surname>Ho</surname><given-names>Q</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>T</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Transformer architecture based on bert and 2d convolutional neural network to identify dna enhancers from sequence information</article-title>. <source>Brief Bioinform</source>  <year>2021</year>;<volume>22</volume>.</mixed-citation>
    </ref>
    <ref id="ref24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niu</surname><given-names>K</given-names></string-name>, <string-name><surname>Luo</surname><given-names>X</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>S</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ienhancer-eblstm: identifying enhancers and strengths by ensembles of bidirectional long short-term memory</article-title>. <source>Front Genet</source>  <year>2021</year>;<volume>12</volume>:665498.</mixed-citation>
    </ref>
    <ref id="ref25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cai</surname><given-names>L</given-names></string-name>, <string-name><surname>Ren</surname><given-names>X</given-names></string-name>, <string-name><surname>Fu</surname><given-names>X</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ienhancer-xg: interpretable sequence-based enhancers and their strength predictor</article-title>. <source>Bioinformatics</source>  <year>2021</year>;<volume>37</volume>(<issue>8</issue>):<fpage>1060</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">33119044</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bai</surname><given-names>X</given-names></string-name>, <string-name><surname>Shi</surname><given-names>S</given-names></string-name>, <string-name><surname>Ai</surname><given-names>B</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Endb: a manually curated database of experimentally supported enhancers for human and mouse</article-title>. <source>Nucleic Acids Res</source>  <year>2020</year>;<volume>48</volume>(<issue>D1</issue>):<fpage>D51</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">31665430</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heinz</surname><given-names>S</given-names></string-name>, <string-name><surname>Romanoski</surname><given-names>CE</given-names></string-name>, <string-name><surname>Benner</surname><given-names>C</given-names></string-name>, <string-name><surname>Glass</surname><given-names>CK</given-names></string-name></person-group>. <article-title>The selection and function of cell type-specific enhancers</article-title>. <source>Nat Rev Mol Cell Biol</source>  <year>2015</year>;<volume>16</volume>(<issue>3</issue>):<fpage>144</fpage>–<lpage>54</lpage>.<pub-id pub-id-type="pmid">25650801</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Basith</surname><given-names>S</given-names></string-name>, <string-name><surname>Hasan</surname><given-names>MM</given-names></string-name>, <string-name><surname>Lee</surname><given-names>G</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Manavalan, integrative machine learning framework for the identification of cell-specific enhancers from the human genomes</article-title>. <source>Brief Bioinform</source>  <year>2021</year>;<volume>22</volume>.</mixed-citation>
    </ref>
    <ref id="ref29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname><given-names>T</given-names></string-name>, <string-name><surname>Qian</surname><given-names>J</given-names></string-name></person-group>. <article-title>Enhanceratlas 2.0: an updated resource with enhancer annotation in 586 tissue/cell types across nine species</article-title>. <source>Nucleic Acids Res</source>  <year>2020</year>;<volume>48</volume>(<issue>D1</issue>):<fpage>D58</fpage>–<lpage>64</lpage>.<pub-id pub-id-type="pmid">31740966</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>P</given-names></string-name>, <string-name><surname>Boisson</surname><given-names>B</given-names></string-name>, <string-name><surname>Stenson</surname><given-names>PD</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Seqtailor: a user-friendly webserver for the extraction of dna or protein sequences from next- generation sequencing data</article-title>. <source>Nucleic Acids Res</source>  <year>2019</year>;<volume>47</volume>(<issue>W1</issue>):<fpage>W623</fpage>–<lpage>31</lpage>.<pub-id pub-id-type="pmid">31045209</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dao</surname><given-names>F</given-names></string-name>, <string-name><surname>Lv</surname><given-names>H</given-names></string-name>, <string-name><surname>Su</surname><given-names>W</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Idhs-deep: an integrated tool for predicting dnase i hypersensitive sites by deep neural network</article-title>. <source>Brief Bioinform</source>  <year>2021</year>;<volume>22</volume>.</mixed-citation>
    </ref>
    <ref id="ref32">
      <label>32.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Xhafa</surname><given-names>F</given-names></string-name></person-group>. <article-title>Lecture Notes on Data Engineering and Communications Technologies</article-title>. Cham, Germany: Springer, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>P</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>H</given-names></string-name>, <string-name><surname>Wu</surname><given-names>H</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ipro-wael: a comprehensive and robust framework for identifying promoters in multiple species</article-title>. <source>Nucleic Acids Res</source>  <year>2022</year>;<volume>50</volume>(<issue>18</issue>):<fpage>10278</fpage>–<lpage>89</lpage>.<pub-id pub-id-type="pmid">36161334</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref34">
      <label>34.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ng</surname><given-names>P</given-names></string-name></person-group>. <article-title>dna2vec: consistent vector representations of variable-length k-mers</article-title>. <comment>arXiv preprint arXiv 2017;1701.06279</comment>.</mixed-citation>
    </ref>
    <ref id="ref35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Latchman</surname><given-names>DS</given-names></string-name></person-group>. <article-title>Transcription factors: an overview</article-title>. <source>Int J Biochem Cell Biol</source>  <year>1997</year>;<volume>29</volume>(<issue>12</issue>):<fpage>1305</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">9570129</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kulakovskiy</surname><given-names>IV</given-names></string-name>, <string-name><surname>Vorontsov</surname><given-names>IE</given-names></string-name>, <string-name><surname>Yevshin</surname><given-names>IS</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Hocomoco: towards a complete collection of transcription factor binding models for human and mouse via large-scale chip-seq analysis</article-title>. <source>Nucleic Acids Res</source>  <year>2018</year>;<volume>46</volume>(<issue>D1</issue>):<fpage>D252</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">29140464</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref37">
      <label>37.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>T</given-names></string-name>, <string-name><surname>Goyal</surname><given-names>P</given-names></string-name>, <string-name><surname>Girshick</surname><given-names>R</given-names></string-name>, <etal>et al</etal></person-group>  <article-title>Focal loss for dense object detection</article-title>. <italic toggle="yes">in:</italic><source>Proceedings of the IEEE international conference on computer vision</source>. Venice, Italy: IEEE, <year>2017</year>;<fpage>2980</fpage>–<lpage>2988</lpage>.</mixed-citation>
    </ref>
    <ref id="ref38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cai</surname><given-names>J</given-names></string-name>, <string-name><surname>Wang</surname><given-names>S</given-names></string-name>, <string-name><surname>Xu</surname><given-names>C</given-names></string-name>, <string-name><surname>Guo</surname><given-names>W</given-names></string-name></person-group>. <article-title>Unsupervised deep clustering via contractive feature representation and focal loss</article-title>. <source>Pattern Recognit</source>  <year>2022</year>;<volume>123</volume>:<fpage>108386</fpage>.</mixed-citation>
    </ref>
    <ref id="ref39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tran</surname><given-names>GS</given-names></string-name>, <string-name><surname>Nghiem</surname><given-names>TP</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>VT</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Improving accuracy of lung nodule classification using deep learning with focal loss</article-title>. <source>J Healthcare Eng</source>  <year>2019</year>;<volume>2019</volume>:<fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="ref40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pang</surname><given-names>B</given-names></string-name>, <string-name><surname>Nijkamp</surname><given-names>E</given-names></string-name>, <string-name><surname>Wu</surname><given-names>YN</given-names></string-name></person-group>. <article-title>Deep learning with tensorflow: a review[J]</article-title>. <source>JEduc Behav Stat</source>  <year>2020</year>;<volume>45</volume>(<issue>2</issue>):<fpage>227</fpage>–<lpage>48</lpage>.</mixed-citation>
    </ref>
    <ref id="ref41">
      <label>41.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>X</given-names></string-name>, <string-name><surname>Shi</surname><given-names>Z</given-names></string-name>, <string-name><surname>Wu</surname><given-names>Y</given-names></string-name>, <etal>et al</etal></person-group>  <article-title>schicsc: A novel single-cell hi-c clustering framework by contact-weight-based smoothing and feature fusion</article-title>. In: <source>2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM).</source> Las Vegas, NV: IEEE, <year>2022</year>;<fpage>44</fpage>–<lpage>50</lpage>.</mixed-citation>
    </ref>
    <ref id="ref42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname><given-names>L</given-names></string-name>, <string-name><surname>Yuan</surname><given-names>R</given-names></string-name>, <string-name><surname>Han</surname><given-names>C</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Cellenboost: a boosting-based ligand-receptor interaction identification model for cell-to-cell communication inference</article-title>. <source>IEEE Trans Nanobioscience</source>  <year>2023</year>;<volume>22</volume>:<fpage>705</fpage>–<lpage>15</lpage>.<pub-id pub-id-type="pmid">37216267</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>P</given-names></string-name>, <string-name><surname>Wu</surname><given-names>H</given-names></string-name></person-group>. <article-title>Ichrom-deep: an attention-based deep learning model for identifying chromatin interactions</article-title>. <source>IEEE J Biomed Health Inform</source>  <year>2023</year>;<volume>27</volume>:<fpage>4559</fpage>–<lpage>68</lpage>.<pub-id pub-id-type="pmid">37402191</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>H</given-names></string-name>, <string-name><surname>Li</surname><given-names>D</given-names></string-name>, <string-name><surname>Wu</surname><given-names>H</given-names></string-name></person-group>. <article-title>Lnclocator-imb: an imbalance-tolerant ensemble deep learning framework for predicting Long non-coding RNA subcellular localization[J]</article-title>. <source>IEEE J Biomed Health Inform</source>  <year>2023</year>;<volume>28</volume>(<issue>1</issue>):<fpage>538</fpage>–<lpage>47</lpage>.</mixed-citation>
    </ref>
    <ref id="ref45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ong</surname><given-names>C</given-names></string-name>, <string-name><surname>Corces</surname><given-names>VG</given-names></string-name></person-group>. <article-title>Enhancers: emerging roles in cell fate specification</article-title>. <source>EMBO Rep</source>  <year>2012</year>;<volume>13</volume>(<issue>5</issue>):<fpage>423</fpage>–<lpage>30</lpage>.<pub-id pub-id-type="pmid">22491032</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weiss</surname><given-names>K</given-names></string-name>, <string-name><surname>Khoshgoftaar</surname><given-names>TM</given-names></string-name>, <string-name><surname>Wang</surname><given-names>D</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>A survey of transfer learning</article-title>. <source>J Big Data</source>  <year>2016</year>;<volume>3</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="ref47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hu</surname><given-names>J</given-names></string-name>, <string-name><surname>Zhong</surname><given-names>Y</given-names></string-name>, <string-name><surname>Shang</surname><given-names>X</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>A versatile and scalable single-cell data integration algorithm based on domain-adversarial and variational approximation</article-title>. <source>Brief Bioinform</source>  <year>2022</year>;<volume>23</volume>.</mixed-citation>
    </ref>
    <ref id="ref48">
      <label>48.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhuang</surname><given-names>Z</given-names></string-name>, <string-name><surname>Shen</surname><given-names>X</given-names></string-name>, <string-name><surname>Pan</surname><given-names>W</given-names></string-name></person-group>. <article-title>A simple convolutional neural network for prediction of enhancer–promoter interactions with dna sequence data</article-title>. <source>Bioinformatics</source>  <year>2019</year>;<volume>35</volume>(<issue>17</issue>):<fpage>2899</fpage>–<lpage>906</lpage>.<pub-id pub-id-type="pmid">30649185</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koch</surname><given-names>F</given-names></string-name>, <string-name><surname>Fenouil</surname><given-names>R</given-names></string-name>, <string-name><surname>Gut</surname><given-names>M</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Transcription initiation platforms and gtf recruitment at tissue-specific enhancers and promoters</article-title>. <source>Nat Struct Mol Biol</source>  <year>2011</year>;<volume>18</volume>(<issue>8</issue>):<fpage>956</fpage>–<lpage>63</lpage>.<pub-id pub-id-type="pmid">21765417</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Y</given-names></string-name>, <string-name><surname>Pai</surname><given-names>AA</given-names></string-name>, <string-name><surname>Herudek</surname><given-names>J</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Principles for rna metabolism and alternative transcription initiation within closely spaced promoters</article-title>. <source>Nat Genet</source>  <year>2016</year>;<volume>48</volume>(<issue>9</issue>):<fpage>984</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">27455346</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname><given-names>M</given-names></string-name>, <string-name><surname>Bai</surname><given-names>X</given-names></string-name>, <string-name><surname>Ai</surname><given-names>B</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Tf-marker: a comprehensive manually curated database for transcription factors and related markers in specific cell and tissue types in human</article-title>. <source>Nucleic Acids Res</source>  <year>2022</year>;<volume>50</volume>(<issue>D1</issue>):<fpage>D402</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">34986601</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref52">
      <label>52.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname><given-names>T</given-names></string-name>, <string-name><surname>He</surname><given-names>B</given-names></string-name>, <string-name><surname>Liu</surname><given-names>S</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Enhanceratlas: a resource for enhancer annotation and analysis in 105 human cell/tissue types</article-title>. <source>Bioinformatics</source>  <year>2016</year>;<volume>32</volume>(<issue>23</issue>):<fpage>3543</fpage>–<lpage>51</lpage>.<pub-id pub-id-type="pmid">27515742</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref53">
      <label>53.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lundberg</surname><given-names>SM</given-names></string-name>, <string-name><surname>Lee</surname><given-names>S</given-names></string-name></person-group>. <article-title>A unified approach to interpreting model predictions</article-title>. <source>Advances in neural information processing systems</source>. Long Beach, CA, USA: NeurIPS Foundation, <year>2017</year>;<volume>30</volume>.</mixed-citation>
    </ref>
    <ref id="ref54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>P</given-names></string-name>, <string-name><surname>Wu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>H</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Clnn-loop: a deep learning model to predict ctcf-mediated chromatin loops in the different cell lines and ctcf-binding sites (cbs) pair types</article-title>. <source>Bioinformatics</source>  <year>2022</year>;<volume>38</volume>(<issue>19</issue>):<fpage>4497</fpage>–<lpage>504</lpage>.<pub-id pub-id-type="pmid">35997565</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname><given-names>J</given-names></string-name>, <string-name><surname>Hoshino</surname><given-names>H</given-names></string-name>, <string-name><surname>Takaku</surname><given-names>K</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Hemoprotein bach1 regulates enhancer availability of heme oxygenase-1 gene</article-title>. <source>EMBO J</source>  <year>2002</year>;<volume>21</volume>(<issue>19</issue>):<fpage>5216</fpage>–<lpage>24</lpage>.<pub-id pub-id-type="pmid">12356737</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref56">
      <label>56.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>X</given-names></string-name>, <string-name><surname>Xing</surname><given-names>G</given-names></string-name>, <string-name><surname>Fraizer</surname><given-names>GC</given-names></string-name>, <string-name><surname>Saunders</surname><given-names>GF</given-names></string-name></person-group>. <article-title>Transactivation of an intronic hematopoietic-specific enhancer of the human wilms’ tumor 1 gene by Gata-1 and c-myb</article-title>. <source>J Biol Chem</source>  <year>1997</year>;<volume>272</volume>(<issue>46</issue>):<fpage>29272</fpage>–<lpage>80</lpage>.<pub-id pub-id-type="pmid">9361007</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref57">
      <label>57.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nolis</surname><given-names>IK</given-names></string-name>, <string-name><surname>McKay</surname><given-names>DJ</given-names></string-name>, <string-name><surname>Mantouvalou</surname><given-names>E</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Transcription factors mediate long-rang enhancer–promoter interactions</article-title>. <source>Proc Natl Acad Sci</source>  <year>2009</year>;<volume>106</volume>(<issue>48</issue>):<fpage>20222</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">19923429</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref58">
      <label>58.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whalen</surname><given-names>S</given-names></string-name>, <string-name><surname>Truty</surname><given-names>RM</given-names></string-name>, <string-name><surname>Pollard</surname><given-names>KS</given-names></string-name></person-group>. <article-title>Enhancer–promoter interactions are encoded by complex genomic signatures on looping chromatin</article-title>. <source>Nature geneticss</source>  <year>2016</year>;<volume>48</volume>(<issue>5</issue>):<fpage>488</fpage>–<lpage>96</lpage>.</mixed-citation>
    </ref>
    <ref id="ref59">
      <label>59.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weintraub</surname><given-names>AS</given-names></string-name>, <string-name><surname>Li</surname><given-names>CH</given-names></string-name>, <string-name><surname>Zamudio</surname><given-names>AV</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>YY1 is a structural regulator of enhancer-promoter loops[J]</article-title>. <source>Cell</source>  <year>2017</year>;<volume>171</volume>(<issue>7</issue>):<fpage>1573</fpage>–<lpage>1588.e28</lpage>.<pub-id pub-id-type="pmid">29224777</pub-id>
</mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Brief Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Brief Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">bib</journal-id>
    <journal-title-group>
      <journal-title>Briefings in Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1467-5463</issn>
    <issn pub-type="epub">1477-4054</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10938904</article-id>
    <article-id pub-id-type="doi">10.1093/bib/bbae083</article-id>
    <article-id pub-id-type="publisher-id">bbae083</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Problem Solving Protocol</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Enhancer-MDLF: a novel deep learning framework for identifying cell-specific enhancers</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0009-0007-4698-7786</contrib-id>
        <name>
          <surname>Zhang</surname>
          <given-names>Yao</given-names>
        </name>
        <aff><institution>School of Software, Shandong University</institution>, <addr-line>Jinan, 250100, Shandong</addr-line>, <country country="CN">China</country></aff>
        <xref rid="afn1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-8696-4983</contrib-id>
        <name>
          <surname>Zhang</surname>
          <given-names>Pengyu</given-names>
        </name>
        <aff><institution>College of Information Engineering, Northwest A&amp;F University</institution>, <addr-line>Yangling, 712100, Shaanxi</addr-line>, <country country="CN">China</country></aff>
        <xref rid="afn1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-2340-9258</contrib-id>
        <name>
          <surname>Wu</surname>
          <given-names>Hao</given-names>
        </name>
        <!--haowu@sdu.edu.cn-->
        <aff><institution>School of Software, Shandong University</institution>, <addr-line>Jinan, 250100, Shandong</addr-line>, <country country="CN">China</country></aff>
        <xref rid="cor1" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="cor1">Corresponding author. Hao Wu, School of Software, Shandong University, Jinan, 250100, Shandong, China. Tel.:+86-18254105536; Fax:+86-0531-88391686; E-mail: <email>haowu@sdu.edu.cn</email></corresp>
      <fn id="afn1">
        <p>Yao Zhang and Pengyu Zhang contributed equally to this work.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2024-03-13">
      <day>13</day>
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>3</month>
      <year>2024</year>
    </pub-date>
    <volume>25</volume>
    <issue>2</issue>
    <elocation-id>bbae083</elocation-id>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>8</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>27</day>
        <month>1</month>
        <year>2024</year>
      </date>
      <date date-type="accepted">
        <day>7</day>
        <month>2</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2024</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="bbae083.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>Enhancers, noncoding DNA fragments, play a pivotal role in gene regulation, facilitating gene transcription. Identifying enhancers is crucial for understanding genomic regulatory mechanisms, pinpointing key elements and investigating networks governing gene expression and disease-related mechanisms. Existing enhancer identification methods exhibit limitations, prompting the development of our novel multi-input deep learning framework, termed Enhancer-MDLF. Experimental results illustrate that Enhancer-MDLF outperforms the previous method, Enhancer-IF, across eight distinct human cell lines and exhibits superior performance on generic enhancer datasets and enhancer–promoter datasets, affirming the robustness of Enhancer-MDLF. Additionally, we introduce transfer learning to provide an effective and potential solution to address the prediction challenges posed by enhancer specificity. Furthermore, we utilize model interpretation to identify transcription factor binding site motifs that may be associated with enhancer regions, with important implications for facilitating the study of enhancer regulatory mechanisms. The source code is openly accessible at <ext-link xlink:href="https://github.com/HaoWuLab-Bioinformatics/Enhancer-MDLF" ext-link-type="uri">https://github.com/HaoWuLab-Bioinformatics/Enhancer-MDLF</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>DNA sequence</kwd>
      <kwd>cell-specific enhancers</kwd>
      <kwd>deep learning</kwd>
      <kwd>transfer learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>62272278</award-id>
        <award-id>61972322</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Key Research and Development Program</institution>
            <institution-id institution-id-type="DOI">10.13039/501100012166</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>2021YFF0704103</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Fundamental Research Funds of Shandong University</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="12"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>INTRODUCTION</title>
    <p>Enhancers, noncoding fragments within DNA sequences, play a pivotal role in regulating gene transcription [<xref rid="ref1" ref-type="bibr">1</xref>, <xref rid="ref2" ref-type="bibr">2</xref>]. As a class of regulatory elements, enhancers exert control over diverse cellular activities, including tissue-specific gene expression [<xref rid="ref3" ref-type="bibr">3</xref>], cell growth and differentiation [<xref rid="ref4" ref-type="bibr">4</xref>] and cell carcinogenesis [<xref rid="ref5" ref-type="bibr">5</xref>]. Mutations or abnormal expression of enhancers can disrupt gene regulatory networks, thereby affecting cellular function, tissue development and disease progression [<xref rid="ref6" ref-type="bibr">6</xref>]. Many recent studies have found that the genetic mechanisms of complex diseases can be better revealed by understanding the role of enhancers in gene expression [<xref rid="ref7" ref-type="bibr">7–9</xref>]. Therefore, the identification of enhancers is crucial for advancing the comprehension of gene expression and regulation.</p>
    <p>High-throughput computational and experimental methods have been employed to predict enhancers. Several methods for identifying enhancers are as follows: (i) Computational Analysis Using Conserved Sequences and Transcription Factor Binding Site Data [<xref rid="ref10" ref-type="bibr">10</xref>, <xref rid="ref11" ref-type="bibr">11</xref>]. This method effectively predicts the genomic locations where known transcription factors (TFs) with binding sequence motifs are likely to interact. However, it may yield false positives by encompassing regulatory element sequences that bind TFs but do not serve as enhancers. (ii) Utilizing ChIP-seq Data for Transcription Factors and P300. ChIP-seq data for TFs can identify enhancers bound by known TFs [<xref rid="ref12" ref-type="bibr">12</xref>]. However, it cannot distinguish between enhancer and promoter regions because both can bind TFs. Furthermore, not all enhancers necessarily bind TFs. ChIP-seq data for p300 [<xref rid="ref13" ref-type="bibr">13</xref>], commonly used for enhancer prediction, faces limitations in distinguishing between active and inactive enhancers. (iii) Chromatin Accessibility-Related Data (e.g. DNase-seq [<xref rid="ref14" ref-type="bibr">14</xref>], FAIRE-seq [<xref rid="ref15" ref-type="bibr">15</xref>], ATAC-seq [<xref rid="ref16" ref-type="bibr">16</xref>]). This method relies on data related to chromatin accessibility, However, it may yield false positives by including other transcriptional regulatory elements, such as promoters, insulators and silencers. (iv) Histone Modification Data [<xref rid="ref17" ref-type="bibr">17</xref>] (e.g. H3K4me1 and H3K27ac). Using both H3K4me1, which marks active and poised enhancers, and H3K27ac, which marks associated with active regulatory regions from both promoters and enhancers to identify activated enhancer regions, this method benefits from the widespread availability of histone modification data across different species, effectively supporting various research needs. However, its drawback lies in the broad nature of histone modification data across the entire genome, hindering precise enhancer prediction. (v) Prediction based on enhancer RNA (eRNA) data [<xref rid="ref18" ref-type="bibr">18</xref>] (e.g. RNA-seq, ChAR-seq, GRO-seq and NET-seq). Enhancers transcribe eRNAs, and their locations can be predicted using eRNA data obtained through sequencing techniques. However, this method is limited in predicting enhancers that are not actively transcribed. Despite their individual merits, these experimental methods have inherent limitations, and they are both time-consuming and expensive. Therefore, it is essential to develop reliable computational tools for enhancer identification.</p>
    <p>In recent times, several computational methods for enhancer identification have been proposed, including ienhancer-2L [<xref rid="ref19" ref-type="bibr">19</xref>], Enhancerpred [<xref rid="ref20" ref-type="bibr">20</xref>], ienhancer-EL [<xref rid="ref21" ref-type="bibr">21</xref>], ienhancer-ECNN [<xref rid="ref22" ref-type="bibr">22</xref>], BERT-enhancer [<xref rid="ref23" ref-type="bibr">23</xref>], ienhancer-EBLSTM [<xref rid="ref24" ref-type="bibr">24</xref>] and ienhancer-XG [<xref rid="ref25" ref-type="bibr">25</xref>]. Notably, these methods are based on the dataset created by Liu <italic toggle="yes">et al</italic>. [<xref rid="ref19" ref-type="bibr">19</xref>]. However, two notable issues arise with this dataset. Firstly, the enhancers within it are extracted as short sequences of a fixed length (200bp), raising questions about the adaptability of these methods to unequal-length sequences and their ability to maintain optimal performance under such circumstances. Secondly, the dataset is a mixed general dataset encompassing nine cell lines, notwithstanding the established understanding that enhancers exhibit cell-specificity [<xref rid="ref26" ref-type="bibr">26</xref>, <xref rid="ref27" ref-type="bibr">27</xref>].</p>
    <p>To further investigate the cell-specific nature of enhancers, Enhancer-IF [<xref rid="ref28" ref-type="bibr">28</xref>], a framework based on integrated machine learning (ML), was proposed to identify enhancers. This framework utilizes eight cell lines with known cell-specific enhancers. Cross-cell line validation results demonstrate that a significant majority of enhancers indeed exhibit cell-type specificity. This underscores the importance of considering cell specificity in enhancer identification, a facet not fully addressed by the earlier methods relying on the general dataset approach.</p>
    <p>Despite notable advancements in enhancer identification, there exist notable limitations. Firstly, the predictive performance of Enhancer-IF for cell-specific enhancers is not ideal, which may be due to the conventional feature encoding scheme and the relatively simplistic design of the model framework. Secondly, Enhancer-IF integrates five commonly used classifiers (Random Forest, Extremely Randomized Tree, MultiLayer Perceptron, Support Vector Machine and Extreme Gradient Boosting) and employs a grid search algorithm to optimize parameters for each classifier across every cell line. Undoubtedly, this approach is time-consuming when applied to new cell lines. Furthermore, there is a lack of in-depth exploration of potential strategies to mitigate the impact of cell specificity on the overall performance of enhancer prediction. Lastly, Enhancer-IF lacks explanations for its prediction models, which is crucial for exploring transcription factor binding sites (TFBSs) motifs in enhancer regions. The absence of such interpretability hinders a comprehensive understanding of the biological insights derived from the predictions. Addressing these limitations is crucial for advancing the accuracy, efficiency and biological interpretability of enhancer prediction models.</p>
    <p>Therefore, we propose Enhancer-MDLF, a Multi-input Deep Learning Framework designed to predict cell-specific enhancers across multiple human cell lines. Our approach amalgamates word vector features derived from the human genome sequence and motif features extracted from the position weight matrix (PWM) of motifs. Through comprehensive evaluation on cell-specific datasets and other pertinent datasets, we demonstrate the superior performance of enhancer-MDLF. The principal contributions of our work include (i) the introduction of a novel deep learning framework, employing multi-module inputs for the identification of cell-specific enhancers; (ii) the substantiation of Enhancer-MDLF’s substantial outperformance relative to state-of-the-art predictors, accomplished without the need of parameter tuning; (iii) the incorporation of transfer learning into our model to address the challenges in cross-cell line predictions stemming from enhancer specificity and (iv) a meticulous analysis of the conservation and specificity of enhancers at the motif level, culminating in the identification of the most important TFBS motifs within enhancer regions. The overall framework of our study is depicted in <xref rid="f1" ref-type="fig">Figure 1</xref>.</p>
    <fig position="float" id="f1">
      <label>Figure 1</label>
      <caption>
        <p><bold>The overall flowchart of Enhancer-MDLF.</bold> (<bold>A</bold>) Sequence encoding: Enhancer-MDLF utilizes two distinct sequence encoding schemes, namely dna2vec and Motif frequency. (<bold>B</bold>) Model construction: Enhancer-MDLF is structured by integrating two pivotal modules: the dna2vec module and the motif module. (<bold>C</bold>) Model evaluation. Enhancer-MDLF undergoes thorough evaluation across multiple dimensions to assess its performance. (<bold>D</bold>) Motif analysis: Utilizing the SHAP framework, we conduct an in-depth analysis of the important features identified by Enhancer-MDLF.</p>
      </caption>
      <graphic xlink:href="bbae083f1" position="float"/>
    </fig>
  </sec>
  <sec id="sec2">
    <title>MATERIALS AND METHODS</title>
    <sec id="sec2a">
      <title>Dataset</title>
      <p>In this study, we utilize the benchmark dataset derived from Enhancer-IF [<xref rid="ref28" ref-type="bibr">28</xref>]. The dataset encompasses eight distinct cell lines, namely GM12878, HEK293, HMEC, HSMM, HUVEC, K562, NHEK and NHLF. They utilize Enhancer Atlas 2.0 [<xref rid="ref29" ref-type="bibr">29</xref>] (<ext-link xlink:href="http://www.enhanceratlas.org/indexv2.php" ext-link-type="uri">http://www.enhanceratlas.org/indexv2.php</ext-link>) to extract enhancer locations for each cell line, and their corresponding sequences are obtained through Retailor [<xref rid="ref30" ref-type="bibr">30</xref>] (<ext-link xlink:href="http://shiva.rockefeller.edu/SeqTailor/" ext-link-type="uri">http://shiva.rockefeller.edu/SeqTailor/</ext-link>). To ensure diversity, the CD-HIT software is applied to eliminate paired sequences with a similarity exceeding 60%. The construction of negative samples follows the methodology introduced by Dao <italic toggle="yes">et al</italic>. [<xref rid="ref31" ref-type="bibr">31</xref>]. Finally, a training set and an independent test set are obtained for each of the eight cell lines. For more details regarding the dataset employed in this study, please refer to <xref rid="TB1" ref-type="table">Table 1</xref>.</p>
      <table-wrap position="float" id="TB1">
        <label>Table 1</label>
        <caption>
          <p>Statistical summary of training and independent datasets for different cell lines</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" span="1"/>
            <col span="2" align="left"/>
            <col span="3" align="left"/>
            <col span="4" align="left"/>
            <col span="5" align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th align="left" rowspan="1" colspan="1">Cell lines</th>
              <th colspan="2" align="left" rowspan="1">Training</th>
              <th colspan="2" align="left" rowspan="1">Independent</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th align="left" rowspan="1" colspan="1">Positives</th>
              <th align="left" rowspan="1" colspan="1">Negatives</th>
              <th align="left" rowspan="1" colspan="1">Positives</th>
              <th align="left" rowspan="1" colspan="1">Negatives</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">GM12878</td>
              <td rowspan="1" colspan="1">2187</td>
              <td rowspan="1" colspan="1">2187</td>
              <td rowspan="1" colspan="1">1187</td>
              <td rowspan="1" colspan="1">2356</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HEK293</td>
              <td rowspan="1" colspan="1">3756</td>
              <td rowspan="1" colspan="1">3756</td>
              <td rowspan="1" colspan="1">2662</td>
              <td rowspan="1" colspan="1">5324</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HMEC</td>
              <td rowspan="1" colspan="1">3333</td>
              <td rowspan="1" colspan="1">3333</td>
              <td rowspan="1" colspan="1">1795</td>
              <td rowspan="1" colspan="1">3590</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HSMM</td>
              <td rowspan="1" colspan="1">2821</td>
              <td rowspan="1" colspan="1">2821</td>
              <td rowspan="1" colspan="1">1520</td>
              <td rowspan="1" colspan="1">3040</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HUVEC</td>
              <td rowspan="1" colspan="1">4750</td>
              <td rowspan="1" colspan="1">4750</td>
              <td rowspan="1" colspan="1">2559</td>
              <td rowspan="1" colspan="1">5118</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">K562</td>
              <td rowspan="1" colspan="1">3318</td>
              <td rowspan="1" colspan="1">3318</td>
              <td rowspan="1" colspan="1">1787</td>
              <td rowspan="1" colspan="1">3754</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NHEK</td>
              <td rowspan="1" colspan="1">2896</td>
              <td rowspan="1" colspan="1">2896</td>
              <td rowspan="1" colspan="1">1559</td>
              <td rowspan="1" colspan="1">3118</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">NHLF</td>
              <td rowspan="1" colspan="1">1462</td>
              <td rowspan="1" colspan="1">1462</td>
              <td rowspan="1" colspan="1">788</td>
              <td rowspan="1" colspan="1">1576</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec2b">
      <title>DNA sequence encoding schemes</title>
      <p>Enhancer-MDLF is a deep learning model, including a dna2vec module and a motif module. The two modules utilize different sequence encoding schemes as input, as depicted in the following sections.</p>
      <sec id="sec2b1">
        <title>dna2vec</title>
        <p>In recent times, word embedding techniques have gained significant popularity within the bioinformatics community, offering a promising solution to address the challenge posed by the similarity of kmer-based features in different sequences, even when their orders are reversed [<xref rid="ref32" ref-type="bibr">32</xref>, <xref rid="ref33" ref-type="bibr">33</xref>]. However, a notable limitation arises when employing word vector techniques for sequence coding. Typically, the training corpus for learning word vectors encompasses only one cell line dataset. This constrained corpus imparts a limited amount of information to the learned word vectors, thereby restricting their representational capacity.</p>
        <p>To address the above limitation, we utilize the pre-trained DNA vectors provided in dna2vec as a sequence encoding index. dna2vec, a method grounded in the word2vec word embedding model, is employed to compute distributed representations of variable-length k-mers within DNA sequences. Earlier investigations have substantiated that the mathematical operations applied to dna2vec vectors exhibit similarity to nucleotide concatenation [<xref rid="ref34" ref-type="bibr">34</xref>]. dna2vec employs the human genome sequence as a learning corpus for unsupervised training, employing the continuous skip-gram (Skip-gram) model in word2vec. This process results in embedding k-mers into a continuous vector space with 100 dimensions.</p>
        <p>In this study, we conducted experiments within the range of [3,8] (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>) to determine that the optimal value for k is 3. Subsequently, we utilize the pre-trained dna2vec model to obtain a 100-dimensional feature vector for each word. These feature vectors are obtained by concatenating the vectors of short sequences with a step size of 1 in the overall sequence. They are then input to the dna2vec module as sequence features. Assuming that the sequence length is denoted as L, the input dimension of the dna2vec module becomes 100<inline-formula><tex-math notation="LaTeX" id="ImEquation1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\times $\end{document}</tex-math></inline-formula> (L - k + 1). It is noteworthy that due to varying feature dimensions for unequal sequences in the dataset, adaptive pooling operations are employed to flatten the feature dimensions to 10 000 dimensions. This pre-processing step is essential for subsequent input into the deep learning model.</p>
      </sec>
      <sec id="sec2b2">
        <title>Motif frequency</title>
        <p>In dna2vec, the parameter k in k-mer is set to 3, signifying that DNA fragments are divided into short sequences with a nucleotide length of 3. Given the relatively brief nature of these sequences, addressing this limitation necessitates the utilization of longer features for capturing intricate sequence patterns. Notably, TFs play an important role in gene transcription by directly binding motifs in the genome. A previous study has successfully identified some potential TF binding within DNA sequences, particularly those inclined to bind in the enhancer regions [<xref rid="ref35" ref-type="bibr">35</xref>]. Leveraging this biological characteristic, we extract the count of TFBS motifs within each DNA sequence and convert it into a frequency representation for input to the motif module.</p>
        <p>We extract the PWM of motifs from the HOCOMOCO Human v11 database [<xref rid="ref36" ref-type="bibr">36</xref>] for sliding-scale matching to the sequence data in this dataset. Assuming that the length of the motif is denoted as <inline-formula><tex-math notation="LaTeX" id="ImEquation2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula>, the PWM is structured as a matrix with <inline-formula><tex-math notation="LaTeX" id="ImEquation3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula> rows and four columns, representing the values corresponding to each base (A, C, G and T). The process involves dividing each sequence of length <inline-formula><tex-math notation="LaTeX" id="ImEquation4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{s}$\end{document}</tex-math></inline-formula> into subsequences of length <inline-formula><tex-math notation="LaTeX" id="ImEquation5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula> with a stride of 1. Consequently, we obtain <inline-formula><tex-math notation="LaTeX" id="ImEquation6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{s}$\end{document}</tex-math></inline-formula> - <inline-formula><tex-math notation="LaTeX" id="ImEquation7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula> +1 subsequence segments, each of length <inline-formula><tex-math notation="LaTeX" id="ImEquation8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{m}$\end{document}</tex-math></inline-formula>. For each subsequence segment, we calculate the sum of the corresponding value for each base, and this sum serves as the final matching score. This score is then compared with the predetermined threshold score to determine whether this subsequence segment matches a motif. The criteria for subsequence segment comparison are defined by the following equation: </p>
        <disp-formula id="deqn01">
          <label>(1)</label>
          <tex-math notation="LaTeX" id="DmEquation1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*}&amp; Q = \sum_{i=0}^{l-1} PWM_{ij},\end{align*}\end{document}</tex-math>
        </disp-formula>
        <p>where the variable j takes on values 0, 1, 2 and 3, corresponding to the nucleotides A, C, G and T in the subsequence segments, and Q represents the matching score. Assuming P represents the <italic toggle="yes">P</italic>-value threshold score (set at <inline-formula><tex-math notation="LaTeX" id="ImEquation9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$10^{-4}$\end{document}</tex-math></inline-formula>) for respective motifs, the subsequence segment is deemed to match this motif when <inline-formula><tex-math notation="LaTeX" id="ImEquation10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$Q&gt; P$\end{document}</tex-math></inline-formula>.</p>
        <p>Upon traversing each sequence, we gather information on the count of each TFBS motif, resulting in a 401-dimensional feature vector denoted as <inline-formula><tex-math notation="LaTeX" id="ImEquation11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$V_{count}$\end{document}</tex-math></inline-formula>. Additionally, to accommodate varying sequence lengths, we utilize the feature vector <inline-formula><tex-math notation="LaTeX" id="ImEquation12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$V_{frequency}$\end{document}</tex-math></inline-formula> as the final model input, calculated through the following equation: </p>
        <disp-formula id="deqn02">
          <label>(2)</label>
          <tex-math notation="LaTeX" id="DmEquation2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*}&amp; V_{frequency} = \frac{V_{count}}{L_{s}},\end{align*}\end{document}</tex-math>
        </disp-formula>
        <p>where <inline-formula><tex-math notation="LaTeX" id="ImEquation13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$L_{s}$\end{document}</tex-math></inline-formula> represents the length of each sequence.</p>
      </sec>
    </sec>
    <sec id="sec2e">
      <title>The framework of Enhancer-MDLF</title>
      <sec id="sec2e1">
        <title>Model architecture</title>
        <p>The DNA fragments extracted from the dataset undergo encoding through two schemes, namely dna2vec and motif. Subsequently, these encoded fragments are input into the model to predict whether the fragment contains enhancer regions. Through systematic experimentation involving various combinations of convolutional layers, max-pooling layers and dense layers, along with meticulous parameter tuning for optimal balance between accuracy, efficiency and generalization capabilities, we have defined the comprehensive framework of Enhancer-MDLF, as illustrated in <xref rid="f2" ref-type="fig">Figure 2A</xref>. The details are elaborated as follows.</p>
        <fig position="float" id="f2">
          <label>Figure 2</label>
          <caption>
            <p><bold>The model architecture and training procedure of Enhancer-MDLF.</bold> (<bold>A</bold>) Enhancer-MDLF comprises a dna2vec module predominantly composed of convolutional layers and a motif module primarily consisting of Dense layers. It takes dna2vec and motif frequency, the two sequence encoding results, as inputs. The fused features are derived by concatenating after feature extraction and subsequently passed through a sigmoid function for enhancer detection. (<bold>B</bold>) The training procedure of the Enhancer-MDLF framework. The process involves iterations through the training set five times to iteratively refine the predictive model. Subsequently, this model undergoes testing on the test set to yield the final prediction results.</p>
          </caption>
          <graphic xlink:href="bbae083f2" position="float"/>
        </fig>
        <p><bold>Feature Extraction:</bold> We employ a combination of three 1D convolutional layers with corresponding 1D max-pooling layers in the dna2vec module, and three dense layers in the motif module. The convolutional layers are instrumental in capturing complex features from the inputs through convolutional computations. Simultaneously, the max-pooling layers implement a down-sampling approach, selecting the maximum value for each sub-region. This not only improves the robustness of the model but also mitigates the risk of overfitting. Specifically, we define three convolutional layers with 64 filters, a kernel size of 7 and a stride of 3. Additionally, three max-pooling layers are constructed with a pool size of 2, respectively.</p>
        <p>To further enhance the model’s generalization and prevent overfitting, we introduce a dropout layer with a probability of 0.6. This layer randomly removes certain neural network units during the training phase, contributing to the model’s overall robustness and preventing excessive adaptation to the training data.</p>
        <p><bold>Prediction:</bold> The dna2vec module undergoes computation through a dense layer comprising 500 neurons after flattening. Simultaneously, the motif module is designed to culminate in 16 neurons, with a choice of the ‘relu’ activation function for both modules. Within the motif module, we incorporate a dropout layer with a probability of 0.6 following the three dense layers. Finally, Enhancer-MDLF combines the results from the dna2vec and motif modules and generates predictive values using a dense layer with a single neuron and a ‘sigmoid’ activation function. To further enhance model generalization, a dropout layer with a probability of 0.5 is applied after concatenating the two sets of results. The classification criterion is set such that a sample is considered positive if the predicted value exceeds 0.5; otherwise, it is deemed negative.</p>
        <p><bold>Hyperparameters:</bold> The hyperparameters of Enhancer-MDLF include learning rate, batch size and maximum epoch. Following a comprehensive comparison of performance across multiple hyperparameter combinations through the grid search method, we establish the optimal settings as follows: learning rate = 0.0001, batch size = 100 and max epoch = 200. The specific details of the grid search method and the details of the search ranges for hyperparameters can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>.</p>
      </sec>
      <sec id="sec2e2">
        <title>Loss function</title>
        <p>The dataset we utilize exhibits an imbalance, a characteristic that poses challenges for traditional loss functions. Traditional approaches tend to disproportionately penalize dominant classes, often neglecting the informative contributions of minority classes in imbalanced datasets, thereby resulting in suboptimal prediction performance. Recognizing this, the focal loss [<xref rid="ref37" ref-type="bibr">37</xref>] initially employed in computer vision [<xref rid="ref38" ref-type="bibr">38</xref>, <xref rid="ref39" ref-type="bibr">39</xref>] has emerged as a solution. Consequently, we use the focal loss as the primary loss function for Enhancer-MDLF, aiming to mitigate the drawbacks associated with the traditional cross-entropy loss function in managing imbalanced datasets. The focal loss is formally defined as follows: </p>
        <disp-formula id="deqn03">
          <label>(3)</label>
          <tex-math notation="LaTeX" id="DmEquation3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*}&amp; FL(P_{t})=-\alpha(1-P_{t})^{\gamma}\log(P_{t}),\end{align*}\end{document}</tex-math>
        </disp-formula>
        <p>where <inline-formula><tex-math notation="LaTeX" id="ImEquation14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$P_{t}$\end{document}</tex-math></inline-formula> represents the predicted probability, <inline-formula><tex-math notation="LaTeX" id="ImEquation15">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula> denotes the balance parameter and <inline-formula><tex-math notation="LaTeX" id="ImEquation16">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\gamma $\end{document}</tex-math></inline-formula> is the focus parameter. To determine the optimal configuration for Enhancer-MDLF, we conduct a thorough performance evaluation using the grid search method over a range of <inline-formula><tex-math notation="LaTeX" id="ImEquation17">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula> values [0.25, 0.5, 0.75] and <inline-formula><tex-math notation="LaTeX" id="ImEquation18">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\gamma $\end{document}</tex-math></inline-formula> values [1, 2, 3]. Our findings reveal that the model achieves optimal performance when <inline-formula><tex-math notation="LaTeX" id="ImEquation19">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula>=0.5 and <inline-formula><tex-math notation="LaTeX" id="ImEquation20">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\gamma $\end{document}</tex-math></inline-formula>=3.</p>
      </sec>
      <sec id="sec2e3">
        <title>Training procedure</title>
        <p>We utilize a novel training strategy for Enhancer-MDLF to enhance its capacity to effectively learn information from the input feature vectors of the dna2vec and motif modules. First, we implement a 5-fold cross-validation algorithm, randomly partitioning the training set into five folds with a 4:1 ratio between the training and validation sets. To prevent overfitting, an early stopping mechanism is applied to the validation set. The model undergoes training five times, with the initial training parameters being the model’s starting parameters, determined by TensorFlow’s default network initialization methods [<xref rid="ref40" ref-type="bibr">40</xref>]. Xavier initialization is frequently used in TensorFlow for a variety of neural network layers, such as dense and convolutional layers. This method initializes weights randomly from a uniform or normal distribution, and the calculation of the standard deviation depends on the number of input and output units in the layer. Subsequent trainings build upon the most recent model, utilizing different folds of the training and validation sets. Following the completion of the five training sessions, the model is applied to the testing set for prediction.</p>
        <p>As shown in <xref rid="f2" ref-type="fig">Figure 2B</xref>, the workflow of Enhancer-MDLF is as follows:</p>
        <p>(i) Divide the training dataset <inline-formula><tex-math notation="LaTeX" id="ImEquation21">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{train}$\end{document}</tex-math></inline-formula> into five folds: D1, D2, D3, D4 and D5 utilizing a 5-fold cross-validation strategy. For the first training iteration, choose <inline-formula><tex-math notation="LaTeX" id="ImEquation22">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{train}$\end{document}</tex-math></inline-formula> - D1 (The remaining samples after removing D1 in <inline-formula><tex-math notation="LaTeX" id="ImEquation23">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{train}$\end{document}</tex-math></inline-formula>) as the current training set, utilizing ‘D1’ as the validation set to train the model and obtain <inline-formula><tex-math notation="LaTeX" id="ImEquation24">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$model_{1}$\end{document}</tex-math></inline-formula>.</p>
        <p>(ii) Subsequent to model1, for the second training iteration, employ <inline-formula><tex-math notation="LaTeX" id="ImEquation25">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{train}$\end{document}</tex-math></inline-formula> - D2 as the current training set and D2 as the validation set to continue training <inline-formula><tex-math notation="LaTeX" id="ImEquation26">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$model_{1}$\end{document}</tex-math></inline-formula>, obtaining <inline-formula><tex-math notation="LaTeX" id="ImEquation27">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$model_{2}$\end{document}</tex-math></inline-formula>. By analogy, repeat the training process five times to obtain the final model, denoted as <inline-formula><tex-math notation="LaTeX" id="ImEquation28">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$model_{5}$\end{document}</tex-math></inline-formula>.</p>
        <p>(iii) Evaluate the performance of the final model on the test set <inline-formula><tex-math notation="LaTeX" id="ImEquation29">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$D_{test}$\end{document}</tex-math></inline-formula>.</p>
      </sec>
    </sec>
    <sec id="sec2i">
      <title>Evaluation metrics</title>
      <p>We utilize seven evaluation metrics to assess the performance of Enhancer-MDLF and compare it with that of other methods. These metrics encompass accuracy (ACC), balanced accuracy (BACC), area under the receiver operating characteristics (AUROC), Matthews correlation coefficient (MCC), sensitivity (Sn), specificity (Sp) and F1 score. The details of these evaluation metrics are provided in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>. In general, a higher value for these metrics indicates superior model performance.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>RESULTS</title>
    <sec id="sec3a">
      <title>Performance evaluation of combinatorial module in comparison with individual modules</title>
      <p>Recent studies have underscored the superiority of leveraging multiple features over individual ones in sequence-based prediction tasks [<xref rid="ref41" ref-type="bibr">41–44</xref>]. Consequently, Enhancer-MDLF, proposed in this study, incorporates a dna2vec module and a motif module to comprehensively extract information on enhancer sequences. To assess the effectiveness of this combinatorial module in improving the model’s performance for predicting cell-specific enhancers, we individually train the dna2vec module and the motif module utilizing training sets for each cell line. Subsequently, we evaluate the performance of these modules on independent test sets and draw comparisons with the performance of the combinatorial module.</p>
      <p>Given that the datasets used in this study are imbalanced, we utilize the BACC metric rather than ACC to evaluate the performance of models. What surprised us is that Enhancer-MDLF can effectively improve the performance of the model by fusing the two modules while maintaining a reasonable computational cost (refer to <xref rid="f3" ref-type="fig">Figure 3</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S2</xref> and <xref rid="sup1" ref-type="supplementary-material">S3</xref>). All six metrics exhibit improvement across the eight cell lines. Especially, the performance of two individual modules on the HMEC cell line is unsatisfactory, while the combinatorial module greatly improves the prediction performance. Furthermore, the training time for the combined module is shorter than that for the dna2vec module alone in the HSMM and NHEK cell lines (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>), potentially due to an accelerated convergence speed facilitated by our early stopping strategy. Overall, these results indicate that leveraging multiple features indeed improves prediction performance, establishing Enhancer-MDLF as a powerful and robust tool for predicting cell-specific enhancers.</p>
      <fig position="float" id="f3">
        <label>Figure 3</label>
        <caption>
          <p><bold>Performance evaluation of combinatorial module in comparison with individual modules.</bold> Panels (<bold>A–F</bold>) depict the evaluation of model performance across eight cell lines using metrics such as AUROC, BACC, F1 score, MCC, SN and SP.</p>
        </caption>
        <graphic xlink:href="bbae083f3" position="float"/>
      </fig>
    </sec>
    <sec id="sec3b">
      <title>Performance comparison with state-of-the-art method</title>
      <p>To comprehensively assess the predictive capabilities of Enhancer-MDLF for cell-specific enhancers, we conduct a thorough comparison with Enhancer-IF, a model dedicated to cell-specific enhancer prediction, across eight cell lines. As mentioned in the introduction section, existing enhancer prediction methods predominantly rely on generic datasets, neglecting the crucial aspect of enhancer’s cell specificity. To validate the robustness of our model, we conduct performance comparisons for both methods on independent test sets and through 10-fold cross-validation (refer to <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>).</p>
      <p>As can be seen in <xref rid="f4" ref-type="fig">Figure 4</xref> and detailed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref>, Enhancer-MDLF consistently outperforms Enhancer-IF across all five metrics and all cell lines, demonstrating its significant superiority. The robust predictive performance of Enhancer-MDLF could be attributed to its unique multi-module fusion approach and a novel training strategy that incorporates comprehensive and in-depth sequence information. It is crucial to highlight that, in contrast to the training strategy of Enhancer-IF, we design a unique training strategy for Enhancer-MDLF. This strategy involves learning more distributions iteratively within a limited dataset. Our approach and Enhancer-IF employ identical input data and generate results on the same test dataset, ensuring a fair and meaningful comparison within the training process from an end-to-end perspective.</p>
      <fig position="float" id="f4">
        <label>Figure 4</label>
        <caption>
          <p><bold>Performance comparison between Enhance-MDLF and Enhancer-IF on independent test sets.</bold> Panels (<bold>A–D</bold>) present the evaluation of model performance across eight cell lines using metrics such as AUROC, BACC, MCC and SN. Panels (<bold>E–H</bold>) present the evaluation of model performance across eight cell lines using metrics such as AUROC, BACC, MCC and SN.</p>
        </caption>
        <graphic xlink:href="bbae083f4" position="float"/>
      </fig>
      <p>Notably, a pronounced performance gap is observed between the 10-fold cross-validation and independent test sets for Enhancer-MDLF on the HMEC cell line. This discrepancy may arise from the distinct data distributions between the training and test sets, posing a challenge for the model to effectively generalize information learned from the training set to the test set. However, even under these challenging conditions, our method outperforms Enhancer-IF by 9.54% on the independent test set in terms of BACC.</p>
      <p>However, to the best of our knowledge, there is no specific pattern observed in Enhancer-IF for the HMEC dataset. This lack of observation could be attributed to Enhancer-IF consistently performing within the range of 70–80% across multiple cell lines, which may imply limited knowledge acquisition, possibly concealing specific characteristics within the HMEC dataset. It is precisely due to the robust predictive capabilities of Enhancer-MDLF that these discrepant results may indicate that the enhancers within the HMEC cell line have more complex gene regulatory mechanisms. This suggests an intriguing direction for further exploration to help uncover the reasons behind enhancer cell specificity. Overall, these results demonstrate the robustness and effectiveness of Enhancer-MDLF in accurately predicting cell-specific enhancers.</p>
    </sec>
    <sec id="sec3c">
      <title>Performance evaluation across cell lines</title>
      <p>Previous studies have established that enhancers exhibit cell-specific functionalities [<xref rid="ref27" ref-type="bibr">27</xref>, <xref rid="ref45" ref-type="bibr">45</xref>]. To explore the potential relationships among enhancers across different cell lines, we conduct a comprehensive cross-cell line performance evaluation to investigate the transferability of cell-specific models. Our approach involves training models on one cell line and evaluating the performance of Enhancer-MDLF and Enhancer-IF on the test sets of seven other cell lines. The results demonstrate that Enhancer-MDLF achieves optimal performance in terms of average BACC across all eight cell lines (<xref rid="f5" ref-type="fig">Figure 5A</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S6</xref> and <xref rid="sup1" ref-type="supplementary-material">S7</xref>). Notably, the HEK293 model achieves an average BACC of 86.18% when predicting outcomes in other cell lines, indicating its capacity to transfer to other cell lines. Despite the prevalence of cell-specific enhancers observed in most instances, such as those identified in NHLF cell lines, it is important to acknowledge the existence of enhancers that exhibit significant similarities across certain cell lines. For instance, Enhancer-MDLF demonstrates satisfactory mutual predictive performance on the HUVEC and K562 cell lines, as well as on the NHEK and HMEC cell lines (<xref rid="f5" ref-type="fig">Figure 5B</xref>). These results highlight a commonality of enhancers between some cell lines. Overall, enhancers generally display specificity among most cell lines, but there exist similarities between enhancers across specific cell lines. Therefore, investigating both cell-specific and non-cell-specific enhancers emerges as a critical avenue for unveiling insights into cell specificity and differentiation.</p>
      <fig position="float" id="f5">
        <label>Figure 5</label>
        <caption>
          <p><bold>Comprehensive performance evaluation across multiple aspects.</bold> (<bold>A</bold>) Average BACC for cross-cell line prediction on eight cell lines. (<bold>B</bold>) Heat map of BACC in cross-cell line evaluation. Columns represent pre-trained models trained on different training sets, and rows represent testing on their own or other cell lines’ test sets. (<bold>C</bold>) Exploration of Transfer Learning Strategies: Three transfer learning strategies are presented to address the challenge of poor direct prediction performance in cross-cell line evaluation. (<bold>D</bold>) Performance comparison of Enhancer-MDLF with other methods on a universal dataset. (<bold>E–H</bold>) Performance comparison of Enhancer-MDLF with other methods on enhancer–promoter datasets.</p>
        </caption>
        <graphic xlink:href="bbae083f5" position="float"/>
      </fig>
    </sec>
    <sec id="sec3d">
      <title>Performance evaluation across cell lines with transfer learning</title>
      <p>The challenge of predicting enhancers across different cell lines, attributed to the inherent cell specificity of enhancers, significantly hinders the exploration of gene regulatory mechanisms. To overcome this obstacle and advance the field, we further explore potential approaches. An emerging ML technique, transfer learning, proves promising as it leverages knowledge acquired from a source domain to enhance learning performance in a target domain. Notably, this technique has demonstrated success in predicting cell-specific enhancer–promoter interactions [<xref rid="ref46" ref-type="bibr">46</xref>, <xref rid="ref47" ref-type="bibr">47</xref>]. To comprehensively evaluate the potential of transfer learning in the context of enhancer prediction, we adopt three strategies: transfer_1, transfer_2 and transfer_3. These strategies align with both traditional transfer learning methods and those previously utilized in a relevant study [<xref rid="ref48" ref-type="bibr">48</xref>]. Detailed descriptions of these strategies are provided in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>, aiming to shed light on their effectiveness in mitigating the challenges posed by the cell specificity of enhancers.</p>
      <p>We apply these three distinct transfer strategies separately to Enhancer-MDLF to validate their effectiveness and explore their practical applicability (<xref rid="f5" ref-type="fig">Figure 5C</xref>). The results, as detailed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S8</xref>, <xref rid="sup1" ref-type="supplementary-material">S9</xref> and <xref rid="sup1" ref-type="supplementary-material">S10</xref> across eight cell lines, illustrate significant insights. Enhancer-MDLF with transfer_1 demonstrates notable improvement in prediction performance, albeit falling short compared with predicting enhancers in the same cell line. However, this outcome underscores the effectiveness of transfer learning. In contrast, Enhancer-MDLF with transfer_2, leveraging additional enhancer information from multiple cell lines during the model pre-training, exhibits exceptional performance. Compared with the model directly trained with random initial weights (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S11</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2</xref>), the results indicate that pre-training the model with enhancers from diverse cell lines yields more effective predictions in new cell lines. This is attributed to the more representative initial weights acquired from the pre-trained model, highlighting its potential for accurately labeling other unannotated data, especially under the constraint of limited annotated data size. Particularly noteworthy is the observation that Enhancer-MDLF with transfer_3 exhibits optimal performance across the eight cell lines.</p>
      <p>These findings present a practical scenario for addressing enhancer-specific prediction challenges. If Enhancer-MDLF is pre-trained using as many (or even all) cell line enhancers following our transfer learning strategies, a comprehensive pre-trained model may be obtained, serving as an effective initial model for enhancer prediction tasks in various cell lines. Even with potential cost constraints in obtaining enhancers from numerous cell lines, Enhancer-MDLF with transfer_2 demonstrates robust performance with only a few cell lines’ enhancers. Overall, the utilization of transfer learning provides an effective and promising solution to overcome prediction challenges arising from enhancer specificity in practical applications. We have provided the pre-trained model, utilizing enhancers from these eight cell lines, available on GitHub for further exploration and utilization.</p>
    </sec>
    <sec id="sec3e">
      <title>Performance evaluation on other datasets</title>
      <p>To demonstrate the superiority of our approach comprehensively, we subject Enhancer-MDLF to evaluation using the generic dataset created by Liu <italic toggle="yes">et al</italic>. [<xref rid="ref21" ref-type="bibr">21</xref>], a widely used benchmark in enhancer identification tasks. To maintain comparability with prior studies, we utilize the same training set, test set and evaluation metrics. The outcomes reveal that Enhancer-MDLF achieves optimal performance on the independent test set, with MCC, SN and ACC of 0.6067, 0.84 and 0.8025, respectively (<xref rid="f5" ref-type="fig">Figure 5D</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S12</xref>). These results highlight the robustness and generality of Enhancer-MDLF, demonstrating its efficacy and versatility as a powerful tool for predicting enhancers in human cell lines.</p>
      <p>Besides, existing studies have demonstrated that enhancers and promoters exhibit similar sequence structures [<xref rid="ref49" ref-type="bibr">49</xref>, <xref rid="ref50" ref-type="bibr">50</xref>]. Leveraging datasets derived from iPro-WAEL [<xref rid="ref33" ref-type="bibr">33</xref>], we conduct an investigation to assess whether Enhancer-MDLF effectively distinguishes between enhancers and promoters. Given the limitations of some enhancer prediction methods for this dataset, as detailed in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>, we restrict our comparison to Enhancer-MDLF, ienhancer-2L and enhancer-IF. The results unequivocally showcase Enhancer-MDLF’s robust capacity to discriminate between promoters and enhancers, effectively capturing the distinct information associated with these two regulatory elements despite their similar sequence structures (<xref rid="f5" ref-type="fig">Figure 5E–H</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S14</xref>).</p>
    </sec>
    <sec id="sec3f">
      <title>Exploration of TFBS motifs in enhancer regions</title>
      <p>Investigating TFBS motifs within enhancer regions is crucial for understanding the regulatory mechanisms of TFs. TFs typically exert their transcriptional influence by recognizing specific TFBS motifs and binding to the regulatory regions of genes. Enhancers, along with the associated TFs, play a significant role in human diseases and biological processes [<xref rid="ref35" ref-type="bibr">35</xref>, <xref rid="ref51" ref-type="bibr">51</xref>, <xref rid="ref52" ref-type="bibr">52</xref>]. To identify key motifs associated with enhancers, we utilize the Shapley Additive exPlanations (SHAP) framework [<xref rid="ref53" ref-type="bibr">53</xref>, <xref rid="ref54" ref-type="bibr">54</xref>] to interpret the input features of the motif module in Enhancer-MDLF. The details of the SHAP framework are provided in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>.</p>
      <p><xref rid="f6" ref-type="fig">Figure 6</xref> displays the top 50 motifs with the most significant impact on the model’s output across eight cell lines. The x-axis represents the SHAP value, where a positive value indicates a positive effect on the model’s output, while a negative value indicates a negative effect. Taking SP2 in <xref rid="f6" ref-type="fig">Figure 6A</xref> as an example, higher feature values predominantly cluster in the region where SHAP values &gt;0, indicating that SP2 has a positive effect on predicting the samples as enhancers. It can be seen from <xref rid="f6" ref-type="fig">Figure 6</xref> that the contributions of different motifs to the model’s output vary across different cell lines, providing further insight into the cell specificity of enhancers at the motif level.</p>
      <fig position="float" id="f6">
        <label>Figure 6</label>
        <caption>
          <p><bold>Analysis of top 50 important motif features across eight cell lines.</bold> (<bold>A–H</bold>) This study analyzes the top 50 motifs on eight cell lines that impact the model’s output utilizing the SHAP framework. The x-axis represents the SHAP values that impact the model’s output. Each point on the graph represents a sample, and the color of each sample point ranges from blue to red, representing the corresponding feature values of each sample. The color transition reflects the variation in feature values from low (blue) to high (red).</p>
        </caption>
        <graphic xlink:href="bbae083f6" position="float"/>
      </fig>
      <p>Besides, we find that certain important motifs identified in our study align well with findings from previous studies. For instance, the BACH1 motif emerges exclusively among the top 50 important features in the HUVEC cell line. Previous studies have shown that the heme-binding factor BACH1 can bind to multiple Maf recognition elements in the heme oxygenase 1 (HO-1) enhancer, thereby inhibiting its activity and participating in gene regulation [<xref rid="ref55" ref-type="bibr">55</xref>]. Similarly, the GATA1 motif is uniquely present among the top 50 important features solely in the K562 cell line. Previous studies have revealed that the GATA1 protein binds to the GATA-A site in the intronic WT1 enhancer <italic toggle="yes">in vitro</italic> in K562 cells, transactivating the enhancer and markedly increasing the CAT reporter activity 10–15-fold [<xref rid="ref56" ref-type="bibr">56</xref>].</p>
      <p>To visualize the impact of these features on the model’s output, force plots of SHAP are employed, and specific details are available in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1</xref>. Additionally, to enhance the user’s interactive experience, we have provided HTML files on GitHub (<ext-link xlink:href="https://github.com/HaoWuLab-Bioinformatics/Enhancer-MDLF" ext-link-type="uri">https://github.com/HaoWuLab-Bioinformatics/Enhancer-MDLF</ext-link>). These files showcase the contributions of each feature to the model’s output on the test sets of eight cell lines. Users can interactively select various features and samples on the graph, facilitating a dynamic exploration of their influence on the model’s output.</p>
      <p>Furthermore, the SP1 motif significantly contributes to enhancer predictions across numerous cell lines, as it has been shown to regulate chromatin loops between enhancers and distal promoters, thereby influencing transcriptional activity [<xref rid="ref57" ref-type="bibr">57</xref>]. Additionally, our analysis reveals certain key motifs, such as RUNX and MAX, which have not been extensively studied but are highlighted in TargetFinder [<xref rid="ref58" ref-type="bibr">58</xref>], These results underscore that enhancers can exert their effects through the involvement of certain proteins in specific cells, emphasizing the complexity of gene regulation involving enhancers.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <title>DISCUSSION AND CONCLUSION</title>
    <p>The identification of cell-specific enhancers is of significant importance for understanding cell-specific gene regulation and deciphering tissue development. In this study, we develop Enhancer-MDLF, a deep learning framework with multi-inputs, designed for accurate identification of cell-specific enhancers by integrating the dna2vec module and motif module. Experimental analyses demonstrate that Enhancer-MDLF outperforms existing methods across multiple cell-specific enhancer datasets, and significantly outperforms previous studies on general datasets and enhancer–promoter datasets, highlighting the robustness and versatility of Enhancer-MDLF. While our evaluations are constrained by the scale of the annotated dataset to specific cell lines, various experimental results instill confidence in the generalization ability of our model to extend to broader datasets. In summary, our proposed Enhancer-MDLF emerges as a superior and efficient tool for identifying enhancers.</p>
    <p>Additionally, in the course of cross-cell-line evaluation, we observe a notable challenge wherein models trained on one cell line often exhibit unsatisfactory performance when applied to predict enhancers on other cell lines. This limitation stems from the inherent cell specificity of enhancers, suggesting that a model trained on a specific cell line achieves optimal prediction performance solely within that context, posing a substantial limitation in practical applications. To overcome this limitation, we endeavor to mitigate it through the implementation of transfer learning. The pre-trained model with transfer_2 achieves remarkably precise predictions with only a modest amount of annotated data (e.g. 292 samples, constituting 10% of the training set, in the NHLF cell line). In practical scenarios, the scarcity of annotated data in private datasets may face challenges for the effective application of supervised learning. While unsupervised learning methods, such as clustering, can partially emulate similar functionality, their accuracy often falls significantly below the standards required for practical applications. Consequently, we assert that this approach substantiates an effective strategy for addressing the challenges posed by the cell-specific nature of enhancers. Among the three transfer learning strategies employed, transfer_3, a pre-trained model trained on all cell lines, demonstrates the most promising performance. This outcome suggests that with a sufficiently extensive dataset encompassing a diverse range of cell lines as input to our model, the development of a universal pre-trained model for identifying human enhancers becomes a plausible avenue of exploration—a prospect that holds significant interest and potential in research endeavors.</p>
    <p>Moreover, we employ a computational approach to analyze potentially important TFBS motifs associated with enhancers. Our analysis reveals several motifs that have undergone extensive study in previous research, such as BACH1, GATA1, SP1, RUNX and MAX. Additionally, we identify motifs like FOXJ3 and SP2, which significantly contribute to enhancer predictions across numerous cell lines but have received limited attention in prior studies. We infer that these less-explored motifs may play roles in gene regulation through intricate and as-yet-unidentified processes. While our current research conditions constrain further validation and exploration of the relationships between these new motifs and enhancer function, we believe these findings open new avenues for future enhancer-related research in the field of biology. Notably, Whalen <italic toggle="yes">et al</italic>. [<xref rid="ref58" ref-type="bibr">58</xref>] extensively explored the critical role of YY1 in enhancer–promoter interactions by interpreting ML models. Later on, it was further confirmed that YY1 regulates enhancer–promoter chromatin loops [<xref rid="ref59" ref-type="bibr">59</xref>].</p>
    <boxed-text id="box01" position="float">
      <sec id="sec28a">
        <title>Key Points</title>
        <list list-type="bullet">
          <list-item>
            <p>We propose a novel deep learning framework, called Enhancer-MDLF, employing multi-module inputs to identify the cell-specific enhancers.</p>
          </list-item>
          <list-item>
            <p>We confirm that Enhancer-MDLF substantially outperforms state-of-the-art predictors without the need for parameter tuning.</p>
          </list-item>
          <list-item>
            <p>We incorporate three transfer learning strategies into our model to address the challenges posed by enhancer specificity for cross-cell lineage prediction.</p>
          </list-item>
          <list-item>
            <p>We analyze the conservation and specificity of enhancers at the motif level, exploring the most important TFBS motifs within enhancer regions utilizing the SHAP framework.</p>
          </list-item>
        </list>
      </sec>
    </boxed-text>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>Supplementary_Information_bbae083</label>
      <media xlink:href="supplementary_information_bbae083.docx"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>ACKNOWLEDGMENTS</title>
    <p>We thank members of the group for their valuable discussions and comments. The scientific calculations in this study have been done on the HPC Cloud Platform of Shandong University.</p>
  </ack>
  <sec id="sec50a">
    <title>FUNDING</title>
    <p>This work is supported by the National Natural Science Foundation of China (Grant No. 62272278 and 61972322), the National Key Research and Development Program (Grant No. 2021YFF0704103) and the Fundamental Research Funds of Shandong University. The funders did not play any role in the design of the study, the collection, analysis and interpretation of data or the writing of the manuscript.</p>
  </sec>
  <sec id="sec50b">
    <title>AUTHOR CONTRIBUTIONS STATEMENT</title>
    <p>H.W., Y.Z. and P.Z. conceived the experiments. Y.Z. and P.Z. conducted and analyzed the experiments. Y.Z. and P.Z. wrote the manuscript. H.W. reviewed the manuscript.</p>
  </sec>
  <notes id="bio1">
    <title>Author Biographies</title>
    <p><bold>Yao Zhang</bold> is currently a graduate student at the School of Software, Shandong University. Her research interests focus on bioinformatics and artificial intelligence.</p>
    <sec sec-type="author-bio" id="sec23b">
      <p><bold>Pengyu Zhang</bold> is currently pursuing a PhD degree at Xi'an Jiaotong University, China, with his research interests focused on computational biology, genomics, bioinformatics and deep learning.</p>
    </sec>
    <sec sec-type="author-bio" id="sec23c">
      <p><bold>Hao Wu</bold> is currently an associate professor at the School of Software, Shandong University. His research interests include artificial intelligence, data mining, and biomedical big data mining.</p>
    </sec>
  </notes>
  <ref-list id="bib1">
    <title>References</title>
    <ref id="ref1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pennacchio</surname><given-names>LA</given-names></string-name>, <string-name><surname>Bickmore</surname><given-names>W</given-names></string-name>, <string-name><surname>Dean</surname><given-names>A</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Enhancers: five essential questions</article-title>. <source>Nat Rev Genet</source>  <year>2013</year>;<volume>14</volume>(<issue>4</issue>):<fpage>288</fpage>–<lpage>95</lpage>.<pub-id pub-id-type="pmid">23503198</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Omar</surname><given-names>N</given-names></string-name>, <string-name><surname>Wong</surname><given-names>YS</given-names></string-name>, <string-name><surname>Li</surname><given-names>X</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Enhancer prediction in proboscis monkey genome:a comparative study</article-title>. <source>J Telecommun Electron Comput Eng</source>  <year>2017</year>;<volume>9</volume>(<issue>2–9</issue>):<fpage>175</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="ref3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ong</surname><given-names>C</given-names></string-name>, <string-name><surname>Corces</surname><given-names>vG.</given-names></string-name></person-group>  <article-title>Enhancer function: new insights into the regulation of tissue-specific gene expression</article-title>. <source>Nat Rev Genet</source>  <year>2011</year>;<volume>12</volume>(<issue>4</issue>):<fpage>283</fpage>–<lpage>93</lpage>.<pub-id pub-id-type="pmid">21358745</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>X</given-names></string-name>, <string-name><surname>Si</surname><given-names>J</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Y</given-names></string-name>, <string-name><surname>DeWille</surname><given-names>JW</given-names></string-name></person-group>. <article-title>Ccaat/enhancer binding protein-delta (c/ebp-delta) regulates cell growth, migration and differentiation</article-title>. <source>Cancer Cell Int</source>  <year>2010</year>;<volume>10</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>11</lpage>.<pub-id pub-id-type="pmid">20142996</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herz</surname><given-names>H</given-names></string-name></person-group>. <article-title>Enhancer deregulation in cancer and other diseases</article-title>. <source>Bioessays</source>  <year>2016</year>;<volume>38</volume>(<issue>10</issue>):<fpage>1003</fpage>–<lpage>15</lpage>.<pub-id pub-id-type="pmid">27570183</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Corradin</surname><given-names>O</given-names></string-name>, <string-name><surname>Scacheri</surname><given-names>PC</given-names></string-name></person-group>. <article-title>Enhancer variants: evaluating functions in common disease</article-title>. <source>Genome Med</source>  <year>2014</year>;<volume>6</volume>(<issue>10</issue>):<fpage>1</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">24433494</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moore</surname><given-names>JE</given-names></string-name>, <string-name><surname>Purcaro</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Pratt</surname><given-names>HE</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Expanded encyclopaedias of DNA elements in the human and mouse genomes</article-title>. <source>Nature</source>  <year>2020</year>;<volume>583</volume>(<issue>7818</issue>):<fpage>699</fpage>–<lpage>710</lpage>.<pub-id pub-id-type="pmid">32728249</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Finucane</surname><given-names>HK</given-names></string-name>, <string-name><surname>Bulik-Sullivan</surname><given-names>B</given-names></string-name>, <string-name><surname>Gusev</surname><given-names>A</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Partitioning heritability by functional annotation using genome-wide association summary statistics</article-title>. <source>Nat Genet</source>  <year>2015</year>;<volume>47</volume>(<issue>11</issue>):<fpage>1228</fpage>–<lpage>35</lpage>.<pub-id pub-id-type="pmid">26414678</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koido</surname><given-names>M</given-names></string-name>, <string-name><surname>Hon</surname><given-names>CC</given-names></string-name>, <string-name><surname>Koyama</surname><given-names>S</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Prediction of the cell-type-specific transcription of non-coding RNAs from genome sequences via machine learning</article-title>. <source>Nat Biomed Eng</source>  <year>2023</year>;<volume>7</volume>(<issue>6</issue>):<fpage>830</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">36411359</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Woolfe</surname><given-names>A</given-names></string-name>, <string-name><surname>Goodson</surname><given-names>M</given-names></string-name>, <string-name><surname>Goode</surname><given-names>DK</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Highly conserved non-coding sequences are associated with vertebrate development</article-title>. <source>PLoS Biol</source>  <year>2005</year>;<volume>3</volume>(<issue>1</issue>):e7.</mixed-citation>
    </ref>
    <ref id="ref11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pennacchio</surname><given-names>LA</given-names></string-name>, <string-name><surname>Ahituv</surname><given-names>N</given-names></string-name>, <string-name><surname>Moses</surname><given-names>AM</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>In vivo enhancer analysis of human conserved non-coding sequences</article-title>. <source>Nature</source>  <year>2006</year>;<volume>444</volume>(<issue>7118</issue>):<fpage>499</fpage>–<lpage>502</lpage>.<pub-id pub-id-type="pmid">17086198</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>X</given-names></string-name>, <string-name><surname>Xu</surname><given-names>H</given-names></string-name>, <string-name><surname>Yuan</surname><given-names>P</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Integration of external signaling pathways with the Core transcriptional network in embryonic stem cells</article-title>. <source>Cell</source>  <year>2008</year>;<volume>133</volume>(<issue>6</issue>):<fpage>1106</fpage>–<lpage>17</lpage>.<pub-id pub-id-type="pmid">18555785</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Visel</surname><given-names>A</given-names></string-name>, <string-name><surname>Blow</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Li</surname><given-names>Z</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Chip-seq accurately predicts tissue-specific activity of enhancers</article-title>. <source>Nature</source>  <year>2009</year>;<volume>457</volume>(<issue>7231</issue>):<fpage>854</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">19212405</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dorschner</surname><given-names>MO</given-names></string-name>, <string-name><surname>Hawrylycz</surname><given-names>M</given-names></string-name>, <string-name><surname>Humbert</surname><given-names>R</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>High-throughput localization of functional elements by quantitative chromatin profiling</article-title>. <source>Nat Methods</source>  <year>2004</year>;<volume>1</volume>(<issue>3</issue>):<fpage>219</fpage>–<lpage>25</lpage>.<pub-id pub-id-type="pmid">15782197</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giresi</surname><given-names>PG</given-names></string-name>, <string-name><surname>Kim</surname><given-names>J</given-names></string-name>, <string-name><surname>McDaniell</surname><given-names>RM</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>FAIRE (formaldehyde-assisted isolation of regulatory elements) isolates active regulatory elements from human chromatin</article-title>. <source>Genome Res</source>  <year>2007</year>;<volume>17</volume>(<issue>6</issue>):<fpage>877</fpage>–<lpage>85</lpage>.<pub-id pub-id-type="pmid">17179217</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buenrostro</surname><given-names>JD</given-names></string-name>, <string-name><surname>Giresi</surname><given-names>PG</given-names></string-name>, <string-name><surname>Zaba</surname><given-names>LC</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Transposition of native chromatin for fast and sensitive epigenomic profiling of open chromatin, DNA-binding proteins and nucleosome position</article-title>. <source>Nat Methods</source>  <year>2013</year>;<volume>10</volume>(<issue>12</issue>):<fpage>1213</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">24097267</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heintzman</surname><given-names>ND</given-names></string-name>, <string-name><surname>Stuart</surname><given-names>RK</given-names></string-name>, <string-name><surname>Hon</surname><given-names>G</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Distinct and predictive chromatin signatures of transcriptional promoters and enhancers in the human genome</article-title>. <source>Nat Genet</source>  <year>2007</year>;<volume>39</volume>(<issue>3</issue>):<fpage>311</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">17277777</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andersson</surname><given-names>R</given-names></string-name>, <string-name><surname>Gebhard</surname><given-names>C</given-names></string-name>, <string-name><surname>Miguel-Escalada</surname><given-names>I</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>An atlas of active enhancers across human cell types and tissues</article-title>. <source>Nature</source>  <year>2014</year>;<volume>507</volume>(<issue>7493</issue>):<fpage>455</fpage>–<lpage>61</lpage>.<pub-id pub-id-type="pmid">24670763</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B</given-names></string-name>, <string-name><surname>Fang</surname><given-names>L</given-names></string-name>, <string-name><surname>Long</surname><given-names>R</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ienhancer-2l: a two-layer predictor for identifying enhancers and their strength by pseudo k-tuple nucleotide composition</article-title>. <source>Bioinformatics</source>  <year>2016</year>;<volume>32</volume>(<issue>3</issue>):<fpage>362</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">26476782</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jia</surname><given-names>C</given-names></string-name>, <string-name><surname>He</surname><given-names>W</given-names></string-name></person-group>. <article-title>Enhancerpred: a predictor for discovering enhancers based on the combination and selection of multiple features</article-title>. <source>Sci Rep</source>  <year>2016</year>;<volume>6</volume>(<issue>1</issue>):<fpage>38741</fpage>.<pub-id pub-id-type="pmid">27941893</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B</given-names></string-name>, <string-name><surname>Li</surname><given-names>K</given-names></string-name>, <string-name><surname>Huang</surname><given-names>D</given-names></string-name>, <string-name><surname>Chou</surname><given-names>KC</given-names></string-name></person-group>. <article-title>Ienhancer-el: identifying enhancers and their strength with ensemble learning approach</article-title>. <source>Bioinformatics</source>  <year>2018</year>;<volume>34</volume>(<issue>22</issue>):<fpage>3835</fpage>–<lpage>42</lpage>.<pub-id pub-id-type="pmid">29878118</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nguyen</surname><given-names>QH</given-names></string-name>, <string-name><surname>Nguyen-Vo</surname><given-names>T</given-names></string-name>, <string-name><surname>Le</surname><given-names>NQK</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ienhancer-ecnn: identifying enhancers and their strength using ensembles of convolutional neural networks</article-title>. <source>BMC Genomics</source>  <year>2019</year>;<volume>20</volume>:<fpage>1</fpage>–<lpage>10</lpage>.<pub-id pub-id-type="pmid">30606130</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Le</surname><given-names>NQK</given-names></string-name>, <string-name><surname>Ho</surname><given-names>Q</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>T</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Transformer architecture based on bert and 2d convolutional neural network to identify dna enhancers from sequence information</article-title>. <source>Brief Bioinform</source>  <year>2021</year>;<volume>22</volume>.</mixed-citation>
    </ref>
    <ref id="ref24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Niu</surname><given-names>K</given-names></string-name>, <string-name><surname>Luo</surname><given-names>X</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>S</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ienhancer-eblstm: identifying enhancers and strengths by ensembles of bidirectional long short-term memory</article-title>. <source>Front Genet</source>  <year>2021</year>;<volume>12</volume>:665498.</mixed-citation>
    </ref>
    <ref id="ref25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cai</surname><given-names>L</given-names></string-name>, <string-name><surname>Ren</surname><given-names>X</given-names></string-name>, <string-name><surname>Fu</surname><given-names>X</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ienhancer-xg: interpretable sequence-based enhancers and their strength predictor</article-title>. <source>Bioinformatics</source>  <year>2021</year>;<volume>37</volume>(<issue>8</issue>):<fpage>1060</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">33119044</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bai</surname><given-names>X</given-names></string-name>, <string-name><surname>Shi</surname><given-names>S</given-names></string-name>, <string-name><surname>Ai</surname><given-names>B</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Endb: a manually curated database of experimentally supported enhancers for human and mouse</article-title>. <source>Nucleic Acids Res</source>  <year>2020</year>;<volume>48</volume>(<issue>D1</issue>):<fpage>D51</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">31665430</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heinz</surname><given-names>S</given-names></string-name>, <string-name><surname>Romanoski</surname><given-names>CE</given-names></string-name>, <string-name><surname>Benner</surname><given-names>C</given-names></string-name>, <string-name><surname>Glass</surname><given-names>CK</given-names></string-name></person-group>. <article-title>The selection and function of cell type-specific enhancers</article-title>. <source>Nat Rev Mol Cell Biol</source>  <year>2015</year>;<volume>16</volume>(<issue>3</issue>):<fpage>144</fpage>–<lpage>54</lpage>.<pub-id pub-id-type="pmid">25650801</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Basith</surname><given-names>S</given-names></string-name>, <string-name><surname>Hasan</surname><given-names>MM</given-names></string-name>, <string-name><surname>Lee</surname><given-names>G</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Manavalan, integrative machine learning framework for the identification of cell-specific enhancers from the human genomes</article-title>. <source>Brief Bioinform</source>  <year>2021</year>;<volume>22</volume>.</mixed-citation>
    </ref>
    <ref id="ref29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname><given-names>T</given-names></string-name>, <string-name><surname>Qian</surname><given-names>J</given-names></string-name></person-group>. <article-title>Enhanceratlas 2.0: an updated resource with enhancer annotation in 586 tissue/cell types across nine species</article-title>. <source>Nucleic Acids Res</source>  <year>2020</year>;<volume>48</volume>(<issue>D1</issue>):<fpage>D58</fpage>–<lpage>64</lpage>.<pub-id pub-id-type="pmid">31740966</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>P</given-names></string-name>, <string-name><surname>Boisson</surname><given-names>B</given-names></string-name>, <string-name><surname>Stenson</surname><given-names>PD</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Seqtailor: a user-friendly webserver for the extraction of dna or protein sequences from next- generation sequencing data</article-title>. <source>Nucleic Acids Res</source>  <year>2019</year>;<volume>47</volume>(<issue>W1</issue>):<fpage>W623</fpage>–<lpage>31</lpage>.<pub-id pub-id-type="pmid">31045209</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dao</surname><given-names>F</given-names></string-name>, <string-name><surname>Lv</surname><given-names>H</given-names></string-name>, <string-name><surname>Su</surname><given-names>W</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Idhs-deep: an integrated tool for predicting dnase i hypersensitive sites by deep neural network</article-title>. <source>Brief Bioinform</source>  <year>2021</year>;<volume>22</volume>.</mixed-citation>
    </ref>
    <ref id="ref32">
      <label>32.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Xhafa</surname><given-names>F</given-names></string-name></person-group>. <article-title>Lecture Notes on Data Engineering and Communications Technologies</article-title>. Cham, Germany: Springer, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>P</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>H</given-names></string-name>, <string-name><surname>Wu</surname><given-names>H</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Ipro-wael: a comprehensive and robust framework for identifying promoters in multiple species</article-title>. <source>Nucleic Acids Res</source>  <year>2022</year>;<volume>50</volume>(<issue>18</issue>):<fpage>10278</fpage>–<lpage>89</lpage>.<pub-id pub-id-type="pmid">36161334</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref34">
      <label>34.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ng</surname><given-names>P</given-names></string-name></person-group>. <article-title>dna2vec: consistent vector representations of variable-length k-mers</article-title>. <comment>arXiv preprint arXiv 2017;1701.06279</comment>.</mixed-citation>
    </ref>
    <ref id="ref35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Latchman</surname><given-names>DS</given-names></string-name></person-group>. <article-title>Transcription factors: an overview</article-title>. <source>Int J Biochem Cell Biol</source>  <year>1997</year>;<volume>29</volume>(<issue>12</issue>):<fpage>1305</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">9570129</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kulakovskiy</surname><given-names>IV</given-names></string-name>, <string-name><surname>Vorontsov</surname><given-names>IE</given-names></string-name>, <string-name><surname>Yevshin</surname><given-names>IS</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Hocomoco: towards a complete collection of transcription factor binding models for human and mouse via large-scale chip-seq analysis</article-title>. <source>Nucleic Acids Res</source>  <year>2018</year>;<volume>46</volume>(<issue>D1</issue>):<fpage>D252</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">29140464</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref37">
      <label>37.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>T</given-names></string-name>, <string-name><surname>Goyal</surname><given-names>P</given-names></string-name>, <string-name><surname>Girshick</surname><given-names>R</given-names></string-name>, <etal>et al</etal></person-group>  <article-title>Focal loss for dense object detection</article-title>. <italic toggle="yes">in:</italic><source>Proceedings of the IEEE international conference on computer vision</source>. Venice, Italy: IEEE, <year>2017</year>;<fpage>2980</fpage>–<lpage>2988</lpage>.</mixed-citation>
    </ref>
    <ref id="ref38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cai</surname><given-names>J</given-names></string-name>, <string-name><surname>Wang</surname><given-names>S</given-names></string-name>, <string-name><surname>Xu</surname><given-names>C</given-names></string-name>, <string-name><surname>Guo</surname><given-names>W</given-names></string-name></person-group>. <article-title>Unsupervised deep clustering via contractive feature representation and focal loss</article-title>. <source>Pattern Recognit</source>  <year>2022</year>;<volume>123</volume>:<fpage>108386</fpage>.</mixed-citation>
    </ref>
    <ref id="ref39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tran</surname><given-names>GS</given-names></string-name>, <string-name><surname>Nghiem</surname><given-names>TP</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>VT</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Improving accuracy of lung nodule classification using deep learning with focal loss</article-title>. <source>J Healthcare Eng</source>  <year>2019</year>;<volume>2019</volume>:<fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="ref40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pang</surname><given-names>B</given-names></string-name>, <string-name><surname>Nijkamp</surname><given-names>E</given-names></string-name>, <string-name><surname>Wu</surname><given-names>YN</given-names></string-name></person-group>. <article-title>Deep learning with tensorflow: a review[J]</article-title>. <source>JEduc Behav Stat</source>  <year>2020</year>;<volume>45</volume>(<issue>2</issue>):<fpage>227</fpage>–<lpage>48</lpage>.</mixed-citation>
    </ref>
    <ref id="ref41">
      <label>41.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>X</given-names></string-name>, <string-name><surname>Shi</surname><given-names>Z</given-names></string-name>, <string-name><surname>Wu</surname><given-names>Y</given-names></string-name>, <etal>et al</etal></person-group>  <article-title>schicsc: A novel single-cell hi-c clustering framework by contact-weight-based smoothing and feature fusion</article-title>. In: <source>2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM).</source> Las Vegas, NV: IEEE, <year>2022</year>;<fpage>44</fpage>–<lpage>50</lpage>.</mixed-citation>
    </ref>
    <ref id="ref42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname><given-names>L</given-names></string-name>, <string-name><surname>Yuan</surname><given-names>R</given-names></string-name>, <string-name><surname>Han</surname><given-names>C</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Cellenboost: a boosting-based ligand-receptor interaction identification model for cell-to-cell communication inference</article-title>. <source>IEEE Trans Nanobioscience</source>  <year>2023</year>;<volume>22</volume>:<fpage>705</fpage>–<lpage>15</lpage>.<pub-id pub-id-type="pmid">37216267</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>P</given-names></string-name>, <string-name><surname>Wu</surname><given-names>H</given-names></string-name></person-group>. <article-title>Ichrom-deep: an attention-based deep learning model for identifying chromatin interactions</article-title>. <source>IEEE J Biomed Health Inform</source>  <year>2023</year>;<volume>27</volume>:<fpage>4559</fpage>–<lpage>68</lpage>.<pub-id pub-id-type="pmid">37402191</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>H</given-names></string-name>, <string-name><surname>Li</surname><given-names>D</given-names></string-name>, <string-name><surname>Wu</surname><given-names>H</given-names></string-name></person-group>. <article-title>Lnclocator-imb: an imbalance-tolerant ensemble deep learning framework for predicting Long non-coding RNA subcellular localization[J]</article-title>. <source>IEEE J Biomed Health Inform</source>  <year>2023</year>;<volume>28</volume>(<issue>1</issue>):<fpage>538</fpage>–<lpage>47</lpage>.</mixed-citation>
    </ref>
    <ref id="ref45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ong</surname><given-names>C</given-names></string-name>, <string-name><surname>Corces</surname><given-names>VG</given-names></string-name></person-group>. <article-title>Enhancers: emerging roles in cell fate specification</article-title>. <source>EMBO Rep</source>  <year>2012</year>;<volume>13</volume>(<issue>5</issue>):<fpage>423</fpage>–<lpage>30</lpage>.<pub-id pub-id-type="pmid">22491032</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weiss</surname><given-names>K</given-names></string-name>, <string-name><surname>Khoshgoftaar</surname><given-names>TM</given-names></string-name>, <string-name><surname>Wang</surname><given-names>D</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>A survey of transfer learning</article-title>. <source>J Big Data</source>  <year>2016</year>;<volume>3</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="ref47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hu</surname><given-names>J</given-names></string-name>, <string-name><surname>Zhong</surname><given-names>Y</given-names></string-name>, <string-name><surname>Shang</surname><given-names>X</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>A versatile and scalable single-cell data integration algorithm based on domain-adversarial and variational approximation</article-title>. <source>Brief Bioinform</source>  <year>2022</year>;<volume>23</volume>.</mixed-citation>
    </ref>
    <ref id="ref48">
      <label>48.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhuang</surname><given-names>Z</given-names></string-name>, <string-name><surname>Shen</surname><given-names>X</given-names></string-name>, <string-name><surname>Pan</surname><given-names>W</given-names></string-name></person-group>. <article-title>A simple convolutional neural network for prediction of enhancer–promoter interactions with dna sequence data</article-title>. <source>Bioinformatics</source>  <year>2019</year>;<volume>35</volume>(<issue>17</issue>):<fpage>2899</fpage>–<lpage>906</lpage>.<pub-id pub-id-type="pmid">30649185</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Koch</surname><given-names>F</given-names></string-name>, <string-name><surname>Fenouil</surname><given-names>R</given-names></string-name>, <string-name><surname>Gut</surname><given-names>M</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Transcription initiation platforms and gtf recruitment at tissue-specific enhancers and promoters</article-title>. <source>Nat Struct Mol Biol</source>  <year>2011</year>;<volume>18</volume>(<issue>8</issue>):<fpage>956</fpage>–<lpage>63</lpage>.<pub-id pub-id-type="pmid">21765417</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Y</given-names></string-name>, <string-name><surname>Pai</surname><given-names>AA</given-names></string-name>, <string-name><surname>Herudek</surname><given-names>J</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Principles for rna metabolism and alternative transcription initiation within closely spaced promoters</article-title>. <source>Nat Genet</source>  <year>2016</year>;<volume>48</volume>(<issue>9</issue>):<fpage>984</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">27455346</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname><given-names>M</given-names></string-name>, <string-name><surname>Bai</surname><given-names>X</given-names></string-name>, <string-name><surname>Ai</surname><given-names>B</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Tf-marker: a comprehensive manually curated database for transcription factors and related markers in specific cell and tissue types in human</article-title>. <source>Nucleic Acids Res</source>  <year>2022</year>;<volume>50</volume>(<issue>D1</issue>):<fpage>D402</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">34986601</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref52">
      <label>52.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname><given-names>T</given-names></string-name>, <string-name><surname>He</surname><given-names>B</given-names></string-name>, <string-name><surname>Liu</surname><given-names>S</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Enhanceratlas: a resource for enhancer annotation and analysis in 105 human cell/tissue types</article-title>. <source>Bioinformatics</source>  <year>2016</year>;<volume>32</volume>(<issue>23</issue>):<fpage>3543</fpage>–<lpage>51</lpage>.<pub-id pub-id-type="pmid">27515742</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref53">
      <label>53.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lundberg</surname><given-names>SM</given-names></string-name>, <string-name><surname>Lee</surname><given-names>S</given-names></string-name></person-group>. <article-title>A unified approach to interpreting model predictions</article-title>. <source>Advances in neural information processing systems</source>. Long Beach, CA, USA: NeurIPS Foundation, <year>2017</year>;<volume>30</volume>.</mixed-citation>
    </ref>
    <ref id="ref54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>P</given-names></string-name>, <string-name><surname>Wu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>H</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Clnn-loop: a deep learning model to predict ctcf-mediated chromatin loops in the different cell lines and ctcf-binding sites (cbs) pair types</article-title>. <source>Bioinformatics</source>  <year>2022</year>;<volume>38</volume>(<issue>19</issue>):<fpage>4497</fpage>–<lpage>504</lpage>.<pub-id pub-id-type="pmid">35997565</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname><given-names>J</given-names></string-name>, <string-name><surname>Hoshino</surname><given-names>H</given-names></string-name>, <string-name><surname>Takaku</surname><given-names>K</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Hemoprotein bach1 regulates enhancer availability of heme oxygenase-1 gene</article-title>. <source>EMBO J</source>  <year>2002</year>;<volume>21</volume>(<issue>19</issue>):<fpage>5216</fpage>–<lpage>24</lpage>.<pub-id pub-id-type="pmid">12356737</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref56">
      <label>56.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>X</given-names></string-name>, <string-name><surname>Xing</surname><given-names>G</given-names></string-name>, <string-name><surname>Fraizer</surname><given-names>GC</given-names></string-name>, <string-name><surname>Saunders</surname><given-names>GF</given-names></string-name></person-group>. <article-title>Transactivation of an intronic hematopoietic-specific enhancer of the human wilms’ tumor 1 gene by Gata-1 and c-myb</article-title>. <source>J Biol Chem</source>  <year>1997</year>;<volume>272</volume>(<issue>46</issue>):<fpage>29272</fpage>–<lpage>80</lpage>.<pub-id pub-id-type="pmid">9361007</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref57">
      <label>57.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nolis</surname><given-names>IK</given-names></string-name>, <string-name><surname>McKay</surname><given-names>DJ</given-names></string-name>, <string-name><surname>Mantouvalou</surname><given-names>E</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>Transcription factors mediate long-rang enhancer–promoter interactions</article-title>. <source>Proc Natl Acad Sci</source>  <year>2009</year>;<volume>106</volume>(<issue>48</issue>):<fpage>20222</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">19923429</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref58">
      <label>58.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whalen</surname><given-names>S</given-names></string-name>, <string-name><surname>Truty</surname><given-names>RM</given-names></string-name>, <string-name><surname>Pollard</surname><given-names>KS</given-names></string-name></person-group>. <article-title>Enhancer–promoter interactions are encoded by complex genomic signatures on looping chromatin</article-title>. <source>Nature geneticss</source>  <year>2016</year>;<volume>48</volume>(<issue>5</issue>):<fpage>488</fpage>–<lpage>96</lpage>.</mixed-citation>
    </ref>
    <ref id="ref59">
      <label>59.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weintraub</surname><given-names>AS</given-names></string-name>, <string-name><surname>Li</surname><given-names>CH</given-names></string-name>, <string-name><surname>Zamudio</surname><given-names>AV</given-names></string-name>, <etal>et al</etal>.</person-group>  <article-title>YY1 is a structural regulator of enhancer-promoter loops[J]</article-title>. <source>Cell</source>  <year>2017</year>;<volume>171</volume>(<issue>7</issue>):<fpage>1573</fpage>–<lpage>1588.e28</lpage>.<pub-id pub-id-type="pmid">29224777</pub-id>
</mixed-citation>
    </ref>
  </ref-list>
</back>
