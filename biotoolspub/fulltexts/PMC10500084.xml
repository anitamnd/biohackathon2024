<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10500084</article-id>
    <article-id pub-id-type="pmid">37672035</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btad546</article-id>
    <article-id pub-id-type="publisher-id">btad546</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Gene Expression</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>scAce: an adaptive embedding and clustering method for single-cell gene expression data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>He</surname>
          <given-names>Xinwei</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Formal analysis" vocab-term-identifier="http://credit.niso.org/contributor-roles/formal-analysis" degree-contribution="equal">Formal analysis</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology" degree-contribution="equal">Methodology</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Software" vocab-term-identifier="http://credit.niso.org/contributor-roles/software" degree-contribution="equal">Software</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-original-draft" degree-contribution="equal">Writing - original draft</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - review &amp; editing" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="supporting">Writing - review &amp; editing</role>
        <aff><institution>School of Mathematics and Physics, China University of Geosciences</institution>, Wuhan 430074, <country country="CN">China</country></aff>
        <xref rid="btad546-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Qian</surname>
          <given-names>Kun</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Formal analysis" vocab-term-identifier="http://credit.niso.org/contributor-roles/formal-analysis" degree-contribution="equal">Formal analysis</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology" degree-contribution="equal">Methodology</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Software" vocab-term-identifier="http://credit.niso.org/contributor-roles/software" degree-contribution="equal">Software</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-original-draft" degree-contribution="supporting">Writing - original draft</role>
        <aff><institution>School of Mathematics and Physics, China University of Geosciences</institution>, Wuhan 430074, <country country="CN">China</country></aff>
        <xref rid="btad546-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Ziqian</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology" degree-contribution="supporting">Methodology</role>
        <aff><institution>School of Mathematics and Physics, China University of Geosciences</institution>, Wuhan 430074, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zeng</surname>
          <given-names>Shirou</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-original-draft" degree-contribution="supporting">Writing - original draft</role>
        <aff><institution>School of Mathematics and Physics, China University of Geosciences</institution>, Wuhan 430074, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Li</surname>
          <given-names>Hongwei</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Funding acquisition" vocab-term-identifier="http://credit.niso.org/contributor-roles/funding-acquisition" degree-contribution="equal">Funding acquisition</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Supervision" vocab-term-identifier="http://credit.niso.org/contributor-roles/supervision" degree-contribution="equal">Supervision</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - review &amp; editing" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="supporting">Writing - review &amp; editing</role>
        <aff><institution>School of Mathematics and Physics, China University of Geosciences</institution>, Wuhan 430074, <country country="CN">China</country></aff>
        <xref rid="btad546-cor1" ref-type="corresp"/>
        <!--hwli@cug.edu.cn-->
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2087-2709</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Wei Vivian</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Funding acquisition" vocab-term-identifier="http://credit.niso.org/contributor-roles/funding-acquisition" degree-contribution="equal">Funding acquisition</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology" degree-contribution="equal">Methodology</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Supervision" vocab-term-identifier="http://credit.niso.org/contributor-roles/supervision" degree-contribution="equal">Supervision</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-original-draft" degree-contribution="equal">Writing - original draft</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - review &amp; editing" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="lead">Writing - review &amp; editing</role>
        <aff><institution>Department of Statistics, University of California, Riverside</institution>, Riverside 92521, <country country="US">United States</country></aff>
        <xref rid="btad546-cor1" ref-type="corresp"/>
        <!--weil@ucr.edu-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Nikolski</surname>
          <given-names>Macha</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btad546-cor1">Corresponding authors. Department of Statistics, University of California, Riverside, 900 University Ave., Riverside, CA 92521, United States. E-mail: <email>weil@ucr.edu</email> (W.V.L.); School of Mathematics and Physics, China University of Geosciences, No. 388 Lumo Road, Wuhan 430074, China. E-mail: <email>hwli@cug.edu.cn</email> (H.L.)</corresp>
      <fn id="btad546-FM1">
        <p>Xinwei He and Kun Qian Equal contribution.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <month>9</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-09-06">
      <day>06</day>
      <month>9</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>06</day>
      <month>9</month>
      <year>2023</year>
    </pub-date>
    <volume>39</volume>
    <issue>9</issue>
    <elocation-id>btad546</elocation-id>
    <history>
      <date date-type="received">
        <day>04</day>
        <month>5</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>01</day>
        <month>8</month>
        <year>2023</year>
      </date>
      <date date-type="editorial-decision">
        <day>30</day>
        <month>8</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>05</day>
        <month>9</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>13</day>
        <month>9</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btad546.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Since the development of single-cell RNA sequencing (scRNA-seq) technologies, clustering analysis of single-cell gene expression data has been an essential tool for distinguishing cell types and identifying novel cell types. Even though many methods have been available for scRNA-seq clustering analysis, the majority of them are constrained by the requirement on predetermined cluster numbers or the dependence on selected initial cluster assignment.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>In this article, we propose an adaptive embedding and clustering method named scAce, which constructs a variational autoencoder to simultaneously learn cell embeddings and cluster assignments. In the scAce method, we develop an adaptive cluster merging approach which achieves improved clustering results without the need to estimate the number of clusters in advance. In addition, scAce provides an option to perform clustering enhancement, which can update and enhance cluster assignments based on previous clustering results from other methods. Based on computational analysis of both simulated and real datasets, we demonstrate that scAce outperforms state-of-the-art clustering methods for scRNA-seq data, and achieves better clustering accuracy and robustness.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The scAce package is implemented in python 3.8 and is freely available from <ext-link xlink:href="https://github.com/sldyns/scAce" ext-link-type="uri">https://github.com/sldyns/scAce</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>42274172</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health (National Institute of General Medical Sciences</institution>
          </institution-wrap>
        </funding-source>
        <award-id>R35GM142702</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="10"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Advances in single-cell RNA sequencing (scRNA-seq) technologies have made them powerful tools for understanding heterogeneous gene expression in diverse cell populations and for quantifying single-cell activities in the study of development, physiology, and disease. In computational analysis of scRNA-seq data, unsupervised clustering is a crucial approach for identifying distinct cell populations based on their gene expression levels (<xref rid="btad546-B13" ref-type="bibr">Kiselev <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btad546-B28" ref-type="bibr">Sheng and Li 2021</xref>, <xref rid="btad546-B18" ref-type="bibr">Li 2022</xref>). By unsupervised clustering, it is possible to identify clusters of cells and then annotate them as known or novel cell types based on prior knowledge of marker genes and biological pathways. However, due to the high sparsity and high-dimensional nature of scRNA-seq data (<xref rid="btad546-B25" ref-type="bibr">Petegrosso <italic toggle="yes">et al.</italic> 2020</xref>, <xref rid="btad546-B26" ref-type="bibr">Qi <italic toggle="yes">et al.</italic> 2020</xref>), it is challenging to cluster single cells directly using generic clustering methods.</p>
    <p>To better account for the characteristics of scRNA-seq data, new clustering methods tailored for single-cell gene expression levels have been developed. Earlier methods, such as Seurat (<xref rid="btad546-B27" ref-type="bibr">Satija <italic toggle="yes">et al.</italic> 2015</xref>), SC3 (<xref rid="btad546-B14" ref-type="bibr">Kiselev <italic toggle="yes">et al.</italic> 2017</xref>), and CIDR (<xref rid="btad546-B20" ref-type="bibr">Lin <italic toggle="yes">et al.</italic> 2017</xref>), treat dimensionality reduction and cell clustering as two successive steps. They first use principal component analysis to reduce the dimensions of the gene expression matrix or the cell-cell distance matrix, and then utilize a generic clustering method, such as the Louvain (<xref rid="btad546-B3" ref-type="bibr">Blondel <italic toggle="yes">et al.</italic> 2008</xref>) or hierarchical clustering (<xref rid="btad546-B36" ref-type="bibr">Ward <italic toggle="yes">et al.</italic> 1963</xref>) to obtain the inferred cluster labels. More recent methods such as scScope (<xref rid="btad546-B7" ref-type="bibr">Deng <italic toggle="yes">et al.</italic> 2019</xref>) use an autoencoder, a deep-learning-based model, to learn low-dimensional latent representation of data and then perform cell clustering on the low-dimensional features. Another example is graph-sc (<xref rid="btad546-B6" ref-type="bibr">Ciortan and Defrance 2022</xref>), which uses a convolutional graph autoencoder to process a gene-to-cell graph before applying the K-means or Leiden (<xref rid="btad546-B33" ref-type="bibr">Traag <italic toggle="yes">et al.</italic> 2019</xref>) clustering algorithm. Since the latent space learned by traditional autoencoders is discontinuous and unregularized, deep embedding methods based on variational autoencoder (VAE) networks have gained popularity in scRNA-seq analysis. VAEs can learn a continuous latent representation of the input data by constraining the distribution of the latent variables to follow a prior distribution. VAE-based methods such as VASC (<xref rid="btad546-B34" ref-type="bibr">Wang and Gu 2018</xref>) and siVAE (<xref rid="btad546-B5" ref-type="bibr">Choi <italic toggle="yes">et al.</italic> 2023</xref>) focus more on the cell embedding problem, while other methods such as scVI (<xref rid="btad546-B22" ref-type="bibr">Lopez <italic toggle="yes">et al.</italic> 2018</xref>), scVAE (<xref rid="btad546-B9" ref-type="bibr">Grønbech <italic toggle="yes">et al.</italic> 2020</xref>), and scGMAAE (<xref rid="btad546-B35" ref-type="bibr">Wang <italic toggle="yes">et al.</italic> 2023</xref>) can simultaneously achieve cell embedding and clustering.</p>
    <p>As most clustering methods for scRNA-seq data rely on external or internal dimensionality reduction as an intermediate step, the quality of the low-dimensional representation has significant impact on the downstream clustering accuracy. Instead of treating dimensionality reduction and clustering as two separate tasks, some clustering methods based on deep embedding aim to simultaneously learn low-dimensional embeddings and clusters (<xref rid="btad546-B39" ref-type="bibr">Xie <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btad546-B10" ref-type="bibr">Guo <italic toggle="yes">et al.</italic> 2017</xref>). By adapting this idea of deep embedding clustering to cluster single cells, multiple methods have been proposed for scRNA-seq data. For example, DESC (<xref rid="btad546-B19" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2020</xref>) applies deep embedding clustering to scRNA-seq data after normalization and gene selection, with cluster centers initialized by the Louvain algorithm. scDeepCluster (<xref rid="btad546-B30" ref-type="bibr">Tian <italic toggle="yes">et al.</italic> 2019</xref>) and scDCC (<xref rid="btad546-B31" ref-type="bibr">Tian <italic toggle="yes">et al.</italic> 2021</xref>) extended this idea by incorporating a zero-inflated negative binomial (ZINB) model, which was first proposed in the DCA method for scRNA-seq denoising (<xref rid="btad546-B8" ref-type="bibr">Eraslan <italic toggle="yes">et al.</italic> 2019</xref>), into an autoencoder network. Even though these methods based on deep embedding sometimes lead to better clustering results by allowing for nonlinear transformations, one limitation they share is that their optimization procedure depends on cell clusters initialized by a generic algorithm such as K-means or Louvain. The K-means algorithm requires a cluster number as input to perform clustering, while the Louvain algorithm requires a resolution parameter to control the size of the clusters. If these parameters are mis-specified or if the initial clustering results contain too many incorrect mixtures of cell populations, these errors will propagate to the iterative update of neural networks, affecting the accuracy of final clustering results.</p>
    <p>In light of the above limitation, some clustering methods attempt to reduce the dependence on predetermined parameters (such as cluster number) by starting with a relatively large number of micro clusters and gradually merging similar ones into larger clusters. For example, both SCCAF (<xref rid="btad546-B23" ref-type="bibr">Miao <italic toggle="yes">et al.</italic> 2020</xref>) and ADClust (<xref rid="btad546-B42" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>) obtain initial clusters via the Louvain algorithm. Then, SCCAF iteratively updates the cluster labels by training a classifier on the clusters and evaluating the similarities between the clusters based on a confusion matrix; ADClust uses a unimodality test to evaluate the similarity between clusters and identify those that could be merged. Although cluster merging has reduced their reliance on the initial cluster number, as we will show in our results, their merging process uses non-data-adaptive stopping criteria and could be prone to under-clustering or over-clustering on certain datasets.</p>
    <p>Inspired by these preceding endeavors in clustering analysis, we propose a method named scAce for scRNA-seq data to simultaneously achieve embedding of gene expression data and clustering of single cells. The scAce method constructs a VAE network to learn smoother low-dimensional embeddings compared with those methods based on traditional autoencoders. It utilizes a data-adaptive clustering approach based on the idea of cluster merging, and the merging process is controlled by evaluating intra-cluster and inter-cluster distances. By iteratively updating the VAE network and the cluster labels, scAce improves both the embedding and clustering of single cells. Another feature of the scAce method is that it enables clustering enhancement by taking the clustering results of another method as its initialization, and then uses its network model to further enhance the accuracy of final clusters. We have assessed the clustering performance of scAce in comparison with state-of-the-art clustering methods. The results show that scAce is more accurate and robust on both simulated and real scRNA-seq data. In addition, with its cluster enhancement option, scAce is able to correct and improve previous clustering results produced by other clustering methods.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 An outline of the scAce method</title>
      <p>The scAce method is consisted of three major steps, a pretraining step based on an improved variational autoencoder (<italic toggle="yes">β</italic>-VAE) (<xref rid="btad546-B11" ref-type="bibr">Higgins <italic toggle="yes">et al.</italic> 2017</xref>), a cluster initialization step to obtain initial cluster labels, and an adaptive cluster merging step to iteratively update cluster labels and cell embeddings (<xref rid="btad546-F1" ref-type="fig">Fig. 1</xref>). We introduce each of these steps in detail below.</p>
      <fig position="float" id="btad546-F1">
        <label>Figure 1.</label>
        <caption>
          <p>Overview of the scAce method. (A) Pretraining. scAce takes the single-cell gene expression matrix as its input to train a VAE network. The encoder learns low-dimensional hidden variables for the single cells, which serve as the input of the decoder. For each gene, the VAE learns and outputs three parameters of a ZINB distribution (mean, dispersion, and inflated proportion of zero). (B) Cluster initialization. With <italic toggle="yes">de novo</italic> initialization, the Leiden algorithm is used to obtain initial cluster labels. With clustering enhancement, initial cluster labels are obtained by applying a cluster splitting approach to a set of existing clustering results (from another clustering method). (C) Adaptive cluster merging. Given the pretrained VAE network and the initial cluster labels, the network parameters, cell embeddings, cluster labels, and centroids are iteratively updated by alternately performing network update and cluster merging steps. The final results of cell embeddings and cluster labels are output by scAce after the iteration process stops.</p>
        </caption>
        <graphic xlink:href="btad546f1" position="float"/>
      </fig>
      <p><bold><italic toggle="yes">Pretraining</italic></bold>. The input of scAce is a scRNA-seq read count matrix <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, based on which scAce will obtain the normalized expression matrix <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref rid="sup1" ref-type="supplementary-material">Supplementary Methods</xref>), where <italic toggle="yes">m</italic> and <italic toggle="yes">n</italic> are the numbers of genes and cells after preprocessing, respectively. In the VAE, the expression levels are first encoded to obtain the mean and variance parameters in the hidden layer. Then, the decoder receives a hidden variable <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo>≪</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> generated in the lower-dimensional space produced by the encoder. Based on <italic toggle="yes">Z</italic>, the decoder learns three parameters (mean, dispersion, and inflated zero proportion) of a ZINB distribution for every gene (<xref rid="btad546-F1" ref-type="fig">Fig. 1A</xref>).</p>
      <p><bold><italic toggle="yes">Cluster initialization</italic></bold>. scAce provides two options to perform cluster initialization (<xref rid="btad546-F1" ref-type="fig">Fig. 1B</xref>). The first option is used when scAce is applied in a <italic toggle="yes">de novo</italic> manner, and an existing clustering result is not available. In this case, scAce uses the Leiden algorithm in Scanpy (<xref rid="btad546-B37" ref-type="bibr">Wolf <italic toggle="yes">et al.</italic> 2018</xref>) to obtain the initial cluster assignments and centroids. For each cluster, the initial cluster centroid is the center of the cluster based on the hidden variable <italic toggle="yes">Z</italic> obtained from the pretraining step and the Euclidean distance. The resolution parameter of the Leiden algorithm is set to a large value (defaults to 2) so that the clusters have high purity. The second option is used when enhancement of an existing clustering result (e.g. obtained using Seurat) is desired. When using this option, scAce takes the cluster labels from the existing clustering result, and applies an adaptive cluster splitting method (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods</xref>) to split the current clusters into smaller and purer clusters based on intra-cluster distances. Then, scAce uses the new clusters after cluster splitting as the initial cluster assignment and obtains cluster centroids as described above.</p>
      <p>The purpose of cluster splitting in clustering enhancement or using a large resolution parameter in the <italic toggle="yes">de novo</italic> initialization is to ensure that each of the initial clusters contains cells of the same cell type or state. Even though there might be multiple clusters in the initial assignment that belong to the same cell types, they will be merged into the same cluster at the adaptive cluster merging step. This approach was inspired by the observation that methods depending on initial clustering results of the K-means or Louvain algorithm are likely to generate mixed clusters if the initial ones contain a large proportion of mixtures.</p>
      <p><bold><italic toggle="yes">Adaptive cluster merging</italic></bold>. In the step of adaptive cluster merging, scAce iteratively performs network update and cluster merging based on the initial VAE network obtained from the pretraining step and the clusters from the cluster initialization step (<xref rid="btad546-F1" ref-type="fig">Fig. 1C</xref>). At each iteration of network update, scAce constructs a loss function consisted of two components. The first component is the loss of the VAE network and the second component is a clustering loss defined based on cluster labels and centroids. Given this loss function, the VAE network is updated to improve cell embeddings. At each iteration of cluster merging, scAce decides if a pair of clusters should be merged into a single cluster by comparing inter-cluster and intra-cluster distances. Network update and cluster merging are performed alternately in scAce until no clusters can be merged in the cluster merging step.</p>
    </sec>
    <sec>
      <title>2.2 Pretraining: ZINB-based variational autoencoder network</title>
      <p>Since the latent space learned by traditional autoencoders are discontinuous and unregularized, which is not ideal for generative modeling, we use a VAE network with a Gaussian prior (<xref rid="btad546-B12" ref-type="bibr">Kingma <italic toggle="yes">et al.</italic> 2022</xref>) to learn the latent space of single-cell gene expression data. In addition, we use ZINB as the generative distribution in the decoder to model the scRNA-seq count data. For gene <italic toggle="yes">i</italic> in cell <italic toggle="yes">j</italic> (<inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>), we assume that its count follows a ZINB distribution with the following parameterization:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mtext>ZINB</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>π</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>π</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>δ</mml:mo></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>π</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mtext>NB</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mtext>NB</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes a negative binomial (NB) distribution; <italic toggle="yes">μ<sub>ij</sub></italic> and <italic toggle="yes">θ<sub>ij</sub></italic> are the mean and dispersion parameters of the NB distribution, respectively; <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the Dirac delta function which takes the values of 1 when <italic toggle="yes">x </italic>=<italic toggle="yes"> </italic>0 and 0 when <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>; <italic toggle="yes">π<sub>ij</sub></italic> denotes the inflated zero proportion.</p>
      <p>We now introduce the architecture of the VAE network in detail (<xref rid="btad546-F1" ref-type="fig">Fig. 1A</xref>). Given the normalized single-cell gene expression matrix <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, the encoder first obtains the mean and variance parameters of the Gaussian distributions in the hidden space. Then, the network uses the resampling technique to obtain the hidden Gaussian variables <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which serves as the input of the decoder. Lastly, the output of the decoder are the parameters of the ZINB distributions. We denote the mean parameters as <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mo>≜</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, the dispersion parameters as <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:mo>Θ</mml:mo><mml:mo>≜</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and the inflated zero proportions as <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:mo>Π</mml:mo><mml:mo>≜</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>π</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The VAE model is therefore summarized as follows:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>H</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mi>H</mml:mi><mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mi>H</mml:mi><mml:mo>;</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>⊙</mml:mo><mml:mo>ϵ</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mo>⊙</mml:mo><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>μ</mml:mo></mml:msub></mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>Θ</mml:mo><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo>Π</mml:mo><mml:mo>=</mml:mo><mml:mtext>sigmoid</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>π</mml:mo></mml:msub></mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo> </mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where:</p>
      <list list-type="bullet">
        <list-item>
          <p><inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the input normalized gene expression matrix;</p>
        </list-item>
        <list-item>
          <p><inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mi>H</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo>&lt;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the output of <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>H</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>H</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is a layer of fully connected neural network with the ReLU activation function;</p>
        </list-item>
        <list-item>
          <p><inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are the mean and standard deviation parameters of the hidden Gaussian variables; <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mo>μ</mml:mo><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="normal">ℝ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo> </mml:mo><mml:mtext>and</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mo>σ</mml:mo><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="normal">ℝ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are the corresponding weights;</p>
        </list-item>
        <list-item>
          <p><inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is drawn from the learned Gaussian distributions and serves as the input of the decoder <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>; each element in <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:mo>ϵ</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is an independent standard Gaussian variable, and <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mo>⊙</mml:mo></mml:math></inline-formula> denotes the element-wise multiplication;</p>
        </list-item>
        <list-item>
          <p><inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is a layer of fully connected neural network with the ReLU activation function;</p>
        </list-item>
        <list-item>
          <p><inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo>&lt;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the output of the decoder <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>; <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>μ</mml:mo></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mtext>and</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>π</mml:mo></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are the weight parameters.</p>
        </list-item>
      </list>
      <p>In this model, we use the exponential activation function to generate the mean parameters (<italic toggle="yes">M</italic>) and dispersion parameters (Θ) because these parameters should be nonnegative. We also use the sigmoid activation function to calculate Π so that the values are between 0 and 1. Given this model, the loss function of the VAE network is derived as
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>VAE</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mtext>ZINB</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>π</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mfrac><mml:mo>β</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>The first term in <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>VAE</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the negative log-likelihood function, and the second term is derived from the Kullback–Leibler divergence <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:mtext>KL</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref rid="sup1" ref-type="supplementary-material">Supplementary Methods</xref>). <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:mo>β</mml:mo><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> is an adjustable parameter that reflects the strength of the disentanglement constraint.</p>
    </sec>
    <sec>
      <title>2.3 Adaptive cluster merging</title>
      <p>Given results of the pretraining and cluster initialization steps, scAce first performs a network update, and then iteratively performs cluster merging and network update to obtain the final clustering results (<xref rid="btad546-F1" ref-type="fig">Fig. 1C</xref>). The network update step and the cluster merging step are performed alternately, and the iteration stops if no clusters can be merged after a network update step. The final cluster labels and cell embeddings are the output of scAce.</p>
      <p>At each step of network update, given the current cluster labels, scAce updates the VAE network parameters and cluster centroids using the deep embedded clustering technique. A loss function is constructed to represent the quality of the clustering results based on the current cell embeddings and cluster labels. This function is then combined with the loss function of the VAE network to simultaneously update the network parameters and cluster centroids. After the update, scAce switches to the cluster merging step.</p>
      <p>At each step of cluster merging, given the current cell embeddings (from updated VAE network) and cluster centroids, scAce first assigns cells to their closest centroids to update the cluster labels. Then, using a data-adaptive criterion, scAce merges pairs of clusters with highly similar gene expression profiles into the same cluster. When two clusters are merged, the new centroid of the merged cluster is updated as the center of that cluster based on the current data embedding. The merging process is repeated until no more clusters can be merged given the data-adaptive criterion. Then, scAce switches to the network update step.</p>
      <p><bold><italic toggle="yes">Network update</italic></bold>. Suppose at the beginning of a network update step there are <italic toggle="yes">K</italic> clusters and corresponding centroids. We calculate the probability that cell <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> belongs to cluster <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> as the conditional probability (<inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) of cell <italic toggle="yes">i</italic> given the centroid of cluster <italic toggle="yes">j</italic> (denoted as <italic toggle="yes">c<sub>j</sub></italic>). The conditional probability is obtained using the student’s <italic toggle="yes">t</italic>-distribution as a kernel to measure the similarity between the embedded cells and cluster centroids (<xref rid="btad546-B39" ref-type="bibr">Xie <italic toggle="yes">et al.</italic> 2016</xref>):
<disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo> </mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>To improve cluster quality by putting more emphasis on cells assigned with a high confidence, we define an auxiliary target distribution based on the distribution represented by the conditional probabilities:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The model is then trained to increase the similarity between the current distribution and the target distribution. The goal is to minimize the sum of the KL divergence over all the cells:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mi>C</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>Then, in order to update the network parameters, the overall loss function is defined as a weighted sum of <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>VAE</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">L<sub>C</sub></italic>: <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>VAE</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mo>λ</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mi>C</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mrow><mml:mo>λ</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> is a parameter controlling the relative weights of network loss and clustering loss. By minimizing the loss function <italic toggle="yes">L</italic>, the network parameters are optimized, and the embeddings of the single cells and cluster centroids are updated, thus updating the cluster labels. After the cluster labels are updated, we recalculate the conditional probabilities in <xref rid="E4" ref-type="disp-formula">Equation (4)</xref> and repeat the above process to update cluster labels until the proportion of cells that change their cluster labels between two consecutive repeats is smaller than 5%. The purpose of this repeating process is to ensure that the network update has reached a stable state before cluster merging.</p>
      <p><bold><italic toggle="yes">Cluster merging</italic></bold>. Given the updated data from the previous network update step, we use a data-adaptive criterion to decide if two smaller clusters should be merged into a larger cluster (<xref rid="btad546-B16" ref-type="bibr">Lei <italic toggle="yes">et al.</italic> 2016</xref>). To define this criterion, we first introduce the intra-cluster distance. Suppose there are <italic toggle="yes">K</italic> clusters in a given iteration. For the <italic toggle="yes">i</italic>th cluster (<inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula>), its intra-cluster distance is defined as
<disp-formula id="E7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mtext>intra</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">N<sub>i</sub></italic> denotes the indices of cells in cluster <italic toggle="yes">i</italic>; <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> is the number of cells in cluster <italic toggle="yes">i</italic>; <italic toggle="yes">c<sub>i</sub></italic> denotes the centroid of cluster <italic toggle="yes">i</italic>; <italic toggle="yes">z<sub>q</sub></italic> is the current embedding of cell <italic toggle="yes">q</italic>. In addition, we define the inter-cluster distance between clusters <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> as
<disp-formula id="E8"><label>(8)</label><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>inter</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">c<sub>i</sub></italic> and <italic toggle="yes">c<sub>j</sub></italic> denote the centroids of clusters <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic>, respectively.</p>
      <p>The rationale underlying the adaptive criterion is that two small clusters should be merged into a larger cluster when their inter-cluster distance is small compared with the average inter-cluster distance defined as
<disp-formula id="E9"><label>(9)</label><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>inter</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">w<sub>ij</sub></italic> is the assigned weight on the inter-cluster distance between the <italic toggle="yes">i</italic>th and the <italic toggle="yes">j</italic>th clusters. This weight is used to account for the effect of cluster size on inter-cluster distances, and is defined as
<disp-formula id="E10"><label>(10)</label><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mtext>intra</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mtext>intra</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>j</mml:mi><mml:mrow><mml:mtext>intra</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mtext>intra</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mtext>intra</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>j</mml:mi><mml:mrow><mml:mtext>intra</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>For example, if the size of cluster <italic toggle="yes">i</italic> is large, then it will naturally have relatively large inter-cluster distances with other clusters. Consequently, it will have relatively smaller weights to offset this effect. In summary, we find all cluster pairs whose weighted intra-cluster distance (<inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>inter</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is smaller than <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> and merge the cluster pair with the smallest weighted distance.</p>
      <p>Following this principle, after each merge, <xref rid="E7" ref-type="disp-formula">Equations (7–</xref><xref rid="E10" ref-type="disp-formula">10)</xref> are updated based on the new cluster memberships, and the merging process is repeated until no more clusters meet the merging criterion. In other words, we stop the cluster merging step when <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>inter</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> for any pairs of <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> (<inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>).</p>
    </sec>
    <sec>
      <title>2.4 Implementation</title>
      <p>The scAce package is implemented in python 3.8, using Scanpy version 1.9.1 (<xref rid="btad546-B37" ref-type="bibr">Wolf <italic toggle="yes">et al.</italic> 2018</xref>) for preprocessing and Pytorch version 1.10.0 for implementing the VAE network. In the ZINB-based VAE network, <italic toggle="yes">β</italic> is set to <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.001</mml:mn><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> during pretraining and <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.01</mml:mn><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> during adaptive cluster merging. The values of <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> were set to 512, 32, and 512 in our analysis, and these are provided as tuning parameters in the scAce software. The parameter <italic toggle="yes">λ</italic> is set to 1 by default. A comparison for different values of <italic toggle="yes">λ</italic> and <italic toggle="yes">β</italic> at the adaptive cluster merging stage is presented in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S1 and S2</xref>. The required input of the scAce software is a scRNA-seq read count matrix. If clustering enhancement of an existing clustering result is desired, then the optional input of existing cluster labels should also be provided. The output of scAce includes the low-dimensional embeddings and final cluster assignments of the single cells.</p>
    </sec>
    <sec>
      <title>2.5 Clustering methods used for comparison</title>
      <p>For comparison in performance evaluation, we considered nine alternative clustering methods developed for scRNA-seq data based on both citation number and publication time. We restricted our considerations to methods that have software functions to directly perform clustering. From the traditional clustering methods (not using deep-learning methods), we selected Seurat (<xref rid="btad546-B27" ref-type="bibr">Satija <italic toggle="yes">et al.</italic> 2015</xref>) and CIDR (<xref rid="btad546-B20" ref-type="bibr">Lin <italic toggle="yes">et al.</italic> 2017</xref>), both of which are highly cited. From the clustering methods based on classical autoencoders, we selected the most widely used scDeepCluster (<xref rid="btad546-B30" ref-type="bibr">Tian <italic toggle="yes">et al.</italic> 2019</xref>) and DESC (<xref rid="btad546-B19" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2020</xref>). From clustering methods based on VAEs, we chose the most widely used scVI method (<xref rid="btad546-B22" ref-type="bibr">Lopez <italic toggle="yes">et al.</italic> 2018</xref>) and the recently published scGMAAE method (<xref rid="btad546-B35" ref-type="bibr">Wang <italic toggle="yes">et al.</italic> 2023</xref>). In addition, we selected two clustering methods that also use cluster merging approaches, SCCAF (<xref rid="btad546-B23" ref-type="bibr">Miao <italic toggle="yes">et al.</italic> 2020</xref>) and ADClust (<xref rid="btad546-B42" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>), and one clustering method based on the graph neural network, graph-sc (<xref rid="btad546-B6" ref-type="bibr">Ciortan and Defrance 2022</xref>). For methods requiring a cluster number as input, the number of real cell types was provided to the algorithms. The main characteristics of scAce and the other nine methods are summarized in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>.</p>
    </sec>
    <sec>
      <title>2.6 Datasets</title>
      <p>For simulated data, we used the R package scDesign2 (<xref rid="btad546-B29" ref-type="bibr">Sun <italic toggle="yes">et al.</italic> 2021</xref>) to generate a synthetic single-cell gene expression matrix with ground truth cell type labels. In this simulated dataset, there were 16 653 genes and 1150 cells belonging to five cell types. The number of cells in each cell type was 600, 200, 200, 100, and 50, respectively. The last cell type accounted for &lt;5% of the cells and was used to represent a rare cell type. The real scRNA-seq dataset used by scDesign2 to learn gene expression parameters was a peripheral blood mononuclear cell (PBMC) dataset generated by the 10x Genomics technology (<xref rid="btad546-B43" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic> 2017</xref>).</p>
      <p>For real data, we downloaded seven real scRNA-seq datasets with annotated cell type labels (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S4</xref>). The seven datasets included three human datasets, three mouse datasets, and one turtle dataset. For simplicity, the datasets are referred to as Human pancreas (<xref rid="btad546-B2" ref-type="bibr">Baron <italic toggle="yes">et al.</italic> 2016</xref>) (3605 cells), Human PBMC (<xref rid="btad546-B43" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic> 2017</xref>) (4271 cells), Human kidney (<xref rid="btad546-B40" ref-type="bibr">Young <italic toggle="yes">et al.</italic> 2018</xref>) (5685 cells), Mouse ES (<xref rid="btad546-B15" ref-type="bibr">Klein <italic toggle="yes">et al.</italic> 2015</xref>) (embryonic stem, 2717 cells), Mouse hypothalamus (<xref rid="btad546-B4" ref-type="bibr">Chen <italic toggle="yes">et al.</italic> 2017</xref>) (12 089 cells), Mouse kidney (<xref rid="btad546-B1" ref-type="bibr">Adam <italic toggle="yes">et al.</italic> 2017</xref>) (3660 cells), and Turtle brain (<xref rid="btad546-B32" ref-type="bibr">Tosches <italic toggle="yes">et al.</italic> 2018</xref>) (18 664 cells).</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 scAce improves clustering accuracy and robustness on both simulated and real data</title>
      <p>To evaluate the performance of scAce in clustering scRNA-seq data, we first applied it and the other nine clustering methods on the simulated data. We generated data of five cell types by the scDesign2 package (<xref rid="btad546-B29" ref-type="bibr">Sun <italic toggle="yes">et al.</italic> 2021</xref>), which can learn gene expression parameters and gene–gene correlations from real data (Section 2). For methods that require a cluster number as input (scDeepCluster, scGMAAE, and graph-sc), we input the real cell type number; for other methods including scAce, the cluster numbers were automatically determined by the methods.</p>
      <p>When comparing the clustering results (<xref rid="btad546-F2" ref-type="fig">Fig. 2A</xref>) with the ground truth cell type labels (<xref rid="btad546-F2" ref-type="fig">Fig. 2B</xref>), we found that scAce was the only method that achieved an ARI of 1 with a clear separation of the five cell types in the low-dimensional space. In contrast, scVI, SCCAF, and Seurat would divide some cell types into smaller clusters, while ADClust and CIDR were unable to distinguish the rare cell type from other major cell types. DESC and graph-sc also achieved a high ARI, but both methods misclassified a small proportion of cells from the rare cell type. It is also worth noting that among the seven methods which do not require a cluster number as input, scAce and DESC were the only two methods that identified the correct number of clusters.</p>
      <fig position="float" id="btad546-F2">
        <label>Figure 2.</label>
        <caption>
          <p>Comparison of the 10 clustering methods on the simulated dataset. (A) UMAP plots of the cell embeddings produced by the 10 methods. Each point represents a cell and each color represents an inferred cluster. The number of inferred clusters (<italic toggle="yes">K</italic>) and the ARI values of the clustering results are marked on top of the corresponding plots. (B) Same UMAP plots as shown in (A) but colored by the five true cell types. (C) Boxplots of ARI values obtained by applying the ten clustering methods to randomly selected subsamples of the complete simulated data. (D) Boxplots of NMI values obtained by applying the 10 clustering methods to randomly selected subsamples of the complete simulated data.</p>
        </caption>
        <graphic xlink:href="btad546f2" position="float"/>
      </fig>
      <p>In order to further evaluate the robustness of different methods, we repeatedly applied the methods to a subset of the simulated cells. Each time, we randomly chose 95% of the cells in the dataset to create a new dataset, and performed the clustering analysis on the new data. The experiments were repeated 10 times. scAce achieved the highest median ARI and NMI values among the ten methods (<xref rid="btad546-F2" ref-type="fig">Fig. 2C and D</xref>), demonstrating its high robustness compared with alternative methods.</p>
      <p>After confirming scAce’s accuracy on the simulated data, we then applied scAce and the other nine methods on seven real scRNA-seq datasets and evaluated their clustering accuracy (Section 2). Based on the mean ARI scores across the seven datasets, scAce achieved the best clustering accuracy, followed by DESC, graph-sc, scDeepCluster, and ADClust (<xref rid="btad546-F3" ref-type="fig">Fig. 3A</xref>). The different cell types were clearly separated in the low-dimensional embeddings obtained by scAce, whereas the other methods had difficulty deriving embeddings that correctly reflected cellular identities for at least one dataset (<xref rid="btad546-F3" ref-type="fig">Fig. 3B and C</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S1 and S2</xref>). Compared with other clustering methods based on standard autoencoders (ADClust, DESC, and scDeepCluster), the embeddings obtained by scAce tended to be smoother, as VAE allows the latent variables to be continuous and smooth. When compared with scVI and scGMAAE, which also use VAEs to obtain cell embeddings, scAce’s clustering accuracy was higher than both on all seven datasets.</p>
      <fig position="float" id="btad546-F3">
        <label>Figure 3.</label>
        <caption>
          <p>Comparison of clustering methods on real datasets. (A) ARI and NMI values obtained from the 10 methods on the seven real datasets. The methods are ordered based on the mean ARI/NMI values. (B) UMAP plots of the cell embeddings produced by the four methods with the highest average ARI values across datasets. Each point represents a cell and each color represents an inferred cluster. The inferred cluster numbers are marked on top of the corresponding plots. (C) Same UMAP plots as shown in (B) but colored by the true cell types. The ARI values of the clustering results are marked on top of the corresponding plots.</p>
        </caption>
        <graphic xlink:href="btad546f3" position="float"/>
      </fig>
      <p>To further evaluate and compare the robustness of the clustering methods, we then applied them to subsets of 95% cells randomly selected from each real dataset, and repeated 10 times for each dataset. Based on both ARI and NMI scores, scAce again achieved the highest clustering accuracy across the repeated experiments (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S3 and S4</xref>). For example, the average ARI of scAce across all experiments was 0.76, followed by graph-sc (average ARI = 0.66), and DESC (average ARI = 0.65). As for NMI, the average of scAce is 0.80, followed by DESC (average NMI = 0.78), and graph-sc (average NMI = 0.76).</p>
    </sec>
    <sec>
      <title>3.2 scAce improves clustering accuracy by adaptive cluster merging</title>
      <p>In order to investigate the necessity of the adaptive cluster merging step in scAce, we compared the clustering results of scAce (as described in Section 2) with the initial clustering results when setting the resolution parameter in the Leiden algorithm such that the initial cluster number was the same as the true cell type number. The ARI values under the two settings show that the results of scAce were more accurate than the initial clustering without performing adaptive cluster merging on six datasets (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S5A</xref>). The clustering accuracy of the two settings was similar on the Mouse ES dataset as the initial result already achieved an ARI close to 1 (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S5B</xref>). For the other more challenging datasets, scAce was able to achieve higher clustering accuracy even without prior knowledge about the true cell type number.</p>
      <p>Of the nine alternative methods that were compared, SCCAF and ADClust also discover and merge clusters that might represent the same cellular identity in an iterative manner. In order to better compare the effectiveness and accuracy of the three methods, we studied the intermediate results of these methods in the initial and subsequent iterations. Their clustering results in the first iteration show that all three methods initially obtained a relatively large number of clusters (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S6A and B</xref>). Even though multiple clusters might correspond to the same cell type, most of these clusters had high purity, allowing for improvement by cluster merging through additional iterations.</p>
      <p>We then compared how the correspondence between true cell types and identified clusters changed in the cluster merging process, using the Mouse kidney dataset as an example (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S6 and S7</xref>). While the initial clustering of the three methods all showed high purity, the final clustering accuracy varied considerably. ADClust initially obtained 42 clusters, which were updated to 19 and 11 clusters in the second and third iteration, respectively. Even though each of these clusters only corresponded to a single cell type, starting from the fourth iteration, ADClust merged several groups of cells from different cell types into the same cluster. Its final result led to five clusters, all of which were mixtures of cells from multiple cell types. As for the SCCAF method, it stopped its merging process right after the initial iteration, leading to 17 clusters. It divided each of the eight cell types into multiple clusters, which made downstream annotations and comparisons error-prone. In contrast, scAce initially obtained 25 clusters, and gradually updated them to 17, 10, 9, and 8 clusters in the second to fifth iteration, respectively. Throughout this process, scAce was able to maintain the high purity of the identified clusters, and thus ultimately obtained clustering results that were in close agreement with the ground truth cell types (ARI = 0.93). Similar results were observed on the other six datasets (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S8</xref>).</p>
      <p>To further investigate the observed advantages of scAce, we evaluated the performance of other clustering methods when they were combined with the same cluster initialization and cluster merging approaches as proposed in scAce. Among the nine alternative methods, we studied the four methods which can take initialized clusters as input (scDeepCluster, DESC, SCCAF, and ADClust), and performed cluster initialization as in scAce. After obtaining the clustering results from these methods, we then performed an additional cluster merging step as in scAce. Compared with scAce (average ARI = 0.82), the second and third best method in this comparison was scDeepCluster (average ARI = 0.72) and ADClust (average ARI = 0.61), respectively (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S9A</xref>). Our results show that simply combining the proposed cluster initialization and cluster merging approaches with other existing methods does not optimize the clustering results (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S9 and S10</xref>). In contrast, the adaptive cluster merging approach used by scAce leads to improved results by combining cluster merging with network update in an iterative manner.</p>
    </sec>
    <sec>
      <title>3.3 scAce is robust to cluster initialization</title>
      <p>In the cluster initialization step, scAce uses the Leiden algorithm to obtain the initial clustering results. To evaluate the robustness of scAce to the selection of the initial clustering algorithm, we also applied scAce by using the Louvain or K-means algorithm to perform cluster initialization. The resolution parameter in Louvain was set to 2. For the cluster number in K-means, we tried different numbers between 15 and 30, and selected the number that maximized the corresponding silhouette coefficient.</p>
      <p>We calculated the final ARI and NMI values of scAce given the three different algorithms for cluster initialization. The difference in ARI values was between 0.001 and 0.069 across the seven real datasets, and the difference in NMI values was between 0.001 and 0.059 (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S11</xref>). Our results show that scAce is robust to the selection of the initial clustering algorithm. In fact, regardless of which initial clustering algorithm is used, as long as the initial clusters are of high purity, scAce is expected to merge highly similar small clusters into larger ones in the adaptive merging process. In addition to its robustness to initial clustering algorithms, we also observed that scAce was robust to the resolution parameter in the Leiden algorithm when its value was between 1.4 and 2.0 (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref>).</p>
    </sec>
    <sec>
      <title>3.4 scAce enhances the performance of existing clustering methods</title>
      <p>We provide a clustering enhancement option in the scAce method, which allows it to start from cluster labels inferred by another scRNA-seq clustering method and use its cluster splitting, VAE network update, and cluster merging steps to adaptively improve the clustering results (see Section 2 and <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods</xref>). To evaluate the effectiveness of this option, we implemented clustering enhancement by applying scAce with initial cluster labels obtained by Seurat or CIDR, both of which do not utilize neural networks or cluster merging approaches. Our results show that, given initial cluster labels from either Seurat or CIDR, scAce obviously improved the final clustering accuracy on most real datasets, in terms of both ARI and NMI scores (<xref rid="btad546-F4" ref-type="fig">Fig. 4</xref>).</p>
      <fig position="float" id="btad546-F4">
        <label>Figure 4.</label>
        <caption>
          <p>ARI and NMI values of clustering results before and after scAce’s clustering enhancement for Seurat and CIDR.</p>
        </caption>
        <graphic xlink:href="btad546f4" position="float"/>
      </fig>
      <p>We visualized the cell embeddings and cluster labels obtained by scAce right after pretraining and cluster initialization, and the final embeddings and labels after clustering enhancement, given initial results from Seurat (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S12</xref>) and CIDR (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S13</xref>), respectively. Compared with the original clustering results of Seurat and CIDR, scAce obtained smaller and purer clusters after its initialization (<xref rid="sup1" ref-type="supplementary-material">Supplementary Methods</xref>). <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S12B and S13B</xref> show the cell embeddings and cluster labels given scAce’s final output, which demonstrates scAce’s ability to enhance cluster assignment and learning of the latent space through adaptive cluster merging.</p>
    </sec>
    <sec>
      <title>3.5 Computational time and memory usage</title>
      <p>We measured the running time and maximum memory usage of scAce and the other nine methods on the seven real datasets (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S14</xref>). In the experiments, CIDR, SCCAF, and Seurat only needed to use CPUs, and the other seven methods needed GPUs. All experiments used a single core. The average running time of scAce ranked fourth, and Seurat and ADClust were the fastest. Although scAce was slightly slower than Seurat and ADClust, it was on average faster than the other deep-learning-based methods (scGMAAE, scVI, scDeepCluster, graph-sc, and DESC) (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S14A</xref>). As for the memory usage, the maximum memory usage of scAce was higher than most alternative methods, but was on the same magnitude as that of other deep-learning-based methods (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S14B</xref>).</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>In this article, we propose the scAce method for unsupervised clustering analysis of single cells using scRNA-seq data. Using a variational autoencoder network that adaptively learns both low-dimensional cell representations and cluster assignment, scAce allows for accurate clustering of cells without the need to predetermine the cluster number or other parameters indicating preferences on cluster resolution. We evaluated the performance of scAce in comparison with nine alternative clustering methods for scRNA-seq data based on both simulated and real datasets. Our results suggest that scAce outperforms existing state-of-the-art methods in terms of both clustering accuracy and robustness. Based on the clustering enhancement option of scAce, it is also possible to further improve the accuracy of an existing clustering assignment generated by other methods.</p>
    <p>Compared with existing deep embedding clustering algorithms that use traditional autoencoders to obtain low-dimensional representations, the VAE network used by scAce obtains better low-dimensional embeddings of single cells and therefore improves clustering results by enforcing a continuous and smooth latent space representation of the gene expression data. Another feature of scAce that contributes to its improved clustering performance is its ability to adaptively update the cluster assignments in a deep embedding framework. scAce starts with relatively large number of clusters which have a high purity and iteratively merges similar clusters together using the proposed adaptive cluster merging approach. This approach takes advantage of the trained VAE network and simultaneously achieves network update and cluster update to improve both cell embeddings and cluster assignments.</p>
    <p>When summarizing our computational results, we noticed that even though scAce successfully identified the rare cell type (with a cell proportion of 4%) in the simulated data, it did not always identify the rare cell types in the real datasets. Actually, none of the 10 methods evaluated in this work was able to consistently identify rare cell types that only accounted for between 0.027% and 1.498% of cells. In a recent benchmark study of 14 clustering methods, it was also reported that most methods significantly underestimated the true cell type numbers when the proportion of cells in rare cell types was around 2% (<xref rid="btad546-B41" ref-type="bibr">Yu <italic toggle="yes">et al.</italic> 2022</xref>). Since this is a systematic challenge, a future direction is to investigate how to further improve the identification of rare cell types. One possible solution is to first evaluate the heterogeneity of inferred cell clusters (<xref rid="btad546-B18" ref-type="bibr">Li 2022</xref>, <xref rid="btad546-B21" ref-type="bibr">Liu <italic toggle="yes">et al.</italic> 2020</xref>), and then perform another round of clustering analysis just using the cell clusters appearing to be mixtures of multiple cell types. When the focus is to discover rare cell types, an alternative solution is to modify the adaptive cluster merging process in scAce and use a more stringent merging criterion. Another future direction to consider is how to extend scAce to account for the cell type hierarchy. There have been a few methods which aim to use a tree structure to recover hierarchical relationships among cell types or account for this factor when evaluating clustering results (<xref rid="btad546-B38" ref-type="bibr">Wu and Wu 2020</xref>, <xref rid="btad546-B24" ref-type="bibr">Peng <italic toggle="yes">et al.</italic> 2021</xref>). As scAce is based on an adaptive cluster merging approach, it would be possible to learn cluster hierarchy from the merging orders.</p>
    <p>Although scAce has been developed as a clustering method for scRNA-seq data, we believe that its VAE framework and adaptive cluster merging approach can be extended to model additional types of data collected from other technologies, such as scATAC-seq and spatial transcriptomics (<xref rid="btad546-B17" ref-type="bibr">Lei <italic toggle="yes">et al.</italic> 2021</xref>). The framework can also be modified for noncount data by changing the output format of the decoder and the corresponding loss functions. Given the pivotal role of clustering analysis in single-cell studies and many other scientific problems, we anticipate scAce to be a useful method for discovering meaningful clusters in high-dimensional data.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btad546_Supplementary_Data</label>
      <media xlink:href="btad546_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We thank members of the Wei Vivian Li and Hongwei Li labs for the helpful discussions and suggestions. We also thank the anonymous reviewers for the insightful comments.</p>
  </ack>
  <sec>
    <title>Supplementary data</title>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Conflict of interest</title>
    <p>None declared.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the National Natural Science Foundation of China [42274172 to H.L.] and National Institutes of Health (National Institute of General Medical Sciences) [R35GM142702 to W.V.L.].</p>
  </sec>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>scAce is implemented as a Python package, which is freely available from its GitHub repository <ext-link xlink:href="https://github.com/sldyns/scAce" ext-link-type="uri">https://github.com/sldyns/scAce</ext-link>. The accession numbers of all real data used in this work are described in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>. The data and code used to reproduce the analyses are available at <ext-link xlink:href="https://github.com/sldyns/scAce/tree/main/reproducibility" ext-link-type="uri">https://github.com/sldyns/scAce/tree/main/reproducibility</ext-link>.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btad546-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adam</surname><given-names>M</given-names></string-name>, <string-name><surname>Potter</surname><given-names>AS</given-names></string-name>, <string-name><surname>Potter</surname><given-names>SS</given-names></string-name></person-group><etal>et al</etal><article-title>Psychrophilic proteases dramatically reduce single-cell RNA-seq artifacts: a molecular atlas of kidney development</article-title>. <source>Development</source><year>2017</year>;<volume>144</volume>:<fpage>3625</fpage>–<lpage>32</lpage>.<pub-id pub-id-type="pmid">28851704</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baron</surname><given-names>M</given-names></string-name>, <string-name><surname>Veres</surname><given-names>A</given-names></string-name>, <string-name><surname>Wolock</surname><given-names>SL</given-names></string-name></person-group><etal>et al</etal><article-title>A single-cell transcriptomic map of the human and mouse pancreas reveals inter-and intra-cell population structure</article-title>. <source>Cell Syst</source><year>2016</year>;<volume>3</volume>:<fpage>346</fpage>–<lpage>60.e4</lpage>.<pub-id pub-id-type="pmid">27667365</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blondel</surname><given-names>VD</given-names></string-name>, <string-name><surname>Guillaume</surname><given-names>J-L</given-names></string-name>, <string-name><surname>Lambiotte</surname><given-names>R</given-names></string-name></person-group><etal>et al</etal><article-title>Fast unfolding of communities in large networks</article-title>. <source>J Stat Mech</source><year>2008</year>;<volume>2008</volume>:<fpage>P10008</fpage>.</mixed-citation>
    </ref>
    <ref id="btad546-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>R</given-names></string-name>, <string-name><surname>Wu</surname><given-names>X</given-names></string-name>, <string-name><surname>Jiang</surname><given-names>L</given-names></string-name></person-group><etal>et al</etal><article-title>Single-cell RNA-seq reveals hypothalamic cell diversity</article-title>. <source>Cell Rep</source><year>2017</year>;<volume>18</volume>:<fpage>3227</fpage>–<lpage>41</lpage>.<pub-id pub-id-type="pmid">28355573</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Choi</surname><given-names>Y</given-names></string-name>, <string-name><surname>Li</surname><given-names>R</given-names></string-name>, <string-name><surname>Quon</surname><given-names>G</given-names></string-name></person-group><etal>et al</etal><article-title>sivae: interpretable deep generative models for single-cell transcriptomes</article-title>. <source>Genome Biol</source><year>2023</year>;<volume>24</volume>:<fpage>29</fpage>.<pub-id pub-id-type="pmid">36803416</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ciortan</surname><given-names>M</given-names></string-name>, <string-name><surname>Defrance</surname><given-names>M.</given-names></string-name></person-group><article-title>GNN-based embedding for clustering scRNA-seq data</article-title>. <source>Bioinformatics</source><year>2022</year>;<volume>38</volume>:<fpage>1037</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">34850828</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deng</surname><given-names>Y</given-names></string-name>, <string-name><surname>Bao</surname><given-names>F</given-names></string-name>, <string-name><surname>Dai</surname><given-names>Q</given-names></string-name></person-group><etal>et al</etal><article-title>Scalable analysis of cell-type composition from single-cell transcriptomics using deep recurrent learning</article-title>. <source>Nat Methods</source><year>2019</year>;<volume>16</volume>:<fpage>311</fpage>–<lpage>4</lpage>.<pub-id pub-id-type="pmid">30886411</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eraslan</surname><given-names>G</given-names></string-name>, <string-name><surname>Simon</surname><given-names>LM</given-names></string-name>, <string-name><surname>Mircea</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Single-cell RNA-seq denoising using a deep count autoencoder</article-title>. <source>Nat Commun</source><year>2019</year>;<volume>10</volume>:<fpage>390</fpage>.<pub-id pub-id-type="pmid">30674886</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grønbech</surname><given-names>CH</given-names></string-name>, <string-name><surname>Vording</surname><given-names>MF</given-names></string-name>, <string-name><surname>Timshel</surname><given-names>PN</given-names></string-name></person-group><etal>et al</etal><article-title>scvae: variational auto-encoders for single-cell gene expression data</article-title>. <source>Bioinformatics</source><year>2020</year>;<volume>36</volume>:<fpage>4415</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">32415966</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Guo</surname><given-names>X</given-names></string-name>, <string-name><surname>Gao</surname><given-names>L</given-names></string-name>, <string-name><surname>Liu</surname><given-names>X</given-names></string-name></person-group><etal>et al</etal> Improved deep embedded clustering with local structure preservation. In: <italic toggle="yes">Proceedings of 26th International Joint Conference on Artificial Intelligence, IJCAI</italic>, Melbourne, Australia, <year>2017</year>, <fpage>1753</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="btad546-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Higgins</surname><given-names>I</given-names></string-name>, <string-name><surname>Matthey</surname><given-names>L</given-names></string-name>, <string-name><surname>Pal</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal> beta-vae: Learning basic visual concepts with a constrained variational framework. In: 5<italic toggle="yes"><italic toggle="yes">th</italic> International Conference on Learning Representations</italic>, <italic toggle="yes">Toulon, France</italic>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="btad546-B12">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kingma</surname><given-names>DP</given-names></string-name>, <string-name><surname>Welling</surname><given-names>M</given-names></string-name></person-group>. Auto-encoding variational Bayes. arXiv prerint, arXiv:1312.6114, <year>2022</year>, preprint: not peer reviewed.</mixed-citation>
    </ref>
    <ref id="btad546-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kiselev</surname><given-names>VY</given-names></string-name>, <string-name><surname>Andrews</surname><given-names>TS</given-names></string-name>, <string-name><surname>Hemberg</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Challenges in unsupervised clustering of single-cell RNA-seq data</article-title>. <source>Nat Rev Genet</source><year>2019</year>;<volume>20</volume>:<fpage>273</fpage>–<lpage>82</lpage>.<pub-id pub-id-type="pmid">30617341</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kiselev</surname><given-names>VY</given-names></string-name>, <string-name><surname>Kirschner</surname><given-names>K</given-names></string-name>, <string-name><surname>Schaub</surname><given-names>MT</given-names></string-name></person-group><etal>et al</etal><article-title>Sc3: consensus clustering of single-cell RNA-seq data</article-title>. <source>Nat Methods</source><year>2017</year>;<volume>14</volume>:<fpage>483</fpage>–<lpage>6</lpage>.<pub-id pub-id-type="pmid">28346451</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klein</surname><given-names>AM</given-names></string-name>, <string-name><surname>Mazutis</surname><given-names>L</given-names></string-name>, <string-name><surname>Akartuna</surname><given-names>I</given-names></string-name></person-group><etal>et al</etal><article-title>Droplet barcoding for single-cell transcriptomics applied to embryonic stem cells</article-title>. <source>Cell</source><year>2015</year>;<volume>161</volume>:<fpage>1187</fpage>–<lpage>201</lpage>.<pub-id pub-id-type="pmid">26000487</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lei</surname><given-names>J</given-names></string-name>, <string-name><surname>Jiang</surname><given-names>T</given-names></string-name>, <string-name><surname>Wu</surname><given-names>K</given-names></string-name></person-group><etal>et al</etal><article-title>Robust k-means algorithm with automatically splitting and merging clusters and its applications for surveillance data</article-title>. <source>Multimed Tools Appl</source><year>2016</year>;<volume>75</volume>:<fpage>12043</fpage>–<lpage>59</lpage>.</mixed-citation>
    </ref>
    <ref id="btad546-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lei</surname><given-names>Y</given-names></string-name>, <string-name><surname>Tang</surname><given-names>R</given-names></string-name>, <string-name><surname>Xu</surname><given-names>J</given-names></string-name></person-group><etal>et al</etal><article-title>Applications of single-cell sequencing in cancer research: progress and perspectives</article-title>. <source>J Hematol Oncol</source><year>2021</year>;<volume>14</volume>:<fpage>91</fpage>.<pub-id pub-id-type="pmid">34108022</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>WV.</given-names></string-name></person-group><article-title>Phitest for analyzing the homogeneity of single-cell populations</article-title>. <source>Bioinformatics</source><year>2022</year>;<volume>38</volume>:<fpage>2639</fpage>–<lpage>41</lpage>.<pub-id pub-id-type="pmid">35238346</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>X</given-names></string-name>, <string-name><surname>Wang</surname><given-names>K</given-names></string-name>, <string-name><surname>Lyu</surname><given-names>Y</given-names></string-name></person-group><etal>et al</etal><article-title>Deep learning enables accurate clustering with batch effect removal in single-cell RNA-seq analysis</article-title>. <source>Nat Commun</source><year>2020</year>;<volume>11</volume>:<fpage>2338</fpage>.<pub-id pub-id-type="pmid">32393754</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>P</given-names></string-name>, <string-name><surname>Troup</surname><given-names>M</given-names></string-name>, <string-name><surname>Ho</surname><given-names>JWK</given-names></string-name></person-group><etal>et al</etal><article-title>CIDR: ultrafast and accurate clustering through imputation for single-cell RNA-seq data</article-title>. <source>Genome Biol</source><year>2017</year>;<volume>18</volume>:<fpage>59</fpage>.<pub-id pub-id-type="pmid">28351406</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B</given-names></string-name>, <string-name><surname>Li</surname><given-names>C</given-names></string-name>, <string-name><surname>Li</surname><given-names>Z</given-names></string-name></person-group><etal>et al</etal><article-title>An entropy-based metric for assessing the purity of single cell populations</article-title>. <source>Nat Commun</source><year>2020</year>;<volume>11</volume>:<fpage>3155</fpage>.<pub-id pub-id-type="pmid">32572028</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lopez</surname><given-names>R</given-names></string-name>, <string-name><surname>Regier</surname><given-names>J</given-names></string-name>, <string-name><surname>Cole</surname><given-names>MB</given-names></string-name></person-group><etal>et al</etal><article-title>Deep generative modeling for single-cell transcriptomics</article-title>. <source>Nat Methods</source><year>2018</year>;<volume>15</volume>:<fpage>1053</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">30504886</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miao</surname><given-names>Z</given-names></string-name>, <string-name><surname>Moreno</surname><given-names>P</given-names></string-name>, <string-name><surname>Huang</surname><given-names>N</given-names></string-name></person-group><etal>et al</etal><article-title>Putative cell type discovery from single-cell gene expression data</article-title>. <source>Nat Methods</source><year>2020</year>;<volume>17</volume>:<fpage>621</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">32424270</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname><given-names>M</given-names></string-name>, <string-name><surname>Wamsley</surname><given-names>B</given-names></string-name>, <string-name><surname>Elkins</surname><given-names>AG</given-names></string-name></person-group><etal>et al</etal><article-title>Cell type hierarchy reconstruction via reconciliation of multi-resolution cluster tree</article-title>. <source>Nucleic Acids Res</source><year>2021</year>;<volume>49</volume>:<fpage>e91</fpage>.<pub-id pub-id-type="pmid">34125905</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petegrosso</surname><given-names>R</given-names></string-name>, <string-name><surname>Li</surname><given-names>Z</given-names></string-name>, <string-name><surname>Kuang</surname><given-names>R.</given-names></string-name></person-group><article-title>Machine learning and statistical methods for clustering single-cell rna-sequencing data</article-title>. <source>Brief Bioinform</source><year>2020</year>;<volume>21</volume>:<fpage>1209</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">31243426</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Qi</surname><given-names>R</given-names></string-name>, <string-name><surname>Ma</surname><given-names>A</given-names></string-name>, <string-name><surname>Ma</surname><given-names>Q</given-names></string-name></person-group><etal>et al</etal><article-title>Clustering and classification methods for single-cell RNA-sequencing data</article-title>. <source>Brief Bioinform</source><year>2020</year>;<volume>21</volume>:<fpage>1196</fpage>–<lpage>208</lpage>.<pub-id pub-id-type="pmid">31271412</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Satija</surname><given-names>R</given-names></string-name>, <string-name><surname>Farrell</surname><given-names>JA</given-names></string-name>, <string-name><surname>Gennert</surname><given-names>D</given-names></string-name></person-group><etal>et al</etal><article-title>Spatial reconstruction of single-cell gene expression data</article-title>. <source>Nat Biotechnol</source><year>2015</year>;<volume>33</volume>:<fpage>495</fpage>–<lpage>502</lpage>.<pub-id pub-id-type="pmid">25867923</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sheng</surname><given-names>J</given-names></string-name>, <string-name><surname>Li</surname><given-names>WV.</given-names></string-name></person-group><article-title>Selecting gene features for unsupervised analysis of single-cell gene expression data</article-title>. <source>Brief Bioinform</source><year>2021</year>;<volume>22</volume>:<fpage>bbab295</fpage>.<pub-id pub-id-type="pmid">34351383</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname><given-names>T</given-names></string-name>, <string-name><surname>Song</surname><given-names>D</given-names></string-name>, <string-name><surname>Li</surname><given-names>WV</given-names></string-name></person-group><etal>et al</etal><article-title>scdesign2: a transparent simulator that generates high-fidelity single-cell gene expression count data with gene correlations captured</article-title>. <source>Genome Biol</source><year>2021</year>;<volume>22</volume>:<fpage>163</fpage>.<pub-id pub-id-type="pmid">34034771</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname><given-names>T</given-names></string-name>, <string-name><surname>Wan</surname><given-names>J</given-names></string-name>, <string-name><surname>Song</surname><given-names>Q</given-names></string-name></person-group><etal>et al</etal><article-title>Clustering single-cell RNA-seq data with a model-based deep learning approach</article-title>. <source>Nat Mach Intell</source><year>2019</year>;<volume>1</volume>:<fpage>191</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="btad546-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname><given-names>T</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>J</given-names></string-name>, <string-name><surname>Lin</surname><given-names>X</given-names></string-name></person-group><etal>et al</etal><article-title>Model-based deep embedding for constrained clustering analysis of single cell RNA-seq data</article-title>. <source>Nat Commun</source><year>2021</year>;<volume>12</volume>:<fpage>1873</fpage>.<pub-id pub-id-type="pmid">33767149</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tosches</surname><given-names>MA</given-names></string-name>, <string-name><surname>Yamawaki</surname><given-names>TM</given-names></string-name>, <string-name><surname>Naumann</surname><given-names>RK</given-names></string-name></person-group><etal>et al</etal><article-title>Evolution of pallium, hippocampus, and cortical cell types revealed by single-cell transcriptomics in reptiles</article-title>. <source>Science</source><year>2018</year>;<volume>360</volume>:<fpage>881</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">29724907</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Traag</surname><given-names>VA</given-names></string-name>, <string-name><surname>Waltman</surname><given-names>L</given-names></string-name>, <string-name><surname>van Eck</surname><given-names>NJ</given-names></string-name></person-group><etal>et al</etal><article-title>From louvain to leiden: guaranteeing well-connected communities</article-title>. <source>Sci Rep</source><year>2019</year>;<volume>9</volume>:<fpage>5233</fpage>.<pub-id pub-id-type="pmid">30914743</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>D</given-names></string-name>, <string-name><surname>Gu</surname><given-names>J.</given-names></string-name></person-group><article-title>Vasc: dimension reduction and visualization of single-cell RNA-seq data by deep variational autoencoder</article-title>. <source>Genomics Proteomics Bioinf</source><year>2018</year>;<volume>16</volume>:<fpage>320</fpage>–<lpage>31</lpage>.</mixed-citation>
    </ref>
    <ref id="btad546-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>H-Y</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>J-P</given-names></string-name>, <string-name><surname>Zheng</surname><given-names>C-H</given-names></string-name></person-group>, <etal>et al</etal><article-title>scgmaae: Gaussian mixture adversarial autoencoders for diversification analysis of scRNA-seq data</article-title>. <source>Brief Bioinform</source><year>2023</year>;<volume>24</volume>:<fpage>bbac585</fpage>.<pub-id pub-id-type="pmid">36592058</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ward</surname><given-names>JH</given-names><suffix>Jr</suffix></string-name></person-group>. <article-title>Hierarchical grouping to optimize an objective function</article-title>. <source>J Am Stat Assoc</source><year>1963</year>;<volume>58</volume>:<fpage>236</fpage>–<lpage>44</lpage>.</mixed-citation>
    </ref>
    <ref id="btad546-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolf</surname><given-names>FA</given-names></string-name>, <string-name><surname>Angerer</surname><given-names>P</given-names></string-name>, <string-name><surname>Theis</surname><given-names>FJ</given-names></string-name></person-group><etal>et al</etal><article-title>Scanpy: large-scale single-cell gene expression data analysis</article-title>. <source>Genome Biol</source><year>2018</year>;<volume>19</volume>:<fpage>15</fpage>.<pub-id pub-id-type="pmid">29409532</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>Z</given-names></string-name>, <string-name><surname>Wu</surname><given-names>H.</given-names></string-name></person-group><article-title>Accounting for cell type hierarchy in evaluating single cell RNA-seq clustering</article-title>. <source>Genome Biol</source><year>2020</year>;<volume>21</volume>:<fpage>123</fpage>.<pub-id pub-id-type="pmid">32450895</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B39">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Xie</surname><given-names>J</given-names></string-name>, <string-name><surname>Girshick</surname><given-names>R</given-names></string-name>, <string-name><surname>Farhadi</surname><given-names>A.</given-names></string-name></person-group> Unsupervised deep embedding for clustering analysis. In: <italic toggle="yes">International Conference on Machine Learning.</italic> New York, United States: PMLR <year>2016</year>, <fpage>478</fpage>–<lpage>87</lpage>.</mixed-citation>
    </ref>
    <ref id="btad546-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Young</surname><given-names>MD</given-names></string-name>, <string-name><surname>Mitchell</surname><given-names>TJ</given-names></string-name>, <string-name><surname>Vieira Braga</surname><given-names>FA</given-names></string-name></person-group><etal>et al</etal><article-title>Single-cell transcriptomes from human kidneys reveal the cellular identity of renal tumors</article-title>. <source>Science</source><year>2018</year>;<volume>361</volume>:<fpage>594</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">30093597</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>L</given-names></string-name>, <string-name><surname>Cao</surname><given-names>Y</given-names></string-name>, <string-name><surname>Yang</surname><given-names>JYH</given-names></string-name></person-group><etal>et al</etal><article-title>Benchmarking clustering algorithms on estimating the number of cell types from single-cell RNA-sequencing data</article-title>. <source>Genome Biol</source><year>2022</year>;<volume>23</volume>:<fpage>49</fpage>.<pub-id pub-id-type="pmid">35135612</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>Y</given-names></string-name></person-group><etal>et al</etal><article-title>A parameter-free deep embedded clustering method for single-cell RNA-seq data</article-title>. <source>Brief Bioinform</source><year>2022</year>;<volume>23</volume>:<fpage>bbac172</fpage>.<pub-id pub-id-type="pmid">35524494</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zheng</surname><given-names>GXY</given-names></string-name>, <string-name><surname>Terry</surname><given-names>JM</given-names></string-name>, <string-name><surname>Belgrader</surname><given-names>P</given-names></string-name></person-group><etal>et al</etal><article-title>Massively parallel digital transcriptional profiling of single cells</article-title>. <source>Nat Commun</source><year>2017</year>;<volume>8</volume>:<fpage>14049</fpage>.<pub-id pub-id-type="pmid">28091601</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10500084</article-id>
    <article-id pub-id-type="pmid">37672035</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btad546</article-id>
    <article-id pub-id-type="publisher-id">btad546</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Gene Expression</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>scAce: an adaptive embedding and clustering method for single-cell gene expression data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>He</surname>
          <given-names>Xinwei</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Formal analysis" vocab-term-identifier="http://credit.niso.org/contributor-roles/formal-analysis" degree-contribution="equal">Formal analysis</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology" degree-contribution="equal">Methodology</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Software" vocab-term-identifier="http://credit.niso.org/contributor-roles/software" degree-contribution="equal">Software</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-original-draft" degree-contribution="equal">Writing - original draft</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - review &amp; editing" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="supporting">Writing - review &amp; editing</role>
        <aff><institution>School of Mathematics and Physics, China University of Geosciences</institution>, Wuhan 430074, <country country="CN">China</country></aff>
        <xref rid="btad546-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Qian</surname>
          <given-names>Kun</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Formal analysis" vocab-term-identifier="http://credit.niso.org/contributor-roles/formal-analysis" degree-contribution="equal">Formal analysis</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology" degree-contribution="equal">Methodology</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Software" vocab-term-identifier="http://credit.niso.org/contributor-roles/software" degree-contribution="equal">Software</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-original-draft" degree-contribution="supporting">Writing - original draft</role>
        <aff><institution>School of Mathematics and Physics, China University of Geosciences</institution>, Wuhan 430074, <country country="CN">China</country></aff>
        <xref rid="btad546-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Ziqian</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology" degree-contribution="supporting">Methodology</role>
        <aff><institution>School of Mathematics and Physics, China University of Geosciences</institution>, Wuhan 430074, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zeng</surname>
          <given-names>Shirou</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-original-draft" degree-contribution="supporting">Writing - original draft</role>
        <aff><institution>School of Mathematics and Physics, China University of Geosciences</institution>, Wuhan 430074, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Li</surname>
          <given-names>Hongwei</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Funding acquisition" vocab-term-identifier="http://credit.niso.org/contributor-roles/funding-acquisition" degree-contribution="equal">Funding acquisition</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Supervision" vocab-term-identifier="http://credit.niso.org/contributor-roles/supervision" degree-contribution="equal">Supervision</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - review &amp; editing" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="supporting">Writing - review &amp; editing</role>
        <aff><institution>School of Mathematics and Physics, China University of Geosciences</institution>, Wuhan 430074, <country country="CN">China</country></aff>
        <xref rid="btad546-cor1" ref-type="corresp"/>
        <!--hwli@cug.edu.cn-->
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2087-2709</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Wei Vivian</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization" degree-contribution="equal">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Funding acquisition" vocab-term-identifier="http://credit.niso.org/contributor-roles/funding-acquisition" degree-contribution="equal">Funding acquisition</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology" degree-contribution="equal">Methodology</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Supervision" vocab-term-identifier="http://credit.niso.org/contributor-roles/supervision" degree-contribution="equal">Supervision</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-original-draft" degree-contribution="equal">Writing - original draft</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org" vocab-term="Writing - review &amp; editing" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="lead">Writing - review &amp; editing</role>
        <aff><institution>Department of Statistics, University of California, Riverside</institution>, Riverside 92521, <country country="US">United States</country></aff>
        <xref rid="btad546-cor1" ref-type="corresp"/>
        <!--weil@ucr.edu-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Nikolski</surname>
          <given-names>Macha</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btad546-cor1">Corresponding authors. Department of Statistics, University of California, Riverside, 900 University Ave., Riverside, CA 92521, United States. E-mail: <email>weil@ucr.edu</email> (W.V.L.); School of Mathematics and Physics, China University of Geosciences, No. 388 Lumo Road, Wuhan 430074, China. E-mail: <email>hwli@cug.edu.cn</email> (H.L.)</corresp>
      <fn id="btad546-FM1">
        <p>Xinwei He and Kun Qian Equal contribution.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <month>9</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-09-06">
      <day>06</day>
      <month>9</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>06</day>
      <month>9</month>
      <year>2023</year>
    </pub-date>
    <volume>39</volume>
    <issue>9</issue>
    <elocation-id>btad546</elocation-id>
    <history>
      <date date-type="received">
        <day>04</day>
        <month>5</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>01</day>
        <month>8</month>
        <year>2023</year>
      </date>
      <date date-type="editorial-decision">
        <day>30</day>
        <month>8</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>05</day>
        <month>9</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>13</day>
        <month>9</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btad546.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Since the development of single-cell RNA sequencing (scRNA-seq) technologies, clustering analysis of single-cell gene expression data has been an essential tool for distinguishing cell types and identifying novel cell types. Even though many methods have been available for scRNA-seq clustering analysis, the majority of them are constrained by the requirement on predetermined cluster numbers or the dependence on selected initial cluster assignment.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>In this article, we propose an adaptive embedding and clustering method named scAce, which constructs a variational autoencoder to simultaneously learn cell embeddings and cluster assignments. In the scAce method, we develop an adaptive cluster merging approach which achieves improved clustering results without the need to estimate the number of clusters in advance. In addition, scAce provides an option to perform clustering enhancement, which can update and enhance cluster assignments based on previous clustering results from other methods. Based on computational analysis of both simulated and real datasets, we demonstrate that scAce outperforms state-of-the-art clustering methods for scRNA-seq data, and achieves better clustering accuracy and robustness.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The scAce package is implemented in python 3.8 and is freely available from <ext-link xlink:href="https://github.com/sldyns/scAce" ext-link-type="uri">https://github.com/sldyns/scAce</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>42274172</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health (National Institute of General Medical Sciences</institution>
          </institution-wrap>
        </funding-source>
        <award-id>R35GM142702</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="10"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Advances in single-cell RNA sequencing (scRNA-seq) technologies have made them powerful tools for understanding heterogeneous gene expression in diverse cell populations and for quantifying single-cell activities in the study of development, physiology, and disease. In computational analysis of scRNA-seq data, unsupervised clustering is a crucial approach for identifying distinct cell populations based on their gene expression levels (<xref rid="btad546-B13" ref-type="bibr">Kiselev <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btad546-B28" ref-type="bibr">Sheng and Li 2021</xref>, <xref rid="btad546-B18" ref-type="bibr">Li 2022</xref>). By unsupervised clustering, it is possible to identify clusters of cells and then annotate them as known or novel cell types based on prior knowledge of marker genes and biological pathways. However, due to the high sparsity and high-dimensional nature of scRNA-seq data (<xref rid="btad546-B25" ref-type="bibr">Petegrosso <italic toggle="yes">et al.</italic> 2020</xref>, <xref rid="btad546-B26" ref-type="bibr">Qi <italic toggle="yes">et al.</italic> 2020</xref>), it is challenging to cluster single cells directly using generic clustering methods.</p>
    <p>To better account for the characteristics of scRNA-seq data, new clustering methods tailored for single-cell gene expression levels have been developed. Earlier methods, such as Seurat (<xref rid="btad546-B27" ref-type="bibr">Satija <italic toggle="yes">et al.</italic> 2015</xref>), SC3 (<xref rid="btad546-B14" ref-type="bibr">Kiselev <italic toggle="yes">et al.</italic> 2017</xref>), and CIDR (<xref rid="btad546-B20" ref-type="bibr">Lin <italic toggle="yes">et al.</italic> 2017</xref>), treat dimensionality reduction and cell clustering as two successive steps. They first use principal component analysis to reduce the dimensions of the gene expression matrix or the cell-cell distance matrix, and then utilize a generic clustering method, such as the Louvain (<xref rid="btad546-B3" ref-type="bibr">Blondel <italic toggle="yes">et al.</italic> 2008</xref>) or hierarchical clustering (<xref rid="btad546-B36" ref-type="bibr">Ward <italic toggle="yes">et al.</italic> 1963</xref>) to obtain the inferred cluster labels. More recent methods such as scScope (<xref rid="btad546-B7" ref-type="bibr">Deng <italic toggle="yes">et al.</italic> 2019</xref>) use an autoencoder, a deep-learning-based model, to learn low-dimensional latent representation of data and then perform cell clustering on the low-dimensional features. Another example is graph-sc (<xref rid="btad546-B6" ref-type="bibr">Ciortan and Defrance 2022</xref>), which uses a convolutional graph autoencoder to process a gene-to-cell graph before applying the K-means or Leiden (<xref rid="btad546-B33" ref-type="bibr">Traag <italic toggle="yes">et al.</italic> 2019</xref>) clustering algorithm. Since the latent space learned by traditional autoencoders is discontinuous and unregularized, deep embedding methods based on variational autoencoder (VAE) networks have gained popularity in scRNA-seq analysis. VAEs can learn a continuous latent representation of the input data by constraining the distribution of the latent variables to follow a prior distribution. VAE-based methods such as VASC (<xref rid="btad546-B34" ref-type="bibr">Wang and Gu 2018</xref>) and siVAE (<xref rid="btad546-B5" ref-type="bibr">Choi <italic toggle="yes">et al.</italic> 2023</xref>) focus more on the cell embedding problem, while other methods such as scVI (<xref rid="btad546-B22" ref-type="bibr">Lopez <italic toggle="yes">et al.</italic> 2018</xref>), scVAE (<xref rid="btad546-B9" ref-type="bibr">Grønbech <italic toggle="yes">et al.</italic> 2020</xref>), and scGMAAE (<xref rid="btad546-B35" ref-type="bibr">Wang <italic toggle="yes">et al.</italic> 2023</xref>) can simultaneously achieve cell embedding and clustering.</p>
    <p>As most clustering methods for scRNA-seq data rely on external or internal dimensionality reduction as an intermediate step, the quality of the low-dimensional representation has significant impact on the downstream clustering accuracy. Instead of treating dimensionality reduction and clustering as two separate tasks, some clustering methods based on deep embedding aim to simultaneously learn low-dimensional embeddings and clusters (<xref rid="btad546-B39" ref-type="bibr">Xie <italic toggle="yes">et al.</italic> 2016</xref>, <xref rid="btad546-B10" ref-type="bibr">Guo <italic toggle="yes">et al.</italic> 2017</xref>). By adapting this idea of deep embedding clustering to cluster single cells, multiple methods have been proposed for scRNA-seq data. For example, DESC (<xref rid="btad546-B19" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2020</xref>) applies deep embedding clustering to scRNA-seq data after normalization and gene selection, with cluster centers initialized by the Louvain algorithm. scDeepCluster (<xref rid="btad546-B30" ref-type="bibr">Tian <italic toggle="yes">et al.</italic> 2019</xref>) and scDCC (<xref rid="btad546-B31" ref-type="bibr">Tian <italic toggle="yes">et al.</italic> 2021</xref>) extended this idea by incorporating a zero-inflated negative binomial (ZINB) model, which was first proposed in the DCA method for scRNA-seq denoising (<xref rid="btad546-B8" ref-type="bibr">Eraslan <italic toggle="yes">et al.</italic> 2019</xref>), into an autoencoder network. Even though these methods based on deep embedding sometimes lead to better clustering results by allowing for nonlinear transformations, one limitation they share is that their optimization procedure depends on cell clusters initialized by a generic algorithm such as K-means or Louvain. The K-means algorithm requires a cluster number as input to perform clustering, while the Louvain algorithm requires a resolution parameter to control the size of the clusters. If these parameters are mis-specified or if the initial clustering results contain too many incorrect mixtures of cell populations, these errors will propagate to the iterative update of neural networks, affecting the accuracy of final clustering results.</p>
    <p>In light of the above limitation, some clustering methods attempt to reduce the dependence on predetermined parameters (such as cluster number) by starting with a relatively large number of micro clusters and gradually merging similar ones into larger clusters. For example, both SCCAF (<xref rid="btad546-B23" ref-type="bibr">Miao <italic toggle="yes">et al.</italic> 2020</xref>) and ADClust (<xref rid="btad546-B42" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>) obtain initial clusters via the Louvain algorithm. Then, SCCAF iteratively updates the cluster labels by training a classifier on the clusters and evaluating the similarities between the clusters based on a confusion matrix; ADClust uses a unimodality test to evaluate the similarity between clusters and identify those that could be merged. Although cluster merging has reduced their reliance on the initial cluster number, as we will show in our results, their merging process uses non-data-adaptive stopping criteria and could be prone to under-clustering or over-clustering on certain datasets.</p>
    <p>Inspired by these preceding endeavors in clustering analysis, we propose a method named scAce for scRNA-seq data to simultaneously achieve embedding of gene expression data and clustering of single cells. The scAce method constructs a VAE network to learn smoother low-dimensional embeddings compared with those methods based on traditional autoencoders. It utilizes a data-adaptive clustering approach based on the idea of cluster merging, and the merging process is controlled by evaluating intra-cluster and inter-cluster distances. By iteratively updating the VAE network and the cluster labels, scAce improves both the embedding and clustering of single cells. Another feature of the scAce method is that it enables clustering enhancement by taking the clustering results of another method as its initialization, and then uses its network model to further enhance the accuracy of final clusters. We have assessed the clustering performance of scAce in comparison with state-of-the-art clustering methods. The results show that scAce is more accurate and robust on both simulated and real scRNA-seq data. In addition, with its cluster enhancement option, scAce is able to correct and improve previous clustering results produced by other clustering methods.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 An outline of the scAce method</title>
      <p>The scAce method is consisted of three major steps, a pretraining step based on an improved variational autoencoder (<italic toggle="yes">β</italic>-VAE) (<xref rid="btad546-B11" ref-type="bibr">Higgins <italic toggle="yes">et al.</italic> 2017</xref>), a cluster initialization step to obtain initial cluster labels, and an adaptive cluster merging step to iteratively update cluster labels and cell embeddings (<xref rid="btad546-F1" ref-type="fig">Fig. 1</xref>). We introduce each of these steps in detail below.</p>
      <fig position="float" id="btad546-F1">
        <label>Figure 1.</label>
        <caption>
          <p>Overview of the scAce method. (A) Pretraining. scAce takes the single-cell gene expression matrix as its input to train a VAE network. The encoder learns low-dimensional hidden variables for the single cells, which serve as the input of the decoder. For each gene, the VAE learns and outputs three parameters of a ZINB distribution (mean, dispersion, and inflated proportion of zero). (B) Cluster initialization. With <italic toggle="yes">de novo</italic> initialization, the Leiden algorithm is used to obtain initial cluster labels. With clustering enhancement, initial cluster labels are obtained by applying a cluster splitting approach to a set of existing clustering results (from another clustering method). (C) Adaptive cluster merging. Given the pretrained VAE network and the initial cluster labels, the network parameters, cell embeddings, cluster labels, and centroids are iteratively updated by alternately performing network update and cluster merging steps. The final results of cell embeddings and cluster labels are output by scAce after the iteration process stops.</p>
        </caption>
        <graphic xlink:href="btad546f1" position="float"/>
      </fig>
      <p><bold><italic toggle="yes">Pretraining</italic></bold>. The input of scAce is a scRNA-seq read count matrix <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, based on which scAce will obtain the normalized expression matrix <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref rid="sup1" ref-type="supplementary-material">Supplementary Methods</xref>), where <italic toggle="yes">m</italic> and <italic toggle="yes">n</italic> are the numbers of genes and cells after preprocessing, respectively. In the VAE, the expression levels are first encoded to obtain the mean and variance parameters in the hidden layer. Then, the decoder receives a hidden variable <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo>≪</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> generated in the lower-dimensional space produced by the encoder. Based on <italic toggle="yes">Z</italic>, the decoder learns three parameters (mean, dispersion, and inflated zero proportion) of a ZINB distribution for every gene (<xref rid="btad546-F1" ref-type="fig">Fig. 1A</xref>).</p>
      <p><bold><italic toggle="yes">Cluster initialization</italic></bold>. scAce provides two options to perform cluster initialization (<xref rid="btad546-F1" ref-type="fig">Fig. 1B</xref>). The first option is used when scAce is applied in a <italic toggle="yes">de novo</italic> manner, and an existing clustering result is not available. In this case, scAce uses the Leiden algorithm in Scanpy (<xref rid="btad546-B37" ref-type="bibr">Wolf <italic toggle="yes">et al.</italic> 2018</xref>) to obtain the initial cluster assignments and centroids. For each cluster, the initial cluster centroid is the center of the cluster based on the hidden variable <italic toggle="yes">Z</italic> obtained from the pretraining step and the Euclidean distance. The resolution parameter of the Leiden algorithm is set to a large value (defaults to 2) so that the clusters have high purity. The second option is used when enhancement of an existing clustering result (e.g. obtained using Seurat) is desired. When using this option, scAce takes the cluster labels from the existing clustering result, and applies an adaptive cluster splitting method (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods</xref>) to split the current clusters into smaller and purer clusters based on intra-cluster distances. Then, scAce uses the new clusters after cluster splitting as the initial cluster assignment and obtains cluster centroids as described above.</p>
      <p>The purpose of cluster splitting in clustering enhancement or using a large resolution parameter in the <italic toggle="yes">de novo</italic> initialization is to ensure that each of the initial clusters contains cells of the same cell type or state. Even though there might be multiple clusters in the initial assignment that belong to the same cell types, they will be merged into the same cluster at the adaptive cluster merging step. This approach was inspired by the observation that methods depending on initial clustering results of the K-means or Louvain algorithm are likely to generate mixed clusters if the initial ones contain a large proportion of mixtures.</p>
      <p><bold><italic toggle="yes">Adaptive cluster merging</italic></bold>. In the step of adaptive cluster merging, scAce iteratively performs network update and cluster merging based on the initial VAE network obtained from the pretraining step and the clusters from the cluster initialization step (<xref rid="btad546-F1" ref-type="fig">Fig. 1C</xref>). At each iteration of network update, scAce constructs a loss function consisted of two components. The first component is the loss of the VAE network and the second component is a clustering loss defined based on cluster labels and centroids. Given this loss function, the VAE network is updated to improve cell embeddings. At each iteration of cluster merging, scAce decides if a pair of clusters should be merged into a single cluster by comparing inter-cluster and intra-cluster distances. Network update and cluster merging are performed alternately in scAce until no clusters can be merged in the cluster merging step.</p>
    </sec>
    <sec>
      <title>2.2 Pretraining: ZINB-based variational autoencoder network</title>
      <p>Since the latent space learned by traditional autoencoders are discontinuous and unregularized, which is not ideal for generative modeling, we use a VAE network with a Gaussian prior (<xref rid="btad546-B12" ref-type="bibr">Kingma <italic toggle="yes">et al.</italic> 2022</xref>) to learn the latent space of single-cell gene expression data. In addition, we use ZINB as the generative distribution in the decoder to model the scRNA-seq count data. For gene <italic toggle="yes">i</italic> in cell <italic toggle="yes">j</italic> (<inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>), we assume that its count follows a ZINB distribution with the following parameterization:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mtext>ZINB</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>π</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>π</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>δ</mml:mo></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>π</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mtext>NB</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mtext>NB</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes a negative binomial (NB) distribution; <italic toggle="yes">μ<sub>ij</sub></italic> and <italic toggle="yes">θ<sub>ij</sub></italic> are the mean and dispersion parameters of the NB distribution, respectively; <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the Dirac delta function which takes the values of 1 when <italic toggle="yes">x </italic>=<italic toggle="yes"> </italic>0 and 0 when <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>; <italic toggle="yes">π<sub>ij</sub></italic> denotes the inflated zero proportion.</p>
      <p>We now introduce the architecture of the VAE network in detail (<xref rid="btad546-F1" ref-type="fig">Fig. 1A</xref>). Given the normalized single-cell gene expression matrix <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, the encoder first obtains the mean and variance parameters of the Gaussian distributions in the hidden space. Then, the network uses the resampling technique to obtain the hidden Gaussian variables <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which serves as the input of the decoder. Lastly, the output of the decoder are the parameters of the ZINB distributions. We denote the mean parameters as <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mo>≜</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, the dispersion parameters as <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:mo>Θ</mml:mo><mml:mo>≜</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and the inflated zero proportions as <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:mo>Π</mml:mo><mml:mo>≜</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>π</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. The VAE model is therefore summarized as follows:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>H</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mi>H</mml:mi><mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mi>H</mml:mi><mml:mo>;</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>⊙</mml:mo><mml:mo>ϵ</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mo>⊙</mml:mo><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>μ</mml:mo></mml:msub></mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>Θ</mml:mo><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo>Π</mml:mo><mml:mo>=</mml:mo><mml:mtext>sigmoid</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>π</mml:mo></mml:msub></mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo> </mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where:</p>
      <list list-type="bullet">
        <list-item>
          <p><inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the input normalized gene expression matrix;</p>
        </list-item>
        <list-item>
          <p><inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mi>H</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo>&lt;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the output of <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>H</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>H</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is a layer of fully connected neural network with the ReLU activation function;</p>
        </list-item>
        <list-item>
          <p><inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are the mean and standard deviation parameters of the hidden Gaussian variables; <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mo>μ</mml:mo><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="normal">ℝ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo> </mml:mo><mml:mtext>and</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:msub><mml:mo>σ</mml:mo><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="normal">ℝ</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are the corresponding weights;</p>
        </list-item>
        <list-item>
          <p><inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is drawn from the learned Gaussian distributions and serves as the input of the decoder <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>; each element in <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:mo>ϵ</mml:mo><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is an independent standard Gaussian variable, and <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mo>⊙</mml:mo></mml:math></inline-formula> denotes the element-wise multiplication;</p>
        </list-item>
        <list-item>
          <p><inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is a layer of fully connected neural network with the ReLU activation function;</p>
        </list-item>
        <list-item>
          <p><inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo>&lt;</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the output of the decoder <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>; <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>μ</mml:mo></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mtext>and</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>π</mml:mo></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are the weight parameters.</p>
        </list-item>
      </list>
      <p>In this model, we use the exponential activation function to generate the mean parameters (<italic toggle="yes">M</italic>) and dispersion parameters (Θ) because these parameters should be nonnegative. We also use the sigmoid activation function to calculate Π so that the values are between 0 and 1. Given this model, the loss function of the VAE network is derived as
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>VAE</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mtext>ZINB</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>π</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mfrac><mml:mo>β</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>The first term in <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>VAE</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> represents the negative log-likelihood function, and the second term is derived from the Kullback–Leibler divergence <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:mtext>KL</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref rid="sup1" ref-type="supplementary-material">Supplementary Methods</xref>). <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:mo>β</mml:mo><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> is an adjustable parameter that reflects the strength of the disentanglement constraint.</p>
    </sec>
    <sec>
      <title>2.3 Adaptive cluster merging</title>
      <p>Given results of the pretraining and cluster initialization steps, scAce first performs a network update, and then iteratively performs cluster merging and network update to obtain the final clustering results (<xref rid="btad546-F1" ref-type="fig">Fig. 1C</xref>). The network update step and the cluster merging step are performed alternately, and the iteration stops if no clusters can be merged after a network update step. The final cluster labels and cell embeddings are the output of scAce.</p>
      <p>At each step of network update, given the current cluster labels, scAce updates the VAE network parameters and cluster centroids using the deep embedded clustering technique. A loss function is constructed to represent the quality of the clustering results based on the current cell embeddings and cluster labels. This function is then combined with the loss function of the VAE network to simultaneously update the network parameters and cluster centroids. After the update, scAce switches to the cluster merging step.</p>
      <p>At each step of cluster merging, given the current cell embeddings (from updated VAE network) and cluster centroids, scAce first assigns cells to their closest centroids to update the cluster labels. Then, using a data-adaptive criterion, scAce merges pairs of clusters with highly similar gene expression profiles into the same cluster. When two clusters are merged, the new centroid of the merged cluster is updated as the center of that cluster based on the current data embedding. The merging process is repeated until no more clusters can be merged given the data-adaptive criterion. Then, scAce switches to the network update step.</p>
      <p><bold><italic toggle="yes">Network update</italic></bold>. Suppose at the beginning of a network update step there are <italic toggle="yes">K</italic> clusters and corresponding centroids. We calculate the probability that cell <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> belongs to cluster <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:mi>j</mml:mi><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> as the conditional probability (<inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) of cell <italic toggle="yes">i</italic> given the centroid of cluster <italic toggle="yes">j</italic> (denoted as <italic toggle="yes">c<sub>j</sub></italic>). The conditional probability is obtained using the student’s <italic toggle="yes">t</italic>-distribution as a kernel to measure the similarity between the embedded cells and cluster centroids (<xref rid="btad546-B39" ref-type="bibr">Xie <italic toggle="yes">et al.</italic> 2016</xref>):
<disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo> </mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>To improve cluster quality by putting more emphasis on cells assigned with a high confidence, we define an auxiliary target distribution based on the distribution represented by the conditional probabilities:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The model is then trained to increase the similarity between the current distribution and the target distribution. The goal is to minimize the sum of the KL divergence over all the cells:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mi>C</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>Then, in order to update the network parameters, the overall loss function is defined as a weighted sum of <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>VAE</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">L<sub>C</sub></italic>: <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext>VAE</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mo>λ</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mi>C</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mrow><mml:mo>λ</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> is a parameter controlling the relative weights of network loss and clustering loss. By minimizing the loss function <italic toggle="yes">L</italic>, the network parameters are optimized, and the embeddings of the single cells and cluster centroids are updated, thus updating the cluster labels. After the cluster labels are updated, we recalculate the conditional probabilities in <xref rid="E4" ref-type="disp-formula">Equation (4)</xref> and repeat the above process to update cluster labels until the proportion of cells that change their cluster labels between two consecutive repeats is smaller than 5%. The purpose of this repeating process is to ensure that the network update has reached a stable state before cluster merging.</p>
      <p><bold><italic toggle="yes">Cluster merging</italic></bold>. Given the updated data from the previous network update step, we use a data-adaptive criterion to decide if two smaller clusters should be merged into a larger cluster (<xref rid="btad546-B16" ref-type="bibr">Lei <italic toggle="yes">et al.</italic> 2016</xref>). To define this criterion, we first introduce the intra-cluster distance. Suppose there are <italic toggle="yes">K</italic> clusters in a given iteration. For the <italic toggle="yes">i</italic>th cluster (<inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula>), its intra-cluster distance is defined as
<disp-formula id="E7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mtext>intra</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>q</mml:mi></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">N<sub>i</sub></italic> denotes the indices of cells in cluster <italic toggle="yes">i</italic>; <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> is the number of cells in cluster <italic toggle="yes">i</italic>; <italic toggle="yes">c<sub>i</sub></italic> denotes the centroid of cluster <italic toggle="yes">i</italic>; <italic toggle="yes">z<sub>q</sub></italic> is the current embedding of cell <italic toggle="yes">q</italic>. In addition, we define the inter-cluster distance between clusters <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> as
<disp-formula id="E8"><label>(8)</label><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>inter</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">c<sub>i</sub></italic> and <italic toggle="yes">c<sub>j</sub></italic> denote the centroids of clusters <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic>, respectively.</p>
      <p>The rationale underlying the adaptive criterion is that two small clusters should be merged into a larger cluster when their inter-cluster distance is small compared with the average inter-cluster distance defined as
<disp-formula id="E9"><label>(9)</label><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>inter</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">w<sub>ij</sub></italic> is the assigned weight on the inter-cluster distance between the <italic toggle="yes">i</italic>th and the <italic toggle="yes">j</italic>th clusters. This weight is used to account for the effect of cluster size on inter-cluster distances, and is defined as
<disp-formula id="E10"><label>(10)</label><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mtext>intra</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mtext>intra</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>j</mml:mi><mml:mrow><mml:mtext>intra</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mtext>intra</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mtext>intra</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>j</mml:mi><mml:mrow><mml:mtext>intra</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>For example, if the size of cluster <italic toggle="yes">i</italic> is large, then it will naturally have relatively large inter-cluster distances with other clusters. Consequently, it will have relatively smaller weights to offset this effect. In summary, we find all cluster pairs whose weighted intra-cluster distance (<inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>inter</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is smaller than <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> and merge the cluster pair with the smallest weighted distance.</p>
      <p>Following this principle, after each merge, <xref rid="E7" ref-type="disp-formula">Equations (7–</xref><xref rid="E10" ref-type="disp-formula">10)</xref> are updated based on the new cluster memberships, and the merging process is repeated until no more clusters meet the merging criterion. In other words, we stop the cluster merging step when <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>inter</mml:mtext></mml:mrow></mml:msubsup></mml:mrow><mml:mo>&gt;</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> for any pairs of <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> (<inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>).</p>
    </sec>
    <sec>
      <title>2.4 Implementation</title>
      <p>The scAce package is implemented in python 3.8, using Scanpy version 1.9.1 (<xref rid="btad546-B37" ref-type="bibr">Wolf <italic toggle="yes">et al.</italic> 2018</xref>) for preprocessing and Pytorch version 1.10.0 for implementing the VAE network. In the ZINB-based VAE network, <italic toggle="yes">β</italic> is set to <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.001</mml:mn><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> during pretraining and <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.01</mml:mn><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> during adaptive cluster merging. The values of <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> were set to 512, 32, and 512 in our analysis, and these are provided as tuning parameters in the scAce software. The parameter <italic toggle="yes">λ</italic> is set to 1 by default. A comparison for different values of <italic toggle="yes">λ</italic> and <italic toggle="yes">β</italic> at the adaptive cluster merging stage is presented in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S1 and S2</xref>. The required input of the scAce software is a scRNA-seq read count matrix. If clustering enhancement of an existing clustering result is desired, then the optional input of existing cluster labels should also be provided. The output of scAce includes the low-dimensional embeddings and final cluster assignments of the single cells.</p>
    </sec>
    <sec>
      <title>2.5 Clustering methods used for comparison</title>
      <p>For comparison in performance evaluation, we considered nine alternative clustering methods developed for scRNA-seq data based on both citation number and publication time. We restricted our considerations to methods that have software functions to directly perform clustering. From the traditional clustering methods (not using deep-learning methods), we selected Seurat (<xref rid="btad546-B27" ref-type="bibr">Satija <italic toggle="yes">et al.</italic> 2015</xref>) and CIDR (<xref rid="btad546-B20" ref-type="bibr">Lin <italic toggle="yes">et al.</italic> 2017</xref>), both of which are highly cited. From the clustering methods based on classical autoencoders, we selected the most widely used scDeepCluster (<xref rid="btad546-B30" ref-type="bibr">Tian <italic toggle="yes">et al.</italic> 2019</xref>) and DESC (<xref rid="btad546-B19" ref-type="bibr">Li <italic toggle="yes">et al.</italic> 2020</xref>). From clustering methods based on VAEs, we chose the most widely used scVI method (<xref rid="btad546-B22" ref-type="bibr">Lopez <italic toggle="yes">et al.</italic> 2018</xref>) and the recently published scGMAAE method (<xref rid="btad546-B35" ref-type="bibr">Wang <italic toggle="yes">et al.</italic> 2023</xref>). In addition, we selected two clustering methods that also use cluster merging approaches, SCCAF (<xref rid="btad546-B23" ref-type="bibr">Miao <italic toggle="yes">et al.</italic> 2020</xref>) and ADClust (<xref rid="btad546-B42" ref-type="bibr">Zeng <italic toggle="yes">et al.</italic> 2022</xref>), and one clustering method based on the graph neural network, graph-sc (<xref rid="btad546-B6" ref-type="bibr">Ciortan and Defrance 2022</xref>). For methods requiring a cluster number as input, the number of real cell types was provided to the algorithms. The main characteristics of scAce and the other nine methods are summarized in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>.</p>
    </sec>
    <sec>
      <title>2.6 Datasets</title>
      <p>For simulated data, we used the R package scDesign2 (<xref rid="btad546-B29" ref-type="bibr">Sun <italic toggle="yes">et al.</italic> 2021</xref>) to generate a synthetic single-cell gene expression matrix with ground truth cell type labels. In this simulated dataset, there were 16 653 genes and 1150 cells belonging to five cell types. The number of cells in each cell type was 600, 200, 200, 100, and 50, respectively. The last cell type accounted for &lt;5% of the cells and was used to represent a rare cell type. The real scRNA-seq dataset used by scDesign2 to learn gene expression parameters was a peripheral blood mononuclear cell (PBMC) dataset generated by the 10x Genomics technology (<xref rid="btad546-B43" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic> 2017</xref>).</p>
      <p>For real data, we downloaded seven real scRNA-seq datasets with annotated cell type labels (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S4</xref>). The seven datasets included three human datasets, three mouse datasets, and one turtle dataset. For simplicity, the datasets are referred to as Human pancreas (<xref rid="btad546-B2" ref-type="bibr">Baron <italic toggle="yes">et al.</italic> 2016</xref>) (3605 cells), Human PBMC (<xref rid="btad546-B43" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic> 2017</xref>) (4271 cells), Human kidney (<xref rid="btad546-B40" ref-type="bibr">Young <italic toggle="yes">et al.</italic> 2018</xref>) (5685 cells), Mouse ES (<xref rid="btad546-B15" ref-type="bibr">Klein <italic toggle="yes">et al.</italic> 2015</xref>) (embryonic stem, 2717 cells), Mouse hypothalamus (<xref rid="btad546-B4" ref-type="bibr">Chen <italic toggle="yes">et al.</italic> 2017</xref>) (12 089 cells), Mouse kidney (<xref rid="btad546-B1" ref-type="bibr">Adam <italic toggle="yes">et al.</italic> 2017</xref>) (3660 cells), and Turtle brain (<xref rid="btad546-B32" ref-type="bibr">Tosches <italic toggle="yes">et al.</italic> 2018</xref>) (18 664 cells).</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 scAce improves clustering accuracy and robustness on both simulated and real data</title>
      <p>To evaluate the performance of scAce in clustering scRNA-seq data, we first applied it and the other nine clustering methods on the simulated data. We generated data of five cell types by the scDesign2 package (<xref rid="btad546-B29" ref-type="bibr">Sun <italic toggle="yes">et al.</italic> 2021</xref>), which can learn gene expression parameters and gene–gene correlations from real data (Section 2). For methods that require a cluster number as input (scDeepCluster, scGMAAE, and graph-sc), we input the real cell type number; for other methods including scAce, the cluster numbers were automatically determined by the methods.</p>
      <p>When comparing the clustering results (<xref rid="btad546-F2" ref-type="fig">Fig. 2A</xref>) with the ground truth cell type labels (<xref rid="btad546-F2" ref-type="fig">Fig. 2B</xref>), we found that scAce was the only method that achieved an ARI of 1 with a clear separation of the five cell types in the low-dimensional space. In contrast, scVI, SCCAF, and Seurat would divide some cell types into smaller clusters, while ADClust and CIDR were unable to distinguish the rare cell type from other major cell types. DESC and graph-sc also achieved a high ARI, but both methods misclassified a small proportion of cells from the rare cell type. It is also worth noting that among the seven methods which do not require a cluster number as input, scAce and DESC were the only two methods that identified the correct number of clusters.</p>
      <fig position="float" id="btad546-F2">
        <label>Figure 2.</label>
        <caption>
          <p>Comparison of the 10 clustering methods on the simulated dataset. (A) UMAP plots of the cell embeddings produced by the 10 methods. Each point represents a cell and each color represents an inferred cluster. The number of inferred clusters (<italic toggle="yes">K</italic>) and the ARI values of the clustering results are marked on top of the corresponding plots. (B) Same UMAP plots as shown in (A) but colored by the five true cell types. (C) Boxplots of ARI values obtained by applying the ten clustering methods to randomly selected subsamples of the complete simulated data. (D) Boxplots of NMI values obtained by applying the 10 clustering methods to randomly selected subsamples of the complete simulated data.</p>
        </caption>
        <graphic xlink:href="btad546f2" position="float"/>
      </fig>
      <p>In order to further evaluate the robustness of different methods, we repeatedly applied the methods to a subset of the simulated cells. Each time, we randomly chose 95% of the cells in the dataset to create a new dataset, and performed the clustering analysis on the new data. The experiments were repeated 10 times. scAce achieved the highest median ARI and NMI values among the ten methods (<xref rid="btad546-F2" ref-type="fig">Fig. 2C and D</xref>), demonstrating its high robustness compared with alternative methods.</p>
      <p>After confirming scAce’s accuracy on the simulated data, we then applied scAce and the other nine methods on seven real scRNA-seq datasets and evaluated their clustering accuracy (Section 2). Based on the mean ARI scores across the seven datasets, scAce achieved the best clustering accuracy, followed by DESC, graph-sc, scDeepCluster, and ADClust (<xref rid="btad546-F3" ref-type="fig">Fig. 3A</xref>). The different cell types were clearly separated in the low-dimensional embeddings obtained by scAce, whereas the other methods had difficulty deriving embeddings that correctly reflected cellular identities for at least one dataset (<xref rid="btad546-F3" ref-type="fig">Fig. 3B and C</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S1 and S2</xref>). Compared with other clustering methods based on standard autoencoders (ADClust, DESC, and scDeepCluster), the embeddings obtained by scAce tended to be smoother, as VAE allows the latent variables to be continuous and smooth. When compared with scVI and scGMAAE, which also use VAEs to obtain cell embeddings, scAce’s clustering accuracy was higher than both on all seven datasets.</p>
      <fig position="float" id="btad546-F3">
        <label>Figure 3.</label>
        <caption>
          <p>Comparison of clustering methods on real datasets. (A) ARI and NMI values obtained from the 10 methods on the seven real datasets. The methods are ordered based on the mean ARI/NMI values. (B) UMAP plots of the cell embeddings produced by the four methods with the highest average ARI values across datasets. Each point represents a cell and each color represents an inferred cluster. The inferred cluster numbers are marked on top of the corresponding plots. (C) Same UMAP plots as shown in (B) but colored by the true cell types. The ARI values of the clustering results are marked on top of the corresponding plots.</p>
        </caption>
        <graphic xlink:href="btad546f3" position="float"/>
      </fig>
      <p>To further evaluate and compare the robustness of the clustering methods, we then applied them to subsets of 95% cells randomly selected from each real dataset, and repeated 10 times for each dataset. Based on both ARI and NMI scores, scAce again achieved the highest clustering accuracy across the repeated experiments (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S3 and S4</xref>). For example, the average ARI of scAce across all experiments was 0.76, followed by graph-sc (average ARI = 0.66), and DESC (average ARI = 0.65). As for NMI, the average of scAce is 0.80, followed by DESC (average NMI = 0.78), and graph-sc (average NMI = 0.76).</p>
    </sec>
    <sec>
      <title>3.2 scAce improves clustering accuracy by adaptive cluster merging</title>
      <p>In order to investigate the necessity of the adaptive cluster merging step in scAce, we compared the clustering results of scAce (as described in Section 2) with the initial clustering results when setting the resolution parameter in the Leiden algorithm such that the initial cluster number was the same as the true cell type number. The ARI values under the two settings show that the results of scAce were more accurate than the initial clustering without performing adaptive cluster merging on six datasets (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S5A</xref>). The clustering accuracy of the two settings was similar on the Mouse ES dataset as the initial result already achieved an ARI close to 1 (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S5B</xref>). For the other more challenging datasets, scAce was able to achieve higher clustering accuracy even without prior knowledge about the true cell type number.</p>
      <p>Of the nine alternative methods that were compared, SCCAF and ADClust also discover and merge clusters that might represent the same cellular identity in an iterative manner. In order to better compare the effectiveness and accuracy of the three methods, we studied the intermediate results of these methods in the initial and subsequent iterations. Their clustering results in the first iteration show that all three methods initially obtained a relatively large number of clusters (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S6A and B</xref>). Even though multiple clusters might correspond to the same cell type, most of these clusters had high purity, allowing for improvement by cluster merging through additional iterations.</p>
      <p>We then compared how the correspondence between true cell types and identified clusters changed in the cluster merging process, using the Mouse kidney dataset as an example (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S6 and S7</xref>). While the initial clustering of the three methods all showed high purity, the final clustering accuracy varied considerably. ADClust initially obtained 42 clusters, which were updated to 19 and 11 clusters in the second and third iteration, respectively. Even though each of these clusters only corresponded to a single cell type, starting from the fourth iteration, ADClust merged several groups of cells from different cell types into the same cluster. Its final result led to five clusters, all of which were mixtures of cells from multiple cell types. As for the SCCAF method, it stopped its merging process right after the initial iteration, leading to 17 clusters. It divided each of the eight cell types into multiple clusters, which made downstream annotations and comparisons error-prone. In contrast, scAce initially obtained 25 clusters, and gradually updated them to 17, 10, 9, and 8 clusters in the second to fifth iteration, respectively. Throughout this process, scAce was able to maintain the high purity of the identified clusters, and thus ultimately obtained clustering results that were in close agreement with the ground truth cell types (ARI = 0.93). Similar results were observed on the other six datasets (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S8</xref>).</p>
      <p>To further investigate the observed advantages of scAce, we evaluated the performance of other clustering methods when they were combined with the same cluster initialization and cluster merging approaches as proposed in scAce. Among the nine alternative methods, we studied the four methods which can take initialized clusters as input (scDeepCluster, DESC, SCCAF, and ADClust), and performed cluster initialization as in scAce. After obtaining the clustering results from these methods, we then performed an additional cluster merging step as in scAce. Compared with scAce (average ARI = 0.82), the second and third best method in this comparison was scDeepCluster (average ARI = 0.72) and ADClust (average ARI = 0.61), respectively (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S9A</xref>). Our results show that simply combining the proposed cluster initialization and cluster merging approaches with other existing methods does not optimize the clustering results (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S9 and S10</xref>). In contrast, the adaptive cluster merging approach used by scAce leads to improved results by combining cluster merging with network update in an iterative manner.</p>
    </sec>
    <sec>
      <title>3.3 scAce is robust to cluster initialization</title>
      <p>In the cluster initialization step, scAce uses the Leiden algorithm to obtain the initial clustering results. To evaluate the robustness of scAce to the selection of the initial clustering algorithm, we also applied scAce by using the Louvain or K-means algorithm to perform cluster initialization. The resolution parameter in Louvain was set to 2. For the cluster number in K-means, we tried different numbers between 15 and 30, and selected the number that maximized the corresponding silhouette coefficient.</p>
      <p>We calculated the final ARI and NMI values of scAce given the three different algorithms for cluster initialization. The difference in ARI values was between 0.001 and 0.069 across the seven real datasets, and the difference in NMI values was between 0.001 and 0.059 (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S11</xref>). Our results show that scAce is robust to the selection of the initial clustering algorithm. In fact, regardless of which initial clustering algorithm is used, as long as the initial clusters are of high purity, scAce is expected to merge highly similar small clusters into larger ones in the adaptive merging process. In addition to its robustness to initial clustering algorithms, we also observed that scAce was robust to the resolution parameter in the Leiden algorithm when its value was between 1.4 and 2.0 (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref>).</p>
    </sec>
    <sec>
      <title>3.4 scAce enhances the performance of existing clustering methods</title>
      <p>We provide a clustering enhancement option in the scAce method, which allows it to start from cluster labels inferred by another scRNA-seq clustering method and use its cluster splitting, VAE network update, and cluster merging steps to adaptively improve the clustering results (see Section 2 and <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods</xref>). To evaluate the effectiveness of this option, we implemented clustering enhancement by applying scAce with initial cluster labels obtained by Seurat or CIDR, both of which do not utilize neural networks or cluster merging approaches. Our results show that, given initial cluster labels from either Seurat or CIDR, scAce obviously improved the final clustering accuracy on most real datasets, in terms of both ARI and NMI scores (<xref rid="btad546-F4" ref-type="fig">Fig. 4</xref>).</p>
      <fig position="float" id="btad546-F4">
        <label>Figure 4.</label>
        <caption>
          <p>ARI and NMI values of clustering results before and after scAce’s clustering enhancement for Seurat and CIDR.</p>
        </caption>
        <graphic xlink:href="btad546f4" position="float"/>
      </fig>
      <p>We visualized the cell embeddings and cluster labels obtained by scAce right after pretraining and cluster initialization, and the final embeddings and labels after clustering enhancement, given initial results from Seurat (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S12</xref>) and CIDR (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S13</xref>), respectively. Compared with the original clustering results of Seurat and CIDR, scAce obtained smaller and purer clusters after its initialization (<xref rid="sup1" ref-type="supplementary-material">Supplementary Methods</xref>). <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S12B and S13B</xref> show the cell embeddings and cluster labels given scAce’s final output, which demonstrates scAce’s ability to enhance cluster assignment and learning of the latent space through adaptive cluster merging.</p>
    </sec>
    <sec>
      <title>3.5 Computational time and memory usage</title>
      <p>We measured the running time and maximum memory usage of scAce and the other nine methods on the seven real datasets (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S14</xref>). In the experiments, CIDR, SCCAF, and Seurat only needed to use CPUs, and the other seven methods needed GPUs. All experiments used a single core. The average running time of scAce ranked fourth, and Seurat and ADClust were the fastest. Although scAce was slightly slower than Seurat and ADClust, it was on average faster than the other deep-learning-based methods (scGMAAE, scVI, scDeepCluster, graph-sc, and DESC) (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S14A</xref>). As for the memory usage, the maximum memory usage of scAce was higher than most alternative methods, but was on the same magnitude as that of other deep-learning-based methods (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S14B</xref>).</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>In this article, we propose the scAce method for unsupervised clustering analysis of single cells using scRNA-seq data. Using a variational autoencoder network that adaptively learns both low-dimensional cell representations and cluster assignment, scAce allows for accurate clustering of cells without the need to predetermine the cluster number or other parameters indicating preferences on cluster resolution. We evaluated the performance of scAce in comparison with nine alternative clustering methods for scRNA-seq data based on both simulated and real datasets. Our results suggest that scAce outperforms existing state-of-the-art methods in terms of both clustering accuracy and robustness. Based on the clustering enhancement option of scAce, it is also possible to further improve the accuracy of an existing clustering assignment generated by other methods.</p>
    <p>Compared with existing deep embedding clustering algorithms that use traditional autoencoders to obtain low-dimensional representations, the VAE network used by scAce obtains better low-dimensional embeddings of single cells and therefore improves clustering results by enforcing a continuous and smooth latent space representation of the gene expression data. Another feature of scAce that contributes to its improved clustering performance is its ability to adaptively update the cluster assignments in a deep embedding framework. scAce starts with relatively large number of clusters which have a high purity and iteratively merges similar clusters together using the proposed adaptive cluster merging approach. This approach takes advantage of the trained VAE network and simultaneously achieves network update and cluster update to improve both cell embeddings and cluster assignments.</p>
    <p>When summarizing our computational results, we noticed that even though scAce successfully identified the rare cell type (with a cell proportion of 4%) in the simulated data, it did not always identify the rare cell types in the real datasets. Actually, none of the 10 methods evaluated in this work was able to consistently identify rare cell types that only accounted for between 0.027% and 1.498% of cells. In a recent benchmark study of 14 clustering methods, it was also reported that most methods significantly underestimated the true cell type numbers when the proportion of cells in rare cell types was around 2% (<xref rid="btad546-B41" ref-type="bibr">Yu <italic toggle="yes">et al.</italic> 2022</xref>). Since this is a systematic challenge, a future direction is to investigate how to further improve the identification of rare cell types. One possible solution is to first evaluate the heterogeneity of inferred cell clusters (<xref rid="btad546-B18" ref-type="bibr">Li 2022</xref>, <xref rid="btad546-B21" ref-type="bibr">Liu <italic toggle="yes">et al.</italic> 2020</xref>), and then perform another round of clustering analysis just using the cell clusters appearing to be mixtures of multiple cell types. When the focus is to discover rare cell types, an alternative solution is to modify the adaptive cluster merging process in scAce and use a more stringent merging criterion. Another future direction to consider is how to extend scAce to account for the cell type hierarchy. There have been a few methods which aim to use a tree structure to recover hierarchical relationships among cell types or account for this factor when evaluating clustering results (<xref rid="btad546-B38" ref-type="bibr">Wu and Wu 2020</xref>, <xref rid="btad546-B24" ref-type="bibr">Peng <italic toggle="yes">et al.</italic> 2021</xref>). As scAce is based on an adaptive cluster merging approach, it would be possible to learn cluster hierarchy from the merging orders.</p>
    <p>Although scAce has been developed as a clustering method for scRNA-seq data, we believe that its VAE framework and adaptive cluster merging approach can be extended to model additional types of data collected from other technologies, such as scATAC-seq and spatial transcriptomics (<xref rid="btad546-B17" ref-type="bibr">Lei <italic toggle="yes">et al.</italic> 2021</xref>). The framework can also be modified for noncount data by changing the output format of the decoder and the corresponding loss functions. Given the pivotal role of clustering analysis in single-cell studies and many other scientific problems, we anticipate scAce to be a useful method for discovering meaningful clusters in high-dimensional data.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btad546_Supplementary_Data</label>
      <media xlink:href="btad546_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We thank members of the Wei Vivian Li and Hongwei Li labs for the helpful discussions and suggestions. We also thank the anonymous reviewers for the insightful comments.</p>
  </ack>
  <sec>
    <title>Supplementary data</title>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Conflict of interest</title>
    <p>None declared.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the National Natural Science Foundation of China [42274172 to H.L.] and National Institutes of Health (National Institute of General Medical Sciences) [R35GM142702 to W.V.L.].</p>
  </sec>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>scAce is implemented as a Python package, which is freely available from its GitHub repository <ext-link xlink:href="https://github.com/sldyns/scAce" ext-link-type="uri">https://github.com/sldyns/scAce</ext-link>. The accession numbers of all real data used in this work are described in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>. The data and code used to reproduce the analyses are available at <ext-link xlink:href="https://github.com/sldyns/scAce/tree/main/reproducibility" ext-link-type="uri">https://github.com/sldyns/scAce/tree/main/reproducibility</ext-link>.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btad546-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adam</surname><given-names>M</given-names></string-name>, <string-name><surname>Potter</surname><given-names>AS</given-names></string-name>, <string-name><surname>Potter</surname><given-names>SS</given-names></string-name></person-group><etal>et al</etal><article-title>Psychrophilic proteases dramatically reduce single-cell RNA-seq artifacts: a molecular atlas of kidney development</article-title>. <source>Development</source><year>2017</year>;<volume>144</volume>:<fpage>3625</fpage>–<lpage>32</lpage>.<pub-id pub-id-type="pmid">28851704</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baron</surname><given-names>M</given-names></string-name>, <string-name><surname>Veres</surname><given-names>A</given-names></string-name>, <string-name><surname>Wolock</surname><given-names>SL</given-names></string-name></person-group><etal>et al</etal><article-title>A single-cell transcriptomic map of the human and mouse pancreas reveals inter-and intra-cell population structure</article-title>. <source>Cell Syst</source><year>2016</year>;<volume>3</volume>:<fpage>346</fpage>–<lpage>60.e4</lpage>.<pub-id pub-id-type="pmid">27667365</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blondel</surname><given-names>VD</given-names></string-name>, <string-name><surname>Guillaume</surname><given-names>J-L</given-names></string-name>, <string-name><surname>Lambiotte</surname><given-names>R</given-names></string-name></person-group><etal>et al</etal><article-title>Fast unfolding of communities in large networks</article-title>. <source>J Stat Mech</source><year>2008</year>;<volume>2008</volume>:<fpage>P10008</fpage>.</mixed-citation>
    </ref>
    <ref id="btad546-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>R</given-names></string-name>, <string-name><surname>Wu</surname><given-names>X</given-names></string-name>, <string-name><surname>Jiang</surname><given-names>L</given-names></string-name></person-group><etal>et al</etal><article-title>Single-cell RNA-seq reveals hypothalamic cell diversity</article-title>. <source>Cell Rep</source><year>2017</year>;<volume>18</volume>:<fpage>3227</fpage>–<lpage>41</lpage>.<pub-id pub-id-type="pmid">28355573</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Choi</surname><given-names>Y</given-names></string-name>, <string-name><surname>Li</surname><given-names>R</given-names></string-name>, <string-name><surname>Quon</surname><given-names>G</given-names></string-name></person-group><etal>et al</etal><article-title>sivae: interpretable deep generative models for single-cell transcriptomes</article-title>. <source>Genome Biol</source><year>2023</year>;<volume>24</volume>:<fpage>29</fpage>.<pub-id pub-id-type="pmid">36803416</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ciortan</surname><given-names>M</given-names></string-name>, <string-name><surname>Defrance</surname><given-names>M.</given-names></string-name></person-group><article-title>GNN-based embedding for clustering scRNA-seq data</article-title>. <source>Bioinformatics</source><year>2022</year>;<volume>38</volume>:<fpage>1037</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">34850828</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Deng</surname><given-names>Y</given-names></string-name>, <string-name><surname>Bao</surname><given-names>F</given-names></string-name>, <string-name><surname>Dai</surname><given-names>Q</given-names></string-name></person-group><etal>et al</etal><article-title>Scalable analysis of cell-type composition from single-cell transcriptomics using deep recurrent learning</article-title>. <source>Nat Methods</source><year>2019</year>;<volume>16</volume>:<fpage>311</fpage>–<lpage>4</lpage>.<pub-id pub-id-type="pmid">30886411</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eraslan</surname><given-names>G</given-names></string-name>, <string-name><surname>Simon</surname><given-names>LM</given-names></string-name>, <string-name><surname>Mircea</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Single-cell RNA-seq denoising using a deep count autoencoder</article-title>. <source>Nat Commun</source><year>2019</year>;<volume>10</volume>:<fpage>390</fpage>.<pub-id pub-id-type="pmid">30674886</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grønbech</surname><given-names>CH</given-names></string-name>, <string-name><surname>Vording</surname><given-names>MF</given-names></string-name>, <string-name><surname>Timshel</surname><given-names>PN</given-names></string-name></person-group><etal>et al</etal><article-title>scvae: variational auto-encoders for single-cell gene expression data</article-title>. <source>Bioinformatics</source><year>2020</year>;<volume>36</volume>:<fpage>4415</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">32415966</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Guo</surname><given-names>X</given-names></string-name>, <string-name><surname>Gao</surname><given-names>L</given-names></string-name>, <string-name><surname>Liu</surname><given-names>X</given-names></string-name></person-group><etal>et al</etal> Improved deep embedded clustering with local structure preservation. In: <italic toggle="yes">Proceedings of 26th International Joint Conference on Artificial Intelligence, IJCAI</italic>, Melbourne, Australia, <year>2017</year>, <fpage>1753</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="btad546-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Higgins</surname><given-names>I</given-names></string-name>, <string-name><surname>Matthey</surname><given-names>L</given-names></string-name>, <string-name><surname>Pal</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal> beta-vae: Learning basic visual concepts with a constrained variational framework. In: 5<italic toggle="yes"><italic toggle="yes">th</italic> International Conference on Learning Representations</italic>, <italic toggle="yes">Toulon, France</italic>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="btad546-B12">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kingma</surname><given-names>DP</given-names></string-name>, <string-name><surname>Welling</surname><given-names>M</given-names></string-name></person-group>. Auto-encoding variational Bayes. arXiv prerint, arXiv:1312.6114, <year>2022</year>, preprint: not peer reviewed.</mixed-citation>
    </ref>
    <ref id="btad546-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kiselev</surname><given-names>VY</given-names></string-name>, <string-name><surname>Andrews</surname><given-names>TS</given-names></string-name>, <string-name><surname>Hemberg</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Challenges in unsupervised clustering of single-cell RNA-seq data</article-title>. <source>Nat Rev Genet</source><year>2019</year>;<volume>20</volume>:<fpage>273</fpage>–<lpage>82</lpage>.<pub-id pub-id-type="pmid">30617341</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kiselev</surname><given-names>VY</given-names></string-name>, <string-name><surname>Kirschner</surname><given-names>K</given-names></string-name>, <string-name><surname>Schaub</surname><given-names>MT</given-names></string-name></person-group><etal>et al</etal><article-title>Sc3: consensus clustering of single-cell RNA-seq data</article-title>. <source>Nat Methods</source><year>2017</year>;<volume>14</volume>:<fpage>483</fpage>–<lpage>6</lpage>.<pub-id pub-id-type="pmid">28346451</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klein</surname><given-names>AM</given-names></string-name>, <string-name><surname>Mazutis</surname><given-names>L</given-names></string-name>, <string-name><surname>Akartuna</surname><given-names>I</given-names></string-name></person-group><etal>et al</etal><article-title>Droplet barcoding for single-cell transcriptomics applied to embryonic stem cells</article-title>. <source>Cell</source><year>2015</year>;<volume>161</volume>:<fpage>1187</fpage>–<lpage>201</lpage>.<pub-id pub-id-type="pmid">26000487</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lei</surname><given-names>J</given-names></string-name>, <string-name><surname>Jiang</surname><given-names>T</given-names></string-name>, <string-name><surname>Wu</surname><given-names>K</given-names></string-name></person-group><etal>et al</etal><article-title>Robust k-means algorithm with automatically splitting and merging clusters and its applications for surveillance data</article-title>. <source>Multimed Tools Appl</source><year>2016</year>;<volume>75</volume>:<fpage>12043</fpage>–<lpage>59</lpage>.</mixed-citation>
    </ref>
    <ref id="btad546-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lei</surname><given-names>Y</given-names></string-name>, <string-name><surname>Tang</surname><given-names>R</given-names></string-name>, <string-name><surname>Xu</surname><given-names>J</given-names></string-name></person-group><etal>et al</etal><article-title>Applications of single-cell sequencing in cancer research: progress and perspectives</article-title>. <source>J Hematol Oncol</source><year>2021</year>;<volume>14</volume>:<fpage>91</fpage>.<pub-id pub-id-type="pmid">34108022</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>WV.</given-names></string-name></person-group><article-title>Phitest for analyzing the homogeneity of single-cell populations</article-title>. <source>Bioinformatics</source><year>2022</year>;<volume>38</volume>:<fpage>2639</fpage>–<lpage>41</lpage>.<pub-id pub-id-type="pmid">35238346</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>X</given-names></string-name>, <string-name><surname>Wang</surname><given-names>K</given-names></string-name>, <string-name><surname>Lyu</surname><given-names>Y</given-names></string-name></person-group><etal>et al</etal><article-title>Deep learning enables accurate clustering with batch effect removal in single-cell RNA-seq analysis</article-title>. <source>Nat Commun</source><year>2020</year>;<volume>11</volume>:<fpage>2338</fpage>.<pub-id pub-id-type="pmid">32393754</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>P</given-names></string-name>, <string-name><surname>Troup</surname><given-names>M</given-names></string-name>, <string-name><surname>Ho</surname><given-names>JWK</given-names></string-name></person-group><etal>et al</etal><article-title>CIDR: ultrafast and accurate clustering through imputation for single-cell RNA-seq data</article-title>. <source>Genome Biol</source><year>2017</year>;<volume>18</volume>:<fpage>59</fpage>.<pub-id pub-id-type="pmid">28351406</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>B</given-names></string-name>, <string-name><surname>Li</surname><given-names>C</given-names></string-name>, <string-name><surname>Li</surname><given-names>Z</given-names></string-name></person-group><etal>et al</etal><article-title>An entropy-based metric for assessing the purity of single cell populations</article-title>. <source>Nat Commun</source><year>2020</year>;<volume>11</volume>:<fpage>3155</fpage>.<pub-id pub-id-type="pmid">32572028</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lopez</surname><given-names>R</given-names></string-name>, <string-name><surname>Regier</surname><given-names>J</given-names></string-name>, <string-name><surname>Cole</surname><given-names>MB</given-names></string-name></person-group><etal>et al</etal><article-title>Deep generative modeling for single-cell transcriptomics</article-title>. <source>Nat Methods</source><year>2018</year>;<volume>15</volume>:<fpage>1053</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">30504886</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miao</surname><given-names>Z</given-names></string-name>, <string-name><surname>Moreno</surname><given-names>P</given-names></string-name>, <string-name><surname>Huang</surname><given-names>N</given-names></string-name></person-group><etal>et al</etal><article-title>Putative cell type discovery from single-cell gene expression data</article-title>. <source>Nat Methods</source><year>2020</year>;<volume>17</volume>:<fpage>621</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">32424270</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname><given-names>M</given-names></string-name>, <string-name><surname>Wamsley</surname><given-names>B</given-names></string-name>, <string-name><surname>Elkins</surname><given-names>AG</given-names></string-name></person-group><etal>et al</etal><article-title>Cell type hierarchy reconstruction via reconciliation of multi-resolution cluster tree</article-title>. <source>Nucleic Acids Res</source><year>2021</year>;<volume>49</volume>:<fpage>e91</fpage>.<pub-id pub-id-type="pmid">34125905</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petegrosso</surname><given-names>R</given-names></string-name>, <string-name><surname>Li</surname><given-names>Z</given-names></string-name>, <string-name><surname>Kuang</surname><given-names>R.</given-names></string-name></person-group><article-title>Machine learning and statistical methods for clustering single-cell rna-sequencing data</article-title>. <source>Brief Bioinform</source><year>2020</year>;<volume>21</volume>:<fpage>1209</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">31243426</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Qi</surname><given-names>R</given-names></string-name>, <string-name><surname>Ma</surname><given-names>A</given-names></string-name>, <string-name><surname>Ma</surname><given-names>Q</given-names></string-name></person-group><etal>et al</etal><article-title>Clustering and classification methods for single-cell RNA-sequencing data</article-title>. <source>Brief Bioinform</source><year>2020</year>;<volume>21</volume>:<fpage>1196</fpage>–<lpage>208</lpage>.<pub-id pub-id-type="pmid">31271412</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Satija</surname><given-names>R</given-names></string-name>, <string-name><surname>Farrell</surname><given-names>JA</given-names></string-name>, <string-name><surname>Gennert</surname><given-names>D</given-names></string-name></person-group><etal>et al</etal><article-title>Spatial reconstruction of single-cell gene expression data</article-title>. <source>Nat Biotechnol</source><year>2015</year>;<volume>33</volume>:<fpage>495</fpage>–<lpage>502</lpage>.<pub-id pub-id-type="pmid">25867923</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sheng</surname><given-names>J</given-names></string-name>, <string-name><surname>Li</surname><given-names>WV.</given-names></string-name></person-group><article-title>Selecting gene features for unsupervised analysis of single-cell gene expression data</article-title>. <source>Brief Bioinform</source><year>2021</year>;<volume>22</volume>:<fpage>bbab295</fpage>.<pub-id pub-id-type="pmid">34351383</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname><given-names>T</given-names></string-name>, <string-name><surname>Song</surname><given-names>D</given-names></string-name>, <string-name><surname>Li</surname><given-names>WV</given-names></string-name></person-group><etal>et al</etal><article-title>scdesign2: a transparent simulator that generates high-fidelity single-cell gene expression count data with gene correlations captured</article-title>. <source>Genome Biol</source><year>2021</year>;<volume>22</volume>:<fpage>163</fpage>.<pub-id pub-id-type="pmid">34034771</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname><given-names>T</given-names></string-name>, <string-name><surname>Wan</surname><given-names>J</given-names></string-name>, <string-name><surname>Song</surname><given-names>Q</given-names></string-name></person-group><etal>et al</etal><article-title>Clustering single-cell RNA-seq data with a model-based deep learning approach</article-title>. <source>Nat Mach Intell</source><year>2019</year>;<volume>1</volume>:<fpage>191</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="btad546-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname><given-names>T</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>J</given-names></string-name>, <string-name><surname>Lin</surname><given-names>X</given-names></string-name></person-group><etal>et al</etal><article-title>Model-based deep embedding for constrained clustering analysis of single cell RNA-seq data</article-title>. <source>Nat Commun</source><year>2021</year>;<volume>12</volume>:<fpage>1873</fpage>.<pub-id pub-id-type="pmid">33767149</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tosches</surname><given-names>MA</given-names></string-name>, <string-name><surname>Yamawaki</surname><given-names>TM</given-names></string-name>, <string-name><surname>Naumann</surname><given-names>RK</given-names></string-name></person-group><etal>et al</etal><article-title>Evolution of pallium, hippocampus, and cortical cell types revealed by single-cell transcriptomics in reptiles</article-title>. <source>Science</source><year>2018</year>;<volume>360</volume>:<fpage>881</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">29724907</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Traag</surname><given-names>VA</given-names></string-name>, <string-name><surname>Waltman</surname><given-names>L</given-names></string-name>, <string-name><surname>van Eck</surname><given-names>NJ</given-names></string-name></person-group><etal>et al</etal><article-title>From louvain to leiden: guaranteeing well-connected communities</article-title>. <source>Sci Rep</source><year>2019</year>;<volume>9</volume>:<fpage>5233</fpage>.<pub-id pub-id-type="pmid">30914743</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>D</given-names></string-name>, <string-name><surname>Gu</surname><given-names>J.</given-names></string-name></person-group><article-title>Vasc: dimension reduction and visualization of single-cell RNA-seq data by deep variational autoencoder</article-title>. <source>Genomics Proteomics Bioinf</source><year>2018</year>;<volume>16</volume>:<fpage>320</fpage>–<lpage>31</lpage>.</mixed-citation>
    </ref>
    <ref id="btad546-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>H-Y</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>J-P</given-names></string-name>, <string-name><surname>Zheng</surname><given-names>C-H</given-names></string-name></person-group>, <etal>et al</etal><article-title>scgmaae: Gaussian mixture adversarial autoencoders for diversification analysis of scRNA-seq data</article-title>. <source>Brief Bioinform</source><year>2023</year>;<volume>24</volume>:<fpage>bbac585</fpage>.<pub-id pub-id-type="pmid">36592058</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ward</surname><given-names>JH</given-names><suffix>Jr</suffix></string-name></person-group>. <article-title>Hierarchical grouping to optimize an objective function</article-title>. <source>J Am Stat Assoc</source><year>1963</year>;<volume>58</volume>:<fpage>236</fpage>–<lpage>44</lpage>.</mixed-citation>
    </ref>
    <ref id="btad546-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolf</surname><given-names>FA</given-names></string-name>, <string-name><surname>Angerer</surname><given-names>P</given-names></string-name>, <string-name><surname>Theis</surname><given-names>FJ</given-names></string-name></person-group><etal>et al</etal><article-title>Scanpy: large-scale single-cell gene expression data analysis</article-title>. <source>Genome Biol</source><year>2018</year>;<volume>19</volume>:<fpage>15</fpage>.<pub-id pub-id-type="pmid">29409532</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>Z</given-names></string-name>, <string-name><surname>Wu</surname><given-names>H.</given-names></string-name></person-group><article-title>Accounting for cell type hierarchy in evaluating single cell RNA-seq clustering</article-title>. <source>Genome Biol</source><year>2020</year>;<volume>21</volume>:<fpage>123</fpage>.<pub-id pub-id-type="pmid">32450895</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B39">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Xie</surname><given-names>J</given-names></string-name>, <string-name><surname>Girshick</surname><given-names>R</given-names></string-name>, <string-name><surname>Farhadi</surname><given-names>A.</given-names></string-name></person-group> Unsupervised deep embedding for clustering analysis. In: <italic toggle="yes">International Conference on Machine Learning.</italic> New York, United States: PMLR <year>2016</year>, <fpage>478</fpage>–<lpage>87</lpage>.</mixed-citation>
    </ref>
    <ref id="btad546-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Young</surname><given-names>MD</given-names></string-name>, <string-name><surname>Mitchell</surname><given-names>TJ</given-names></string-name>, <string-name><surname>Vieira Braga</surname><given-names>FA</given-names></string-name></person-group><etal>et al</etal><article-title>Single-cell transcriptomes from human kidneys reveal the cellular identity of renal tumors</article-title>. <source>Science</source><year>2018</year>;<volume>361</volume>:<fpage>594</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">30093597</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>L</given-names></string-name>, <string-name><surname>Cao</surname><given-names>Y</given-names></string-name>, <string-name><surname>Yang</surname><given-names>JYH</given-names></string-name></person-group><etal>et al</etal><article-title>Benchmarking clustering algorithms on estimating the number of cell types from single-cell RNA-sequencing data</article-title>. <source>Genome Biol</source><year>2022</year>;<volume>23</volume>:<fpage>49</fpage>.<pub-id pub-id-type="pmid">35135612</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zeng</surname><given-names>Y</given-names></string-name></person-group><etal>et al</etal><article-title>A parameter-free deep embedded clustering method for single-cell RNA-seq data</article-title>. <source>Brief Bioinform</source><year>2022</year>;<volume>23</volume>:<fpage>bbac172</fpage>.<pub-id pub-id-type="pmid">35524494</pub-id></mixed-citation>
    </ref>
    <ref id="btad546-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zheng</surname><given-names>GXY</given-names></string-name>, <string-name><surname>Terry</surname><given-names>JM</given-names></string-name>, <string-name><surname>Belgrader</surname><given-names>P</given-names></string-name></person-group><etal>et al</etal><article-title>Massively parallel digital transcriptional profiling of single cells</article-title>. <source>Nat Commun</source><year>2017</year>;<volume>8</volume>:<fpage>14049</fpage>.<pub-id pub-id-type="pmid">28091601</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
