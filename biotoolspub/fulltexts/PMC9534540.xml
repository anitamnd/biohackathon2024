<?DTDIdentifier.IdentifierValue http://null/schema/dtds/document/fulltext/xcr/xocs-article.xsd?>
<?DTDIdentifier.IdentifierType schema?>
<?SourceDTD.DTDName xocs-article.xsd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName ftrr2jats.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Inf Fusion</journal-id>
    <journal-id journal-id-type="iso-abbrev">Inf Fusion</journal-id>
    <journal-title-group>
      <journal-title>An International Journal on Information Fusion</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1566-2535</issn>
    <issn pub-type="epub">1872-6305</issn>
    <publisher>
      <publisher-name>Elsevier B.V.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9534540</article-id>
    <article-id pub-id-type="pii">S1566-2535(22)00160-9</article-id>
    <article-id pub-id-type="doi">10.1016/j.inffus.2022.09.023</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Full Length Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>UncertaintyFuseNet: Robust uncertainty-aware hierarchical feature fusion model with Ensemble Monte Carlo Dropout for COVID-19 detection</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au000001">
        <name>
          <surname>Abdar</surname>
          <given-names>Moloud</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au000002">
        <name>
          <surname>Salari</surname>
          <given-names>Soorena</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">b</xref>
        <xref rid="fn1" ref-type="fn">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au000003">
        <name>
          <surname>Qahremani</surname>
          <given-names>Sina</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">c</xref>
        <xref rid="fn1" ref-type="fn">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au000004">
        <name>
          <surname>Lam</surname>
          <given-names>Hak-Keung</given-names>
        </name>
        <xref rid="aff4" ref-type="aff">d</xref>
      </contrib>
      <contrib contrib-type="author" id="au000005">
        <name>
          <surname>Karray</surname>
          <given-names>Fakhri</given-names>
        </name>
        <xref rid="aff5" ref-type="aff">e</xref>
        <xref rid="aff6" ref-type="aff">f</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au000006">
        <name>
          <surname>Hussain</surname>
          <given-names>Sadiq</given-names>
        </name>
        <xref rid="aff7" ref-type="aff">g</xref>
      </contrib>
      <contrib contrib-type="author" id="au000007">
        <name>
          <surname>Khosravi</surname>
          <given-names>Abbas</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au000008">
        <name>
          <surname>Acharya</surname>
          <given-names>U. Rajendra</given-names>
        </name>
        <xref rid="aff8" ref-type="aff">h</xref>
        <xref rid="aff9" ref-type="aff">i</xref>
        <xref rid="aff10" ref-type="aff">j</xref>
      </contrib>
      <contrib contrib-type="author" id="au000009">
        <name>
          <surname>Makarenkov</surname>
          <given-names>Vladimir</given-names>
        </name>
        <xref rid="aff11" ref-type="aff">k</xref>
      </contrib>
      <contrib contrib-type="author" id="au000010">
        <name>
          <surname>Nahavandi</surname>
          <given-names>Saeid</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <aff id="aff1"><label>a</label>Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Geelong, Australia</aff>
      <aff id="aff2"><label>b</label>Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada</aff>
      <aff id="aff3"><label>c</label>Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran</aff>
      <aff id="aff4"><label>d</label>Centre for Robotics Research, Department of Engineering, King’s College London, London, United Kingdom</aff>
      <aff id="aff5"><label>e</label>Centre for Pattern Analysis and Machine Intelligence, Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada</aff>
      <aff id="aff6"><label>f</label>Department of Machine Learning, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, United Arab Emirates</aff>
      <aff id="aff7"><label>g</label>System Administrator, Dibrugarh University, Dibrugarh, India</aff>
      <aff id="aff8"><label>h</label>Department of Electronics and Computer Engineering, Ngee Ann Polytechnic, Clementi, Singapore</aff>
      <aff id="aff9"><label>i</label>Department of Biomedical Engineering, School of Science and Technology, SUSS University, Singapore</aff>
      <aff id="aff10"><label>j</label>Department of Biomedical Informatics and Medical Engineering, Asia University, Taichung, Taiwan</aff>
      <aff id="aff11"><label>k</label>Department of Computer Science, University of Quebec in Montreal, Montreal, Canada</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding author at: Centre for Pattern Analysis and Machine Intelligence, Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada.</corresp>
      <fn id="fn1">
        <label>1</label>
        <p id="d1e3102">These authors contributed equally to this work.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>5</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="ppub">
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>5</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <volume>90</volume>
    <fpage>364</fpage>
    <lpage>381</lpage>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>23</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>25</day>
        <month>9</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Elsevier B.V. All rights reserved.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Elsevier B.V.</copyright-holder>
      <license>
        <license-p>Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</license-p>
      </license>
    </permissions>
    <abstract id="d1e3108">
      <p>The COVID-19 (Coronavirus disease 2019) pandemic has become a major global threat to human health and well-being. Thus, the development of computer-aided detection (CAD) systems that are capable of accurately distinguishing COVID-19 from other diseases using chest computed tomography (CT) and X-ray data is of immediate priority. Such automatic systems are usually based on traditional machine learning or deep learning methods. Differently from most of the existing studies, which used either CT scan or X-ray images in COVID-19-case classification, we present a new, simple but efficient deep learning feature fusion model, called <inline-formula><mml:math id="d1e3115" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, which is able to classify accurately large datasets of both of these types of images. We argue that the uncertainty of the model’s predictions should be taken into account in the learning process, even though most of the existing studies have overlooked it. We quantify the prediction uncertainty in our feature fusion model using effective Ensemble Monte Carlo Dropout (EMCD) technique. A comprehensive simulation study has been conducted to compare the results of our new model to the existing approaches, evaluating the performance of competing models in terms of Precision, Recall, F-Measure, Accuracy and ROC curves. The obtained results prove the efficiency of our model which provided the prediction accuracy of 99.08% and 96.35% for the considered CT scan and X-ray datasets, respectively. Moreover, our <inline-formula><mml:math id="d1e3156" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> model was generally robust to noise and performed well with previously unseen data. The source code of our implementation is freely available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/moloud1987/UncertaintyFuseNet-for-COVID-19-Classification" id="interref1">https://github.com/moloud1987/UncertaintyFuseNet-for-COVID-19-Classification</ext-link>.</p>
    </abstract>
    <kwd-group id="d1e3231">
      <title>Keywords</title>
      <kwd>COVID-19</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Early fusion</kwd>
      <kwd>Feature fusion</kwd>
      <kwd>Uncertainty quantification</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p id="d1e3262">The 2019 coronavirus (COVID-19) has been spreading astonishingly fast across the globe since its emergence in December 2019 and its exact origin is still unknown <xref rid="b1" ref-type="bibr">[1]</xref>, <xref rid="b2" ref-type="bibr">[2]</xref>, <xref rid="b3" ref-type="bibr">[3]</xref>. Overall, the COVID-19 pandemic has caused a consecutive series of catastrophic losses worldwide, infecting more than 287 million people and causing around 5.4 million deaths around the world up to the present. The rapid spread of COVID-19 is continuing to threaten human’s life and health with the emergence of novel variants such as Delta and Omicron. All of this makes COVID-19 not only an epidemiological disaster, but also a psychological and emotional one. The uncertainties and grappling with the loss of normalcy caused by this pandemic provoke severe anxiety, stress and sadness among people.</p>
    <p id="d1e3268">Easy respiratory transmission of the disease from person to person triggers swift spread of the pandemic. While many of the COVID-19 cases show milder symptoms, the symptoms of the remaining cases are unfortunately life-critical. The health-care systems in many countries seem to have arrived at the point of collapse as the number of cases has been increasing drastically due to the fast propagation of some of its variants. Regarding the COVID-19 diagnostic, the reverse transcription polymerase chain reaction (RT-PCR) is one of the gold standards for COVID-19 detection. However, RT-PCR has a low sensitivity. Hence, many COVID-19 cases will not be recognized by this test and thus the patients may not get the proper treatments. These unrecognized patients pose a threat to the healthy population due to highly infectious nature of the virus. Chest X-ray (CXR) and Computed Tomography (CT) have been widely used to identify prominent pneumonia patterns in the chest. These imaging technologies accompanied by artificial intelligence tools may be used to diagnose COVID-19 patients in a more accurate, fast and cost-effective manner. Failure to provide prompt detection and treatment of COVID-19 patients increases the mortality rate. Hence, the detection of COVID-19 cases using deep learning models using both CXR and CT images may have huge potential in healthcare applications.</p>
    <p id="d1e3270">In recent years, deep learning models have had the widespread applicability not only in medical imaging field but also in many other areas <xref rid="b4" ref-type="bibr">[4]</xref>, <xref rid="b5" ref-type="bibr">[5]</xref>, <xref rid="b6" ref-type="bibr">[6]</xref>, <xref rid="b7" ref-type="bibr">[7]</xref>. These models have also been extensively applied for COVID-19 detection. It is critical to discriminate COVID-19 from other forms of pneumonia and flu. Farooq et al. <xref rid="b8" ref-type="bibr">[8]</xref> introduced an open-access dataset and the open-source code of their implementation using a CNN framework for distinguishing COVID-19 from analogous pneumonia cohorts from chest X-ray images. The authors designed their COVIDResNet model by utilizing a pre-trained ResNet-50 framework allowing them to improve the model’s performance and reduce its training time. An automatic and accurate identification of COVID-19 using CT images helps radiologists to screen patients in a better way. Zheng et al. in <xref rid="b9" ref-type="bibr">[9]</xref> proposed a fully automated system for COVID-19 detection from chest CT images. Their deep learning model, called COVNet, investigates visual features of the chest CT images. Moreover, Hall et al. <xref rid="b10" ref-type="bibr">[10]</xref> presented a new deep learning model, named COVIDX-Net, to aid radiologists with COVID-19 detection from CXR image data. The authors explored seven deep learning architectures, including DenseNet, VGG-19 and MobileNet v2.0. In another study, Abbas et al. <xref rid="b11" ref-type="bibr">[11]</xref> designed the Decompose, Transfer, and Compose (DeTraC) model of COVID-19 image classification using CXR data. A class decomposition approach was employed to identify irregularities in iCXR data by scrutinizing the class boundaries.</p>
    <p id="d1e3293">Segmentation also plays a key role in COVID-19 quantification applied to CT scan data. Chen et al. <xref rid="b12" ref-type="bibr">[12]</xref> proposed a novel deep learning method for segmentation of COVID-19 infection regions automatically. Aggregated Residual Transformations were employed to learn a robust and expressive feature representation and the soft attention technique was applied to improve the potential of the system to distinguish several symptoms of COVID-19. However, we noticed that there are still some open issues in the recently proposed traditional machine learning and deep learning models for COVID-19 detection. For this reason, optimizing the existing models should be a priority in COVID-19 detection and classification. Ensemble and fusion-based models <xref rid="b13" ref-type="bibr">[13]</xref> have shown outstanding performance in different medical applications. In the following, we provide more information about fusion-based models, discussing how they can be used in the framework of the deep learning approach.</p>
    <sec id="sec1.1">
      <label>1.1</label>
      <title>Uncertainty quantification (UQ)</title>
      <p id="d1e3308">Many traditional machine learning and deep learning models have been developed not only for analysis of CXR and CT image data but also for many other medical applications, often yielding high accuracy results even for a limited number of images <xref rid="b7" ref-type="bibr">[7]</xref>. However, DNNs require a large number of data to fine-tune trainable parameters. A limited number of images usually leads to epistemic uncertainty. Trust is an issue for these models, deployed with lower numbers of training samples. Out-of-distribution (OoD) samples and discrimination between the training and testing samples make such models fail in real world applications. Lack of confidence in unknown or new cases is usually not reported for these models. However, this information is essential for the development of reliable medical diagnostic tools. These unknown samples, which are generally hard to predict, often have important practical value. It is essential to estimate uncertainties with an extra insight in their point estimates. This additional vision aims at enhancing the overall trustworthiness of the systems, allowing clinicians to know where they can trust predictions made by the models. The flawed decisions made by some models can be fatal for the patients at risk. Hence, proper uncertainty estimations are necessary to improve the efficiency of ML models making them trustworthy and reliable <xref rid="b14" ref-type="bibr">[14]</xref>, <xref rid="b15" ref-type="bibr">[15]</xref>, <xref rid="b16" ref-type="bibr">[16]</xref>. Trustworthy uncertainty estimates can facilitate clinical decision making, and more importantly, provide clinicians with appropriate feedback on the reliability of the obtained results <xref rid="b17" ref-type="bibr">[17]</xref>. As discussed above, COVID-19 has had many negative effects on all aspects of human life around the world. The COVID-19 pandemic has caused millions of deaths worldwide. In this regard, our study attempts to propose a simple and accurate deep learning model, called <italic>UncertaintyFuseNet</italic>, for detecting COVID-19 cases. Our model includes an uncertainty quantification method to increase the reliability of the obtained results.</p>
    </sec>
    <sec id="sec1.2">
      <label>1.2</label>
      <title>Research gaps</title>
      <p id="d1e3330">Our comprehensive literature review helped us to identify several important research gaps related to the use of the COVID-19 detection/segmentation methods. Below, we list the most important of them:</p>
      <p id="d1e3332">
        <list list-type="simple" id="d1e3334">
          <list-item id="lst1">
            <label>•</label>
            <p id="d1e3338">There are no sufficient COVID-19 image data to develop accurate and robust deep learning models. This lack of data can impact the performance of deep learning approaches.</p>
          </list-item>
          <list-item id="lst2">
            <label>•</label>
            <p id="d1e3343">To the best of our knowledge, there are very few studies that have used both types of images (CT scan and X-ray) simultaneously.</p>
          </list-item>
          <list-item id="lst3">
            <label>•</label>
            <p id="d1e3348">There are very few studies that have examined the uncertainty of the COVID-19 predictions provided by deep learning models.</p>
          </list-item>
          <list-item id="lst4">
            <label>•</label>
            <p id="d1e3353">Moreover, we found that there are very few COVID-19 classification studies considering the model’s robustness and its ability to process unknown data.</p>
          </list-item>
          <list-item id="lst5">
            <label>•</label>
            <p id="d1e3358">The impressive effect of different feature fusion methods has received less attention in the COVID-19 classification research. It is worth noting that feature fusion techniques are very effective both for improving the model’s performance and for dealing with uncertainty within ML and DL models.</p>
          </list-item>
        </list>
      </p>
    </sec>
    <sec id="sec1.3">
      <label>1.3</label>
      <title>Main contributions</title>
      <p id="d1e3365">The main contributions of this study are as follows:</p>
      <p id="d1e3367">
        <list list-type="simple" id="d1e3369">
          <list-item id="lst6">
            <label>•</label>
            <p id="d1e3373">We proposed a novel feature fusion model for accurate detection of COVID-19 cases.</p>
          </list-item>
          <list-item id="lst7">
            <label>•</label>
            <p id="d1e3378">We quantified the uncertainty in our proposed feature fusion model using effective Ensemble MC Dropout (EMCD) technique.</p>
          </list-item>
          <list-item id="lst8">
            <label>•</label>
            <p id="d1e3383">The proposed feature fusion model demonstrates strong robustness to data contamination (data noise).</p>
          </list-item>
          <list-item id="lst9">
            <label>•</label>
            <p id="d1e3388">Our new model provided very encouraging results in terms of unknown data detection.</p>
          </list-item>
        </list>
      </p>
      <p id="d1e3390">The main characteristics of the proposed <italic>UncertaintyFuseNet</italic> model are as follows: (i) It is an accurate model with promising performance, (ii) It can be used efficiently to carry out classification analysis of large CT and X-ray image datasets, (iii) It quantifies the prediction uncertainty, (iv) It is a reliable model in terms of processing noisy data, and finally, (v) It allows for an accurate detection of OoD samples.</p>
      <p id="d1e3395">The rest of this study is organized as follows. Section <xref rid="sec2" ref-type="sec">2</xref> formulates the proposed methodology. The main experiments of this study are discussed in Section <xref rid="sec3" ref-type="sec">3</xref>. Section <xref rid="sec4" ref-type="sec">4</xref> presents the obtained results and provides a comprehensive comparison with existing studies. Finally, the conclusions are presented in Section <xref rid="sec5" ref-type="sec">5</xref>.</p>
    </sec>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Proposed methodology</title>
    <p id="d1e3423">This section includes two main sub-sections describing: (i) Basic deep learning models in sub-Section <xref rid="sec2.1" ref-type="sec">2.1</xref>, (ii) and our novel feature fusion model, <inline-formula><mml:math id="d1e3431" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, in sub-Section <xref rid="sec2.2" ref-type="sec">2.2</xref>. It may be noted that we also applied two traditional machine learning algorithms (<italic>i.e.</italic>, <inline-formula><mml:math id="d1e3481" altimg="si4.svg" display="inline"><mml:mrow><mml:mi>R</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mspace class="nbsp" width="1em"/><mml:mi>F</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> (RF) and <inline-formula><mml:math id="d1e3511" altimg="si5.svg" display="inline"><mml:mrow><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace class="nbsp" width="1em"/><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> (DT, max-depth <inline-formula><mml:math id="d1e3541" altimg="si6.svg" display="inline"><mml:mo>=</mml:mo></mml:math></inline-formula> 50 and n-estimators <inline-formula><mml:math id="d1e3547" altimg="si6.svg" display="inline"><mml:mo>=</mml:mo></mml:math></inline-formula> 200)) and compared their performances with the considered deep learning models.</p>
    <sec id="sec2.1">
      <label>2.1</label>
      <title>Basic deep learning models</title>
      <p id="d1e3556">In this sub-section, we provide more details regarding two basic deep learning models: (i) Deep 1 (Simple CNN), and (ii) deep 2 (Multi-headed CNN). <xref rid="fig1" ref-type="fig">Fig. 1</xref>, <xref rid="fig2" ref-type="fig">Fig. 2</xref> show deep 1 (Simple CNN) and deep 2 (Multi-headed CNN) models, respectively. The first deep learning model (Simple CNN) includes three convolutional layers followed by MC dropout in the feature extraction layer. The extracted features are then given to the classification layer, including three dense layers and MC dropout. More details of the deep 1 model can be found in <xref rid="fig1" ref-type="fig">Fig. 1</xref>. In our second deep learning model, deep 2, <italic>i.e.</italic>, multi-headed CNN, comprises three main heads (as feature extractors). The extracted features in each branch are then given to the fusion layers, followed by the classification layer as illustrated in <xref rid="fig2" ref-type="fig">Fig. 2</xref>.</p>
      <p id="d1e3573">
        <fig id="fig1">
          <label>Fig. 1</label>
          <caption>
            <p>A general overview of the applied deep learning model deep 1 (simple CNN).</p>
          </caption>
          <graphic xlink:href="gr1_lrg"/>
        </fig>
        <fig id="fig2">
          <label>Fig. 2</label>
          <caption>
            <p>A general overview of the applied deep learning mode deep 2 (multi-headed CNN).</p>
          </caption>
          <graphic xlink:href="gr2_lrg"/>
        </fig>
      </p>
    </sec>
    <sec id="sec2.2">
      <label>2.2</label>
      <title>Proposed feature fusion model: <inline-formula><mml:math id="d1e3584" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula></title>
      <p id="d1e3623">Feature fusion is an approach used to combine features (different information) of the same sample (input) extracted by various methods. Assume <inline-formula><mml:math id="d1e3626" altimg="si9.svg" display="inline"><mml:mrow><mml:mi>Ω</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>ξ</mml:mi><mml:mo>∣</mml:mo><mml:mi>ξ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be a training sample (image) space of <inline-formula><mml:math id="d1e3654" altimg="si10.svg" display="inline"><mml:mi>m</mml:mi></mml:math></inline-formula> labeled samples (images). Given <inline-formula><mml:math id="d1e3659" altimg="si11.svg" display="inline"><mml:mrow><mml:mi>A</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>x</mml:mi><mml:mo>∣</mml:mo><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="d1e3688" altimg="si12.svg" display="inline"><mml:mrow><mml:mi>B</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>y</mml:mi><mml:mo>∣</mml:mo><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, …, and <inline-formula><mml:math id="d1e3716" altimg="si13.svg" display="inline"><mml:mrow><mml:mi>Z</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mi>n</mml:mi><mml:mo>∣</mml:mo><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="d1e3744" altimg="si14.svg" display="inline"><mml:mi>x</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="d1e3749" altimg="si15.svg" display="inline"><mml:mi>y</mml:mi></mml:math></inline-formula>, …, <inline-formula><mml:math id="d1e3755" altimg="si16.svg" display="inline"><mml:mi>n</mml:mi></mml:math></inline-formula> are the feature vectors of the same input sample <inline-formula><mml:math id="d1e3760" altimg="si17.svg" display="inline"><mml:mi>ξ</mml:mi></mml:math></inline-formula> extracted by various deep learning models, respectively. Therefore, the total feature fusion vector space <inline-formula><mml:math id="d1e3765" altimg="si18.svg" display="inline"><mml:msubsup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> obtained from different sources can be calculated as follows: <disp-formula id="fd1"><label>(1)</label><mml:math id="d1e3788" altimg="si19.svg" display="block"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msubsup><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>Z</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
</p>
      <p id="d1e3850">In this study, after preprocessing the data, we feed our dataset to the model. Our model consists of two major branches: The first branch has five convolutional blocks. Each block is made up of two tandem convolutional layers followed by batch normalization and max-pooling layers. Also, the fourth and fifth blocks have dropout layers in their outputs. It is worth noting that separable convolutions were utilized in the <inline-formula><mml:math id="d1e3853" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> model in the second and subsequent layers. However, we used the usual convolution layer in other architectures (First layer of <inline-formula><mml:math id="d1e3894" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, Simple CNN, and Multi-headed CNN). The following training parameters were used in our experiments: The learning rate of the proposed model is 0.0005, the batch size is 128, the number of epochs is 200, and Adam is selected as our optimizer. The second branch is a VGG16 transfer learning network whose output is used in the fusion layer. After two branches, the model is followed by a fusion layer that concatenates the third, fourth, and fifth convolutional layers’ outputs with VGG16’s output.</p>
      <p id="d1e3934">Finally, we used fully connected layers to process the fused features and classify the data. In this part, we have used four dense layers with 512, 128, 64, and 3 neurons with the ReLU activation function, respectively. The output of the first three dense layers has a dropout in their outcomes with a rate equal to 0.7, 0.5, and 0.3, respectively.</p>
      <p id="d1e3936">The stated model is not simplistic. Indeed, to boost the model’s power in dealing with data and extracting high-quality features, we have employed a novel feature fusion approach combining different sources:</p>
      <p id="d1e3938">
        <list list-type="simple" id="d1e3940">
          <list-item id="lst10">
            <label>•</label>
            <p id="d1e3944">We selected the third convolutional block’s output as a fusion source to have a holistic perspective about the data distribution. These features help the model to consider the unprocessed and raw information and use it in the prediction.</p>
          </list-item>
          <list-item id="lst11">
            <label>•</label>
            <p id="d1e3949">We included the final and penultimate convolutional blocks’ outputs in the feature fusion layer to have more accurate information. This feature gives a detailed view of the dataset to model and helps the model to process advanced classification features.</p>
          </list-item>
          <list-item id="lst12">
            <label>•</label>
            <p id="d1e3954">As has been suggested by recent pneumonia detection studies, where the pretrained networks have been successively used to create high-quality generalizable features, we used the output of VGG16 in the fusion layer.</p>
          </list-item>
        </list>
      </p>
      <p id="d1e3956">The pseudo-code of the proposed <inline-formula><mml:math id="d1e3959" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> model for detecting the COVID-19 cases is reported in Algorithm 1. Its general view is illustrated in <xref rid="fig3" ref-type="fig">Fig. 3</xref>. <fig id="dfig1"><graphic xlink:href="fx1001_lrg"/></fig>
</p>
      <p id="d1e4012">It should be noted that the detailed information about Convolution blocks in <xref rid="fig1" ref-type="fig">Figs. 1</xref>, <xref rid="fig2" ref-type="fig">2</xref>, and <xref rid="fig3" ref-type="fig">3</xref> is reported in <xref rid="tblB.12" ref-type="table">Table B.12</xref>, in the <xref rid="appA" ref-type="sec">Appendix</xref>. To generate the final prediction, after training the applied models with uncertainty module, we have first run each model <inline-formula><mml:math id="d1e4071" altimg="si24.svg" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula> times. Thereafter, we average the predicted softmax probabilities (outputs) in those <inline-formula><mml:math id="d1e4076" altimg="si24.svg" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula> random predictions of data <inline-formula><mml:math id="d1e4081" altimg="si14.svg" display="inline"><mml:mi>x</mml:mi></mml:math></inline-formula> through <inline-formula><mml:math id="d1e4087" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> and stochastic sampling dropout mask <inline-formula><mml:math id="d1e4128" altimg="si28.svg" display="inline"><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each single prediction. <disp-formula id="fd2"><label>(2)</label><mml:math id="d1e4144" altimg="si29.svg" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo class="qopname">Softmax</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">UncertaintyFuseNet</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
<disp-formula id="fd3"><label>(3)</label><mml:math id="d1e4191" altimg="si30.svg" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo linebreak="badbreak">∗</mml:mo></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
<fig id="fig3"><label>Fig. 3</label><caption><p>A general overview of the proposed <inline-formula><mml:math id="d1e42" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> model inspired by a hierarchical feature fusion approach and EMCD.</p></caption><graphic xlink:href="gr3_lrg"/></fig><fig id="fig4"><label>Fig. 4</label><caption><p>Some random image samples from the CT scan and X-ray datasets considered in our study.</p></caption><graphic xlink:href="gr4_lrg"/></fig><fig id="fig5"><label>Fig. 5</label><caption><p>ROC curves obtained for the five considered ML models for the CT scan data without quantifying uncertainty.</p></caption><graphic xlink:href="gr5_lrg"/></fig><fig id="fig6"><label>Fig. 6</label><caption><p>ROC curves obtained for the five considered ML models for the X-ray data without quantifying uncertainty.</p></caption><graphic xlink:href="gr6_lrg"/></fig><fig id="fig7"><label>Fig. 7</label><caption><p>ROC curves obtained for the three considered DL models for the CT scan data with UQ.</p></caption><graphic xlink:href="gr7_lrg"/></fig><fig id="fig8"><label>Fig. 8</label><caption><p>ROC curves obtained for the three considered DL models for the X-ray data with UQ.</p></caption><graphic xlink:href="gr8_lrg"/></fig><fig id="fig9"><label>Fig. 9</label><caption><p>The MNIST sample image fed to the deep learning models as an unknown sample.</p></caption><graphic xlink:href="gr9_lrg"/></fig><fig id="fig10"><label>Fig. 10</label><caption><p>T-SNE visualization of different models applied to the CT scan data without and with quantifying uncertainty.</p></caption><graphic xlink:href="gr10_lrg"/></fig><fig id="fig11"><label>Fig. 11</label><caption><p>T-SNE visualization of different models applied to the X-ray data without and with quantifying uncertainty.</p></caption><graphic xlink:href="gr11_lrg"/></fig><fig id="fig12"><label>Fig. 12</label><caption><p>Grad-CAM visualization for our proposed fusion model without and with UQ for nCT (<xref rid="fig12" ref-type="fig">Fig. 12</xref>, <xref rid="fig12" ref-type="fig">Fig. 12</xref>), NiCT (<xref rid="fig12" ref-type="fig">Fig. 12</xref>, <xref rid="fig12" ref-type="fig">Fig. 12</xref>), and pCT (<xref rid="fig12" ref-type="fig">Fig. 12</xref>, <xref rid="fig12" ref-type="fig">Fig. 12</xref>) classes using CT scan dataset.</p></caption><graphic xlink:href="gr12_lrg"/></fig><fig id="fig13"><label>Fig. 13</label><caption><p>Grad-CAM visualization for our proposed fusion model without and with UQ for COVID-19 (<xref rid="fig13" ref-type="fig">Fig. 13</xref>, <xref rid="fig13" ref-type="fig">Fig. 13</xref>), Normal (<xref rid="fig13" ref-type="fig">Fig. 13</xref>, <xref rid="fig13" ref-type="fig">Fig. 13</xref>), and Pneumonia (<xref rid="fig13" ref-type="fig">Fig. 13</xref>, <xref rid="fig13" ref-type="fig">Fig. 13</xref>) classes using the X-ray dataset.</p></caption><graphic xlink:href="gr13_lrg"/></fig><fig id="fig14"><label>Fig. 14</label><caption><p>The output posterior distributions of our proposed feature fusion model calculated for the nCT <xref rid="fig14" ref-type="fig">14(a)</xref>, NiCT <xref rid="fig14" ref-type="fig">14(b)</xref> and pCT <xref rid="fig14" ref-type="fig">14(c)</xref> data classes for the CT scan dataset, and the COVID-19 <xref rid="fig14" ref-type="fig">14(d)</xref>, Normal <xref rid="fig14" ref-type="fig">14(e)</xref> and Pneumonia <xref rid="fig14" ref-type="fig">14(f)</xref> data classes for the X-ray dataset.</p></caption><graphic xlink:href="gr14_lrg"/></fig><fig id="figB.15"><label>Fig. B.15</label><caption><p>Confusion matrices obtained using different models for the CT scan datasets without quantifying uncertainty.</p></caption><graphic xlink:href="gr15_lrg"/></fig><fig id="figB.16"><label>Fig. B.16</label><caption><p>Confusion matrices obtained using different models for the X-ray dataset without quantifying uncertainty.</p></caption><graphic xlink:href="gr16_lrg"/></fig><fig id="figB.17"><label>Fig. B.17</label><caption><p>Confusion matrices obtained using different models for the CT scan dataset with quantifying uncertainty.</p></caption><graphic xlink:href="gr17_lrg"/></fig><fig id="figB.18"><label>Fig. B.18</label><caption><p>Confusion matrices obtained using different models for the X-ray dataset with quantifying uncertainty.</p></caption><graphic xlink:href="gr18_lrg"/></fig></p>
      <p id="d1e4238">We then used the model ensembling and acquired predictions from the <inline-formula><mml:math id="d1e4241" altimg="si24.svg" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula> trained models with various weight distributions and initialized weights using this strategy. This allowed us to improve the model’s performance drastically. Thus, after training the model, we use the MC equation with <inline-formula><mml:math id="d1e4246" altimg="si32.svg" display="inline"><mml:mi>K</mml:mi></mml:math></inline-formula>
<inline-formula><mml:math id="d1e4251" altimg="si6.svg" display="inline"><mml:mo>=</mml:mo></mml:math></inline-formula> 200 (see Eq. <xref rid="fd3" ref-type="disp-formula">(3)</xref>) to obtain predictions of the model through different stochastic paths (using MC dropouts to create randomness in our architectures). After getting all predictions, we calculate the mean for each sample. Using this approach, we obtain an ensemble of different models which helps boost the model’s performance. Precisely, we run the proposed model 200 times for each sample at the test stage and get an average prediction as the final prediction of the model. <disp-formula id="fd4"><label>(4)</label><mml:math id="d1e4268" altimg="si34.svg" display="block"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>_</mml:mi><mml:mi>C</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo class="qopname">Argmax</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mo linebreak="badbreak">∗</mml:mo></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
</p>
      <p id="d1e4324">The pseudo-code of the applied EMCD procedure included in our <inline-formula><mml:math id="d1e4327" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> model for detecting COVID-19 cases is summarized in Algorithm 2. Furthermore, the learning rate of the proposed model is 0.0005, the batch size is 128, the number of epochs is 200 and Adam is selected as our optimizer. <fig id="dfig2"><graphic xlink:href="fx1002_lrg"/></fig>
</p>
    </sec>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Experiments</title>
    <p id="d1e4381">In this section, we present : the data considered in our study (see sub-Section <xref rid="sec3.1" ref-type="sec">3.1</xref>), the results obtained using our new model (see sub-Section <xref rid="sec3.2" ref-type="sec">3.2</xref>), the results showing that our new model is robust against noise (see sub-Section <xref rid="sec3.3" ref-type="sec">3.3</xref>), and the results showing how our new model copes with unknown data (see sub-Section <xref rid="sec3.4" ref-type="sec">3.4</xref>).</p>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Datas considered</title>
      <p id="d1e4409">In this study, two types of input image data were used: CT scan <xref rid="b18" ref-type="bibr">[18]</xref>
<xref rid="fn2" ref-type="fn">2</xref>
and X-ray<xref rid="fn3" ref-type="fn">3</xref>
images (see <xref rid="tbl1" ref-type="table">Table 1</xref>). Some random samples of the CT scan and X-ray datasets considered in this study are shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>. The CT scan dataset has classes of data: non-informative CT (NiCT), positive CT (pCT), and negative CT (nCT) images. The X-ray dataset also has three data classes: COVID-19, Normal, and Pneumonia images.</p>
      <p id="d1e4447">It should be pointed out that for the CT scan dataset we randomly used 70% of the whole data for training and the rest (30%) for testing the applied models. However, the X-ray dataset was originally divided into two main categories: train (5144 images) and test (1288 images). Thus, we used these train and test categories in our study as well.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Characteristics of the CT scan and X-ray datasets considered in our study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left"># of samples</th><th align="left"># of classes</th></tr></thead><tbody><tr><td align="left">CT scan images</td><td align="left">19 685 (70% train, 30% test)</td><td align="left">3</td></tr><tr><td align="left">X-ray images</td><td align="left">6432 (train: 5144, test: 1288)</td><td align="left">3</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>Experimental results</title>
      <p id="d1e4456">In this section, the experimental results are presented and discussed. Since we also considered the impact of UQ methods, our experiments have been conducted with and without applying them for detection of COVID-19 cases. In our first experiment, we compared five different machine learning models, including Random Forest (RF), Decision Trees (DT, max-depth <inline-formula><mml:math id="d1e4459" altimg="si6.svg" display="inline"><mml:mo>=</mml:mo></mml:math></inline-formula> 50, and n-estimators <inline-formula><mml:math id="d1e4464" altimg="si6.svg" display="inline"><mml:mo>=</mml:mo></mml:math></inline-formula> 200), Deep 1 (Simple CNN), Deep 2 (Multi-headed CNN), and our proposed model (feature fusion model).</p>
      <sec id="sec3.2.1">
        <label>3.2.1</label>
        <title>COVID-19 classification without considering uncertainty</title>
        <p id="d1e4473">First, we investigated the performance of the five considered classifiers (RF, DT, simple CNN, multi-headed CNN and our proposed feature fusion model) without considering uncertainty. The obtained results are presented in <xref rid="tbl2" ref-type="table">Table 2</xref>, <xref rid="tbl3" ref-type="table">Table 3</xref> for the CT scan and X-ray datasets, respectively. As shown in <xref rid="tbl2" ref-type="table">Table 2</xref> our feature fusion model outperformed the other methods for the CT scan dataset, providing the accuracy of 99.136%, and followed by simple CNN with the accuracy of 98.763%. The obtained results also indicate that DT provided the weakest performance for the CT scan dataset among the five competing models. <xref rid="figB.15" ref-type="fig">Figs. B.15</xref> (in the <xref rid="appA" ref-type="sec">Appendix</xref>) and <xref rid="fig5" ref-type="fig">5</xref> present the confusion matrices and the ROC curves obtained for the CT scan dataset without quantifying uncertainty, respectively.</p>
        <p id="d1e4496">To demonstrate the effectiveness of the proposed feature fusion model, the same five ML have been applied to analyze X-ray data. It can be observed from <xref rid="tbl3" ref-type="table">Table 3</xref> that our feature fusion model performed much better than the other competing ML models, providing the accuracy of 97.127%, followed by the multi-headed CNN model with the accuracy of 94.953%. The traditional Decision Tree model provided much worse results for the X-ray data (the recall value of 84.006%) than for the CT scan data (the recall value of 93.040%). <xref rid="figB.16" ref-type="fig">Figs. B.16</xref> (in the <xref rid="appB" ref-type="sec">Appendix</xref>) and <xref rid="fig6" ref-type="fig">6</xref> present the confusion matrices and the ROC curves obtained by the five ML models for the X-ray dataset without quantifying uncertainty.<table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>Comparison of the results (given in %) provided by different ML models for detecting COVID-19 cases for the CT scan dataset: Results without considering uncertainty.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">ML model</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F-measure</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td align="left">RF</td><td align="left">97.111</td><td align="left">97.070</td><td align="left">97.091</td><td align="left">97.070</td></tr><tr><td align="left">DT</td><td align="left">93.049</td><td align="left">93.040</td><td align="left">93.045</td><td align="left">93.040</td></tr><tr><td align="left">Deep 1 (Simple CNN)</td><td align="left">98.787</td><td align="left">98.763</td><td align="left">98.775</td><td align="left">98.763</td></tr><tr><td align="left">Deep 2 (Multi-headed CNN)</td><td align="left">98.599</td><td align="left">98.577</td><td align="left">98.588</td><td align="left">98.577</td></tr><tr><td align="left"><bold>Proposed (Fusion model)</bold></td><td align="left"><bold>99.137</bold></td><td align="left"><bold>99.136</bold></td><td align="left"><bold>99.136</bold></td><td align="left"><bold>99.136</bold></td></tr></tbody></table></table-wrap></p>
        <p id="d1e4517">
          <table-wrap position="anchor" id="tbl3">
            <label>Table 3</label>
            <caption>
              <p>Comparison of the results (given in %) provided by different ML models for detecting COVID-19 cases for the X-ray dataset: Results without considering uncertainty.</p>
            </caption>
            <table frame="hsides" rules="groups">
              <thead>
                <tr>
                  <th align="left">ML model</th>
                  <th align="left">Precision</th>
                  <th align="left">Recall</th>
                  <th align="left">F-measure</th>
                  <th align="left">Accuracy</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left">RF</td>
                  <td align="left">91.532</td>
                  <td align="left">91.381</td>
                  <td align="left">91.456</td>
                  <td align="left">91.381</td>
                </tr>
                <tr>
                  <td align="left">DT</td>
                  <td align="left">83.828</td>
                  <td align="left">84.006</td>
                  <td align="left">83.917</td>
                  <td align="left">84.006</td>
                </tr>
                <tr>
                  <td align="left">Deep 1 (Simple CNN)</td>
                  <td align="left">93.847</td>
                  <td align="left">93.167</td>
                  <td align="left">93.506</td>
                  <td align="left">93.167</td>
                </tr>
                <tr>
                  <td align="left">Deep 2 (Multi-headed CNN)</td>
                  <td align="left">95.041</td>
                  <td align="left">94.953</td>
                  <td align="left">94.997</td>
                  <td align="left">94.953</td>
                </tr>
                <tr>
                  <td align="left">
                    <bold>Proposed (Fusion model)</bold>
                  </td>
                  <td align="left">
                    <bold>97.121</bold>
                  </td>
                  <td align="left">
                    <bold>97.127</bold>
                  </td>
                  <td align="left">
                    <bold>97.124</bold>
                  </td>
                  <td align="left">
                    <bold>97.127</bold>
                  </td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
        </p>
      </sec>
      <sec id="sec3.2.2">
        <label>3.2.2</label>
        <title>COVID-19 classification considering uncertainty</title>
        <p id="d1e4525">The results, discussed in the previous sub-Section <xref rid="sec3.2.1" ref-type="sec">3.2.1</xref>, provided by our new feature fusion model are promising, suggesting that it can be used by clinical practitioners for automatic detection of COVID-19 cases. We believe that new efficient intelligent (<italic>i.e.</italic> ML and DL) models to deal with COVID-19 data are urgently needed. At the same time, we believe in the uncertainty estimates should accompany such intelligent models. To accomplish this, we applied the uncertainty quantification method, called EMC dropout, to estimate the uncertainty of our deep learning predictions. The EMC method was used in the framework of the Deep 1 (Simple CNN) and Deep 2 (Multi-headed CNN) models, and our proposed <inline-formula><mml:math id="d1e4536" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> model.</p>
        <p id="d1e4576"><xref rid="tbl4" ref-type="table">Table 4</xref> and <xref rid="figB.17" ref-type="fig">Fig. B.17</xref> in the <xref rid="appB" ref-type="sec">Appendix</xref> (confusion matrices) and <xref rid="fig7" ref-type="fig">Fig. 7</xref> (ROC curves) show the results provided by the three compared deep learning models considering uncertainty for the CT scan dataset. As shown in <xref rid="tbl4" ref-type="table">Table 4</xref>, our feature fusion model yielded a better classification performance compared to the Deep 1 and Deep 2 CNN-based models. <inline-formula><mml:math id="d1e4599" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> provided the accuracy value of 99.085%, followed by the Deep 1 model with the accuracy value of 98.831%, for the CT scan data. The results obtained using deep learning models with and without uncertainty quantification (UQ) reveal that our proposed feature fusion model with UQ method has had a slightly poorer performance than the model without UQ. The Deep 1 CNN model performed slightly better with UQ, while the Deep 2 CNN model performed slightly better without UQ.</p>
        <p id="d1e4639">We also evaluated the performance of three considered DL models with uncertainty quantification on the X-ray dataset. The obtained statistics, confusion matrices, and the ROC curves for the three competing DL models applied are presented in <xref rid="tbl5" ref-type="table">Table 5</xref> and <xref rid="figB.18" ref-type="fig">Fig. B.18</xref> in the <xref rid="appB" ref-type="sec">Appendix</xref> and <xref rid="fig8" ref-type="fig">Fig. 8</xref> (ROC curves), respectively. Our proposed feature fusion model achieved the best performance for COVID-19 detection using X-ray dataset with an accuracy of 96.350% compared to the simple CNN (accuracy of 95.263%). For the X-ray data, the proposed <inline-formula><mml:math id="d1e4661" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> model outperformed the Deep 1 simple CNN and Deep 2 multi-headed CNNmodels, but the Deep 1 simple CNN was slightly surpassed by the Deep 2 multi-headed CNN (see <xref rid="tbl5" ref-type="table">Table 5</xref>, <xref rid="fig8" ref-type="fig">Fig. 8</xref> and also <xref rid="figB.18" ref-type="fig">Fig. B.18</xref> in the <xref rid="appA" ref-type="sec">Appendix</xref>).<table-wrap position="float" id="tbl4"><label>Table 4</label><caption><p>Comparison of the results (given in %) provided by the 3 DL models for detecting COVID-19 cases for the CT scan dataset: Results obtained with uncertainty quantification.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">DL model</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F-measure</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td align="left">Deep 1 (Simple CNN)</td><td align="left">98.831</td><td align="left">98.854</td><td align="left">98.843</td><td align="left">98.831</td></tr><tr><td align="left">Deep 2 (Multi-headed CNN)</td><td align="left">98.493</td><td align="left">98.523</td><td align="left">98.508</td><td align="left">98.493</td></tr><tr><td align="left"><bold>Proposed (Fusion model)</bold></td><td align="left"><bold>99.085</bold></td><td align="left"><bold>99.085</bold></td><td align="left"><bold>99.085</bold></td><td align="left"><bold>99.085</bold></td></tr></tbody></table></table-wrap></p>
        <p id="d1e4718">
          <table-wrap position="anchor" id="tbl5">
            <label>Table 5</label>
            <caption>
              <p>Comparison of the results (given in %) provided by the 3 DL models for detecting COVID-19 cases for the X-ray dataset: Results obtained with uncertainty quantification.</p>
            </caption>
            <table frame="hsides" rules="groups">
              <thead>
                <tr>
                  <th align="left">Method</th>
                  <th align="left">Precision</th>
                  <th align="left">Recall</th>
                  <th align="left">F-measure</th>
                  <th align="left">Accuracy</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left">Deep 1 (Simple CNN)</td>
                  <td align="left">95.263</td>
                  <td align="left">95.354</td>
                  <td align="left">95.309</td>
                  <td align="left">95.263</td>
                </tr>
                <tr>
                  <td align="left">Deep 2 (Multi-headed)</td>
                  <td align="left">95.186</td>
                  <td align="left">95.257</td>
                  <td align="left">95.222</td>
                  <td align="left">95.186</td>
                </tr>
                <tr>
                  <td align="left">
                    <bold>Proposed (Fusion model)</bold>
                  </td>
                  <td align="left">
                    <bold>96.350</bold>
                  </td>
                  <td align="left">
                    <bold>96.370</bold>
                  </td>
                  <td align="left">
                    <bold>96.360</bold>
                  </td>
                  <td align="left">
                    <bold>96.350</bold>
                  </td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
        </p>
      </sec>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Robustness against noise</title>
      <p id="d1e4726">An individual visual system is significantly robust against a wide variety of natural noises and corruptions occurring in the nature such as snow, fog or rain <xref rid="b19" ref-type="bibr">[19]</xref>. However, the overall performance of various modern image and speech recognition systems is greatly degraded when evaluated using previously unseen noises and corruptions. Thus, conducting robustness tests for considered ML and DL models can be necessary to reveal their level of stability against noise. In this study, the robustness of the applied deep learning models against noise has been investigated.</p>
      <p id="d1e4732">We also added different noise variables to both CT scan and X-ray datasets to evaluate the performance of Simple CNN, Multi-headed CNN and our proposed feature fusion model. Gaussian noise variables with different standard deviations (STD) were generated. The generated STD values were the following: 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, and 0.6, whereas the value of Mean was equal to 0. Our simulation results obtained for the CT scan and X-ray datasets are presented in <xref rid="tbl6" ref-type="table">Table 6</xref>, <xref rid="tbl7" ref-type="table">Table 7</xref>, respectively. It may be noted from <xref rid="tbl6" ref-type="table">Table 6</xref> (CT scan data results) that both Simple CNN and Multi-headed CNN models did not perform well with noisy data compared to our feature fusion model. The results reported in <xref rid="tbl6" ref-type="table">Table 6</xref> indicate that the values of all metrics computed for Simple and Multi-headed CNNs decrease dramatically as the level of noise increases. In contrast, our feature fusion model has been much more robust against noise according to all metrics considered.</p>
      <p id="d1e4746"><xref rid="tbl7" ref-type="table">Table 7</xref> reports the performance of the three selected deep learning models under different noise conditions for the X-ray dataset. Both Simple CNN and Multi-headed CNN did not perform well in this context, whereas our new model was usually much more robust against noise. It should be noted that our feature fusion model performed better for the CT scan data than for the X-ray data.<table-wrap position="float" id="tbl6"><label>Table 6</label><caption><p>Robustness against noise results (given in %) provided by the 3 compared DL models for detecting COVID-19 cases for the CT scan dataset. Here, <inline-formula><mml:math id="d1e603" altimg="si41.svg" display="inline"><mml:mrow><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e617" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>S</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="d1e646" altimg="si43.svg" display="inline"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the mean of the noise and <inline-formula><mml:math id="d1e657" altimg="si44.svg" display="inline"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is standard deviation of the noise.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">DL model</th><th align="left">Noise STD</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F-measure</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td align="left" rowspan="9">Deep 1 (Simple CNN)</td><td align="left">0.0001</td><td align="left">98.852</td><td align="left">98.831</td><td align="left">98.842</td><td align="left">98.831</td></tr><tr><td align="left">0.001</td><td align="left">98.868</td><td align="left">98.848</td><td align="left">98.858</td><td align="left">98.848</td></tr><tr><td align="left">0.01</td><td align="left">98.754</td><td align="left">98.730</td><td align="left">98.742</td><td align="left">98.730</td></tr><tr><td align="left">0.1</td><td align="left">95.785</td><td align="left">95.614</td><td align="left">95.700</td><td align="left">95.614</td></tr><tr><td align="left">0.2</td><td align="left">89.270</td><td align="left">87.284</td><td align="left">88.266</td><td align="left">87.284</td></tr><tr><td align="left">0.3</td><td align="left">86.370</td><td align="left">82.593</td><td align="left">84.439</td><td align="left">82.593</td></tr><tr><td align="left">0.4</td><td align="left">82.628</td><td align="left">75.194</td><td align="left">78.736</td><td align="left">75.194</td></tr><tr><td align="left">0.5</td><td align="left">78.303</td><td align="left">64.053</td><td align="left">70.465</td><td align="left">64.053</td></tr><tr><td align="left">0.6</td><td align="left">75.770</td><td align="left">57.534</td><td align="left">65.405</td><td align="left">57.534</td></tr><tr><td colspan="6"><hr/></td></tr><tr><td align="left" rowspan="9">Deep 2 (Multi-headed)</td><td align="left">0.0001</td><td align="left">98.526</td><td align="left">98.493</td><td align="left">98.509</td><td align="left">98.493</td></tr><tr><td align="left">0.001</td><td align="left">98.524</td><td align="left">98.493</td><td align="left">98.508</td><td align="left">98.493</td></tr><tr><td align="left">0.01</td><td align="left">98.558</td><td align="left">98.526</td><td align="left">98.542</td><td align="left">98.526</td></tr><tr><td align="left">0.1</td><td align="left">93.447</td><td align="left">92.871</td><td align="left">93.158</td><td align="left">92.871</td></tr><tr><td align="left">0.2</td><td align="left">86.943</td><td align="left">83.423</td><td align="left">85.147</td><td align="left">83.423</td></tr><tr><td align="left">0.3</td><td align="left">80.168</td><td align="left">69.065</td><td align="left">74.203</td><td align="left">69.065</td></tr><tr><td align="left">0.4</td><td align="left">76.032</td><td align="left">58.465</td><td align="left">66.102</td><td align="left">58.465</td></tr><tr><td align="left">0.5</td><td align="left">73.837</td><td align="left">54.690</td><td align="left">62.837</td><td align="left">54.690</td></tr><tr><td align="left">0.6</td><td align="left">72.914</td><td align="left">53.149</td><td align="left">61.482</td><td align="left">53.149</td></tr><tr><td colspan="6"><hr/></td></tr><tr><td align="left" rowspan="9"><bold>Proposed (Fusion model)</bold></td><td align="left">0.0001</td><td align="left">99.085</td><td align="left">99.085</td><td align="left">99.085</td><td align="left">99.085</td></tr><tr><td align="left">0.001</td><td align="left">99.119</td><td align="left">99.119</td><td align="left">99.119</td><td align="left">99.119</td></tr><tr><td align="left">0.01</td><td align="left">99.194</td><td align="left">99.187</td><td align="left">99.190</td><td align="left">99.187</td></tr><tr><td align="left">0.1</td><td align="left">99.098</td><td align="left">99.085</td><td align="left">99.092</td><td align="left">99.085</td></tr><tr><td align="left">0.2</td><td align="left">98.828</td><td align="left">98.814</td><td align="left">98.821</td><td align="left">98.814</td></tr><tr><td align="left">0.3</td><td align="left">98.109</td><td align="left">98.086</td><td align="left">98.097</td><td align="left">98.086</td></tr><tr><td align="left">0.4</td><td align="left">96.956</td><td align="left">96.884</td><td align="left">96.920</td><td align="left">96.884</td></tr><tr><td align="left">0.5</td><td align="left">96.201</td><td align="left">96.088</td><td align="left">96.145</td><td align="left">96.088</td></tr><tr><td align="left">0.6</td><td align="left">95.804</td><td align="left">95.665</td><td align="left">95.734</td><td align="left">95.665</td></tr></tbody></table></table-wrap></p>
      <p id="d1e4754">This stage of the experiments was necessary to demonstrate the stability of the applied models against noise. Our results clearly indicate that the proposed feature fusion model is robust against noise for both considered types of image data: CT scan and X-ray images. It should be mentioned that there are various COVID-19 diagnostic resources, the main being CT scan and X-ray imaging tools. Thus, we were motivated to propose an efficient deep learning-based COVID-19 detection model working promisingly on both CT scan and X-ray images in order to assist clinicians in providing timely diagnostics and clinical support to their patients in both of these fields.<table-wrap position="float" id="tbl7"><label>Table 7</label><caption><p>Robustness against noise results (given in %) provided by the 3 compared DL models for detecting COVID-19 cases for the X-ray dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">DL model</th><th align="left">Noise STD</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F-measure</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td align="left" rowspan="9">Deep 1 (Simple CNN)</td><td align="left">0.0001</td><td align="left">95.408</td><td align="left">95.341</td><td align="left">95.375</td><td align="left">95.341</td></tr><tr><td align="left">0.001</td><td align="left">95.408</td><td align="left">95.341</td><td align="left">95.341</td><td align="left">95.341</td></tr><tr><td align="left">0.01</td><td align="left">95.338</td><td align="left">95.263</td><td align="left">95.301</td><td align="left">95.263</td></tr><tr><td align="left">0.1</td><td align="left">94.554</td><td align="left">94.254</td><td align="left">94.404</td><td align="left">94.254</td></tr><tr><td align="left">0.2</td><td align="left">91.540</td><td align="left">89.285</td><td align="left">90.398</td><td align="left">89.285</td></tr><tr><td align="left">0.3</td><td align="left">88.534</td><td align="left">82.065</td><td align="left">85.176</td><td align="left">82.065</td></tr><tr><td align="left">0.4</td><td align="left">85.770</td><td align="left">73.136</td><td align="left">78.951</td><td align="left">73.136</td></tr><tr><td align="left">0.5</td><td align="left">84.294</td><td align="left">64.518</td><td align="left">73.092</td><td align="left">64.518</td></tr><tr><td align="left">0.6</td><td align="left">82.545</td><td align="left">57.375</td><td align="left">67.696</td><td align="left">57.375</td></tr><tr><td colspan="6"><hr/></td></tr><tr><td align="left" rowspan="9">Deep 2 (Multi-headed)</td><td align="left">0.0001</td><td align="left">95.474</td><td align="left">95.419</td><td align="left">95.446</td><td align="left">95.419</td></tr><tr><td align="left">0.001</td><td align="left">95.188</td><td align="left">95.108</td><td align="left">95.148</td><td align="left">95.108</td></tr><tr><td align="left">0.01</td><td align="left">95.404</td><td align="left">95.341</td><td align="left">95.372</td><td align="left">95.341</td></tr><tr><td align="left">0.1</td><td align="left">93.922</td><td align="left">93.322</td><td align="left">93.621</td><td align="left">93.322</td></tr><tr><td align="left">0.2</td><td align="left">88.861</td><td align="left">82.453</td><td align="left">85.537</td><td align="left">82.453</td></tr><tr><td align="left">0.3</td><td align="left">83.781</td><td align="left">58.074</td><td align="left">68.598</td><td align="left">58.074</td></tr><tr><td align="left">0.4</td><td align="left">82.207</td><td align="left">40.062</td><td align="left">53.871</td><td align="left">40.062</td></tr><tr><td align="left">0.5</td><td align="left">81.750</td><td align="left">31.521</td><td align="left">45.499</td><td align="left">31.521</td></tr><tr><td align="left">0.6</td><td align="left">81.366</td><td align="left">27.639</td><td align="left">41.262</td><td align="left">27.639</td></tr><tr><td colspan="6"><hr/></td></tr><tr><td align="left" rowspan="9"><bold>Proposed (Fusion model)</bold></td><td align="left">0.0001</td><td align="left">96.498</td><td align="left">96.506</td><td align="left">96.502</td><td align="left">96.506</td></tr><tr><td align="left">0.001</td><td align="left">96.568</td><td align="left">96.583</td><td align="left">96.576</td><td align="left">96.583</td></tr><tr><td align="left">0.01</td><td align="left">96.492</td><td align="left">96.506</td><td align="left">96.499</td><td align="left">96.506</td></tr><tr><td align="left">0.1</td><td align="left">96.363</td><td align="left">96.350</td><td align="left">96.357</td><td align="left">96.350</td></tr><tr><td align="left">0.2</td><td align="left">94.403</td><td align="left">94.254</td><td align="left">94.329</td><td align="left">94.254</td></tr><tr><td align="left">0.3</td><td align="left">91.769</td><td align="left">91.071</td><td align="left">91.418</td><td align="left">91.071</td></tr><tr><td align="left">0.4</td><td align="left">88.225</td><td align="left">85.714</td><td align="left">86.951</td><td align="left">85.714</td></tr><tr><td align="left">0.5</td><td align="left">84.181</td><td align="left">78.804</td><td align="left">81.404</td><td align="left">78.804</td></tr><tr><td align="left">0.6</td><td align="left">81.082</td><td align="left">68.322</td><td align="left">74.157</td><td align="left">68.322</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec3.4">
      <label>3.4</label>
      <title>Unknown data detection</title>
      <p id="d1e4763">In this sub-section, we evaluate the performance of deep learning models when they are fed by unknown images. In these experimental settings the models either do not know or cannot clearly estimate the uncertainty of their predictions. To perform this evaluation, we fed the DL models being compared with one sample image from the well-known MNIST dataset (see <xref rid="fig9" ref-type="fig">Fig. 9</xref>). The mean and the STD values of the Simple CNN model, Multi-headed CNN model and our proposed feature fusion model are reported in <xref rid="tbl8" ref-type="table">Table 8</xref>. The obtained results indicate that our feature fusion model showed its uncertainty towards unknown data much better than the two other DL models.</p>
      <p id="d1e4773">We fed the MNIST sample image presented in <xref rid="fig9" ref-type="fig">Fig. 9</xref> to the three deep learning models trained on CT scan and X-ray datasets, and then predicted the class of this unknown image sample.</p>
      <p id="d1e4779">Estimating uncertainty of traditional machine learning and deep learning models using different UQ methods is vital during critical predictions such as medical case studies. Ideally, the applied ML models should be able to capture a portion of both epistemic and aleatoric uncertainties. In this study, we applied a new feature fusion model to classify two types of medical data: CT scan and X-ray images. <xref rid="tbl8" ref-type="table">Table 8</xref> reports the <italic>Mean</italic> and the <italic>STD</italic> values of the three considered deep learning models applied to unknown data. It should be noted that the <italic>Mean</italic> value accounts for the model’s prediction and <italic>STD</italic> accounts for its uncertainty. As reported in <xref rid="tbl8" ref-type="table">Table 8</xref>, our model usually provides zero (or close to zero) values of <italic>Mean</italic> and <italic>STD</italic> for one of the image classes (for both CT scan and X-ray image datasets).<table-wrap position="float" id="tbl8"><label>Table 8</label><caption><p>Unknown image class detection by Simple CNN, Multi-headed CNN and our proposed feature fusion model when fed with the image presented in <xref rid="fig9" ref-type="fig">Fig. 9</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2" align="left">DL model</th><th colspan="3" align="left">CT scan<hr/></th><th colspan="3" align="left">X-ray<hr/></th></tr><tr><th colspan="2" align="left"/><th align="left">nCT</th><th align="left">NiCT</th><th align="left">pCT</th><th align="left">COVID-19</th><th align="left">Normal</th><th align="left">Pneumonia</th></tr></thead><tbody><tr><td align="left" rowspan="2">Deep 1 (Simple CNN)</td><td align="left">Mean</td><td align="left">0.02</td><td align="left">0.98</td><td align="left">0.0</td><td align="left">0.57</td><td align="left">0.15</td><td align="left">0.28</td></tr><tr><td align="left">STD</td><td align="left">0.10</td><td align="left">0.10</td><td align="left">0.01</td><td align="left">0.39</td><td align="left">0.26</td><td align="left">0.35</td></tr><tr><td colspan="8"><hr/></td></tr><tr><td align="left" rowspan="2">Deep 2 (Multi-headed CNN)</td><td align="left">Mean</td><td align="left">0.05</td><td align="left">0.30</td><td align="left">0.65</td><td align="left">0.68</td><td align="left">0.22</td><td align="left">0.10</td></tr><tr><td align="left">STD</td><td align="left">0.19</td><td align="left">0.43</td><td align="left">0.45</td><td align="left">0.32</td><td align="left">0.27</td><td align="left">0.16</td></tr><tr><td colspan="8"><hr/></td></tr><tr><td align="left" rowspan="2"><bold>Proposed fusion model</bold></td><td align="left">Mean</td><td align="left"><bold>0.56</bold></td><td align="left"><bold>0.0</bold></td><td align="left"><bold>0.44</bold></td><td align="left"><bold>0.41</bold></td><td align="left"><bold>0.59</bold></td><td align="left"><bold>0.0</bold></td></tr><tr><td align="left">STD</td><td align="left"><bold>0.50</bold></td><td align="left"><bold>0.07</bold></td><td align="left"><bold>0.50</bold></td><td align="left"><bold>0.49</bold></td><td align="left"><bold>0.49</bold></td><td align="left"><bold>0.0</bold></td></tr></tbody></table></table-wrap></p>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Discussion</title>
    <p id="d1e4815">Nowadays, timely and accurate detection of COVID-19 cases has become a crucial healthcare task. Various methods from different fields of science have been proposed to tackle the problem of accurate COVID-19 diagnostic. Traditional machine learning (ML) and deep learning (DL) methods have been among the most effective of them. In this work, we mainly focused on the detection of COVID-19 cases using CT scan and X-ray image data. We proposed a new simple but very efficient feature fusion model, called <inline-formula><mml:math id="d1e4818" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, and compared its performance with several classical ML and DL techniques. The prediction results we obtained confirm that our feature fusion model can be highly effective in detecting the COVID-19 cases. Moreover, we have shown the superiority of our model in dealing with noise data. The obtained results also reveal that the proposed <inline-formula><mml:math id="d1e4859" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> model can be effectively used for classifying previously unseen images.</p>
    <p id="d1e4899">Our study attempts to fill the gap reported in the literature <xref rid="b20" ref-type="bibr">[20]</xref>. To do so, we have compared the performance of the proposed feature fusion model to recent state-of-the-art machine learning techniques used to classify CT scan and X-ray image data (see <xref rid="tbl11" ref-type="table">Table 11</xref>). The Grad-CAM visualization procedure was carried out to identify the important features for each data class (this analysis was conducted for both CT scan and X-ray image datasets). <xref rid="fig12" ref-type="fig">Fig. 12</xref>, <xref rid="fig13" ref-type="fig">Fig. 13</xref> illustrate the most important features used by our feature fusion model to identify each data class separately for CT scan and X-ray image datasets, respectively. Moreover, the T-SNE visualization of different models applied to the CT scan and X-ray datasets without and with quantifying uncertainty are presented in <xref rid="fig10" ref-type="fig">Fig. 10</xref>, <xref rid="fig11" ref-type="fig">Fig. 11</xref>. Finally, the output posterior distributions of our proposed feature fusion model for both considered image datasets are presented in <xref rid="fig14" ref-type="fig">Fig. 14</xref>. This figure clearly shows that the correctly classified samples of a given class do not overlap with samples of the other classes (incorrect classes).</p>
    <sec id="sec4.1">
      <label>4.1</label>
      <title>Comparison with the state-of-the-art</title>
      <p id="d1e4927">In this sub-section, we quickly compare the results provided by our new model with those yielded by the state-of-the-art DL techniques (see <xref rid="tbl9" ref-type="table">Table 9</xref>, <xref rid="tbl10" ref-type="table">Table 10</xref>). The state-of-the-art models used in our comparison are the Bayesian Deep Learning <xref rid="b21" ref-type="bibr">[21]</xref>, DarkCovidNet <xref rid="b22" ref-type="bibr">[22]</xref>, CNN <xref rid="b23" ref-type="bibr">[23]</xref>, DeTraC (Decompose, Transfer, and Compose) <xref rid="b24" ref-type="bibr">[24]</xref>, and ResNet50 <xref rid="b25" ref-type="bibr">[25]</xref> models.</p>
      <p id="d1e4954">As can be seen from <xref rid="tbl9" ref-type="table">Table 9</xref>, <xref rid="tbl10" ref-type="table">Table 10</xref>, our proposed feature fusion model not just only achieved superior performance but also significantly outperformed the state-of-the-art models applied to the same datasets.<table-wrap position="float" id="tbl9"><label>Table 9</label><caption><p>Comparison of the results of our DL feature fusion model with the state-of-the-art DL models for CT scan data.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">DL model</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F-measure</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td align="left">Bayesian Deep Learning <xref rid="b21" ref-type="bibr">[21]</xref></td><td align="left">98.351</td><td align="left">98.333</td><td align="left">98.342</td><td align="left">98.333</td></tr><tr><td align="left">DarkCovidNet <xref rid="b22" ref-type="bibr">[22]</xref></td><td align="left">97.460</td><td align="left">97.458</td><td align="left">97.459</td><td align="left">97.458</td></tr><tr><td align="left">CNN <xref rid="b23" ref-type="bibr">[23]</xref></td><td align="left">97.753</td><td align="left">97.750</td><td align="left">97.751</td><td align="left">97.750</td></tr><tr><td align="left">DeTraC <xref rid="b24" ref-type="bibr">[24]</xref></td><td align="left">96.972</td><td align="left">96.958</td><td align="left">96.965</td><td align="left">96.958</td></tr><tr><td align="left">ResNet50 <xref rid="b25" ref-type="bibr">[25]</xref></td><td align="left">95.571</td><td align="left">95.541</td><td align="left">95.556</td><td align="left">95.541</td></tr><tr><td align="left"><bold>Proposed fusion model</bold></td><td align="left"><bold>99.085</bold></td><td align="left"><bold>99.085</bold></td><td align="left"><bold>99.085</bold></td><td align="left"><bold>99.085</bold></td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl10"><label>Table 10</label><caption><p>Comparison of the results of our DL feature fusion model with the state-of-the-art DL models for X-ray data.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">DL model</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F-measure</th><th align="left">Accuracy</th></tr></thead><tbody><tr><td align="left">Bayesian Deep Learning <xref rid="b21" ref-type="bibr">[21]</xref></td><td align="left">95.398</td><td align="left">95.419</td><td align="left">95.408</td><td align="left">95.419</td></tr><tr><td align="left">DarkCovidNet <xref rid="b22" ref-type="bibr">[22]</xref></td><td align="left">95.752</td><td align="left">95.729</td><td align="left">95.741</td><td align="left">95.729</td></tr><tr><td align="left">CNN <xref rid="b23" ref-type="bibr">[23]</xref></td><td align="left">95.400</td><td align="left">95.341</td><td align="left">95.370</td><td align="left">95.341</td></tr><tr><td align="left">DeTraC <xref rid="b24" ref-type="bibr">[24]</xref></td><td align="left">95.276</td><td align="left">95.263</td><td align="left">95.270</td><td align="left">95.263</td></tr><tr><td align="left">ResNet50 <xref rid="b25" ref-type="bibr">[25]</xref></td><td align="left">94.153</td><td align="left">94.177</td><td align="left">94.165</td><td align="left">94.177</td></tr><tr><td align="left"><bold>Proposed fusion model</bold></td><td align="left"><bold>96.350</bold></td><td align="left"><bold>96.370</bold></td><td align="left"><bold>96.360</bold></td><td align="left"><bold>96.350</bold></td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec4.2">
      <label>4.2</label>
      <title>Significance of the <inline-formula><mml:math id="d1e4970" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> feature fusion model</title>
      <p id="d1e5010">Wang et al. <xref rid="b20" ref-type="bibr">[20]</xref> proposed a DL feature fusion model for COVID-19 case detection. The model introduced by Wang et al. provided excellent prediction performance for CT scan data considered. However, the authors stated that their model may be much less efficient for other types of medical data such as X-ray images. In another study, Tang et al. <xref rid="b26" ref-type="bibr">[26]</xref> proposed an ensemble deep learning model for COVID-19 detection using X-ray image data only. Moreover, most of the existing studies focus on COVID-19 case detection without conducting any uncertainty analysis of the model’s predictions. Shamsi et al. <xref rid="b7" ref-type="bibr">[7]</xref> have been among rare authors who considered uncertainty in their study; however, they used very small datasets in their training experiments.</p>
      <p id="d1e5024">In this work, we proposed a novel general feature fusion model which can be effectively used to analyze large CT scan and X-ray datasets (both of these types of images can be processed successively), while quantifying the uncertainty of the model’s predictions using the Ensemble MC Dropout (EMCD) technique. It should be noted that the proposed feature fusion model could be easily generalized to classify other complex diseases. Moreover, the model’s performance could be further improved by incorporating into its different optimization algorithms such as the Arithmetic optimization algorithm <xref rid="b27" ref-type="bibr">[27]</xref>, Aquila optimizer <xref rid="b28" ref-type="bibr">[28]</xref>, Artificial Immune System (AIS) algorithm <xref rid="b29" ref-type="bibr">[29]</xref>, Marine Predators algorithm <xref rid="b30" ref-type="bibr">[30]</xref>, or Cuckoo search optimization algorithm <xref rid="b31" ref-type="bibr">[31]</xref>. Finally, Neural Architecture Search (NAS) is a new technique for automating the design of various deep learning models. Therefore, the architecture of the proposed <inline-formula><mml:math id="d1e5048" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> feature fusion model can be further improved using newly proposed NAS techniques <xref rid="b32" ref-type="bibr">[32]</xref>, <xref rid="b33" ref-type="bibr">[33]</xref>, <xref rid="b34" ref-type="bibr">[34]</xref>, <xref rid="b35" ref-type="bibr">[35]</xref>.</p>
      <p id="d1e5092">A comprehensive comparison of the results provided by our proposed model with the state-of-the-art techniques for automated detection of COVID-19 cases using both the CT scan and X-ray image datasets is presented in <xref rid="tbl11" ref-type="table">Table 11</xref>. Moreover, the most important features of our <inline-formula><mml:math id="d1e5099" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> feature fusion model are summarized below:</p>
      <p id="d1e5139">
        <list list-type="simple" id="d1e5141">
          <list-item id="lst13">
            <label>1.</label>
            <p id="d1e5145">Our model provided the highest COVID-19 detection performance compared to traditional machine learning models, some simple deep learning models as well as to state-of-the-art deep learning techniques for both considered types of medical data (CT scan and X-ray images).</p>
          </list-item>
          <list-item id="lst14">
            <label>2.</label>
            <p id="d1e5150">Proposed model takes advantage of an uncertainty quantification strategy based on the effective Ensemble MC Dropout (EMCD) technique.</p>
          </list-item>
          <list-item id="lst15">
            <label>3.</label>
            <p id="d1e5155">Proposed model is robust against noise.</p>
          </list-item>
          <list-item id="lst16">
            <label>4.</label>
            <p id="d1e5160">Proposed model is able to detect unknown data with high accuracy.</p>
          </list-item>
        </list>
      </p>
      <p id="d1e5162">
        <table-wrap position="anchor" id="tbl11">
          <label>Table 11</label>
          <caption>
            <p>Comprehensive comparison of the results provided by our proposed model with the state-of-the-art techniques for automated detection of COVID-19 cases using both the CT scan and X-ray image datasets.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left">Dataset</th>
                <th align="left">Study</th>
                <th align="left">Year</th>
                <th align="left"># of samples</th>
                <th colspan="5" align="left">Performance<hr/></th>
                <th align="left">UQ</th>
                <th align="left">Code</th>
              </tr>
              <tr>
                <th align="left"/>
                <th align="left"/>
                <th align="left"/>
                <th align="left"/>
                <th align="left">Precision</th>
                <th align="left">Recall</th>
                <th align="left">F-measure</th>
                <th align="left">Accuracy</th>
                <th align="left">AUC</th>
                <th align="left"/>
                <th align="left"/>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="3">CT scan</td>
                <td align="left">Li et al. <xref rid="b36" ref-type="bibr">[36]</xref></td>
                <td align="left">2020</td>
                <td align="left">1540 (3 classes)</td>
                <td align="left">N/A</td>
                <td align="left">82.60</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">0.918</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left">Jaiswal et al. <xref rid="b37" ref-type="bibr">[37]</xref></td>
                <td align="left">2020</td>
                <td align="left">2492 (2 classes)</td>
                <td align="left">96.29</td>
                <td align="left">96.29</td>
                <td align="left">96.29</td>
                <td align="left">96.25</td>
                <td align="left">0.970</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left">Wang et al. <xref rid="b38" ref-type="bibr">[38]</xref></td>
                <td align="left">2020</td>
                <td align="left">640 (2 classes)</td>
                <td align="left">96.61</td>
                <td align="left">97.71</td>
                <td align="left">97.14</td>
                <td align="left">97.15</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Sharma <xref rid="b39" ref-type="bibr">[39]</xref></td>
                <td align="left">2020</td>
                <td align="left">2200 (3 classes)</td>
                <td align="left">N/A</td>
                <td align="left">92.10</td>
                <td align="left">N/A</td>
                <td align="left">91.00</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Panwar et al. <xref rid="b40" ref-type="bibr">[40]</xref></td>
                <td align="left">2020</td>
                <td align="left">1600 (2 classes)</td>
                <td align="left">95.00</td>
                <td align="left">95.00</td>
                <td align="left">95.00</td>
                <td align="left">95.00</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Do and Vu <xref rid="b41" ref-type="bibr">[41]</xref></td>
                <td align="left">2020</td>
                <td align="left">746 (2 classes)</td>
                <td align="left">85.00</td>
                <td align="left">85.00</td>
                <td align="left">85.00</td>
                <td align="left">85.00</td>
                <td align="left">0.922</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Singh <xref rid="b42" ref-type="bibr">[42]</xref></td>
                <td align="left">2020</td>
                <td align="left">N/A (2 classes)</td>
                <td align="left">N/A</td>
                <td align="left">91.00</td>
                <td align="left">89.97</td>
                <td align="left">93.50</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Pham <xref rid="b43" ref-type="bibr">[43]</xref></td>
                <td align="left">2020</td>
                <td align="left">746 (2 classes)</td>
                <td align="left">N/A</td>
                <td align="left">91.14</td>
                <td align="left">93.00</td>
                <td align="left">92.62</td>
                <td align="left">0.980</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Martinez <xref rid="b44" ref-type="bibr">[44]</xref></td>
                <td align="left">2020</td>
                <td align="left">746 (2 classes)</td>
                <td align="left">94.40</td>
                <td align="left">86.60</td>
                <td align="left">90.30</td>
                <td align="left">90.40</td>
                <td align="left">0.965</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Loey et al. <xref rid="b45" ref-type="bibr">[45]</xref></td>
                <td align="left">2020</td>
                <td align="left">11 012 (2 classes)</td>
                <td align="left">N/A</td>
                <td align="left">80.85</td>
                <td align="left">N/A</td>
                <td align="left">81.41</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Ning et al. <xref rid="b18" ref-type="bibr">[18]</xref></td>
                <td align="left">2020</td>
                <td align="left">19 685 (3 classes)</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">0.978</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Han et al. <xref rid="b46" ref-type="bibr">[46]</xref></td>
                <td align="left">2020</td>
                <td align="left">460 (3 classes)</td>
                <td align="left">95.90</td>
                <td align="left">90.50</td>
                <td align="left">92.30</td>
                <td align="left">94.30</td>
                <td align="left">0.988</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Shamsi Jokandan et al. <xref rid="b7" ref-type="bibr">[7]</xref></td>
                <td align="left">2021</td>
                <td align="left">746 (2 classes)</td>
                <td align="left">N/A</td>
                <td align="left">86.50</td>
                <td align="left">N/A</td>
                <td align="left">87.90</td>
                <td align="left">0.942</td>
                <td align="left">✓</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Benmalek et al. <xref rid="b47" ref-type="bibr">[47]</xref></td>
                <td align="left">2021</td>
                <td align="left">19 685 (3 classes)</td>
                <td align="left">98.50</td>
                <td align="left">98.60</td>
                <td align="left">98.50</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">✓</td>
                <td align="left">✓</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Kumar et al. <xref rid="b48" ref-type="bibr">[48]</xref></td>
                <td align="left">2022</td>
                <td align="left">2926 (2 classes)</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">98.87</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">✓</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Masood et al. <xref rid="b49" ref-type="bibr">[49]</xref></td>
                <td align="left">2022</td>
                <td align="left">19 685 (2 classes)</td>
                <td align="left">99.75</td>
                <td align="left">99.70</td>
                <td align="left">99.72</td>
                <td align="left">99.75</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">
                  <bold>Ours</bold>
                </td>
                <td align="left">
                  <bold>2022</bold>
                </td>
                <td align="left">
                  <bold>19 685 (3 classes)</bold>
                </td>
                <td align="left">
                  <bold>99.08</bold>
                </td>
                <td align="left">
                  <bold>99.08</bold>
                </td>
                <td align="left">
                  <bold>99.08</bold>
                </td>
                <td align="left">
                  <bold>99.08</bold>
                </td>
                <td align="left">
                  <bold>1.00</bold>
                </td>
                <td align="left">✓</td>
                <td align="left">✓</td>
              </tr>
              <tr>
                <td colspan="11">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td align="left" rowspan="3">X-ray</td>
                <td align="left">Khan et al. <xref rid="b50" ref-type="bibr">[50]</xref></td>
                <td align="left">2020</td>
                <td align="left">1251 (4 classes)</td>
                <td align="left">90.00</td>
                <td align="left">89.92</td>
                <td align="left">89.80</td>
                <td align="left">89.60</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">✓</td>
              </tr>
              <tr>
                <td align="left">Ozturk et al. <xref rid="b22" ref-type="bibr">[22]</xref></td>
                <td align="left">2020</td>
                <td align="left">1125 (3 classes)</td>
                <td align="left">89.96</td>
                <td align="left">85.35</td>
                <td align="left">87.37</td>
                <td align="left">87.02</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">✓</td>
              </tr>
              <tr>
                <td align="left">Mesut and <xref rid="b51" ref-type="bibr">[51]</xref></td>
                <td align="left">2020</td>
                <td align="left">458 (3 classes)</td>
                <td align="left">98.89</td>
                <td align="left">98.33</td>
                <td align="left">98.57</td>
                <td align="left">99.27</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">✓</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Mahmud et al. <xref rid="b52" ref-type="bibr">[52]</xref></td>
                <td align="left">2020</td>
                <td align="left">1220 (4 classes)</td>
                <td align="left">82.87</td>
                <td align="left">83.82</td>
                <td align="left">83.37</td>
                <td align="left">90.30</td>
                <td align="left">0.825</td>
                <td align="left">×</td>
                <td align="left">✓</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Heidari et al. <xref rid="b53" ref-type="bibr">[53]</xref></td>
                <td align="left">2020</td>
                <td align="left">2544 (3 classes)</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">94.50</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Rahimzadeh and Attar <xref rid="b54" ref-type="bibr">[54]</xref></td>
                <td align="left">2020</td>
                <td align="left">11 302 (3 classes)</td>
                <td align="left">72.83</td>
                <td align="left">87.31</td>
                <td align="left">N/A</td>
                <td align="left">91.40</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">✓</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Pereira et al. <xref rid="b55" ref-type="bibr">[55]</xref></td>
                <td align="left">2020</td>
                <td align="left">1144 (7 classes)</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">64.91</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">De Moura et al. <xref rid="b56" ref-type="bibr">[56]</xref></td>
                <td align="left">2020</td>
                <td align="left">1616 (3 classes)</td>
                <td align="left">79.00</td>
                <td align="left">79.33</td>
                <td align="left">79.33</td>
                <td align="left">79.86</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Yoo et al. <xref rid="b57" ref-type="bibr">[57]</xref></td>
                <td align="left">2020</td>
                <td align="left">1170 (2 classes)</td>
                <td align="left">97.00</td>
                <td align="left">99.00</td>
                <td align="left">97.98</td>
                <td align="left">98.00</td>
                <td align="left">0.980</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Chandra et al. <xref rid="b58" ref-type="bibr">[58]</xref></td>
                <td align="left">2020</td>
                <td align="left">2346 (2 classes)</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">91.32</td>
                <td align="left">0.914</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Zhang et al. <xref rid="b59" ref-type="bibr">[59]</xref></td>
                <td align="left">2020</td>
                <td align="left">2706 (2 classes)</td>
                <td align="left">77.13</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">78.57</td>
                <td align="left">0.844</td>
                <td align="left">×</td>
                <td align="left">✓</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Shamsi Jokandan et al. <xref rid="b7" ref-type="bibr">[7]</xref></td>
                <td align="left">2021</td>
                <td align="left">100 (2 classes)</td>
                <td align="left">N/A</td>
                <td align="left">99.90</td>
                <td align="left">N/A</td>
                <td align="left">98.60</td>
                <td align="left">0.997</td>
                <td align="left">✓</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Ahmad et al. <xref rid="b60" ref-type="bibr">[60]</xref></td>
                <td align="left">2021</td>
                <td align="left">4200 (4 classes)</td>
                <td align="left">93.01</td>
                <td align="left">92.97</td>
                <td align="left">92.97</td>
                <td align="left">96.49</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Patel <xref rid="b61" ref-type="bibr">[61]</xref></td>
                <td align="left">2021</td>
                <td align="left">6432 (3 classes)</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">93.67</td>
                <td align="left">N/A</td>
                <td align="left">✓</td>
                <td align="left">✓</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Basu et al. <xref rid="b62" ref-type="bibr">[62]</xref></td>
                <td align="left">2022</td>
                <td align="left">2926 (2 classes)</td>
                <td align="left">N/A</td>
                <td align="left">92.90</td>
                <td align="left">N/A</td>
                <td align="left">97.60</td>
                <td align="left">N/A</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Masud <xref rid="b63" ref-type="bibr">[63]</xref></td>
                <td align="left">2022</td>
                <td align="left">6432 (2 classes)</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">N/A</td>
                <td align="left">92.70</td>
                <td align="left">0.964</td>
                <td align="left">×</td>
                <td align="left">×</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">
                  <bold>Ours</bold>
                </td>
                <td align="left">
                  <bold>2022</bold>
                </td>
                <td align="left">
                  <bold>6432 (3 classes)</bold>
                </td>
                <td align="left">
                  <bold>96.35</bold>
                </td>
                <td align="left">
                  <bold>96.37</bold>
                </td>
                <td align="left">
                  <bold>96.36</bold>
                </td>
                <td align="left">
                  <bold>96.35</bold>
                </td>
                <td align="left">
                  <bold>0.993</bold>
                </td>
                <td align="left">✓</td>
                <td align="left">✓</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
    </sec>
  </sec>
  <sec id="sec5">
    <label>5</label>
    <title>Conclusion</title>
    <p id="d1e5170">In this study, we have described a new deep learning feature fusion model to accurately detect COVID-19 cases using CT scan and X-ray data. In order to detect the COVID-19 cases accurately and provide health practitioners with an efficient diagnostic tool they could rely on, we carried out the uncertainty quantification of the model’s predictions while detecting the disease cases. Moreover, our <inline-formula><mml:math id="d1e5173" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> model demonstrated an excellent robustness to noise and ability to process unknown data. A class-wise analysis procedure has been implemented to ensure a steady performance of the model. We have demonstrated the effectiveness of our model using various computational experiments. Our experimental results suggest that the presented feature fusion model can be applied to analyze efficiently both CT and X-ray data. The use of hierarchical features in the model’s architecture helped <inline-formula><mml:math id="d1e5214" altimg="si23.svg" display="inline"><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> to outperform the considered traditional machine learning models, classical deep learning models, and state-of-the-art deep learning models. The limitations of the proposed feature fusion model will be addressed in our future studies. Thus, in the future, we intend to: (i) expand the considered COVID-19 datasets and test our feature fusion model using multi-modal data, (ii) include an attention mechanism while merging features, and (iii) integrate into our model some modern data fusion techniques such as decision level fusion.</p>
  </sec>
  <sec id="d1e5254">
    <title>CRediT authorship contribution statement</title>
    <p id="d1e5257"><bold>Moloud Abdar:</bold> Conception of the project, Design of methodology, Data selection and collection, Analysis and experimental protocol, Writing – original draft, Result discussion, Writing – review &amp; editing. <bold>Soorena Salari:</bold> Analysis and experimental protocol, Drafting the initial draft, Experimental protocol, Result discussion, Writing – review &amp; editing. <bold>Sina Qahremani:</bold> Analysis and experimental protocol, Drafting the initial draft, Experimental protocol, Result discussion, Writing – review &amp; editing. <bold>Hak-Keung Lam:</bold> Writing – review &amp; editing, Supervision. <bold>Fakhri Karray:</bold> Writing – review &amp; editing, Supervision. <bold>Sadiq Hussain:</bold> Drafting the initial draft. <bold>Abbas Khosravi:</bold> Writing – review &amp; editing, Supervision. <bold>U. Rajendra Acharya:</bold> Writing – review &amp; editing, Supervision. <bold>Vladimir Makarenkov:</bold> Writing – review &amp; editing, Supervision. <bold>Saeid Nahavandi:</bold> Supervision.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="d1e5292">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="bib1">
    <title>References</title>
    <ref id="b1">
      <label>1</label>
      <element-citation publication-type="book" id="sb1">
        <person-group person-group-type="author">
          <name>
            <surname>Narin</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kaya</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Pamuk</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <part-title>Automatic detection of coronavirus disease (covid-19) using x-ray images and deep convolutional neural networks</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2003.10849" id="interref4">arXiv:2003.10849</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b2">
      <label>2</label>
      <element-citation publication-type="journal" id="sb2">
        <person-group person-group-type="author">
          <name>
            <surname>Makarenkov</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Mazoure</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Rabusseau</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Legendre</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Horizontal gene transfer and recombination analysis of SARS-CoV-2 genes helps discover its close relatives and shed light on its origin</article-title>
        <source>BMC Ecol. Evol.</source>
        <volume>21:5</volume>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1186/s12862-020-01732-2</pub-id>
      </element-citation>
    </ref>
    <ref id="b3">
      <label>3</label>
      <element-citation publication-type="journal" id="sb3">
        <person-group person-group-type="author">
          <name>
            <surname>Domingo</surname>
            <given-names>J.L.</given-names>
          </name>
        </person-group>
        <article-title>What we know and what we need to know about the origin of SARS-CoV-2</article-title>
        <source>Environ. Res.</source>
        <volume>200</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">111785</object-id>
        <pub-id pub-id-type="doi">10.1016/j.envres.2021.111785</pub-id>
      </element-citation>
    </ref>
    <ref id="b4">
      <label>4</label>
      <element-citation publication-type="journal" id="sb4">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Pourpanah</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Recent advances in deep learning</article-title>
        <source>Int. J. Mach. Learn. Cybern.</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="b5">
      <label>5</label>
      <element-citation publication-type="book" id="sb5">
        <person-group person-group-type="author">
          <name>
            <surname>Pourpanah</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Abdar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>C.P.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X.-Z.</given-names>
          </name>
        </person-group>
        <part-title>A review of generalized zero-shot learning methods</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2011.08641" id="interref5">arXiv:2011.08641</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b6">
      <label>6</label>
      <element-citation publication-type="journal" id="sb6">
        <person-group person-group-type="author">
          <name>
            <surname>Luo</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Pourpanah</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Dual VAEGAN: A generative model for generalized zero-shot learning</article-title>
        <source>Appl. Soft Comput.</source>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">107352</object-id>
      </element-citation>
    </ref>
    <ref id="b7">
      <label>7</label>
      <element-citation publication-type="journal" id="sb7">
        <person-group person-group-type="author">
          <name>
            <surname>Shamsi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Asgharnezhad</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Jokandan</surname>
            <given-names>S.S.</given-names>
          </name>
          <name>
            <surname>Khosravi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kebria</surname>
            <given-names>P.M.</given-names>
          </name>
          <name>
            <surname>Nahavandi</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Nahavandi</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Srinivasan</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>An uncertainty-aware transfer learning-based framework for COVID-19 diagnosis</article-title>
        <source>IEEE Trans. Neural Netw. Learn. Syst.</source>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="b8">
      <label>8</label>
      <element-citation publication-type="book" id="sb8">
        <person-group person-group-type="author">
          <name>
            <surname>Farooq</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hafeez</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Covid-resnet: A deep learning framework for screening of covid19 from radiographs</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2003.14395" id="interref6">arXiv:2003.14395</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b9">
      <label>9</label>
      <element-citation publication-type="book" id="sb9">
        <person-group person-group-type="author">
          <name>
            <surname>Zheng</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <part-title>Deep learning-based detection for COVID-19 from chest CT using weak label</part-title>
        <year>2020</year>
        <comment>MedRxiv</comment>
      </element-citation>
    </ref>
    <ref id="b10">
      <label>10</label>
      <element-citation publication-type="book" id="sb10">
        <person-group person-group-type="author">
          <name>
            <surname>Hall</surname>
            <given-names>L.O.</given-names>
          </name>
          <name>
            <surname>Paul</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Goldgof</surname>
            <given-names>D.B.</given-names>
          </name>
          <name>
            <surname>Goldgof</surname>
            <given-names>G.M.</given-names>
          </name>
        </person-group>
        <part-title>Finding covid-19 from chest x-rays using deep learning on a small dataset</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2004.02060" id="interref7">arXiv:2004.02060</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b11">
      <label>11</label>
      <element-citation publication-type="book" id="sb11">
        <person-group person-group-type="author">
          <name>
            <surname>Abbas</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Abdelsamea</surname>
            <given-names>M.M.</given-names>
          </name>
          <name>
            <surname>Gaber</surname>
            <given-names>M.M.</given-names>
          </name>
        </person-group>
        <part-title>Classification of COVID-19 in chest X-ray images using DeTraC deep convolutional neural network</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2003.13815" id="interref8">arXiv:2003.13815</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b12">
      <label>12</label>
      <element-citation publication-type="book" id="sb12">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <part-title>Residual attention U-net for automated multi-class segmentation of COVID-19 chest CT images</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2004.05645" id="interref9">arXiv:2004.05645</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b13">
      <label>13</label>
      <element-citation publication-type="journal" id="sb13">
        <person-group person-group-type="author">
          <name>
            <surname>Ali</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>El-Sappagh</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Islam</surname>
            <given-names>S.R.</given-names>
          </name>
          <name>
            <surname>Kwak</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Ali</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Imran</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kwak</surname>
            <given-names>K.-S.</given-names>
          </name>
        </person-group>
        <article-title>A smart healthcare monitoring system for heart disease prediction based on ensemble deep learning and feature fusion</article-title>
        <source>Inf. Fusion</source>
        <volume>63</volume>
        <year>2020</year>
        <fpage>208</fpage>
        <lpage>222</lpage>
      </element-citation>
    </ref>
    <ref id="b14">
      <label>14</label>
      <element-citation publication-type="journal" id="sb14">
        <person-group person-group-type="author">
          <name>
            <surname>Begoli</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Bhattacharya</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Kusnezov</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>The need for uncertainty quantification in machine-assisted medical decision making</article-title>
        <source>Nat. Mach. Intell.</source>
        <volume>1</volume>
        <issue>1</issue>
        <year>2019</year>
        <fpage>20</fpage>
        <lpage>23</lpage>
      </element-citation>
    </ref>
    <ref id="b15">
      <label>15</label>
      <element-citation publication-type="journal" id="sb15">
        <person-group person-group-type="author">
          <name>
            <surname>Abdar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Pourpanah</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Hussain</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Rezazadegan</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Ghavamzadeh</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Fieguth</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Khosravi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Acharya</surname>
            <given-names>U.R.</given-names>
          </name>
          <name>
            <surname>Makarenkov</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Nahavandi</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>A review of uncertainty quantification in deep learning: Techniques, applications and challenges</article-title>
        <source>Inf. Fusion</source>
        <volume>76</volume>
        <year>2021</year>
        <fpage>243</fpage>
        <lpage>297</lpage>
      </element-citation>
    </ref>
    <ref id="b16">
      <label>16</label>
      <element-citation publication-type="journal" id="sb16">
        <person-group person-group-type="author">
          <name>
            <surname>Abdar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Samami</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Mahmoodabad</surname>
            <given-names>S.D.</given-names>
          </name>
          <name>
            <surname>Doan</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Mazoure</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Hashemifesharaki</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Khosravi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Acharya</surname>
            <given-names>U.R.</given-names>
          </name>
          <name>
            <surname>Makarenkov</surname>
            <given-names>V.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Uncertainty quantification in skin cancer classification using three-way decision-based Bayesian deep learning</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">104418</object-id>
      </element-citation>
    </ref>
    <ref id="b17">
      <label>17</label>
      <element-citation publication-type="journal" id="sb17">
        <person-group person-group-type="author">
          <name>
            <surname>Abdar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Khosravi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Islam</surname>
            <given-names>S.M.S.</given-names>
          </name>
          <name>
            <surname>Acharya</surname>
            <given-names>U.R.</given-names>
          </name>
          <name>
            <surname>Vasilakos</surname>
            <given-names>A.V.</given-names>
          </name>
        </person-group>
        <article-title>The need for quantification of uncertainty in artificial intelligence for clinical data analysis: increasing the level of trust in the decision-making process</article-title>
        <source>IEEE Syst. Man Cybern. Mag.</source>
        <volume>8</volume>
        <issue>3</issue>
        <year>2022</year>
        <fpage>28</fpage>
        <lpage>40</lpage>
      </element-citation>
    </ref>
    <ref id="b18">
      <label>18</label>
      <element-citation publication-type="book" id="sb18">
        <person-group person-group-type="author">
          <name>
            <surname>Ning</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Lei</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Geng</surname>
            <given-names>Z.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>iCTCF: an integrative resource of chest computed tomography images and clinical features of patients with COVID-19 pneumonia</part-title>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="b19">
      <label>19</label>
      <element-citation publication-type="book" id="sb19">
        <person-group person-group-type="author">
          <name>
            <surname>Rusak</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Schott</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zimmermann</surname>
            <given-names>R.S.</given-names>
          </name>
          <name>
            <surname>Bitterwolf</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Bringmann</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Bethge</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Brendel</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <part-title>A simple way to make neural networks robust against diverse image corruptions</part-title>
        <source>European Conference on Computer Vision</source>
        <year>2020</year>
        <publisher-name>Springer</publisher-name>
        <fpage>53</fpage>
        <lpage>69</lpage>
      </element-citation>
    </ref>
    <ref id="b20">
      <label>20</label>
      <element-citation publication-type="journal" id="sb20">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S.-H.</given-names>
          </name>
          <name>
            <surname>Govindaraj</surname>
            <given-names>V.V.</given-names>
          </name>
          <name>
            <surname>Górriz</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.-D.</given-names>
          </name>
        </person-group>
        <article-title>Covid-19 classification by FGCNet with deep feature fusion from graph convolutional network and convolutional neural network</article-title>
        <source>Inf. Fusion</source>
        <volume>67</volume>
        <year>2021</year>
        <fpage>208</fpage>
        <lpage>229</lpage>
        <pub-id pub-id-type="pmid">33052196</pub-id>
      </element-citation>
    </ref>
    <ref id="b21">
      <label>21</label>
      <element-citation publication-type="book" id="sb21">
        <person-group person-group-type="author">
          <name>
            <surname>Ghoshal</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Tucker</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Estimating uncertainty and interpretability in deep learning for coronavirus (COVID-19) detection</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2003.10769" id="interref10">arXiv:2003.10769</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b22">
      <label>22</label>
      <element-citation publication-type="journal" id="sb22">
        <person-group person-group-type="author">
          <name>
            <surname>Ozturk</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Talo</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yildirim</surname>
            <given-names>E.A.</given-names>
          </name>
          <name>
            <surname>Baloglu</surname>
            <given-names>U.B.</given-names>
          </name>
          <name>
            <surname>Yildirim</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Acharya</surname>
            <given-names>U.R.</given-names>
          </name>
        </person-group>
        <article-title>Automated detection of COVID-19 cases using deep neural networks with X-ray images</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">103792</object-id>
      </element-citation>
    </ref>
    <ref id="b23">
      <label>23</label>
      <element-citation publication-type="book" id="sb23">
        <person-group person-group-type="author">
          <name>
            <surname>Jadon</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>COVID-19 detection from scarce chest x-ray image data using few-shot deep learning approach</part-title>
        <source>Medical Imaging 2021: Imaging Informatics for Healthcare, Research, and Applications, Vol. 11601</source>
        <year>2021</year>
        <publisher-name>International Society for Optics and Photonics</publisher-name>
        <object-id pub-id-type="publisher-id">116010X</object-id>
      </element-citation>
    </ref>
    <ref id="b24">
      <label>24</label>
      <element-citation publication-type="journal" id="sb24">
        <person-group person-group-type="author">
          <name>
            <surname>Abbas</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Abdelsamea</surname>
            <given-names>M.M.</given-names>
          </name>
          <name>
            <surname>Gaber</surname>
            <given-names>M.M.</given-names>
          </name>
        </person-group>
        <article-title>Classification of COVID-19 in chest X-ray images using DeTraC deep convolutional neural network</article-title>
        <source>Appl. Intell.</source>
        <volume>51</volume>
        <issue>2</issue>
        <year>2021</year>
        <fpage>854</fpage>
        <lpage>864</lpage>
      </element-citation>
    </ref>
    <ref id="b25">
      <label>25</label>
      <element-citation publication-type="journal" id="sb25">
        <person-group person-group-type="author">
          <name>
            <surname>Narin</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kaya</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Pamuk</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Automatic detection of coronavirus disease (covid-19) using x-ray images and deep convolutional neural networks</article-title>
        <source>Pattern Anal. Appl.</source>
        <year>2021</year>
        <fpage>1</fpage>
        <lpage>14</lpage>
      </element-citation>
    </ref>
    <ref id="b26">
      <label>26</label>
      <element-citation publication-type="journal" id="sb26">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Nie</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xiong</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Barnawi</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>EDL-COVID: Ensemble deep learning for COVID-19 cases detection from chest X-Ray images</article-title>
        <source>IEEE Trans. Ind. Inf.</source>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="b27">
      <label>27</label>
      <element-citation publication-type="journal" id="sb27">
        <person-group person-group-type="author">
          <name>
            <surname>Abualigah</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Diabat</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mirjalili</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Abd Elaziz</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Gandomi</surname>
            <given-names>A.H.</given-names>
          </name>
        </person-group>
        <article-title>The arithmetic optimization algorithm</article-title>
        <source>Comput. Methods Appl. Mech. Engrg.</source>
        <volume>376</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">113609</object-id>
      </element-citation>
    </ref>
    <ref id="b28">
      <label>28</label>
      <element-citation publication-type="journal" id="sb28">
        <person-group person-group-type="author">
          <name>
            <surname>Abualigah</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Yousri</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Abd Elaziz</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ewees</surname>
            <given-names>A.A.</given-names>
          </name>
          <name>
            <surname>Al-qaness</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Gandomi</surname>
            <given-names>A.H.</given-names>
          </name>
        </person-group>
        <article-title>Aquila optimizer: A novel meta-heuristic optimization algorithm</article-title>
        <source>Comput. Ind. Eng.</source>
        <volume>157</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">107250</object-id>
      </element-citation>
    </ref>
    <ref id="b29">
      <label>29</label>
      <element-citation publication-type="journal" id="sb29">
        <person-group person-group-type="author">
          <name>
            <surname>Abdar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Wijayaningrum</surname>
            <given-names>V.N.</given-names>
          </name>
          <name>
            <surname>Hussain</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Alizadehsani</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Plawiak</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Acharya</surname>
            <given-names>U.R.</given-names>
          </name>
          <name>
            <surname>Makarenkov</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>IAPSO-AIRS: A novel improved machine learning-based system for wart disease treatment</article-title>
        <source>J. Med. Syst.</source>
        <volume>43</volume>
        <issue>220</issue>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="b30">
      <label>30</label>
      <element-citation publication-type="journal" id="sb30">
        <person-group person-group-type="author">
          <name>
            <surname>Sahlol</surname>
            <given-names>A.T.</given-names>
          </name>
          <name>
            <surname>Yousri</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Ewees</surname>
            <given-names>A.A.</given-names>
          </name>
          <name>
            <surname>Al-Qaness</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Damasevicius</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Abd Elaziz</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>COVID-19 image classification using deep features and fractional-order marine predators algorithm</article-title>
        <source>Sci. Rep.</source>
        <volume>10</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="pmid">31913322</pub-id>
      </element-citation>
    </ref>
    <ref id="b31">
      <label>31</label>
      <element-citation publication-type="journal" id="sb31">
        <person-group person-group-type="author">
          <name>
            <surname>Yousri</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Abd Elaziz</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Abualigah</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Oliva</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Al-Qaness</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Ewees</surname>
            <given-names>A.A.</given-names>
          </name>
        </person-group>
        <article-title>COVID-19 X-ray images classification based on enhanced fractional-order cuckoo search optimizer using heavy-tailed distributions</article-title>
        <source>Appl. Soft Comput.</source>
        <volume>101</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">107052</object-id>
      </element-citation>
    </ref>
    <ref id="b32">
      <label>32</label>
      <element-citation publication-type="book" id="sb32">
        <person-group person-group-type="author">
          <name>
            <surname>Pham</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zoph</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Efficient neural architecture search via parameters sharing</part-title>
        <source>International Conference on Machine Learning</source>
        <year>2018</year>
        <publisher-name>PMLR</publisher-name>
        <fpage>4095</fpage>
        <lpage>4104</lpage>
      </element-citation>
    </ref>
    <ref id="b33">
      <label>33</label>
      <element-citation publication-type="journal" id="sb33">
        <person-group person-group-type="author">
          <name>
            <surname>Elsken</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Metzen</surname>
            <given-names>J.H.</given-names>
          </name>
          <name>
            <surname>Hutter</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Neural architecture search: A survey</article-title>
        <source>J. Mach. Learn. Res.</source>
        <volume>20</volume>
        <issue>1</issue>
        <year>2019</year>
        <fpage>1997</fpage>
        <lpage>2017</lpage>
      </element-citation>
    </ref>
    <ref id="b34">
      <label>34</label>
      <element-citation publication-type="journal" id="sb34">
        <person-group person-group-type="author">
          <name>
            <surname>Xue</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Slowik</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>A self-adaptive mutation neural architecture search algorithm based on blocks</article-title>
        <source>IEEE Comput. Intell. Mag.</source>
        <volume>16</volume>
        <issue>3</issue>
        <year>2021</year>
        <fpage>67</fpage>
        <lpage>78</lpage>
      </element-citation>
    </ref>
    <ref id="b35">
      <label>35</label>
      <element-citation publication-type="journal" id="sb35">
        <person-group person-group-type="author">
          <name>
            <surname>Xue</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Qin</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Partial connection based on channel attention for differentiable neural architecture search</article-title>
        <source>IEEE Trans. Ind. Inf.</source>
        <year>2022</year>
      </element-citation>
    </ref>
    <ref id="b36">
      <label>36</label>
      <element-citation publication-type="journal" id="sb36">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Qin</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Zhong</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>CT image visual quantitative evaluation and clinical classification of coronavirus disease (COVID-19)</article-title>
        <source>Eur. Radiol.</source>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="b37">
      <label>37</label>
      <element-citation publication-type="journal" id="sb37">
        <person-group person-group-type="author">
          <name>
            <surname>Jaiswal</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gianchandani</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Kaur</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Classification of the COVID-19 infected patients using DenseNet201 based deep transfer learning</article-title>
        <source>J. Biomol. Struct. Dyn.</source>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="b38">
      <label>38</label>
      <element-citation publication-type="journal" id="sb38">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S.-H.</given-names>
          </name>
          <name>
            <surname>Govindaraj</surname>
            <given-names>V.V.</given-names>
          </name>
          <name>
            <surname>Górriz</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.-D.</given-names>
          </name>
        </person-group>
        <article-title>Covid-19 classification by FGCNet with deep feature fusion from graph convolutional network and convolutional neural network</article-title>
        <source>Inf. Fusion</source>
        <volume>67</volume>
        <year>2020</year>
        <fpage>208</fpage>
        <lpage>229</lpage>
        <pub-id pub-id-type="pmid">33052196</pub-id>
      </element-citation>
    </ref>
    <ref id="b39">
      <label>39</label>
      <element-citation publication-type="journal" id="sb39">
        <person-group person-group-type="author">
          <name>
            <surname>Sharma</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Drawing insights from COVID-19-infected patients using CT scan images and machine learning techniques: a study on 200 patients</article-title>
        <source>Environ. Sci. Pollut. Res.</source>
        <volume>27</volume>
        <issue>29</issue>
        <year>2020</year>
        <fpage>37155</fpage>
        <lpage>37163</lpage>
      </element-citation>
    </ref>
    <ref id="b40">
      <label>40</label>
      <element-citation publication-type="journal" id="sb40">
        <person-group person-group-type="author">
          <name>
            <surname>Panwar</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Gupta</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Siddiqui</surname>
            <given-names>M.K.</given-names>
          </name>
          <name>
            <surname>Morales-Menendez</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Bhardwaj</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>A deep learning and grad-CAM based color visualization approach for fast detection of COVID-19 cases using chest X-ray and CT-scan images</article-title>
        <source>Chaos Solitons Fractals</source>
        <volume>140</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">110190</object-id>
      </element-citation>
    </ref>
    <ref id="b41">
      <label>41</label>
      <element-citation publication-type="book" id="sb41">
        <person-group person-group-type="author">
          <name>
            <surname>Do</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Vu</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <part-title>An approach for recognizing COVID-19 cases using convolutional neural networks applied to CT scan images</part-title>
        <source>Applications of Digital Image Processing XLIII, Vol. 11510</source>
        <year>2020</year>
        <publisher-name>International Society for Optics and Photonics</publisher-name>
        <object-id pub-id-type="publisher-id">1151034</object-id>
      </element-citation>
    </ref>
    <ref id="b42">
      <label>42</label>
      <element-citation publication-type="journal" id="sb42">
        <person-group person-group-type="author">
          <name>
            <surname>Singh</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Kaur</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Classification of COVID-19 patients from chest CT images using multi-objective differential evolution–based convolutional neural networks</article-title>
        <source>Eur. J. Clin. Microbiol. Infect. Dis.</source>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>11</lpage>
      </element-citation>
    </ref>
    <ref id="b43">
      <label>43</label>
      <element-citation publication-type="journal" id="sb43">
        <person-group person-group-type="author">
          <name>
            <surname>Pham</surname>
            <given-names>T.D.</given-names>
          </name>
        </person-group>
        <article-title>A comprehensive study on classification of COVID-19 on computed tomography with pretrained convolutional neural networks</article-title>
        <source>Sci. Rep.</source>
        <volume>10</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="pmid">31913322</pub-id>
      </element-citation>
    </ref>
    <ref id="b44">
      <label>44</label>
      <element-citation publication-type="book" id="sb44">
        <person-group person-group-type="author">
          <name>
            <surname>Martinez</surname>
            <given-names>A.R.</given-names>
          </name>
        </person-group>
        <part-title>Classification of covid-19 in ct scans using multi-source transfer learning</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2009.10474" id="interref11">arXiv:2009.10474</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b45">
      <label>45</label>
      <element-citation publication-type="journal" id="sb45">
        <person-group person-group-type="author">
          <name>
            <surname>Loey</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Manogaran</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Khalifa</surname>
            <given-names>N.E.M.</given-names>
          </name>
        </person-group>
        <article-title>A deep transfer learning model with classical data augmentation and cgan to detect covid-19 from chest ct radiography digital images</article-title>
        <source>Neural Comput. Appl.</source>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>13</lpage>
      </element-citation>
    </ref>
    <ref id="b46">
      <label>46</label>
      <element-citation publication-type="journal" id="sb46">
        <person-group person-group-type="author">
          <name>
            <surname>Han</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Cong</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>Accurate screening of COVID-19 using attention-based deep 3D multiple instance learning</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <volume>39</volume>
        <issue>8</issue>
        <year>2020</year>
        <fpage>2584</fpage>
        <lpage>2594</lpage>
        <pub-id pub-id-type="pmid">32730211</pub-id>
      </element-citation>
    </ref>
    <ref id="b47">
      <label>47</label>
      <element-citation publication-type="journal" id="sb47">
        <person-group person-group-type="author">
          <name>
            <surname>Benmalek</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Elmhamdi</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jilbab</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Comparing CT scan and chest X-ray imaging for COVID-19 diagnosis</article-title>
        <source>Biomed. Eng. Adv.</source>
        <volume>1</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">100003</object-id>
      </element-citation>
    </ref>
    <ref id="b48">
      <label>48</label>
      <element-citation publication-type="journal" id="sb48">
        <person-group person-group-type="author">
          <name>
            <surname>Kumar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Tripathi</surname>
            <given-names>A.R.</given-names>
          </name>
          <name>
            <surname>Satapathy</surname>
            <given-names>S.C.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.-D.</given-names>
          </name>
        </person-group>
        <article-title>SARS-net: COVID-19 detection from chest x-rays by combining graph convolutional network and convolutional neural network</article-title>
        <source>Pattern Recognit.</source>
        <volume>122</volume>
        <year>2022</year>
        <object-id pub-id-type="publisher-id">108255</object-id>
      </element-citation>
    </ref>
    <ref id="b49">
      <label>49</label>
      <element-citation publication-type="book" id="sb49">
        <person-group person-group-type="author">
          <name>
            <surname>Masood</surname>
            <given-names>M.Z.</given-names>
          </name>
          <name>
            <surname>Jamil</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hameed</surname>
            <given-names>A.A.</given-names>
          </name>
        </person-group>
        <part-title>Efficient artificial intelligence-based models for COVID-19 disease detection and diagnosis from CT-scans</part-title>
        <source>2022 2nd International Conference on Computing and Machine Intelligence (ICMI)</source>
        <year>2022</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>6</lpage>
      </element-citation>
    </ref>
    <ref id="b50">
      <label>50</label>
      <element-citation publication-type="journal" id="sb50">
        <person-group person-group-type="author">
          <name>
            <surname>Khan</surname>
            <given-names>A.I.</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>J.L.</given-names>
          </name>
          <name>
            <surname>Bhat</surname>
            <given-names>M.M.</given-names>
          </name>
        </person-group>
        <article-title>Coronet: A deep neural network for detection and diagnosis of COVID-19 from chest x-ray images</article-title>
        <source>Comput. Methods Programs Biomed.</source>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">105581</object-id>
      </element-citation>
    </ref>
    <ref id="b51">
      <label>51</label>
      <element-citation publication-type="journal" id="sb51">
        <person-group person-group-type="author">
          <name>
            <surname>Toğaçar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ergen</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Cömert</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>COVID-19 detection using deep learning models to exploit social mimic optimization and structured chest X-ray images using fuzzy color and stacking approaches</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">103805</object-id>
      </element-citation>
    </ref>
    <ref id="b52">
      <label>52</label>
      <element-citation publication-type="journal" id="sb52">
        <person-group person-group-type="author">
          <name>
            <surname>Mahmud</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Rahman</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Fattah</surname>
            <given-names>S.A.</given-names>
          </name>
        </person-group>
        <article-title>CovXNet: A multi-dilation convolutional neural network for automatic COVID-19 and other pneumonia detection from chest X-ray images with transferable multi-receptive feature optimization</article-title>
        <source>Comput. Biol. Med.</source>
        <volume>122</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">103869</object-id>
      </element-citation>
    </ref>
    <ref id="b53">
      <label>53</label>
      <element-citation publication-type="journal" id="sb53">
        <person-group person-group-type="author">
          <name>
            <surname>Heidari</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Mirniaharikandehei</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Khuzani</surname>
            <given-names>A.Z.</given-names>
          </name>
          <name>
            <surname>Danala</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Improving the performance of CNN to predict the likelihood of COVID-19 using chest X-ray images with preprocessing algorithms</article-title>
        <source>Int. J. Med. Inform.</source>
        <volume>144</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">104284</object-id>
      </element-citation>
    </ref>
    <ref id="b54">
      <label>54</label>
      <element-citation publication-type="journal" id="sb54">
        <person-group person-group-type="author">
          <name>
            <surname>Rahimzadeh</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Attar</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>A modified deep convolutional neural network for detecting COVID-19 and pneumonia from chest X-ray images based on the concatenation of xception and ResNet50V2</article-title>
        <source>Inform. Med. Unlocked</source>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">100360</object-id>
      </element-citation>
    </ref>
    <ref id="b55">
      <label>55</label>
      <element-citation publication-type="journal" id="sb55">
        <person-group person-group-type="author">
          <name>
            <surname>Pereira</surname>
            <given-names>R.M.</given-names>
          </name>
          <name>
            <surname>Bertolini</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Teixeira</surname>
            <given-names>L.O.</given-names>
          </name>
          <name>
            <surname>Silla</surname>
            <given-names>C.N.</given-names>
            <suffix>Jr.</suffix>
          </name>
          <name>
            <surname>Costa</surname>
            <given-names>Y.M.</given-names>
          </name>
        </person-group>
        <article-title>COVID-19 identification in chest X-ray images on flat and hierarchical classification scenarios</article-title>
        <source>Comput. Methods Programs Biomed.</source>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">105532</object-id>
      </element-citation>
    </ref>
    <ref id="b56">
      <label>56</label>
      <element-citation publication-type="journal" id="sb56">
        <person-group person-group-type="author">
          <name>
            <surname>De Moura</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>García</surname>
            <given-names>L.R.</given-names>
          </name>
          <name>
            <surname>Vidal</surname>
            <given-names>P.F.L.</given-names>
          </name>
          <name>
            <surname>Cruz</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>López</surname>
            <given-names>L.A.</given-names>
          </name>
          <name>
            <surname>Lopez</surname>
            <given-names>E.C.</given-names>
          </name>
          <name>
            <surname>Novo</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ortega</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Deep convolutional approaches for the analysis of covid-19 using chest x-ray images from portable devices</article-title>
        <source>IEEE Access</source>
        <volume>8</volume>
        <year>2020</year>
        <fpage>195594</fpage>
        <lpage>195607</lpage>
        <pub-id pub-id-type="pmid">34786295</pub-id>
      </element-citation>
    </ref>
    <ref id="b57">
      <label>57</label>
      <element-citation publication-type="journal" id="sb57">
        <person-group person-group-type="author">
          <name>
            <surname>Yoo</surname>
            <given-names>S.H.</given-names>
          </name>
          <name>
            <surname>Geng</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Chiu</surname>
            <given-names>T.L.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>S.K.</given-names>
          </name>
          <name>
            <surname>Cho</surname>
            <given-names>D.C.</given-names>
          </name>
          <name>
            <surname>Heo</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>M.S.</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>I.H.</given-names>
          </name>
          <name>
            <surname>Cung Van</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Nhung</surname>
            <given-names>N.V.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning-based decision-tree classifier for COVID-19 diagnosis from chest X-ray imaging</article-title>
        <source>Front. Med.</source>
        <volume>7</volume>
        <year>2020</year>
        <fpage>427</fpage>
      </element-citation>
    </ref>
    <ref id="b58">
      <label>58</label>
      <element-citation publication-type="journal" id="sb58">
        <person-group person-group-type="author">
          <name>
            <surname>Chandra</surname>
            <given-names>T.B.</given-names>
          </name>
          <name>
            <surname>Verma</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>B.K.</given-names>
          </name>
          <name>
            <surname>Jain</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Netam</surname>
            <given-names>S.S.</given-names>
          </name>
        </person-group>
        <article-title>Coronavirus disease (COVID-19) detection in chest X-Ray images using majority voting based classifier ensemble</article-title>
        <source>Expert Syst. Appl.</source>
        <volume>165</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">113909</object-id>
      </element-citation>
    </ref>
    <ref id="b59">
      <label>59</label>
      <element-citation publication-type="journal" id="sb59">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Pang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Verjans</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Viral pneumonia screening on chest X-rays using confidence-aware anomaly detection</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="b60">
      <label>60</label>
      <mixed-citation publication-type="other" id="sb60">F. Ahmad, A. Farooq, M.U. Ghani, Deep Ensemble Model for Classification of Novel Coronavirus in Chest X-Ray Images, Comput. Intell. Neurosci. 2021.</mixed-citation>
    </ref>
    <ref id="b61">
      <label>61</label>
      <element-citation publication-type="journal" id="sb61">
        <person-group person-group-type="author">
          <name>
            <surname>Patel</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Classification of COVID-19 from chest X-ray images using a deep convolutional neural network</article-title>
        <source>Turk. J. Comput. Math. Educ. (TURCOMAT)</source>
        <volume>12</volume>
        <issue>9</issue>
        <year>2021</year>
        <fpage>2643</fpage>
        <lpage>2651</lpage>
      </element-citation>
    </ref>
    <ref id="b62">
      <label>62</label>
      <element-citation publication-type="journal" id="sb62">
        <person-group person-group-type="author">
          <name>
            <surname>Basu</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sheikh</surname>
            <given-names>K.H.</given-names>
          </name>
          <name>
            <surname>Cuevas</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Sarkar</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>COVID-19 detection from CT scans using a two-stage framework</article-title>
        <source>Expert Syst. Appl.</source>
        <year>2022</year>
        <object-id pub-id-type="publisher-id">116377</object-id>
      </element-citation>
    </ref>
    <ref id="b63">
      <label>63</label>
      <element-citation publication-type="journal" id="sb63">
        <person-group person-group-type="author">
          <name>
            <surname>Masud</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>A light-weight convolutional neural network architecture for classification of COVID-19 chest X-Ray images</article-title>
        <source>Multimedia Syst.</source>
        <year>2022</year>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="b64">
      <label>64</label>
      <mixed-citation publication-type="other" id="sb64">Z. Shanshan, Original paper multi-source information fusion technology and its engineering application.</mixed-citation>
    </ref>
    <ref id="b65">
      <label>65</label>
      <element-citation publication-type="journal" id="sb65">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Fulham</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Fusing texture, shape and deep model-learned information at decision level for automated classification of lung nodules on chest CT</article-title>
        <source>Inf. Fusion</source>
        <volume>42</volume>
        <year>2018</year>
        <fpage>102</fpage>
        <lpage>110</lpage>
      </element-citation>
    </ref>
    <ref id="b66">
      <label>66</label>
      <element-citation publication-type="journal" id="sb66">
        <person-group person-group-type="author">
          <name>
            <surname>Martínez-Ballesteros</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>García-Heredia</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Nepomuceno-Chamorro</surname>
            <given-names>I.A.</given-names>
          </name>
          <name>
            <surname>Riquelme-Santos</surname>
            <given-names>J.C.</given-names>
          </name>
        </person-group>
        <article-title>Machine learning techniques to discover genes with potential prognosis role in Alzheimer’s disease using different biological sources</article-title>
        <source>Inf. Fusion</source>
        <volume>36</volume>
        <year>2017</year>
        <fpage>114</fpage>
        <lpage>129</lpage>
      </element-citation>
    </ref>
    <ref id="b67">
      <label>67</label>
      <element-citation publication-type="journal" id="sb67">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Hassan</surname>
            <given-names>M.M.</given-names>
          </name>
          <name>
            <surname>Alamri</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>AI-skin: Skin disease recognition based on self-learning and wide data collection through a closed-loop framework</article-title>
        <source>Inf. Fusion</source>
        <volume>54</volume>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="b68">
      <label>68</label>
      <element-citation publication-type="journal" id="sb68">
        <person-group person-group-type="author">
          <name>
            <surname>Yao</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Multi-class arrhythmia detection from 12-lead varied-length ECG using attention-based time-incremental convolutional neural network</article-title>
        <source>Inf. Fusion</source>
        <volume>53</volume>
        <year>2020</year>
        <fpage>174</fpage>
        <lpage>182</lpage>
      </element-citation>
    </ref>
    <ref id="b69">
      <label>69</label>
      <element-citation publication-type="journal" id="sb69">
        <person-group person-group-type="author">
          <name>
            <surname>James</surname>
            <given-names>A.P.</given-names>
          </name>
          <name>
            <surname>Dasarathy</surname>
            <given-names>B.V.</given-names>
          </name>
        </person-group>
        <article-title>Medical image fusion: A survey of the state of the art</article-title>
        <source>Inf. Fusion</source>
        <volume>19</volume>
        <year>2014</year>
        <fpage>4</fpage>
        <lpage>19</lpage>
      </element-citation>
    </ref>
    <ref id="b70">
      <label>70</label>
      <element-citation publication-type="journal" id="sb70">
        <person-group person-group-type="author">
          <name>
            <surname>Acharya</surname>
            <given-names>U.R.</given-names>
          </name>
          <name>
            <surname>Fujita</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Sudarshan</surname>
            <given-names>V.K.</given-names>
          </name>
          <name>
            <surname>Mookiah</surname>
            <given-names>M.R.K.</given-names>
          </name>
          <name>
            <surname>Koh</surname>
            <given-names>J.E.</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>J.H.</given-names>
          </name>
          <name>
            <surname>Hagiwara</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Chua</surname>
            <given-names>C.K.</given-names>
          </name>
          <name>
            <surname>Junnarkar</surname>
            <given-names>S.P.</given-names>
          </name>
          <name>
            <surname>Vijayananthan</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An integrated index for identification of fatty liver disease using radon transform and discrete cosine transform features in ultrasound images</article-title>
        <source>Inf. Fusion</source>
        <volume>31</volume>
        <year>2016</year>
        <fpage>43</fpage>
        <lpage>53</lpage>
      </element-citation>
    </ref>
    <ref id="b71">
      <label>71</label>
      <element-citation publication-type="journal" id="sb71">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>D.N.</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Gu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhen</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Feasibility study of a multi-criteria decision-making based hierarchical model for multi-modality feature and multi-classifier fusion: Applications in medical prognosis prediction</article-title>
        <source>Inf. Fusion</source>
        <volume>55</volume>
        <year>2020</year>
        <fpage>207</fpage>
        <lpage>219</lpage>
      </element-citation>
    </ref>
    <ref id="b72">
      <label>72</label>
      <element-citation publication-type="journal" id="sb72">
        <person-group person-group-type="author">
          <name>
            <surname>Leslie</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Goddard</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>The influence of clinical information on the reporting of CT by radiologists</article-title>
        <source>Br. J. Radiol.</source>
        <volume>73</volume>
        <issue>874</issue>
        <year>2000</year>
        <fpage>1052</fpage>
        <lpage>1055</lpage>
        <pub-id pub-id-type="pmid">11271897</pub-id>
      </element-citation>
    </ref>
    <ref id="b73">
      <label>73</label>
      <element-citation publication-type="journal" id="sb73">
        <person-group person-group-type="author">
          <name>
            <surname>Boonn</surname>
            <given-names>W.W.</given-names>
          </name>
          <name>
            <surname>Langlotz</surname>
            <given-names>C.P.</given-names>
          </name>
        </person-group>
        <article-title>Radiologist use of and perceived need for patient data access</article-title>
        <source>J. Digital Imaging</source>
        <volume>22</volume>
        <issue>4</issue>
        <year>2009</year>
        <fpage>357</fpage>
        <lpage>362</lpage>
      </element-citation>
    </ref>
    <ref id="b74">
      <label>74</label>
      <element-citation publication-type="journal" id="sb74">
        <person-group person-group-type="author">
          <name>
            <surname>Jonas</surname>
            <given-names>J.B.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y.X.</given-names>
          </name>
          <name>
            <surname>Dong</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Panda-Jonas</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>High myopia and glaucoma-like optic neuropathy</article-title>
        <source>Asia-Pac. J. Ophthalmol. (Philadelphia, PA)</source>
        <volume>9</volume>
        <issue>3</issue>
        <year>2020</year>
        <fpage>234</fpage>
      </element-citation>
    </ref>
    <ref id="b75">
      <label>75</label>
      <element-citation publication-type="journal" id="sb75">
        <person-group person-group-type="author">
          <name>
            <surname>Kumar</surname>
            <given-names>E.S.</given-names>
          </name>
          <name>
            <surname>Jayadev</surname>
            <given-names>P.S.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning for clinical decision support systems: a review from the panorama of smart healthcare</article-title>
        <source>Deep Learn. Tech. Biomed. Health Inform.</source>
        <year>2020</year>
        <fpage>79</fpage>
        <lpage>99</lpage>
      </element-citation>
    </ref>
    <ref id="b76">
      <label>76</label>
      <mixed-citation publication-type="other" id="sb76">M.W. Dusenberry, D. Tran, E. Choi, J. Kemp, J. Nixon, G. Jerfel, K. Heller, A.M. Dai, Analyzing the role of model uncertainty for electronic health records, in: Proceedings of the ACM Conference on Health, Inference, and Learning, 2020, pp. 204–213.</mixed-citation>
    </ref>
    <ref id="b77">
      <label>77</label>
      <element-citation publication-type="journal" id="sb77">
        <person-group person-group-type="author">
          <name>
            <surname>Abualigah</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Diabat</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sumari</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Gandomi</surname>
            <given-names>A.H.</given-names>
          </name>
        </person-group>
        <article-title>A novel evolutionary arithmetic optimization algorithm for multilevel thresholding segmentation of covid-19 ct images</article-title>
        <source>Processes</source>
        <volume>9</volume>
        <issue>7</issue>
        <year>2021</year>
        <fpage>1155</fpage>
      </element-citation>
    </ref>
    <ref id="b78">
      <label>78</label>
      <element-citation publication-type="journal" id="sb78">
        <person-group person-group-type="author">
          <name>
            <surname>Pathak</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Shukla</surname>
            <given-names>P.K.</given-names>
          </name>
          <name>
            <surname>Tiwari</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Stalin</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Deep transfer learning based classification model for COVID-19 disease</article-title>
        <source>Irbm</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="b79">
      <label>79</label>
      <element-citation publication-type="journal" id="sb79">
        <person-group person-group-type="author">
          <name>
            <surname>Ardakani</surname>
            <given-names>A.A.</given-names>
          </name>
          <name>
            <surname>Kanafi</surname>
            <given-names>A.R.</given-names>
          </name>
          <name>
            <surname>Acharya</surname>
            <given-names>U.R.</given-names>
          </name>
          <name>
            <surname>Khadem</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Mohammadi</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Application of deep learning technique to manage COVID-19 in routine clinical practice using CT images: Results of 10 convolutional neural networks</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">103795</object-id>
      </element-citation>
    </ref>
    <ref id="b80">
      <label>80</label>
      <element-citation publication-type="journal" id="sb80">
        <person-group person-group-type="author">
          <name>
            <surname>Chimmula</surname>
            <given-names>V.K.R.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Time series forecasting of COVID-19 transmission in Canada using LSTM networks</article-title>
        <source>Chaos Solitons Fractals</source>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">109864</object-id>
      </element-citation>
    </ref>
    <ref id="b81">
      <label>81</label>
      <element-citation publication-type="book" id="sb81">
        <person-group person-group-type="author">
          <name>
            <surname>Afshar</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Heidarian</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Naderkhani</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Oikonomou</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Plataniotis</surname>
            <given-names>K.N.</given-names>
          </name>
          <name>
            <surname>Mohammadi</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Covid-caps: A capsule network-based framework for identification of covid-19 cases from x-ray images</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2004.02696" id="interref12">arXiv:2004.02696</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b82">
      <label>82</label>
      <element-citation publication-type="journal" id="sb82">
        <person-group person-group-type="author">
          <name>
            <surname>Oh</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>J.C.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning covid-19 features on cxr using limited training data sets</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="b83">
      <label>83</label>
      <element-citation publication-type="book" id="sb83">
        <person-group person-group-type="author">
          <name>
            <surname>Punn</surname>
            <given-names>N.S.</given-names>
          </name>
          <name>
            <surname>Agarwal</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray images using fine-tuned deep neural networks</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2004.11676" id="interref13">arXiv:2004.11676</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b84">
      <label>84</label>
      <element-citation publication-type="journal" id="sb84">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Aertsen</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Deprest</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Vercauteren</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Aleatoric uncertainty estimation with test-time augmentation for medical image segmentation with convolutional neural networks</article-title>
        <source>Neurocomputing</source>
        <volume>338</volume>
        <year>2019</year>
        <fpage>34</fpage>
        <lpage>45</lpage>
      </element-citation>
    </ref>
    <ref id="b85">
      <label>85</label>
      <element-citation publication-type="journal" id="sb85">
        <person-group person-group-type="author">
          <name>
            <surname>Luo</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Dong</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Tam</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Howey</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ohorodnyk</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Commensal correlation network between segmentation and direct area estimation for bi-ventricle quantification</article-title>
        <source>Med. Image Anal.</source>
        <volume>59</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">101591</object-id>
      </element-citation>
    </ref>
    <ref id="b86">
      <label>86</label>
      <element-citation publication-type="journal" id="sb86">
        <person-group person-group-type="author">
          <name>
            <surname>Mazoure</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Mazoure</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bédard</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Makarenkov</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>DUNEScan: a web server for uncertainty estimation in skin cancer detection with deep neural networks</article-title>
        <source>Sci. Rep.</source>
        <volume>12</volume>
        <issue>1</issue>
        <year>2022</year>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="pmid">34992227</pub-id>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="appA">
    <label>Appendix A</label>
    <title>Information fusion</title>
    <p id="d1e5318">Information fusion is initiated from data fusion. It can also be termed as multi-sensor information fusion <xref rid="b64" ref-type="bibr">[64]</xref>, feature fusion for combining different features <xref rid="b65" ref-type="bibr">[65]</xref>, various biological sources <xref rid="b66" ref-type="bibr">[66]</xref>, <xref rid="b67" ref-type="bibr">[67]</xref>, medical signals <xref rid="b68" ref-type="bibr">[68]</xref>, or medical image fusion <xref rid="b69" ref-type="bibr">[69]</xref>, <xref rid="b70" ref-type="bibr">[70]</xref>, <xref rid="b71" ref-type="bibr">[71]</xref>. Different data fusion models have been widely used in military applications. Their purpose was to integrate or correlate data of several sensors of different, or the same, type(s) to achieve better results than those yielded by a single sensor. Gradually, data fusion models have been converted into information fusion models. Information fusion does not rely on multi-sensor data only. Its areas of research and application have been growing drastically. The rapid emergence of network technologies allowed the information fusion to change from centralized single node information fusion to distributed information fusion.</p>
    <p id="d1e5341">Modern medicine nowadays depends on amalgamation of data and information from manifold sources that include structured imaging data, laboratory data, unstructured narrative data, and even observational or audio data in some cases <xref rid="b72" ref-type="bibr">[72]</xref>. Substantial clinical context is required for medical image interpretation to facilitate diagnostic decisions <xref rid="b73" ref-type="bibr">[73]</xref>. Imaging data are not only limited to radiology but also concern many other image-based medical specialties such as dermatology, ophthalmology, and pathology <xref rid="b74" ref-type="bibr">[74]</xref>. Unstructured and structured clinical data from the electronic heath records (EHR) are crucial for clinically relevant medical image interpretation <xref rid="b75" ref-type="bibr">[75]</xref>. Clinically relevant models rely on automated diagnosis and classification systems that use both clinical data from EHR and medical imaging data. In various applications, such as video classification, autonomous driving, and medical data analysis, multimodal learning models use various imaging data along with other data types (data fusion approach). The current medical imaging paradigm showcases a drift where both pixel and EHR data are employed in fusion-domains for tackling complicated tasks which cannot be resolved by single modality. A wide variety of fusion techniques have been applied with traditional machine learning and deep learning techniques. This facilitates an increasing interest in several areas, each of which has its specific prerequisites. In medicine, customized predictions carry significant meaning as incorrect decisions are associated with severe costs due to associated ethical concerns and risk to human life <xref rid="b76" ref-type="bibr">[76]</xref>.</p>
    <p id="d1e5364">Deep neural networks (DNNs) are now prevailing in many medical applications. The performance of DNNs can depend on either one DNN model or an ensemble of several DNN models, focusing on enhancing the accuracy of probabilistic predictions. Model’s uncertainty is inherent in fitting DNNs, which is not well addressed in the literature, while some DNN models can use probabilities to capture data uncertainty. For example, when the mortality of the patients is predicted using intensive care unit (ICU) data, the state-of-the-art methods may be able to yield high values of the AUC-ROC statistics. However, these methods are unable to discriminate between the cases in which the model is certain about its predictions or fairly uncertain about them. Hence, there is an urgent need for examining the use of both model and data uncertainty, specifically in the context of predictive medicine. Recently proposed model uncertainty techniques include: function priors, deep ensembles, Monte Carlo (MC) dropout, and reparameterization-based variational Bayesian neural networks (BNNs). Thus, several clinical care problems can be efficiently addressed by DNNs integrating model uncertainty techniques.</p>
  </sec>
  <sec id="appB">
    <label>Appendix B</label>
    <title>Literature review</title>
    <p id="d1e5371">In this section, we will briefly review a few recent studies conducted on COVID-19 detection/segmentation as well as those using UQ in medical image analysis.</p>
    <sec id="appB.1">
      <label>B.1</label>
      <title>COVID-19 Classification/Segmentation</title>
      <p id="d1e5378">It is crucial to recognize COVID-19 cases quickly to better manage and prevent the pandemic from further spreading. A wide variety of traditional machine learning and deep learning models have been used for this purpose <xref rid="b77" ref-type="bibr">[77]</xref>. For example, Pathak et al. <xref rid="b78" ref-type="bibr">[78]</xref> showed that deep transfer learning is a useful approach for COVID-19 classification. In another work, Ardakani et al. <xref rid="b79" ref-type="bibr">[79]</xref> analyzed 108 COVID-19 patients, those with viral pneumonia, and other atypical patients, using CT scan images. They tested ten CNN models to discriminate the COVID-19 group of patients from the non-COVID-19 cohorts. The Xception and ResNet-101 models demonstrated a superior performance for their data with an AUC value of 0.994 for both of them. Deep learning models can assist the clinicians and radiologists utilizing CXR scans for the detection of COVID-19. In this context, Khan et al. <xref rid="b50" ref-type="bibr">[50]</xref> introduced CoroNet, a deep CNN model, allowing for automated detection of COVID-19 cases. Xception architecture was used for pretraining and two publicly available X-ray datasets were used for classification of normal, pneumonia and COVID-19 cases. The model by Khan et al. yielded the accuracy of 95% for 3-class (Normal vs Pneumonia vs COVID-19) classification. In addition, their model demonstrated an overall accuracy of 89.6% for 4-class (Pneumonia bacterial vs Pneumonia viral vs Normal vs COVID-19) classification. CoroNet provided promising results with minimal preprocessing of data. Chimmula et al. <xref rid="b80" ref-type="bibr">[80]</xref> used modern deep learning methods to design a COVID-19 prediction model (the Long Short-Term Memory (LSTM) method) using publicly available Canadian health authority and John Hopkins University data. The authors also scrutinized some vital features to predict probable stopping time and eventual trends of the pandemic.</p>
      <p id="d1e5401">Afshar et al. <xref rid="b81" ref-type="bibr">[81]</xref> devised a COVID-19 prediction approach based on Capsule networks (COVID-CAPS) and produced efficient results with smaller X-ray datasets. Their framework exhibited better performance than the existing CNN-based models. COVID-CAPS exhibited the AUC value of 0.97, the specificity value of 95.8%, the value of sensitivity of 90%, and the accuracy value of 95.7% while dealing with a lower number of network parameters than its counterparts. Transfer learning and pretraining were used to further enhance the diagnostic nature of the framework and tested with a new X-ray dataset. The use of artificial intelligence (AI) to analyze CXR images for accurate COVID-19 patient triage is of supreme importance. Lack of systematic collection of CXR data for training of deep learning strategies hinders the proper diagnosis. To address this issue, Oh et al. <xref rid="b82" ref-type="bibr">[82]</xref> presented a patch-based CNN technique for COVID-19 patient detection using a low number of trainable parameters. Punn et al. <xref rid="b83" ref-type="bibr">[83]</xref> proposed the weighted class loss function and random oversampling methods for transfer learning for different SOTA deep learning models. They used posteroanterior CXR images for multiclass classification: (Pneumonia, COVID-19, and Normal cases) and for binary classification (COVID-19 and Normal cases). The experimental results of Punn et al. demonstrated that each of the models they considered was scenario-dependent, and that NASNetLarge showed better scores compared to its counterparts.</p>
    </sec>
    <sec id="appB.2">
      <label>B.2</label>
      <title>Uncertainty quantification in medical image analysis</title>
      <p id="d1e5420">There are numerous studies conducted on Uncertainty Quantification (UQ) in medical image analysis using traditional ML and DL methods. In this section, we briefly discuss a few recent studies which applied different UQ methods in medical image analysis. Deep CNNs do not facilitate uncertainty estimation in medical image segmentation, e.g., image-based (aleatoric) and model-based (epistemic) uncertainties, despite delivering the SOTA performance. Wang et al. <xref rid="b84" ref-type="bibr">[84]</xref> examined different types of uncertainties related to 3D and 2D medical image segmentation tasks at both structural and pixel levels. Moreover, they introduced test-time augmentation-based aleatoric uncertainty to measure the effect of various transformations of the input image on the output segmentation. MC simulation with prior distributions of parameters was used to estimate a distribution of predictions in an image acquisition model with noise and image transformations. It helped to formulate test-time augmentation.</p>
      <p id="d1e5426">The direct ventricle function index estimation and bi-ventricle segmentation can be used to tackle ventricle quantification issue. Luo et al. <xref rid="b85" ref-type="bibr">[85]</xref> introduced a unified bi-ventricle quantification approach based on commensal correlation between the direct area estimation and bi-ventricle segmentation. The authors devised a new deep commensal network (DCN) to combine these two commensal tasks into a unified framework based on the proposed commensal correlation loss. The proposed DCN ensured fast convergence, carrying out end-to-end optimization as well as uncertainty estimation with one-time inference. Colorectal cancer is one of the prime reasons of cancer-related fatalities around the globe. Its key precursors are colorectal polyps. Some modern CNNs based decision support systems for segmentation and detection of colorectal polyps provide an excellent performance. In another study, Ghoshal et al. <xref rid="b21" ref-type="bibr">[21]</xref> used drop-weights based Bayesian CNN (BCNN) to measure uncertainty in deep learning methods to enhance the diagnostic performance. They demonstrated that accuracy of the prediction was highly correlated with the uncertainty in prediction. Recently, Mazoure et al. <xref rid="b86" ref-type="bibr">[86]</xref> have presented the DUNEScan (Deep Uncertainty Estimation for Skin Cancer) web application performing in-depth analysis of uncertainty within some modern CNN models. DUNEScan relies on efficient Grad-CAM and UMAP methods to visualize the classification manifold for the user’s input, yielding key information about its closeness to skin lesion images available in the ISIC data repository.</p>
      <p id="d1e5440">
        <table-wrap position="anchor" id="tblB.12">
          <label>Table B.12</label>
          <caption>
            <p>The detailed parts of Convolution blocks used in Deep 1 (Simple CNN), Deep 2 (Multi-headed CNN), and our proposed model (Fusion model).</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left">Model</th>
                <th align="left">Layer name</th>
                <th align="left">Input size</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="3">Deep 1 (Simple CNN)</td>
                <td align="left">Conv1</td>
                <td align="left">2D convolution, Kernel size: 3, Activation: ReLU, Max Pooling: 2</td>
              </tr>
              <tr>
                <td align="left">Conv2</td>
                <td align="left">2D convolution, Kernel size: 3, Activation: ReLU, Batch Normalization, Max Pooling: 2</td>
              </tr>
              <tr>
                <td align="left">Conv3</td>
                <td align="left">2D convolution, Kernel size: 3, Activation: ReLU, Batch Normalization</td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td align="left" rowspan="3">Deep 2 (Multi-headed CNN)</td>
                <td align="left">Conv1</td>
                <td align="left">2D convolution, Kernel size: 3, Activation: ReLU, Batch Normalization, Max Pooling: 2</td>
              </tr>
              <tr>
                <td align="left">Conv2</td>
                <td align="left">2D convolution, Kernel size: 3, Activation: ReLU, Batch Normalization, Max Pooling: 2</td>
              </tr>
              <tr>
                <td align="left">Conv3</td>
                <td align="left">2D convolution, Kernel size: 3, Activation: ReLU, Batch Normalization, Max Pooling: 2</td>
              </tr>
              <tr>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td align="left" rowspan="2">Proposed (Fusion model)</td>
                <td align="left">Conv1</td>
                <td align="left">2D convolution, Kernel size: 3, Activation: ReLU, Max Pooling: 2</td>
              </tr>
              <tr>
                <td align="left">Conv2</td>
                <td align="left">2D Separable convolution, Kernel size: 3, Activation: ReLU, Batch Normalization, Max Pooling: 2</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Conv3</td>
                <td align="left">2D Separable convolution, Kernel size: 3, Activation: ReLU, Batch Normalization, Max Pooling: 2</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Conv4</td>
                <td align="left">2D Separable convolution, Kernel size: 3, Activation: ReLU, Batch Normalization, Max Pooling: 2</td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left">Conv5</td>
                <td align="left">2D Separable convolution, Kernel size: 3, Activation: ReLU, Batch Normalization, Max Pooling: 2</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
    </sec>
  </sec>
  <sec sec-type="data-availability" id="da1">
    <title>Data availability</title>
    <p id="d1e3253">Data will be made available on request.</p>
  </sec>
  <ack id="d1e5294">
    <title>Acknowledgments</title>
    <p id="d1e5297">This research was partially supported by the <funding-source id="GS1"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100000923</institution-id><institution>Australian Research Council’s Discovery</institution></institution-wrap></funding-source> Projects funding scheme (project DP190102181 and DP210101465).</p>
  </ack>
  <fn-group>
    <fn id="fn2">
      <label>2</label>
      <p id="d1e4420">Sources: <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/azaemon/preprocessed-ct-scans-for-covid19" id="interref2">https://www.kaggle.com/azaemon/preprocessed-ct-scans-for-covid19</ext-link>.</p>
    </fn>
    <fn id="fn3">
      <label>3</label>
      <p id="d1e4432">Sources: <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/prashant268/chest-xray-covid19-pneumonia" id="interref3">https://www.kaggle.com/prashant268/chest-xray-covid19-pneumonia</ext-link>.</p>
    </fn>
  </fn-group>
</back>
