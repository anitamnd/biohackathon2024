<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PeerJ</journal-id>
    <journal-id journal-id-type="iso-abbrev">PeerJ</journal-id>
    <journal-id journal-id-type="pmc">PeerJ</journal-id>
    <journal-id journal-id-type="publisher-id">PeerJ</journal-id>
    <journal-title-group>
      <journal-title>PeerJ</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2167-8359</issn>
    <publisher>
      <publisher-name>PeerJ Inc.</publisher-name>
      <publisher-loc>San Diego, USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8048404</article-id>
    <article-id pub-id-type="publisher-id">11155</article-id>
    <article-id pub-id-type="doi">10.7717/peerj.11155</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Bioengineering</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Conservation Biology</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Entomology</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Zoology</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Computational Science</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title><italic>scAnt</italic>—an open-source platform for the creation of 3D models of arthropods (and other small objects)</article-title>
    </title-group>
    <contrib-group>
      <contrib id="author-1" contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-1012-6646</contrib-id>
        <name>
          <surname>Plum</surname>
          <given-names>Fabian</given-names>
        </name>
      </contrib>
      <contrib id="author-2" contrib-type="author" corresp="yes">
        <name>
          <surname>Labonte</surname>
          <given-names>David</given-names>
        </name>
        <email>d.labonte@imperial.ac.uk</email>
      </contrib>
      <aff><institution>Department of Bioengineering, Imperial College London</institution>, <addr-line>London</addr-line>, <country>UK</country></aff>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Gillespie</surname>
          <given-names>Joseph</given-names>
        </name>
      </contrib>
    </contrib-group>
    <pub-date pub-type="epub" date-type="pub" iso-8601-date="2021-04-12">
      <day>12</day>
      <month>4</month>
      <year iso-8601-date="2021">2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>9</volume>
    <elocation-id>e11155</elocation-id>
    <history>
      <date date-type="received" iso-8601-date="2020-12-27">
        <day>27</day>
        <month>12</month>
        <year iso-8601-date="2020">2020</year>
      </date>
      <date date-type="accepted" iso-8601-date="2021-03-04">
        <day>4</day>
        <month>3</month>
        <year iso-8601-date="2021">2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 Plum and Labonte</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Plum and Labonte</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="pmc" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, reproduction and adaptation in any medium and for any purpose provided that it is properly attributed. For attribution, the original author(s), title, publication source (PeerJ) and either DOI or URL of the article must be cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="https://peerj.com/articles/11155"/>
    <abstract>
      <p>We present <italic>scAnt</italic>, an open-source platform for the creation of digital 3D models of arthropods and small objects. <italic>scAnt</italic> consists of a scanner and a Graphical User Interface, and enables the automated generation of Extended Depth Of Field images from multiple perspectives. These images are then masked with a novel automatic routine which combines random forest-based edge-detection, adaptive thresholding and connected component labelling. The masked images can then be processed further with a photogrammetry software package of choice, including open-source options such as <italic>Meshroom</italic>, to create high-quality, textured 3D models. We demonstrate how these 3D models can be rigged to enable realistic digital specimen posing, and introduce a novel simple yet effective method to include semi-realistic representations of approximately planar and transparent structures such as wings. As a result of the exclusive reliance on generic hardware components, rapid prototyping and open-source software, <italic>scAnt</italic> costs only a fraction of available comparable systems. The resulting accessibility of <italic>scAnt</italic> will (i) drive the development of novel and powerful methods for machine learning-driven behavioural studies, leveraging synthetic data; (ii) increase accuracy in comparative morphometric studies as well as extend the available parameter space with area and volume measurements; (iii) inspire novel forms of outreach; and (iv) aid in the digitisation efforts currently underway in several major natural history collections.</p>
    </abstract>
    <kwd-group kwd-group-type="author">
      <kwd>Photogrammetry</kwd>
      <kwd>Morphometry</kwd>
      <kwd>Zoology</kwd>
      <kwd>Macro imaging</kwd>
      <kwd>3D</kwd>
      <kwd>Digitisation</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="fund-1">
        <funding-source>Imperial College’s President’s PhD Scholarship</funding-source>
      </award-group>
      <award-group id="fund-2">
        <funding-source>European Research Council (ERC)</funding-source>
      </award-group>
      <award-group id="fund-3">
        <funding-source>European Union’s Horizon 2020</funding-source>
        <award-id>851705</award-id>
      </award-group>
      <funding-statement>This study was funded by the Imperial College’s President’s PhD Scholarship (to Fabian Plum) and is part of a project that has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (Grant agreement No. 851705, to David Labonte). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro">
    <title>Introduction</title>
    <p>The diversity of arthropods is unparalleled (<xref rid="ref-50" ref-type="bibr">Misof et al., 2014</xref>). Key institutions such as the Natural History Museum in London, the Smithsonian National Museum of Natural History, or the Australian National Insect Collection house upwards of ten million insect specimens, and grow continuously, so archiving part of this diversity (<xref rid="ref-44" ref-type="bibr">Mantle, La Salle &amp; Fisher, 2012</xref>). Clearly, the utility of these collections hinges on the accessibility of the specimens. However, specimen access typically requires to be either physically present on-site, or for specimens to be posted, so reducing the practical value of the collections. This issue is particularly severe for rare and valuable specimens such as holotypes, which can be difficult to access despite their scientific importance. In recognition of these limitations, significant efforts have been underway to digitise natural history collections (<xref rid="ref-6" ref-type="bibr">Beaman &amp; Cellinese, 2012</xref>; <xref rid="ref-7" ref-type="bibr">Blagoderov et al., 2012</xref>; <xref rid="ref-44" ref-type="bibr">Mantle, La Salle &amp; Fisher, 2012</xref>; <xref rid="ref-52" ref-type="bibr">Nguyen et al., 2017</xref>, <xref rid="ref-53" ref-type="bibr">2014</xref>; <xref rid="ref-32" ref-type="bibr">Hudson et al., 2015</xref>; <xref rid="ref-45" ref-type="bibr">Martins et al., 2015</xref>; <xref rid="ref-28" ref-type="bibr">Galantucci, Pesce &amp; Lavecchia, 2016</xref>; <xref rid="ref-22" ref-type="bibr">Erolin, Jarron &amp; Csetenyi, 2017</xref>; <xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>; <xref rid="ref-26" ref-type="bibr">Galantucci, Guerra &amp; Lavecchia, 2018</xref>; <xref rid="ref-57" ref-type="bibr">Qian et al., 2019</xref>; <xref rid="ref-12" ref-type="bibr">Brecko &amp; Mathys, 2020</xref>). Such a cybertaxonomy has been predicted to revolutionise collaborative taxonomy, and fundamentally change formal and public taxonomic education (<xref rid="ref-74" ref-type="bibr">Zhang, Gao &amp; Caelli, 2010</xref>; <xref rid="ref-72" ref-type="bibr">Wheeler et al., 2012</xref>). However, the vast majority of these efforts have focused on high-throughput capture of 2D images, and the convenient automated inclusion of metadata such as barcodes, labels etc. Photographs are doubtlessly useful, but by definition contain substantially less information than the original specimen, as they are restricted to a single image plane (<xref rid="ref-53" ref-type="bibr">Nguyen et al., 2014</xref>; <xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>). Even obtaining simple 1D measurements from 2D images is error-prone, due to parallax errors and intra-observer variability, which is larger for measurements obtained from 2D photographs compared to 3D models (<xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>; <xref rid="ref-57" ref-type="bibr">Qian et al., 2019</xref>; <xref rid="ref-12" ref-type="bibr">Brecko &amp; Mathys, 2020</xref>). As a consequence, the “gold standard” for digitisation are photorealistic and anatomically accurate 3D models (<xref rid="ref-72" ref-type="bibr">Wheeler et al., 2012</xref>).</p>
    <p>Perhaps the most promising method for the creation of 3D models is photogrammetric reconstruction, which retains colour information, and represents an excellent compromise between portability, price and quality (<xref rid="ref-47" ref-type="bibr">Mathys, Brecko &amp; Semal, 2013</xref>; <xref rid="ref-12" ref-type="bibr">Brecko &amp; Mathys, 2020</xref>). To the best of our knowledge, there exist two photogrammetry devices specifically designed for the creation of 3D models of arthropods (<xref rid="ref-53" ref-type="bibr">Nguyen et al., 2014</xref>; <xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>). However, both systems rely on specialised hardware and commercial software for scanner control, image processing and 3D reconstruction, hampering widespread use.</p>
    <p>Here, we address this limitation by introducing an open-source platform for the automated generation of 3D models of arthropods and small objects from a series of 2D images. This platform, <italic>scAnt</italic>, consists of (i) a low-cost scanner, built from generic structural and electronic components, (ii) an intuitive yet powerful Graphical User Interface (GUI) providing full control over (iii) a processing pipeline which combines several community-driven open-source applications to automate image capture and simultaneous image processing. <italic>scAnt</italic> runs on Windows and Linux operating systems, and includes full support for scanner control, image capture, image post-processing, as well as additional scripts to facilitate reconstruction. It is accompanied by a guide to manual mesh post-processing. All component drawings and assemblies are available on Thingiverse (<uri xlink:href="http://thingiverse.com/evobiomech/designs">thingiverse.com/evobiomech/designs</uri>, licensed under CC BY 4.0), and the manual and all code have been deposited on GitHub (<uri xlink:href="http://github.com/evo-biomech/scAnt">github.com/evo-biomech/scAnt</uri>, made available under MIT License). Due to the combination of low-costs, open documentation and system flexibility, <italic>scAnt</italic> enables scientists and dedicated amateurs alike to create high-quality 3D models of arthropods, so contributing to a growing digital library which can be used for research, outreach and conservation.</p>
  </sec>
  <sec sec-type="materials|methods">
    <title>Materials and Methods</title>
    <p><italic>scAnt</italic> is designed to automate the capturing and processing of 2D images from variable viewing angles. It is built entirely with generic hardware components, and exclusively leverages recent developments in open-source image processing software, so remaining affordable, accessible, and flexible.</p>
    <p>With <italic>scAnt</italic>, 3D models are produced through a series of five steps, described in detail below: (i) mounting a pinned specimen in an illumination dome; (ii) configuring and conducting a “scan”, using a custom-made GUI; (iii) processing captured images into Extended Depth Of Field (EDOF) images and creating masks for each EDOF image; (iv) generating textured 3D meshes from the EDOF images; and (v) post-processing of the created mesh.</p>
    <sec>
      <title>Scanner design</title>
      <p>In the design of <italic>scAnt</italic>, we built on and benefited from several insights and innovations from previous studies: (i) We deploy focus stacking to overcome the limitations of single-focal plane images (<xref rid="ref-29" ref-type="bibr">Gallo, Muzzupappa &amp; Bruno, 2014</xref>); (ii) We use a two-axis gimbaled system to maximise the number of possible viewing angles (<xref rid="ref-53" ref-type="bibr">Nguyen et al., 2014</xref>); (iii) We designed an illumination dome to achieve “flat” lighting, thus minimising specular reflections and other artefacts arising from variations in appearance with viewing angle (<xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>).</p>
      <p>All structural components of the scanner are fabricated via 3D printing and laser cutting, methods readily available in most laboratories and museums, but also accessible to the keen amateur. Technical drawings of all components, the assembly, and a complete parts list are available for download (<uri xlink:href="http://thingiverse.com/evobiomech/designs">thingiverse.com/evobiomech/designs</uri>, licensed under CC BY 4.0). The hardware costs for <italic>scAnt</italic> are approximately €200, and—due to the exclusive use of open-source software—the only remaining costs are related to the camera and lens (for our system, this added another €700). The total costs are hence between a factor of five to ten lower than for existing systems (~€5,000 for <xref rid="ref-53" ref-type="bibr">Nguyen et al., 2014</xref>; ~€8,000 for <xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>).</p>
      <p>In order to enhance hardware durability and to minimise print post-processing, we used PLA filament for all prints. An acrylic sheet, laser-cut from 4 mm thick acrylic sheets, serves as the base plate for the mounting of all printed elements, and for the routing of all wires (<xref ref-type="fig" rid="fig-1">Fig. 1A</xref>).</p>
      <fig id="fig-1" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj.11155/fig-1</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Computer Aided Design (CAD) drawings and photograph of the assembled 3D scanner.</title>
          <p>(A) Top-down drawing, including wiring. The stepper motors for each axis as well as their corresponding drivers are labelled X (horizontal axis, red wires), Y (vertical axis, orange wires) and Z (camera slider, yellow wires), respectively. Two end-stops (limit switches) are attached to the actuated camera rail to provide reference positions for the <italic>X</italic> and <italic>Z</italic> axes. Navy blue and maroon wires indicate connections from the 12 V power supply to the stepper drivers and LEDs, respectively. Blue wires indicate USB cables, connecting stepper drivers and camera to the computer, using USB 2.0 and USB 3.0 ports, respectively. (B) Photograph of the open illumination dome, comprised of two half-domes; a specimen is mounted inside. Two LEDs are mounted on the inside of the half-domes, and are covered by 3D printed diffusers. The slits at the bottom of each semi-sphere allow the pin of the <italic>X</italic>-axes stepper to move, as indicated in (C). (C) Side-view drawing of the assembled scanner, illustrating the range of motion of the <italic>X</italic>, <italic>Y</italic> and <italic>Z</italic> axes. The <italic>Y</italic>-axis stepper is unlimited. Additional photographs of the assembled scanner, as well as links to the parts lists and 3D models, can be found in the <xref ref-type="supplementary-material" rid="supplemental-information">Supplemental Material</xref>.</p>
        </caption>
        <graphic xlink:href="peerj-09-11155-g001"/>
      </fig>
      <p>The scanner consists of three main components (<xref ref-type="fig" rid="fig-1">Fig. 1</xref>): (i) an illumination dome which ensures flat and diffuse lighting (inspired by the design in <xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>), (ii) a two-axis gimbal to change the orientation of specimen inside the dome (inspired by the design of <xref rid="ref-53" ref-type="bibr">Nguyen et al. (2014)</xref>), and (iii) an actuated camera slider to alter the position of the focal plane.</p>
      <p>(i) The illumination dome is comprised of two semi-domes (<xref ref-type="fig" rid="fig-1">Fig 1B</xref>). As the image background needs to be as uniform as possible to achieve high-quality results in subsequent masking and reconstruction steps (see below), the inside of both semi-domes was sanded down, and coated in a matt light grey spray paint, so ensuring flat and diffuse lighting, regardless of specimen orientation. The dome is illuminated by two arrays of LEDs. Accurate colour information requires LED strips with a Colour Rendering Index (CRI) of ≥90 and a colour temperature of ≥5,000 K. In order to reduce sharp specular reflections, translucent, 3D-printed rings are mounted in each half of the illumination dome and act as diffusers (<xref ref-type="fig" rid="fig-1">Fig. 1B</xref>). Further specifications, as well as links to the parts list and 3D models, can be found in the <xref ref-type="supplementary-material" rid="supplemental-information">Supplemental Material</xref>.</p>
      <p>(ii) Inside the dome, the specimens are mounted directly on a rod connected to the two-axis gimbal. The horizontal (<italic>X</italic>) and vertical (<italic>Y</italic>) axes of the gimbal are actuated by two 4-lead NEMA 17 stepper motors (0.7 A, 1.8° step size). In order to minimise the noise introduced by motor jitter, a counterweight is attached to the top of the gimbal, opposite to the vertically oriented stepper motor actuating the <italic>Y</italic>-axis. The gimbal is mounted such that the <italic>X</italic> and <italic>Y</italic> axes run directly through the centre of the illumination dome, enabling specimen rotation without translation relative to the imaging plane. Mirroring slits on the underside of the dome provide the space required for the upwards facing pin of the gimbal to move freely (<xref ref-type="fig" rid="fig-1">Figs. 1B</xref> and <xref ref-type="fig" rid="fig-1">1C</xref>).</p>
      <p>(iii) To extend the depth of field per XY position, the camera is mounted on a linear slider (<italic>Z</italic> axis, <xref ref-type="fig" rid="fig-1">Fig. 1A</xref>), so that the focal plane can be moved relative to the specimen. The stepper-controlled camera slider is mounted on top of an elevated platform, manufactured from 4 mm acrylic sheets via laser cutting, and is placed outside the illumination dome such that the lens points at the centre of the dome (<xref ref-type="fig" rid="fig-1">Fig 1C</xref>). We used a 20 MP colour sensor camera (BFS-U3-200S6C-C: 20 MP; FLIR Systems, Wilsonville, OR, USA) with custom made extension tubes and a compact 35 mm, F16 MPZ Computar lens (Computar, CBC Group, Phoenix, AZ, USA). <italic>scAnt</italic> also supports DSLR cameras, and support for other cameras may readily be added (see GitHub as well as Thingiverse).</p>
      <p>All communication between the computer and electronic components of the scanner occurs directly via USB to alleviate the need for additional control hardware. All three stepper motors are controlled by separate USB driver boards (Tic T500; Pololu Corporation, Las Vegas, NV, USA). In order to minimise the number of cables and ports running to or being used at the computer, respectively, all stepper drivers are connected to a USB Hub; a single 2.0 connection from the hub to the computer is then sufficient for all communication apart from camera control. The camera requires higher bandwidth and is therefore connected via a USB 3.0/3.1 port (<xref ref-type="fig" rid="fig-1">Fig. 1A</xref>).</p>
      <p>All stepper drivers, as well as the LEDs, are connected to a generic DC12V, 5A power supply. Due to the low load required to actuate the gimbal, the four-wire steppers do not draw a current close to the peak of either driver or power supply, so that the voltage remains approximately constant. In order to further reduce the peak current draw, steppers are always actuated successively rather than simultaneously.</p>
      <p>The stepper drivers controlling the <italic>X</italic>- and <italic>Z</italic>-axes are connected to a limit switch, which provides a reference position, and prevents gimbal and camera from moving to positions where they may physically interact (<xref ref-type="fig" rid="fig-1">Figs. 1A</xref> and <xref ref-type="fig" rid="fig-1">1C</xref>). The system is constructed such that the object of interest can be scanned from any orientation about the <italic>Y</italic>-axis, and an angular range of 100° about the <italic>X</italic>-axis (<xref ref-type="fig" rid="fig-1">Fig. 1C</xref>). The range of motion in the <italic>X</italic>-axis is limited to ensure that the back half of the illumination dome is the background throughout the imaging range. Viewing angles larger than 100° in the x-axis angles are not necessarily required to generate high-quality models as long as the dorsoventral axis of the specimen is approximately aligned with the <italic>Y</italic>-axis: multiple images obtained for increasingly dorsal or ventral views then contain redundant information, due to the lateral symmetry of insects in the sagittal plane.</p>
      <p>The physical dimensions of the scanner impose an upper size limit of approximately 8 cm for the longest specimen axis, whereas the lower size limit depends on the desired resolution, and can be controlled via an appropriate choice of camera and lens. Imaging larger specimens requires a geometric scaling of the scanner, which simple, as all software components and subsequent processing steps remain identical. Hence, <italic>scAnt</italic> can be readily adjusted to suit the specific needs of the end-user.</p>
    </sec>
    <sec>
      <title>Mounting pinned specimens</title>
      <p>Photogrammetric reconstruction requires that the appearance of the captured subject is independent of the perspective (<xref rid="ref-47" ref-type="bibr">Mathys, Brecko &amp; Semal, 2013</xref>; <xref rid="ref-26" ref-type="bibr">Galantucci, Guerra &amp; Lavecchia, 2018</xref>). As a consequence, lighting must be as uniform as possible, and any motion of parts of the imaged object relative to each other needs to be avoided. Practically, this means that insect specimens should be dried prior to imaging.</p>
      <p>In preparing a specimen for scanning, we recommend posing it with its legs and antennae spread in the frontal plane to minimise occlusion (see <xref rid="ref-70" ref-type="bibr">Walker et al. (1999)</xref> for an in-depth review of the handling and pinning of arthropods). Specimens need to be connected to a pin, which can be glued onto the rod connector of the gimbal; we recommend UV glue as it is easy to remove, but other solutions, such as attaching additional clamps, are possible.</p>
    </sec>
    <sec>
      <title>Automated image capture and processing using the <italic>scAnt</italic> GUI</title>
      <p>In order to automate the scanning process as much as possible, we developed a GUI in Python, which provides full control over all relevant settings. Key Python libraries which power the GUI include NumPy (<xref rid="ref-54" ref-type="bibr">Oliphant, 2006</xref>), scikit-image (<xref rid="ref-67" ref-type="bibr">Van der Walt et al., 2014</xref>), OpenCV (<xref rid="ref-10" ref-type="bibr">Bradski, 2000</xref>) and Pillow (<xref rid="ref-15" ref-type="bibr">Clark, 2015</xref>). The GUI consists of five sections (see <xref ref-type="fig" rid="fig-2">Fig. 2</xref>): (i) The video stream of the camera is shown in the “Live-View”-section of the GUI. Overexposed areas, as well as a normalised histogram for each RGB colour channel, can be displayed as an overlay to aid the selection of suitable settings. (ii) All relevant camera parameters such as exposure time, gain level and white balance ratios can be adjusted in the “Camera Settings”-section. In addition, initial exposure- and gain levels can be determined automatically with a custom-written function. All settings, such as gain level and exposure time, stepper positions and processing options, can be saved as presets to be reloaded for subsequent scans. (iii) The “Scanner Setup”-section is used to configure the project output location on the connected PC. Defined presets of all relevant scanning parameters can be saved to be re-used at a later stage. The scan is also started or stopped from this sub-window. Simultaneous stacking and masking of captured images can be enabled, which may reduce the total time required for a single scan, provided that sufficient computational power is available. Due to the size of the uncompressed images, processing speed does not only depend on CPU core count and clock speed, but also on reading and writing speeds of the hard drive on which the output is stored. (iv) The “Stepper Control”-section allows users to move the scanner to any position within the available range, to home or reset the three axes, and to define the number of images to be taken, determined by the minimum and maximum position for the three axes, and the corresponding step size. These parameters generally vary with specimen size and may be adjusted accordingly. (v) The “Info”-section displays the progress of the scan, information about the status of the scanner, a timestamp.</p>
      <fig id="fig-2" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj.11155/fig-2</object-id>
        <label>Figure 2</label>
        <caption>
          <title>Graphical User Interface of <italic>scAnt</italic>.</title>
          <p>The Live View (maroon) displays the camera video stream. Bright-red areas indicate over-exposed pixels to aid the adjustment of the camera settings; a histogram of the RGB colour channels is displayed in the lower right corner. All relevant Camera Settings (red) such as exposure time and gain can be adjusted, the live view can be started, and images can be captured. The Scanner Setup-section (yellow) allows users to define an output folder, load existing project configurations (“presets”), write the scanner configuration to the specified output folder, choose stacking and masking methods, and start/abort the scanning process. The Stepper Control-section (navy) is used to change and configure the positions of the camera and mounted specimen. The position limits for the scanning process are also defined in this section. The <italic>X</italic>- and <italic>Z</italic>-axes (see <xref ref-type="fig" rid="fig-1">Figs. 1A</xref> and <xref ref-type="fig" rid="fig-1">1C</xref>) of the scanner can be returned to their home positions, and the zero-position of the <italic>Y</italic>-axis can be reset. The Info-section (blue) displays the progress of the current scan, and all events logged by the scanner in chronological order.</p>
        </caption>
        <graphic xlink:href="peerj-09-11155-g002"/>
      </fig>
    </sec>
    <sec>
      <title>Image processing</title>
      <p>The images recorded during a scan are processed in three successive steps: (i) A single EDOF image per perspective is produced by stacking all images per unique XY position; (ii) each EDOF image is masked to remove the image background; (iii) meta-data required for 3D reconstruction, such as sensor size, focal length and camera model, are written to the EDOF image files. All image processing steps are either implemented directly in Python or called from within Python, if they are based on pre-compiled software packages (details below). All processing scripts can be run from within the <italic>scAnt</italic> GUI, but are also available as standalone Python files available on our GitHub page (<uri xlink:href="https://github.com/evo-biomech/scAnt">https://github.com/evo-biomech/scAnt</uri>).</p>
      <sec>
        <title>Generating EDOF images</title>
        <p>As the depth of field is typically small compared to the dimensions of the specimen normal to the image plane, multiple images per unique XY position are required to capture every part of the body in focus (<xref ref-type="fig" rid="fig-3">Fig. 3A</xref>). These multiple images are then processed into a single EDOF image, using a series of processing steps (<xref ref-type="fig" rid="fig-3">Fig. 3B</xref>). First, images are aligned to correct for minor movements in the XY-plane of the camera rail. Second, the aligned images are blended, resulting in a single EDOF image on which all body parts are in focus. Image alignment is performed with <italic>Hugin</italic> (by Pablo d’Angelo, License: GNU GPLv2), an open-source toolbox for panorama photo stitching and High dynamic range (HDR) merging. High-quality merging requires to exclude all images which do not contain any relevant in-focus pixels. As it is not feasible to programme the scanner such that imaging is automatically constrained to relevant imaging planes, some images will fall into this category. For convenience, these images are automatically removed prior to the stacking process. The variance of a Laplacian 3 × 3 convolutional kernel is computed for each image, and images with a variance below a specified threshold are discarded. The optimal threshold depends on image noise, <italic>Z</italic>-axis step size, aperture and magnification, and hence has to be determined empirically; we provide a set of suitable parameter combinations as configuration presets (see GitHub). The selected images of a stack are then passed to <italic>Hugin</italic>’s align_image_stack function with a set of modifiers (see <xref rid="table-1" ref-type="table">Table 1</xref>).</p>
        <fig id="fig-3" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.7717/peerj.11155/fig-3</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Generation of Extended Depth Of Field (EDOF) images and masks.</title>
            <p>(A) Image capturing: Images with different focal planes are automatically captured by moving the camera along the <italic>Z</italic>-axis for each unique XY position (perspective). Between 20 and 50 images for each perspective are usually sufficient to cover the extent of a specimen of typical size normal to the imaging plane. The <italic>Leptoglossus zonatus</italic> specimen shown in (A) is captured at a step size of 18° about the <italic>Y</italic>-axis, but higher spatial resolutions can be configured in the GUI. (B) Image stacking: the images of each stack are aligned, and focus masks are generated to combine the sharpest regions of each image into a single EDOF image. (C) Image masking: These EDOF images are then “masked” to remove the image background. Noise is removed by applying a 5 × 5 Gaussian kernel, and edges are enhanced using Contrast Limited Adaptive Histogram Equalisation (CLAHE) (<xref rid="ref-76" ref-type="bibr">Zuiderveld, 1994</xref>). Subsequently, the largest contour within the image is extracted using a random forest-based edge detector (<xref rid="ref-20" ref-type="bibr">Dollar &amp; Zitnick, 2013</xref>). The resulting outline may include unwanted areas (highlighted in red) which are removed using adaptive thresholding. The noise introduced by the thresholding process is reduced with Connected Component Labelling (CCL) to produce a clean image mask (<xref rid="ref-23" ref-type="bibr">Fiorio &amp; Gustedt, 1996</xref>; <xref rid="ref-73" ref-type="bibr">Wu, Otoo &amp; Shoshani, 2005</xref>). (D) Image cut-out: The generated mask is applied to the EDOF image to create a single cut-out image per XY-orientation.</p>
          </caption>
          <graphic xlink:href="peerj-09-11155-g003"/>
        </fig>
        <table-wrap id="table-1" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.7717/peerj.11155/table-1</object-id>
          <label>Table 1</label>
          <caption>
            <title>List of modifiers required for image stack alignment.</title>
          </caption>
          <alternatives>
            <graphic xlink:href="peerj-09-11155-g007"/>
            <table frame="hsides" rules="groups" content-type="text">
              <colgroup span="1">
                <col span="1"/>
                <col span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <th rowspan="1" colspan="1">Modifier</th>
                  <th rowspan="1" colspan="1">Effect</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>-m</monospace>
                  </td>
                  <td rowspan="1" colspan="1">Use the field of view of the most distal captured focal plane as a reference, and align and rescale all successive images accordingly (required as the magnification of successive images increases as the camera moves towards the object; more sophisticated methods are available, provided that the exact position of the camera relative to the scanned object is known, see <xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>).</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>-x -y</monospace>
                  </td>
                  <td rowspan="1" colspan="1">Align each successive camera view in both <italic>x</italic>- and <italic>y</italic>-axes.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>-c 100</monospace>
                  </td>
                  <td rowspan="1" colspan="1">Set the number of extracted control points to 100. The number of control points affects the accuracy of the correction to image pitch and yaw, as well as of the lens distortion. While more control points generally increase the accuracy of the alignment process, the processing time increases exponentially. In some cases, excessive numbers of extracted control points can even decrease alignment quality due to inaccurate matches, amplified by shallow focus overlap between images.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>--gpu</monospace>
                  </td>
                  <td rowspan="1" colspan="1">Force the use of the GPU for remapping (optional)</td>
                </tr>
              </tbody>
            </table>
          </alternatives>
        </table-wrap>
        <p>Aligned images are exported to a temporary folder and subsequently merged into a single EDOF image using <italic>Enblend-Enfuse</italic> (licenced under GNU GPL 2+), initially developed by Andrew Mihal and now maintained by the <italic>Hugin</italic> developer team. As for image alignment, we provide a pre-defined set of modifiers to <italic>Hugin</italic>’s enfuse function (see <xref rid="table-2" ref-type="table">Table 2</xref>), chosen to represent a sensible compromise between quality and processing time. Most modifiers alter luminance control, owing to the history of <italic>Enblend-Enfuse</italic>, which was developed to perform exposure correction for HDR image processing.</p>
        <table-wrap id="table-2" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.7717/peerj.11155/table-2</object-id>
          <label>Table 2</label>
          <caption>
            <title>List of modifiers required for image stack merging.</title>
          </caption>
          <alternatives>
            <graphic xlink:href="peerj-09-11155-g008"/>
            <table frame="hsides" rules="groups" content-type="text">
              <colgroup span="1">
                <col span="1"/>
                <col span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <th rowspan="1" colspan="1">Modifier</th>
                  <th rowspan="1" colspan="1">Effect</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>--exposure-weight=0</monospace>
                  </td>
                  <td rowspan="1" colspan="1">Determines the contribution of pixels close to the ideal luminance of the blended image. A value of zero implies that all pixels contribute equally.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>--saturation-weight=0</monospace>
                  </td>
                  <td rowspan="1" colspan="1">Determines the contribution of highly-saturated pixels. A value of zero implies that all pixels contribute equally.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>--contrast-weight=1</monospace>
                  </td>
                  <td rowspan="1" colspan="1">Determines the contribution of pixels with high local contrast, which result from sharp edges. A value of one amplifies the contribution of these pixels.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>--hard-mask</monospace>
                  </td>
                  <td rowspan="1" colspan="1">The use of hard-masks increases the level of detail in the final image and reduces halos where the outlines of focal planes overlap, as it uses only information from the sharpest focal plane</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>--contrast-edge-scale=1</monospace>
                  </td>
                  <td rowspan="1" colspan="1">Determines the pre-processing function used prior to edge detection. A value of unity uses local-contrast-enhancement.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>--gray-projector=l-star</monospace>
                  </td>
                  <td rowspan="1" colspan="1">Determines the relative weight of the colour channels for greyscale conversion. By default, the colour channels are averaged, but if halos are visible in the EDOF image, 1-star can be set as the grey- projector instead (resulting in an emphasis of small contrast variations in highlights and shadows). The 1-star conversion is disabled by default, as it is more computationally expensive, but it can be activated within the scanner <bold>setup section</bold> of the GUI under the option “stacking method” (<xref ref-type="fig" rid="fig-2">Fig. 2</xref>).</td>
                </tr>
              </tbody>
            </table>
          </alternatives>
        </table-wrap>
      </sec>
      <sec>
        <title>Image masking</title>
        <p>The background of the EDOF images is removed in a masking step, which noticeably increases the quality of the mesh created during 3D reconstruction for at least three reasons. First, incorrect matching of background-features in the initial camera alignment step of the reconstruction process is avoided. Second, the number of “floating artefacts” is reduced. Third, the contours of the resulting model are retained more accurately. Masking is hence supported if not required by most of the existing photogrammetry software. The masking process is conducted in a sequence of five steps (see <xref ref-type="fig" rid="fig-3">Fig. 3C</xref>): (i) Enhancing contours of the EDOF image, (ii) approximating the specimen’s outline, (iii) removing superfluous infill, (iv) cleaning of the generated binary mask, and (v) applying it to the input image. This process is of comparable accuracy to alternative high-quality masking methods such as the use of backlighting (<xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>; <xref rid="ref-39" ref-type="bibr">Li &amp; Nguyen, 2019</xref>), but has the advantage of significantly reducing capturing and processing times, as well as the required hard-drive space (see <xref ref-type="supplementary-material" rid="supplemental-information">Supplemental Material</xref>).</p>
        <p>(i) The contours of each EDOF image are enhanced by increasing the local image contrast via Contrast Limited Adaptive Histogram Equalisation (CLAHE) (<xref rid="ref-76" ref-type="bibr">Zuiderveld, 1994</xref>). Subsequently, (ii) contours within each EDOF image are identified using a pre-trained random forest-based edge detector (<xref rid="ref-20" ref-type="bibr">Dollar &amp; Zitnick, 2013</xref>), which extracts the outline of the largest shape in the image—the specimen. In order to reduce the detector’s susceptibility to noise, Gaussian and median blurs are applied to each image prior to edge detection. Compared to a Sobel filter alone, the random forest-based edge detector returns more coherent outlines at comparable inference times, which is favourable when extracting the single largest shape in a given image (<xref rid="ref-10" ref-type="bibr">Bradski, 2000</xref>). The resulting outlines separate the specimen from its background, but they may include unwanted background areas, such as the area enclosed between a leg and the main body (<xref ref-type="fig" rid="fig-3">Fig. 3C</xref>). (iii) These incorrect assignments are removed with adaptive thresholding (akin to chroma- or luminance-keying to remove a specified colour region from an image), which works well as long as the lighting of the image centre is relatively uniform. (iv) In order to identify both incorrectly retained or removed areas of a size below a threshold determined by the image resolution, the mask is then cleaned, using connected component-labelling (<xref rid="ref-23" ref-type="bibr">Fiorio &amp; Gustedt, 1996</xref>; <xref rid="ref-73" ref-type="bibr">Wu, Otoo &amp; Shoshani, 2005</xref>). Finally, the resulting mask of the EDOF image is exported as a binary *.png file, in which white areas represent parts of the specimen, and black areas represent the background. (v) Not all photogrammetry software natively supports the use of masks, and additional cut-outs can be generated by applying the binary mask to the input EDOF image, either as an alpha channel or simply by setting all background pixels to a value of zero.</p>
      </sec>
      <sec>
        <title>Adding metadata to processed images</title>
        <p>In a final step, all relevant metadata is written into the generated EDOF image, using the open-source tool <italic>ExifTool</italic> (v. 12.00, developed by Phil Harvey, licenced under GPLv1+). Accurate 3D reconstruction relies on setup-specific image metadata, such as the camera’s sensor width and the focal length of the lens, to “undistort” the EDOF images, match features between images, and approximate the dimensions of the scanned object. The parameters are saved automatically (see <xref rid="table-3" ref-type="table">Table 3</xref>), and can be readily adjusted (the required parameters depend on the reconstruction software, see config.yaml in the <xref ref-type="supplementary-material" rid="supplemental-information">Supplemental Material</xref> on GitHub).</p>
        <table-wrap id="table-3" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.7717/peerj.11155/table-3</object-id>
          <label>Table 3</label>
          <caption>
            <title>List of key metadata parameters required for the photogrammetry process.</title>
          </caption>
          <alternatives>
            <graphic xlink:href="peerj-09-11155-g009"/>
            <table frame="hsides" rules="groups" content-type="text">
              <colgroup span="1">
                <col span="1"/>
                <col span="1"/>
                <col span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <th rowspan="1" colspan="1">Parameter</th>
                  <th rowspan="1" colspan="1">Value example</th>
                  <th rowspan="1" colspan="1">Effect</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>Make</monospace>
                  </td>
                  <td rowspan="1" colspan="1">FLIR</td>
                  <td rowspan="2" colspan="1">Required for correct assignment in the camera sensor database</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>Model</monospace>
                  </td>
                  <td rowspan="1" colspan="1">BFS-U3-200S6C-C</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>CameraSerialNumber</monospace>
                  </td>
                  <td rowspan="1" colspan="1">XXXXXXXX</td>
                  <td rowspan="1" colspan="1">All EDOF images must share the same CameraSerialNumber, so that the same camera intrinsics, solved once for a single scene, can be applied to every scene</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>FocalLength</monospace>
                  </td>
                  <td rowspan="1" colspan="1">35.0</td>
                  <td rowspan="1" colspan="1">Required to compute the correct magnification and distortion of each image. The FocalLength parameter refers to the value provided on the lens.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>FocalLengthIn35mmFormat</monospace>
                  </td>
                  <td rowspan="1" colspan="1">95.0</td>
                  <td rowspan="1" colspan="1">This parameter refers to the equivalent focal length on 35 mm film, and therefore depends on the sensor width of the camera.</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">
                    <monospace>SensorWidth</monospace>
                  </td>
                  <td rowspan="1" colspan="1">13.1</td>
                  <td rowspan="1" colspan="1">This parameter is required by some meshing software to correctly compute the camera intrinsics in addition to the FocalLengthIn35mm-parameter.</td>
                </tr>
              </tbody>
            </table>
          </alternatives>
        </table-wrap>
      </sec>
    </sec>
    <sec>
      <title>Reconstruction</title>
      <p>The result of a complete scan is a set of masked EDOF images from multiple perspectives. These images now need to be combined to reconstruct a 3D model of the imaged specimen. Numerous photogrammetry software packages are available for this task, be they commercial (e.g. 3DSOM (CDSL Ltd., London, UK), Metashape (Agisoft LLC, St. Petersburg, Russia), 3DF <italic>Zephyr</italic> (3DFLOW, Verona, Italy)), or open-source (e.g. <italic>Meshroom</italic> (<xref rid="ref-2" ref-type="bibr">AliceVision, 2018</xref>)). All these software packages have in common the stages in which a mesh is generated from the aligned images: (i) extraction of image features; (ii) matching extracted features between images; (iii) generating structure-from-motion (SfM), structure-from-silhouette (SfS), or a combination thereof; (iv) meshing and mesh-filtering; and (v) texturing. As a consequence, the procedure described below is to a large extent independent of the software choice. SfM reconstructs 3D models by matching extracted features, or “descriptors”, between images; additional information such as data from accelerometers, gyroscopes, or GPS may aid in solving the “camera motion”. The resulting triangulated features, or “sparse point clouds”, form the basis for reconstructing the three-dimensional geometry of the captured scene, and are often combined with approximated depth maps (<xref rid="ref-61" ref-type="bibr">Seitz et al., 2006</xref>; <xref rid="ref-33" ref-type="bibr">Jancosek &amp; Pajdla, 2010</xref>). In contrast, SfS instead uses silhouette, or “masked”, images to produce a visual hull (<xref rid="ref-38" ref-type="bibr">Laurentini, 1994</xref>; <xref rid="ref-34" ref-type="bibr">Jancosek &amp; Pajdla, 2011</xref>; <xref rid="ref-53" ref-type="bibr">Nguyen et al., 2014</xref>). SfM is particularly powerful when entire scenes are to be reconstructed, whereas SfS plays out its strength in the reconstruction of single objects in somewhat “sterile” conditions. We choose SfM, as it outperforms SfS in retaining structural detail such as concave shapes within the model that cannot be captured by image masks alone (<xref rid="ref-3" ref-type="bibr">Atsushi et al., 2011</xref>; <xref rid="ref-53" ref-type="bibr">Nguyen et al., 2014</xref>).</p>
      <p>We did the bulk of our work with the open-source solution <italic>Meshroom</italic> 2020, to maximise the accessibility of <italic>scAnt</italic>, and hence focus the following description on the reconstruction process with <italic>Meshroom</italic>. In order to demonstrate both the competitiveness and limitations of open-source software, we reconstructed some models with the “lite” version of the commercial software 3DF <italic>Zephyr</italic> V5.009 (~€120).</p>
      <p>In the first reconstruction step, camera perspectives are aligned based on their visual content, which is represented by extracted feature vectors. In (i) feature extraction with descriptors such as SIFT (<xref rid="ref-40" ref-type="bibr">Lindeberg, 2012</xref>), ORB (<xref rid="ref-71" ref-type="bibr">Wang et al., 2015</xref>) and AKAZE (<xref rid="ref-1" ref-type="bibr">Alcantarilla, Bartoli &amp; Davison, 2012</xref>; <xref rid="ref-64" ref-type="bibr">Tareen &amp; Saleem, 2018</xref>), there is a stereotypical trade-off between the number of descriptors (which determines the accuracy of the camera position matching and sampling density of triangulated features in subsequent steps) and the resulting processing times. Higher sampling densities consistently lead to higher model quality, and in particular, excel at retaining smaller structures such as antennae or thin leg segments. However, as long as each camera perspective is matched correctly in the subsequent step, the number of extracted features does not need to be excessively large. As the SfM calculations are based on the assumption of a constant scene and a moving camera, the background needs to be excluded using the generated masks. In <italic>Meshroom</italic>, image masks need to be applied directly to the input EDOF images prior to loading the “cut-outs” into the software (<xref ref-type="fig" rid="fig-3">Fig. 3D</xref>). A detailed description of the key parameters which require adjustment within <italic>Meshroom</italic> can be found on our GitHub page (see also <xref ref-type="supplementary-material" rid="supplemental-information">Supplemental Material</xref>).</p>
      <p>(ii) After extracting feature vectors from each camera perspective, they are matched, i.e. the extracted feature vectors are compared to propose a set of likely neighbouring views. (iii) These view-pairs are then directly compared to reconstruct the location and rotation of each camera perspective iteratively, and to triangulate extracted features—the core process of SfM reconstruction. The quality of the camera alignment is crucial for the final mesh quality, but is strongly dependent on the number and quality of the previously extracted features. As a consequence, the reconstruction may fail, particularly when scanning small objects or objects which are visually similar across multiple perspectives. In order to address this problem, we have written a Python script which computes an approximated SfM reconstruction of the camera positions to serve as a starting point. This script, <monospace>estimate_camera_positions.py</monospace>, takes the project configuration file generated during the scanning process as input and produces a *.<monospace>sfm</monospace> file with the solved camera intrinsics and transformation matrices for all EDOF images. This file can then be loaded into the open-source software <italic>Meshroom</italic> v2020 or later (<xref ref-type="fig" rid="fig-4">Fig. 4</xref>).</p>
      <fig id="fig-4" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj.11155/fig-4</object-id>
        <label>Figure 4</label>
        <caption>
          <title>Resulting camera positions of a completed scan.</title>
          <p>(A) Approximated camera positions computed from the settings provided by the project configuration file and the scanner dimensions. A transformation matrix for each camera is calculated and displayed as a vector (blue). The initial camera position is highlighted in red. The black dot in the centre represents the scanned object (B) Solved camera positions of the structure-from-motion (SfM) reconstruction performed with Meshroom.</p>
        </caption>
        <graphic xlink:href="peerj-09-11155-g004"/>
      </fig>
      <p>Camera perspective alignment may also be improved through the use of a textured reference object (<xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>). Although this approach can increase the quality of the alignment, the calibration is uniquely tied to the specific imaging positions used during the scan. In other words, the system would need to be recalibrated every time the step sizes are changed. In contrast, computing approximated positions from a new project file only takes a few seconds.</p>
      <p>The result of the SfM reconstruction is a set of aligned camera positions, as well as a “sparse point cloud” of the scanned specimen. (iv) On the basis of this information, a depth map is then computed for each camera view, and a cohesive mesh is calculated. We set the meshing parameters to the highest levels of detail, favouring retention of small features over noise-reduction and smoothing. An in-depth explanation of the meshing process within <italic>Meshroom</italic> can be found in the literature (<xref rid="ref-9" ref-type="bibr">Boykov &amp; Kolmogorov, 2004</xref>), <xref rid="ref-37" ref-type="bibr">Labatut &amp; Keriven (2009)</xref> and <xref rid="ref-33" ref-type="bibr">Jancosek &amp; Pajdla (2010</xref>, <xref rid="ref-34" ref-type="bibr">2011</xref>, <xref rid="ref-35" ref-type="bibr">2014)</xref>. Several mesh-filtering steps, specific to <italic>Meshroom</italic>, are then applied to the generated mesh:<list list-type="bullet"><list-item><p><bold>Mesh Filtering</bold> is primarily used to remove unwanted elements of the resulting mesh by defining a size-threshold. Due to the masking process (see above), the background noise is minimal, so that the number of excluded elements is usually small.</p></list-item><list-item><p><bold>Mesh Denoising</bold> improves mesh topology and is best applied prior to <bold>Mesh Decimation</bold> or <bold>Mesh Resampling</bold>. Rather than reducing detail across the entire mesh, <bold>Denoising</bold> smoothens the input mesh across large surfaces, while leaving sharp edges intact, so decreasing noise without simplifying the mesh’s overall topology. <italic>Meshroom</italic>’s implementation of de-noising is based on the filtering methods described in <xref rid="ref-75" ref-type="bibr">Zhang et al. (2019)</xref>.</p></list-item><list-item><p><bold>Mesh Resampling</bold> can be used to reduce the number of vertices while retaining the overall shape, volume, and boundaries of the mesh. In contrast to <bold>Mesh Decimation</bold>, i.e. the deletion of redundant vertices, the mesh is rebuilt entirely using elements of predetermined dimensions. Generally, only one of the two methods should be used.</p></list-item></list></p>
      <p>Last, (v) an image texture is generated by projecting the colour channel information of all undistorted camera views back onto the model. Depending on the size of the model and the number of faces, different unwrapping methods may be used to fit the colour information into a single image texture. We used the Least Squares Conformal Maps unwrapping method (<xref rid="ref-41" ref-type="bibr">Lévy et al., 2002</xref>) to produce image textures of 4,096 px * 4,096 px for meshes with less than 600,000 faces. For larger meshes, we used mosaic texturing. In our trials, LSCM performed consistently better than mosaic texturing methods in retaining colour relationships between neighbouring regions. In addition, the resulting model textures can be edited more easily in subsequent manual post-processing steps.</p>
      <p>We accurately replicated the lighting and camera setup of the scanner in <italic>Blender</italic> v2.8 to compare the quality and accuracy of the model to the original images. We also matched the position of exemplary EDOF and masked images, in order to demonstrate the absence of distortion and the level of detail retained both in topology and texture (see <xref ref-type="supplementary-material" rid="supplemental-information">Supplemental Material</xref>).</p>
    </sec>
    <sec>
      <title>3D model post-processing and rigging</title>
      <p>All manual post-processing was performed with <italic>Blender</italic> v2.8. The mesh of the mounting pin was removed, either by selecting the vertices connecting to the insect mesh and removing all connected faces, or by a simple Boolean intersection. Subsequently, the resulting hole was filled by collapsing the surrounding vertices to their centre. If desired, basic or Laplacian smoothing modifiers, as well as manual smoothing via sculpting can be used to improve the local mesh quality (<xref ref-type="fig" rid="fig-5">Fig. 5A</xref>).</p>
      <fig id="fig-5" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj.11155/fig-5</object-id>
        <label>Figure 5</label>
        <caption>
          <title>Mesh post-processing steps.</title>
          <p>(A) Mesh cleaning process (here for <italic>Pachnoda marginata)</italic>. (i) The mounting pin and any floating artefacts are removed from the generated mesh, and (ii) the topology is cleaned, using Laplacian smoothing, before (iii) producing the final mesh. (B) Thin, planar structures, such as some wings, are difficult to reconstruct but may be added to the models by approximating them as 2D objects. (i) Original, generated mesh (<italic>Sympetrum striolatum</italic>). (ii) cut-outs of each wing are taken from masked EDOF images. (iii) The cut-outs are used as image textures for planes of the corresponding size, containing an alpha layer, and are merged with the cleaned mesh. (C) Rigging process. Insets show close-ups of the legs and abdomen at each stage of the process (<italic>Sungaya inexpectata</italic>). (i) Reconstructed mesh pose as-scanned. (ii) Assigned armature superimposed over the untextured mesh. Joints are placed at each abdomen intersection as well as at every identified joint location to create a fully articulated model (in principle, the number of joints will be well defined for body parts such as legs, but may be harder to define for body parts such as antennae, where an appropriate number of joints may be determined by the desired flexibility and degree of accuracy for the posing). (iii) After assigning the surrounding vertices to their respective rigid bodies, the model can be posed arbitrarily. As an illustrative example, we extended the curled abdomen and posed the legs to reflect the posture of a freely standing stick insect.</p>
        </caption>
        <graphic xlink:href="peerj-09-11155-g005"/>
      </fig>
      <p>Thin and transparent structures, such as wings, are challenging for SfM-based reconstruction and often appear fragmented (<xref ref-type="fig" rid="fig-5">Fig. 5B</xref> and see <xref rid="ref-53" ref-type="bibr">Nguyen et al., 2014</xref>; <xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>). However, structures such as wings may be approximately described as two-dimensional, provided that their thickness and curvature is small compared to the resolution of the scans. In such instances, there exists an easy yet high-quality option for reconstruction: wings can be added to the model during post-processing, by re-projecting the wing structure from EDOF images onto image planes, using the corresponding mask as an alpha-channel (<xref ref-type="fig" rid="fig-5">Fig. 5B</xref>). An orthogonal top-down view of the fragmented structure or sparse point cloud of the scanned specimen serves as the wing outline. The cut-out structure is then manually transformed in scale and rotation to match the original structure, using an image editing program such as GIMP (licenced under GPLv3+); the scaled wing is then set as the image texture of a plane object with corresponding dimensions. By blending a diffuse shader for the colour information with a transparent shader for the opacity, the wings are added to the textured mesh, using the image plane’s alpha channel as an input. Subsequently, all unconnected vertices of the fragmented wing are removed, using the process outlined above for the removal of the mounting pin. This process retains key visual information of the wing such as colour and venation patterns, but of course does not correspond to a “true” 3D reconstruction, as the wing is simplified as a planar structure. This approximation may be valid in some cases (see <xref ref-type="fig" rid="fig-6">Fig. 6</xref>, <italic>Orthomeria versicolor</italic>), but will correspond to a significant simplification in others. For a better overview of the resulting model quality, refer to our <xref ref-type="supplementary-material" rid="supplemental-information">Sketchfab Model collection</xref>.</p>
      <fig id="fig-6" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj.11155/fig-6</object-id>
        <label>Figure 6</label>
        <caption>
          <title>Examples of fully textured 3D models created with <italic>scAnt</italic>.</title>
          <p>All models have been reconstructed with either Meshroom 2020 or 3DF Zephyr Lite (<xref rid="table-4" ref-type="table">Table 4</xref>), and were post-processed and <bold/>rendered in blender v2.8, using cycles. Boxes on the right illustrate various quality criteria for model reconstruction (A) Ventral, sagittal, anterior, and posterior view of <italic>Porcellio scaber</italic>, demonstrating the achievable model quality for the smallest scanned specimen. (B) Front right tarsus and claws of a <italic>Dinomyrmex gigas</italic> specimen, illustrating the mesh density, as well as the texture quality at small scales. (C) Rigged, untextured mesh after the pin has been removed and mesh smoothing has been applied to the model. (D) and (E) Close-ups of the colour detail retained for the elytra and thorax of the reflective <italic>Metallyticus splendidus</italic> specimen. Topology and texture detail of a <italic>Sungaya inexpectata</italic> model reconstructed with Meshroom (F) and 3DF Zephyr Lite (G). Zephyr captures fine surface texture better than Meshroom. All models, as well as additional details on SfM reconstruction and post-processing steps can be found on Sketchfab.</p>
        </caption>
        <graphic xlink:href="peerj-09-11155-g006"/>
      </fig>
      <table-wrap id="table-4" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj.11155/table-4</object-id>
        <label>Table 4</label>
        <caption>
          <title>Overview of scanned specimens.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="peerj-09-11155-g010"/>
          <table frame="hsides" rules="groups" content-type="text">
            <colgroup span="1">
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
              <col span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Species</th>
                <th rowspan="1" colspan="1">Length of longest axis (excluding antennae and legs) (mm)</th>
                <th rowspan="1" colspan="1">Authority</th>
                <th rowspan="1" colspan="1">Reconstructed using Meshroom (M), 3DF Zephyr Lite (Z)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>Amphipyra pyramidea</italic>
                </td>
                <td rowspan="1" colspan="1">20.4</td>
                <td rowspan="1" colspan="1">Linnaeus 1758</td>
                <td rowspan="1" colspan="1">M &amp; Z<xref ref-type="fn" rid="table-4fn1">*</xref></td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>Atta vollenweideri</italic>
                </td>
                <td rowspan="1" colspan="1">9.5</td>
                <td rowspan="1" colspan="1">Forel 1893</td>
                <td rowspan="1" colspan="1">Z<xref ref-type="fn" rid="table-4fn1">*</xref></td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>Blatta orientalis</italic>
                </td>
                <td rowspan="1" colspan="1">27.5</td>
                <td rowspan="1" colspan="1">Linnaeus 1758</td>
                <td rowspan="1" colspan="1">Z<xref ref-type="fn" rid="table-4fn1">*</xref></td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>Diacamma indicum</italic>
                </td>
                <td rowspan="1" colspan="1">9.3</td>
                <td rowspan="1" colspan="1">Santschi 1920</td>
                <td rowspan="1" colspan="1">Z<xref ref-type="fn" rid="table-4fn1">*</xref></td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>Dinomyrmex gigas</italic>
                </td>
                <td rowspan="1" colspan="1">22.6</td>
                <td rowspan="1" colspan="1">Smith 1858</td>
                <td rowspan="1" colspan="1">Z<xref ref-type="fn" rid="table-4fn1">*</xref></td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>Leptoglossus zonatus</italic>
                </td>
                <td rowspan="1" colspan="1">19.4</td>
                <td rowspan="1" colspan="1">Dallas 1852</td>
                <td rowspan="1" colspan="1">Z<xref ref-type="fn" rid="table-4fn1">*</xref></td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>Metallyticus splendidus</italic>
                </td>
                <td rowspan="1" colspan="1">30.0</td>
                <td rowspan="1" colspan="1">Westwood 1835</td>
                <td rowspan="1" colspan="1">M, Z<xref ref-type="fn" rid="table-4fn1">*</xref></td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>Myathropa florea</italic>
                </td>
                <td rowspan="1" colspan="1">14.8</td>
                <td rowspan="1" colspan="1">Linnaeus 1758</td>
                <td rowspan="1" colspan="1">M<xref ref-type="fn" rid="table-4fn1">*</xref>, Z</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>Orthomeria versicolor</italic>
                </td>
                <td rowspan="1" colspan="1">37.5</td>
                <td rowspan="1" colspan="1">Redtenbacher 1906</td>
                <td rowspan="1" colspan="1">M<xref ref-type="fn" rid="table-4fn1">*</xref></td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>Pachnoda marginata</italic>
                </td>
                <td rowspan="1" colspan="1">26.7</td>
                <td rowspan="1" colspan="1">Kolbe 1906</td>
                <td rowspan="1" colspan="1">M<xref ref-type="fn" rid="table-4fn1">*</xref></td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>Porcellio scaber</italic>
                </td>
                <td rowspan="1" colspan="1">7</td>
                <td rowspan="1" colspan="1">Latreille 1804</td>
                <td rowspan="1" colspan="1">M, Z<xref ref-type="fn" rid="table-4fn1">*</xref></td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>Sungaya inexpectata</italic>
                </td>
                <td rowspan="1" colspan="1">36.3</td>
                <td rowspan="1" colspan="1">Zompro 1996</td>
                <td rowspan="1" colspan="1">M<xref ref-type="fn" rid="table-4fn1">*</xref>, Z<xref ref-type="fn" rid="table-4fn1">*</xref></td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>Sympetrum striolatum</italic>
                </td>
                <td rowspan="1" colspan="1">28.3</td>
                <td rowspan="1" colspan="1">Charpentier 1840</td>
                <td rowspan="1" colspan="1">Z<xref ref-type="fn" rid="table-4fn1">*</xref></td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="table-4fn">
            <p>
              <bold>Notes:</bold>
            </p>
          </fn>
          <fn id="table-4fn1" fn-type="other">
            <label>*</label>
            <p>Indicates the mesh shown in <xref ref-type="fig" rid="fig-6">Fig. 6</xref> (side by side comparisons available on Sketchfab).</p>
          </fn>
          <fn id="table-4fn2" fn-type="other">
            <p>The length of the longest body axis is measured on the final mesh within Blender v2.8. When both Meshroom (<bold>M</bold>) and 3DF Zephyr (<bold>Z</bold>) are listed in the right column, separate meshes were created from the same masked EDOF images for comparison.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>In a final step, the mesh may be rigged, by which we mean the assignment of virtual joints and rigid body parts, subsequently allowing users to pose the models (<xref ref-type="fig" rid="fig-5">Fig. 5C</xref>); this process is possible in arthropods, because their bodies are reasonably approximated as a linked series of rigid bodies. As long as joint-location and type can be defined with reasonable accuracy, rigging allows for anatomically correct posing. In order to rig a scan, we create an armature inside the mesh and define joint types and locations. In order to ensure that labelled segments are treated as rigid bodies, we use weight painting, which avoids incorrect deformation arising from smoothing algorithms which deform tissue locally.</p>
      <p>In order to demonstrate that rigging of 3D models is not only aesthetically pleasing but also scientifically useful, we asked seven individuals of varying degree of experience (undergraduate students, PhD students, Postdocs) to measure a set of three anatomical parameters of a specimen of <italic>Sungaya inexpectata</italic>: Head width (HW), femur length (FL, right front leg), and abdomen length (AL, see also <xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>). These measurements were taken either on the physical specimen, using a LEICA Z6 Microscope (Leica Camera AG, Wetzlar Germany), in the following referred to as a “2D measurement”, or on the final 3D model using the internal measuring tool in <italic>Blender</italic> v2.8. In both cases, participants were allowed to rotate the specimen and choose their preferred magnification freely. We included HW and FL as they are relatively easy to measure and unlikely to be subject to large parallax error. As a consequence, we expect the difference between measurements from 2D vs 3D data to be small. In contrast, the abdomen of the original specimen was curved in two axes (<xref ref-type="fig" rid="fig-5">Fig. 5C</xref>); we removed this curvature via rigging, straightening the abdomen to a position similar to that observed in live animals, so enabling us to test if the flexibility provided by rigging can help to increase measurement precision.</p>
    </sec>
    <sec>
      <title>Accuracy evaluation</title>
      <p>In order to quantify the reconstruction accuracy achievable with <italic>scAnt</italic>, we performed a set of measurements on certified gauge blocks, as described by <xref rid="ref-29" ref-type="bibr">Gallo, Muzzupappa &amp; Bruno (2014)</xref>. Grade 0 gauge blocks (uncertainty of ± 0.00008 mm, UKAS certified) of 1.50, 1.10, 1.05, and 1.00 mm were scanned in pairs in a 3D printed tray with a rectified surface to create step-sizes of 500, 100 and 50 µm, respectively. The gauge cubes were then reconstructed using the parameters described in the <italic>Meshroom</italic> Guide on our GitHub Page. The step-sizes were measured in <italic>Blender</italic> v2.8, using a custom-written Python script. In brief, we measured the average vertical distance between all vertices of the top plane of the step cube (1.50, 1.00 and 1.01 mm) and the 1.00 mm reference gauge cube.</p>
    </sec>
    <sec>
      <title>Scanned species</title>
      <p>In order to demonstrate the versatility and quality of <italic>scAnt</italic>, we created 3D models for a series of arthropods, selected to present various challenges such as size, transparency, hairy surfaces, or iridescence (see <xref rid="table-4" ref-type="table">Table 4</xref>; <xref ref-type="fig" rid="fig-6">Fig. 6</xref>, <bold>Sketchfab</bold>).</p>
    </sec>
  </sec>
  <sec sec-type="results|discussion">
    <title>Results and discussion</title>
    <p>We created several models to demonstrate the quality achievable with <italic>scAnt</italic> (see <xref ref-type="fig" rid="fig-6">Fig. 6</xref> and Sketchfab). The reconstruction error, as estimated from the reconstruction of pairs of grade 0 gauge cubes, is approximately 12–15 µm across step sizes (see <xref rid="table-5" ref-type="table">Table 5</xref>). In the following, we briefly address some key features of these models to demonstrate their quality, highlight key challenges and difficulties which users may encounter, and outline the current performance limits of <italic>scAnt</italic>. We then proceed to describe how 3D models generated with <italic>scAnt</italic> may be used in research, outreach and conservation.</p>
    <table-wrap id="table-5" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.7717/peerj.11155/table-5</object-id>
      <label>Table 5</label>
      <caption>
        <title>Distance measurements on certified gauge cubes.</title>
      </caption>
      <alternatives>
        <graphic xlink:href="peerj-09-11155-g011"/>
        <table frame="hsides" rules="groups" content-type="text">
          <colgroup span="1">
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
            <col span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Step-size (mm)</th>
              <th rowspan="1" colspan="1">0.500</th>
              <th rowspan="1" colspan="1">0.100</th>
              <th rowspan="1" colspan="1">0.050</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Measured mean (mm)</td>
              <td rowspan="1" colspan="1">0.517</td>
              <td rowspan="1" colspan="1">0.112</td>
              <td rowspan="1" colspan="1">0.065</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Std (mm)</td>
              <td rowspan="1" colspan="1">0.005</td>
              <td rowspan="1" colspan="1">0.006</td>
              <td rowspan="1" colspan="1">0.005</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Relative error</td>
              <td rowspan="1" colspan="1">3.45%</td>
              <td rowspan="1" colspan="1">12.67%</td>
              <td rowspan="1" colspan="1">30.83%</td>
            </tr>
          </tbody>
        </table>
      </alternatives>
      <table-wrap-foot>
        <fn id="table-5fn">
          <p>
            <bold>Note:</bold>
          </p>
        </fn>
        <fn id="table-5fn1" fn-type="other">
          <p>The reported measured mean refers to the distance between the vertices of the top plane of the step-cube (1.50, 1.10 and 1.05 mm respectively) and the reference cube (1.00 mm).</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <sec>
      <title>Quality, challenges and limitations</title>
      <p>The quality of the final models is determined by the quality of the EDOF images and the reconstruction process. The former hinges on the quality of the used camera and lens (to first-order; image processing during stacking introduces second-order effects on quality); high-quality cameras and lenses are available where the budget allows, and the resolution and scanner dimensions can easily be adjusted to specific needs. Hence, model quality is mostly limited by the quality of the 3D reconstruction, for which we identified four major challenges.</p>
      <p>First, there exists a lower size limit for features that can be retained during reconstruction. The smallest features that can be retained depend on the resolution of the input images, the quality of the generated masks, the number of camera positions, the quality of meshing algorithms and—crucially—the size of the scanned animal (<xref ref-type="fig" rid="fig-6">Fig. 6A</xref>): The larger the animal, the coarser the absolute resolution. As an illustrative example, the thinnest preserved structures of the <italic>Dinomyrmex gigas</italic> specimen with a body length of 27.5 mm are the claws, with a width of approximately 30 µm, or about three times the accuracy of the reconstruction (they are likely even thinner towards their tips; <xref ref-type="fig" rid="fig-6">Figs. 6B</xref> and <xref ref-type="fig" rid="fig-6">6C</xref>). The choice of reconstruction software also notably contributes to the smallest retained features; Using the same input images, the low-cost commercial software 3DF <italic>Zephyr</italic> Lite generated a higher level of topology detail than the open-source option <italic>Meshroom</italic> (see <xref ref-type="fig" rid="fig-6">Figs. 6F</xref> and <xref ref-type="fig" rid="fig-6">6G</xref>), and thin segments such as tarsi were less likely to be fragmented. We provide a number of direct comparisons between the two software options on Sketchfab.</p>
      <p>The reconstruction of small features is particularly challenging where large size differences between feature and body size are combined with high local curvature, such as for the proboscis of <italic>Leptoglossus zonatus</italic> (<xref ref-type="fig" rid="fig-6">Fig. 6</xref>, second row and see Sketchfab as well as <xref ref-type="supplementary-material" rid="supplemental-information">Supplemental Material</xref>). The reconstructed proboscis was partially fragmented, which required manual post-processing in the form of manually joining the segments into a coherent mesh with <italic>Blender</italic>. Generally, structures with high-aspect ratios, such as hairs, may be visible in EDOF images, but fail to reconstruct. The most straightforward way to address this issue is to increase the number of camera positions during the scan, which increases the number of matched descriptors, but comes at the cost of longer processing times. In order to counter this increase in processing time, orientations near the extreme X-axis angles could be sampled more sparsely: an approximately constant angular distance between neighbouring views would maximise the unique information per image, and reduce the total number of images for the same spherical coverage <xref rid="ref-53" ref-type="bibr">Nguyen et al. (2014)</xref>. As SfM-based approaches require a sufficient number of clearly defined features to preserve a structure, increasing the contrast using additional filtering methods such as Wallis filters may also help to address this issue (<xref rid="ref-29" ref-type="bibr">Gallo, Muzzupappa &amp; Bruno, 2014</xref>). Alternative meshing methods, such as Visibility-Consistent Meshing, may retain thin features without the need for manual post-processing (see <xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>), but are currently only available as an experimental feature in commercial software such as of PhotoScan Pro (Agisoft LLC). The quality of EDOF images may also be improved further by enforcing perspective consistent images, which requires high-accuracy calibration of camera positions during image stacking or re-designing the system as a fixed lens setup (<xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>; <xref rid="ref-39" ref-type="bibr">Li &amp; Nguyen, 2019</xref>). Such additional stacking and calibration options may be included in future updates of <italic>scAnt</italic>.</p>
      <p>Second, highly reflective surfaces, such as the head capsule of the leaf-cutter ant, <italic>Atta vollenweideri</italic> (<xref ref-type="fig" rid="fig-6">Fig. 6</xref>, first row), or of the cockroach, <italic>Blatta orientalis</italic> (<xref ref-type="fig" rid="fig-6">Fig. 6</xref>, third row) may ironically lead to a comparably rough mesh topology, as the reflection introduces noise in the reconstruction process. This noise is caused by a combination of small variations in appearance with viewing angle and low surface detail, so reducing the number of features matched between views, and leading to incorrect depth maps. In contrast, the reconstructions of seemingly more challenging reflective and even iridescent structures with high levels of surface detail are far less noise ridden (<xref ref-type="fig" rid="fig-6">Figs. 6D</xref> and <xref ref-type="fig" rid="fig-6">6E</xref>). Noise may be reduced using Mesh de-noising, manual smoothing, or similar post-processing techniques, but the specimen may then appear artificially smooth, and it is difficult to achieve a photorealistic appearance. Two alternatives exist. First, the application of fine powder to the specimen’s surface prior to scanning may reduce surface reflectivity, but it may come at the cost of a less realistic optical impression when the model is viewed in variable lighting conditions. Second, the uniformity of the lighting conditions may be improved further, but some reflections from the camera lens or the dome opening are impossible to avoid.</p>
      <p>Third, transparent structures, such as some wings, can be difficult to reconstruct in 3D for at least three reasons. First, because they may not be recognised as part of the body, second, because their appearance varies strongly with viewing angle, and third, because pixels may be wrongly assigned to the dorsal or ventral side of the wing. Two solutions to this problem exist: First, the generated masked EDOF images can be used to re-project the fragmented structures with relatively little effort (<xref ref-type="fig" rid="fig-5">Fig. 5B</xref>). However, this approach is limited to thin and approximately planar wings, as it is unable to capture any out-of-plane variation in wing morphology. Second, alternative meshing methods such as SfS, or Visibility-Consistent Meshing can be used to enforce the inclusion of such regions, but are currently only implemented in commercial software, such as Agisoft PhotoScan Pro. To provide some perspective, the cost of this software exceeds the costs of the scanner, including camera and lens, by a factor of three.</p>
      <p>Fourth, model creation is time-intense. High-throughput model creation requires (i) scripting of the complete reconstruction process, which is possible with most photogrammetry software, including <italic>Meshroom</italic>; and (ii) a reduction in the total processing time. As an illustrative example, all models in <xref ref-type="fig" rid="fig-6">Fig. 6</xref> were created with 180–360 EDOF images recorded at 20 MP resolution; Capturing and simultaneously processing the EDOF images using our GUI takes between 2 and 5 h, depending on the number of camera positions (intel core i7, 8 Core processor at 4.3 GHz, 32 GB DDR4 RAM, NVIDIA RTX 2070 QMax). The reconstruction of 3D models took between 3 and 12 h (intel core i9, 12 Core processor at 4.0 GHz, 64 GB DDR4 RAM, 2 × NVIDIA RTX 2080 Ti). Total model creation time may be reduced by sacrificing resolution (in step size or pixel density), by using more powerful computers, or by further improvements in the algorithms underlying the reconstruction.</p>
      <p>Notwithstanding these challenges and limitations, the quality of the generated models is sufficient to enable their use in a number of different activities, including research, outreach, and conservation, as we briefly outline in the following.</p>
    </sec>
    <sec>
      <title>Research</title>
      <sec>
        <title>Morphometry</title>
        <p>The development of increasingly sophisticated comparative and geometric morphometric methods has revived the popularity of morphometry in entomology (see e.g. <xref rid="ref-65" ref-type="bibr">Tatsuta, Takahashi &amp; Sakamaki, 2018</xref>). The immediate appeal of 3D models in morphometric studies is two-fold:</p>
        <p>First, 3D models enable measurements which are difficult or even impossible to obtain from 2D images. Such measurements include area measurements of structures with complex shape, or measurements of volumes. For example, it is straightforward to extract body surface area and body volume from our 3D models in <italic>Blender</italic> v2.8, using its default measurement toolbox and 3D-Print add-on (we assigned a uniform thickness to the wings, equivalent to the thickness of the reconstructed fragmented wing). An ordinary least squares regression of <inline-formula id="ieqn-1"><alternatives><inline-graphic xlink:href="peerj-09-11155-i001.jpg"/><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}lo{g_{10}}\end{document}</tex-math><mml:math id="mml-ieqn-1"><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">g</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>-transformed data yields a slope of 0.61 (95% CI [0.33–0.98]), consistent with isometry, and the results from <xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref> (data extracted with WebPlotDigitzer, slope 0.59, 95% CI [0.49–0.68]). Notwithstanding the ease with which such data can be obtained from 3D models, two sources of error require attention. (i) The relative measurement error, defined as the ratio between the measured quantity and the resolution, is usually small for linear measurements, but it is additive, and can hence become sizeable for area and volume measurements, so reducing statistical power. In practice, this error is nevertheless only relevant for small structures, and is unlikely to bias scaling analyses conducted across multiple orders of magnitude, as there is no <italic>a priori</italic> reason to assume that it is systematic. (ii) Variations in fractal dimension across specimens may introduce a potential systematic error: area (and volume) measurements may rely on perimeter measurements, which can vary strongly with image resolution (<xref rid="ref-42" ref-type="bibr">Mandelbrot, 1967</xref>, <xref rid="ref-43" ref-type="bibr">1982</xref>). Arthropods are unlikely to have a large fractal dimension, and occupy only a small range of physical length scales (from atomic dimensions to a few centimetres, say). However, body surfaces are often highly structured, so that a correlation between resolution and estimated area/volume is likely. Methods to estimate fractal dimension exist (e.g. <xref rid="ref-51" ref-type="bibr">Neal &amp; Russ, 2012</xref>), and this issue needs further attention to enable a robust comparison of areas or volumes across scans or species.</p>
        <p>Second, 3D models may increase measurement accuracy and precision even for measurements which can in principle be taken from 2D images (see, e.g. <xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>). Indeed, intra-observer errors can be large even for seemingly straightforward measurements (<xref rid="ref-69" ref-type="bibr">Viscardi, Sakamoto &amp; Sigwart, 2010</xref>). We replicated the finding of <xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref> that the coefficient of variation may be reduced in some (but not all) cases when measurements are taken from 3D models instead of 2D images (see <xref rid="table-6" ref-type="table">Table 6</xref>). We demonstrated further that the increase in precision is largest for structures which can be re-positioned via rigging in the 3D model (see <xref rid="table-6" ref-type="table">Table 6</xref> and <xref ref-type="fig" rid="fig-5">Fig. 5C</xref>).</p>
        <table-wrap id="table-6" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.7717/peerj.11155/table-6</object-id>
          <label>Table 6</label>
          <caption>
            <title>Distances measured on a <italic>Sungaya inexpectata</italic> specimen.</title>
          </caption>
          <alternatives>
            <graphic xlink:href="peerj-09-11155-g012"/>
            <table frame="hsides" rules="groups" content-type="text">
              <colgroup span="1">
                <col span="1"/>
                <col span="1"/>
                <col span="1"/>
                <col span="1"/>
                <col span="1"/>
                <col span="1"/>
                <col span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <th rowspan="1" colspan="1"/>
                  <th colspan="2" rowspan="1">Head width (mm)</th>
                  <th colspan="2" rowspan="1">Femur length (mm)</th>
                  <th colspan="2" rowspan="1">Abdomen length (mm)</th>
                </tr>
                <tr>
                  <th rowspan="1" colspan="1"><italic>n</italic> = 7</th>
                  <th rowspan="1" colspan="1">2D</th>
                  <th rowspan="1" colspan="1">3D</th>
                  <th rowspan="1" colspan="1">2D</th>
                  <th rowspan="1" colspan="1">3D</th>
                  <th rowspan="1" colspan="1">2D (curved)</th>
                  <th rowspan="1" colspan="1">3D (straight)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td rowspan="1" colspan="1">min</td>
                  <td rowspan="1" colspan="1">3.04</td>
                  <td rowspan="1" colspan="1">3.17</td>
                  <td rowspan="1" colspan="1">6.08</td>
                  <td rowspan="1" colspan="1">6.44</td>
                  <td rowspan="1" colspan="1">16.58</td>
                  <td rowspan="1" colspan="1">19.43</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">max</td>
                  <td rowspan="1" colspan="1">3.17</td>
                  <td rowspan="1" colspan="1">3.22</td>
                  <td rowspan="1" colspan="1">6.42</td>
                  <td rowspan="1" colspan="1">6.70</td>
                  <td rowspan="1" colspan="1">19.78</td>
                  <td rowspan="1" colspan="1">20.10</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">µ</td>
                  <td rowspan="1" colspan="1">3.12</td>
                  <td rowspan="1" colspan="1">3.20</td>
                  <td rowspan="1" colspan="1">6.25</td>
                  <td rowspan="1" colspan="1">6.57</td>
                  <td rowspan="1" colspan="1">18.15</td>
                  <td rowspan="1" colspan="1">19.59</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">σ</td>
                  <td rowspan="1" colspan="1">0.04</td>
                  <td rowspan="1" colspan="1">0.01</td>
                  <td rowspan="1" colspan="1">0.10</td>
                  <td rowspan="1" colspan="1">0.07</td>
                  <td rowspan="1" colspan="1">1.16</td>
                  <td rowspan="1" colspan="1">0.20</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">
                    <italic>c<sub>V</sub></italic>
                  </td>
                  <td rowspan="1" colspan="1">0.012</td>
                  <td rowspan="1" colspan="1">0.004</td>
                  <td rowspan="1" colspan="1">0.016</td>
                  <td rowspan="1" colspan="1">0.011</td>
                  <td rowspan="1" colspan="1">0.064</td>
                  <td rowspan="1" colspan="1">0.010</td>
                </tr>
                <tr>
                  <td rowspan="1" colspan="1">statistic</td>
                  <td colspan="2" rowspan="1"><italic>D<sub>AD</sub></italic> = 4.85, <italic>p</italic> = 0.028</td>
                  <td colspan="2" rowspan="1"><italic>D<sub>AD</sub></italic> = 0.85, <italic>p</italic> = 0.36</td>
                  <td colspan="2" rowspan="1"><italic>D<sub>AD</sub></italic> = 12.50, <italic>p</italic> &lt; 0.0001</td>
                </tr>
              </tbody>
            </table>
          </alternatives>
          <table-wrap-foot>
            <fn id="table-6fn">
              <p>
                <bold>Note:</bold>
              </p>
            </fn>
            <fn id="table-6fn1" fn-type="other">
              <p>Measurements have been performed using a LEICA Z6 Microscope on the specimen itself (2D), or blender v2.8 on the generated model (3D) (<italic>n</italic> = 7 different observers). Listed are the shortest (min) and longest (max) distances, the mean (µ), the standard deviation (σ) and the coefficient of variation (<italic>c<sub>v</sub></italic>). We tested for differences in the coefficient of variation using the asymptotic test for the equality of coefficients of variation from k populations after Feltz and Miller, 1996, implemented in the R-package cvequality (<xref rid="ref-77" ref-type="bibr">Marwick &amp; Krishnamoorthy, 2019</xref>), for which we provide the test-statistic (D<sub>AD</sub>), and the associated <italic>p</italic>-value.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>As a consequence of both advantages outlined above, morphometric data extracted from 3D models may be more versatile and accurate, increase statistical power, and reduce the ambiguity present in 2D images (<xref rid="ref-60" ref-type="bibr">Roth, 1993</xref>; <xref rid="ref-14" ref-type="bibr">Cardini, 2014</xref>; <xref rid="ref-30" ref-type="bibr">Gould, 2014</xref>; <xref rid="ref-24" ref-type="bibr">Fruciano, 2016</xref>; <xref rid="ref-13" ref-type="bibr">Buser, Sidlauskas &amp; Summers, 2018</xref>; <xref rid="ref-5" ref-type="bibr">Bas &amp; Smith, 2019</xref> but see <xref rid="ref-17" ref-type="bibr">Courtenay et al., 2018</xref>; <xref rid="ref-48" ref-type="bibr">McWhinnie &amp; Parsons, 2019</xref>). While the use of 3D data is still somewhat limited by the power of available statistical methods for subsequent analyses (<xref rid="ref-56" ref-type="bibr">Polly &amp; Motz, 2016</xref>), increasingly advanced methods are available (<xref rid="ref-4" ref-type="bibr">Bardua et al., 2019</xref>), and 3D morphometry is expected to grow in importance.</p>
      </sec>
      <sec>
        <title>Machine learning-based segmentation, detection and tracking</title>
        <p>Computer vision and machine learning are quickly finding their way into the standard methodological toolbox deployed in behavioural and kinematic studies of animals (<xref rid="ref-11" ref-type="bibr">Branson, 2014</xref>; <xref rid="ref-19" ref-type="bibr">Dell et al., 2014</xref>; <xref rid="ref-21" ref-type="bibr">Egnor &amp; Branson, 2016</xref>; <xref rid="ref-59" ref-type="bibr">Robie et al., 2017</xref>). Recent advances in the use of pre-trained networks have reduced the quantity of required training data—and hence tedious hand-labelling—substantially (<xref rid="ref-46" ref-type="bibr">Mathis et al., 2018</xref>; <xref rid="ref-58" ref-type="bibr">Redmon, Farhadi &amp; Ap, 2018</xref>; <xref rid="ref-31" ref-type="bibr">Graving et al., 2019</xref>; <xref rid="ref-55" ref-type="bibr">Pereira et al., 2019</xref>; <xref rid="ref-8" ref-type="bibr">Bochkovskiy, Wang &amp; Liao, 2020</xref>). We anticipate that 3D models which are photorealistic at least at low resolutions may further improve the generalisability and precision of these approaches, as they enable the creation of labelled “synthetic datasets” (<xref rid="ref-18" ref-type="bibr">De Souza et al., 2017</xref>; <xref rid="ref-66" ref-type="bibr">Tobin et al., 2017</xref>; <xref rid="ref-68" ref-type="bibr">Varol et al., 2017</xref>; <xref rid="ref-36" ref-type="bibr">Kar et al., 2019</xref>; <xref rid="ref-49" ref-type="bibr">Mehta et al., 2019</xref>; <xref rid="ref-62" ref-type="bibr">Stout et al., 2019</xref>). Combining the power of free software engines such as <italic>Unreal</italic> or <italic>Unity3D</italic> with the flexibility of rigged 3D models, we can freely pose and position individuals and groups of individuals, control image background, lighting conditions, image noise and degree of occlusion, so providing the opportunity to create a virtually unlimited variety of labelled training images. As illustrative examples, we provide some animations and still images in the <xref ref-type="supplementary-material" rid="supplemental-information">Supplemental Material</xref>.</p>
        <p>Clearly, the effort required to generate synthetic data may exceed the effort to create sufficient hand-labelled training data if all experiments occur in well-defined and controlled environments. However, we hope that the development of an integrated pipeline to create synthetic training data from 3D models may render network performance robust and accurate enough even in “unseen” (and unpredictable) conditions in the field. Synthetic datasets may be a particularly powerful method to increase the performance of detection networks on complex backgrounds, or in the presence of extreme occlusion/overlap between individuals, as occurs regularly in studies on social insects (<xref rid="ref-25" ref-type="bibr">Gal, Saragosti &amp; Kronauer, 2020</xref>).</p>
      </sec>
    </sec>
    <sec>
      <title>Conservation and outreach</title>
      <p>High-quality 3D models of arthropods may aid in both conservation and outreach, as they bring with them the following advantages (<xref rid="ref-53" ref-type="bibr">Nguyen et al., 2014</xref>; <xref rid="ref-27" ref-type="bibr">Galantucci, Pesce &amp; Lavecchia, 2015</xref>; <xref rid="ref-22" ref-type="bibr">Erolin, Jarron &amp; Csetenyi, 2017</xref>; <xref rid="ref-63" ref-type="bibr">Ströbel et al., 2018</xref>; <xref rid="ref-16" ref-type="bibr">Cobb et al., 2019</xref>; <xref rid="ref-12" ref-type="bibr">Brecko &amp; Mathys, 2020</xref>):</p>
      <p>First, digital models can be made available online, so reducing the need for physical visits to collections, or specimen exchange between collections. Therefore, they maximise collection utility and accessibility, and minimise the risk of specimen damage. Second, in contrast to physical specimens, digital models do not deteriorate. Both points may be particularly relevant for the study and availability of valuable holotypes, provided that the digital models are of sufficient accuracy. Third, 3D models add significant value to online encyclopaedias which currently are almost exclusively populated by 2D images. Fourth, 3D models can be animated, so increasing the information content which can be stored and communicated. For example, we are working on an extension of our pipeline to animate our models directly with kinematic data recorded with live animals during natural locomotion. Such animations may be used in teaching, to demonstrate different gaits, say, but also in computer games or educational TV programmes. Fifth, 3D models may be used in “digital exhibitions”, enabling an unprecedented possibility of visitors to interact with the exhibits, and increasing accessibility of rare specimen.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions">
    <title>Conclusion</title>
    <p>We introduced <italic>scAnt</italic> 3D, a low-cost, open-source photogrammetry pipeline to create digital 3D models of arthropods and small objects. The process of 3D model creation is largely automated and can be easily controlled via a user-friendly GUI; all required code, technical drawings and component lists are freely available. We achieve state-of-the-art model quality at a fraction of the cost of comparable systems, paving the way for the widespread creation of near photorealistic 3D models of arthropods to be used in research, conservation, and outreach.</p>
  </sec>
  <sec sec-type="supplementary-material" id="supplemental-information">
    <title>Supplemental Information</title>
    <supplementary-material content-type="local-data" id="supp-1">
      <object-id pub-id-type="doi">10.7717/peerj.11155/supp-1</object-id>
      <label>Supplemental Information 1</label>
      <caption>
        <title>Renderings of the 3D Scanner.</title>
        <p>(<bold>A</bold>) Assembled scanner, excluding wiring. (<bold>B</bold>) Inside of the illumination dome, with the insect pin (vertical rotation axis) at its centre, and a circular array of LEDs in both halves of the spherical illumination dome. (<bold>C</bold>) Range of positions for the horizontal axis and camera, as controllable via the stepper-controlled camera slider provided in the graphical user interface of <italic>scAnt</italic> (see <xref ref-type="fig" rid="fig-2">Fig. 2</xref>). (<bold>D</bold>) Photograph of the scanner as built.</p>
      </caption>
      <media xlink:href="peerj-09-11155-s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="supp-2">
      <object-id pub-id-type="doi">10.7717/peerj.11155/supp-2</object-id>
      <label>Supplemental Information 2</label>
      <caption>
        <title>Laplacian filter applied to an image to highlight in-focus areas.</title>
        <p>Prior to image stacking, images which do not include relevant in-focus pixels are discarded in an automated process. (<bold>A</bold>) Image of a <italic>Dinomyrmex gigas</italic> specimen; the focal plane is approximately aligned with the hind legs. (<bold>B</bold>) A Laplacian filter can be used as a simple edge detector, as it highlights areas with a high local variation in intensity. Sharp areas result in greater variance so that the variance of the Laplacian can be used as a suitable scalar proxy for the fraction of the image which is in focus.</p>
      </caption>
      <media xlink:href="peerj-09-11155-s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="supp-3">
      <object-id pub-id-type="doi">10.7717/peerj.11155/supp-3</object-id>
      <label>Supplemental Information 3</label>
      <caption>
        <title>Comparison between (A) the original EDOF image and (B) the 3D model of a <italic>Leptoglossus zonatus</italic> specimen.</title>
        <p>The lighting of the scanner, as well as the camera parameters were replicated in blender v2.8 and rendered with Cycles to qualitatively inspect the models for evidence of distortion as a result of the reconstruction process.</p>
      </caption>
      <media xlink:href="peerj-09-11155-s003.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="supp-4">
      <object-id pub-id-type="doi">10.7717/peerj.11155/supp-4</object-id>
      <label>Supplemental Information 4</label>
      <caption>
        <title>Comparison between masks generated from Extended Depth Of Field (EDOF) images.</title>
        <p>(<bold>A</bold>) EDOF image captured with regular, flat lighting. (<bold>B</bold>) EDOF image captured with backlighting only. (<bold>C</bold>) Mask generated from the flat lighting EDOF image via random forest-based outline detection. (<bold>D</bold>) Hand-annotated ground truth mask generated from the flat lighting EDOF image (<bold>A</bold>). (<bold>E</bold>) Mask generated from the backlighting EDOF via backlight thresholding as described by <xref rid="ref-63" ref-type="bibr">Ströbel et al. (2018)</xref>.</p>
      </caption>
      <media xlink:href="peerj-09-11155-s004.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="supp-5">
      <object-id pub-id-type="doi">10.7717/peerj.11155/supp-5</object-id>
      <label>Supplemental Information 5</label>
      <caption>
        <title>Comparison between masks generated from Extended Depth Of Field (EDOF) images.</title>
        <p>31 uncompressed images for the flat lighting and backlighting EDOF images were captured at 20 MP resolution. The masking accuracy was computed by comparing the generated mask of each method to a hand labelled ground truth mask (see <bold>Fig. S4</bold>). The processing times refer to the combined time of the stacking and masking. The total file sizes refer to the combined amount of hard-drive space required to store all original and processed images.</p>
      </caption>
      <media xlink:href="peerj-09-11155-s005.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>The authors are grateful for the kind support and expert input from René Bulla, in particular in respect to 3D modelling and processing of visual data.</p>
  </ack>
  <sec sec-type="additional-information">
    <title>Additional Information and Declarations</title>
    <fn-group content-type="competing-interests">
      <title>Competing Interests</title>
      <fn fn-type="COI-statement" id="conflict-1">
        <p>The authors declare that they have no competing interests.</p>
      </fn>
    </fn-group>
    <fn-group content-type="author-contributions">
      <title>Author Contributions</title>
      <fn fn-type="con" id="contribution-1">
        <p><xref ref-type="contrib" rid="author-1">Fabian Plum</xref> conceived and designed the experiments, performed the experiments, analyzed the data, prepared figures and/or tables, authored or reviewed drafts of the paper, designed the scanner hardware and implemeted all code, and approved the final draft.</p>
      </fn>
      <fn fn-type="con" id="contribution-2">
        <p><xref ref-type="contrib" rid="author-2">David Labonte</xref> conceived and designed the experiments, analyzed the data, authored or reviewed drafts of the paper, and approved the final draft.</p>
      </fn>
    </fn-group>
    <fn-group content-type="other">
      <title>Data Availability</title>
      <fn id="addinfo-1">
        <p>The following information was supplied regarding data availability:</p>
        <p>All code is available at GitHub: <uri xlink:href="https://github.com/evo-biomech/scAnt">https://github.com/evo-biomech/scAnt</uri></p>
        <p>Manufacturing and assembly instructions, as well as component lists, are available at Thingiverse: <uri xlink:href="https://www.thingiverse.com/evobiomech/designs">https://www.thingiverse.com/evobiomech/designs</uri></p>
        <p>Example models generated with the described 3D scanner and processing pipeline are available at Sketchfab: <uri xlink:href="https://sketchfab.com/EvoBiomech">https://sketchfab.com/EvoBiomech</uri></p>
      </fn>
    </fn-group>
  </sec>
  <ref-list content-type="authoryear">
    <title>References</title>
    <ref id="ref-1">
      <label>Alcantarilla, Bartoli &amp; Davison (2012)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Alcantarilla</surname>
            <given-names>PF</given-names>
          </name>
          <name>
            <surname>Bartoli</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Davison</surname>
            <given-names>AJ</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Fitzgibbon</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lazebnik</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Perona</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Sato</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Schmid</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>KAZE features</article-title>
        <source>Computer Vision—ECCV 2012</source>
        <year>2012</year>
        <publisher-loc>Berlin</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>214</fpage>
        <lpage>227</lpage>
      </element-citation>
    </ref>
    <ref id="ref-2">
      <label>AliceVision (2018)</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <collab>
            <institution>AliceVision</institution>
          </collab>
        </person-group>
        <article-title>Meshroom: a 3D reconstruction software</article-title>
        <year>2018</year>
        <uri xlink:href="https://github.com/alicevision/meshroom">https://github.com/alicevision/meshroom</uri>
      </element-citation>
    </ref>
    <ref id="ref-3">
      <label>Atsushi et al. (2011)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Atsushi</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sueyasu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Funayama</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Maekawa</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>System for reconstruction of three-dimensional micro objects from multiple photographic images</article-title>
        <source>CAD Computer Aided Design</source>
        <year>2011</year>
        <volume>43</volume>
        <issue>8</issue>
        <fpage>1045</fpage>
        <lpage>1055</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cad.2011.01.019</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-4">
      <label>Bardua et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bardua</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Felice</surname>
            <given-names>RN</given-names>
          </name>
          <name>
            <surname>Watanabe</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Fabre</surname>
            <given-names>A-C</given-names>
          </name>
          <name>
            <surname>Goswami</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A practical guide to sliding and surface semilandmarks in morphometric analyses</article-title>
        <source>Integrative Organismal Biology</source>
        <year>2019</year>
        <volume>1</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="doi">10.1093/iob/obz016</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-5">
      <label>Bas &amp; Smith (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bas</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>WAP</given-names>
          </name>
        </person-group>
        <article-title>What does 2D geometric information really tell us about 3D face shape?</article-title>
        <source>International Journal of Computer Vision</source>
        <year>2019</year>
        <volume>127</volume>
        <issue>10</issue>
        <fpage>1455</fpage>
        <lpage>1473</lpage>
        <pub-id pub-id-type="doi">10.1007/s11263-019-01197-x</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-6">
      <label>Beaman &amp; Cellinese (2012)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Beaman</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Cellinese</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Mass digitization of scientific collections: new opportunities to transform the use of biological specimens and underwrite biodiversity science</article-title>
        <source>ZooKeys</source>
        <year>2012</year>
        <volume>209</volume>
        <fpage>7</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.3897/zookeys.209.3313</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-7">
      <label>Blagoderov et al. (2012)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blagoderov</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Kitching</surname>
            <given-names>IJ</given-names>
          </name>
          <name>
            <surname>Livermore</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Simonsen</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>VS</given-names>
          </name>
        </person-group>
        <article-title>No specimen left behind: industrial scale digitization of natural history collections</article-title>
        <source>ZooKeys</source>
        <year>2012</year>
        <volume>209</volume>
        <fpage>133</fpage>
        <lpage>146</lpage>
        <pub-id pub-id-type="doi">10.3897/zookeys.209.3178</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-8">
      <label>Bochkovskiy, Wang &amp; Liao (2020)</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Bochkovskiy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>C-Y</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>H-YM</given-names>
          </name>
        </person-group>
        <article-title>YOLOv4: optimal speed and accuracy of object detection</article-title>
        <year>2020</year>
        <uri xlink:href="http://arxiv.org/abs/2004.10934">http://arxiv.org/abs/2004.10934</uri>
      </element-citation>
    </ref>
    <ref id="ref-9">
      <label>Boykov &amp; Kolmogorov (2004)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boykov</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Kolmogorov</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</article-title>
        <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
        <year>2004</year>
        <volume>26</volume>
        <issue>9</issue>
        <fpage>1124</fpage>
        <lpage>1137</lpage>
        <pub-id pub-id-type="pmid">15742889</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-10">
      <label>Bradski (2000)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bradski</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>The OpenCV library</article-title>
        <source>Dr. Dobb’s Journal of Software Tools</source>
        <year>2000</year>
        <volume>120</volume>
        <fpage>122</fpage>
        <lpage>125</lpage>
      </element-citation>
    </ref>
    <ref id="ref-11">
      <label>Branson (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Branson</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Distinguishing seemingly indistinguishable animals with computer vision</article-title>
        <source>Nature Publishing Group</source>
        <year>2014</year>
        <volume>11</volume>
        <issue>7</issue>
        <fpage>721</fpage>
        <lpage>722</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3004</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-12">
      <label>Brecko &amp; Mathys (2020)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brecko</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mathys</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Handbook of best practice and standards for 2D+ and 3D imaging of natural history collections</article-title>
        <source>European Journal of Taxonomy</source>
        <year>2020</year>
        <volume>2020</volume>
        <issue>623</issue>
        <fpage>1</fpage>
        <lpage>115</lpage>
        <pub-id pub-id-type="doi">10.5852/ejt.2020.623</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-13">
      <label>Buser, Sidlauskas &amp; Summers (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Buser</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Sidlauskas</surname>
            <given-names>BL</given-names>
          </name>
          <name>
            <surname>Summers</surname>
            <given-names>AP</given-names>
          </name>
        </person-group>
        <article-title>2D or not 2D? Testing the utility of 2D vs. 3D landmark data in geometric morphometrics of the sculpin subfamily oligocottinae (Pisces; Cottoidea)</article-title>
        <source>Anatomical Record</source>
        <year>2018</year>
        <volume>301</volume>
        <issue>5</issue>
        <fpage>806</fpage>
        <lpage>818</lpage>
        <pub-id pub-id-type="doi">10.1002/ar.23752</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-14">
      <label>Cardini (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cardini</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Missing the third dimension in geometric morphometrics: how to assess if 2D images really are a good proxy for 3D structures?</article-title>
        <source>Hystrix</source>
        <year>2014</year>
        <volume>25</volume>
        <issue>2</issue>
        <fpage>73</fpage>
        <lpage>81</lpage>
        <pub-id pub-id-type="doi">10.4404/hystrix-25.2-10993</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-15">
      <label>Clark (2015)</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Clark</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Pillow (PIL Fork) documentation—readthedocs</article-title>
        <year>2015</year>
        <uri xlink:href="https://buildmedia.readthedocs.org/media/pdf/pillow/latest/pillow.pdf">https://buildmedia.readthedocs.org/media/pdf/pillow/latest/pillow.pdf</uri>
      </element-citation>
    </ref>
    <ref id="ref-16">
      <label>Cobb et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cobb</surname>
            <given-names>NS</given-names>
          </name>
          <name>
            <surname>Gall</surname>
            <given-names>LF</given-names>
          </name>
          <name>
            <surname>Zaspel</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Dowdy</surname>
            <given-names>NJ</given-names>
          </name>
          <name>
            <surname>McCabe</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Kawahara</surname>
            <given-names>AY</given-names>
          </name>
        </person-group>
        <article-title>Assessment of North American arthropod collections: prospects and challenges for addressing biodiversity research</article-title>
        <source>PeerJ</source>
        <year>2019</year>
        <volume>7</volume>
        <elocation-id>e8086</elocation-id>
        <pub-id pub-id-type="doi">10.7717/peerj.8086</pub-id>
        <pub-id pub-id-type="pmid">31788358</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-17">
      <label>Courtenay et al. (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Courtenay</surname>
            <given-names>LA</given-names>
          </name>
          <name>
            <surname>Maté-González</surname>
            <given-names>MÁ</given-names>
          </name>
          <name>
            <surname>Aramendi</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yravedra</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>González-Aguilera</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Domínguez-Rodrigo</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Testing accuracy in 2D and 3D geometric morphometric methods for cut mark identification and classification</article-title>
        <source>PeerJ</source>
        <year>2018</year>
        <volume>6</volume>
        <elocation-id>e5133</elocation-id>
        <pub-id pub-id-type="doi">10.7717/peerj.5133</pub-id>
        <pub-id pub-id-type="pmid">30002969</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-18">
      <label>De Souza et al. (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>De Souza</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>Gaidon</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cabon</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>López</surname>
            <given-names>AM</given-names>
          </name>
        </person-group>
        <article-title>Procedural generation of videos to train deep action recognition networks</article-title>
        <year>2017</year>
        <conf-name>Proceedings—30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>2594</fpage>
        <lpage>2604</lpage>
      </element-citation>
    </ref>
    <ref id="ref-19">
      <label>Dell et al. (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dell</surname>
            <given-names>AI</given-names>
          </name>
          <name>
            <surname>Bender</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Branson</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Couzin</surname>
            <given-names>ID</given-names>
          </name>
          <name>
            <surname>De Polavieja</surname>
            <given-names>GG</given-names>
          </name>
          <name>
            <surname>Noldus</surname>
            <given-names>LPJJ</given-names>
          </name>
          <name>
            <surname>Pérez-Escudero</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Perona</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Straw</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Wikelski</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Brose</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>Automated image-based tracking and its application in ecology</article-title>
        <source>Trends in Ecology &amp; Evolution</source>
        <year>2014</year>
        <volume>29</volume>
        <issue>7</issue>
        <fpage>417</fpage>
        <lpage>428</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tree.2014.05.004</pub-id>
        <pub-id pub-id-type="pmid">24908439</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-20">
      <label>Dollar &amp; Zitnick (2013)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Dollar</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zitnick</surname>
            <given-names>CL</given-names>
          </name>
        </person-group>
        <article-title>Structured forests for fast edge detection</article-title>
        <year>2013</year>
        <conf-name>Proceedings of the IEEE International Conference on Computer Vision</conf-name>
        <fpage>1841</fpage>
        <lpage>1848</lpage>
      </element-citation>
    </ref>
    <ref id="ref-21">
      <label>Egnor &amp; Branson (2016)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Egnor</surname>
            <given-names>SER</given-names>
          </name>
          <name>
            <surname>Branson</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Computational analysis of behavior</article-title>
        <source>Annual Review of Neuroscience</source>
        <year>2016</year>
        <volume>39</volume>
        <fpage>217</fpage>
        <lpage>238</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev-neuro-070815-013845</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-22">
      <label>Erolin, Jarron &amp; Csetenyi (2017)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Erolin</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Jarron</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Csetenyi</surname>
            <given-names>LJ</given-names>
          </name>
        </person-group>
        <article-title>Zoology 3D: creating a digital collection of specimens from the D’Arcy Thompson Zoology Museum</article-title>
        <source>Digital Applications in Archaeology and Cultural Heritage</source>
        <year>2017</year>
        <volume>7</volume>
        <issue>September</issue>
        <fpage>51</fpage>
        <lpage>55</lpage>
        <pub-id pub-id-type="doi">10.1016/j.daach.2017.11.002</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-23">
      <label>Fiorio &amp; Gustedt (1996)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fiorio</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Gustedt</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Two linear time Union-Find strategies for image processing</article-title>
        <source>Theoretical Computer Science</source>
        <year>1996</year>
        <volume>154</volume>
        <issue>2</issue>
        <fpage>165</fpage>
        <lpage>181</lpage>
      </element-citation>
    </ref>
    <ref id="ref-24">
      <label>Fruciano (2016)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fruciano</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Measurement error in geometric morphometrics</article-title>
        <source>Development Genes and Evolution</source>
        <year>2016</year>
        <volume>226</volume>
        <issue>3</issue>
        <fpage>139</fpage>
        <lpage>158</lpage>
        <pub-id pub-id-type="doi">10.1007/s00427-016-0537-4</pub-id>
        <pub-id pub-id-type="pmid">27038025</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-25">
      <label>Gal, Saragosti &amp; Kronauer (2020)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gal</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Saragosti</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kronauer</surname>
            <given-names>DJ</given-names>
          </name>
        </person-group>
        <article-title>anTraX: a software package for high-throughput video tracking of color-tagged insects</article-title>
        <source>eLife</source>
        <year>2020</year>
        <volume>9</volume>
        <elocation-id>e58145</elocation-id>
        <pub-id pub-id-type="doi">10.7554/eLife.58145</pub-id>
        <pub-id pub-id-type="pmid">33211008</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-26">
      <label>Galantucci, Guerra &amp; Lavecchia (2018)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Galantucci</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Guerra</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Lavecchia</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Ni</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Majstorovic</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Djurdjanovic</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Photogrammetry applied to small and micro scaled objects: a review</article-title>
        <source>Proceedings of 3rd International Conference on the Industry 4.0 Model for Advanced Manufacturing: AMP 2018—Lecture Notes in Mechanical Engineering</source>
        <year>2018</year>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>57</fpage>
        <lpage>77</lpage>
      </element-citation>
    </ref>
    <ref id="ref-27">
      <label>Galantucci, Pesce &amp; Lavecchia (2015)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Galantucci</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Pesce</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lavecchia</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A stereo photogrammetry scanning methodology, for precise and accurate 3D digitization of small parts with sub-millimeter sized features</article-title>
        <source>CIRP Annals—Manufacturing Technology</source>
        <year>2015</year>
        <volume>64</volume>
        <issue>1</issue>
        <fpage>507</fpage>
        <lpage>510</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cirp.2015.04.016</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-28">
      <label>Galantucci, Pesce &amp; Lavecchia (2016)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Galantucci</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Pesce</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lavecchia</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A powerful scanning methodology for 3D measurements of small parts with complex surfaces and sub millimeter-sized features, based on close range photogrammetry</article-title>
        <source>Precision Engineering</source>
        <year>2016</year>
        <volume>43</volume>
        <fpage>211</fpage>
        <lpage>219</lpage>
        <pub-id pub-id-type="doi">10.1016/j.precisioneng.2015.07.010</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-29">
      <label>Gallo, Muzzupappa &amp; Bruno (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gallo</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Muzzupappa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bruno</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>3D reconstruction of small sized objects from a sequence of multi-focused images</article-title>
        <source>Journal of Cultural Heritage</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>2</issue>
        <fpage>173</fpage>
        <lpage>182</lpage>
        <pub-id pub-id-type="doi">10.1016/j.culher.2013.04.009</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-30">
      <label>Gould (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gould</surname>
            <given-names>FDH</given-names>
          </name>
        </person-group>
        <article-title>To 3D or not to 3D, that is the question: do 3D surface analyses improve the ecomorphological power of the distal femur in placental mammals?</article-title>
        <source>PLOS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <issue>3</issue>
        <elocation-id>e91719</elocation-id>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0091719</pub-id>
        <pub-id pub-id-type="pmid">24633081</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-31">
      <label>Graving et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Graving</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Chae</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Naik</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Koger</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Costelloe</surname>
            <given-names>BR</given-names>
          </name>
          <name>
            <surname>Couzin</surname>
            <given-names>ID</given-names>
          </name>
        </person-group>
        <article-title>Fast and robust animal pose estimation</article-title>
        <source>bioRxiv</source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.1101/620245</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-32">
      <label>Hudson et al. (2015)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hudson</surname>
            <given-names>LN</given-names>
          </name>
          <name>
            <surname>Blagoderov</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Heaton</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Holtzhausen</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Livermore</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Price</surname>
            <given-names>BW</given-names>
          </name>
          <name>
            <surname>Van der Walt</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>VS</given-names>
          </name>
        </person-group>
        <article-title>Inselect: automating the digitization of natural history collections</article-title>
        <source>PLOS ONE</source>
        <year>2015</year>
        <volume>10</volume>
        <issue>11</issue>
        <fpage>1</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0143402</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-33">
      <label>Jancosek &amp; Pajdla (2010)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Jancosek</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pajdla</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Kutulakos</surname>
            <given-names>KN</given-names>
          </name>
        </person-group>
        <article-title>Hallucination-free multi-view stereo</article-title>
        <source>Trends and Topics in Computer Vision. ECCV 2010. Lecture Notes in Computer Science</source>
        <year>2010</year>
        <volume>6554</volume>
        <publisher-loc>Berlin, Heidelberg</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <pub-id pub-id-type="doi">10.1007/978-3-642-35740-4_15</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-34">
      <label>Jancosek &amp; Pajdla (2011)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Jancosek</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pajdla</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Multi-view reconstruction preserving weakly-supported surfaces</article-title>
        <year>2011</year>
        <conf-name>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>3121</fpage>
        <lpage>3128</lpage>
      </element-citation>
    </ref>
    <ref id="ref-35">
      <label>Jancosek &amp; Pajdla (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jancosek</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pajdla</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Exploiting visibility information in surface reconstruction to preserve weakly supported surfaces</article-title>
        <source>International Scholarly Research Notices</source>
        <year>2014</year>
        <volume>2014</volume>
        <fpage>798595</fpage>
        <pub-id pub-id-type="doi">10.1155/2014/798595</pub-id>
        <pub-id pub-id-type="pmid">27437454</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-36">
      <label>Kar et al. (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Kar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Prakash</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>M-Y</given-names>
          </name>
          <name>
            <surname>Cameracci</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rusiniak</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Acuna</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Torralba</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Fidler</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Meta-sim: learning to generate synthetic datasets</article-title>
        <year>2019</year>
        <conf-name>Proceedings of the IEEE International Conference on Computer Vision</conf-name>
        <fpage>4550</fpage>
        <lpage>4559</lpage>
      </element-citation>
    </ref>
    <ref id="ref-37">
      <label>Labatut &amp; Keriven (2009)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Labatut</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Keriven</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Robust and efficient surface reconstruction from range data</article-title>
        <source>Computer Graphics Forum</source>
        <year>2009</year>
        <volume>28</volume>
        <issue>8</issue>
        <fpage>2275</fpage>
        <lpage>2290</lpage>
      </element-citation>
    </ref>
    <ref id="ref-38">
      <label>Laurentini (1994)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Laurentini</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>The visual hull concept for silhouette-based image understanding</article-title>
        <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
        <year>1994</year>
        <volume>16</volume>
        <issue>2</issue>
        <fpage>150</fpage>
        <lpage>162</lpage>
        <pub-id pub-id-type="doi">10.1109/34.273735</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-39">
      <label>Li &amp; Nguyen (2019)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Perspective-consistent multifocus multiview 3D reconstruction of small objects</article-title>
        <year>2019</year>
        <conf-name>2019 Digital Image Computing: Techniques and Applications, DICTA 2019</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="ref-40">
      <label>Lindeberg (2012)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lindeberg</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Scale invariant feature transform</article-title>
        <source>Scholarpedia</source>
        <year>2012</year>
        <volume>7</volume>
        <issue>5</issue>
        <fpage>10491</fpage>
        <pub-id pub-id-type="doi">10.4249/scholarpedia.10491</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-41">
      <label>Lévy et al. (2002)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Lévy</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Petitjean</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ray</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Maillot</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Least squares conformal maps for automatic texture atlas generation</article-title>
        <year>2002</year>
        <conf-name>ACM Transactions on Graphics</conf-name>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>ACM</publisher-name>
        <fpage>362</fpage>
        <lpage>371</lpage>
      </element-citation>
    </ref>
    <ref id="ref-42">
      <label>Mandelbrot (1967)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mandelbrot</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>How long is the coast of Britain? Statistical self-similarity and fractional dimension</article-title>
        <source>Science</source>
        <year>1967</year>
        <volume>156</volume>
        <issue>3775</issue>
        <fpage>636</fpage>
        <lpage>638</lpage>
        <pub-id pub-id-type="doi">10.1126/science.156.3775.636</pub-id>
        <pub-id pub-id-type="pmid">17837158</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-43">
      <label>Mandelbrot (1982)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mandelbrot</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>The fractal geometry of nature</article-title>
        <source>Berichte der Bunsengesellschaft für Physikalische Chemie</source>
        <year>1982</year>
        <volume>89</volume>
        <issue>2</issue>
        <fpage>209</fpage>
        <pub-id pub-id-type="doi">10.1002/bbpc.19850890223</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-44">
      <label>Mantle, La Salle &amp; Fisher (2012)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mantle</surname>
            <given-names>BL</given-names>
          </name>
          <name>
            <surname>La Salle</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Fisher</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Whole-drawer imaging for digital management and curation of a large entomological collection</article-title>
        <source>ZooKeys</source>
        <year>2012</year>
        <volume>209</volume>
        <fpage>147</fpage>
        <lpage>163</lpage>
        <pub-id pub-id-type="doi">10.3897/zookeys.209.3169</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-45">
      <label>Martins et al. (2015)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Martins</surname>
            <given-names>AF</given-names>
          </name>
          <name>
            <surname>Bessant</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Manukyan</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Milinkovitch</surname>
            <given-names>MC</given-names>
          </name>
        </person-group>
        <article-title>R2OBBIE-3D: a fast robotic high-resolution system for quantitative phenotyping of surface geometry and colour-texture</article-title>
        <source>PLOS ONE</source>
        <year>2015</year>
        <volume>10</volume>
        <issue>6</issue>
        <elocation-id>e0126740</elocation-id>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0126740</pub-id>
        <pub-id pub-id-type="pmid">26039509</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-77">
      <label>Marwick &amp; Krishnamoorthy (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marwick</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Krishnamoorthy</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>cvequality: tests for the equality of coefficients of variation from multiple groups</article-title>
        <comment>R software package version 0.1.3</comment>
        <year>2019</year>
        <uri xlink:href="https://github.com/benmarwick/cvequality">https://github.com/benmarwick/cvequality</uri>
      </element-citation>
    </ref>
    <ref id="ref-46">
      <label>Mathis et al. (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mamidanna</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Cury</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Abe</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Murthy</surname>
            <given-names>VN</given-names>
          </name>
          <name>
            <surname>Mathis</surname>
            <given-names>MW</given-names>
          </name>
          <name>
            <surname>Bethge</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>
        <source>Nature Neuroscience</source>
        <year>2018</year>
        <volume>21</volume>
        <issue>9</issue>
        <fpage>1281</fpage>
        <lpage>1289</lpage>
        <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id>
        <pub-id pub-id-type="pmid">30127430</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-47">
      <label>Mathys, Brecko &amp; Semal (2013)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Mathys</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Brecko</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Semal</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Comparing 3D digitizing technologies: what are the differences?</article-title>
        <year>2013</year>
        <conf-name>Proceedings of the DigitalHeritage 2013—Federating the 19th Int’l VSMM, 10th Eurographics GCH, and 2nd UNESCO Memory of the World Conferences</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>201</fpage>
        <lpage>204</lpage>
      </element-citation>
    </ref>
    <ref id="ref-48">
      <label>McWhinnie &amp; Parsons (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McWhinnie</surname>
            <given-names>KC</given-names>
          </name>
          <name>
            <surname>Parsons</surname>
            <given-names>KJ</given-names>
          </name>
        </person-group>
        <article-title>Shaping up? A direct comparison between 2D and low-cost 3D shape analysis using African cichlid mandibles</article-title>
        <source>Environmental Biology of Fishes</source>
        <year>2019</year>
        <volume>102</volume>
        <issue>7</issue>
        <fpage>927</fpage>
        <lpage>938</lpage>
        <pub-id pub-id-type="doi">10.1007/s10641-019-00879-2</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-49">
      <label>Mehta et al. (2019)</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Mehta</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Diaz</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Golemo</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Pal</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Paull</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Active domain randomization</article-title>
        <year>2019</year>
        <uri xlink:href="http://arxiv.org/abs/1904.04762">http://arxiv.org/abs/1904.04762</uri>
      </element-citation>
    </ref>
    <ref id="ref-50">
      <label>Misof et al. (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Misof</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Meusemann</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Donath</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mayer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Frandsen</surname>
            <given-names>PB</given-names>
          </name>
          <name>
            <surname>Ware</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Flouri</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Beute</surname>
            <given-names>RG</given-names>
          </name>
          <name>
            <surname>Niehuis</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Petersen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Izquierdo-Carrasco</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Wappler</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Rust</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Aberer</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Aspöck</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Aspöck</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Bartel</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Blanke</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Berger</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Böhm</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Buckley</surname>
            <given-names>TR</given-names>
          </name>
          <name>
            <surname>Calcott</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Friedrich</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Fukui</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Fujita</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Greve</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Grobe</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Gu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Jermiin</surname>
            <given-names>LS</given-names>
          </name>
          <name>
            <surname>Kawahara</surname>
            <given-names>AY</given-names>
          </name>
          <name>
            <surname>Krogmann</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Kubiak</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lanfear</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Letsch</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Machida</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Mashimo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Kapli</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>McKenna</surname>
            <given-names>DD</given-names>
          </name>
          <name>
            <surname>Meng</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Nakagaki</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Navarrete-Heredia</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Ott</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ou</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Pass</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Podsiadlowski</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Poh</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Von Reumont</surname>
            <given-names>BM</given-names>
          </name>
          <name>
            <surname>Schütte</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sekiya</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Shimizu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Slipinski</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Stamatakis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Szucsich</surname>
            <given-names>NU</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Timelthaler</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Tomizuka</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Trautwein</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tong</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Uchifune</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Walzl</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Wiegmann</surname>
            <given-names>BM</given-names>
          </name>
          <name>
            <surname>Wilbrandt</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wipfler</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>TKF</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Yeates</surname>
            <given-names>DK</given-names>
          </name>
          <name>
            <surname>Yoshizawa</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Ziesmann</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kjer</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Phylogenomics resolves the timing and pattern of insect evolution</article-title>
        <source>Science</source>
        <year>2014</year>
        <volume>346</volume>
        <issue>6210</issue>
        <fpage>763</fpage>
        <lpage>767</lpage>
        <pub-id pub-id-type="pmid">25378627</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-51">
      <label>Neal &amp; Russ (2012)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Neal</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Russ</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <source>Measuring shape</source>
        <year>2012</year>
        <edition designator="1">First Edition</edition>
        <publisher-loc>Boca Ranton</publisher-loc>
        <publisher-name>CRC Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-52">
      <label>Nguyen et al. (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Nguyen</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Adcock</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Anderson</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lovell</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Fisher</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>La Salle</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Towards high-throughput 3d insect capture for species discovery and diagnostics</article-title>
        <year>2017</year>
        <conf-name>Proceedings—13th IEEE International Conference on eScience, 2017</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>559</fpage>
        <lpage>560</lpage>
      </element-citation>
    </ref>
    <ref id="ref-53">
      <label>Nguyen et al. (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nguyen</surname>
            <given-names>CV</given-names>
          </name>
          <name>
            <surname>Lovell</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Adcock</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>La Salle</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Capturing natural-colour 3D models of insects for species discovery and diagnostics</article-title>
        <source>PLOS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <issue>4</issue>
        <elocation-id>e94346</elocation-id>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0094346</pub-id>
        <pub-id pub-id-type="pmid">24759838</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-54">
      <label>Oliphant (2006)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Oliphant</surname>
            <given-names>TE</given-names>
          </name>
        </person-group>
        <source>A guide to NumPy</source>
        <year>2006</year>
        <volume>1</volume>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Trelgol Publishing USA</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-55">
      <label>Pereira et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pereira</surname>
            <given-names>TD</given-names>
          </name>
          <name>
            <surname>Aldarondo</surname>
            <given-names>DE</given-names>
          </name>
          <name>
            <surname>Willmore</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Kislin</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>SS-H</given-names>
          </name>
          <name>
            <surname>Murthy</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Shaevitz</surname>
            <given-names>JW</given-names>
          </name>
        </person-group>
        <article-title>Fast animal pose estimation using deep neural networks</article-title>
        <source>Nature Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <issue>1</issue>
        <fpage>117</fpage>
        <lpage>125</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id>
        <pub-id pub-id-type="pmid">30573820</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-56">
      <label>Polly &amp; Motz (2016)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Polly</surname>
            <given-names>PD</given-names>
          </name>
          <name>
            <surname>Motz</surname>
            <given-names>GJ</given-names>
          </name>
        </person-group>
        <article-title>Patterns and processes in morphospace: geometic morphometrics of three-dimensional objects</article-title>
        <source>Paleontological Society Papers</source>
        <year>2016</year>
        <volume>22</volume>
        <fpage>71</fpage>
        <lpage>99</lpage>
        <pub-id pub-id-type="doi">10.1017/scs.2017.9</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-57">
      <label>Qian et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qian</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Dang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Dan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Tong</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Bai</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lei</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Large-scale 3D imaging of insects with natural color</article-title>
        <source>Optics Express</source>
        <year>2019</year>
        <volume>27</volume>
        <issue>4</issue>
        <fpage>4845</fpage>
        <pub-id pub-id-type="doi">10.1364/oe.27.004845</pub-id>
        <pub-id pub-id-type="pmid">30876094</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-58">
      <label>Redmon, Farhadi &amp; Ap (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Redmon</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Farhadi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ap</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>YOLOv3: an incremental improvement</article-title>
        <source>CoRR</source>
        <year>2018</year>
        <uri xlink:href="http://arxiv.org/abs/1804.02767">http://arxiv.org/abs/1804.02767</uri>
      </element-citation>
    </ref>
    <ref id="ref-59">
      <label>Robie et al. (2017)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Robie</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Seagraves</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Roian Egnor</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Branson</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Machine vision methods for analyzing social interactions</article-title>
        <source>Journal of Experimental Biology</source>
        <year>2017</year>
        <volume>220</volume>
        <fpage>25</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="doi">10.1242/jeb.142281</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-60">
      <label>Roth (1993)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Roth</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Marcus</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Bello</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Carcia-Valdecasas</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>On three-dimensional morphometrics, and on the identification of landmark points</article-title>
        <source>Contributions to Morphometrics</source>
        <year>1993</year>
        <publisher-loc>Madrid</publisher-loc>
        <publisher-name>Museo Nacional de Ciencias Naturales</publisher-name>
        <fpage>41</fpage>
        <lpage>46</lpage>
      </element-citation>
    </ref>
    <ref id="ref-61">
      <label>Seitz et al. (2006)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Seitz</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Curless</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Diebel</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Scharstein</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Szeliski</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A comparison and evaluation of multi-view stereo reconstruction algorithms</article-title>
        <year>2006</year>
        <conf-name>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>519</fpage>
        <lpage>526</lpage>
      </element-citation>
    </ref>
    <ref id="ref-62">
      <label>Stout et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stout</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Madineni</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Tremblay</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Tane</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>The development of synthetic thermal image generation tools and training data at FLIR</article-title>
        <source>Proceedings of the SPIE</source>
        <year>2019</year>
        <volume>10988</volume>
        <fpage>10988146</fpage>
        <pub-id pub-id-type="doi">10.1117/12.2518655</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-63">
      <label>Ströbel et al. (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ströbel</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Schmelzle</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Blüthgen</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Heethoff</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>An automated device for the digitization and 3D modelling of insects, combining extended-depth-of-field and all-side multi-view imaging</article-title>
        <source>ZooKeys</source>
        <year>2018</year>
        <volume>2018</volume>
        <issue>759</issue>
        <fpage>1</fpage>
        <lpage>27</lpage>
        <pub-id pub-id-type="doi">10.3897/zookeys.759.24584</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-64">
      <label>Tareen &amp; Saleem (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Tareen</surname>
            <given-names>SAK</given-names>
          </name>
          <name>
            <surname>Saleem</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>A comparative analysis of SIFT, SURF, KAZE, AKAZE, ORB, and BRISK</article-title>
        <year>2018</year>
        <conf-name>2018 International Conference on Computing, Mathematics and Engineering Technologies: Invent, Innovate and Integrate for Socioeconomic Development, iCoMET 2018—Proceedings</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <comment>2018</comment>
      </element-citation>
    </ref>
    <ref id="ref-65">
      <label>Tatsuta, Takahashi &amp; Sakamaki (2018)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tatsuta</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Takahashi</surname>
            <given-names>KH</given-names>
          </name>
          <name>
            <surname>Sakamaki</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Geometric morphometrics in entomology: basics and applications</article-title>
        <source>Entomological Science</source>
        <year>2018</year>
        <volume>21</volume>
        <issue>2</issue>
        <fpage>164</fpage>
        <lpage>184</lpage>
        <pub-id pub-id-type="doi">10.1111/ens.12293</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-66">
      <label>Tobin et al. (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Tobin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ray</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zaremba</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Abbeel</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Domain randomization for transferring deep neural networks from simulation to the real world</article-title>
        <year>2017</year>
        <conf-name>IEEE International Conference on Intelligent Robots and Systems</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>23</fpage>
        <lpage>30</lpage>
      </element-citation>
    </ref>
    <ref id="ref-67">
      <label>Van der Walt et al. (2014)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van der Walt</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schönberger</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Nunez-Iglesias</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Boulogne</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Warner</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Yager</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Gouillart</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Scikit-image: image processing in Python</article-title>
        <source>PeerJ</source>
        <year>2014</year>
        <volume>2</volume>
        <elocation-id>e453</elocation-id>
        <pub-id pub-id-type="pmid">25024921</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-68">
      <label>Varol et al. (2017)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Varol</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Romero</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Mahmood</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Black</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Laptev</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Schmid</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Learning from synthetic humans</article-title>
        <year>2017</year>
        <conf-name>Proceedings—30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>4627</fpage>
        <lpage>4635</lpage>
      </element-citation>
    </ref>
    <ref id="ref-69">
      <label>Viscardi, Sakamoto &amp; Sigwart (2010)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Viscardi</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Sakamoto</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sigwart</surname>
            <given-names>JD</given-names>
          </name>
        </person-group>
        <article-title>How long is a piece of strix? Methods in measuring and measuring the measurers</article-title>
        <source>Zoomorphology</source>
        <year>2010</year>
        <volume>129</volume>
        <issue>3</issue>
        <fpage>185</fpage>
        <lpage>194</lpage>
        <pub-id pub-id-type="doi">10.1007/s00435-010-0111-y</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-70">
      <label>Walker et al. (1999)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Walker</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Fitton</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Vane-Wright</surname>
            <given-names>RI</given-names>
          </name>
          <name>
            <surname>Carter</surname>
            <given-names>DJ</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Carter</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Walker</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Insects and other invertebrates</article-title>
        <source>Chapter 2: Care and Conservation of Natural History Collections</source>
        <year>1999</year>
        <publisher-loc>Oxford</publisher-loc>
        <publisher-name>Butterwoth Heinemann</publisher-name>
        <fpage>37</fpage>
        <lpage>60</lpage>
      </element-citation>
    </ref>
    <ref id="ref-71">
      <label>Wang et al. (2015)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>License plate localization in complex scenes based on oriented FAST and rotated BRIEF feature</article-title>
        <source>Journal of Electronic Imaging</source>
        <year>2015</year>
        <volume>24</volume>
        <issue>5</issue>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1117/1.JEI.24.5.053011</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-72">
      <label>Wheeler et al. (2012)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wheeler</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Bourgoin</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Coddington</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Gostony</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hamilton</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Larimer</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Polaszek</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Schauff</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Alma Solis</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Nomenclatural benchmarking: the roles of digital typification and telemicroscopy</article-title>
        <source>ZooKeys</source>
        <year>2012</year>
        <volume>209</volume>
        <fpage>193</fpage>
        <lpage>202</lpage>
        <pub-id pub-id-type="doi">10.3897/zookeys.209.3486</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-73">
      <label>Wu, Otoo &amp; Shoshani (2005)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Otoo</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Shoshani</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Optimizing connected component labeling algorithms</article-title>
        <source>Medical Imaging 2005: Image Processing</source>
        <year>2005</year>
        <volume>5747</volume>
        <fpage>1965</fpage>
        <pub-id pub-id-type="doi">10.1117/12.596105</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-74">
      <label>Zhang, Gao &amp; Caelli (2010)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Caelli</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Primitive-based 3D structure inference from a single 2D image for insect modeling: towards an electronic field guide for insect identification</article-title>
        <year>2010</year>
        <conf-name>11th International Conference on Control, Automation, Robotics and Vision, ICARCV 2010</conf-name>
        <fpage>866</fpage>
        <lpage>871</lpage>
      </element-citation>
    </ref>
    <ref id="ref-75">
      <label>Zhang et al. (2019)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Qin</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Static/dynamic filtering for mesh geometry</article-title>
        <source>IEEE Transactions on Visualization and Computer Graphics</source>
        <year>2019</year>
        <volume>25</volume>
        <issue>4</issue>
        <fpage>1774</fpage>
        <lpage>1787</lpage>
        <pub-id pub-id-type="doi">10.1109/TVCG.2018.2816926</pub-id>
        <pub-id pub-id-type="pmid">29993982</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-76">
      <label>Zuiderveld (1994)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Zuiderveld</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <source>Contrast limited adaptive histogram equalization—graphics gems</source>
        <year>1994</year>
        <publisher-loc>Cambridge</publisher-loc>
        <publisher-name>Academic Press</publisher-name>
        <fpage>474</fpage>
        <lpage>485</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
