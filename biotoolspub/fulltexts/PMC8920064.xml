<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?covid-19-tdm?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <?QA-only?>
    <journal-id journal-id-type="iso-abbrev">J Netw Syst Manage</journal-id>
    <journal-title-group>
      <journal-title>Journal of Network and Systems Management</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1064-7570</issn>
    <issn pub-type="epub">1573-7705</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8920064</article-id>
    <article-id pub-id-type="publisher-id">9649</article-id>
    <article-id pub-id-type="doi">10.1007/s10922-022-09649-5</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title><sc>Explora-VR</sc>: Content Prefetching for Tile-Based Immersive Video Streaming Applications</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1215-9209</contrib-id>
        <name>
          <surname>Ordonez-Ante</surname>
          <given-names>Leandro</given-names>
        </name>
        <address>
          <email>Leandro.OrdonezAnte@UGent.be</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>van der Hooft</surname>
          <given-names>Jeroen</given-names>
        </name>
        <address>
          <email>Jeroen.vanderHooft@UGent.be</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wauters</surname>
          <given-names>Tim</given-names>
        </name>
        <address>
          <email>Tim.Wauters@UGent.be</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Van Seghbroeck</surname>
          <given-names>Gregory</given-names>
        </name>
        <address>
          <email>Gregory.VanSeghbroeck@UGent.be</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Volckaert</surname>
          <given-names>Bruno</given-names>
        </name>
        <address>
          <email>Bruno.Volckaert@UGent.be</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>De Turck</surname>
          <given-names>Filip</given-names>
        </name>
        <address>
          <email>Filip.DeTurck@UGent.be</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.5342.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 2069 7798</institution-id><institution>Department of Information Technology, IDLab, </institution><institution>Ghent University - imec, </institution></institution-wrap>Technologiepark Zwijnaarde 126, 9052 Gent, Belgium </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2022</year>
    </pub-date>
    <volume>30</volume>
    <issue>3</issue>
    <elocation-id>38</elocation-id>
    <history>
      <date date-type="received">
        <day>28</day>
        <month>7</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>21</day>
        <month>12</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>2</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2022</copyright-statement>
      <license>
        <license-p>This article is made available via the PMC Open Access Subset for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original source. These permissions are granted for the duration of the World Health Organization (WHO) declaration of COVID-19 as a global pandemic.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Despite the growing popularity of immersive video applications during the last few years, the stringent low latency requirements of this kind of services remain a major challenge for the existing network infrastructure. Edge-assisted solutions compensate for network latency by relying on cache-enabled edge servers to bring frequently accessed video content closer to the client. However, these approaches often require historical request traces from previous watching sessions or adopt passive caching strategies subject to the <italic>cold-start</italic> problem and prone to playout freezes. This paper introduces <sc>Explora-VR</sc>, a novel edge-assisted content prefetching method for tile-based 360<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M2"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq1.gif"/></alternatives></inline-formula> video streaming. This method leverages the client’s rate adaptation heuristic to preemptively retrieve the content that the viewer will most likely watch in the upcoming segments, and loads it into a nearby edge server. At the same time, <sc>Explora-VR</sc> incrementally builds a dynamic collective buffer for serving the requests from active streaming sessions based on the estimated popularity of video tiles per segment. An evaluation of the proposed method was conducted on head movement traces collected from 48 unique users while watching three different 360<inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M4"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq2.gif"/></alternatives></inline-formula> videos. Results show that <sc>Explora-VR</sc> is able to serve over 98% of the client requests from the cache-enabled edge server, leading to an average increase of 2.5<inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M6"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq3.gif"/></alternatives></inline-formula> and 1.4<inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M8"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq4.gif"/></alternatives></inline-formula> in the client’s perceived throughput, compared to a conventional client-server setup and a <italic>least recently used</italic> caching policy, respectively. This enables <sc>Explora-VR</sc> to serve higher quality video content while providing a freeze-free playback experience and effectively reducing network traffic to the content server.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Virtual reality</kwd>
      <kwd>Video streaming</kwd>
      <kwd>Content prefetching</kwd>
      <kwd>Collective buffering</kwd>
      <kwd>Edge-assisted streaming</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Springer Science+Business Media, LLC, part of Springer Nature 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">The recent outbreak of the COVID-19 pandemic has forced a radical shift in reality for a vast majority of the human population. Given the strict restrictions on mobility and social contact, people were compelled to move several aspects of their daily life into the digital world. These circumstances have boosted the interest in 360<inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M10"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq5.gif"/></alternatives></inline-formula> immersive video applications (<italic>augmented and virtual reality—AR/VR</italic>) as a means to provide realistic and engaging user experiences, that make up for the lack of presence and physical interaction [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. However, the stringent demands in terms of bandwidth and very low latency of AR/VR applications still represent a major challenge for the existing network infrastructure [<xref ref-type="bibr" rid="CR4">4</xref>].</p>
    <p id="Par3">For services relying on VR headsets for content delivery, the delay perceived by the user is a critical factor for determining the overall experience. Research on this topic signals that the motion-to-photon (MTP) latency for VR displays should be less than 20 ms to prevent the perception of scene instability and <italic>cybersickness</italic> [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>]. For on-demand tile-based 360<inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M12"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq6.gif"/></alternatives></inline-formula> video streaming in particular, many of the existing studies have focused on mitigating the effect of latency by increasing viewport prediction accuracy and applying <italic>HTTP adaptive streaming</italic> (HAS) methods to adapt the quality of the requested content to the network conditions [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>]. While these approaches achieve a rational use of the bandwidth as perceived at the client’s side, the network latency due to distant content servers can still substantially degrade the viewer experience.</p>
    <p id="Par4">As an answer to this problem, network-supported solutions leveraging cache-enabled edge servers have been proposed [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>]. The idea behind these approaches consists of bringing frequently accessed video tiles closer to the client; this offsets the network delay, which in turn leads to a significant improvement in the quality of the delivered content. This is, however, easier said than done: the high variety of possible viewport configurations—due to the freedom of device orientation, added to different network conditions—makes it hard to determine a priori the set of tiles that should be cached. In this sense, network-supported solutions often rely on log traces obtained from previous streaming sessions to estimate the popularity of the content, and/or adopt passive caching strategies in which only those tiles that are requested get cached at the edge server. These approaches entail two fundamental problems: (<italic>i</italic>) historical request traces are not always available for every piece of content, and (<italic>ii</italic>) the <italic>cold-start</italic> problem: early users would barely experience any improvement from having a cache nearby, due to the fact that most of their requests for content end up being forwarded to the origin server.</p>
    <p id="Par5">To address these issues, in this paper we introduce <sc>Explora-VR</sc>, a content prefetching mechanism for tile-based immersive video streaming. Our solution introduces two fundamental changes in the traditional workflow of content consumption for this kind of services: (<italic>1</italic>) the early advertising of the outcome of the viewport prediction and rate adaptation algorithm, running on the <italic>head-mounted display</italic> (HMD), and (<italic>2</italic>) the incremental building of a collective buffer that incorporates fixation patterns shared by the viewers. The rationale behind this is two-fold: <list list-type="order"><list-item><p id="Par6">The information on the predicted user’s viewport is forwarded to the cache-enabled edge server before the client’s device starts buffering content. The edge server uses this information to preemptively retrieve—at a given quality level—the video tiles that the user is likely to watch in the upcoming segments, and it loads them into memory. Then the client starts consuming the video content from a closer server. In these circumstances, a HAS client would perceive that the content is downloaded with low latency, leading to a high network throughput estimation, and consequently to an increase in quality of the requested video tiles.</p></list-item><list-item><p id="Par7">Since having multiple clients consume the same VR content within a small time window is a common use case (e.g., the <italic>on-demand</italic>, <italic>near-live</italic> scenario when a content provider premieres a new release), we have devised a stream-processing pipeline which enables combining the different user predicted viewports into a dynamic collective buffer (henceforth referred to as <italic>DCoB</italic>) which is built and refined incrementally as new users join the streaming session. The purpose of this DCoB is to serve as a cache holding frequently accessed content, preventing the edge server from flooding the content server with duplicate requests.</p></list-item></list>This paper presents the following three main contributions of the solution we propose: (<italic>1</italic>) an edge-assisted, content-agnostic mechanism that proactively downloads the video tiles that an individual viewer is likely to watch in the near future (<italic>2</italic>) the formal definition of the data structure and stream processing pipeline behind the DCoB, which enables low-latency delivery of 360<inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M14"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq7.gif"/></alternatives></inline-formula> video content to multiple users taking part of an on-demand, near-live streaming scenario, and (<italic>3</italic>) the experimental evaluation of the proposed approach on a public dataset which comprises the viewport traces from 48 users, collected throughout immersive video sessions. We have benchmarked the <sc>Explora-VR</sc> prefetching mechanism against a conventional client-server configuration (without caching/prefetching), and a setup implementing a traditional <italic>least-recently used</italic> (LRU) caching replacement policy. Results show that the devised prefetching mechanism substantially improves the quality of experience (QoE) perceived by the viewer, in terms of video quality, startup latency, and occurrence of playout freezes, while reducing the backhaul traffic and content server’s load.</p>
    <p id="Par8">It should be pointed out that it is not in the scope of this paper to reach an optimal trade-off between network resource consumption and video delivered quality, as is the case for approaches in the literature such as [<xref ref-type="bibr" rid="CR10">10</xref>] and [<xref ref-type="bibr" rid="CR11">11</xref>]. Our work is focused on investigating data processing methods for enabling preemptive retrieval of immersive video content which are able to adapt to the fixation patterns of multiple concurrent viewers. To achieve this, we leverage the computing resources of cache-enabled edge nodes, and we rely on existing methods for client-side viewport prediction and tile-based rate adaptation such as those introduced in [<xref ref-type="bibr" rid="CR12">12</xref>] and [<xref ref-type="bibr" rid="CR13">13</xref>].</p>
    <p id="Par9">The remainder of this paper is structured as follows. Section <xref rid="Sec2" ref-type="sec">2</xref> discusses the related work. Section <xref rid="Sec6" ref-type="sec">3</xref> describes the detailed description of the techniques behind the content prefetching mechanism for tile-based 360<inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M16"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq8.gif"/></alternatives></inline-formula> video streaming. Section <xref rid="Sec12" ref-type="sec">4</xref> elaborates on the architecture of a proof-of-concept implementation of the proposed approach. Section <xref rid="Sec16" ref-type="sec">5</xref> presents the experimental setup and the results derived from the evaluation. Finally, conclusions and perspectives for further research are provided in Sect. <xref rid="Sec19" ref-type="sec">6</xref>. Table <xref rid="Tab1" ref-type="table">1</xref> below provides the list of acronyms used throughout the paper.<table-wrap id="Tab1"><label>Table 1</label><caption><p>List of acronyms used in this paper</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Acronym</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">CDN</td><td align="left">Content delivery network</td></tr><tr><td align="left">CRF</td><td align="left">Constant rate factor</td></tr><tr><td align="left">CTF</td><td align="left">Center tile first</td></tr><tr><td align="left">DCoB</td><td align="left">Dynamic collective buffer</td></tr><tr><td align="left">DNN</td><td align="left">Deep neural networks</td></tr><tr><td align="left">DRL</td><td align="left">Deep reinforcement learning</td></tr><tr><td align="left">FoV</td><td align="left">Field of view</td></tr><tr><td align="left">FPS</td><td align="left">Frames per second</td></tr><tr><td align="left">GOP</td><td align="left">Group of pictures</td></tr><tr><td align="left">HAS</td><td align="left">HTTP adaptive streaming</td></tr><tr><td align="left">HEVC</td><td align="left">High efficiency video coding</td></tr><tr><td align="left">HMD</td><td align="left">Head-mounted device</td></tr><tr><td align="left">LRU</td><td align="left">Least-recently used</td></tr><tr><td align="left">QoE</td><td align="left">Quality of experience</td></tr><tr><td align="left">RTT</td><td align="left">Round trip time</td></tr><tr><td align="left">VR</td><td align="left">Virtual reality</td></tr></tbody></table></table-wrap></p>
  </sec>
  <sec id="Sec2">
    <title>Related Work</title>
    <p id="Par10">Immersive video applications are typically bandwidth-hungry and highly sensitive to latency. A large body of research in this field has been devoted to develop efficient mechanisms of content delivery. Existing approaches can be grouped into three categories according to the main focus of their respective contribution, namely <italic>client-driven</italic>, <italic>server optimization</italic>, and <italic>edge-assisted</italic> solutions.</p>
    <sec id="Sec3">
      <title>Client-Driven HAS Streaming for Tile-Based 360<inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M18"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq9.gif"/></alternatives></inline-formula> Video</title>
      <p id="Par11">To improve transmission efficiency, approaches in this category divide an equirectangular projection of the spherical video into several rectangular areas of the same size, referred to as tiles. By implementing said tiling scheme, the client can opt to prioritize the tiles that overlap with the viewer’s viewport and request them in a higher quality representation than the tiles that are not visible to the user. Representative approaches of these tile-based viewport-dependent adaptive video streaming solutions include those by Hosseini [<xref ref-type="bibr" rid="CR14">14</xref>], Xie et al. [<xref ref-type="bibr" rid="CR15">15</xref>], Graf et al. [<xref ref-type="bibr" rid="CR16">16</xref>], Nguyen et al. [<xref ref-type="bibr" rid="CR17">17</xref>], and van der Hooft et al. [<xref ref-type="bibr" rid="CR13">13</xref>]. These works are fundamentally focused on addressing two main challenges: (<italic>i</italic>) <italic>viewport prediction</italic>: anticipate user movements to ensure content is timely displayed following the <italic>field of view</italic> (FoV) of the user; and (<italic>ii</italic>) quality of experience (QoE): providing a smooth, responsive viewing experience at the highest possible video quality that the best-effort network can deliver [<xref ref-type="bibr" rid="CR8">8</xref>]. In essence, these approaches adopt traditional HTTP adaptive streaming techniques, and augment them to support tile-based content delivery, while meeting the stringent demands in terms of latency and interactivity of omnidirectional video streaming. Although these solutions allow for an efficient use of the link capacity, they are still highly sensitive to network latency due to content servers situated in distant locations which severely degrades the user experience.</p>
    </sec>
    <sec id="Sec4">
      <title>Server Optimization Solutions</title>
      <p id="Par12">This category comprises works mainly focused on maximizing viewer’s QoE while optimally allocating server and network resources. Long et al. [<xref ref-type="bibr" rid="CR10">10</xref>] propose a solution to the problem of optimal transmission resource allocation on the server side given a specific requirement of video quality from the viewer, as well as the optimal encoding rate for each video tile given a certain transmission energy budget. The solution contemplates exploiting several multicast opportunities that involve balancing trade-offs between video quality, computation, and consumption of communication resources. One of the implications of the proposed multicasting mechanisms is that the server might transmit video tiles at a higher quality representation than requested by a certain client. In such a case, the client application would incur a processing cost in order to scale down the received video tile to the appropriate quality representation. Building upon [<xref ref-type="bibr" rid="CR10">10</xref>], the work by Zhao et al. [<xref ref-type="bibr" rid="CR11">11</xref>] investigates the impact of viewport prediction on adaptive streaming of tiled 360<inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M20"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq10.gif"/></alternatives></inline-formula> video in a multi-carrier wireless system. The authors consider a setup with a multi-antenna base station from which video content is transmitted to one or multiple single-antenna clients. Within the scope of said setup, the authors propose a framework that optimizes the downlink subcarrier allocations as well as the encoding rates for tiles and FoVs at the server side. The solution proposed in [<xref ref-type="bibr" rid="CR11">11</xref>] aims at maximizing the video quality delivered to the clients, while controlling the rebuffering time for different levels of certainty about the outcome of the viewport prediction. It is noteworthy that the optimization investigated in [<xref ref-type="bibr" rid="CR11">11</xref>] relies on methods that operate on the radio link layer, which is out of the scope of the work we present in this paper.</p>
      <p id="Par13">Another approach that fits within this category is introduced by Shi et al. [<xref ref-type="bibr" rid="CR5">5</xref>], who propose a remote rendering solution in which the server is able to stream only the scenes within the user’s FoV plus a margin area around it whose width depends on the perceived system latency. Instead of a tiling scheme, the server uses an adaptive cropping filter that adjusts the delivered content to the fraction of the VR video overlapping with the current user viewport. A design decision made by the authors consists of minimizing the use of video buffering to reduce the system’s response latency. As a consequence, the proposed remote rendering solution is sensitive to network jitter and prone to frame dropping. Furthermore, the authors do not provide a clear indication concerning the performance of the proposed solution under high server load (i.e., serving multiple concurrent viewers).</p>
    </sec>
    <sec id="Sec5">
      <title>Edge-Assisted Solutions</title>
      <p id="Par14">Thanks to the recent availability of public datasets on Virtual Reality (VR) video streaming —such as those by Lo et al. [<xref ref-type="bibr" rid="CR18">18</xref>], David et al. [<xref ref-type="bibr" rid="CR19">19</xref>], Fremerey et al. [<xref ref-type="bibr" rid="CR20">20</xref>], and Wu et al. [<xref ref-type="bibr" rid="CR21">21</xref>], among others— there has been an increasing interest in investigating methods for mining behavioral patterns from user movement traces. According to the study by Rossy et al. [<xref ref-type="bibr" rid="CR22">22</xref>], navigation trajectories followed by viewers with high affinity exhibit patterns that can be used for optimizing the content delivery in streaming systems. Approaches aligned with this idea are often labelled as <italic>edge-assisted</italic> or <italic>network-supported</italic> solutions. Papaioannou et al. [<xref ref-type="bibr" rid="CR23">23</xref>] addressed the problem of optimal caching for tile-based VR video streaming in the wireless edge network. Specifically, the solution introduced in [<xref ref-type="bibr" rid="CR23">23</xref>] formulates a tile and tile resolution caching policy that aims at minimizing the error between the cached and requested content. The authors studied a static caching scenario in which the caching decision is made upfront, based on statistical data of the tile resolution demands from past watching sessions. Similarly, Mahzari et al. [<xref ref-type="bibr" rid="CR24">24</xref>] explored the application of edge caching as a measure to compensate for network latency, and offload the content servers and backhaul network. The authors of this work conceived a FoV-aware caching policy based on a bayesian model which takes in the sequence of requests made by previous viewers. The proposed model gauges the popularity of individual tiles, and makes decisions on which content to cache/evict based on said metric. Similarly, Maniotis and Thomos [<xref ref-type="bibr" rid="CR25">25</xref>] devised a cache replacement strategy for tile-based omnidirectional video, supported by a <italic>deep reinforcement learning</italic> (DRL) framework. This strategy takes into account the popularity of both videos and individual tiles. The authors introduced the concept of <italic>virtual viewports</italic> defined as the most popular video tiles resulting from the overlapping FoV of multiple users. To learn the optimal policy for tile placement in the cache, the DRL framework first requires to train a deep neural network (DNN) on past user requests.</p>
      <p id="Par15">These approaches (and related proposals such as [<xref ref-type="bibr" rid="CR26">26</xref>–<xref ref-type="bibr" rid="CR29">29</xref>]) have proven the pertinence and substantial benefit of edge caching to improve QoE in 360<inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M22"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq11.gif"/></alternatives></inline-formula> video services, while reducing the load on the core network. However, these solutions often require an <italic>offline stage</italic> in which they fit a certain data model to traces of user requests. Afterward, in a subsequent <italic>online stage</italic>, this model is used to make decisions on which content to cache/evict, according to the demands from new users consuming the streaming service. In addition, the studies discussed above adopt a passive approach to caching, i.e. tiles are stored into the edge-server memory only after they have been requested. Under these circumstances, early viewers would experience little benefit from the caching strategy in place, an issue that is commonly referred to in the literature as the <italic>cold-start</italic> problem [<xref ref-type="bibr" rid="CR30">30</xref>]. Using a cold cache translates into cache miss events, which in turn increases the likelihood of playback freezes, since user requests have to be relayed back to the content server, thus incurring additional latency. To counter this issue, we propose a new FoV-aware content prefetching approach for tile-based adaptive 360<inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M24"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq12.gif"/></alternatives></inline-formula> video streaming. This approach takes advantage of existing viewport prediction techniques to preemptively retrieve and cache the video content that the viewer is most likely to consume in the upcoming segments. Additionally, this mechanism does not rely on training data from historical traces as it is able to learn a <italic>collective viewport</italic> on the fly, out of the requests made by viewers with active streaming sessions. The content inside the collective viewport dynamically adapts in response to the content that is most demanded by the audience at a given point in time, which makes this approach specially appealing for near-live immersive video streaming applications.</p>
    </sec>
  </sec>
  <sec id="Sec6">
    <title><sc>Explora-VR</sc>: Approach Overview</title>
    <p id="Par16">
      <fig id="Fig1">
        <label>Fig. 1</label>
        <caption>
          <p>High-level component view of the VR content prefetching scenario. The link between <italic>Content</italic> and <italic>Prefetch</italic> servers features a larger capacity and higher latency than the one between <italic>Prefetch</italic> server and <italic>Client</italic></p>
        </caption>
        <graphic xlink:href="10922_2022_9649_Fig1_HTML" id="MO1"/>
      </fig>
    </p>
    <p id="Par17">Figure <xref rid="Fig1" ref-type="fig">1</xref> illustrates the components that make up the content prefetching mechanism we propose. This mechanism is deployed on a cache-enabled edge server acting as a transparent proxy between the client and the content server. In this section we elaborate on the techniques that lay the foundation of our solution, namely (<italic>1</italic>) the early advertising of the outcome of the viewport prediction and rate adaptation algorithm, and (<italic>2</italic>) the <italic>dynamic collective buffer</italic> (DCoB).</p>
    <sec id="Sec7">
      <title>Viewport Prediction Advertising and Prefetching</title>
      <p id="Par18">In immersive video applications based on 360<inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M26"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq13.gif"/></alternatives></inline-formula> video, it is common for content to be segmented not only in time but also in the spatial dimension. The HEVC/H.265 standard, for instance, allows to split an equirectangular projection of the content into <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m\times n$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq14.gif"/></alternatives></inline-formula> tiles of the same resolution. By adding this spatial dimension, clients can prioritize the content within the user’s <italic>field of view</italic>, assigning a higher quality to specific regions of the video, hence making more optimal use of the bandwidth resources [<xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR32">32</xref>].</p>
      <p id="Par19">To prevent buffer starvation and ensure a smooth playback in these highly interactive applications, traditional HAS methods need to be augmented. HAS clients for VR applications rely on techniques for predicting the users’ target field of view or viewport, and rate adaptation heuristics to fine-tune the quality level of the requested content in response to the users’ movements and network conditions [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR32">32</xref>].</p>
      <p id="Par20">Several methods have been introduced for viewport prediction in tile-based VR video streaming over the last years. On the one hand, <italic>content-agnostic</italic> approaches estimate the trajectory the viewer is likely to follow based on the viewport center locations of the last few milliseconds. To do so, some of these approaches use linear projection on the previous viewport positions [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR34">34</xref>], while others rely on machine learning models trained on user movement traces [<xref ref-type="bibr" rid="CR35">35</xref>, <xref ref-type="bibr" rid="CR36">36</xref>]. <italic>Content-aware</italic> techniques on the other hand, attempt to anticipate user movements based not only on an estimation of the viewer’s trajectory, but also on specific features derived from the video content itself such as <italic>image saliency</italic>, <italic>fixation density</italic> and <italic>object motion maps</italic> [<xref ref-type="bibr" rid="CR37">37</xref>–<xref ref-type="bibr" rid="CR40">40</xref>].</p>
      <p id="Par21">In this work, we adopt the content-agnostic method proposed by van der Hooft et al. in [<xref ref-type="bibr" rid="CR12">12</xref>] for predicting the user’s viewport. In contrast to other content-agnostic solutions that assume the user moving on a path in the two-dimensional space defined by the equirectangular projection of the video, the method proposed in [<xref ref-type="bibr" rid="CR12">12</xref>] models the viewer’s movement as a trajectory on the <italic>unit sphere</italic>’s surface. In this way, the future location of the viewport center is estimated by unidirectionally extending the path covered by the viewer thus far across the surface of the unit sphere (<italic>spherical walk</italic>). This approach to viewport prediction provides a more natural approximation of the viewer’s motion within the 360<inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M30"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq15.gif"/></alternatives></inline-formula> video scene. This allows for a more accurate prediction compared to alternative content-agnostic solutions using linear extrapolation of the user’s trajectory over the equirectangular projection of the video.</p>
      <p id="Par22">It is worth noting that the content prefetching mechanism we propose does not involve any substantial modification to the adopted viewport prediction scheme. Besides, while we favor the use of spherical-walk based viewport prediction—mainly due to its enhanced accuracy—the devised prefetching mechanism is easily compatible with other alternative content-agnostic viewport prediction methods such as those proposed by Petrangeli et al. [<xref ref-type="bibr" rid="CR31">31</xref>] and Xu et al. [<xref ref-type="bibr" rid="CR34">34</xref>].</p>
      <p id="Par23">Along with the viewport prediction scheme based on spherical walks, we also adopt the <italic>Center Tile First</italic> (CTF) rate adaptation heuristic proposed in [<xref ref-type="bibr" rid="CR13">13</xref>]. The intent behind this heuristic is to maximize the quality level for the video tiles located closer to the viewport center. In doing so, tiles from an equirectangular VR video are ranked according to the great-circle distance between their center and the viewport predicted location. The closer a certain tile is to the viewport center, the higher its priority and the quality representation that gets assigned to it.</p>
      <p id="Par24">As illustration, consider the example in Fig. <xref rid="Fig2" ref-type="fig">2</xref> for a <inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$4\times 4$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq16.gif"/></alternatives></inline-formula> tiling scheme and two quality levels. The diagram outlines both the viewport (circular area on the sphere in Fig. <xref rid="Fig2" ref-type="fig">2</xref>a) and viewport center (indicated as a cross mark). In this example, the CTF heuristic has prioritized the six tiles that lie closer to the viewport center, assigning them a high quality representation.<fig id="Fig2"><label>Fig. 2</label><caption><p>Example of the application of the CTF rate adaptation heuristic for a setup with a <inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$4\times 4$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq17.gif"/></alternatives></inline-formula> tiling scheme and two quality levels: The highest quality representation gets assigned to the blue-shaded tiles, while the remaining ones are requested in the lowest quality. The number of high/low quality tiles in this example is arbitrary as it depends on the network conditions between client and server</p></caption><graphic xlink:href="10922_2022_9649_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par25">The output of the rate adaptation heuristic is represented as an array that encodes the tile ranking, along with the quality level assigned to each of the tiles. Traditionally, a VR client would take said array and download each of the tiles, at the specified quality level, into the playout buffer. The prefetching mechanism we propose contemplates an extra step: forwarding the rate adaptation result to the cache-enabled edge server as soon as it is generated, before the client starts buffering video content for a given segment.</p>
      <p id="Par26">Returning to the example introduced earlier, the output of the CTF rate adaptation heuristic in that case comprises six high-quality plus ten low-quality tiles, following the order indicated below in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. As the diagram illustrates, the client relays this output array to the <italic>Prefetch server</italic>. With this information, the specified tiles are requested concurrently from the <italic>Content server</italic>, taking advantage of a high-capacity link between them. Then, the corresponding video files are loaded into the cache memory, which serves the forthcoming requests from the client with low latency. This in turn should lead to an increase in the bandwidth perceived by the client, and as consequence, also in the quality of the content requested for subsequent video segments.<fig id="Fig3"><label>Fig. 3</label><caption><p>VR content prefetching: The output of the rate adaptation algorithm is fed to the prefetch server before the client’s buffer starts filling up</p></caption><graphic xlink:href="10922_2022_9649_Fig3_HTML" id="MO3"/></fig></p>
      <p id="Par27">Clearly, conducting such a prefetching procedure for each individual user would entail a misuse of the cache memory resources and a substantial increase in the backhaul traffic and the content server’s load. To address this issue, we propose a stream processing method for estimating the most salient tiles according to the viewers fixation patterns, on a per-video segment basis. Said set of per-segment salient tiles is then stored into the data structure we refer to as DCoB.</p>
    </sec>
    <sec id="Sec8">
      <title>Dynamic Collective Buffer (DCoB)</title>
      <p id="Par28">The DCoB can be understood as a common playout buffer shared by active viewers consuming the same VR content at a certain point in time. Think about the scenario in which a content provider premieres a new episode of a popular show. Many viewers are likely to start a streaming session soon after the episode has been released. In such scenario, clients can benefit greatly from a nearby cache serving content that has been previously requested by other users. Of course, to make the most of the limited memory resources, only a subset of the tiles per segment should be stored into this cache, i.e. those that are most likely to be consumed in ongoing streaming sessions. Arranged in this way, the data in the cache configures a <italic>per-segment collective viewport</italic> or <italic>collective buffer</italic> keeping content from the last <italic>N</italic> video segments consumed thus far.</p>
      <p id="Par29">This collective buffer has been modeled as a FIFO queue of limited size, backed by a hash table to allow for instantaneous retrieval (Fig. <xref rid="Fig4" ref-type="fig">4</xref> below). Once the configured capacity is exceeded, the tiles corresponding to the least recently requested segment are evicted, freeing up space in memory for new segments.<fig id="Fig4"><label>Fig. 4</label><caption><p>Collective buffer as a FIFO queue. Each item in the queue corresponds to one segment of a given video and contains the most relevant tiles prefetched from the content server</p></caption><graphic xlink:href="10922_2022_9649_Fig4_HTML" id="MO4"/></fig></p>
      <p id="Par30">The set of video tiles contained within each of the segments of the collective buffer should be dynamically adjusted in response to viewers’ fixation patterns. In Sect. <xref rid="Sec7" ref-type="sec">3.1</xref>, a tile ranking was obtained as output of the CTF rate adaptation heuristic. This ordered list of tiles encodes the estimated fixation map of an individual user when watching a particular video segment. In this sense, we have devised an incremental procedure that enables merging the ordered preferences of all the users with an active streaming session, into a single list of video tiles per segment, composing a collective fixation map.</p>
      <p id="Par31">Let us represent a viewer’s fixation map for video <italic>v</italic> and segment <italic>s</italic> as:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \phi _{v, s} = \{ \langle t, \rho (t)\rangle : t \in \{1, ..., m\cdot n\}\} \end{aligned}$$\end{document}</tex-math><mml:math id="M36" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>ρ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>·</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="10922_2022_9649_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>t</italic> represents each of the <inline-formula id="IEq18"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m\cdot n$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mi>m</mml:mi><mml:mo>·</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq18.gif"/></alternatives></inline-formula> tiles per segment in the tiling scheme (<inline-formula id="IEq19"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m\times n$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq19.gif"/></alternatives></inline-formula>), while <inline-formula id="IEq20"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rho$$\end{document}</tex-math><mml:math id="M42"><mml:mi>ρ</mml:mi></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq20.gif"/></alternatives></inline-formula> is a function that returns the position in the viewer’s tile ranking of the tile passed as argument. Considering the running example from the previous section (Fig. <xref rid="Fig3" ref-type="fig">3</xref>), the corresponding fixation map can be expressed in the following terms:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \begin{aligned} \phi _{v, s} = \{&amp;\langle \mathsf {1, 14}\rangle , \langle \mathsf {2, 8}\rangle , \langle \mathsf {3, 4}\rangle , \langle \mathsf {4, 8}\rangle , \langle \mathsf {5, 13}\rangle , \langle \mathsf {6, 7}\rangle , \langle \mathsf {7, 1}\rangle , \langle \mathsf {8, 3}\rangle , \langle \mathsf {9, 15}\rangle , \\&amp;\langle \mathsf {10, 9}\rangle , \langle \mathsf {11, 2}\rangle , \langle \mathsf {12, 6}\rangle , \langle \mathsf {13, 16}\rangle , \langle \mathsf {14, 12}\rangle , \langle \mathsf {15, 11}\rangle , \langle \mathsf {16, 10}\rangle \} \end{aligned} \end{aligned}$$\end{document}</tex-math><mml:math id="M44" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">1</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">14</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">2</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">8</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">3</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">4</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">4</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">8</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">5</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">13</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">6</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">7</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">7</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">1</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">8</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">3</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">9</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">15</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">10</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">9</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">11</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">2</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">12</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">6</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">13</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">16</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">14</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">12</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">15</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">11</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mn mathvariant="sans-serif">16</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="sans-serif">10</mml:mn></mml:mrow><mml:mo stretchy="false">⟩</mml:mo><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="10922_2022_9649_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>Now, to combine the fixation maps of <italic>K</italic> viewers watching segment <italic>s</italic> of video <italic>v</italic>, we start by computing the average position, <inline-formula id="IEq21"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rho$$\end{document}</tex-math><mml:math id="M46"><mml:mi>ρ</mml:mi></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq21.gif"/></alternatives></inline-formula>, for each video tile over all <italic>K</italic>-fixation maps. The collective fixation map (<inline-formula id="IEq22"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{\phi }_{v, s}$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq22.gif"/></alternatives></inline-formula>) is defined as follows:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \bar{\phi }_{v, s} = \bigg \{ \bigg \langle t, \frac{1}{K}\sum _{i=1}^{K}\rho _{\langle i\rangle }(t)\bigg \rangle : t \in \{1, ..., m\cdot n\}\bigg \} \end{aligned}$$\end{document}</tex-math><mml:math id="M50" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em" stretchy="true">{</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em" stretchy="true">〈</mml:mo></mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em" stretchy="true">〉</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>·</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em" stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="10922_2022_9649_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>The order of the tiles in <inline-formula id="IEq23"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{\phi }_{v, s}$$\end{document}</tex-math><mml:math id="M52"><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq23.gif"/></alternatives></inline-formula> is determined by their average position, i.e. the smaller this value is for a certain tile, the higher the precedence the tile has for the given video segment.</p>
      <p id="Par32">As stated earlier, only a subset of these tiles should make it to the corresponding segment of the collective buffer. We refer to this subset as <italic>collective viewport</italic>, defined as the <italic>top-k</italic> tiles of the collective fixation map. To determine the value of <italic>k</italic> we first estimate the correlation between the viewers’ fixation maps. High correlation between these maps would imply that users are looking at the same sections of the display, i.e. a few specific tiles. We estimate said correlation by using the <italic>Kendall’s tau coefficient</italic> (<inline-formula id="IEq24"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal K}_{\tau }}$$\end{document}</tex-math><mml:math id="M54"><mml:msub><mml:mi mathvariant="script">K</mml:mi><mml:mi>τ</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq24.gif"/></alternatives></inline-formula>) [<xref ref-type="bibr" rid="CR41">41</xref>], which measures the correspondence between two ordered sequences in the range <inline-formula id="IEq25"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left[ -1, 1\right]$$\end{document}</tex-math><mml:math id="M56"><mml:mfenced close="]" open="["><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mfenced></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq25.gif"/></alternatives></inline-formula>: the closer to 1 (resp. <inline-formula id="IEq26"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq26.gif"/></alternatives></inline-formula>) the higher (resp. lower) the correspondence. Finally, the value of <italic>k</italic> is set to be proportional to the complement of this correlation coefficient, which we refer to as <italic>Kendall’s tau distance</italic> (<inline-formula id="IEq27"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal {K}_{\tau dist}$$\end{document}</tex-math><mml:math id="M60"><mml:msub><mml:mi mathvariant="script">K</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq27.gif"/></alternatives></inline-formula>). Let us take <inline-formula id="IEq28"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{\phi }_{{}_{curr}v, s}$$\end{document}</tex-math><mml:math id="M62"><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="italic">curr</mml:mi></mml:mrow></mml:msub><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq28.gif"/></alternatives></inline-formula> as the current collective fixation map for segment <italic>s</italic> of video <italic>v</italic>, and <inline-formula id="IEq29"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\phi _{{}_{u}v, s}$$\end{document}</tex-math><mml:math id="M64"><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:msub><mml:mrow/><mml:mi>u</mml:mi></mml:msub><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq29.gif"/></alternatives></inline-formula> as a new fixation map corresponding to user <italic>u</italic>, for the same video segment. The collective viewport size, <italic>k</italic>, is computed as follows:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} k &amp; = \left\lceil {m \cdot n \cdot \kappa _{{\tau {\text{dist}}}} (\bar{\phi }_{{{\text{curr}}^{{\upsilon ,s}} }} ,\phi _{{u^{{\upsilon ,s}} }} )} \right\rceil ;\quad \\ \kappa _{{\tau {\text{dist}}}} (\bar{\phi }_{{{\text{curr}}^{{\upsilon ,s}} }} ,\phi _{{u^{{\upsilon ,s}} }} ) &amp; = 1 - \frac{{\kappa _{\tau } (\bar{\phi }_{{{\text{curr}}^{{\upsilon ,s}} }} ,\phi _{{u^{{\upsilon ,s}} }} ) + 1}}{2} \\ \end{aligned}$$\end{document}</tex-math><mml:math id="M66" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mi>k</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfenced close="⌉" open="⌈"><mml:mrow><mml:mi>m</mml:mi><mml:mo>·</mml:mo><mml:mi>n</mml:mi><mml:mo>·</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mtext>dist</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover><mml:msup><mml:mrow><mml:mtext>curr</mml:mtext></mml:mrow><mml:mrow><mml:mi>υ</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mi>υ</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mo>;</mml:mo><mml:mspace width="1em"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow/><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mtext>dist</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover><mml:msup><mml:mrow><mml:mtext>curr</mml:mtext></mml:mrow><mml:mrow><mml:mi>υ</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mi>υ</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover><mml:msup><mml:mrow><mml:mtext>curr</mml:mtext></mml:mrow><mml:mrow><mml:mi>υ</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mi>υ</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="10922_2022_9649_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>From the equations in  <xref rid="Equ4" ref-type="">4</xref>, note that in case of perfect correlation (<inline-formula id="IEq30"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal {K}_{\tau } = 1$$\end{document}</tex-math><mml:math id="M68"><mml:mrow><mml:msub><mml:mi mathvariant="script">K</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq30.gif"/></alternatives></inline-formula>), the distance between the fixation maps is zero (<inline-formula id="IEq31"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal {K}_{\tau dist} = 0$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:msub><mml:mi mathvariant="script">K</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq31.gif"/></alternatives></inline-formula>), and therefore the viewport size, <italic>k</italic>, is equal to zero as well. In these circumstances, since both the <italic>collective</italic> and <italic>new</italic> fixation maps contain the same collection of tiles, the collective viewport stored into the DCoB for the given segment and video should remain unmodified.</p>
      <p id="Par33">The collective fixation map is incrementally refined as new viewers show up. For this, the prefetch server keeps track of the number of viewers (<italic>nViews</italic>) that have watched a given video segment, along with the per-segment cumulative Kendall’s tau distance (<inline-formula id="IEq32"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$agg\mathcal {K}_{\tau dist}$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi mathvariant="script">K</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq32.gif"/></alternatives></inline-formula>) computed across all the fixation maps received thus far. This data is kept in a key-value store with the tuple <inline-formula id="IEq33"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\langle v,s\rangle$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq33.gif"/></alternatives></inline-formula> being designated as key:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \mathcal {F} : (\langle v,s\rangle ) \mapsto \left[ \bar{\phi }_{v, s}, nViews, agg\mathcal {K}_{\tau dist}\right] \end{aligned}$$\end{document}</tex-math><mml:math id="M76" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo>:</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>↦</mml:mo><mml:mfenced close="]" open="["><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi mathvariant="script">K</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="10922_2022_9649_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>The formal procedure for processing the stream of fixation maps coming from connected VR clients is specified below in Algorithm 1. The process starts by first initializing <inline-formula id="IEq34"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal {F}$$\end{document}</tex-math><mml:math id="M78"><mml:mi mathvariant="script">F</mml:mi></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq34.gif"/></alternatives></inline-formula> as an empty key-value store (line 7). Then the fixation maps <inline-formula id="IEq35"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\phi _{v, s}$$\end{document}</tex-math><mml:math id="M80"><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq35.gif"/></alternatives></inline-formula> are taken in, one after the other (line 8). Each fixation map updates its corresponding entry on the collective buffer. The <italic>mergeFixationMaps</italic> function in line 16 represents the incremental application of the operation referred earlier in Eq. <xref rid="Equ3" ref-type="">3</xref>. The output of this function is the collective fixation map modified by the fixation map being currently processed. The size of the collective viewport, <italic>k</italic>, is determined as the closest integer to the product of the average Kendall’s tau distance (<inline-formula id="IEq36"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{agg\mathcal {K}_{\tau dist}}{nViews}$$\end{document}</tex-math><mml:math id="M82"><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi mathvariant="script">K</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="italic">nViews</mml:mi></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq36.gif"/></alternatives></inline-formula>) times the total number of tiles (<inline-formula id="IEq37"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m\cdot n$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:mi>m</mml:mi><mml:mo>·</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq37.gif"/></alternatives></inline-formula>). This way the input from previous viewers is weighted and taken into account (line 19). Finally, the tiles belonging to the collective viewport are obtained (i.e. the first <italic>k</italic> tiles from the collective fixation map), the corresponding video files are retrieved from the <italic>content server</italic>, and the up-to-date data is stored into the collective buffer (<bold><italic>DCoB</italic></bold>) and the key-value store (<inline-formula id="IEq38"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal {F}$$\end{document}</tex-math><mml:math id="M86"><mml:mi mathvariant="script">F</mml:mi></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq38.gif"/></alternatives></inline-formula>) (lines 25–29), after ensuring that the maximum configured capacity (<italic>N</italic>) is not exceeded (lines 21–24). <graphic position="anchor" xlink:href="10922_2022_9649_Figa_HTML" id="MO10"/><fig id="Fig5"><label>Fig. 5</label><caption><p>Timeline of a typical interaction between the entities composing the VR content prefetching approach. At <italic>prefetching time</italic> the collective viewport and outstanding tiles are downloaded into the prefetch server memory. These tiles are served to the client with low latency at <italic>querying time</italic></p></caption><graphic xlink:href="10922_2022_9649_Fig5_HTML" id="MO11"/></fig></p>
      <p id="Par34">Along with the collective buffer, we also defined a short-lived buffer into which the prefetch server stores the set of <italic>outstanding tiles</italic>, namely those tiles in the viewer’s fixation map that remain outside the collective viewport. This in order to avoid the client having to wait for the content server to deliver these tiles during querying time, preventing playout freezes from happening. The entries in this <italic>ephemeral buffer</italic> are volatile and expire over a period of time equivalent to one video segment to minimize their memory footprint. Having both the collective and ephemeral buffers in place ensures that the client can always find relevant content loaded into the prefetch server memory. This way we manage to bypass the <italic>cold-start</italic> problem typical of traditional caching solutions. Figure <xref rid="Fig5" ref-type="fig">5</xref> illustrates a typical sequence of interactions that take place between client, servers and data stores for a single viewer.</p>
    </sec>
    <sec id="Sec9">
      <title>Analysis of Computational Cost</title>
      <p id="Par35">The procedure in charge of conducting content prefetching has been conceived as a <italic>stateful streaming algorithm</italic> (see Algorithm 1). The input of said procedure consists of regular array structures representing the viewer’s fixation maps (consider the example in Eq. <xref rid="Equ2" ref-type="">2</xref>). The length of these arrays is fixed and determined by the number of tiles of the tiling scheme in use, i.e., <inline-formula id="IEq39"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m\cdot n$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:mi>m</mml:mi><mml:mo>·</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq39.gif"/></alternatives></inline-formula>. The proposed algorithm processes each array on an individual basis, and the output of such a processing alters the state of a collective fixation map and the collective and ephemeral buffers, for a given video <italic>v</italic> and segment <italic>s</italic>. These data structures represent the state being managed by the algorithm. Let us consider the cost incurred in this procedure both in terms of space and time.</p>
      <sec id="Sec10">
        <title>Space Cost</title>
        <p id="Par36">As described earlier in Sect. <xref rid="Sec8" ref-type="sec">3.2</xref>, the data structures that maintain the state in the proposed algorithm are all arranged into hash tables persisted in memory to allow for fast read and write operations. The hash tables of both the key-value store holding the per-segment collective fixation maps (<inline-formula id="IEq40"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal {F}$$\end{document}</tex-math><mml:math id="M90"><mml:mi mathvariant="script">F</mml:mi></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq40.gif"/></alternatives></inline-formula>), and the collective buffer (<inline-formula id="IEq41"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{DCoB}$$\end{document}</tex-math><mml:math id="M92"><mml:mrow><mml:mi mathvariant="bold-italic">DCoB</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq41.gif"/></alternatives></inline-formula>) have a fixed capacity in terms of the number of segments they can contain. Said capacity is set upfront via a configuration parameter <italic>N</italic>. In this sense, the space cost due to these two data structures is proportional to <italic>O</italic>(<italic>N</italic>).</p>
        <p id="Par37">The hash table backing the ephemeral buffer stores individual tiles which are not part of the collective viewport for a given video and segment. In these circumstances, the space cost is proportional to the number of tiles the viewer is likely to watch in the upcoming segment that fall outside the collective viewport. Said number is never greater than <inline-formula id="IEq42"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m \cdot n$$\end{document}</tex-math><mml:math id="M94"><mml:mrow><mml:mi>m</mml:mi><mml:mo>·</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq42.gif"/></alternatives></inline-formula> (worst-case scenario). Additionally, the video content persisted in this ephemeral buffer is short-lived by design, which further reduces its memory footprint.</p>
      </sec>
      <sec id="Sec11">
        <title>Time Cost</title>
        <p id="Par38">At the core of the procedure for maintaining the collective buffer lie two operations: <def-list><def-item><term><italic>i.</italic></term><def><p id="Par39">the function that updates the collective fixation map (<inline-formula id="IEq43"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{\phi }_{v, s}$$\end{document}</tex-math><mml:math id="M96"><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq43.gif"/></alternatives></inline-formula>) for a certain video <italic>v</italic> and segment <italic>s</italic>, taking in a new unseen fixation map (<inline-formula id="IEq44"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\phi _{v, s}$$\end{document}</tex-math><mml:math id="M98"><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq44.gif"/></alternatives></inline-formula>) (see line 16 in Algorithm 1)</p></def></def-item><def-item><term><italic>ii.</italic></term><def><p id="Par40">the function that incrementally computes the Kendall’s tau distance (<inline-formula id="IEq45"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal {K}_{\tau dist}$$\end{document}</tex-math><mml:math id="M100"><mml:msub><mml:mi mathvariant="script">K</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq45.gif"/></alternatives></inline-formula>) between the current <inline-formula id="IEq46"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar{\phi }_{v, s}$$\end{document}</tex-math><mml:math id="M102"><mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq46.gif"/></alternatives></inline-formula> and the incoming <inline-formula id="IEq47"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\phi _{v, s}$$\end{document}</tex-math><mml:math id="M104"><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq47.gif"/></alternatives></inline-formula> (see line 18 in Algorithm 1)</p></def></def-item></def-list></p>
        <p id="Par41">The first operation consists of computing the element-wise average of two indexed arrays of size <inline-formula id="IEq48"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m \cdot n$$\end{document}</tex-math><mml:math id="M106"><mml:mrow><mml:mi>m</mml:mi><mml:mo>·</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq48.gif"/></alternatives></inline-formula>, and subsequently sorting the resulting array on the obtained values. By using an algorithm such as <italic>mergesort</italic>, the time it takes for this operation to run is proportional to <inline-formula id="IEq49"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$O(mn\log mn)$$\end{document}</tex-math><mml:math id="M108"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mi>n</mml:mi><mml:mo>log</mml:mo><mml:mi>m</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq49.gif"/></alternatives></inline-formula>.</p>
        <p id="Par42">The Kendall’s tau distance in the second operation is computed using the method by Knight [<xref ref-type="bibr" rid="CR42">42</xref>], implemented in the SciPy Python library. This method is known to have linearithmic time complexity, which in this particular case translates to <inline-formula id="IEq50"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$O(mn\log mn)$$\end{document}</tex-math><mml:math id="M110"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mi>n</mml:mi><mml:mo>log</mml:mo><mml:mi>m</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq50.gif"/></alternatives></inline-formula>, just as with the above-mentioned operation.<fig id="Fig6"><label>Fig. 6</label><caption><p>Experimental determination of the time required to compute the collective viewport. An in-depth description of the setup is provided in Sect. <xref rid="Sec17" ref-type="sec">5.1</xref></p></caption><graphic xlink:href="10922_2022_9649_Fig6_HTML" id="MO12"/></fig></p>
        <p id="Par43">Since <italic>m</italic> and <italic>n</italic> values are fixed and typically small (consider for instance a <inline-formula id="IEq51"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$4\times 4$$\end{document}</tex-math><mml:math id="M112"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq51.gif"/></alternatives></inline-formula> tiling scheme), the proposed algorithm is expected to feature a low and fairly consistent execution time. Figure <xref rid="Fig6" ref-type="fig">6</xref> shows an example of the computation times measured on an experimental setting with 48 viewers watching the first 30 segments of three different 360<inline-formula id="IEq52"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M114"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq52.gif"/></alternatives></inline-formula> videos, using a <inline-formula id="IEq53"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$4\times 4$$\end{document}</tex-math><mml:math id="M116"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq53.gif"/></alternatives></inline-formula> tiling scheme. In said setting (described in detail later in Sect. <xref rid="Sec17" ref-type="sec">5.1</xref>), the devised operations for computing the collective viewport run under 20 milliseconds 80% of the time. This is only <inline-formula id="IEq54"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{1}{50}$$\end{document}</tex-math><mml:math id="M118"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>50</mml:mn></mml:mfrac></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq54.gif"/></alternatives></inline-formula> to <inline-formula id="IEq55"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{1}{200}$$\end{document}</tex-math><mml:math id="M120"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>200</mml:mn></mml:mfrac></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq55.gif"/></alternatives></inline-formula> of the video segment length used in tile-based omnidirectional video streaming applications, which typically ranges between one to four seconds [<xref ref-type="bibr" rid="CR16">16</xref>].</p>
        <p id="Par44">Note that the computational cost of the proposed mechanism largely depends on configuration parameters such as the collective buffer capacity (<italic>N</italic>) and the tiling scheme (<inline-formula id="IEq56"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m\times n$$\end{document}</tex-math><mml:math id="M122"><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq56.gif"/></alternatives></inline-formula>). This suggests that, as the number of users increases, memory use will not surge out of control and processing time will remain consistent, which accounts for the scalability of our content prefetching approach.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Architecture and Proof-of-Concept Implementation</title>
    <p id="Par45">The system that implements the content prefetching mechanisms we introduced in the previous section adopts an architecture featuring highly configurable containerized components. This system supports the emulation of multiple VR video streaming scenarios—with and without prefetching enabled—under different network and load conditions. A diagram of the components and submodules that make up the system is presented in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. Next, we address the description of the components of this architecture.<fig id="Fig7"><label>Fig. 7</label><caption><p>VR content prefetching architecture: inspired by the <sc>Explora</sc> framework by Ordonez et al. [<xref ref-type="bibr" rid="CR43">43</xref>]</p></caption><graphic xlink:href="10922_2022_9649_Fig7_HTML" id="MO13"/></fig></p>
    <sec id="Sec13">
      <title>Prefetch Server</title>
      <p id="Par46">This is the core component of the system. In devising the functional submodules of this server, we have drawn inspiration from the data processing pipeline presented by Ordonez et al. in [<xref ref-type="bibr" rid="CR43">43</xref>], which decouples stream data ingestion/preprocessing from data storage and content retrieval. The prefetch server features three main modules: (<italic>1</italic>) the <italic>prefetching component</italic>, (<italic>2</italic>) the <italic>content buffers</italic>, and (<italic>3</italic>) the <italic>retrieval component</italic>.</p>
      <p id="Par47">The <italic>prefetching component</italic> provides an <italic>event bus</italic> which collects the viewers’ fixation maps fed by the VR client. A <italic>stream processor</italic> in this component consumes said fixation maps and runs the procedure specified earlier in Algorithm 1 to incrementally build the collective viewports. The stream processor is also in charge of fetching video content from the <italic>Content server</italic>, and does this by issuing multiple concurrent HTTP requests. We relied on the <italic>Publish/Subscribe</italic> pattern readily available in the <italic>Redis</italic> in-memory data store [<xref ref-type="bibr" rid="CR44">44</xref>] to implement the event bus. As for the stream processor, we implemented it as a Python application running continuously in background, along with the <italic>HTTP API</italic> in charge of handling the interaction with the client.</p>
      <p id="Par48">The video content fetched from the content server by the stream processor is loaded into the <italic>data buffers</italic>. The <italic>collective buffer</italic> hosts the arrangement of video tiles lying inside the incrementally computed viewports, while the <italic>ephemeral buffer</italic> stores the outstanding tiles as defined at the end of Sect. <xref rid="Sec8" ref-type="sec">3.2</xref>. Both buffers are backed by key-value databases implemented in <italic>Redis</italic>.</p>
      <p id="Par49">The <italic>retrieval component</italic> implements the <italic>querying handler</italic> submodule in charge of processing clients’ requests for video content. Upon receiving a query, this handler looks up the corresponding video tile file into both the collective and ephemeral buffers. In case the video file is not available yet in none of the prefetch buffers (e.g., due for instance to quality mismatch or network delay), the handler would relay the request to the content server.</p>
      <p id="Par50">The implementation of the prefetch server is available online at <ext-link ext-link-type="uri" xlink:href="https://github.com/LeandroOrdonez/explora-vr-cache">https://github.com/LeandroOrdonez/explora-vr-cache</ext-link>.</p>
    </sec>
    <sec id="Sec14">
      <title>Content Server</title>
      <p id="Par51">This component plays the role of one of the nodes from a content delivery network (CDN). The <italic>content server</italic> consists of a containerized Web server publishing the tiled video content through a HTTP API. Video files are served from the local file system of this component in response to regular HTTP/1.1 GET requests matching the following the URL pattern: <graphic position="anchor" xlink:href="10922_2022_9649_Figb_HTML" id="MO14"/>
where t_hor and t_vert stand for the number of tiles in the horizontal and vertical axes respectively, according to the applied tilling scheme.</p>
      <p id="Par52">This content server component was implemented as a Python Web application using the <italic>Flask</italic> framework and <italic>NGINX+uWSGI</italic> as application server. The code of this implementation is available online as well at <ext-link ext-link-type="uri" xlink:href="https://github.com/LeandroOrdonez/explora-vr-server">https://github.com/LeandroOrdonez/explora-vr-server</ext-link>.</p>
    </sec>
    <sec id="Sec15">
      <title>Client</title>
      <p id="Par53">This component is a containerized adaptation of the headless Virtual Reality client developed by van der Hooft et al. [<xref ref-type="bibr" rid="CR32">32</xref>]. The headless VR client is an adaptive streaming application written in Python which is able to recreate video streaming sessions from prerecorded head movement traces. By deploying this component as an independent containerized application, we were able to spawn multiple concurrent video streaming sessions, allowing us to assess the response of the proposed VR video content prefetching mechanism under different network and load conditions. The code of the original implementation of the headless VR client is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jvdrhoof/VRClient">https://github.com/jvdrhoof/VRClient</ext-link>, while our adaptation can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/LeandroOrdonez/explora-vr-dash-client">https://github.com/LeandroOrdonez/explora-vr-dash-client</ext-link>.</p>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Experimental Evaluation</title>
    <p id="Par54">To determine the strengths, costs and limitations of the content prefetching mechanism, we have conducted a benchmark evaluation on various VR video streaming setups, with and without the prefetch mechanism in place. The prefetching approach presented in this article was also compared to a caching strategy with a traditional <italic>least-recently used</italic> (LRU) replacement policy which is a common baseline used for evaluating the performance of existing edge-assisted solutions. A description of the environment configuration and the covered test scenarios is presented next, along with the results obtained from this evaluation.</p>
    <sec id="Sec17">
      <title>Experimental Setup</title>
      <p id="Par55">The experimental testbed we used in this evaluation is depicted in Fig. <xref rid="Fig8" ref-type="fig">8</xref>. Each of the components in this diagram were deployed as an isolated <italic>Docker</italic> container, running on a single host machine with 20GB RAM, Intel E5645s @ 2.4GHz processor, and 54GB Hard Disk, using the infrastructure provided by the <italic>imec/IDLab Virtual Wall</italic> environment [<xref ref-type="bibr" rid="CR45">45</xref>]. As is typically the case, we assume the link between the content server and the cache-enabled edge server to have higher capacity/higher latency than the one between the prefetch server and the VR clients. To emulate these conditions, we have run <italic>traffic control</italic> (tc) [<xref ref-type="bibr" rid="CR46">46</xref>] on each of the containers. This way, we have provisioned a connection between content and prefetch server with 1 Gbps bandwidth capacity and 25 milliseconds latency. On the client’s end, we set the latency to 5 milliseconds for the setup with prefetching enabled, and 30 milliseconds in the setup without prefetching—i.e. we kept the same round trip time (RTT) between client and content server in both setups. We gradually increased the bandwidth in the clients link from 10 Mbps to 50 Mbps, and estimated the impact the devised prefetching mechanism has on the quality of experience (QoE) perceived by the user, measured in terms of delivered video quality, startup delay, and occurrence of playout freezes, as reported by the VR client on a per-segment basis. Finally, these results are contrasted to those obtained from a setup implementing a traditional LRU replacement policy in the cache-enabled edge server.<fig id="Fig8"><label>Fig. 8</label><caption><p>Experimental testbed for evaluating the VR content prefetching mechanism</p></caption><graphic xlink:href="10922_2022_9649_Fig8_HTML" id="MO15"/></fig></p>
      <p id="Par56">As for the video content we used the dataset created by Wu et al. [<xref ref-type="bibr" rid="CR21">21</xref>], which provides head movement traces recorded from 360<inline-formula id="IEq57"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M124"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq57.gif"/></alternatives></inline-formula> video streaming sessions. This dataset comprises the traces collected from 48 unique users while watching nine different VR videos. The tests run in this evaluation consider three representative videos out of the original nine: <italic>Sandwich</italic> features a fragment of a talk show in which most of the motion concentrated in the center of the display; <italic>Spotlight</italic> presents a more dynamic sequence typical for an action movie; <italic>Surf</italic> displays a compilation of video clips recorded with a GoPro camera in an open environment. A tiling scheme of <inline-formula id="IEq58"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$4\times 4$$\end{document}</tex-math><mml:math id="M126"><mml:mrow><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq58.gif"/></alternatives></inline-formula> was applied to each of these videos at 4K resolution and 30 FPS, using the same encoder and parameters discussed in [<xref ref-type="bibr" rid="CR32">32</xref>] and listed in Table <xref rid="Tab2" ref-type="table">2</xref>. We used two quality levels to encode each of the three videos, corresponding to constant rate factors (CRF) of 15 (<italic>High quality</italic>) and 35 (<italic>Low quality</italic>). Table <xref rid="Tab3" ref-type="table">3</xref> summarizes the resulting bitrates for both quality representations.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Overview of encoding parameters</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Parameter</th><th align="left">Value</th></tr></thead><tbody><tr><td align="left">Encoder</td><td align="left">HEVC test model (HM)</td></tr><tr><td align="left">Tiling scheme</td><td align="left">4 <inline-formula id="IEq59"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M128"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq59.gif"/></alternatives></inline-formula> 4 at 4K resolution and 30 FPS</td></tr><tr><td align="left">GOP</td><td align="left">32</td></tr><tr><td align="left">Segment duration</td><td align="left"><inline-formula id="IEq60"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\approx$$\end{document}</tex-math><mml:math id="M130"><mml:mo>≈</mml:mo></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq60.gif"/></alternatives></inline-formula> 1.067s</td></tr><tr><td align="left">CRF</td><td align="left">[15, 35]</td></tr></tbody></table></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>Quality levels and corresponding bitrates for the three videos</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Video</th><th align="left" colspan="2">Bitrate [Mbps]</th></tr><tr><th align="left">High quality</th><th align="left">Low quality</th></tr></thead><tbody><tr><td align="left">Sandwich</td><td align="left">21.9 ± 6.6</td><td align="left">1.2 ± 0.3</td></tr><tr><td align="left">Spotlight</td><td align="left">20.8 ± 13.9</td><td align="left">1.4 ± 1.3</td></tr><tr><td align="left">Surf</td><td align="left">26.4 ± 12.7</td><td align="left">2.4 ± 1.4</td></tr></tbody></table></table-wrap></p>
      <p id="Par57">With this setup in place, we proceeded to emulate a scenario with multiple users connecting to a video streaming event. In this scenario, each of the 48 viewers in the dataset by Wu et al. [<xref ref-type="bibr" rid="CR21">21</xref>] would start a streaming session to watch the first 30 segments—this is 32 seconds for a segment duration of 1.067 seconds—of each of the three considered videos. In order to approximate the dynamics of such <italic>near-live on-demand</italic> streaming scenario serving multiple users, we set up the experiment so that viewers arrive to their watching session in quick succession with a 5 second separation between each other. This means there were no more than six users watching the same video at a given time.</p>
      <p id="Par58">We have run this simulation for three different configurations: (<italic>i</italic>) NO_PRE-FETCH: no prefetching/cache enabled, (<italic>ii</italic>) PREFETCH: prefetching enabled with a collective buffer of 30 segments in size, and (<italic>iii</italic>) LRU: caching with LRU replacement strategy and cache size limited to 70MB, which is slightly above the maximum value of memory used by the prefetching mechanism throughout the experiment, as shown below in Table <xref rid="Tab4" ref-type="table">4</xref>. For each configuration, we measured the performance of the system in terms of segment download time, user’s QoE (i.e., video quality, startup time, and occurrence of playback freezes), network traffic between content and edge server, and accuracy of the prefetch buffer/cache.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Memory consumed by the prefetching and caching strategies</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Bandwidth client’s link (Mbps)</th><th align="left" colspan="2">Edge server memory use (MB)</th></tr><tr><th align="left"><bold>LRU</bold> (%)</th><th align="left"><bold>PREFETCH</bold> (%)</th></tr></thead><tbody><tr><td align="left">10</td><td align="left">70</td><td align="left">57.91</td></tr><tr><td align="left">15</td><td align="left">70</td><td align="left">59.25</td></tr><tr><td align="left">20</td><td align="left">70</td><td align="left">61.11</td></tr><tr><td align="left">25</td><td align="left">70</td><td align="left">62.69</td></tr><tr><td align="left">30</td><td align="left">70</td><td align="left">67.04</td></tr><tr><td align="left">35</td><td align="left">70</td><td align="left">64.01</td></tr><tr><td align="left">40</td><td align="left">70</td><td align="left">67.30</td></tr><tr><td align="left">45</td><td align="left">70</td><td align="left">67.46</td></tr><tr><td align="left">50</td><td align="left">70</td><td align="left">66.29</td></tr></tbody></table></table-wrap></p>
      <p id="Par59">Finally, the versions of the software tools used in this evaluation are listed in Table <xref rid="Tab5" ref-type="table">5</xref>.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Versions of the software used in the experimental setup</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Software</th><th align="left">Version</th></tr></thead><tbody><tr><td align="left">Docker</td><td align="left">20.10.6, build 370c289</td></tr><tr><td align="left">Docker compose</td><td align="left">1.17.1</td></tr><tr><td align="left">Operating system</td><td align="left">Ubuntu 18.04.4 LTS</td></tr><tr><td align="left">Redis server</td><td align="left">5.0.3</td></tr><tr><td align="left">NGINX (<italic>content and prefetch</italic> servers)</td><td align="left">1.14.2</td></tr><tr><td align="left">uWSGI (<italic>content and prefetch</italic> servers)</td><td align="left">2.0.17.1</td></tr><tr><td align="left">Flask (<italic>content and prefetch</italic> servers)</td><td align="left">1.0.2</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec18">
      <title>Results</title>
      <p id="Par60">The playout buffer size in VR video streaming is limited to a few segments to allow for fast adaptation to viewport changes. In this sense, these kind of streaming applications are particularly susceptible to buffer starvation and playout freezes. In a setup with a cache-enabled edge server placed between clients and the content server, the rate adaptation heuristic might be tricked into believing that content is closer than it actually is, which leads it to request video tiles in high-quality representations. In case of <italic>cache misses</italic> (i.e. the requested content is not found in the cache’s memory) the request has to be relayed back to the server, which entails additional processing and network latency. In said cases, the segment download time might take longer than the segment playback duration. When such conditions persist for several segments during a watching session, buffer draining-out and playout freezes are bound to happen.<fig id="Fig9"><label>Fig. 9</label><caption><p>ECDF of the per-segment download time for the three tested configurations. The larger the number of segments taking longer than SEG_DUR to download, the more likely playout freezes are to occur</p></caption><graphic xlink:href="10922_2022_9649_Fig9_HTML" id="MO16"/></fig></p>
      <p id="Par61">Figure <xref rid="Fig9" ref-type="fig">9</xref> shows the empirical cumulative distribution function (ECDF) of the per-segment download time for the three configurations under evaluation (NO_PREFETCH, PREFETCH, and LRU), measured for multiple values of bandwidth on the client’s end. Note that both the setup with the proposed prefetching mechanism, as well as the one with the LRU cache replacement policy manage to keep download times under the segment duration limit (SEG_DUR line in Fig. <xref rid="Fig9" ref-type="fig">9</xref>) for most of the segments across all bitrates and videos. However, for the LRU setup, there is in general a larger proportion of segments taking longer to download than the segment duration: on average 14% of the segments in the LRU configuration, compared to only 7.6% of the segments in the PREFETCH setup. As the capacity on the client’s link increases, those segments can take as much as 3.9 seconds to download, which is far higher than the comparable download times from the PREFETCH setup which do not surpass 1.8 seconds in any of the cases. This signals a higher likelihood of cache misses for the LRU configuration, and a more frequent occurrence of playout freezes in this setup, specially for large values of bandwidth on the client’s connection.<fig id="Fig10"><label>Fig. 10</label><caption><p>Occurrence and duration of playout freezes: The PREFETCH and NO_PREFETCH configurations manage to deliver a freeze-free watching experience to the viewer. For the LRU setup, both frequency and duration of playout freezes increase as the bandwidth on the client’s link grows larger</p></caption><graphic xlink:href="10922_2022_9649_Fig10_HTML" id="MO17"/></fig></p>
      <p id="Par62">The foregoing is confirmed by measuring the number and duration of the playout freezes by streaming session. Figure <xref rid="Fig10" ref-type="fig">10</xref> reports on these measurements as a function of the client’s bandwidth, for each of the considered videos. Note that the setup with the proposed prefetch mechanism offers a <italic>freeze-free</italic> playback experience to the user, in contrast to the LRU counterpart. According to Fig. <xref rid="Fig10" ref-type="fig">10</xref>a, the average number of freezes per streaming session on the LRU configuration is always greater than zero, and the number increases for the three videos as the bandwidth grows larger. We can observe a similar behavior for the total freeze duration. Figure <xref rid="Fig10" ref-type="fig">10</xref>b presents this measurement as a proportion of the length of a streaming session, i.e. 32 seconds. These results are clearly inconvenient and counterintuitive from the client’s perspective, and can be attributed to the occurrence of cache misses. Table <xref rid="Tab6" ref-type="table">6</xref> below shows the cache hit ratio measured across the streaming sessions of all 48 users in the dataset, for both LRU and PREFETCH configurations. For the setup with content prefetching enabled, the hit ratio stays above 98% through the entire range of bandwidths, while for the configuration with LRU cache replacement it consistently decreases from 94.6% to 87% as the bandwidth increases. As the bandwidth in the client’s link grows larger, the quality of the requested content tends to increase, as does the size of the video tiles stored in the cache. In these circumstances, the LRU cache is only able to accommodate a few items, which in consequence increases the frequency of eviction cycles and cache misses.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Hit ratio for different values of bandwidth in the client’s link</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Bandwidth client’s link (Mbps)</th><th align="left" colspan="2">Hit ratio</th></tr><tr><th align="left"><bold>LRU</bold> (%)</th><th align="left"><bold>PREFETCH</bold> (%)</th></tr></thead><tbody><tr><td align="left">10</td><td align="left">94.62</td><td align="left">98.44</td></tr><tr><td align="left">15</td><td align="left">93.84</td><td align="left">98.43</td></tr><tr><td align="left">20</td><td align="left">92.56</td><td align="left">98.36</td></tr><tr><td align="left">25</td><td align="left">91.65</td><td align="left">98.51</td></tr><tr><td align="left">30</td><td align="left">89.40</td><td align="left">98.52</td></tr><tr><td align="left">35</td><td align="left">88.17</td><td align="left">98.60</td></tr><tr><td align="left">40</td><td align="left">87.37</td><td align="left">98.69</td></tr><tr><td align="left">45</td><td align="left">87.27</td><td align="left">98.97</td></tr><tr><td align="left">50</td><td align="left">87.02</td><td align="left">99.13</td></tr></tbody></table></table-wrap></p>
      <p id="Par63">Cache misses also occur as a consequence of the <italic>cold-start</italic> problem that affects passive caching strategies such as LRU. Requests issued against a cold cache are likely to be cache misses and therefore result in retrieval from the origin server. This leads to longer startup delays which degrade the QoE mainly for early viewers. Figure <xref rid="Fig11" ref-type="fig">11</xref> presents the startup delay observed across all the streaming sessions as a function of the client’s link capacity. Delay values remain relatively invariable as the bandwidth on the client’s connection increases for all the considered configurations. Note that for both PREFETCH and LRU setups (left and right side in Fig. <xref rid="Fig11" ref-type="fig">11</xref>, respectively), the majority of the values are clustered around 200 milliseconds approximately. This represents a reduction of nearly 3<inline-formula id="IEq61"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M132"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq61.gif"/></alternatives></inline-formula> the startup delay viewers experience in the setup without prefetching/caching enabled (middle chart in Fig. <xref rid="Fig11" ref-type="fig">11</xref>). However, a large number of outliers is observed for the LRU configuration lying beyond the segment duration limit. This indicates that many viewers would experience more than one second latency from the moment they initiate the streaming session to the moment the video playback starts. These outliers represent the startup delay perceived by the first users as a consequence of their request hitting a cold cache and being relayed back to the content server. By forwarding said requests to the origin server, early users of the LRU setup incur an extra network hop which leads to startup delay times higher than those observed for the NO_PREFETCH setup. In Fig. <xref rid="Fig12" ref-type="fig">12</xref> only the startup latency measured for the group of early viewers is plotted. On average, early users in the LRU configuration would observe around 2<inline-formula id="IEq62"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M134"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq62.gif"/></alternatives></inline-formula> and 6<inline-formula id="IEq63"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M136"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq63.gif"/></alternatives></inline-formula> longer delay times compared to viewers in the NO_PREFETCH and PREFETCH setups, respectively. These results show that the proposed content prefetching mechanism is able to bypass the <italic>cold-start</italic> problem and offer not only shorter startup delay times but also a more consistent experience across all viewers compared to the alternative configurations.<fig id="Fig11"><label>Fig. 11</label><caption><p>Startup delay distribution as a function of the client’s link capacity. Observations for PREFETCH and LRU configuration are largely concentrated around comparable values. However, outliers for the LRU setup lie farther apart from the bulk of the data, beyond the segment length in many cases. In comparison, the PREFETCH configuration offers a more consistent experience for all viewers</p></caption><graphic xlink:href="10922_2022_9649_Fig11_HTML" id="MO18"/></fig><fig id="Fig12"><label>Fig. 12</label><caption><p>Startup delay times observed by early users: On average viewers in the PREFETCH configuration would experience 6.5<inline-formula id="IEq64"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M138"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq64.gif"/></alternatives></inline-formula> and 3.4<inline-formula id="IEq65"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M140"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq65.gif"/></alternatives></inline-formula> lower latency than those in the LRU and NO_PREFETCH setups, respectively</p></caption><graphic xlink:href="10922_2022_9649_Fig12_HTML" id="MO19"/></fig></p>
      <p id="Par64">So far, the proposed prefetching mechanism has proven able to deliver a user experience that outperforms that of the alternative setups in terms of segment download time, frequency/duration of playout freezes and startup latency. Let us now look into the perceived video quality. In the HAS client, quality level for each tile in a video segment is determined based on the <italic>perceived bandwidth</italic>, estimated as the quotient between the amount of bits downloaded per segment and the per-segment download time:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} perceived\_bandwidth(s_i) = \frac{size(s_i)}{download\_time(s_i)} \end{aligned}$$\end{document}</tex-math><mml:math id="M142" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>_</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>_</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="10922_2022_9649_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>In the expression above, the size of the segment (<inline-formula id="IEq66"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s_i$$\end{document}</tex-math><mml:math id="M144"><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq66.gif"/></alternatives></inline-formula>) is proportional to the quality level of the tiles it comprises. This way, the perceived bandwidth provides a reliable indication of the video quality as observed by the user. Figure <xref rid="Fig13" ref-type="fig">13</xref> shows the average perceived bandwidth over all watching sessions per video, as a function of the actual bandwidth on the client’s link. The configuration with content prefetching enabled outperforms the LRU setup, most remarkably along the largest values of bandwidth. With the proposed mechanism running on a cache-enabled edge server, clients perceive on average up to 2.5<inline-formula id="IEq67"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M146"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq67.gif"/></alternatives></inline-formula> more link capacity in comparison to the configuration without prefetching, and up to 1.4<inline-formula id="IEq68"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M148"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq68.gif"/></alternatives></inline-formula> compared to the LRU configuration. This results in a higher number of tiles being downloaded in high quality.<fig id="Fig13"><label>Fig. 13</label><caption><p>Client perceived bandwidth as a function of the actual link capacity. User experience greatly benefits from prefetching VR video content into a nearby server</p></caption><graphic xlink:href="10922_2022_9649_Fig13_HTML" id="MO21"/></fig></p>
      <p id="Par65">Figure <xref rid="Fig14" ref-type="fig">14</xref> presents the distribution of the amount of high-quality tiles per segment across the three videos. The mass of the distributions corresponding to each of the setups shifts towards the right (higher number of HQ tiles) as the bandwidth increases. Note that for the configuration with prefetching enabled, the distribution tends to gravitate around 16 tiles/segment at a faster pace than the other two configurations. This proves that across all tested scenarios, the mechanism we propose consistently delivers higher quality of experience for the viewer, compared to the LRU cache alternative, and the plain vanilla client-server configuration.<fig id="Fig14"><label>Fig. 14</label><caption><p>Distribution of the number of HQ tiles per segment: In comparison to the LRU and NO_PREFETCH configurations, the number of tiles retrieved in HQ from the prefetch setup increases more rapidly as the bandwidth grows larger. Bitrate values in the charts are in Mbps</p></caption><graphic xlink:href="10922_2022_9649_Fig14_HTML" id="MO22"/></fig></p>
      <p id="Par66">Another appealing effect of prefetching and caching video content into an edge server is the reduction of network traffic to and from the content site. Table <xref rid="Tab7" ref-type="table">7</xref> presents the network traffic (in gigabytes) measured in the content server interface for the configuration without prefetching/caching enabled, along with the relative change of this metric for the LRU and PREFETCH setups, and how these measurements vary as the bandwidth on the client’s link increases (Fig. <xref rid="Fig15" ref-type="fig">15</xref> for the absolute values). Note that, thanks to the reuse enabled by the LRU and prefetching configurations, there is an important reduction in traffic to the content server: from 75 to 84% for LRU caching, and from 36 to 83% for the prefetching server. Also, it is worth noting that the setup with the proposed prefetching mechanism enables these network traffic savings while serving the highest video quality among the tested configurations. That is to say, serving a comparable video quality directly from the content server—without any prefetch/cache capabilitites—would require several times the network traffic reported in Table <xref rid="Tab7" ref-type="table">7</xref>.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Network traffic between content and prefetch servers for different values of bandwidth in the client’s link</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Bandwidth client’s link (Mbps)</th><th align="left" rowspan="2">NO_PREFETCH network traffic (GB)</th><th align="left" colspan="2">% network traffic reduction</th></tr><tr><th align="left"><bold>LRU</bold> (%)</th><th align="left"><bold>PREFETCH</bold> (%)</th></tr></thead><tbody><tr><td align="left">10</td><td align="left">1.11</td><td align="left">− 75.68</td><td align="left">− 36.04</td></tr><tr><td align="left">15</td><td align="left">1.64</td><td align="left">− 78.66</td><td align="left">− 44.51</td></tr><tr><td align="left">20</td><td align="left">2.24</td><td align="left">− 80.36</td><td align="left">− 48.21</td></tr><tr><td align="left">25</td><td align="left">2.82</td><td align="left">− 82.27</td><td align="left">− 53.90</td></tr><tr><td align="left">30</td><td align="left">3.43</td><td align="left">− 80.76</td><td align="left">− 64.14</td></tr><tr><td align="left">35</td><td align="left">3.89</td><td align="left">− 81.75</td><td align="left">− 64.78</td></tr><tr><td align="left">40</td><td align="left">4.24</td><td align="left">− 82.31</td><td align="left">− 67.45</td></tr><tr><td align="left">45</td><td align="left">4.57</td><td align="left">− 83.15</td><td align="left">− 80.96</td></tr><tr><td align="left">50</td><td align="left">5.01</td><td align="left">− 84.23</td><td align="left">− 83.03</td></tr></tbody></table></table-wrap></p>
      <p id="Par67">
        <fig id="Fig15">
          <label>Fig. 15</label>
          <caption>
            <p>Network traffic to the <italic>content server</italic> for the three considered configurations, as a function of the bandwidth in the client’s link: Both the LRU and PREFETCH setups manage to induce a notable decline in network traffic. The prefetching mechanism enables this while delivering the highest quality of experience among the considered configurations</p>
          </caption>
          <graphic xlink:href="10922_2022_9649_Fig15_HTML" id="MO23"/>
        </fig>
      </p>
      <p id="Par68">To understand why the LRU configuration results in a higher reduction of network traffic with respect to the implementation of the proposed prefetching mechanism, consider the fact that the latter setup is able to consistently deliver higher video quality levels than the former one throughout the entire range of bandwidth values. An increase in the capacity of the client’s connection leads to a corresponding increase in the network throughput. This in turn prompts the client to request video tiles in higher quality representations, which consequently drives up the network traffic consumption. Figure <xref rid="Fig16" ref-type="fig">16</xref> portrays the relation between bandwidth at the client side, network traffic in the content server’s link, and video quality in terms of the number of high-quality tiles per segment delivered to the client. Note that while the setup with the LRU cache replacement strategy gets the upper hand with regard to network traffic reduction, the enhanced video quality added to the smooth playback offered by the proposed prefetching mechanism, makes for a far superior QoE for the viewer. In this sense, the increase in network traffic to the content server for this configuration can be regarded as a reasonable price to pay.<fig id="Fig16"><label>Fig. 16</label><caption><p>Relation between client’s link bandwidth, network traffic in the backhaul link, and video quality. Both LRU and PREFETCH setups drive backhaul traffic down while increasing the number of tiles per segment served in high quality. The increase in network traffic use for the PREFETCH setup in relation to the LRU configuration obeys to a corresponding increase in the delivered video quality</p></caption><graphic xlink:href="10922_2022_9649_Fig16_HTML" id="MO24"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec19">
    <title>Conclusions</title>
    <p id="Par69">Immersive video applications are known for having an immense potential in sectors such as entertainment, education, healthcare, and digital services, among others. However, the existing network infrastructure still struggles to meet the stringent latency and bandwidth requirements of these kind of services, which remains a barrier to enable their broad adoption. In this paper we presented <sc>Explora-VR</sc>, an edge-assisted solution that allows for low-latency video streaming for tile-based immersive content.</p>
    <p id="Par70"><sc>Explora-VR</sc> thrives on prefetching the tiles that users are likely to watch in the upcoming segments by advertising the outcome of the viewport prediction and rate adaptation algorithms, before the client starts consuming the content. Prefetched video tiles are downloaded to a cache-enabled edge server located in close proximity to the user, allowing for low-latency content retrieval. This in turn increases the link capacity perceived at the client’s end, and in consequence also the quality level of the requested video tiles.</p>
    <p id="Par71">Additionally, the proposed solution supports content prefetching for an <italic>on-demand</italic>, <italic>near-live</italic> scenario, i.e. serving multiple active watching sessions streaming the same content within a narrow time window. To prevent the system from overflowing the content server with duplicate requests while doing this, <sc>Explora-VR</sc> features a stream processing mechanism that incrementally builds a <italic>collective playout buffer</italic> to serve the requests from active users. This collective buffer is an in-memory data structure storing a fixed number of <italic>collective viewports</italic>, namely the group of tiles viewers tend to fixate the most on a per-segment basis. The per-segment collective viewports are continuously updated as new viewers arrive to dynamically accommodate to changes in the current preferences from the audience.</p>
    <p id="Par72">We evaluated the performance of <sc>Explora-VR</sc> against a conventional <italic>client-server</italic> setup with no support for caching or prefetching, and an edge-assisted configuration implementing a regular LRU cache replacement strategy. Our solution proved to be effective in providing a smooth video playback, while also increasing the quality of the delivered content. Under equivalent network conditions, the devised prefetching mechanism leads to an average increase of 2.5<inline-formula id="IEq69"><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M150"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq69.gif"/></alternatives></inline-formula> and 1.4<inline-formula id="IEq70"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M152"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq70.gif"/></alternatives></inline-formula> in the effective bandwidth perceived at the client’s device compared to the conventional client-server and LRU setups, respectively. This in turn results in a proportional increase in the number of viewport tiles served in high quality. Moreover, in contrast to the alternative LRU configuration, our solution can consistently serve more than 98% of the content requests from the edge server. This means that only a minor proportion of the client requests get relayed to the origin content server, resulting in a freeze-free playback experience for the user. The foregoing also signals the ability of the proposed approach to bypass the <italic>cold-start</italic> problem that typically affects passive caching strategies. The observed startup delay times show that <sc>Explora-VR</sc> consistently provides low startup latency for all users, including early viewers. These results also hint at the potential of the proposed solution to aid in the recovery from eventual playback freezes. The proximity of the edge server coupled with the high prefetch hit ratio ensures that viewers can quickly resume the playback with a delay we expect to be comparable with the observed startup latency. Additional evaluations with real network traces are needed to confirm this assumption. The devised collective buffer also proved efficient in reducing the load on the content server network. Even though the LRU cache replacement policy outperforms the prefetching mechanism regarding this metric, the superior quality of experience that our approach can offer to the viewer reasonably outweighs this drawback.</p>
    <p id="Par73">In developing <sc>Explora-VR</sc>, we assumed a number of conditions that will be relaxed in future work to make this solution more suitable for a production-level VR video streaming service. In this sense, further research is going to explore the effect of working with a lossy wireless network in the performance of the content prefetching mechanism. Likewise, the proposed solution will be extended to support multiple intermediate quality representations, instead of only <italic>low-quality</italic> and <italic>high-quality</italic> levels. We expect the results of this work will motivate further studies on edge-assisted prefetching techniques for omnidirectional video streaming.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Yépez, J., Guevara, L., Guerrero, G.: Aula<sc>VR</sc>: virtual reality, a telepresence technique applied to distance education. In: 2020 15th Iberian conference on information systems and technologies (CISTI), pp. 1–5. IEEE (2020)</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kwok</surname>
            <given-names>AO</given-names>
          </name>
          <name>
            <surname>Koh</surname>
            <given-names>SG</given-names>
          </name>
        </person-group>
        <article-title>COVID-19 and extended reality ( XR)</article-title>
        <source>Current Issues Tour.</source>
        <year>2020</year>
        <volume>1</volume>
        <fpage>6</fpage>
        <pub-id pub-id-type="doi">10.1080/13683500.2020.1798896</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Singh</surname>
            <given-names>RP</given-names>
          </name>
          <name>
            <surname>Javaid</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kataria</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Tyagi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Haleem</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Suman</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Significant applications of virtual reality for covid-19 pandemic</article-title>
        <source>Diabetes Metab. Syndr.</source>
        <year>2020</year>
        <volume>14</volume>
        <issue>4</issue>
        <fpage>661</fpage>
        <lpage>664</lpage>
        <pub-id pub-id-type="doi">10.1016/j.dsx.2020.05.011</pub-id>
        <pub-id pub-id-type="pmid">32438329</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Khan, W.Z., Ahmed, E., Hakak, S., Yaqoob, I., Ahmed, A.: Edge computing: A survey. Future Gener. Comput. Syst. <bold>97</bold>, 219–235 (2019). 10.1016/j.future.2019.02.050. URL <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0167739X18319903">https://www.sciencedirect.com/science/article/pii/S0167739X18319903</ext-link></mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Shi, S., Gupta, V., Hwang, M., Jana, R.: Mobile vr on edge cloud: a latency-driven design. In: Proceedings of the 10th ACM multimedia systems conference, pp. 222–231 (2019)</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stauffert</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Niebling</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Latoschik</surname>
            <given-names>ME</given-names>
          </name>
        </person-group>
        <article-title>Latency and cybersickness: Impact, causes and measures. a review</article-title>
        <source>Front. Virtual Real.</source>
        <year>2020</year>
        <volume>1</volume>
        <fpage>31</fpage>
        <pub-id pub-id-type="doi">10.3389/frvir.2020.582204</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Torres Vega</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Liaskos</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Abadal</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Papapetrou</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Jain</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mouhouche</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Kalem</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ergüt</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mach</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sabol</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Immersive interconnected virtual and augmented reality: a 5g and iot perspective</article-title>
        <source>J. Netw. Syst. Manag.</source>
        <year>2020</year>
        <volume>28</volume>
        <issue>4</issue>
        <fpage>796</fpage>
        <lpage>826</lpage>
        <pub-id pub-id-type="doi">10.1007/s10922-020-09545-w</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yaqoob</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bi</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Muntean</surname>
            <given-names>GM</given-names>
          </name>
        </person-group>
        <article-title>A survey on adaptive 360<inline-formula id="IEq78"><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M154"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq78.gif"/></alternatives></inline-formula> video streaming: Solutions, challenges and opportunities</article-title>
        <source>IEEE Commun. Surv. Tutor.</source>
        <year>2020</year>
        <volume>22</volume>
        <issue>4</issue>
        <fpage>2801</fpage>
        <lpage>2838</lpage>
        <pub-id pub-id-type="doi">10.1109/COMST.2020.3006999</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">He, D., Westphal, C., Garcia-Luna-Aceves, J.: Network support for ar/vr and immersive video application: a survey. In: ICETE (1), pp. 525–535 (2018)</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Long, K., Cui, Y., Ye, C., Liu, Z.: Optimal wireless streaming of multi-quality 360 vr video by exploiting natural, relative smoothness-enabled and transcoding-enabled multicast opportunities. IEEE Transactions on Multimedia (2020)</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Cui</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Adaptive streaming of 360 videos with perfect, imperfect, and unknown fov viewing probabilities in wireless networks</article-title>
        <source>IEEE Trans. Image Process.</source>
        <year>2021</year>
        <volume>30</volume>
        <fpage>7744</fpage>
        <lpage>7759</lpage>
        <pub-id pub-id-type="doi">10.1109/TIP.2021.3099741</pub-id>
        <pub-id pub-id-type="pmid">34339372</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">van der Hooft, J., Vega, M.T., Petrangeli, S., Wauters, T., De Turck, F.: Optimizing adaptive tile-based virtual reality video streaming. In: 2019 IFIP/IEEE symposium on integrated network and service management (IM), pp. 381–387. IEEE (2019)</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van der Hooft</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Vega</surname>
            <given-names>MT</given-names>
          </name>
          <name>
            <surname>Petrangeli</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wauters</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>De Turck</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Tile-based adaptive streaming for virtual reality video</article-title>
        <source>ACM Trans. Multimed. Comput. Commun. Appl.</source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.1145/3362101</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Hosseini, M.: View-aware tile-based adaptations in 360 virtual reality video streaming. In: 2017 IEEE virtual reality (VR), pp. 423–424. IEEE (2017)</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Xie, L., Xu, Z., Ban, Y., Zhang, X., Guo, Z.: 360probdash: Improving qoe of 360 video streaming using tile-based http adaptive streaming. In: Proceedings of the 25th ACM international conference on Multimedia, pp. 315–323 (2017)</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Graf, M., Timmerer, C., Mueller, C.: Towards bandwidth efficient adaptive streaming of omnidirectional video over http: Design, implementation, and evaluation. In: Proceedings of the 8th ACM on multimedia systems conference, pp. 261–271 (2017)</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nguyen</surname>
            <given-names>DV</given-names>
          </name>
          <name>
            <surname>Tran</surname>
            <given-names>HT</given-names>
          </name>
          <name>
            <surname>Pham</surname>
            <given-names>AT</given-names>
          </name>
          <name>
            <surname>Thang</surname>
            <given-names>TC</given-names>
          </name>
        </person-group>
        <article-title>An optimal tile-based approach for viewport-adaptive 360-degree video streaming</article-title>
        <source>IEEE J. Emerg. Sel. Top. Circuits Syst.</source>
        <year>2019</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>29</fpage>
        <lpage>42</lpage>
        <pub-id pub-id-type="doi">10.1109/JETCAS.2019.2899488</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Lo, W.C., Fan, C.L., Lee, J., Huang, C.Y., Chen, K.T., Hsu, C.H.: 360 video viewing dataset in head-mounted virtual reality. In: Proceedings of the 8th ACM on multimedia systems conference, pp. 211–216 (2017)</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">David, E.J., Gutiérrez, J., Coutrot, A., Da Silva, M.P., Callet, P.L.: A dataset of head and eye movements for 360 videos. In: Proceedings of the 9th ACM multimedia systems conference, pp. 432–437 (2018)</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Fremerey, S., Singla, A., Meseberg, K., Raake, A.: Avtrack360: An open dataset and software recording people’s head rotations watching 360<inline-formula id="IEq80"><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M156"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq80.gif"/></alternatives></inline-formula> videos on an hmd. In: Proceedings of the 9th ACM multimedia systems conference, pp. 403–408 (2018)</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Wu, C., Tan, Z., Wang, Z., Yang, S.: A dataset for exploring user behaviors in vr spherical video streaming. In: Proceedings of the 8th ACM on multimedia systems conference, MMSys’17, p. 193-198. Association for Computing Machinery, New York, NY, USA (2017). 10.1145/3083187.3083210</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rossi</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ozcinar</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Smolic</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Toni</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Do users behave similarly in vr? investigation of the user influence on the system design</article-title>
        <source>ACM Trans. Multimed. Comput. Commun. Appl.</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.1145/3381846</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Papaioannou, G., Koutsopoulos, I.: Tile-based caching optimization for 360<inline-formula id="IEq81"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M158"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq81.gif"/></alternatives></inline-formula> videos. In: Proceedings of the Twentieth ACM international symposium on mobile ad hoc networking and computing, Mobihoc ’19, p. 171-180. Association for Computing Machinery, New York, NY, USA (2019). 10.1145/3323679.3326515</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Mahzari, A., Taghavi Nasrabadi, A., Samiei, A., Prakash, R.: Fov-aware edge caching for adaptive 360 video streaming. In: Proceedings of the 26th ACM international conference on Multimedia, pp. 173–181 (2018)</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Maniotis, P., Thomos, N.: Viewport-aware deep reinforcement learning approach for 360 video caching. IEEE Transactions on Multimedia (2021)</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Carlsson, N., Eager, D.: Had you looked where i’m looking? cross-user similarities in viewing behavior for 360-degree video and caching implications. In: Proceedings of the ACM/SPEC international conference on performance engineering, ICPE ’20, p. 130–137. Association for Computing Machinery, New York, NY, USA (2020). 10.1145/3358960.3379129</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dai</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Mao</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>A view synthesis-based 360<inline-formula id="IEq82"><alternatives><tex-math id="M159">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M160"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq82.gif"/></alternatives></inline-formula> vr caching system over mec-enabled c-ran</article-title>
        <source>IEEE Trans. Circuits Syst. Video Technol</source>
        <year>2019</year>
        <volume>30</volume>
        <issue>10</issue>
        <fpage>3843</fpage>
        <lpage>3855</lpage>
        <pub-id pub-id-type="doi">10.1109/TCSVT.2019.2946755</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Liu, K., Liu, Y., Liu, J., Argyriou, A., Ding, Y.: Joint epc and ran caching of tiled vr videos for mobile networks. In: International conference on multimedia modeling, pp. 92–105. Springer (2019)</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Wang, S., Tan, X., Li, S., Xu, X., Yang, J., Zheng, Q.: A qoe-based 360<inline-formula id="IEq84"><alternatives><tex-math id="M161">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M162"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq84.gif"/></alternatives></inline-formula> video adaptive bitrate delivery and caching scheme for c-ran. In: 2020 16th International conference on mobility, sensing and networking (MSN), pp. 49–56. IEEE (2020)</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Lei</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Mao</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ristaniemi</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Learn to cache: machine learning for network edge caching in the big data era</article-title>
        <source>IEEE Wirel. Commun.</source>
        <year>2018</year>
        <volume>25</volume>
        <issue>3</issue>
        <fpage>28</fpage>
        <lpage>35</lpage>
        <pub-id pub-id-type="doi">10.1109/MWC.2018.1700317</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Petrangeli, S., Swaminathan, V., Hosseini, M., De Turck, F.: An http/2-based adaptive streaming framework for 360<inline-formula id="IEq85"><alternatives><tex-math id="M163">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M164"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq85.gif"/></alternatives></inline-formula> virtual reality videos. In: Proceedings of the 25th ACM international conference on multimedia, MM ’17, p. 306-314. Association for Computing Machinery, New York, NY, USA (2017). 10.1145/3123266.3123453</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">van der Hooft, J., Torres Vega, M., Petrangeli, S., Wauters, T., De Turck, F.: Quality assessment for adaptive virtual reality video streaming: a probabilistic approach on the user’s gaze. In: 2019 22nd conference on innovation in clouds, internet and networks and workshops (ICIN), pp. 19–24 (2019). 10.1109/ICIN.2019.8685904</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Qian, F., Ji, L., Han, B., Gopalakrishnan, V.: Optimizing 360 video delivery over cellular networks. In: Proceedings of the 5th workshop on all things cellular: operations, applications and challenges, ATC ’16, p. 1–6. Association for Computing Machinery, New York, NY, USA (2016). 10.1145/2980055.2980056</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Xu, Z., Zhang, X., Zhang, K., Guo, Z.: Probabilistic viewport adaptive streaming for 360-degree videos. In: 2018 IEEE international symposium on circuits and systems (ISCAS), pp. 1–5. IEEE (2018)</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Zhang, Y., Zhao, P., Bian, K., Liu, Y., Song, L., Li, X.: Drl360: 360-degree video streaming with deep reinforcement learning. In: IEEE INFOCOM 2019-IEEE conference on computer communications, pp. 1252–1260. IEEE (2019)</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Vielhaben, J., Camalan, H., Samek, W., Wenzel, M.: Viewport forecasting in 360<inline-formula id="IEq86"><alternatives><tex-math id="M165">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M166"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="10922_2022_9649_Article_IEq86.gif"/></alternatives></inline-formula> virtual reality videos with machine learning. In: 2019 IEEE international conference on artificial intelligence and virtual reality (AIVR), pp. 74–747. IEEE (2019)</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Fan, C.L., Lee, J., Lo, W.C., Huang, C.Y., Chen, K.T., Hsu, C.H.: Fixation prediction for 360 video streaming in head-mounted virtual reality. In: Proceedings of the 27th workshop on network and operating systems support for digital audio and video, pp. 67–72 (2017)</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhai</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Min</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>The prediction of head and eye movement for 360 degree images</article-title>
        <source>Signal Process.</source>
        <year>2018</year>
        <volume>69</volume>
        <fpage>15</fpage>
        <lpage>25</lpage>
        <pub-id pub-id-type="doi">10.1016/j.sigpro.2017.12.023</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sitzmann</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Serrano</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pavel</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Agrawala</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gutierrez</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Masia</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Wetzstein</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Saliency in vr: How do people explore virtual environments?</article-title>
        <source>IEEE Trans. Vis. Comput. Gr.</source>
        <year>2018</year>
        <volume>24</volume>
        <issue>4</issue>
        <fpage>1633</fpage>
        <lpage>1642</lpage>
        <pub-id pub-id-type="doi">10.1109/TVCG.2018.2793599</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Kasgari</surname>
            <given-names>ATZ</given-names>
          </name>
          <name>
            <surname>Saad</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Deep learning for content-based personalized viewport prediction of 360-degree vr videos</article-title>
        <source>IEEE Netw. Lett.</source>
        <year>2020</year>
        <volume>2</volume>
        <issue>2</issue>
        <fpage>81</fpage>
        <lpage>84</lpage>
        <pub-id pub-id-type="doi">10.1109/LNET.2020.2977124</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Prematunga</surname>
            <given-names>RK</given-names>
          </name>
        </person-group>
        <article-title>Correlational analysis</article-title>
        <source>Aust. Crit. Care</source>
        <year>2012</year>
        <volume>25</volume>
        <issue>3</issue>
        <fpage>195</fpage>
        <lpage>199</lpage>
        <pub-id pub-id-type="doi">10.1016/j.aucc.2012.02.003</pub-id>
        <pub-id pub-id-type="pmid">22464607</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Knight</surname>
            <given-names>WR</given-names>
          </name>
        </person-group>
        <article-title>A computer method for calculating Kendall’s tau with ungrouped data</article-title>
        <source>J. Am. Stat. Assoc.</source>
        <year>1966</year>
        <volume>61</volume>
        <issue>314</issue>
        <fpage>436</fpage>
        <lpage>439</lpage>
        <pub-id pub-id-type="doi">10.1080/01621459.1966.10480879</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ordonez-Ante</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Van Seghbroeck</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Wauters</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Volckaert</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>De Turck</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Explora: interactive querying of multidimensional data in the context of smart cities</article-title>
        <source>Sensors</source>
        <year>2020</year>
        <volume>20</volume>
        <issue>9</issue>
        <fpage>2737</fpage>
        <pub-id pub-id-type="doi">10.3390/s20092737</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Gutierrez</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Gutierrez</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Messaging with redis</article-title>
        <source>Spring Boot Messaging</source>
        <year>2017</year>
        <publisher-loc>Berlin</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>81</fpage>
        <lpage>92</lpage>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">imec/IDLab: Virtual wall: Perform large networking and cloud experiments. (2021). URL <ext-link ext-link-type="uri" xlink:href="https://doc.ilabt.imec.be/ilabt/virtualwall/index.html">https://doc.ilabt.imec.be/ilabt/virtualwall/index.html</ext-link></mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brown</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Traffic control howto</article-title>
        <source>Guide IP Layer Netw.</source>
        <year>2006</year>
        <volume>49</volume>
        <fpage>36</fpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
