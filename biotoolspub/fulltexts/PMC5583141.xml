<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Cheminform</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Cheminform</journal-id>
    <journal-title-group>
      <journal-title>Journal of Cheminformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1758-2946</issn>
    <publisher>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5583141</article-id>
    <article-id pub-id-type="publisher-id">235</article-id>
    <article-id pub-id-type="doi">10.1186/s13321-017-0235-x</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Molecular de-novo design through deep reinforcement learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8177-2787</contrib-id>
        <name>
          <surname>Olivecrona</surname>
          <given-names>Marcus</given-names>
        </name>
        <address>
          <email>m.olivecrona@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Blaschke</surname>
          <given-names>Thomas</given-names>
        </name>
        <address>
          <email>thomas.blaschke@astrazeneca.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Engkvist</surname>
          <given-names>Ola</given-names>
        </name>
        <address>
          <email>ola.engkvist@astrazeneca.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Hongming</given-names>
        </name>
        <address>
          <email>hongming.chen@astrazeneca.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 1519 6403</institution-id><institution-id institution-id-type="GRID">grid.418151.8</institution-id><institution>Hit Discovery, Discovery Sciences, Innovative Medicines and Early Development Biotech Unit, </institution><institution>AstraZeneca R&amp;D Gothenburg, </institution></institution-wrap>43183 Mölndal, Sweden </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>4</day>
      <month>9</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>4</day>
      <month>9</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2017</year>
    </pub-date>
    <volume>9</volume>
    <elocation-id>48</elocation-id>
    <history>
      <date date-type="received">
        <day>14</day>
        <month>5</month>
        <year>2017</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>8</month>
        <year>2017</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2017</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">This work introduces a method to tune a sequence-based generative model for molecular de novo design that through augmented episodic likelihood can learn to generate structures with certain specified desirable properties. We demonstrate how this model can execute a range of tasks such as generating analogues to a query structure and generating compounds predicted to be active against a biological target. As a proof of principle, the model is first trained to generate molecules that do not contain sulphur. As a second example, the model is trained to generate analogues to the drug Celecoxib, a technique that could be used for scaffold hopping or library expansion starting from a single molecule. Finally, when tuning the model towards generating compounds predicted to be active against the dopamine receptor type 2, the model generates structures of which more than 95% are predicted to be active, including experimentally confirmed actives that have not been included in either the generative model nor the activity prediction model.<fig position="anchor" id="Figa"><label>Graphical abstract</label><caption><p>.</p></caption><graphic position="anchor" xlink:href="13321_2017_235_Figa_HTML" id="MO111"/></fig>
</p>
      <sec>
        <title>Electronic supplementary material</title>
        <p>The online version of this article (doi:10.1186/s13321-017-0235-x) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>De novo design</kwd>
      <kwd>Recurrent neural networks</kwd>
      <kwd>Reinforcement learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010665</institution-id>
            <institution>H2020 Marie Skłodowska-Curie Actions</institution>
          </institution-wrap>
        </funding-source>
        <award-id>676434</award-id>
        <principal-award-recipient>
          <name>
            <surname>Blaschke</surname>
            <given-names>Thomas</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2017</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par2">Drug discovery is often described using the metaphor of finding a needle in a haystack. In this case, the haystack comprises on the order of <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{60}{-}10^{100}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>60</mml:mn></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>100</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq1.gif"/></alternatives></inline-formula> synthetically feasible molecules [<xref ref-type="bibr" rid="CR1">1</xref>], out of which we need to find a compound which satisfies the plethora of criteria such as bioactivity, drug metabolism and pharmacokinetic (DMPK) profile, synthetic accessibility, etc. The fraction of this space that we can synthesize and test at all—let alone efficiently—is negligible. By using algorithms to virtually design and assess molecules, de novo design offers ways to reduce the chemical space into something more manageable for the search of the needle.</p>
    <p id="Par3">Early de novo design algorithms [<xref ref-type="bibr" rid="CR1">1</xref>] used structure based approaches to grow ligands to sterically and electronically fit the binding pocket of the target of interest [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]. A limitation of these methods is that the molecules created often possess poor DMPK properties and can be synthetically intractable. In contrast, the ligand based approach is to generate a large virtual library of chemical structures, and search this chemical space using a scoring function that typically takes into account several properties such as DMPK profiles, synthetic accessibility, bioactivity, and query structure similarity [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>]. One way to create such a virtual library is to use known chemical reactions alongside a set of available chemical building blocks, resulting in a large number of synthetically accessible structures [<xref ref-type="bibr" rid="CR6">6</xref>]; another possibility is to use transformational rules based on the expertise of medicinal chemists to design analogues to a query structure. For example, Besnard et al. [<xref ref-type="bibr" rid="CR7">7</xref>] applied a transformation rule approach to the design of novel dopamine receptor type 2 (DRD2) receptor active compounds with specific polypharmacological profiles and appropriate DMPK profiles for a central nervous system indication. Although using either transformation or reaction rules can reliably and effectively generate novel structures, they are limited by the inherent rigidness and scope of the predefined rules and reactions.</p>
    <p id="Par4">A third approach, known as inverse Quantitative Structure Activity Relationship (inverse QSAR), tackles the problem from a different angle: rather than first generating a virtual library and then using a QSAR model to score and search this library, inverse QSAR aims to map a favourable region in terms of predicted activity to the corresponding molecular structures [<xref ref-type="bibr" rid="CR8">8</xref>–<xref ref-type="bibr" rid="CR10">10</xref>]. This is not a trivial problem: first the solutions of molecular descriptors corresponding to the region need to be resolved using the QSAR model, and these then need be mapped back to the corresponding molecular structures. The fact that the molecular descriptors chosen need to be suitable both for building a forward predictive QSAR model as well as for translation back to molecular structure is one of the major obstacles for this type of approach.</p>
    <p id="Par5">The Recurrent Neural Network (RNN) is commonly used as a generative model for data of sequential nature, and have been used successfully for tasks such as natural language processing [<xref ref-type="bibr" rid="CR11">11</xref>] and music generation [<xref ref-type="bibr" rid="CR12">12</xref>]. Recently, there has been an increasing interest in using this type of generative model for the de novo design of molecules [<xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR15">15</xref>]. By using a data-driven method that attempts to learn the underlying probability distribution over a large set of chemical structures, the search over the chemical space can be reduced to only molecules seen as reasonable, without introducing the rigidity of rule based approaches. Segler et al. [<xref ref-type="bibr" rid="CR13">13</xref>] demonstrated that an RNN trained on the canonicalized SMILES representation of molecules can learn both the syntax of the language as well as the distribution in chemical space. They also show how further training of the model using a focused set of actives towards a biological target can produce a fine-tuned model which generates a high fraction of predicted actives.</p>
    <p id="Par6">In two recent studies, reinforcement learning (RL) [<xref ref-type="bibr" rid="CR16">16</xref>] was used to fine tune pre-trained RNNs. Yu et al. [<xref ref-type="bibr" rid="CR15">15</xref>] use an adversarial network to estimate the expected return for state-action pairs sampled from the RNN, and by increasing the likelihood of highly rated pairs improves the generative network for tasks such as poem generation. Jaques et al. [<xref ref-type="bibr" rid="CR17">17</xref>] use Deep Q-learning to improve a pre-trained generative RNN by introducing two ways to score the sequences generated: one is a measure of how well the sequences adhere to music theory, and one is the likelihood of sequences according to the initial pre-trained RNN. Using this concept of prior likelihood they reduce the risk of forgetting what was initially learnt by the RNN, compared to a reward based only on the adherence to music theory. The authors demonstrate significant improvements over both the initial RNN as well as an RL only approach. They later extend this method to several other tasks including the generation of chemical structures, and optimize toward molecular properties such as cLogP [<xref ref-type="bibr" rid="CR18">18</xref>] and QED drug-likeness [<xref ref-type="bibr" rid="CR19">19</xref>]. However, they report that the method is dependent on a reward function incorporating handwritten rules to penalize undesirable types of sequences, and even then can lead to exploitation of the reward resulting in unrealistically simple molecules that are more likely to satisfy the optimization requirements than more complex structures [<xref ref-type="bibr" rid="CR17">17</xref>].</p>
    <p id="Par7">In this study we propose a policy based RL approach to tune RNNs for episodic tasks [<xref ref-type="bibr" rid="CR16">16</xref>], in this case the task of generating molecules with given desirable properties. Through learning an augmented episodic likelihood which is a composite of prior likelihood [<xref ref-type="bibr" rid="CR17">17</xref>] and a user defined scoring function, the method aims to fine-tune an RNN pre-trained on the ChEMBL database [<xref ref-type="bibr" rid="CR20">20</xref>] towards generating desirable compounds. Compared to maximum likelihood estimation finetuning [<xref ref-type="bibr" rid="CR13">13</xref>], this method can make use of negative as well as continuous scores, and may reduce the risk of catastrophic forgetting [<xref ref-type="bibr" rid="CR21">21</xref>]. The method is applied to several different tasks of molecular de novo design, and an investigation was carried out to illustrate how the method affects the behaviour of the generative model on a mechanistic level.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Recurrent neural networks</title>
      <p id="Par8">A recurrent neural network is an architecture of neural networks designed to make use of the symmetry across steps in sequential data while simultaneously at every step keeping track of the most salient information of previously seen steps, which may affect the interpretation of the current one [<xref ref-type="bibr" rid="CR22">22</xref>]. It does so by introducing the concept of a <italic>cell</italic> (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). For any given step <italic>t</italic>, the <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$cell_{t}$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq2.gif"/></alternatives></inline-formula> is a result of the previous <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$cell_{t-1}$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq3.gif"/></alternatives></inline-formula> and the current input <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x^{t-1}$$\end{document}</tex-math><mml:math id="M8"><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq4.gif"/></alternatives></inline-formula>. The content of <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$cell_t$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq5.gif"/></alternatives></inline-formula> will determine both the output at the current step as well as influence the next cell state. The cell thus enables the network to have a memory of past events, which can be used when deciding how to interpret new data. These properties make an RNN particularly well suited for problems in the domain of natural language processing. In this setting, a sequence of words can be encoded into one-hot vectors the length of our vocabulary <italic>X</italic>. Two additional tokens, <italic>GO</italic> and <italic>EOS</italic>, may be added to denote the beginning and end of the sequence respectively.<fig id="Fig1"><label>Fig. 1</label><caption><p>Learning the data. Depiction of maximum likelihood training of an RNN. <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x^t$$\end{document}</tex-math><mml:math id="M12"><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq6.gif"/></alternatives></inline-formula> are the target sequence tokens we are trying to learn by maximizing <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P(x^t)$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq7.gif"/></alternatives></inline-formula> for each step</p></caption><graphic xlink:href="13321_2017_235_Fig1_HTML" id="MO1"/></fig>
</p>
      <sec id="Sec4">
        <title>Learning the data</title>
        <p id="Par9">Training an RNN for sequence modeling is typically done by maximum likelihood estimation of the next token <inline-formula id="IEq9"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x^{t}$$\end{document}</tex-math><mml:math id="M16"><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq9.gif"/></alternatives></inline-formula> in the target sequence given tokens for the previous steps (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). At every step the model will produce a probability distribution over what the next character is likely to be, and the aim is to maximize the likelihood assigned to the correct token:<disp-formula id="Equ1"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J(\Theta ) = -\sum _{t=1}^T {\log P{(x^{t}\mid x^{t-1},\ldots ,x^{1})}}$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:mi>J</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mo>log</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>∣</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math><graphic xlink:href="13321_2017_235_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>The cost function <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J(\Theta )$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq10.gif"/></alternatives></inline-formula>, often applied to a subset of all training examples known as a batch, is minimized with respect to the network parameters <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Theta$$\end{document}</tex-math><mml:math id="M22"><mml:mi mathvariant="normal">Θ</mml:mi></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq11.gif"/></alternatives></inline-formula>. Given a predicted log likelihood <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\log P$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mo>log</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq12.gif"/></alternatives></inline-formula> of the target at step <italic>t</italic>, the gradient of the prediction with respect to <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Theta$$\end{document}</tex-math><mml:math id="M26"><mml:mi mathvariant="normal">Θ</mml:mi></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq13.gif"/></alternatives></inline-formula> is used to make an update of <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Theta$$\end{document}</tex-math><mml:math id="M28"><mml:mi mathvariant="normal">Θ</mml:mi></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq14.gif"/></alternatives></inline-formula>. This method of fitting a neural network is called back-propagation. Due to the architecture of the RNN, changing the network parameters will not only affect the direct output at time <italic>t</italic>, but also affect the flow of information from the previous cell into the current one iteratively. This domino-like effect that the recurrence has on back-propagation gives rise to some particular problems, and back-propagation applied to RNNs is referred to as back-propagation through time (BPTT).</p>
        <p id="Par10">BPTT is dealing with gradients that through the chain-rule contains terms which are multiplied by themselves many times, and this can lead to a phenomenon known as exploding and vanishing gradients. If these terms are not unity, the gradients quickly become either very large or very small. In order to combat this issue, Hochreiter et al. introduced the Long-Short-Term Memory cell [<xref ref-type="bibr" rid="CR23">23</xref>], which through a more controlled flow of information can decide what information to keep and what to discard. The Gated Recurrent Unit is a simplified implementation of the Long-Short-Term Memory architecture that achieves much of the same effect at a reduced computational cost [<xref ref-type="bibr" rid="CR24">24</xref>].</p>
      </sec>
      <sec id="Sec5">
        <title>Generating new samples</title>
        <p id="Par11">Once an RNN has been trained on target sequences, it can then be used to generate new sequences that follow the conditional probability distributions learned from the training set. The first input—the <italic>GO</italic> token—is given and at every timestep after we sample an output token <inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x^t$$\end{document}</tex-math><mml:math id="M30"><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq15.gif"/></alternatives></inline-formula> from the predicted probability distribution <inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P(X^t)$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq16.gif"/></alternatives></inline-formula> over our vocabulary <italic>X</italic> and use <inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x^t$$\end{document}</tex-math><mml:math id="M34"><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq17.gif"/></alternatives></inline-formula> as our next input. Once the <italic>EOS</italic> token is sampled, the sequence is considered finished (Fig. <xref rid="Fig2" ref-type="fig">2</xref>).<fig id="Fig2"><label>Fig. 2</label><caption><p>Generating sequences. Sequence generation by a trained RNN. Every timestep <italic>t</italic> we sample the next token of the sequence <inline-formula id="IEq8"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x^{t}$$\end{document}</tex-math><mml:math id="M36"><mml:msup><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq8.gif"/></alternatives></inline-formula> from the probability distribution given by the RNN, which is then fed in as the next input</p></caption><graphic xlink:href="13321_2017_235_Fig2_HTML" id="MO2"/></fig>
</p>
      </sec>
      <sec id="Sec6">
        <title>Tokenizing and one-hot encoding SMILES</title>
        <p id="Par12">A SMILES [<xref ref-type="bibr" rid="CR25">25</xref>] represents a molecule as a sequence of characters corresponding to atoms as well as special characters denoting opening and closure of rings and branches. The SMILES is, in most cases, tokenized based on a single character, except for atom types which comprise two characters such as “Cl” and “Br” and special environments denoted by square brackets (e.g [nH]), where they are considered as one token. This method of tokenization resulted in 86 tokens present in the training data. Figure <xref rid="Fig3" ref-type="fig">3</xref> exemplifies how a chemical structure is translated to both the SMILES and one-hot encoded representations.<fig id="Fig3"><label>Fig. 3</label><caption><p>Three representations of 4-(chloromethyl)-1H-imidazole. Depiction of a one-hot representation derived from the SMILES of a molecule. Here a reduced vocabulary is shown, while in practice a much larger vocabulary that covers all tokens present in the training data is used</p></caption><graphic xlink:href="13321_2017_235_Fig3_HTML" id="MO4"/></fig>
</p>
        <p id="Par13">There are many different ways to represent a single molecule using SMILES. Algorithms that always represent a certain molecule with the same SMILES are referred to as canonicalization algorithms [<xref ref-type="bibr" rid="CR26">26</xref>]. However, different implementations of the algorithms can still produce different SMILES.</p>
      </sec>
    </sec>
    <sec id="Sec7">
      <title>Reinforcement learning</title>
      <p id="Par14">Consider an Agent, that given a certain state <inline-formula id="IEq18"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s\in {\mathbb {S}}$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq18.gif"/></alternatives></inline-formula> has to choose which action <inline-formula id="IEq19"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a\in {\mathbb {A}}(s)$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq19.gif"/></alternatives></inline-formula> to take, where <inline-formula id="IEq20"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbb {S}}$$\end{document}</tex-math><mml:math id="M42"><mml:mi mathvariant="double-struck">S</mml:mi></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq20.gif"/></alternatives></inline-formula> is the set of possible states and <inline-formula id="IEq21"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbb {A}}(s)$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq21.gif"/></alternatives></inline-formula> is the set of possible actions for that state. The policy <inline-formula id="IEq22"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\pi (a \mid s)$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:mi mathvariant="italic">π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>∣</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq22.gif"/></alternatives></inline-formula> of an Agent maps a state to the probability of each action taken therein. Many problems in reinforcement learning are framed as Markov decision processes, which means that the current state contains all information necessary to guide our choice of action, and that nothing more is gained by also knowing the history of past states. For most real problems, this is an approximation rather than a truth; however, we can generalize this concept to that of a partially observable Markov decision process, in which the Agent can interact with an incomplete representation of the environment. Let <inline-formula id="IEq23"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r(a \mid s)$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>∣</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq23.gif"/></alternatives></inline-formula> be the reward which acts as a measurement of how good it was to take an action at a certain state, and the long-term return <inline-formula id="IEq24"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G(a_t, S_t = \sum _{t}^T{r_t})$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq24.gif"/></alternatives></inline-formula> as the cumulative rewards starting from <italic>t</italic> collected up to time <italic>T</italic>. Since molecular desirability in general is only sensible for a completed SMILES, we will refer only to the return of a complete sequence.</p>
      <p id="Par15">What reinforcement learning concerns, given a set of actions taken from some states and the rewards thus received, is how to improve the Agent policy in such a way as to increase the expected return <inline-formula id="IEq25"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbb {E}}[G]$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq25.gif"/></alternatives></inline-formula>. A task which has a clear endpoint at step <italic>T</italic> is referred to as an episodic task [<xref ref-type="bibr" rid="CR16">16</xref>], where <italic>T</italic> corresponds to the length of the episode. Generating a SMILES is an example of an episodic task, which ends once the <italic>EOS</italic> token is sampled.</p>
      <p id="Par16">The states and actions used to train the agent can be generated both by the agent itself or by some other means. If they are generated by the agent itself the learning is referred to as <italic>on-policy</italic>, and if they are generated by some other means the learning is referred to as <italic>off-policy</italic> [<xref ref-type="bibr" rid="CR16">16</xref>].</p>
      <p id="Par17">There are two different approaches often used in reinforcement learning to obtain a policy: value based RL, and policy based RL [<xref ref-type="bibr" rid="CR16">16</xref>]. In value based RL, the goal is to learn a value function that describes the expected return from a given state. Having learnt this function, a policy can be determined in such a way as to maximize the expected value of the state that a certain action will lead to. In policy based RL on the other hand, the goal is to directly learn a policy. For the problem addressed in this study, we believe that policy based methods is the natural choice for three reasons:<list list-type="bullet"><list-item><p id="Par18">Policy based methods can learn explicitly an optimal stochastic policy [<xref ref-type="bibr" rid="CR16">16</xref>], which is our goal.</p></list-item><list-item><p id="Par19">The method used starts with a prior sequence model. The goal is to finetune this model according to some specified scoring function. Since the prior model already constitutes a policy, learning a finetuned policy might require only small changes to the prior model.</p></list-item><list-item><p id="Par20">The episodes in this case are short and fast to sample, reducing the impact of the variance in the estimate of the gradients.</p></list-item></list>In “<xref rid="Sec15" ref-type="sec">Target activity guided structure generation</xref>“ section the change in policy between the prior and the finetuned model is investigated, providing justification for the second point.</p>
    </sec>
    <sec id="Sec8">
      <title>The prior network</title>
      <p id="Par21">Maximum likelihood estimation was employed to train the initial RNN composed of 3 layers with 1024 Gated Recurrent Units (forget bias 5) in each layer. The RNN was trained on the RDKit [<xref ref-type="bibr" rid="CR27">27</xref>] canonical SMILES of 1.5 million structures from ChEMBL [<xref ref-type="bibr" rid="CR20">20</xref>] where the molecules were restrained to containing between 10 and 50 heavy atoms and elements <inline-formula id="IEq26"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\in \{H, B, C, N, O, F, Si, P, S, Cl, Br, I\}$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:mo>∈</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq26.gif"/></alternatives></inline-formula>. The model was trained with stochastic gradient descent for 50,000 steps using a batch size of 128, utilizing the Adam optimizer [<xref ref-type="bibr" rid="CR28">28</xref>] (<inline-formula id="IEq27"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta _1 = 0.9$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq27.gif"/></alternatives></inline-formula>, <inline-formula id="IEq28"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta _2 = 0.999$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq28.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq29"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\epsilon = 10^{-8}$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mi mathvariant="italic">ϵ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq29.gif"/></alternatives></inline-formula>) with an initial learning rate of 0.001 and a 0.02 learning rate decay every 100 steps. Gradients were clipped to <inline-formula id="IEq30"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[-3, 3]$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>-</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq30.gif"/></alternatives></inline-formula>. Tensorflow [<xref ref-type="bibr" rid="CR29">29</xref>] was used to implement the Prior as well as the RL Agent.</p>
    </sec>
    <sec id="Sec9">
      <title>The agent network</title>
      <p id="Par22">We now frame the problem of generating a SMILES representation of a molecule with specified desirable properties via an RNN as a partially observable Markov decision process, where the agent must make a decision of what character to choose next given the current cell state. We use the probability distributions learnt by the previously described prior model as our initial prior policy. We will refer to the network using the prior policy simply as the <italic>Prior</italic>, and the network whose policy has since been modified as the <italic>Agent</italic>. The Agent is thus also an RNN with the same architecture as the Prior. The task is episodic, starting with the first step of the RNN and ending when the <italic>EOS</italic> token is sampled. The sequence of actions <inline-formula id="IEq31"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A = {a_1, a_2,\ldots ,a_T}$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq31.gif"/></alternatives></inline-formula> during this episode represents the SMILES generated and the product of the action probabilities <inline-formula id="IEq32"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P(A) = \prod _{t = 1}^T{\pi (a_t \mid s_t)}$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mi mathvariant="italic">π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq32.gif"/></alternatives></inline-formula> represents the model likelihood of the sequence formed. Let <inline-formula id="IEq33"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S(A)\in [-1, 1]$$\end{document}</tex-math><mml:math id="M68"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq33.gif"/></alternatives></inline-formula> be a scoring function that rates the desirability of the sequences formed using some arbitrary method. The goal now is to update the agent policy <inline-formula id="IEq34"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\pi$$\end{document}</tex-math><mml:math id="M70"><mml:mi mathvariant="italic">π</mml:mi></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq34.gif"/></alternatives></inline-formula> from the prior policy <inline-formula id="IEq35"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\pi _{Prior}$$\end{document}</tex-math><mml:math id="M72"><mml:msub><mml:mi mathvariant="italic">π</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq35.gif"/></alternatives></inline-formula> in such a way as to increase the expected score for the generated sequences. However, we would like our new policy to be anchored to the prior policy, which has learnt both the syntax of SMILES and distribution of molecular structure in ChEMBL [<xref ref-type="bibr" rid="CR13">13</xref>]. We therefore denote an augmented likelihood <inline-formula id="IEq36"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\log P(A)_{\mathbb {U}}$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:mo>log</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi mathvariant="double-struck">U</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq36.gif"/></alternatives></inline-formula> as a prior likelihood modulated by the desirability of a sequence:<disp-formula id="Equ2"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\log P(A)_{\mathbb {U}} = \log P(A)_{Prior} + \sigma S(A)$$\end{document}</tex-math><mml:math id="M76" display="block"><mml:mrow><mml:mo>log</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi mathvariant="double-struck">U</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>log</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="italic">σ</mml:mi><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="13321_2017_235_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq37"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M78"><mml:mi mathvariant="italic">σ</mml:mi></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq37.gif"/></alternatives></inline-formula> is a scalar coefficient. The return <italic>G</italic>(<italic>A</italic>) of a sequence <italic>A</italic> can in this case be seen as the agreement between the Agent likelihood <inline-formula id="IEq38"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\log P(A)_{\mathbb {A}}$$\end{document}</tex-math><mml:math id="M80"><mml:mrow><mml:mo>log</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq38.gif"/></alternatives></inline-formula> and the augmented likelihood:<disp-formula id="Equ3"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G(A) = -[\log P(A)_{\mathbb {U}} - \log P(A)_{\mathbb {A}}]^2$$\end{document}</tex-math><mml:math id="M82" display="block"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>log</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi mathvariant="double-struck">U</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mo>log</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math><graphic xlink:href="13321_2017_235_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>The goal of the Agent is to learn a policy which maximizes the expected return, achieved by minimizing the cost function <inline-formula id="IEq39"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J(\Theta ) = -G$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq39.gif"/></alternatives></inline-formula>. The fact that we describe the target policy using the policy of the Prior and the scoring function enables us to formulate this cost function. In the Additional file <xref rid="MOESM1" ref-type="media">1</xref> we show how this approach can be described using a REINFORCE [<xref ref-type="bibr" rid="CR30">30</xref>] algorithm with a final step reward of <inline-formula id="IEq40"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r(t) = [\log P(A)_{\mathbb {U}} - \log P(A)_{\mathbb {A}}]^2 / \log P(A)_{\mathbb {A}}$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>log</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi mathvariant="double-struck">U</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mo>log</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">/</mml:mo><mml:mo>log</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi mathvariant="double-struck">A</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq40.gif"/></alternatives></inline-formula>. We believe this is a more natural approach to the problem than REINFORCE algorithms directly using rewards of <italic>S</italic>(<italic>A</italic>) or <inline-formula id="IEq41"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\log P(A)_{Prior} + \sigma S(A)$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:mo>log</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="italic">σ</mml:mi><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq41.gif"/></alternatives></inline-formula>. In “<xref rid="Sec13" ref-type="sec">Learning to avoid sulphur</xref>” section we compare our approach to these methods. The Agent is trained in an on-policy fashion on batches of 128 generated sequences, making an update to <inline-formula id="IEq42"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\pi$$\end{document}</tex-math><mml:math id="M90"><mml:mi mathvariant="italic">π</mml:mi></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq42.gif"/></alternatives></inline-formula> after every batch has been generated and scored. A standard gradient descent optimizer with a learning rate of 0.0005 was used and gradients were clipped to <inline-formula id="IEq43"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[-3, 3]$$\end{document}</tex-math><mml:math id="M92"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>-</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq43.gif"/></alternatives></inline-formula>.</p>
      <p id="Par23">Figure <xref rid="Fig4" ref-type="fig">4</xref> shows an illustration of how the Agent, initially identical to the Prior, is trained using reinforcement learning. The training shifts the probability distribution from that of the Prior towards a distribution modulated by the desirability of the structures. This method adopts a similar concept to Jaques et al. [<xref ref-type="bibr" rid="CR17">17</xref>], while using a policy based RL method that introduces a novel cost function with the aim of addressing the need for handwritten rules and the issues of generating structures that are too simple.<fig id="Fig4"><label>Fig. 4</label><caption><p>The Agent. Illustration of how the model is constructed. Starting from a Prior network trained on ChEMBL, the Agent is trained using the augmented likelihood of the SMILES generated</p></caption><graphic xlink:href="13321_2017_235_Fig4_HTML" id="MO7"/></fig>
</p>
      <p id="Par24">In all the tasks investigated below, the scoring function is fixed during the training of the Agent. If instead the scoring function used is defined by a discriminator network whose task is to distinguish sequences generated by the Agent from ‘real’ SMILES (e.g. a set of actives), the method could be described as a type of Generative Adversarial Network [<xref ref-type="bibr" rid="CR31">31</xref>], where the Agent and the discriminator would be jointly trained in a game where they both strive to beat the other. This is the approach taken by Yu et al. [<xref ref-type="bibr" rid="CR15">15</xref>] to finetune a pretrained sequence model for poem generation. Guimaraes et al. demonstrates how such a method can be combined with a fixed scoring function for molecular de novo design [<xref ref-type="bibr" rid="CR32">32</xref>].</p>
    </sec>
    <sec id="Sec10">
      <title>The DRD2 activity model</title>
      <p id="Par25">In one of our studies the objective of the Agent is to generate molecules that are predicted to be active against a biological target. The dopamine type 2 receptor DRD2 was chosen as the target, and corresponding bioactivity data was extracted from ExCAPE-DB [<xref ref-type="bibr" rid="CR33">33</xref>]. In this dataset there are 7218 actives (pIC50 &gt; 5) and 343204 inactives (pIC50 &lt; 5). A subset of 100,000 inactive compounds was randomly selected. In order to decrease the nearest neighbour similarity between the training and testing structures [<xref ref-type="bibr" rid="CR34">34</xref>–<xref ref-type="bibr" rid="CR36">36</xref>], the actives were grouped in clusters based on their molecular similarity. The Jaccard [<xref ref-type="bibr" rid="CR37">37</xref>] index, for binary vectors also known as the Tanimoto similarity, based on the RDKit implementation of binary Extended Connectivity Molecular Fingerprints with a diameter of 6 (ECFP6 [<xref ref-type="bibr" rid="CR38">38</xref>]) was used as a similarity measure and the actives were clustered using the Butina clustering algorithm [<xref ref-type="bibr" rid="CR39">39</xref>] in RDKit with a clustering cutoff of 0.4. In this algorithm, centroid molecules will be selected, and everything with a similarity higher than 0.4 to these centroids will be assigned to the same cluster. The centroids are chosen such as to maximize the number of molecules that are assigned to any cluster. The clusters were sorted by size and iteratively assigned to the test, validation, and training sets (assigned 4 clusters each iteration) to give a distribution of <inline-formula id="IEq44"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{1}{6}$$\end{document}</tex-math><mml:math id="M94"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>6</mml:mn></mml:mfrac></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq44.gif"/></alternatives></inline-formula>, <inline-formula id="IEq45"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{1}{6}$$\end{document}</tex-math><mml:math id="M96"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>6</mml:mn></mml:mfrac></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq45.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq46"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{4}{6}$$\end{document}</tex-math><mml:math id="M98"><mml:mfrac><mml:mn>4</mml:mn><mml:mn>6</mml:mn></mml:mfrac></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq46.gif"/></alternatives></inline-formula> of the clusters respectively. The inactive compounds, of which less than 0.5% were found to belong to any of the clusters formed by the actives, were split randomly into the three sets using the same ratios.</p>
      <p id="Par26">A support vector machine (SVM) classifier with a Gaussian kernel was built in Scikit-learn [<xref ref-type="bibr" rid="CR40">40</xref>] on the training set as a predictive model for DRD2 activity. The optimal C and Gamma values utilized in the final model were obtained from a grid search for the highest ROC-AUC performance on the validation set.</p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>Results and discussion</title>
    <sec id="Sec12">
      <title>Structure generation by the Prior</title>
      <p id="Par27">After the initial training, 94% of the sequences generated by the Prior as described in “<xref rid="Sec5" ref-type="sec">Generating new samples</xref>” section corresponded to valid molecular structures according to RDKit [<xref ref-type="bibr" rid="CR27">27</xref>] parsing, out of which 90% are novel structures outside of the training set. A set of randomly chosen structures generated by the Prior, as well as by Agents trained in the subsequent examples, are shown in the Additional file <xref rid="MOESM2" ref-type="media">2</xref>. The process of generating a SMILES by the Prior is illustrated in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. For every token in the generated SMILES sequence, the conditional probability distribution over the vocabulary at this step according to the Prior is displayed. The sequence of distributions are depicted in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. For the first step, when no information other than the initial GO token is present, the distribution is an approximation of the distribution of first tokens for the SMILES in the ChEMBL training set. In this case “O” was sampled, but “C”, “N”, and the halogens were all likely as well. Corresponding log likelihoods were −0.3 for “C”, −2.7 for “N”, −1.8 for “O”, and −5.0 for “F” and “Cl”.
<fig id="Fig5"><label>Fig. 5</label><caption><p>How the model thinks while generating the molecule on the right. Conditional probability over the next token as a function of previously chosen ones according to the model. On the y-axis is shown the probability distribution for the character to be choosen at the current step, and on the x-axis is shown the character that in this instance was sampled. E = EOS</p></caption><graphic xlink:href="13321_2017_235_Fig5_HTML" id="MO8"/></fig>
</p>
      <p id="Par28">A few (unsurprising) observations:<list list-type="bullet"><list-item><p id="Par29">Once the aromatic “n” has been sampled, the model has come to expect a ring opening (i.e. a number), since aromatic moieties by definition are cyclic.</p></list-item><list-item><p id="Par30">Once an aromatic ring has been opened, the aromatic atoms “c”, “n”, “o”, and “s” become probable, until 5 or 6 steps later when the model thinks it is time to close the ring.</p></list-item><list-item><p id="Par31">The model has learnt the RDKit canonicalized SMILES format of increasing ring numbers, and expects the first ring to be numbered “1”. Ring numbers can be reused, as in the two first rings in this example. Only once “1” has been sampled does it expect a ring to be numbered “2” and so on.</p></list-item></list>
</p>
    </sec>
    <sec id="Sec13">
      <title>Learning to avoid sulphur</title>
      <p id="Par32">As a proof of principle the Agent was first trained to generate molecules which do not contain sulphur. The method described in “<xref rid="Sec9" ref-type="sec">The Agent network</xref>” is compared with three other policy gradient based methods. The first alternative method is the same as the Agent method, with the only difference that the loss is defined on an action basis rather than on an episodic one, resulting in the cost function:<disp-formula id="Equ4"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J(\Theta ) = \left[ \sum _{t=0}^T{(\log \pi _{Prior}(a_t, s_t) - \log \pi _{\Theta }(a_t, s_t))} + \sigma S(A)\right] ^2$$\end{document}</tex-math><mml:math id="M100" display="block"><mml:mrow><mml:mi>J</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mfenced close="]" open="[" separators=""><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>log</mml:mo><mml:msub><mml:mi mathvariant="italic">π</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mo>log</mml:mo><mml:msub><mml:mi mathvariant="italic">π</mml:mi><mml:mi mathvariant="normal">Θ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="italic">σ</mml:mi><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math><graphic xlink:href="13321_2017_235_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>We refer to this method as ‘Action basis’. The second alternative is a REINFORCE algorithm with a reward of <italic>S</italic>(<italic>A</italic>) given at the last step. This method is similar to the one used by Silver et al. to train the policy network in AlphaGo [<xref ref-type="bibr" rid="CR41">41</xref>], as well as the method used by Yu et al. [<xref ref-type="bibr" rid="CR15">15</xref>]. We refer to this method as ‘REINFORCE’. The corresponding cost function can be written as:<disp-formula id="Equ5"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J(\Theta ) = S(A)\sum _{t=0}^T \log \pi _{\Theta }(a_t, s_t)$$\end{document}</tex-math><mml:math id="M102" display="block"><mml:mrow><mml:mi>J</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mo>log</mml:mo><mml:msub><mml:mi mathvariant="italic">π</mml:mi><mml:mi mathvariant="normal">Θ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="13321_2017_235_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>A variation of this method that considers prior likelihood is defined by changing the reward from <italic>S</italic>(<italic>A</italic>) to <inline-formula id="IEq47"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S(A)+ \log P(A)_{Prior}$$\end{document}</tex-math><mml:math id="M104"><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo>log</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq47.gif"/></alternatives></inline-formula>. This method is referred to as ‘REINFORCE + Prior’, with the cost function:<disp-formula id="Equ6"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J(\Theta ) = [\log P(A)_{Prior} + \sigma S(A)]\sum _{t=0}^T \log \pi _{\Theta }(a_t, s_t)$$\end{document}</tex-math><mml:math id="M106" display="block"><mml:mrow><mml:mi>J</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>log</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="italic">σ</mml:mi><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mo>log</mml:mo><mml:msub><mml:mi mathvariant="italic">π</mml:mi><mml:mi mathvariant="normal">Θ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="13321_2017_235_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>Note that the last method by nature strives to generate only the putative sequence with the highest reward. In contrast to the Agent, the optimal policy for this method is not stochastic. This tendency could be restrained by introducing a regularizing policy entropy term. However, it was found that such regularization undermined the models ability to produce valid SMILES. This method is therefor dependent on only training sufficiently long for the model to reach a point where highly scored sequences are generated, without being settled in a local minima. The experiment aims to answer the following questions:<list list-type="bullet"><list-item><p id="Par33">Can the models achieve the task of generating valid SMILES corresponding to structures that do not contain sulphur?</p></list-item><list-item><p id="Par34">Will the models exploit the reward function by converging on naïve solutions such as ‘C’ if not imposed handwritten rules?</p></list-item><list-item><p id="Par35">Are the distributions of physical chemical properties for the generated structures similar to those of sulphur free structures generated by the Prior?</p></list-item></list>The task is defined by the following scoring function:<disp-formula id="Equ7"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} S(A) =\left\{ \begin{array}{ll} 1 &amp;{}\quad \text {if valid and no S} \\ 0 &amp;{}\quad \text {if not valid} \\ -1 &amp;{}\quad \text {if contains S} \end{array}\right. \end{aligned}$$\end{document}</tex-math><mml:math id="M108" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced close="" open="{" separators=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mspace width="1em"/><mml:mtext>if valid and no S</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mspace width="1em"/><mml:mtext>if not valid</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mspace width="1em"/><mml:mtext>if contains S</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13321_2017_235_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>All the models were trained for 1000 steps starting from the Prior and 12,800 SMILES sequences were sampled from all the models as well as the Prior. A learning rate of 0.0005 was used for the Agent and Action basis methods, and 0.0001 for the two REINFORCE methods. The values of <inline-formula id="IEq48"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M110"><mml:mi mathvariant="italic">σ</mml:mi></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq48.gif"/></alternatives></inline-formula> used were 2 for the Agent and ‘REINFORCE + Prior’, and 8 for ‘Action basis’. To explore what effect the training has on the structures generated, relevant properties for non sulphur containing structures generated by both the Prior and the other models were compared. The molecular weight, cLogP, the number of rotatable bonds, and the number of aromatic rings were all calculated using RDKit. The experiment was repeated three times with different random seeds. The results are shown in Table <xref rid="Tab1" ref-type="table">1</xref> and randomly selected SMILES generated by the Prior and the different models can be seen in Table <xref rid="Tab2" ref-type="table">2</xref>. For the ‘REINFORCE’ method, where the sole aim is to generate valid SMILES that do not contain sulphur, the model quickly learns to exploit the reward funtion by generating sequences containing predominately ‘C’. This is not surprising, since any sequence consisting only of this token always gets rewarded. For the ‘REINFORCE + Prior’ method, the inclusion of the prior likelihood in the reward function means that this is no longer a viable strategy (the sequences would be given a low prior probability). The model instead tries to find the structure with the best combination of score and prior likelihood, but as is evident from the SMILES generated and the statistics shown in Table <xref rid="Tab1" ref-type="table">1</xref>, this results in small, simplistic structures being generated. Thus, both REINFORCE algorithms managed to achieve high scores according to the scoring function, but poorly represented the Prior. Both the Agent and the ‘Action basis’ methods have explicitly specified target policies. For the ‘Action basis’ method the policy is specified exactly on a stepwise level, while for the Agent the target policy is only specified to the likelihoods of entire sequences. Although the ‘Action basis’ method generates structures that are more similar to the Prior than the two REINFORCE methods, it performed worse than the Agent despite the higher value of <inline-formula id="IEq49"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M112"><mml:mi mathvariant="italic">σ</mml:mi></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq49.gif"/></alternatives></inline-formula> used while also being slower to learn. This may be due to the less restricted target policy of the Agent, which could facilitate optimization. The Agent achieved the same fraction of sulphur free structures as the REINFORCE algorithms, while seemingly doing a much better job of representing the Prior. This is indicated by the similarity of the properties of the generated structures shown in Table <xref rid="Tab1" ref-type="table">1</xref> as well as the SMILES themselves shown in Table <xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison of model performance and properties for non-sulphur containing structures generated by the two models</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Prior</th><th align="left">Agent</th><th align="left">Action basis</th><th align="left">REINFORCE</th><th align="left">REINFORCE + Prior</th></tr></thead><tbody><tr><td align="left">Fraction of valid SMILES</td><td char="±" align="char">0.94 ± 0.01</td><td char="±" align="char">0.95 ± 0.01</td><td char="±" align="char">0.95 ± 0.01</td><td char="±" align="char">0.98 ± 0.00</td><td char="±" align="char">0.98 ± 0.00</td></tr><tr><td align="left">Fraction without sulphur</td><td char="±" align="char">0.66 ± 0.01</td><td char="±" align="char">0.98 ± 0.00</td><td char="±" align="char">0.92 ± 0.02</td><td char="±" align="char">0.98 ± 0.00</td><td char="±" align="char">0.92 ± 0.01</td></tr><tr><td align="left">Average molecular weight</td><td char="±" align="char">371 ± 1.70</td><td char="±" align="char">367 ± 3.30</td><td char="±" align="char">372 ± 0.94</td><td char="±" align="char">585 ± 27.4</td><td char="±" align="char">232 ± 5.25</td></tr><tr><td align="left">Average cLogP</td><td char="±" align="char">3.36 ± 0.04</td><td char="±" align="char">3.37 ± 0.09</td><td char="±" align="char">3.39 ± 0.02</td><td char="±" align="char">11.3 ± 0.85</td><td char="±" align="char">3.05 ± 0.02</td></tr><tr><td align="left">Average NumRotBonds</td><td char="±" align="char">5.39 ± 0.04</td><td char="±" align="char">5.41 ± 0.07</td><td char="±" align="char">6.08 ± 0.04</td><td char="±" align="char">30.0 ± 2.17</td><td char="±" align="char">2.8 ± 0.11</td></tr><tr><td align="left">Average NumAromRings</td><td char="±" align="char">2.26 ± 0.02</td><td char="±" align="char">2.26 ± 0.02</td><td char="±" align="char">2.09 ± 0.02</td><td char="±" align="char">0.57 ± 0.04</td><td char="±" align="char">2.11 ± 0.02</td></tr></tbody></table><table-wrap-foot><p>Properties reported as Mean ± SD</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Randomly selected SMILES generated by the different models</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Sampled SMILES</th></tr></thead><tbody><tr><td align="left" rowspan="3">Prior</td><td align="left">CCOC(=O)C1=C(C)OC(N)=C(C#N)C1c1ccccc1C(F)(F)F</td></tr><tr><td align="left">COC(=O)CC(C)=NNc1ccc(N(C)C)cc1[N+](=O)[O-]</td></tr><tr><td align="left">Cc1ccccc1CNS(=O)(=O)c1ccc2c(c1)C(=O)C(=O)N2</td></tr><tr><td align="left" rowspan="3">Agent</td><td align="left">CC(C)(C)NC(=O)c1ccc(OCc2ccccc2C(F)(F)F)nc1-c1ccccc1</td></tr><tr><td align="left">CC(=O)NCC1OC(=O)N2c3ccc(-c4cccnc4)cc3OCC12</td></tr><tr><td align="left">OCCCNCc1cccc(-c2cccc(-c3nc4ccccc4[nH]3)c2OCCOc2ncc(Cl)cc2Br)c1</td></tr><tr><td align="left" rowspan="3">Action level</td><td align="left">CCN1CC(C)(C)OC(=O)c2cc(-c3ccc(Cl)cc3)ccc21</td></tr><tr><td align="left">CCC(CC)C(=O)Nc1ccc2cnn(-c3ccc(C(C)=O)cc3)c2c1</td></tr><tr><td align="left">CCCCN1C(=O)c2ccccc2NC1c1ccc(OC)cc1</td></tr><tr><td align="left" rowspan="3">REINFORCE</td><td align="left">CC1CCCCC12NC(=O)N(CC(=O)Nc1ccccc1C(=O)O)C2=O</td></tr><tr><td align="left">CCCCCCCCCCCCCCCCCCCCCCCCCCCCNC(=O)OCCCCCC</td></tr><tr><td align="left">CCCCCCCCCCCCCCCCCCCCCC1CCC(O)C1(CCC)CCCCCCCCCCCCCCC</td></tr><tr><td align="left" rowspan="3">REINFORCE + Prior</td><td align="left">Nc1ccccc1C(=O)Oc1ccccc1</td></tr><tr><td align="left">O=c1cccccc1Oc1ccccc1</td></tr><tr><td align="left">Nc1ccc(-c2ccccc2O)cc1</td></tr></tbody></table></table-wrap>
</p>
    </sec>
    <sec id="Sec14">
      <title>Similarity guided structure generation</title>
      <p id="Par36">The second task investigated was that of generating structures similar to a query structure. The Jaccard index [<xref ref-type="bibr" rid="CR37">37</xref>] <inline-formula id="IEq50"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J_{i, j}$$\end{document}</tex-math><mml:math id="M114"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq50.gif"/></alternatives></inline-formula> of the RDKit implementation of FCFP4 [<xref ref-type="bibr" rid="CR38">38</xref>] fingerprints was used as a similarity measure between molecules <italic>i</italic> and <italic>j</italic>. Compared to the DRD2 activity model (“<xref rid="Sec10" ref-type="sec">The DRD2 activity model</xref>” section), the feature invariant version of the fingerprints and the smaller diameter 4 was used in order to get a more fuzzy similarity measure. The scoring function was defined as:<disp-formula id="Equ8"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} S(A) = -1 + 2 \times \frac{\min \{ J_{i, j}, k \}}{k} \end{aligned}$$\end{document}</tex-math><mml:math id="M116" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mo movablelimits="true">min</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13321_2017_235_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>This definition means that an increase in similarity is only rewarded up to the point of <inline-formula id="IEq51"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k\in [0, 1]$$\end{document}</tex-math><mml:math id="M118"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq51.gif"/></alternatives></inline-formula>, as well as scaling the reward from <inline-formula id="IEq52"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1$$\end{document}</tex-math><mml:math id="M120"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq52.gif"/></alternatives></inline-formula> (no overlap in the fingerprints between query and generated structure) to 1 (at least <italic>k</italic> degree of overlap). Celecoxib was chosen as our query structure, and we first investigated whether Celecoxib itself could be generated by using the high values of <inline-formula id="IEq53"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k=1$$\end{document}</tex-math><mml:math id="M122"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq53.gif"/></alternatives></inline-formula> and <inline-formula id="IEq54"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma =15$$\end{document}</tex-math><mml:math id="M124"><mml:mrow><mml:mi mathvariant="italic">σ</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq54.gif"/></alternatives></inline-formula>. The Agent was trained for 1000 steps. After a 100 training steps the Agent starts to generate Celecoxib, and after 200 steps it predominately generates this structure (Fig. <xref rid="Fig6" ref-type="fig">6</xref>).</p>
      <p id="Par37">Celecoxib itself as well as many other similar structures appear in the ChEMBL training set used to build the Prior. An interesting question is whether the Agent could succeed in generating Celecoxib when these structures are not part of the chemical space covered by the Prior. To investigate this, all structures with a similarity to Celecoxib higher than 0.5 (corresponding to 1804 molecules) were removed from the training set and a new reduced Prior was trained. The prior likelihood of Celecoxib for the canonical and reduced Priors was compared, as well as the ability of the models to generate structures similar to Celecoxib. As expected, the prior probability of Celecoxib decreased when similar compounds were removed from the training set from <inline-formula id="IEq55"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\log _e P = -12.7$$\end{document}</tex-math><mml:math id="M126"><mml:mrow><mml:msub><mml:mo>log</mml:mo><mml:mi>e</mml:mi></mml:msub><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>12.7</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq55.gif"/></alternatives></inline-formula> to <inline-formula id="IEq56"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\log _e P = -19.2$$\end{document}</tex-math><mml:math id="M128"><mml:mrow><mml:msub><mml:mo>log</mml:mo><mml:mi>e</mml:mi></mml:msub><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>19.2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq56.gif"/></alternatives></inline-formula>, representing a reduction in likelihood of a factor of 700. An Agent was then trained using the same hyperparameters as before, but on the reduced Prior. After 400 steps, the Agent again managed to find Celecoxib, albeit requiring more time to do so. After 1000 steps, Celecoxib was the most commonly generated structure (about a third of the generated structures), followed by demethylated Celecoxib (also a third) whose SMILES is more likely according to the Prior with <inline-formula id="IEq57"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\log _e P = -15.2$$\end{document}</tex-math><mml:math id="M130"><mml:mrow><mml:msub><mml:mo>log</mml:mo><mml:mi>e</mml:mi></mml:msub><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>15.2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq57.gif"/></alternatives></inline-formula> but has a lower similarity (<inline-formula id="IEq58"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J = 0.87$$\end{document}</tex-math><mml:math id="M132"><mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mn>0.87</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq58.gif"/></alternatives></inline-formula>), resulting in an augmented likelihood equal to that of Celecoxib.</p>
      <p id="Par38">These experiments demonstrate that the Agent can be optimized using fingerprint based Jaccard similarity as the objective, but making copies of the query structure is hardly useful. A more useful example is that of generating structures that are moderately to the query structure. The Agent was therefore trained for 3000 steps, starting from both the canonical as well as the reduced Prior, using <inline-formula id="IEq59"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k = 0.7$$\end{document}</tex-math><mml:math id="M134"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq59.gif"/></alternatives></inline-formula> and <inline-formula id="IEq60"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma = 12$$\end{document}</tex-math><mml:math id="M136"><mml:mrow><mml:mi mathvariant="italic">σ</mml:mi><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq60.gif"/></alternatives></inline-formula>. The Agents based on the canonical Prior quickly converge to their targets, while the Agents based on the reduced Prior converged more slowly. For the Agent based on the reduced Prior where <inline-formula id="IEq61"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k=1$$\end{document}</tex-math><mml:math id="M138"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq61.gif"/></alternatives></inline-formula>, the fact that Celecoxib and demethylated Celecoxib are given similar augmented likelihoods means that the average similarity converges to around 0.9 rather than 1.0. For the Agent based on the reduced Prior where <inline-formula id="IEq62"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k=0.7$$\end{document}</tex-math><mml:math id="M140"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq62.gif"/></alternatives></inline-formula>, the lower prior likelihood of compounds similar to Celecoxib translates to a lower augmented likelihood, which lowers the average similarity of the structures generated by the Agent.</p>
      <p id="Par39">To explore whether this reduced prior likelihood could be offset with a higher value of <inline-formula id="IEq63"><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M142"><mml:mi mathvariant="italic">σ</mml:mi></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq63.gif"/></alternatives></inline-formula>, an Agent starting from the reduced Prior was trained using <inline-formula id="IEq64"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma =15$$\end{document}</tex-math><mml:math id="M144"><mml:mrow><mml:mi mathvariant="italic">σ</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq64.gif"/></alternatives></inline-formula>. Though taking slightly more time to converge than the Agent based on the canonical Prior, this Agent too could converge to the target similarity. The learning curves for the different model is shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>.<fig id="Fig6"><label>Fig. 6</label><caption><p>Average similarity <inline-formula id="IEq65"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{J}}}$$\end{document}</tex-math><mml:math id="M146"><mml:mi>J</mml:mi></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq65.gif"/></alternatives></inline-formula> of generated structures as a function of training steps. Difference in learning dynamics for the Agents based on the canonical Prior, and those based on a reduced Prior where everything more similar than <inline-formula id="IEq66"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J=0.5$$\end{document}</tex-math><mml:math id="M148"><mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq66.gif"/></alternatives></inline-formula> to Celecoxib have been removed</p></caption><graphic xlink:href="13321_2017_235_Fig6_HTML" id="MO14"/></fig>
</p>
      <p id="Par40">An illustration of how the type of structures generated by the Agent evolves during training is shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. For the Agent based on the reduced Prior with <inline-formula id="IEq69"><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k=0.7$$\end{document}</tex-math><mml:math id="M150"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq69.gif"/></alternatives></inline-formula> and <inline-formula id="IEq70"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma =15$$\end{document}</tex-math><mml:math id="M152"><mml:mrow><mml:mi mathvariant="italic">σ</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq70.gif"/></alternatives></inline-formula>, three structures were randomly sampled every 100 training steps from step 0 up to step 400. At first, the structures are not similar to Celecoxib. After 200 steps, some features from Celecoxib have started to emerge, and after 300 steps the model generates mostly close analogues of Celecoxib.<fig id="Fig7"><label>Fig. 7</label><caption><p>Evolution of generated structures during training Structures sampled every 100 training steps during the training of the Agent towards similarity to Celecoxib with <inline-formula id="IEq67"><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k=0.7$$\end{document}</tex-math><mml:math id="M154"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq67.gif"/></alternatives></inline-formula> and <inline-formula id="IEq68"><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma =15$$\end{document}</tex-math><mml:math id="M156"><mml:mrow><mml:mi mathvariant="italic">σ</mml:mi><mml:mo>=</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq68.gif"/></alternatives></inline-formula>
</p></caption><graphic xlink:href="13321_2017_235_Fig7_HTML" id="MO15"/></fig>
</p>
      <p id="Par41">We have investigated how various factors affect the learning behaviour of the Agent. In real drug discovery applications, we might be more interested in finding structures with modest similarity to our query molecules rather than very close analogues. For example, one of the structures sampled after 200 steps shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref> displays a type of scaffold hopping where the sulphur functional group on one of the outer aromatic rings has been fused to the central pyrazole. The similarity to Celecoxib of this structure is 0.4, which may be a more interesting solution for scaffold-hopping purposes. One can choose hyperparameters and similarity criterion tailored to the desired output. Other types of similarity measures such as pharmacophoric fingerprints [<xref ref-type="bibr" rid="CR42">42</xref>], Tversky substructure similarity [<xref ref-type="bibr" rid="CR43">43</xref>], or presence/absence of certain pharmacophores could also be explored.</p>
    </sec>
    <sec id="Sec15">
      <title>Target activity guided structure generation</title>
      <p id="Par42">The third example, perhaps the one most interesting and relevant for drug discovery, is to optimize the Agent towards generating structures with predicted biological activity. This can be seen as a form of inverse QSAR, where the Agent is used to implicitly map high predicted probability of activity to molecular structure. DRD2 was chosen as the biological target. The clustering split of the DRD2 activity dataset as described in “<xref rid="Sec10" ref-type="sec">The DRD2 activity model</xref>” section resulted in 1405, 1287, and 4526 actives in the test, validation, and training sets respectively. The average similarity to the nearest neighbour in the training set for the test set compounds was 0.53. For a random split of actives in sets of the same sizes this similarity was 0.69, indicating that the clustering had significantly decreased training-test set similarity which mimics the hit finding practice in drug discovery to identify diverse hits to the training set. Most of the DRD2 actives are also included in the ChEMBL dataset used to train the Prior. To explore the effect of not having the known actives included in the Prior, a reduced Prior was trained on a reduced subset of the ChEMBL training set where all DRD2 actives had been removed.</p>
      <p id="Par43">The optimal hyperparameters found for the SVM activity model were <inline-formula id="IEq76"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C=2^{7}, \gamma =2^{-6}$$\end{document}</tex-math><mml:math id="M158"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mn>7</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="italic">γ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq76.gif"/></alternatives></inline-formula>, resulting in a model whose performance is shown in Table <xref rid="Tab3" ref-type="table">3</xref>. The good performance in general can be explained by the apparent difference between actives and inactive compounds as seen during the clustering, and the better performance on the test set compared to the validation set could be due to slightly higher nearest neighbour similarity to the training actives (0.53 for test actives and 0.48 for validation actives).<table-wrap id="Tab3"><label>Table 3</label><caption><p>Performance of the DRD2 activity model</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Set</th><th align="left">Training</th><th align="left">Validation</th><th align="left">Test</th></tr></thead><tbody><tr><td align="left">Accuracy</td><td char="." align="char">1.00</td><td char="." align="char">0.98</td><td char="." align="char">0.98</td></tr><tr><td align="left">ROC-AUC</td><td char="." align="char">1.00</td><td char="." align="char">0.99</td><td char="." align="char">1.00</td></tr><tr><td align="left">Precision</td><td char="." align="char">1.00</td><td char="." align="char">0.96</td><td char="." align="char">0.97</td></tr><tr><td align="left">Recall</td><td char="." align="char">1.00</td><td char="." align="char">0.73</td><td char="." align="char">0.82</td></tr></tbody></table></table-wrap>
</p>
      <p id="Par44">The output of the DRD2 model for a given structure is an uncalibrated predicted probability of being active <inline-formula id="IEq77"><alternatives><tex-math id="M159">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P_{active}$$\end{document}</tex-math><mml:math id="M160"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq77.gif"/></alternatives></inline-formula>. This value is used to formulate the following scoring function:<disp-formula id="Equ9"><alternatives><tex-math id="M161">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} S(A) = -1 + 2 \times P_{active} \end{aligned}$$\end{document}</tex-math><mml:math id="M162" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13321_2017_235_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>The model was trained for 3000 steps using <inline-formula id="IEq78"><alternatives><tex-math id="M163">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma = 7$$\end{document}</tex-math><mml:math id="M164"><mml:mrow><mml:mi mathvariant="italic">σ</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq78.gif"/></alternatives></inline-formula>. After training, the fraction of predicted actives according to the DRD2 model increased from 0.02 for structures generated by the reduced Prior to 0.96 for structures generated by the corresponding Agent network (Table <xref rid="Tab4" ref-type="table">4</xref>). To see how well the structure-activity-relationship learnt by the activity model is transferred to the type of structures generated by the Agent RNN, the fraction of compounds with an ECFP6 Jaccard similarity greater than 0.4 to any active in the training and test sets was calculated.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Comparison of properties for structures generated by the canonical Prior, the reduced Prior, and corresponding Agents</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Prior</th><th align="left">Agent</th><th align="left">Prior<sup>a</sup>
</th><th align="left">Agent<sup>a</sup>
</th></tr></thead><tbody><tr><td align="left">Fraction valid SMILES</td><td char="." align="char">0.94</td><td char="." align="char">0.99</td><td char="." align="char">0.94</td><td char="." align="char">0.99</td></tr><tr><td align="left">Fraction predicted actives</td><td char="." align="char">0.03</td><td char="." align="char">0.97</td><td char="." align="char">0.02</td><td char="." align="char">0.96</td></tr><tr><td align="left">Fraction similar to train active</td><td char="." align="char">0.02</td><td char="." align="char">0.79</td><td char="." align="char">0.02</td><td char="." align="char">0.75</td></tr><tr><td align="left">Fraction similar to test active</td><td char="." align="char">0.01</td><td char="." align="char">0.46</td><td char="." align="char">0.01</td><td char="." align="char">0.38</td></tr><tr><td align="left">Fraction of test actives recovered (<inline-formula id="IEq73"><alternatives><tex-math id="M165">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times 10^{-3}$$\end{document}</tex-math><mml:math id="M166"><mml:mrow><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq73.gif"/></alternatives></inline-formula>)</td><td char="." align="char">13.5</td><td char="." align="char">126</td><td char="." align="char">2.85</td><td char="." align="char">72.6</td></tr><tr><td align="left">Probability of generating a test set active (<inline-formula id="IEq74"><alternatives><tex-math id="M167">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times 10^{-3}$$\end{document}</tex-math><mml:math id="M168"><mml:mrow><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq74.gif"/></alternatives></inline-formula>)</td><td char="." align="char">0.17</td><td char="." align="char">40.2</td><td char="." align="char">0.05</td><td char="." align="char">15.0</td></tr></tbody></table><table-wrap-foot><p><sup>a</sup>DRD2 actives witheld from the training of the Prior</p></table-wrap-foot></table-wrap>
</p>
      <p id="Par45">In some cases, the model recovered exact matches from the training and test sets (c.f. Segler et al. [<xref ref-type="bibr" rid="CR13">13</xref>]). The fraction of recovered test actives recovered by the canonical and reduced Prior were 1.3 and 0.3% respectively. The Agent derived from the canonical Prior managed to recover 13% test actives; the Agent derived from the reduced Prior recovered 7%. For the Agent derived from the reduced Prior, where the DRD2 actives were excluded from the Prior training set, this means that the model has learnt to generate “novel” structures that have been seen neither by the DRD2 activity model nor the Prior, and are experimentally confirmed actives. We can formalize this observation by calculating the probability of a given generated sequence belonging to the set of test actives. For the canonical and reduced Priors, this probability was <inline-formula id="IEq79"><alternatives><tex-math id="M169">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.17\times 10^{-3}$$\end{document}</tex-math><mml:math id="M170"><mml:mrow><mml:mn>0.17</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq79.gif"/></alternatives></inline-formula> and <inline-formula id="IEq80"><alternatives><tex-math id="M171">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.05\times 10^{-3}$$\end{document}</tex-math><mml:math id="M172"><mml:mrow><mml:mn>0.05</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq80.gif"/></alternatives></inline-formula> respectively. Removing the actives from the Prior thus resulted in a threefold reduction in the probability of generating a structure from the set of test actives. For the Agents, the probabilities rose to <inline-formula id="IEq81"><alternatives><tex-math id="M173">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$15.0\times 10^{-3}$$\end{document}</tex-math><mml:math id="M174"><mml:mrow><mml:mn>15.0</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq81.gif"/></alternatives></inline-formula> and <inline-formula id="IEq82"><alternatives><tex-math id="M175">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$40.2\times 10^{-3}$$\end{document}</tex-math><mml:math id="M176"><mml:mrow><mml:mn>40.2</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq82.gif"/></alternatives></inline-formula> respectively, corresponding to an enrichment of a factor of 250 over the Prior models. Again the consequence of removing the actives from the Prior was a threefold reduction in the probability of generating a test set active: the difference between the two Priors is directly mirrored by their corresponding Agents. Apart from generating a higher fraction of structures that are predicted to be active, both Agents also generate a significantly higher fraction of valid SMILES (Table <xref rid="Tab4" ref-type="table">4</xref>). Sequences that are not valid SMILES receive a score of <inline-formula id="IEq83"><alternatives><tex-math id="M177">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1$$\end{document}</tex-math><mml:math id="M178"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq83.gif"/></alternatives></inline-formula>, which means that the scoring function naturally encourages valid SMILES.</p>
      <p id="Par46">A few of the test set actives generated by the Agent based on the reduced Prior along with a few randomly selected generated structures are shown together with their predicted probability of activity in Fig. <xref rid="Fig8" ref-type="fig">8</xref>. Encouragingly, the recovered test set actives vary considerably in their structure, which would not have been the case had the Agent converged to generating only a certain type of very similar predicted active compounds.<fig id="Fig8"><label>Fig. 8</label><caption><p>Structures designed by the Agent to target DRD2. Molecules generated by the Agent based on the reduced Prior. On the <italic>top</italic> are four of the test set actives that were recovered, and <italic>below</italic> are four randomly selected structures. The structures are annotated with the predicted probability of being active</p></caption><graphic xlink:href="13321_2017_235_Fig8_HTML" id="MO16"/></fig>
</p>
      <p id="Par47">Removing the known actives from the training set of the Prior resulted in an Agent which shows a decrease in all metrics measuring the overlap between the known actives and the structures generated, compared to the Agent derived from the canonical Prior. Interestingly, the fraction of predicted actives did not change significantly. This indicates that the Agent derived from the reduced Prior has managed to find a similar chemical space to that of the canonical Agent, with structures that are equally likely to be predicted as active, but are less similar to the known actives. Whether or not these compounds are active will be dependent on the accuracy of the target activity model. Ideally, any predictive model to be used in conjunction with the generative model should cover a broad chemical space within its domain of applicability, since it initially has to assess representative structures of the dataset used to build the Prior [<xref ref-type="bibr" rid="CR13">13</xref>].</p>
      <p id="Par48">Figure <xref rid="Fig9" ref-type="fig">9</xref> shows a comparison of the conditional probability distributions for the reduced Prior and its corresponding Agent when a molecule from the set of test actives is generated. It can be seen that the changes are not drastic with most of the trends learnt by the Prior being carried over to the Agent. However, a big change in the probability distribution even only at one step can have a large impact on the likelihood of the sequence and could significantly alter the type of structures generated.<fig id="Fig9"><label>Fig. 9</label><caption><p>A (small) change of mind. The conditional probability distributions when the DRD2 test set active ‘COc1ccccc1N1CCN(CCCCNC(=O)c2ccccc2I)CC1’ is generated by the Prior and an Agent trained using the DRD2 activity model, for the case where all actives used to build the activity model have been removed from the Prior. E = EOS</p></caption><graphic xlink:href="13321_2017_235_Fig9_HTML" id="MO18"/></fig>
</p>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Conclusion</title>
    <p id="Par49">To summarize, we believe that an RNN operating on the SMILES representation of molecules is a promising method for molecular de novo design. It is a data-driven generative model that does not rely on pre-defined building blocks and rules, which makes it clearly differentiated from traditional methods. In this study we extend upon previous work [<xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR17">17</xref>] by introducing a reinforcement learning method which can be used to tune the RNN to generate structures with certain desirable properties through augmented episodic likelihood.</p>
    <p id="Par50">The model was tested on the task of generating sulphur free molecules as a proof of principle, and the method using augmented episodic likelihood was compared with traditional policy gradient methods. The results indicate that the Agent can find solutions reflecting the underlying probability distribution of the Prior, representing a significant improvement over both traditional REINFORCE [<xref ref-type="bibr" rid="CR30">30</xref>] algorithms as well as previously reported methods [<xref ref-type="bibr" rid="CR17">17</xref>]. To evaluate if the model could be used to generate analogues to a query structure, the Agent was trained to generate structures similar to the drug Celecoxib. Even when all analogues of Celecoxib were removed from the Prior, the Agent could still locate the intended region of chemical space which was not part of the Prior. Further more, when trained towards generating predicted actives against the dopamine receptor type 2 (DRD2), the Agent generates structures of which more than 95% are predicted to be active, and could recover test set actives even in the case where they were not included in either the activity model nor the Prior. Our results indicate that the method could be a useful tool for drug discovery.</p>
    <p id="Par51">It is clear that the qualities of the Prior are reflected in the corresponding Agents it produces. An exhaustive study which explores how parameters such as training set size, model size, regularization [<xref ref-type="bibr" rid="CR44">44</xref>, <xref ref-type="bibr" rid="CR45">45</xref>], and training time would influence the quality and variety of structures generated by the Prior would be interesting. Another interesting avenue for future research might be that of token embeddings [<xref ref-type="bibr" rid="CR46">46</xref>]. The method was found to be robust with respect to the hyperparameters <inline-formula id="IEq84"><alternatives><tex-math id="M179">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M180"><mml:mi mathvariant="italic">σ</mml:mi></mml:math><inline-graphic xlink:href="13321_2017_235_Article_IEq84.gif"/></alternatives></inline-formula> and the learning rate.</p>
    <p id="Par52">All of the aforementioned examples used single parameter based scoring functions. In a typical drug discovery project, multiple parameters such as target activity, DMPK profile, synthetic accessibility etc. all need to be taken into account simultaneously. Applying this type of multi-parametric scoring functions to the model is an area requiring further research.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Additional files</title>
    <sec id="Sec17">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="13321_2017_235_MOESM1_ESM.pdf">
            <caption>
              <p><bold>Additional file 1.</bold> Equivalence to REINFORCE. Proof that the method used can be described as a REINFORCE type algorithm.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="13321_2017_235_MOESM2_ESM.pdf">
            <caption>
              <p><bold>Additional file 2.</bold> Generated structures. Structures generated by the canonical Prior and different Agents.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>DMPK</term>
        <def>
          <p>drug metabolism and pharmacokinetics</p>
        </def>
      </def-item>
      <def-item>
        <term>DRD2</term>
        <def>
          <p>dopamine receptor D2</p>
        </def>
      </def-item>
      <def-item>
        <term>QSAR</term>
        <def>
          <p>quantitive structure activity relationship</p>
        </def>
      </def-item>
      <def-item>
        <term>RNN</term>
        <def>
          <p>recurrent neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>RL</term>
        <def>
          <p>reinforcement Learning</p>
        </def>
      </def-item>
      <def-item>
        <term>Log</term>
        <def>
          <p>natural logarithm</p>
        </def>
      </def-item>
      <def-item>
        <term>BPTT</term>
        <def>
          <p>back-propagation through time</p>
        </def>
      </def-item>
      <def-item>
        <term>
          <italic>A</italic>
        </term>
        <def>
          <p>sequence of tokens constituting a SMILES</p>
        </def>
      </def-item>
      <def-item>
        <term>Prior</term>
        <def>
          <p>an RNN trained on SMILES from ChEMBL used as a starting point for the Agent</p>
        </def>
      </def-item>
      <def-item>
        <term>Agent</term>
        <def>
          <p>an RNN derived from a Prior, trained using reinforcement learning</p>
        </def>
      </def-item>
      <def-item>
        <term>
          <italic>J</italic>
        </term>
        <def>
          <p>Jaccard index</p>
        </def>
      </def-item>
      <def-item>
        <term>ECFP6</term>
        <def>
          <p>Extended Molecular Fingerprints with diameter 6</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p>support vector machine</p>
        </def>
      </def-item>
      <def-item>
        <term>FCFP4</term>
        <def>
          <p>Extended Molecular Fingerprints with diameter 4 and feature invariants</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Electronic supplementary material</bold>
      </p>
      <p>The online version of this article (doi:10.1186/s13321-017-0235-x) contains supplementary material, which is available to authorized users.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Authors' contributions</title>
    <p>MO contributed concept and implementation. All authors co-designed experiments. All authors contributed to the interpretation of results. MO wrote the manuscript. HC, TB, and OE reviewed and edited the manuscript. All authors read and approved the final manuscript.</p>
    <sec id="FPar1">
      <title>Acknowledgements</title>
      <p id="Par54">The authors thank Thierry Kogej and Christian Tyrchan for general assistance and discussion, and Dominik Peters for his expertise in LATEX.</p>
    </sec>
    <sec id="FPar7">
      <title>Competing interests</title>
      <p id="Par60">The authors declare that they have no competing interests.</p>
    </sec>
    <sec id="FPar2">
      <title>Availability of data and materials</title>
      <p id="Par55">The source code and data supporting the conclusions of this article is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/MarcusOlivecrona/REINVENT">https://github.com/MarcusOlivecrona/REINVENT</ext-link>, doi:10.5281/zenodo.572576.</p>
      <p>
        <list list-type="bullet">
          <list-item>
            <p>Project name: REINVENT</p>
          </list-item>
          <list-item>
            <p>Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/MarcusOlivecrona/REINVENT">https://github.com/MarcusOlivecrona/REINVENT</ext-link>
</p>
          </list-item>
          <list-item>
            <p>Archived version: <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.5281/zenodo.572576">http://doi.org/10.5281/zenodo.572576</ext-link>
</p>
          </list-item>
          <list-item>
            <p>Operating system: Platform independent</p>
          </list-item>
          <list-item>
            <p>Programming language: Python</p>
          </list-item>
          <list-item>
            <p>Other requirements: Python2.7, Tensorflow, RDKit, Scikit-learn</p>
          </list-item>
          <list-item>
            <p>License: MIT.</p>
          </list-item>
        </list>
      </p>
    </sec>
    <sec id="FPar4">
      <title>Consent for publication</title>
      <p id="Par57">Not applicable.</p>
    </sec>
    <sec id="FPar3">
      <title>Ethics approval and consent to participate</title>
      <p id="Par56">Not applicable.</p>
    </sec>
    <sec id="FPar6">
      <title>Funding</title>
      <p id="Par59">MO, HC, and OE are employed by AstraZeneca. TB has received funding from the European Union’s Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement No 676434, “Big Data in Chemistry” (“BIGCHEM”, <ext-link ext-link-type="uri" xlink:href="http://bigchem.eu">http://bigchem.eu</ext-link>). The article reflects only the authors’ view and neither the European Commission nor the Research Executive Agency (REA) are responsible for any use that may be made of the information it contains.</p>
    </sec>
    <sec id="FPar8">
      <title>Publisher’s Note</title>
      <p id="Par61">Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </sec>
  </ack>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schneider</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Fechner</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>Computer-based de novo design of drug-like molecules</article-title>
        <source>Nat Rev Drug Discov</source>
        <year>2005</year>
        <volume>4</volume>
        <issue>8</issue>
        <fpage>649</fpage>
        <lpage>663</lpage>
        <pub-id pub-id-type="doi">10.1038/nrd1799</pub-id>
        <pub-id pub-id-type="pmid">16056391</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Böhm</surname>
            <given-names>HJ</given-names>
          </name>
        </person-group>
        <article-title>The computer program ludi: a new method for the de novo design of enzyme inhibitors</article-title>
        <source>J Comput Aided Mol Des</source>
        <year>1992</year>
        <volume>6</volume>
        <issue>1</issue>
        <fpage>61</fpage>
        <lpage>78</lpage>
        <pub-id pub-id-type="doi">10.1007/BF00124387</pub-id>
        <pub-id pub-id-type="pmid">1583540</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gillet</surname>
            <given-names>VJ</given-names>
          </name>
          <name>
            <surname>Newell</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Mata</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Myatt</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Sike</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zsoldos</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>AP</given-names>
          </name>
        </person-group>
        <article-title>Sprout: recent developments in the de novo design of molecules</article-title>
        <source>J Chem Inf Comput Sci</source>
        <year>1994</year>
        <volume>34</volume>
        <issue>1</issue>
        <fpage>207</fpage>
        <lpage>217</lpage>
        <pub-id pub-id-type="doi">10.1021/ci00017a027</pub-id>
        <pub-id pub-id-type="pmid">8144711</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ruddigkeit</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Blum</surname>
            <given-names>LC</given-names>
          </name>
          <name>
            <surname>Reymond</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>Visualization and virtual screening of the chemical universe database gdb-17</article-title>
        <source>J Chem Inf Model</source>
        <year>2013</year>
        <volume>53</volume>
        <issue>1</issue>
        <fpage>56</fpage>
        <lpage>65</lpage>
        <pub-id pub-id-type="doi">10.1021/ci300535x</pub-id>
        <pub-id pub-id-type="pmid">23259841</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hartenfeller</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zettl</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Walter</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rupp</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Reisen</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Proschak</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Weggen</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Stark</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Dogs: reaction-driven de novo design of bioactive compounds</article-title>
        <source>PLOS Comput Biol</source>
        <year>2012</year>
        <volume>8</volume>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1002380</pub-id>
        <pub-id pub-id-type="pmid">22629235</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schneider</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Geppert</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hartenfeller</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Reisen</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Klenner</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Reutlinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hähnke</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Hiss</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Zettl</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Keppner</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Spänkuch</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Reaction-driven de novo design, synthesis and testing of potential type II kinase inhibitors</article-title>
        <source>Future Med Chem</source>
        <year>2011</year>
        <volume>3</volume>
        <issue>4</issue>
        <fpage>415</fpage>
        <lpage>424</lpage>
        <pub-id pub-id-type="doi">10.4155/fmc.11.8</pub-id>
        <pub-id pub-id-type="pmid">21452978</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Besnard</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ruda</surname>
            <given-names>GF</given-names>
          </name>
          <name>
            <surname>Setola</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Abecassis</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Rodriguiz</surname>
            <given-names>RM</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>X-P</given-names>
          </name>
          <name>
            <surname>Norval</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sassano</surname>
            <given-names>MF</given-names>
          </name>
          <name>
            <surname>Shin</surname>
            <given-names>AI</given-names>
          </name>
          <name>
            <surname>Webster</surname>
            <given-names>LA</given-names>
          </name>
          <name>
            <surname>Simeons</surname>
            <given-names>FRC</given-names>
          </name>
          <name>
            <surname>Stojanovski</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Prat</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Seidah</surname>
            <given-names>NG</given-names>
          </name>
          <name>
            <surname>Constam</surname>
            <given-names>DB</given-names>
          </name>
          <name>
            <surname>Bickerton</surname>
            <given-names>GR</given-names>
          </name>
          <name>
            <surname>Read</surname>
            <given-names>KD</given-names>
          </name>
          <name>
            <surname>Wetsel</surname>
            <given-names>WC</given-names>
          </name>
          <name>
            <surname>Gilbert</surname>
            <given-names>IH</given-names>
          </name>
          <name>
            <surname>Roth</surname>
            <given-names>BL</given-names>
          </name>
          <name>
            <surname>Hopkins</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>Automated design of ligands to polypharmacological profiles</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>492</volume>
        <issue>7428</issue>
        <fpage>215</fpage>
        <lpage>220</lpage>
        <pub-id pub-id-type="doi">10.1038/nature11691</pub-id>
        <pub-id pub-id-type="pmid">23235874</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Miyao</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kaneko</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Funatsu</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Inverse qspr/qsar analysis for chemical structure generation (from y to x)</article-title>
        <source>J Chem Inf Model</source>
        <year>2016</year>
        <volume>56</volume>
        <issue>2</issue>
        <fpage>286</fpage>
        <lpage>299</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jcim.5b00628</pub-id>
        <pub-id pub-id-type="pmid">26818135</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Churchwell</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Rintoul</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>S</given-names>
            <suffix>Jr</suffix>
          </name>
          <name>
            <surname>Visco</surname>
            <given-names>DP</given-names>
          </name>
          <name>
            <surname>Kotu</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Larson</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Sillerud</surname>
            <given-names>LO</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Faulon</surname>
            <given-names>J-L</given-names>
          </name>
        </person-group>
        <article-title>The signature molecular descriptor: 3. Inverse-quantitative structure-activity relationship of icam-1 inhibitory peptides</article-title>
        <source>J Mol Graph Model</source>
        <year>2004</year>
        <volume>22</volume>
        <issue>4</issue>
        <fpage>263</fpage>
        <lpage>273</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmgm.2003.10.002</pub-id>
        <pub-id pub-id-type="pmid">15177078</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wong</surname>
            <given-names>WW</given-names>
          </name>
          <name>
            <surname>Burkowski</surname>
            <given-names>FJ</given-names>
          </name>
        </person-group>
        <article-title>A constructive approach for discovering new drug leads: using a kernel methodology for the inverse-qsar problem</article-title>
        <source>J Cheminform</source>
        <year>2009</year>
        <volume>1</volume>
        <fpage>44</fpage>
        <pub-id pub-id-type="doi">10.1186/1758-2946-1-4</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Mikolov T, Karafiát M, Burget L, Cernock‘y J, Khudanpur S (2010) Recurrent neural network based language model. In: Kobayashi T, Hirose K, Nakamura S (eds) 11th annual conference of the international speech communication association (INTERSPEECH 2010), Makuhari, Chiba, Japan. ISCA, 26–30 Sept 2010</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Eck D, Schmidhuber J (2002) A first look at music composition using lstm recurrent neural networks. Technical report, Istituto Dalle Molle Di Studi Sull Intelligenza Artificiale</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Segler MHS, Kogej T, Tyrchan C, Waller MP (2017) Generating focussed molecule libraries for drug discovery with recurrent neural networks. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1701.01329">arXiv:1701.01329</ext-link></mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Gómez-Bombarelli R, Duvenaud DK, Hernáandez-Lobato JM, Aguilera-Iparraguirre J, Hirzel TD, Adams RP, Aspuru-Guzik A (2016) Automatic chemical design using a data-driven continuous representation of molecules. CoRR. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1610.02415">arXiv:1610.02415</ext-link></mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Yu L, Zhang W, Wang J, Yu Y (2016) Seqgan: sequence generative adversarial nets with policy gradient. CoRR. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1609.05473">arXiv:1609.05473</ext-link></mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Sutton</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Barton</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <source>Reinforcement learning: an introduction</source>
        <year>1998</year>
        <edition>1</edition>
        <publisher-loc>Cambridge</publisher-loc>
        <publisher-name>MIT Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Jaques N, Gu S, Turner RE, Eck D (2016) Tuning recurrent neural networks with reinforcement learning. CoRR. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1611.02796">arXiv:1611.02796</ext-link></mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leo</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hansch</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Elkins</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Partition coefficients and their uses</article-title>
        <source>Chem Rev</source>
        <year>1971</year>
        <volume>71</volume>
        <issue>6</issue>
        <fpage>525</fpage>
        <lpage>616</lpage>
        <pub-id pub-id-type="doi">10.1021/cr60274a001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bickerton</surname>
            <given-names>GR</given-names>
          </name>
          <name>
            <surname>Paolini</surname>
            <given-names>GV</given-names>
          </name>
          <name>
            <surname>Besnard</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Muresan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Hopkins</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>Quantifying the chemical beauty of drugs</article-title>
        <source>Nat Chem</source>
        <year>2012</year>
        <volume>4</volume>
        <fpage>90</fpage>
        <lpage>98</lpage>
        <pub-id pub-id-type="doi">10.1038/nchem.1243</pub-id>
        <pub-id pub-id-type="pmid">22270643</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gaulton</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bellis</surname>
            <given-names>LJ</given-names>
          </name>
          <name>
            <surname>Bento</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Chambers</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Davies</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hersey</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Light</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>McGlinchey</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Michalovich</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Al-Lazikani</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Overington</surname>
            <given-names>JP</given-names>
          </name>
        </person-group>
        <article-title>Chembl: a large-scale bioactivity database for drug discovery</article-title>
        <source>Nucleic Acids Res</source>
        <year>2012</year>
        <volume>40</volume>
        <fpage>1100</fpage>
        <lpage>1107</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkr777</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Goodfellow IJ, Mirza M, Xiao D, Courville A, Bengio Y (2013) An empirical investigation of catastrophic forgetting in gradient-based neural networks. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1312.6211">arXiv:1312.6211</ext-link></mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Goodfellow</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Courville</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <source>Deep learning</source>
        <year>2016</year>
        <edition>1</edition>
        <publisher-loc>Cambridge</publisher-loc>
        <publisher-name>MIT Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>1780</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Chung J, Gulcehre C, Cho K, Bengio Y (2014) Empirical evaluation of gated recurrent neural networks on sequence modeling. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.3555">arXiv:1412.3555</ext-link></mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">SMILES. <ext-link ext-link-type="uri" xlink:href="http://www.daylight.com/dayhtml/doc/theory/theory.smiles.html">http://www.daylight.com/dayhtml/doc/theory/theory.smiles.html</ext-link>. Accessed 7 Apr 2017</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Weininger</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Weininger</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Weininger</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>Smiles. 2. Algorithm for generation of unique smiles notation</article-title>
        <source>J Chem Inf Comput Sci</source>
        <year>1989</year>
        <volume>29</volume>
        <issue>2</issue>
        <fpage>97</fpage>
        <lpage>101</lpage>
        <pub-id pub-id-type="doi">10.1021/ci00062a008</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">RDKit: open source cheminformatics. Version: 2016-09-3. <ext-link ext-link-type="uri" xlink:href="http://www.rdkit.org/">http://www.rdkit.org/</ext-link></mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J: Adam (2014) A method for stochastic optimization. CoRR. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">arXiv:1412.6980</ext-link></mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Tensorflow. Version: 1.0.1. <ext-link ext-link-type="uri" xlink:href="http://www.tensorflow.org">http://www.tensorflow.org</ext-link></mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Williams</surname>
            <given-names>RJ</given-names>
          </name>
        </person-group>
        <article-title>Simple statistical gradient-following algorithms for connectionist reinforcement learning</article-title>
        <source>Mach Learn</source>
        <year>1992</year>
        <volume>8</volume>
        <issue>3</issue>
        <fpage>229</fpage>
        <lpage>256</lpage>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, Bengio Y (2014) Generative adversarial nets. In: Ghahramani Z, Welling M, Cortes C, Lawrence ND, Weinberger KQ (eds) Advances in neural information processing systems 27 (NIPS 2014), Montreal, Quebec, Canada. NIPS foundation, 8–13 Dec 2014</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Lima Guimaraes G, Sanchez-Lengeling B, Cunha Farias PL, Aspuru-Guzik A (2017) Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1705.10843">arXiv:1705.10843</ext-link></mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Jeliazkova</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Chupakin</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Golib-Dzib</surname>
            <given-names>J-F</given-names>
          </name>
          <name>
            <surname>Engkvist</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Carlsson</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wegner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ceulemans</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Georgiev</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Jeliazkov</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Kochev</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Ashby</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Excape-db: an integrated large scale dataset facilitating big data analysis in chemogenomics</article-title>
        <source>J Cheminform</source>
        <year>2017</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>17</fpage>
        <pub-id pub-id-type="doi">10.1186/s13321-017-0203-5</pub-id>
        <pub-id pub-id-type="pmid">28316655</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sheridan</surname>
            <given-names>RP</given-names>
          </name>
        </person-group>
        <article-title>Time-split cross-validation as a method for estimating the goodness of prospective prediction</article-title>
        <source>J Chem Inf Model</source>
        <year>2013</year>
        <volume>53</volume>
        <issue>4</issue>
        <fpage>783</fpage>
        <lpage>790</lpage>
        <pub-id pub-id-type="doi">10.1021/ci400084k</pub-id>
        <pub-id pub-id-type="pmid">23521722</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Unterthiner T, Mayr A, Steijaert M, Wegner JK, Ceulemans H, Hochreiter S (2014) Deep learning as an opportunity in virtual screening. In: Deep learning and representation learning workshop. NIPS, pp 1058–1066</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mayr</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Klambauer</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Unterthiner</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Deeptox: toxicity prediction using deep learning</article-title>
        <source>Front Environ Sci</source>
        <year>2016</year>
        <volume>3</volume>
        <fpage>80</fpage>
        <pub-id pub-id-type="doi">10.3389/fenvs.2015.00080</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jaccard</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Étude comparative de la distribution florale dans une portion des Alpes et des Jura</article-title>
        <source>Bull Soc Vaud Sci Nat</source>
        <year>1901</year>
        <volume>37</volume>
        <fpage>547</fpage>
        <lpage>579</lpage>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rogers</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hahn</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Extended-connectivity fingerprints</article-title>
        <source>J Chem Inf Model</source>
        <year>2010</year>
        <volume>50</volume>
        <issue>5</issue>
        <fpage>742</fpage>
        <lpage>754</lpage>
        <pub-id pub-id-type="doi">10.1021/ci100050t</pub-id>
        <pub-id pub-id-type="pmid">20426451</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Butina</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Unsupervised data base clustering based on daylight’s fingerprint and tanimoto similarity: a fast and automated way to cluster small and large data sets</article-title>
        <source>J Chem Inf Comput Sci</source>
        <year>1999</year>
        <volume>39</volume>
        <issue>4</issue>
        <fpage>747</fpage>
        <lpage>750</lpage>
        <pub-id pub-id-type="doi">10.1021/ci9803381</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Michel</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Grisel</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Blondel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Prettenhofer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Dubourg</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Vanderplas</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Passos</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cournapeau</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Brucher</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Perrot</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Duchesnay</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Scikit-learn: machine learning in Python</article-title>
        <source>J Mach Learn Res</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Silver</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Maddison</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Guez</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sifre</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Van Den Driessche</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Schrittwieser</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Antonoglou</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Panneershelvam</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Lanctot</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mastering the game of go with deep neural networks and tree search</article-title>
        <source>Nature</source>
        <year>2016</year>
        <volume>529</volume>
        <issue>7587</issue>
        <fpage>484</fpage>
        <lpage>489</lpage>
        <pub-id pub-id-type="doi">10.1038/nature16961</pub-id>
        <pub-id pub-id-type="pmid">26819042</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reutlinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Koch</surname>
            <given-names>CP</given-names>
          </name>
          <name>
            <surname>Reker</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Todoroff</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Rodrigues</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Chemically advanced template search (CATS) for scaffold-hopping and prospective target prediction for ’orphan’ molecules</article-title>
        <source>Mol Inform</source>
        <year>2013</year>
        <volume>32</volume>
        <issue>2</issue>
        <fpage>133</fpage>
        <lpage>138</lpage>
        <pub-id pub-id-type="doi">10.1002/minf.201200141</pub-id>
        <pub-id pub-id-type="pmid">23956801</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Senger</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Using Tversky similarity searches for core hopping: finding the needles in the haystack</article-title>
        <source>J Chem Inf Model</source>
        <year>2009</year>
        <volume>49</volume>
        <issue>6</issue>
        <fpage>1514</fpage>
        <lpage>1524</lpage>
        <pub-id pub-id-type="doi">10.1021/ci900092y</pub-id>
        <pub-id pub-id-type="pmid">19453147</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Zaremba W, Sutskever I, Vinyals O (2014) Recurrent neural network regularization. CoRR. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.2329">arXiv:1409.2329</ext-link></mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Wan L, Zeiler M, Zhang S, LeCun Y, Fergus R (2013) Regularization of neural networks using dropconnect. In: Proceedings of the 30th international conference on international conference on machine learning, Vol 28. ICML’13, pp 1058–1066</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ducharme</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Vincent</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Janvin</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>A neural probabilistic language model</article-title>
        <source>J Mach Learn Res</source>
        <year>2003</year>
        <volume>3</volume>
        <fpage>1137</fpage>
        <lpage>1155</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
