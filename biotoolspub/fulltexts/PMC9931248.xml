<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLOS Digit Health</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLOS Digit Health</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLOS Digital Health</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2767-3170</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9931248</article-id>
    <article-id pub-id-type="publisher-id">PDIG-D-22-00181</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pdig.0000174</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Anatomy</subject>
          <subj-group>
            <subject>Cardiovascular Anatomy</subject>
            <subj-group>
              <subject>Blood Vessels</subject>
              <subj-group>
                <subject>Arteries</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Anatomy</subject>
          <subj-group>
            <subject>Cardiovascular Anatomy</subject>
            <subj-group>
              <subject>Blood Vessels</subject>
              <subj-group>
                <subject>Arteries</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Anatomy</subject>
          <subj-group>
            <subject>Cardiovascular Anatomy</subject>
            <subj-group>
              <subject>Blood Vessels</subject>
              <subj-group>
                <subject>Veins</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Anatomy</subject>
          <subj-group>
            <subject>Cardiovascular Anatomy</subject>
            <subj-group>
              <subject>Blood Vessels</subject>
              <subj-group>
                <subject>Veins</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Deep Learning</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Medical Conditions</subject>
          <subj-group>
            <subject>Cardiovascular Diseases</subject>
            <subj-group>
              <subject>Cardiovascular Disease Risk</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Cardiology</subject>
          <subj-group>
            <subject>Cardiovascular Medicine</subject>
            <subj-group>
              <subject>Cardiovascular Diseases</subject>
              <subj-group>
                <subject>Cardiovascular Disease Risk</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Preprocessing</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Preprocessing</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Mathematical and Statistical Techniques</subject>
          <subj-group>
            <subject>Statistical Methods</subject>
            <subj-group>
              <subject>Forecasting</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Statistics</subject>
            <subj-group>
              <subject>Statistical Methods</subject>
              <subj-group>
                <subject>Forecasting</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Physics</subject>
          <subj-group>
            <subject>Classical Mechanics</subject>
            <subj-group>
              <subject>Deformation</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Physics</subject>
          <subj-group>
            <subject>Classical Mechanics</subject>
            <subj-group>
              <subject>Damage Mechanics</subject>
              <subj-group>
                <subject>Deformation</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Cardiology</subject>
          <subj-group>
            <subject>Cardiovascular Medicine</subject>
            <subj-group>
              <subject>Cardiovascular Imaging</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Diagnostic Medicine</subject>
          <subj-group>
            <subject>Diagnostic Radiology</subject>
            <subj-group>
              <subject>Cardiovascular Imaging</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Imaging Techniques</subject>
          <subj-group>
            <subject>Diagnostic Radiology</subject>
            <subj-group>
              <subject>Cardiovascular Imaging</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Radiology and Imaging</subject>
          <subj-group>
            <subject>Diagnostic Radiology</subject>
            <subj-group>
              <subject>Cardiovascular Imaging</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Automated grading system of retinal arterio-venous crossing patterns: A deep learning approach replicating ophthalmologistâ€™s diagnostic process of arteriolosclerosis</article-title>
      <alt-title alt-title-type="running-head">Automated grading system of retinal arterio-venous crossing patterns</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Liangzhi</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing â€“ original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing â€“ review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Verma</surname>
          <given-names>Manisha</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing â€“ original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing â€“ review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Bowen</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nakashima</surname>
          <given-names>Yuta</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing â€“ review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nagahara</surname>
          <given-names>Hajime</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7492-6303</contrib-id>
        <name>
          <surname>Kawasaki</surname>
          <given-names>Ryo</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing â€“ review &amp; editing</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Institute for Datability Science (IDS), Osaka University, Osaka, Japan</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Graduate School of Medicine, Osaka University, Osaka, Japan</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Yoon</surname>
          <given-names>Dukyong</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Yonsei University College of Medicine, KOREA, REPUBLIC OF</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>ryo.kawasaki@ophthal.med.osaka-u.ac.jp</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <volume>2</volume>
    <issue>1</issue>
    <elocation-id>e0000174</elocation-id>
    <history>
      <date date-type="received">
        <day>20</day>
        <month>6</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>29</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Â© 2023 Li et al</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Li et al</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pdig.0000174.pdf"/>
    <abstract>
      <p>The morphological feature of retinal arterio-venous crossing patterns is a valuable source of cardiovascular risk stratification as it directly captures vascular health. Although Scheieâ€™s classification, which was proposed in 1953, has been used to grade the severity of arteriolosclerosis as diagnostic criteria, it is not widely used in clinical settings as mastering this grading is challenging as it requires vast experience. In this paper, we propose a deep learning approach to replicate a diagnostic process of ophthalmologists while providing a checkpoint to secure explainability to understand the grading process. The proposed pipeline is three-fold to replicate a diagnostic process of ophthalmologists. First, we adopt segmentation and classification models to automatically obtain vessels in a retinal image with the corresponding artery/vein labels and find candidate arterio-venous crossing points. Second, we use a classification model to validate the true crossing point. At last, the grade of severity for the vessel crossings is classified. To better address the problem of label ambiguity and imbalanced label distribution, we propose a new model, named multi-diagnosis team network (MDTNet), in which the sub-models with different structures or different loss functions provide different decisions. MDTNet unifies these diverse theories to give the final decision with high accuracy. Our automated grading pipeline was able to validate crossing points with precision and recall of 96.3% and 96.3%, respectively. Among correctly detected crossing points, the kappa value for the agreement between the grading by a retina specialist and the estimated score was 0.85, with an accuracy of 0.92. The numerical results demonstrate that our method can achieve a good performance in both arterio-venous crossing validation and severity grading tasks following the diagnostic process of ophthalmologists. By the proposed models, we could build a pipeline reproducing ophthalmologistsâ€™ diagnostic process without requiring subjective feature extractions. The code is available (<ext-link xlink:href="https://github.com/conscienceli/MDTNet" ext-link-type="uri">https://github.com/conscienceli/MDTNet</ext-link>).</p>
    </abstract>
    <abstract abstract-type="summary">
      <title>Author Summary</title>
      <p>Assessment of arterio-venous crossing points in retinal images provides rich cues for quick screening of arteriosclerosis and even for classifying them into different severity grades. Considering the ever-increasing demand for ophthalmologic examination, computer-aided diagnosis (CAD) is extremely helpful for quick screening. However, retinal image analysis for CAD is a challenging task due to the high complexity of the vessel system and huge visual differences among retinal images. To address the aforementioned problems, we propose a whole pipeline for an automatic method for severity grading of artery hardening. Our method can find and validate possible arterio-venous crossing points, for which the severity grade is predicted. We also design a new model, MDTNet, which uses the focal loss to address the problem of data ambiguity and unbalance. Therefore, we believe that this research contributes to the advancement of research in machine learning on retinal images.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000646</institution-id>
            <institution>Japan Society for the Promotion of Science London</institution>
          </institution-wrap>
        </funding-source>
        <award-id>21K17764</award-id>
        <principal-award-recipient>
          <name>
            <surname>Li</surname>
            <given-names>Liangzhi</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This work was supported by Council for Science, Technology and Innovation (CSTI) cross-ministerial Strategic Innovation Promotion Program (SIP) "Innovative AI Hospital System" (Funding Agency: National Institute of Biomedical Innovation, Health and Nutrition (NIBIOHN)) (LL, MV, HN, RK). This work was also supported by JSPS KAKENHI Grant Number 19K10662 (RK), 20K23343 (LL), and 21K17764 (LL). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="6"/>
      <table-count count="3"/>
      <page-count count="12"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>All codes are available on github (<ext-link xlink:href="https://github.com/conscienceli/MDTNet" ext-link-type="uri">https://github.com/conscienceli/MDTNet</ext-link>). Image data access is possible but limited due to the range of patientsâ€™ consent. Please contact the Ethics Review Board of Osaka University Hospital (<ext-link xlink:href="https://www.med.osaka-u.ac.jp/pub/hp-crc/person_concerned/index.html" ext-link-type="uri">https://www.med.osaka-u.ac.jp/pub/hp-crc/person_concerned/index.html</ext-link>) for detailed information.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>All codes are available on github (<ext-link xlink:href="https://github.com/conscienceli/MDTNet" ext-link-type="uri">https://github.com/conscienceli/MDTNet</ext-link>). Image data access is possible but limited due to the range of patientsâ€™ consent. Please contact the Ethics Review Board of Osaka University Hospital (<ext-link xlink:href="https://www.med.osaka-u.ac.jp/pub/hp-crc/person_concerned/index.html" ext-link-type="uri">https://www.med.osaka-u.ac.jp/pub/hp-crc/person_concerned/index.html</ext-link>) for detailed information.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>Retina provides a window to directly visualize vascular structure <italic toggle="yes">in vivo</italic>, and ophthalmologic examination has been regarded as an important routine for detecting not only eye diseases but also ocular manifestations of cardiovascular diseases or their accumulated risks [<xref rid="pdig.0000174.ref001" ref-type="bibr">1</xref>]. Among these detectable retinal vascular signs, arteriolosclerosis is critical yet asymptomatic, of which diagnosis requires detailed retinal observation. It is not widely conducted in the modern medical practice as it depends on mostly subjective qualitative observations, and most importantly, it requires vast experiences.</p>
    <p>Assessment of arterio-venous crossing points in retinal images provides rich cues for screening arteriosclerosis and for evaluating accumulated cardiovascular risks. Typically, arterio-venous crossing points are classified into severity grades [<xref rid="pdig.0000174.ref002" ref-type="bibr">2</xref>]. The assessment is based on some diagnostic criteria, for example, Scheieâ€™s classification [<xref rid="pdig.0000174.ref003" ref-type="bibr">3</xref>], as shown in <xref rid="pdig.0000174.g001" ref-type="fig">Fig 1(b)â€“1(e)</xref>. The grades are described as follows: (i) <italic toggle="yes">none</italic> (no anomaly observed); (ii) <italic toggle="yes">mild</italic> (slight shrink in the caliber at venular edges); (iii) <italic toggle="yes">moderate</italic> (narrowed caliber at a single venular edge); and (iv) <italic toggle="yes">severe</italic> (narrowed caliber at both venular edges).</p>
    <fig position="float" id="pdig.0000174.g001">
      <object-id pub-id-type="doi">10.1371/journal.pdig.0000174.g001</object-id>
      <label>Fig 1</label>
      <caption>
        <title>Typical examples of our prediction targets.</title>
        <p>Images in the first and second rows are raw retinal patches and automatically-generated vessel maps with manually-annotated artery/vein labels, respectively. Red represents arteries while blue represents veins. (a) is false crossing (the vein runs above the artery), while (b)â€“(e) are for <italic toggle="yes">none</italic>, <italic toggle="yes">mild</italic>, <italic toggle="yes">moderate</italic>, and <italic toggle="yes">severe</italic> grades, respectively. Note that even the state-of-the-art segmentation techniques cannot capture caliber narrowing, therefore, the arterioloscleroses are not very obvious in the vessel maps.</p>
      </caption>
      <graphic xlink:href="pdig.0000174.g001" position="float"/>
    </fig>
    <p>However, human graders are subjective and usually with different levels of experience, and there has been a criticism of the low reproducibility of severity grading, which makes grading results from human graders unreliable for clinical practice, screening, and clinical trials [<xref rid="pdig.0000174.ref004" ref-type="bibr">4</xref>]. Also, considering the ever-increasing demand for ophthalmologic examination, computer-aided diagnosis (CAD) is extremely helpful for quick screening. Yet, retinal image analysis for CAD is a challenging task due to the high complexity of the vessel system and huge visual differences among retinal images.</p>
    <p>In fact, most researchers in this area have been focusing on preliminary tasks, such as vessel segmentation [<xref rid="pdig.0000174.ref005" ref-type="bibr">5</xref>â€“<xref rid="pdig.0000174.ref007" ref-type="bibr">7</xref>], artery/vein classification [<xref rid="pdig.0000174.ref008" ref-type="bibr">8</xref>â€“<xref rid="pdig.0000174.ref010" ref-type="bibr">10</xref>], etc. A few works address higher-level tasks [<xref rid="pdig.0000174.ref004" ref-type="bibr">4</xref>, <xref rid="pdig.0000174.ref011" ref-type="bibr">11</xref>], mostly on top of vessel segmentation, such as vessel width measurement, vessel-to-vessel ratio calculation, etc. However, they usually struggle in actual diagnoses: Firstly, vessel segmentation in retinal images <italic toggle="yes">per se</italic> is a challenging task. The vessel maps in <xref rid="pdig.0000174.g001" ref-type="fig">Fig 1(c)â€“1(e)</xref>, which are produced by the state-of-the-art segmentation model [<xref rid="pdig.0000174.ref012" ref-type="bibr">12</xref>], cannot capture such deformation. This may imply that deformation is too minor to be captured by segmentation models, although such kind of segmentation-based approach is a typical solution for automatic severity grading. Secondly, the existing methods detect arterio-venous crossing points by applying some morphological operators to vessel maps [<xref rid="pdig.0000174.ref013" ref-type="bibr">13</xref>]. This approach may not be accurate enough to find crossing points that satisfy diagnostic requirements. For example, we can only use crossing points at which the artery is above the vein for diagnosis, and <xref rid="pdig.0000174.g001" ref-type="fig">Fig 1</xref>(a) is not a diagnostic crossing point since the artery goes below the vein.</p>
    <p>Instead of fully relying on segmentation results, we propose a multi-stage approach, in which segmentation results are used only for finding crossing point candidates, and actual prediction of the severity grade is conducted for an image patch around each crossing point after validating if the crossing point is an actual and informative one. To the best of our knowledge, this is the first work proposing a fully-automatic methodology aiming at grading arteriolosclerosis through the joint detection and analysis of retinal crossings.</p>
    <p>Another issue in our severity grading task, which is very common in medical imaging, is the imbalanced label distribution. Most patients in our dataset have the slightest signs (<italic toggle="yes">none</italic> and <italic toggle="yes">mild</italic>) of arteriolosclerosis while only a few patients suffer from the <italic toggle="yes">severe</italic> grades of artery hardening. Also, the boundaries among different severity labels are not always obvious, making accurate diagnosis challenging.</p>
    <p>Inspired by the concept of the multidisciplinary team [<xref rid="pdig.0000174.ref014" ref-type="bibr">14</xref>], which strives to make a comprehensive assessment of a patient, we propose a multi-diagnosis team network (MDTNet) in this paper to address the imbalanced label distribution and label ambiguity problems at the same time. MDTNet can combine the features from multiple classification models with different structures or different loss functions. Some of the underlying models in MDTNet use the class-balanced focal loss [<xref rid="pdig.0000174.ref015" ref-type="bibr">15</xref>] to handle hard or rare samples, of which the original version requires hyperparameter tuning, while MDTNet can utilize the advantage of the focal loss without tuning its hyperparameters.</p>
    <p>Our main contribution is two-fold: (i) We propose a whole pipeline for an automatic method for severity grading of artery hardening. Our method can find and validate possible arterio-venous crossing points, for which the severity grade is predicted. (ii) We design a new model, MDTNet, which uses the focal loss to address the problem of data ambiguity and unbalance.</p>
  </sec>
  <sec id="sec002">
    <title>Dataset</title>
    <sec id="sec003">
      <title>Ethics statement</title>
      <p>This study was performed in accordance with the World Medical Association Declaration of Helsinki. Patients gave written informed consent to participate and the study protocol was approved by the institutional review board of the Osaka University Hospital.</p>
      <p>We built a vessel crossing point dataset extracted from our retinal image database of the Ohasama study, a cohort to study cardiovascular diseases risk, where we could utilize 1, 440 images in the size of 5, 184 Ã— 3, 456 pixels, which are captured by the CR-2 AF Digital Non-Mydriatic Retinal Camera (Canon, Tokyo) between 2013 and 2017 as JPEG files. This database includes the medical data of 684 people, which are with an average age of 64.5 (standard deviation: 6.1). The ratio between female and male is 65.2% : 34.8% and 47.6% of all participants have hypertension. Details of the study profile were published elsewhere [<xref rid="pdig.0000174.ref016" ref-type="bibr">16</xref>].</p>
      <p>To find crossing points in these images (<xref rid="pdig.0000174.g002" ref-type="fig">Fig 2(a)â€“2(d)</xref>), we used a segmentation model ([<xref rid="pdig.0000174.ref012" ref-type="bibr">12</xref>]) to get vessel maps. We then classified each pixel on extracted vessels into artery/vein using [<xref rid="pdig.0000174.ref017" ref-type="bibr">17</xref>]. We combine the vessel segmentation and classification results to find crossing points because classification results, which are more beneficial for crossing point detection, tend to have more errors while segmented vessel maps are more accurate. Therefore, we refine the classification results based on the vessel maps. A classic approach then finds crossing points in these refined artery/vein maps. Specifically, we find the artery pixels neighbouring vein pixels and check whether it is a crossing point or not using the skeletonized vessel map. The points marked in yellow in <xref rid="pdig.0000174.g002" ref-type="fig">Fig 2</xref> are detected crossing point candidates. Note that for cup zones as indicated by a pink circle and dot in <xref rid="pdig.0000174.g002" ref-type="fig">Fig 2</xref>, we exclude candidates because the vessel system in this area is with high complexity and thus segmentation and classification are not reliable. Image patches are of size 150 Ã— 150, centered at the crossing point candidates. Consequently, we detected 4, 240 crossing points and extracted corresponding image patches, centered at these crossing points.</p>
      <fig position="float" id="pdig.0000174.g002">
        <object-id pub-id-type="doi">10.1371/journal.pdig.0000174.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Overall pipeline of our severity grading.</title>
        </caption>
        <graphic xlink:href="pdig.0000174.g002" position="float"/>
      </fig>
      <p>Each image patch was carefully reviewed by a highly experienced ophthalmologist. Due to the errors in vessel segmentation and artery/vein classification, the detected crossing points may not be actual or informative. Therefore, the specialist first annotated each image patch with a label on its validity, <italic toggle="yes">i.e</italic>., if the image patch contains an actual and informative crossing point (<italic toggle="yes">true</italic>) or not (<italic toggle="yes">false</italic>). The numbers of true and false crossing points are 2, 507 and 1, 733, respectively. For each true crossing point, the specialist gave its severity label in <italic toggle="yes">C</italic> = {<italic toggle="yes">none</italic>, <italic toggle="yes">mild</italic>, <italic toggle="yes">moderate</italic>, <italic toggle="yes">severe</italic>}. The numbers of image patches with respective labels are 1, 177, 816, 457, and 57. In both tasks, the datasets will be divided into training, validation, and test sets following a ratio of 8:1:1. As an examinee may have multiple retinal images, it is important to strictly put them into one same subset to prevent the training data contamination.</p>
    </sec>
  </sec>
  <sec id="sec004">
    <title>Severity grading pipeline</title>
    <p>Our method forms a pipeline with three main modules, <italic toggle="yes">i.e</italic>., preprocessing, patch validation, and severity grade prediction. The whole pipeline is shown in <xref rid="pdig.0000174.g002" ref-type="fig">Fig 2</xref>.</p>
    <sec id="sec005">
      <title>Preprocessing</title>
      <p>Steps (a)â€“(d) in the figure are preprocessing, in which the same processes as our dataset construction are applied to get image patches of 150 Ã— 150 pixels with crossing point candidates.</p>
    </sec>
    <sec id="sec006">
      <title>Crossing point validation</title>
      <p>Both crossing point validation and severity grading are classification problems, whereas validation is easier because the label distribution is more balanced and the differences between real and false crossing points are more obvious. We find that commonly used classification models, such as [<xref rid="pdig.0000174.ref018" ref-type="bibr">18</xref>â€“<xref rid="pdig.0000174.ref020" ref-type="bibr">20</xref>], work well for our validation task (refer to <italic toggle="yes">Experiments and Results</italic> Section).</p>
    </sec>
    <sec id="sec007">
      <title>Severity grade prediction</title>
      <p>The severity grade prediction task is much more challenging: Firstly, the label distribution is highly biased. For example, samples with the <italic toggle="yes">none</italic> label account for 68% of the total samples, while ones with the <italic toggle="yes">severe</italic> label only take up 3%. Secondly, the difference among samples with different labels may not be clear enough. Even medical doctors may make diverse decisions on a single image patch.</p>
      <p>For such classification tasks with ambiguous or imbalanced classes, the focal loss [<xref rid="pdig.0000174.ref015" ref-type="bibr">15</xref>] has been used, which makes a model more aware of hard samples than easy ones. The focal loss introduces a hyperparameter <italic toggle="yes">Î³</italic>, on which a modelâ€™s performance depends significantly. Tuning this hyperparameter is extremely important yet computationally expensive [<xref rid="pdig.0000174.ref021" ref-type="bibr">21</xref>]. A greater <italic toggle="yes">Î³</italic> may make the model focus too much on hard samples, spoiling the accuracy of other samples, while a smaller <italic toggle="yes">Î³</italic> may decrease its ability to classify hard samples.</p>
      <p>We propose a multi-diagnosis team network (MDTNet) to address the aforementioned problems in severity grade prediction. As shown in <xref rid="pdig.0000174.g003" ref-type="fig">Fig 3</xref>, MDTNet consists of three modules, <italic toggle="yes">i.e</italic>., a base module, a focal module, and a fusion module.</p>
      <fig position="float" id="pdig.0000174.g003">
        <object-id pub-id-type="doi">10.1371/journal.pdig.0000174.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>MDTNet for severity grade prediction.</title>
        </caption>
        <graphic xlink:href="pdig.0000174.g003" position="float"/>
      </fig>
      <p>The base and focal modules have multiple sub-models, and all of them take the same image patch as input. The difference between the sub-models in the base and focal modules is the losses: Ones in the base module adopt the cross entropy (CE) loss while ones in the focal module use the focal loss. These sub-models are trained independently with respective losses. The fusion module concatenates all features (<italic toggle="yes">i.e</italic>., the outputs of the second last layers of the sub-models) into a single vector, which is then fed into two fully-connected layers to make the final prediction.</p>
      <p>The focal loss is originally designed for object detection [<xref rid="pdig.0000174.ref015" ref-type="bibr">15</xref>], defined as
<disp-formula id="pdig.0000174.e001"><alternatives><graphic xlink:href="pdig.0000174.e001.jpg" id="pdig.0000174.e001g" position="anchor"/><mml:math id="M1" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:munder><mml:mo>âˆ‘</mml:mo><mml:mi>l</mml:mi></mml:munder><mml:msub><mml:mi>t</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>Î³</mml:mi></mml:msup><mml:mspace width="2pt"/><mml:mtext>log</mml:mtext><mml:mspace width="2pt"/><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(1)</label></disp-formula>
where <italic toggle="yes">t</italic> is the one-hot representation of label and <italic toggle="yes">y</italic> is the softmax output from a model (<italic toggle="yes">t</italic><sub><italic toggle="yes">l</italic></sub> and <italic toggle="yes">y</italic><sub><italic toggle="yes">l</italic></sub> are the <italic toggle="yes">l</italic>-th entries of <italic toggle="yes">t</italic> and <italic toggle="yes">y</italic>); <italic toggle="yes">Î³</italic> is a hyperparameter to weight hard examples. The focal loss reduces to the CE loss when <italic toggle="yes">Î³</italic> = 0, and a larger <italic toggle="yes">Î³</italic> weights more on hard examples. One possible criticism of the focal loss is its sensitivity to <italic toggle="yes">Î³</italic>. We therefore propose to ensemble sub-models with different <italic toggle="yes">Î³</italic>â€™s. The hypothesis behind this choice is that different <italic toggle="yes">Î³</italic>â€™s may rely on different cues for prediction and aggregating respective features may help in improving the final decision. This is embodied in the focal module. The same idea can also be applied to different network architectures, embodied in the base module. These sub-models thus provide diagnostic features that may complement each other.</p>
      <p>To cope with the imbalanced class distribution, we adopt class weighting [<xref rid="pdig.0000174.ref022" ref-type="bibr">22</xref>, <xref rid="pdig.0000174.ref023" ref-type="bibr">23</xref>]. We multiply weight <italic toggle="yes">Î±</italic><sub><italic toggle="yes">l</italic></sub> = ln <italic toggle="yes">N</italic><sub><italic toggle="yes">l</italic></sub>/ln <italic toggle="yes">N</italic> to each term (<italic toggle="yes">i.e</italic>. different <italic toggle="yes">l</italic>â€™s) in the CE/focal loss, where <italic toggle="yes">N</italic> and <italic toggle="yes">N</italic><sub><italic toggle="yes">l</italic></sub> are the numbers of all samples and of samples with the label corresponding to the <italic toggle="yes">l</italic>-th entry of <italic toggle="yes">t</italic>. We pre-train the sub-models using their own classifiers and losses, and then freeze their weights to train the additional two fully-connected layers for the final decision.</p>
    </sec>
    <sec id="sec008">
      <title>Data augmentation</title>
      <p>We adopt extensive data augmentation. During the training process, the input images have 50% chance of getting each operator in <xref rid="pdig.0000174.g004" ref-type="fig">Fig 4</xref>. Among them, (bâˆ¼h) are used for shape modification, changing the locations and the shapes of the attention areas of the deep learning models; (iâˆ¼k) are to provide variety on imaging quality by blurring or adding random noises; (l) represents sensor characteristics of color (hue and saturation).</p>
      <fig position="float" id="pdig.0000174.g004">
        <object-id pub-id-type="doi">10.1371/journal.pdig.0000174.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Our data augmentation operator pool.</title>
          <p>(a) Raw image, (b) vertical flipping, (c) horizontal flipping, (d) cropping and padding, (e) scaling, (f) translating, (g) rotating, (h) sheering, (i) blurring, (j) additional noise, (k) additional frequency noise, and (l) color modification.</p>
        </caption>
        <graphic xlink:href="pdig.0000174.g004" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec id="sec009">
    <title>Experiments and results</title>
    <sec id="sec010">
      <title>Implementation</title>
      <p>For sub-models in the base module, we used ResNet [<xref rid="pdig.0000174.ref018" ref-type="bibr">18</xref>], Inception [<xref rid="pdig.0000174.ref020" ref-type="bibr">20</xref>], and DenseNet [<xref rid="pdig.0000174.ref019" ref-type="bibr">19</xref>]. In the focal module, DenseNet with <italic toggle="yes">Î³</italic> = 1, 2, or 3 were used. All these models are pre-trained over the ImageNet dataset [<xref rid="pdig.0000174.ref024" ref-type="bibr">24</xref>]. The fully-connected layers in the fusion module are followed by the ReLU nonlinearity. For optimization, Adam [<xref rid="pdig.0000174.ref025" ref-type="bibr">25</xref>] was adopted with a learning rate of 0.0001. Models are trained on the training set, and the weights with the highest performance on the validation set are selected as the best models, which will be evaluated on the test set.</p>
    </sec>
    <sec id="sec011">
      <title>Performance of base models</title>
      <p>We first evaluated the performance of the base moduleâ€™s sub-models for the crossing point validation and severity grade prediction tasks. For comparison, we also give the results of models without pre-training (w/o PT) and without data augmentation (w/o DA), as well as models using only the green channel (GC Only).</p>
      <p>The crossing point validation performances are shown in the left part of <xref rid="pdig.0000174.t001" ref-type="table">Table 1</xref>. We use two metrics, precision and recall, and the time measurement to show the timing performance. We can see that pre-training and data augmentation can improve the overall performance of the crossing point validation. The Inception model with PT and DA achieved the best recall and the second-best precision. Note that PT and DA will not change the running time of the model because they do not modify the network structure.</p>
      <table-wrap position="float" id="pdig.0000174.t001">
        <object-id pub-id-type="doi">10.1371/journal.pdig.0000174.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Performances of base models with ablation.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pdig.0000174.t001" id="pdig.0000174.t001g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="2" colspan="1">Models</th>
                <th align="center" colspan="3" rowspan="1">Cross. Point Val.</th>
                <th align="center" colspan="3" rowspan="1">Severity Grade Pred.</th>
              </tr>
              <tr>
                <th align="center" rowspan="1" colspan="1">Pre.</th>
                <th align="center" rowspan="1" colspan="1">Rec.</th>
                <th align="center" rowspan="1" colspan="1"><italic toggle="yes">t</italic> (ms)</th>
                <th align="center" rowspan="1" colspan="1">Acc.</th>
                <th align="center" rowspan="1" colspan="1">Kappa</th>
                <th align="center" rowspan="1" colspan="1"><italic toggle="yes">t</italic> (ms)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">ResNet-50</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9427</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9526</td>
                <td align="char" char="." rowspan="1" colspan="1">0.274</td>
                <td align="char" char="." rowspan="1" colspan="1">0.8063</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6629</td>
                <td align="char" char="." rowspan="1" colspan="1">0.278</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">â€ƒâ€”w/o PT</td>
                <td align="char" char="." rowspan="1" colspan="1">0.8646</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6975</td>
                <td align="char" char="." rowspan="1" colspan="1">0.274</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5445</td>
                <td align="char" char="." rowspan="1" colspan="1">0.0177</td>
                <td align="char" char="." rowspan="1" colspan="1">0.278</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">â€ƒâ€”w/o DA</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9531</td>
                <td align="char" char="." rowspan="1" colspan="1">0.8551</td>
                <td align="char" char="." rowspan="1" colspan="1">0.274</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5340</td>
                <td align="char" char="." rowspan="1" colspan="1">0.0036</td>
                <td align="char" char="." rowspan="1" colspan="1">0.278</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">â€ƒâ€”GC Only</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9583</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9154</td>
                <td align="char" char="." rowspan="1" colspan="1">0.273</td>
                <td align="char" char="." rowspan="1" colspan="1">0.7277</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5288</td>
                <td align="char" char="." rowspan="1" colspan="1">0.273</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Inception v3</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9635</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.9635</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">0.218</td>
                <td align="char" char="." rowspan="1" colspan="1">0.8534</td>
                <td align="char" char="." rowspan="1" colspan="1">0.7432</td>
                <td align="char" char="." rowspan="1" colspan="1">0.222</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">â€ƒâ€”w/o PT</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9010</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6865</td>
                <td align="char" char="." rowspan="1" colspan="1">0.218</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5183</td>
                <td align="char" char="." rowspan="1" colspan="1">0.0313</td>
                <td align="char" char="." rowspan="1" colspan="1">0.222</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">â€ƒâ€”w/o DA</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9323</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9179</td>
                <td align="char" char="." rowspan="1" colspan="1">0.218</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5393</td>
                <td align="char" char="." rowspan="1" colspan="1">0.0000</td>
                <td align="char" char="." rowspan="1" colspan="1">0.222</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">â€ƒâ€”GC Only</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9167</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9119</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.216</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">0.8115</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6771</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.216</bold>
                </td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">DenseNet-121</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9479</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9630</td>
                <td align="char" char="." rowspan="1" colspan="1">0.266</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.8795</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.7892</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">0.269</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">â€ƒâ€”w/o PT</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9375</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6742</td>
                <td align="char" char="." rowspan="1" colspan="1">0.266</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5288</td>
                <td align="char" char="." rowspan="1" colspan="1">0.0050</td>
                <td align="char" char="." rowspan="1" colspan="1">0.269</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">â€ƒâ€”w/o DA</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.9740</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">0.8274</td>
                <td align="char" char="." rowspan="1" colspan="1">0.266</td>
                <td align="char" char="." rowspan="1" colspan="1">0.7225</td>
                <td align="char" char="." rowspan="1" colspan="1">0.4865</td>
                <td align="char" char="." rowspan="1" colspan="1">0.269</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">â€ƒâ€”GC Only</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.9740</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">0.9212</td>
                <td align="char" char="." rowspan="1" colspan="1">0.266</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6702</td>
                <td align="char" char="." rowspan="1" colspan="1">0.4406</td>
                <td align="char" char="." rowspan="1" colspan="1">0.267</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>The right part of <xref rid="pdig.0000174.t001" ref-type="table">Table 1</xref> gives the results of the base models on the severity grade prediction task, and <xref rid="pdig.0000174.t002" ref-type="table">Table 2</xref> presents the performance of MDTNet and models using the focal loss. In addition to the classification accuracy, we also adopt Cohenâ€™s kappa, which can measure the agreement between the ground-truth labels and predictions. We can see that, compared with the focal loss models, the DenseNet can achieve higher overall accuracy with the CE loss. However, the combination of different models, different losses, as well as different <italic toggle="yes">Î³</italic> values can boost the performance. MDTNet achieved the highest performance in this experiment when <italic toggle="yes">n</italic> = 3.</p>
      <table-wrap position="float" id="pdig.0000174.t002">
        <object-id pub-id-type="doi">10.1371/journal.pdig.0000174.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Performance of MDTNet models for severity grade prediction.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pdig.0000174.t002" id="pdig.0000174.t002g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="2" colspan="1">Metrics</th>
                <th align="center" colspan="4" rowspan="1">DenseNet-121 (Focal Loss)</th>
                <th align="center" colspan="3" rowspan="1">MDTNet</th>
              </tr>
              <tr>
                <th align="center" rowspan="1" colspan="1"><italic toggle="yes">Î³</italic> = 1</th>
                <th align="center" rowspan="1" colspan="1"><italic toggle="yes">Î³</italic> = 2</th>
                <th align="center" rowspan="1" colspan="1"><italic toggle="yes">Î³</italic> = 5</th>
                <th align="center" rowspan="1" colspan="1"><italic toggle="yes">Î³</italic> = 10</th>
                <th align="center" rowspan="1" colspan="1"><italic toggle="yes">n</italic> = 0</th>
                <th align="center" rowspan="1" colspan="1"><italic toggle="yes">n</italic> = 1</th>
                <th align="center" rowspan="1" colspan="1"><italic toggle="yes">n</italic> = 3</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="1" colspan="1">Acc.</td>
                <td align="char" char="." rowspan="1" colspan="1">0.8639</td>
                <td align="char" char="." rowspan="1" colspan="1">0.7434</td>
                <td align="char" char="." rowspan="1" colspan="1">0.8639</td>
                <td align="char" char="." rowspan="1" colspan="1">0.7958</td>
                <td align="char" char="." rowspan="1" colspan="1">0.8953</td>
                <td align="char" char="." rowspan="1" colspan="1">0.9110</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.9162</bold>
                </td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">Kappa.</td>
                <td align="char" char="." rowspan="1" colspan="1">0.7642</td>
                <td align="char" char="." rowspan="1" colspan="1">0.5685</td>
                <td align="char" char="." rowspan="1" colspan="1">0.7641</td>
                <td align="char" char="." rowspan="1" colspan="1">0.6508</td>
                <td align="char" char="." rowspan="1" colspan="1">0.8183</td>
                <td align="char" char="." rowspan="1" colspan="1">0.8453</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.8542</bold>
                </td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1"><italic toggle="yes">t</italic> (ms)</td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.268</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.268</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.268</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.268</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">0.767</td>
                <td align="char" char="." rowspan="1" colspan="1">1.047</td>
                <td align="char" char="." rowspan="1" colspan="1">1.571</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>To better analyze the severity grade prediction performance, we present the confusion matrices in <xref rid="pdig.0000174.g005" ref-type="fig">Fig 5</xref>. It can be seen that, with the increment of the underlying sub-models, MDTNet gains the classification ability. <xref rid="pdig.0000174.g006" ref-type="fig">Fig 6</xref> shows visual explanation of MDTNet by Grad-CAM [<xref rid="pdig.0000174.ref026" ref-type="bibr">26</xref>]. <xref rid="pdig.0000174.g006" ref-type="fig">Fig 6 (a) and 6(b)</xref> show two examples for the crossing point validation. The ground-truth labels are <italic toggle="yes">false</italic> and the predictions were also <italic toggle="yes">false</italic>, <italic toggle="yes">i.e</italic>., these are not effective crossing points as the arteries are under the veins. The model mainly counted the red area in the second row along the vein. The model might find the vein, track it down, and reach the conclusion that it lies above the artery. <xref rid="pdig.0000174.g006" ref-type="fig">Fig 6 (c) and 6(d)</xref> are for the severity grade prediction. The ground-truth labels are respectively <italic toggle="yes">mild</italic> and <italic toggle="yes">moderate</italic> and were both correctly predicted. We can see the artery runs over the vein deforming the vein. Being different from the example in (a) and (b), the model looks at the crossing points and looks for possible shape deformations and their extent.</p>
      <fig position="float" id="pdig.0000174.g005">
        <object-id pub-id-type="doi">10.1371/journal.pdig.0000174.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>Confusion matrices for three different severity grade prediction models.</title>
          <p>The recall is shown in the last row and the precision is shown in the last column. (a) MDTNet without the focal module, (b) MDTNet for <italic toggle="yes">n</italic> = 1, and (c) MDTNet for <italic toggle="yes">n</italic> = 3.</p>
        </caption>
        <graphic xlink:href="pdig.0000174.g005" position="float"/>
      </fig>
      <fig position="float" id="pdig.0000174.g006">
        <object-id pub-id-type="doi">10.1371/journal.pdig.0000174.g006</object-id>
        <label>Fig 6</label>
        <caption>
          <title>Visual explanation of prediction results.</title>
          <p>(a,b) are for the crossing point validation model and (c,d) are from the severity grade prediction model. The first row is the raw input images and the second row is the class-discriminative regions.</p>
        </caption>
        <graphic xlink:href="pdig.0000174.g006" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec012">
    <title>Conclusion</title>
    <p>The paper presents a method to automatically classify the arteriolosclerosis severity from retinal images following ophthalmologistsâ€™ diagnostic process. To improve the accuracy for ambiguous and unbalanced samples, we design the multi-diagnosis team network (MDTNet), which can jointly consider diagnostic cues from multiple sub-models, without tuning the hyperparameter for the focal loss. Experimental results show the superiority of our method, achieving over 91% accuracy. Most importantly, the whole process can be checked to see how the grading was determined as it is designed to be a step-by-step approach replicating ophthalmologistsâ€™ diagnostic process. Therefore, the proposed method can serve as a supporting tool for experienced ophthalmologists to efficiently grade the images in a consistently reproducible manner. A quality checklist [<xref rid="pdig.0000174.ref027" ref-type="bibr">27</xref>] for the proposed deep learning method is shown in <xref rid="pdig.0000174.t003" ref-type="table">Table 3</xref>.</p>
    <table-wrap position="float" id="pdig.0000174.t003">
      <object-id pub-id-type="doi">10.1371/journal.pdig.0000174.t003</object-id>
      <label>Table 3</label>
      <caption>
        <title>The MI-CLAIM checklist.</title>
      </caption>
      <alternatives>
        <graphic xlink:href="pdig.0000174.t003" id="pdig.0000174.t003g" position="float"/>
        <table frame="box" rules="all" border="0">
          <colgroup span="1">
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th align="left" colspan="3" rowspan="1">Before paper submission</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">Study design (Part 1)</td>
              <td align="center" rowspan="1" colspan="1">Page</td>
              <td align="center" rowspan="1" colspan="1">Notes</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">The clinical problem in which the model will be employed is clearly detailed in the paper.</td>
              <td align="center" rowspan="1" colspan="1">2-3</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">The research question is clearly stated.</td>
              <td align="center" rowspan="1" colspan="1">3</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">The characteristics of the cohorts (training and test sets) are detailed in the text.</td>
              <td align="center" rowspan="1" colspan="1">3-4</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">The cohorts (training and test sets) are shown to be representative of real-world clinical settings.</td>
              <td align="center" rowspan="1" colspan="1">3-4</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">The state-of-the-art solution used as a baseline for comparison has been identified and detailed.</td>
              <td align="center" rowspan="1" colspan="1">7-8</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Data and optimization</td>
              <td align="center" rowspan="1" colspan="1">Page</td>
              <td align="center" rowspan="1" colspan="1">Notes</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">The origin of the data is described and the original format is detailed in the paper.</td>
              <td align="center" rowspan="1" colspan="1">3-4</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Transformations of the data before it is applied to the proposed model are described.</td>
              <td align="center" rowspan="1" colspan="1">6-7</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">The independence between training and test sets has been proven in the paper.</td>
              <td align="center" rowspan="1" colspan="1">4</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Details on the models that were evaluated and the code developed to select the best model are provided.</td>
              <td align="center" rowspan="1" colspan="1">7</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Is the input data type structured or unstructured?</td>
              <td align="center" rowspan="1" colspan="1">â˜‘ Structured</td>
              <td align="center" rowspan="1" colspan="1">â–¡ Unstructured</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Model performance (Part 4)</td>
              <td align="center" rowspan="1" colspan="1">Page</td>
              <td align="center" rowspan="1" colspan="1">Notes</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">The primary metric selected to evaluate algorithm performance, including the justification for selection, has been clearly stated.</td>
              <td align="center" rowspan="1" colspan="1">7-8</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">The primary metric selected to evaluate the clinical utility of the model, including the justification for selection, has been clearly stated.</td>
              <td align="center" rowspan="1" colspan="1">7-8</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">The performance comparison between baseline and proposed model is presented with the appropriate statistical significance.</td>
              <td align="center" rowspan="1" colspan="1">7-9</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Model examination (Part 5)</td>
              <td align="center" rowspan="1" colspan="1">Page</td>
              <td align="center" rowspan="1" colspan="1">Notes</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Examination technique</td>
              <td align="center" rowspan="1" colspan="1">7-9</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">A discussion of the relevance of the examination results with respect to model/algorithm performance is presented.</td>
              <td align="center" rowspan="1" colspan="1">7-9</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">A discussion of the feasibility and significance of model interpretability at the case level if examination methods are uninterpretable is presented.</td>
              <td align="center" rowspan="1" colspan="1">8</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">A discussion of the reliability and robustness of the model as the underlying data distribution shifts is included.</td>
              <td align="center" rowspan="1" colspan="1">4,7-9</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" colspan="2" rowspan="1">Reproducibility (Part 6): choose appropriate tier of transparency</td>
              <td align="center" rowspan="1" colspan="1">Notes</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Tier 1: complete sharing of the code</td>
              <td align="center" rowspan="1" colspan="1">â˜‘</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Tier 2: allow a third party to evaluate the code for accuracy/fairness; share the results of this evaluation</td>
              <td align="center" rowspan="1" colspan="1">â–¡</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Tier 3: release of a virtual machine (binary) for running the code on new data without sharing its details</td>
              <td align="center" rowspan="1" colspan="1">â–¡</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Tier 4: no sharing</td>
              <td align="center" rowspan="1" colspan="1">â–¡</td>
              <td align="center" rowspan="1" colspan="1"/>
            </tr>
          </tbody>
        </table>
      </alternatives>
    </table-wrap>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pdig.0000174.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Chatziralli</surname><given-names>IP</given-names></name>, <name><surname>Kanonidou</surname><given-names>ED</given-names></name>, <name><surname>Keryttopoulos</surname><given-names>P</given-names></name>, <name><surname>Dimitriadis</surname><given-names>P</given-names></name>, <name><surname>Papazisis</surname><given-names>LE</given-names></name>. <article-title>The value of fundoscopy in general practice</article-title>. <source>The open ophthalmology journal</source>. <year>2012</year>;<volume>6</volume>:<fpage>4</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.2174/1874364101206010004</pub-id><?supplied-pmid 22435081?><pub-id pub-id-type="pmid">22435081</pub-id></mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Hubbard</surname><given-names>LD</given-names></name>, <name><surname>Brothers</surname><given-names>RJ</given-names></name>, <name><surname>King</surname><given-names>WN</given-names></name>, <name><surname>Clegg</surname><given-names>LX</given-names></name>, <name><surname>Klein</surname><given-names>R</given-names></name>, <name><surname>Cooper</surname><given-names>LS</given-names></name>, <etal>et al</etal>. <article-title>Methods for evaluation of retinal microvascular abnormalities associated with hypertension/sclerosis in the Atherosclerosis Risk in Communities Study</article-title>. <source>Ophthalmology</source>. <year>1999</year>;<volume>106</volume>(<issue>12</issue>):<fpage>2269</fpage>â€“<lpage>2280</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/S0161-6420(99)90525-0</pub-id><?supplied-pmid 10599656?><pub-id pub-id-type="pmid">10599656</pub-id></mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Walsh</surname><given-names>JB</given-names></name>. <article-title>Hypertensive Retinopathy: Description, Classification, and Prognosis</article-title>. <source>Ophthalmology</source>. <year>1982</year>;<volume>89</volume>(<issue>10</issue>):<fpage>1127</fpage>â€“<lpage>1131</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/S0161-6420(82)34664-3</pub-id><?supplied-pmid 7155523?><pub-id pub-id-type="pmid">7155523</pub-id></mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Nguyen</surname><given-names>UTV</given-names></name>, <name><surname>Bhuiyan</surname><given-names>A</given-names></name>, <name><surname>Park</surname><given-names>LAF</given-names></name>, <name><surname>Kawasaki</surname><given-names>R</given-names></name>, <name><surname>Wong</surname><given-names>TY</given-names></name>, <name><surname>Wang</surname><given-names>JJ</given-names></name>, <etal>et al</etal>. <article-title>An Automated Method for Retinal Arteriovenous Nicking Quantification From Color Fundus Images</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>. <year>2013</year>;<volume>60</volume>(<issue>11</issue>):<fpage>3194</fpage>â€“<lpage>3203</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TBME.2013.2271035</pub-id><?supplied-pmid 23807422?><pub-id pub-id-type="pmid">23807422</pub-id></mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Roychowdhury</surname><given-names>S</given-names></name>, <name><surname>Koozekanani</surname><given-names>DD</given-names></name>, <name><surname>Parhi</surname><given-names>KK</given-names></name>. <article-title>Iterative Vessel Segmentation of Fundus Images</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>. <year>2015</year>;<volume>62</volume>(<issue>7</issue>):<fpage>1738</fpage>â€“<lpage>1749</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TBME.2015.2403295</pub-id><?supplied-pmid 25700436?><pub-id pub-id-type="pmid">25700436</pub-id></mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>JU</given-names></name>, <name><surname>Kim</surname><given-names>HG</given-names></name>, <name><surname>Ro</surname><given-names>YM</given-names></name>. <article-title>Iterative deep convolutional encoder-decoder network for medical image segmentation</article-title>. In: <source>IEEE Engineering in Medicine and Biology Society (EMBC)</source>; <year>2017</year>. p. <fpage>685</fpage>â€“<lpage>688</lpage>.</mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Yan</surname><given-names>Z</given-names></name>, <name><surname>Yang</surname><given-names>X</given-names></name>, <name><surname>Cheng</surname><given-names>K</given-names></name>. <article-title>Joint Segment-Level and Pixel-Wise Losses for Deep Learning Based Retinal Vessel Segmentation</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>. <year>2018</year>;<volume>65</volume>(<issue>9</issue>):<fpage>1912</fpage>â€“<lpage>1923</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TBME.2018.2828137</pub-id><?supplied-pmid 29993396?><pub-id pub-id-type="pmid">29993396</pub-id></mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Huang</surname><given-names>F</given-names></name>, <name><surname>Dashtbozorg</surname><given-names>B</given-names></name>, <name><surname>Tan</surname><given-names>T</given-names></name>, <name><surname>ter Haar Romeny</surname><given-names>BM</given-names></name>. <article-title>Retinal artery/vein classification using genetic-search feature selection</article-title>. <source>Computer Methods and Programs in Biomedicine</source>. <year>2018</year>;<volume>161</volume>:<fpage>197</fpage>â€“<lpage>207</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cmpb.2018.04.016</pub-id><?supplied-pmid 29852962?><pub-id pub-id-type="pmid">29852962</pub-id></mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Meyer</surname><given-names>MI</given-names></name>, <name><surname>Galdran</surname><given-names>A</given-names></name>, <name><surname>Costa</surname><given-names>P</given-names></name>, <name><surname>MendonÃ§a</surname><given-names>AM</given-names></name>, <name><surname>Campilho</surname><given-names>A</given-names></name>. <article-title>Deep Convolutional Artery/Vein Classification of Retinal Vessels</article-title>. In: <source>Image Analysis and Recognition</source>; <year>2018</year>. p. <fpage>622</fpage>â€“<lpage>630</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/978-3-319-93000-8_71</pub-id></mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Costa</surname><given-names>P</given-names></name>, <name><surname>Galdran</surname><given-names>A</given-names></name>, <name><surname>Meyer</surname><given-names>MI</given-names></name>, <name><surname>Niemeijer</surname><given-names>M</given-names></name>, <name><surname>AbrÃ moff</surname><given-names>M</given-names></name>, <name><surname>MendonÃ§a</surname><given-names>AM</given-names></name>, <etal>et al</etal>. <article-title>End-to-End Adversarial Retinal Image Synthesis</article-title>. <source>IEEE Transactions on Medical Imaging</source>. <year>2018</year>;<volume>37</volume>(<issue>3</issue>):<fpage>781</fpage>â€“<lpage>791</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TMI.2017.2759102</pub-id><?supplied-pmid 28981409?><pub-id pub-id-type="pmid">28981409</pub-id></mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Hatanaka</surname><given-names>Y</given-names></name>, <name><surname>Muramatsu</surname><given-names>C</given-names></name>, <name><surname>Hara</surname><given-names>T</given-names></name>, <name><surname>Fujita</surname><given-names>H</given-names></name>. <article-title>Automatic arteriovenous crossing phenomenon detection on retinal fundus images</article-title>. In: <source>Medical Imaging 2011: Computer-Aided Diagnosis</source>. vol. <volume>7963</volume>; <year>2011</year>. p. <fpage>79633V</fpage>.</mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref012">
      <label>12</label>
      <mixed-citation publication-type="other">Li L, Verma M, Nakashima Y, Nagahara H, Kawasaki R. IterNet: Retinal Image Segmentation Utilizing Structural Redundancy in Vessel Networks. In: The IEEE Winter Conference on Applications of Computer Vision; 2020. p. 3656â€“3665.</mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref013">
      <label>13</label>
      <mixed-citation publication-type="other">CambÃ² VBS, Cariello L, Mastronardi G. A COMBINED METHOD TO DETECT RETINAL FUNDUS FEATURES. In: IEEE European Conference on Emergent Aspects in Clinical Data Analysis; 2005.</mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Taylor</surname><given-names>C</given-names></name>, <name><surname>Munro</surname><given-names>AJ</given-names></name>, <name><surname>Glynne-Jones</surname><given-names>R</given-names></name>, <name><surname>Griffith</surname><given-names>C</given-names></name>, <name><surname>Trevatt</surname><given-names>P</given-names></name>, <name><surname>Richards</surname><given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Multidisciplinary team working in cancer: what is the evidence?</article-title><source>The BMJ</source>. <year>2010</year>;<volume>340</volume>:<fpage>c951</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1136/bmj.c951</pub-id><?supplied-pmid 20332315?><pub-id pub-id-type="pmid">20332315</pub-id></mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Lin</surname><given-names>T</given-names></name>, <name><surname>Goyal</surname><given-names>P</given-names></name>, <name><surname>Girshick</surname><given-names>R</given-names></name>, <name><surname>He</surname><given-names>K</given-names></name>, <name><surname>DollÃ¡r</surname><given-names>P</given-names></name>. <article-title>Focal Loss for Dense Object Detection</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2020</year>;<volume>42</volume>(<issue>2</issue>):<fpage>318</fpage>â€“<lpage>327</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TPAMI.2018.2858826</pub-id><?supplied-pmid 30040631?><pub-id pub-id-type="pmid">30040631</pub-id></mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Inoue</surname><given-names>R</given-names></name>, <name><surname>Ohkubo</surname><given-names>T</given-names></name>, <name><surname>Kikuya</surname><given-names>M</given-names></name>, <name><surname>Metoki</surname><given-names>H</given-names></name>, <name><surname>Asayama</surname><given-names>K</given-names></name>, <name><surname>Kanno</surname><given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Stroke risk of blood pressure indices determined by home blood pressure measurement: the Ohasama study</article-title>. <source>Stroke</source>. <year>2009</year>;<volume>40</volume>(<issue>8</issue>):<fpage>2859</fpage>â€“<lpage>2861</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1161/STROKEAHA.108.546499</pub-id><?supplied-pmid 19478224?><pub-id pub-id-type="pmid">19478224</pub-id></mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>L</given-names></name>, <name><surname>Verma</surname><given-names>M</given-names></name>, <name><surname>Nakashima</surname><given-names>Y</given-names></name>, <name><surname>Kawasaki</surname><given-names>R</given-names></name>, <name><surname>Nagahara</surname><given-names>H</given-names></name>. <article-title>Joint Learning of Vessel Segmentation and Artery/Vein Classification with Post-processing</article-title>. In: <source>Medical Imaging with Deep Learning</source>; <year>2020</year>.</mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref018">
      <label>18</label>
      <mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2016. p. 770â€“778.</mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Huang</surname><given-names>G</given-names></name>, <name><surname>Liu</surname><given-names>Z</given-names></name>, <name><surname>Pleiss</surname><given-names>G</given-names></name>, <name><surname>Van Der Maaten</surname><given-names>L</given-names></name>, <name><surname>Weinberger</surname><given-names>K</given-names></name>. <article-title>Convolutional Networks with Dense Connectivity</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2019</year>;.</mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref020">
      <label>20</label>
      <mixed-citation publication-type="other">Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z. Rethinking the Inception Architecture for Computer Vision. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2016. p. 2818â€“2826.</mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref021">
      <label>21</label>
      <mixed-citation publication-type="other">Weber M, FÃ¼rst M, ZÃ¶llner JM. Automated Focal Loss for Image based Object Detection. arXiv preprint arXiv:190409048. 2019;.</mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref022">
      <label>22</label>
      <mixed-citation publication-type="other">Huang C, Li Y, Loy CC, Tang X. Learning Deep Representation for Imbalanced Classification. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2016. p. 5375â€“5384.</mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref023">
      <label>23</label>
      <mixed-citation publication-type="other">Cui Y, Jia M, Lin TY, Song Y, Belongie S. Class-Balanced Loss Based on Effective Number of Samples. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2019.</mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Russakovsky</surname><given-names>O</given-names></name>, <name><surname>Deng</surname><given-names>J</given-names></name>, <name><surname>Su</surname><given-names>H</given-names></name>, <name><surname>Krause</surname><given-names>J</given-names></name>, <name><surname>Satheesh</surname><given-names>S</given-names></name>, <name><surname>Ma</surname><given-names>S</given-names></name>, <etal>et al</etal>. <article-title>ImageNet Large Scale Visual Recognition Challenge</article-title>. <source>International Journal of Computer Vision (IJCV)</source>. <year>2015</year>;<volume>115</volume>(<issue>3</issue>):<fpage>211</fpage>â€“<lpage>252</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref025">
      <label>25</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J. Adam: A method for stochastic optimization. arXiv preprint arXiv:14126980. 2014;.</mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref026">
      <label>26</label>
      <mixed-citation publication-type="other">Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. In: International Conference on Computer Vision (ICCV); 2017. p. 618â€“626.</mixed-citation>
    </ref>
    <ref id="pdig.0000174.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Norgeot</surname><given-names>B</given-names></name>, <name><surname>Quer</surname><given-names>G</given-names></name>, <name><surname>Beaulieu-Jones</surname><given-names>BK</given-names></name>, <name><surname>Torkamani</surname><given-names>A</given-names></name>, <name><surname>Dias</surname><given-names>R</given-names></name>, <name><surname>Gianfrancesco</surname><given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist</article-title>. <source>Nature medicine</source>. <year>2020</year>;<volume>26</volume>(<issue>9</issue>):<fpage>1320</fpage>â€“<lpage>1324</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41591-020-1041-y</pub-id><?supplied-pmid 32908275?><pub-id pub-id-type="pmid">32908275</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
