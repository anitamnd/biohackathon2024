<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Database (Oxford)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Database (Oxford)</journal-id>
    <journal-id journal-id-type="publisher-id">databa</journal-id>
    <journal-title-group>
      <journal-title>Database: The Journal of Biological Databases and Curation</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1758-0463</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
      <publisher-loc>UK</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9693804</article-id>
    <article-id pub-id-type="doi">10.1093/database/baac103</article-id>
    <article-id pub-id-type="publisher-id">baac103</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Article</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00960</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>LitCovid ensemble learning for COVID-19 multi-label classification</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Gu</surname>
          <given-names>Jinghang</given-names>
        </name>
        <!--gujinghangnlp@gmail.com-->
        <xref rid="COR0001" ref-type="corresp"/>
        <aff><institution content-type="department">Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic University</institution>, Hong Kong 999077, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chersoni</surname>
          <given-names>Emmanuele</given-names>
        </name>
        <aff><institution content-type="department">Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic University</institution>, Hong Kong 999077, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Xing</given-names>
        </name>
        <aff><institution>Tencent AI Lab</institution>, Shenzhen 518071, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Huang</surname>
          <given-names>Chu-Ren</given-names>
        </name>
        <aff><institution content-type="department">Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic University</institution>, Hong Kong 999077, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Qian</surname>
          <given-names>Longhua</given-names>
        </name>
        <aff><institution content-type="department">School of Computer Science and Technology, Soochow University</institution>, Suzhou 215006, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Guodong</given-names>
        </name>
        <aff><institution content-type="department">School of Computer Science and Technology, Soochow University</institution>, Suzhou 215006, <country country="CN">China</country></aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="COR0001">*Corresponding author: Tel: +852-34008553; Email: <email xlink:href="gujinghangnlp@gmail.com">gujinghangnlp@gmail.com</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-11-25">
      <day>25</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>25</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <volume>2022</volume>
    <elocation-id>baac103</elocation-id>
    <history>
      <date date-type="received">
        <day>05</day>
        <month>3</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>27</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>04</day>
        <month>11</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>28</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>25</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="baac103.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>The Coronavirus Disease 2019 (COVID-19) pandemic has shifted the focus of research worldwide, and more than 10 000 new articles per month have concentrated on COVID-19–related topics. Considering this rapidly growing literature, the efficient and precise extraction of the main topics of COVID-19–relevant articles is of great importance. The manual curation of this information for biomedical literature is labor-intensive and time-consuming, and as such the procedure is insufficient and difficult to maintain. In response to these complications, the BioCreative VII community has proposed a challenging task, LitCovid Track, calling for a global effort to automatically extract semantic topics for COVID-19 literature. This article describes our work on the BioCreative VII LitCovid Track. We proposed the LitCovid Ensemble Learning (LCEL) method for the tasks and integrated multiple biomedical pretrained models to address the COVID-19 multi-label classification problem. Specifically, seven different transformer-based pretrained models were ensembled for the initialization and fine-tuning processes independently. To enhance the representation abilities of the deep neural models, diverse additional biomedical knowledge was utilized to facilitate the fruitfulness of the semantic expressions. Simple yet effective data augmentation was also leveraged to address the learning deficiency during the training phase. In addition, given the imbalanced label distribution of the challenging task, a novel asymmetric loss function was applied to the LCEL model, which explicitly adjusted the negative–positive importance by assigning different exponential decay factors and helped the model focus on the positive samples. After the training phase, an ensemble bagging strategy was adopted to merge the outputs from each model for final predictions. The experimental results show the effectiveness of our proposed approach, as LCEL obtains the state-of-the-art performance on the LitCovid dataset.</p>
      <p><bold>Database URL</bold>: <ext-link xlink:href="https://github.com/JHnlp/LCEL" ext-link-type="uri">https://github.com/JHnlp/LCEL</ext-link></p>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Hong Kong Polytechnic University</institution>
            <institution-id institution-id-type="DOI">10.13039/501100004377</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>1-W182</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Hong Kong Polytechnic University</institution>
            <institution-id institution-id-type="DOI">10.13039/501100004377</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>G-YW4H</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="14"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="s2">
    <title>Introduction</title>
    <p>The reality of the pandemic sweeping across the world and the challenges it has caused have rapidly accelerated the global pace of scientific publications (<xref rid="R1" ref-type="bibr">1</xref>, <xref rid="R2" ref-type="bibr">2</xref>). Since more than 10 000 articles related to COVID-19 have been published monthly (<xref rid="R3" ref-type="bibr">3–5</xref>), the burden of manual curation and downstream interpretation has increased, making it difficult to access scientific analysis, pharmaceutical engineering and public usage.</p>
    <p>In response to the rapid growth of COVID-19–related information, the LitCovid hub (<xref rid="R3" ref-type="bibr">3</xref>, <xref rid="R4" ref-type="bibr">4</xref>), a new, curated literature database, has been developed to track up-to-date published research on COVID-19–related articles in PubMed. To facilitate the information retrieval, all curated publications in LitCovid are annotated by predefined semantic topics and updated daily. These elaborated semantic topics have been shown to be effective for various downstream applications such as citation analysis and knowledge graph generation (<xref rid="R11" ref-type="bibr">11</xref>). Currently, the annotation of biomedical semantic topics for COVID-19 literature is completed manually by human experts with specific domain knowledge (<xref rid="R3" ref-type="bibr">3</xref>, <xref rid="R4" ref-type="bibr">4</xref>). However, the manual annotation of the fast-growing COVID-19 literature is labor-intensive and time-consuming. In order to assure accuracy, experts have to thoroughly examine the entirety of each biomedical article and assign it to a series of suitable predefined semantic topics. Moreover, the fact that biomedical literature often involves multiple topics rather than one single topic further aggravates the challenge of manual biocuration. Hence, the automatic curation and the interpretation of COVID-19 literature have become a problem of great importance.</p>
    <p>Despite the preliminary efforts (<xref rid="R6" ref-type="bibr">6–10</xref>) providing feasible solutions in various knowledge extraction tasks for the biomedical domain, automated identification of COVID-19 semantic topics remains challenging. In addition, few identification tools for COVID-19 topics are freely available, and there are seldom successful examples of such applications in the real world.</p>
    <p>In this regard, to tackle the automated topic identification for COVID-19, the BioCreative VII community stepped out and proposed the LitCovid challenge (<xref rid="R11" ref-type="bibr">11</xref>) in 2021. This task is regarded as a typical multi-label classification problem that calls for a global effort to provide practical benefits to worldwide biomedical curation. For this task, each participant was required to assign one or more semantic topics to each biomedical article. Each topic caters to different information needs of users and is effective for COVID-19–related information retrieval and downstream applications (<xref rid="R3" ref-type="bibr">3</xref>, <xref rid="R12" ref-type="bibr">12</xref>). Particularly, seven specific COVID-19–related topics are proposed in the challenge, namely Treatment, Diagnosis, Prevention, Mechanism, Transmission, Case Report and Epidemic Forecasting. <xref rid="T1" ref-type="table">Table 1</xref> presents the detailed descriptions of the semantic topics used in the LitCovid challenge.</p>
    <table-wrap position="float" id="T1">
      <label>Table 1.</label>
      <caption>
        <p>Description of the semantic topics of the LitCovid challenge</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Topic</th>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" rowspan="1" colspan="1">Treatment</td>
            <td align="left" rowspan="1" colspan="1">Treatment strategies, therapeutic procedures and vaccine development for COVID-19</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Diagnosis</td>
            <td align="left" rowspan="1" colspan="1">COVID-19 assessment through symptoms, test results and radiological features for COVID-19</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Prevention</td>
            <td align="left" rowspan="1" colspan="1">Prevention, control, mitigation and management strategies</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Mechanism</td>
            <td align="left" rowspan="1" colspan="1">Underlying cause(s) of COVID-19 infections and transmission and possible drug mechanism of action</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Transmission</td>
            <td align="left" rowspan="1" colspan="1">Characteristics and modes of COVID-19 transmissions</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Case Report</td>
            <td align="left" rowspan="1" colspan="1">Descriptions of specific patient cases related to COVID-19</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Epidemic Forecasting</td>
            <td align="left" rowspan="1" colspan="1">Estimation on the trend of COVID-19 spread and related modeling approach</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>In this article, building upon our previous research (<xref rid="R13" ref-type="bibr">13</xref>), we present the extension work of LitCovid Ensemble Learning (LCEL) for the challenge of BioCreative VII LitCovid Track. Specifically, we thoroughly explored seven different advanced pretrained models with heterogeneous architectures for ensemble learning, which guarantees the diversity and robustness of the deep neural networks. Moreover, to enhance the representation abilities of deep neural models, additional biomedical knowledge was proposed to facilitate the fruitfulness of the semantic expressions. Simple yet effective data augmentation was also exploited to address the learning deficiency during the training stage. In addition, to handle the imbalanced label distribution, a novel Asymmetric Loss (ASL) function (<xref rid="R14" ref-type="bibr">14</xref>) was introduced to the LCEL model, which explicitly adjusted the negative–positive importance by assigning different exponential decay factors. Benefiting from the usage of ASL, the proposed model was able to dynamically decouple the modulations of the positive and negative samples during the training phase and focused more on the positive samples while mitigating the contribution of negative ones.</p>
    <p>The primary goal of this study was to develop a versatile machine learning approach with favorable robustness and generalizability to be easily applied to the COVID-19 domain and scaled up to other biomedical fields. The experimental results on the LitCovid dataset achieved state-of-the-art performance, demonstrating the effectiveness of our proposed method. The main contributions of this work are summarized as follows:</p>
    <list list-type="roman-lower">
      <list-item>
        <p>We propose a novel ensemble learning framework that can scale up effectively to the COVID-19 domain. Our study shows the superiority of the proposed method, which outperforms the current state-of-the-art systems.</p>
      </list-item>
      <list-item>
        <p>We propose leveraging additional biomedical knowledge as well as data augmentation to enhance the semantic representation ability of the ensemble models. We argue that such kinds of semantic information can benefit the COVID-19 topic classification and supplement the development of relevant biomedical text mining technologies.</p>
      </list-item>
      <list-item>
        <p>We introduce a novel loss function, ASL, which explicitly adjusts the importance of both positive and negative training samples. Due to the employment of ASL, the model can efficiently mitigate the imbalanced label distribution problem.</p>
      </list-item>
      <list-item>
        <p>We make the related codes and materials of the proposed method publicly available to the research community. Our work is capable of offering new insights and building essential foundations for researchers in support of the ongoing fight against COVID-19.</p>
      </list-item>
    </list>
  </sec>
  <sec id="s3">
    <title>Related work</title>
    <p>In previous decades, biomedical topic identification was regarded as the multi-label classification problem, and a series of automated approaches (<xref rid="R15" ref-type="bibr">15–19</xref>) were developed to improve the time-consuming and labor-intensive curation process.</p>
    <p>The National Library of Medicine (NLM) developed the most famous biomedical topic identification system, Medical Text Indexing (MTI) (<xref rid="R15" ref-type="bibr">15</xref>), which has been aiding the NLM human curators since 2002. The main purpose of MTI is to apply a rank-based approach to model the topic identification problem, where the top-ranked topics are recommended as true labels. Recently, with the comprehensive success of deep neural networks, deep learning–based approaches have brought remarkable breakthroughs in various biomedical topic identification tasks (<xref rid="R16" ref-type="bibr">16–19</xref>). FullMeSH (<xref rid="R16" ref-type="bibr">16</xref>) proposed a hybrid architecture integrating both deep neural networks and traditional machine learning methods to improve the topic identification performance. Specifically, it took advantage of Support Vector Machine, K-Nearest Neighbors algorithm and an attention-based convolution neural network to generate semantic evidence for the topic recommendation. Its attention mechanism exhibited remarkable potential by providing automatic feature representations without manual interference. AttentionMeSH (<xref rid="R17" ref-type="bibr">17</xref>) is another effective model based on attention mechanisms for biomedical topic identification. It utilized the architecture of a bidirectional recurrent neural network (RNN) with an attention mechanism to classify semantic topics for biomedical articles. Due to its deep representation capability, AttentionMeSH enabled the model to associate more textual evidence with candidate labels for better prediction results. MeSHProbeNet (<xref rid="R18" ref-type="bibr">18</xref>) and MeSHProbeNet-P (<xref rid="R19" ref-type="bibr">19</xref>) were two homogeneous deep learning methods that incorporated RNN and attention mechanisms simultaneously. By leveraging multiple semantic probes through an attention-based enhancement, MeSHProbeNet and MeSHProbeNet-P were able to acquire much deeper semantic insights into biomedical knowledge than the original documental contexts.</p>
    <p>Despite preliminary efforts (<xref rid="R15" ref-type="bibr">15–19</xref>) that have provided feasible solutions and remarkable signs of progress over automatic topic identification, there is still an apparent gap between these automated methods and their applications to the COVID-19 domain. On the one hand, the above-described topic classification systems mainly concentrate on the topics of Medical Subject Headings (MeSH) (<xref rid="R20" ref-type="bibr">20</xref>, <xref rid="R21" ref-type="bibr">21</xref>), which is a relatively large yet general set of biomedical concepts. Nevertheless, confronting the current pandemic crisis, there is a severe lack of such specialized topic collections targeting the evolving biomedical knowledge of COVID-19. On the other hand, lacking such a standard corpus for the COVID-19 domain drastically restricts the development of data mining techniques for identifying COVID-19–related semantic topics.</p>
    <p>In light of these concerns, the BioCreative VII community proposed the challenging task of the LitCovid Track (<xref rid="R11" ref-type="bibr">11</xref>), which targets assigning multiple topic labels to COVID-19–relevant literature. This task is regarded as a typical multi-label classification problem that calls for a worldwide effort to provide practical benefits to COVID-19 biocuration. For this task, 19 teams participated and submitted a total of 80 valid predictions during the online competition. Pretraining methods dominated the challenging task and exhibited the best performance amid the online evaluation. Specifically, DUT914 (<xref rid="R22" ref-type="bibr">22</xref>) merged feature representations originating from different pretrained models to address the LitCovid multi-label classification problem. Likewise, DonutNLP (<xref rid="R23" ref-type="bibr">23</xref>) utilized a voting-based method integrating multiple pretrained models to enhance the representation ability for the final prediction. Our previous work, PolyU_CBSNLP (<xref rid="R13" ref-type="bibr">13</xref>), proposed to make full use of the homogeneous and heterogeneous structures of different pretrained models and achieved a promising performance during the online competition. Apart from the techniques of pretrained models, Bioformer (<xref rid="R24" ref-type="bibr">24</xref>) also exploited a large amount of external biomedical articles for further fine-tuning in the training phase, which helped the model achieve the best performance during the competition.</p>
    <p>In view of multi-label classification, the characteristic is the inherent imbalanced positive–negative label distribution. This kind of task usually contains a relatively small portion of possible labels, implying that the number of positive samples per category will be, on average, much lower than that of negative ones. To address this issue, resampling-based methods are usually applied to balance the background positive–negative label distribution (<xref rid="R25" ref-type="bibr">25</xref>). However, such resampling methods are not always suitable for multi-label classification tasks, as the tasks contain multiple labels, while resampling cannot change the distribution of the specific label. A prominent solution for multi-label imbalance is to adopt the focal loss (<xref rid="R26" ref-type="bibr">26</xref>), which decays the loss as the label confidence increases. Focal loss helps the model focus on the hard samples while downweighting the easy ones, which demonstrates outstanding results in various object detection tasks. However, treating the positive and negative samples equally, as proposed by focal loss, is sometimes suboptimal, as it results in the accumulation of more loss gradients from negative samples while underemphasizing the importance of the rare positive ones. To this end, on the basis of focal loss (<xref rid="R26" ref-type="bibr">26</xref>), a novel ASL (<xref rid="R14" ref-type="bibr">14</xref>) emerges to operate differently on positive and negative samples. ASL enables deep neural models to dynamically downweight and hard threshold the easy negative samples, whereas the possibly mislabeled samples will be discarded. With the help of ASL, deep neural models achieved state-of-the-art performance on multiple popular image classification tasks (<xref rid="R14" ref-type="bibr">14</xref>). As there is a lack of such distinct research investigating the label imbalance for the COVID-19 domain, we propose introducing ASL to existing achievements to assist the research of COVID-19 topic identification.</p>
    <p>Inspired by previous works, this article is devoted to the COVID-19 topic identification problem. We aim to provide a publicly available benchmark system with robust and flexible inherence for the COVID-19 domain, thus filling the important gap in previous research.</p>
  </sec>
  <sec id="s4">
    <title>Dataset</title>
    <p>In this section, we first present a brief introduction to the LitCovid corpus and then systematically depict the statistics of the corpus.</p>
    <p>The LitCovid corpus developed by the BioCreative VII community originates from a large-scale curated literature hub, whose curated data are updated daily with the latest COVID-19–relevant articles; it is also publicly available for research purposes and industrial applications (<xref rid="R3" ref-type="bibr">3</xref>, <xref rid="R4" ref-type="bibr">4</xref>). The BioCreative VII organizers collected more than 30 000 COVID-19–related articles from the literature hub (<xref rid="R11" ref-type="bibr">11</xref>), which were further split into three subsets of training, development and test datasets, respectively. During the competition phase, the organizers first released the training dataset as well as the development dataset in Comma-Separated Values format. Later, they released the test dataset following the same data schema except for the ground-truth labels, which were supposed to be predicted by the participants.</p>
    <p>Since the LitCovid Track targeted the multi-label classification for COVID-19 semantic topics, seven specific topic labels were annotated in the corpus, namely Treatment, Diagnosis, Prevention, Mechanism, Transmission, Case Report and Epidemic Forecasting. Out of the semantic topics annotated for each article, the organizers also provided various kinds of metadata retrieved from PubMed, enhancing the fruitfulness of the dataset. More detailed information on the LitCovid corpus is shown in <xref rid="T2" ref-type="table">Tables 2</xref> and <xref rid="T3" ref-type="table">3</xref>.</p>
    <table-wrap position="float" id="T2">
      <label>Table 2.</label>
      <caption>
        <p>The metadata statistics of the LitCovid corpus</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Metadata</th>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Train</th>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Development</th>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Test</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" rowspan="1" colspan="1">Title</td>
            <td align="left" rowspan="1" colspan="1">24 960</td>
            <td align="left" rowspan="1" colspan="1">6239</td>
            <td align="left" rowspan="1" colspan="1">2500</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Abstract</td>
            <td align="left" rowspan="1" colspan="1">24 900</td>
            <td align="left" rowspan="1" colspan="1">6219</td>
            <td align="left" rowspan="1" colspan="1">2485</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Journal name</td>
            <td align="left" rowspan="1" colspan="1">24 960</td>
            <td align="left" rowspan="1" colspan="1">6239</td>
            <td align="left" rowspan="1" colspan="1">2500</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Keywords</td>
            <td align="left" rowspan="1" colspan="1">18 968</td>
            <td align="left" rowspan="1" colspan="1">4754</td>
            <td align="left" rowspan="1" colspan="1">2056</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">PMID</td>
            <td align="left" rowspan="1" colspan="1">24 960</td>
            <td align="left" rowspan="1" colspan="1">6239</td>
            <td align="left" rowspan="1" colspan="1">2500</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Authors</td>
            <td align="left" rowspan="1" colspan="1">24 859</td>
            <td align="left" rowspan="1" colspan="1">6212</td>
            <td align="left" rowspan="1" colspan="1">2499</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">DOI</td>
            <td align="left" rowspan="1" colspan="1">24 406</td>
            <td align="left" rowspan="1" colspan="1">6100</td>
            <td align="left" rowspan="1" colspan="1">2474</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Publication type</td>
            <td align="left" rowspan="1" colspan="1">24 960</td>
            <td align="left" rowspan="1" colspan="1">6239</td>
            <td align="left" rowspan="1" colspan="1">2500</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap position="float" id="T3">
      <label>Table 3.</label>
      <caption>
        <p>The label distribution of the LitCovid corpus</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Label</th>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Train</th>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Development</th>
            <th valign="bottom" align="left" rowspan="1" colspan="1">Test</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left" rowspan="1" colspan="1">Treatment</td>
            <td align="left" rowspan="1" colspan="1">8718</td>
            <td align="left" rowspan="1" colspan="1">2207</td>
            <td align="left" rowspan="1" colspan="1">1035</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Diagnosis</td>
            <td align="left" rowspan="1" colspan="1">6193</td>
            <td align="left" rowspan="1" colspan="1">1546</td>
            <td align="left" rowspan="1" colspan="1">722</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Prevention</td>
            <td rowspan="1" colspan="1">11 102</td>
            <td align="left" rowspan="1" colspan="1">2750</td>
            <td align="left" rowspan="1" colspan="1">926</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Mechanism</td>
            <td align="left" rowspan="1" colspan="1">4439</td>
            <td align="left" rowspan="1" colspan="1">1073</td>
            <td align="left" rowspan="1" colspan="1">567</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Transmission</td>
            <td align="left" rowspan="1" colspan="1">1088</td>
            <td align="left" rowspan="1" colspan="1">256</td>
            <td align="left" rowspan="1" colspan="1">128</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Epidemic Forecasting</td>
            <td align="left" rowspan="1" colspan="1">645</td>
            <td align="left" rowspan="1" colspan="1">192</td>
            <td align="left" rowspan="1" colspan="1">41</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Case Report</td>
            <td align="left" rowspan="1" colspan="1">2063</td>
            <td align="left" rowspan="1" colspan="1">482</td>
            <td align="left" rowspan="1" colspan="1">197</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Total</td>
            <td rowspan="1" colspan="1">34 248</td>
            <td align="left" rowspan="1" colspan="1">8506</td>
            <td align="left" rowspan="1" colspan="1">3616</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p><xref rid="T2" ref-type="table">Table 2</xref> summarizes the basic statistical information of the LitCovid corpus. As shown in <xref rid="T2" ref-type="table">Table 2</xref>, there are 33 699 COVID-19–related biomedical articles in the corpus, with a training set of 24 960 articles, a development set of 6239 articles and a test set of 2500 articles. Most of the articles are filled with valid attributes of titles, abstracts, journal names, PubMed Identifiers (PMIDs), author names, Digital Object Identifiers (DOIs) as well as publication types. These abundant attributes guarantee the indispensable information which assures comprehensive coverage for research on COVID-19 topics and downstream applications. However, it is worth noting that despite the organizers trying their best to fill the metadata attributes, around 25% of keywords are still missing due to the incompleteness of the online information.</p>
    <p><xref rid="T3" ref-type="table">Table 3</xref> depicts the label distribution of the LitCovid corpus. As shown in <xref rid="T3" ref-type="table">Table 3</xref>, it is observed that the frequency of different labels varies significantly. Among all topic labels, the labels of Prevention and Treatment dominate the entire corpus with relatively higher frequency, while the label of Epidemic Forecasting barely occurs, indicating an extremely imbalanced label distribution in the corpus, which makes the LitCovid challenge even harder, as most topic labels may never be observed in an article.</p>
  </sec>
  <sec id="s5">
    <title>Methods</title>
    <p>In this section, the LCEL paradigm is proposed for the COVID-19 multi-label classification problem. <xref rid="F1" ref-type="fig">Figure 1</xref> illustrates the architecture of the proposed method, which is a universal ensemble learning framework integrating multiple classifiers generated from different powerful pretrained models.</p>
    <fig position="float" id="F1" fig-type="figure">
      <label>Figure 1.</label>
      <caption>
        <p>The overall framework of LCEL.</p>
      </caption>
      <graphic xlink:href="baac103f1" position="float"/>
    </fig>
    <p>As known in the ensemble learning theory, every single model is taken as a weak learner or classifier due to its bias and variance in the feature representation (<xref rid="R27" ref-type="bibr">27</xref>). On this basis, the LCEL model is to train multiple weak classifiers separately through an ensemble manner and aggregate these weak classifiers into a stronger one to acquire better results. Specifically, we take advantage of multiple advanced pretrained models with different transformer-based structures for the initialization of LCEL (<xref rid="R13" ref-type="bibr">13</xref>). The hypothesis is that when weak classifiers aggregate appropriately, the system is able to efficiently narrow down the bias and variance of such weak learners to create a stronger learner, achieving a more accurate and robust performance.</p>
    <p>In <xref rid="F1" ref-type="fig">Figure 1</xref>, the overall framework of LCEL is shown. To begin, each classifier of the pretrained neural models is fine-tuned independently during the training process, and then all the outputs of these classifiers are merged through an ensemble bagging strategy to obtain the final topic prediction. Moreover, in order to improve the representation diversity and robustness of ensemble learning, the pretrained models with different architectural implementations are taken into consideration. Particularly, seven powerful pretrained transformers are elaborated in LCEL, i.e. PubMedBERT (<xref rid="R28" ref-type="bibr">28</xref>), CovidBERT (<xref rid="R29" ref-type="bibr">29</xref>), BioBERT-Large (<xref rid="R30" ref-type="bibr">30</xref>), BioBERT-Base (<xref rid="R30" ref-type="bibr">30</xref>), BioM-ELECTRA (<xref rid="R31" ref-type="bibr">31</xref>), BioELECTRA (<xref rid="R32" ref-type="bibr">32</xref>) and BioMed-RoBERTa (<xref rid="R33" ref-type="bibr">33</xref>). It is worth noticing that among these pretrained models, there are four variants of BERT (<xref rid="R34" ref-type="bibr">34</xref>), two variants of ELECTRA (<xref rid="R35" ref-type="bibr">35</xref>) and one edition of RoBERTa (<xref rid="R36" ref-type="bibr">36</xref>), respectively. We refer to the models with the same underlying architecture as homogeneous models; otherwise, the models are referred to as heterogeneous ones. These homogeneous and heterogeneous model groups ensure the effectiveness and stability of the proposed ensemble learning method.</p>
    <p><xref rid="F2" ref-type="fig">Figure 2</xref> depicts the holistic structure of each pretrained classifier ensembled in LCEL. As shown in <xref rid="F2" ref-type="fig">Figure 2</xref>, each classifier consists of two main modules, namely Feature Representation and Multi-label Classification. In this figure, the Feature Representation module takes the multiple textual components as the inputs and considers diverse semantic aspects for each input article. These textual inputs are then encoded by transformer-like encoders to generate further feature representations.</p>
    <fig position="float" id="F2" fig-type="figure">
      <label>Figure 2.</label>
      <caption>
        <p>The structure of the transformer-based multi-label classifier.</p>
      </caption>
      <graphic xlink:href="baac103f2" position="float"/>
    </fig>
    <p>After the Feature Representation stage, a linear classifier is adopted to take the extracted features from different semantic aspects to perform the final topic classification. For each candidate topic, the model is able to predict a probability score. In addition, to handle the imbalanced label distribution problem, a novel loss function, i.e., ASL, (<xref rid="R14" ref-type="bibr">14</xref>) is proposed in LCEL to dynamically adjust the learning weights between positive and negative instances during the training phase. More detailed information is described in the following subsections.</p>
    <sec id="s5-s1">
      <title>Feature representation</title>
      <p>Since the titles and abstracts of biomedical literature convey rich contextual information that offers both explicit and implicit cues for determining topics, such contexts are regarded as the textual inputs for LCEL. However, despite the meaningful contexts of the biomedical literature, surface textual expressions are still less informative for semantic representation due to the deficiency of necessary knowledge comprehension.</p>
      <p>Following our previous work (<xref rid="R13" ref-type="bibr">13</xref>), we argue that the additional biomedical knowledge, such as keywords, MeSH terms and journal names, is beneficial for the problem of COVID-19 topic identification. The main idea behind taking these kinds of additional biomedical knowledge is that they carry a large amount of manually refined semantic meanings that have been carefully reviewed by the authors and curators. Therefore, as shown at the bottom of <xref rid="F2" ref-type="fig">Figure 2</xref>, before the training stage, the input sequence of each article needs to be constructed by concatenating all texts of keywords, MeSH terms, journal names, as well as titles and abstracts. Note that since the MeSH terms are not available in the official LitCovid corpus, we thus crawled these crucial materials as supplements from PubMed in terms of the corresponding identifier PMID of each target article.</p>
      <p>After concatenating the above-mentioned contexts and knowledge-based semantic information, a powerful transformer encoder is further applied to the texts for higher-quality feature representation, which has shown promising results in various natural language processing (NLP) tasks (<xref rid="R34" ref-type="bibr">34</xref>, <xref rid="R37" ref-type="bibr">37</xref>). As the transformer encoder makes use of both explicit and implicit textual correlations between the adjacent tokens, each word in the input sequence is accordingly represented by its hidden state, generated as follows:
<disp-formula id="M0001"><label>(1)</label><tex-math notation="LaTeX" id="M0001-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${{\rm{\textbf{h}}}_{\rm{i}}} = {\rm{Transformer}}(\theta ; \ {{\rm{\textbf{w}}}_{\rm{i}}}{\rm{)}} \in {\mathbb{R}^{\rm{d}}}$$\end{document}</tex-math></disp-formula></p>
      <p>where <bold>w<sub>i</sub></bold> is the input word at position i, θ represents the encoder parameters of the transformer, <italic toggle="yes">d</italic> stands for the hidden size and <bold>h<sub>i</sub></bold> means the encoded hidden state for the i-th word. The entire textual input is then accordingly represented by the sequence of the encoded hidden states, which is denoted as follows:
<disp-formula id="M0002"><label>(2)</label><tex-math notation="LaTeX" id="M0002-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${\rm{\textbf{H} = [}}{{\rm{\textbf{h}}}_{\rm{1}}}{\rm{, }}{{\rm{\textbf{h}}}_{\rm{2}}}{\rm{, \ldots , }}{{\rm{\textbf{h}}}_{\rm{L}}}{{\rm{]}}^{\rm{T}}} \in {\mathbb{R}^{{\rm{L}} \times {\rm{d}}}}$$\end{document}</tex-math></disp-formula></p>
      <p>where L is the length of the input sequence, <inline-formula id="ILM0001"><tex-math notation="LaTeX" id="ILM0001-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\rm{\textbf{H}}} \in {\mathbb{R}^{{\rm{L}} \times {\rm{d}}}}$\end{document}</tex-math></inline-formula> is a L × d matrix concatenating all hidden states of the input words.</p>
      <p>Nonetheless, although transformers-based methods have gained prominence in various NLP tasks (<xref rid="R34" ref-type="bibr">34</xref>, <xref rid="R37" ref-type="bibr">37</xref>), their performance still highly relies on the quantity and diversity of the training data and usually suffers from the challenges of inadequate training samples. Moreover, deep neural models are also prone to overfit on small datasets due to their massive number of trainable parameters. In light of these concerns, when confronting the COVID-19 topic identification problem, a simple yet effective data augmentation approach is also introduced to LCEL for further performance improvement. Specifically, a large number of additional 96 804 COVID-19–relevant articles (including titles, abstracts, labels, etc.) are incorporated from the online hub (<xref rid="R11" ref-type="bibr">11</xref>) to enrich the representation capability of LCEL.</p>
    </sec>
    <sec id="s5-s2">
      <title>Multi-label classification</title>
      <p>Benefiting from the superior representation ability of transformers, the first hidden state vector <inline-formula id="ILM0002"><tex-math notation="LaTeX" id="ILM0002-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\rm{r}} \in {\mathbb{R}^{\rm{h}}}$\end{document}</tex-math></inline-formula> out of <bold>H</bold> is considered the final feature representation for the input article and is further fed into a linear projection layer with a Sigmoid activation function for the topic classification. The final output <inline-formula id="ILM0003"><tex-math notation="LaTeX" id="ILM0003-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\hat {\textrm{\textbf{y}}} \in {\mathbb{R}^{\rm{m}}}$\end{document}</tex-math></inline-formula> is consisted of the predicted probabilities of the corresponding labels for each input article:
<disp-formula id="M0003"><label>(3)</label><tex-math notation="LaTeX" id="M0003-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${\hat{\textrm{\textbf{y}}}}\,{\rm{ = }}\,{\sigma (}{\bf{Wr}}\,{\bf{ + }}\,{\bf{b}}{\rm{)}}$$\end{document}</tex-math></disp-formula></p>
      <p>where <inline-formula id="ILM0004"><tex-math notation="LaTeX" id="ILM0004-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\textrm{\textbf{W}}} \in {\mathbb{R}^{{\rm{m \times h}}}}$\end{document}</tex-math></inline-formula> is the linear transformation matrix, <inline-formula id="ILM0005"><tex-math notation="LaTeX" id="ILM0005-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\textrm{\textbf{b}}} \in {\mathbb{R}^{\rm{m}}}$\end{document}</tex-math></inline-formula> is the bias and σ stands for the Sigmoid activation function. The value <italic toggle="yes">m</italic> equals the number of the target topics for the final classification, and each output can be interpreted as the confidence score of the corresponding topic recommendation.</p>
      <p>Since the COVID-19 topic identification task is regarded as a multi-label classification problem, a high negative–positive label imbalance issue is inevitably encountered during the training phase. Therefore, how to tackle the extremely imbalanced label distribution is essential to the overall system performance. In previous decades, the conventional loss function of binary cross-entropy (BCE) has dominated various challenging NLP tasks and similarly exhibited remarkable success in numerous biomedical fields (<xref rid="R16" ref-type="bibr">16–19</xref>). However, the aggressive essence of BCE treats all candidate labels equally without distinguishing the different importance between positive and negative samples. This disadvantage usually leads to suboptimal performance (<xref rid="R14" ref-type="bibr">14</xref>, <xref rid="R26" ref-type="bibr">26</xref>), as it results in the accumulation of more loss gradients from negative samples while downweighting the importance of the rare positive ones.</p>
      <p>To address the aforementioned drawbacks of BCE, a novel loss function, i.e., ASL, (<xref rid="R14" ref-type="bibr">14</xref>) is proposed in the LCEL model to address the label imbalance problem via a dynamical weight adjustment in negative and positive samples. The idea behind leveraging the ASL is that ASL reviews the different contributions of both positive and negative samples and encourages the model to pay more attention to the most difficult examples for better performance. Specifically, given <italic toggle="yes">K</italic> different topic labels, the neural network outputs one logit per label, z<sub>k</sub> and each logit are independently activated by a Sigmoid function σ. For example, if we denote <italic toggle="yes">k</italic> as the ground truth for class k, then the total classification loss, <inline-formula id="ILM0006"><tex-math notation="LaTeX" id="ILM0006-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal{L}_{{\rm{all}}}}$\end{document}</tex-math></inline-formula>, is then obtained by summing up all binary losses from <italic toggle="yes">K</italic> labels:
<disp-formula id="M0004"><label>(4)</label><tex-math notation="LaTeX" id="M0004-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${\mathcal{L}_{{\rm{all}}}}{\rm{ = }}\mathop \sum \limits_{{\rm{k = 1}}}^{\rm{K}} \mathcal{L}{(\sigma (}{{\rm{z}}_{\rm{k}}}{\rm{), \, }}{{\rm{y}}_{\rm{k}}}{\rm{)}}$$\end{document}</tex-math></disp-formula></p>
      <p>where <inline-formula id="ILM0007"><tex-math notation="LaTeX" id="ILM0007-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${{\rm{y}}_{\rm{k}}} \in {\rm{[0, 1]}}$\end{document}</tex-math></inline-formula> is the ground-truth label of the k-th label; y<sub>k</sub> = 1 means the k-th label is manually annotated to the input article; otherwise, y<sub>k</sub> is assigned with 0. Consequently, a more general form of the binary loss per label, <inline-formula id="ILM0008"><tex-math notation="LaTeX" id="ILM0008-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal{L}$\end{document}</tex-math></inline-formula>, is formulated as:
<disp-formula id="M0005"><label>(5)</label><tex-math notation="LaTeX" id="M0005-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\mathcal{L} {\rm{ = - y}}{\mathcal{L}_ + }{\rm{ - (1 - y) }}{\mathcal{L}_ - }$$\end{document}</tex-math></disp-formula></p>
      <p>where <italic toggle="yes">y</italic> is the ground-truth label, <inline-formula id="ILM0009"><tex-math notation="LaTeX" id="ILM0009-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal{L}_ + }$\end{document}</tex-math></inline-formula> and <inline-formula id="ILM0010"><tex-math notation="LaTeX" id="ILM0010-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal{L}_ - }$\end{document}</tex-math></inline-formula> are the positive and negative loss components, respectively. On this basis, the unified form of ASL is defined as follows:
<disp-formula id="M0006"><label>(6)</label><tex-math notation="LaTeX" id="M0006-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${\rm{ASL}} = \left\{ {\begin{matrix}{{\mathcal{L}_ + } = {{{\rm{(1 - p)}}}^{{{\gamma }_{\rm{ + }}}}}{\rm{log(p)}}} \\{{\mathcal{L}_ - } = {{{\rm{(}}{{\rm{p}}_{\rm{m}}}{\rm{)}}}^{{{\gamma }_ - }}}{\rm{log(1 - }}{{\rm{p}}_{\rm{m}}}{\rm{)}}} \\
\end{matrix} }\right.$$\end{document}</tex-math></disp-formula><disp-formula id="M0007"><label>(7)</label><tex-math notation="LaTeX" id="M0007-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${{\rm{p}}_{\rm{m}}}{\rm{ = max(p - m, 0)}}$$\end{document}</tex-math></disp-formula></p>
      <p>where p = σ(z) is the confidence probability predicted by the network, p<sub>m</sub> is the shifted probability and γ<sub>+</sub> and γ- are the asymmetric focusing weights decaying the contribution of both positive and negative samples, respectively. The probability margin m ≥ 0 is a tunable hyper-parameter that controls the acceptance of the negative predictions.</p>
      <p>It is observed that ASL decouples the focusing levels of positive and negative samples through different decay rates γ<sub>+</sub> and γ-. When dealing with multi-label training, higher values of decay rates are able to sufficiently downweight the contribution from easy samples (<xref rid="R14" ref-type="bibr">14</xref>). Specifically, since the number of negative samples is much larger than the positive ones in the corpus, by setting γ<sub>−</sub> &gt; 0 in <xref rid="M0006" ref-type="disp-formula">Equation (6)</xref>, the contribution of easy negatives (with low probability, p ≪ 0.5) to the accumulated loss can be significantly downweighted, enabling the model to focus more on the harder samples during training. Moreover, the additional asymmetric mechanism of probability shifting also performs hard thresholding for the easy negative samples, i.e. it will fully discard the negative samples via a flexible probability margin <italic toggle="yes">m</italic> when their predicted confidence is very low. In short, through asymmetric focusing and probability shifting, ASL is able to obtain better control over the contributions of positive and negative samples to the loss function and help the model to learn meaningful features from positive samples despite their less frequent occurrences.</p>
      <p>It is worth noting that when γ<sub>+</sub> = γ- = p<sub>m</sub> = 0, ASL degenerates to the classic BCE loss. Since we are more interested in highlighting the contribution of positive samples, we set γ<sub>−</sub> &gt; γ<sub>+</sub> for experimentation. It can be convenient to set γ<sub>+</sub> = 0 so that the positive samples will incur the same cross-entropy loss; meanwhile, the model only needs to control the level of the asymmetrically negative part via the hyper-parameter γ<sub>−</sub>. In other words, the model is able to focus on learning features from difficult samples while de-emphasizing the features from the easy ones.</p>
      <p>Benefiting from the effectiveness of ASL, the issue of imbalanced label distribution is lessened and the entire framework of LCEL is trained in an end-to-end fashion by a gradient-based optimization algorithm that minimizes the total loss of <inline-formula id="ILM0011"><tex-math notation="LaTeX" id="ILM0011-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\mathcal{L}_{{\rm{all}}}}$\end{document}</tex-math></inline-formula>.</p>
    </sec>
  </sec>
  <sec id="s6">
    <title>Results</title>
    <p>In this section, we first introduce the evaluation metrics and the experimental settings for the LitCovid multi-label classification problem; we then systematically evaluate the performance of our approach and compare it with the relevant state-of-the-art systems. Finally, the error analysis is carried out at the end of this section.</p>
    <sec id="s6-s1">
      <title>Evaluation metrics</title>
      <p>Amid the LitCovid online competition, all submissions were evaluated from the label-based perspective and the instance-based perspective, both of which are widely utilized for multi-label classification. Specifically, nine different measures at three different levels are applied, i.e. example-based precision (EBP), example-based recall (EBR), example-based F1 (EBF), macro-precision (MaP), macro-recall (MaR), macro-F1 (MaF), micro-precision (MiP), micro-recall (MiR) and micro-F1 (MiF).</p>
      <p>Let <italic toggle="yes">K</italic> denote the total number of all topic labels, and <italic toggle="yes">N</italic> denote the number of the input instances (i.e. biomedical articles). y<sub>i</sub> and <inline-formula id="ILM0012"><tex-math notation="LaTeX" id="ILM0012-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\hat {\textrm{\textbf{y}}}_{\rm{i}}} \in {\left\{ {0,1} \right\}^{\rm{K}}}$\end{document}</tex-math></inline-formula> are the true and predicted labels for instance i, respectively. The foregoing evaluation metrics are further defined as follows:</p>
      <list list-type="roman-lower">
        <list-item>
          <p>EBF:</p>
        </list-item>
      </list>
      <p>EBF is utilized to evaluate the system performance at the instance level, and it can be computed by the harmonic mean of EBP and EBR, as follows:
<disp-formula id="M0008"><label>(8)</label><tex-math notation="LaTeX" id="M0008-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${\rm{EBF = }}{{\rm{1}} \over {\rm{N}}}\mathop \sum \limits_{{\rm{i = 1}}}^{\rm{N}} {\rm{EB}}{{\rm{F}}_{\rm{i}}}$$\end{document}</tex-math></disp-formula></p>
      <p>where
<disp-formula id="M0009"><label>(9)</label><tex-math notation="LaTeX" id="M0009-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${\rm{EB}}{{\rm{F}}_{\rm{i}}}{\rm{ = }}{{{\rm{2}} \cdot {\rm{EB}}{{\rm{P}}_{\rm{i}}} \cdot {\rm{EB}}{{\rm{R}}_{\rm{i}}}} \over {{\rm{EB}}{{\rm{P}}_{\rm{i}}}{\rm{ + EB}}{{\rm{R}}_{\rm{i}}}}}$$\end{document}</tex-math></disp-formula></p>
      <p>where</p>
      <p>
        <disp-formula id="UM0001">
          <tex-math notation="LaTeX" id="UM0001-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${\rm{EB}}{{\rm{P}}_{\rm{i}}}{\rm{ = }}{{\mathop \sum \nolimits_{{\rm{k = 1}}}^{\rm{K}} {\rm{y}}_{\rm{i}}^{\rm{k}} \cdot {\hat {\rm{y}}}_{\rm{i}}^{\rm{k}}} \over {\mathop \sum \nolimits_{{\rm{k = 1}}}^{\rm{K}} {\hat {\rm{y}}}_{\rm{i}}^{\rm{k}}}} \quad\quad {\rm{EB}}{{\rm{R}}_{\rm{i}}} = {{\mathop \sum \nolimits_{{\rm{k = 1}}}^{\rm{K}} {\rm{y}}_{\rm{i}}^{\rm{k}} \cdot {\hat {\rm{y}}}_{\rm{i}}^{\rm{k}}} \over {\mathop \sum \nolimits_{{\rm{k = 1}}}^{\rm{K}} {\rm{y}}_{\rm{i}}^{\rm{k}}}}$$\end{document}</tex-math>
        </disp-formula>
      </p>
      <p>Note that EBP and EBR are calculated by summing EBP<sub>i</sub> and EBR<sub>i</sub> over all instances, respectively.</p>
      <list list-type="roman-lower">
        <list-item>
          <p>MaF</p>
        </list-item>
      </list>
      <p>MaF is utilized to evaluate the system performance at the macro level of labels. In MaF, all the labels are treated equally regardless of their distribution. MaF can be computed by the harmonic mean of MaP and MaR, as follows:
<disp-formula id="M0010"><label>(10)</label><tex-math notation="LaTeX" id="M0010-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${\rm{MaF = }}{{{\rm{2}}\! \cdot\! {\rm{MaP}}\! \cdot\! {\rm{MaR}}} \over {{\rm{MaP\! +\! MaR}}}}$$\end{document}</tex-math></disp-formula></p>
      <p>The MaP and MaR are obtained by computing the precision and recall for each label separately and then averaging them over all labels, as follows:</p>
      <p>
        <disp-formula id="UM0002">
          <tex-math notation="LaTeX" id="UM0002-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${\rm{MaP = }}{{\rm{1}} \over {\rm{K}}}\mathop \sum \limits_{{\rm{k = 1}}}^{\rm{K}} {{\rm{P}}^{\rm{k}}} \quad\quad {\rm{MaR = }}{{\rm{1}} \over {\rm{K}}}\mathop \sum \limits_{{\rm{k = 1}}}^{\rm{K}} {{\rm{R}}^{\rm{k}}}$$\end{document}</tex-math>
        </disp-formula>
      </p>
      <p>where</p>
      <p>
        <disp-formula id="UM0003">
          <tex-math notation="LaTeX" id="UM0003-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${{\rm{P}}^{\rm{k}}} = {{\mathop \sum \nolimits_{{\rm{i = 1}}}^{\rm{N}} {\rm{y}}_{\rm{i}}^{\rm{k}} \cdot {\hat {\rm{y}}}_{\rm{i}}^{\rm{k}}} \over {\mathop \sum \nolimits_{{\rm{i = 1}}}^{\rm{N}} {\hat {\rm{y}}}_{\rm{i}}^{\rm{k}}}} \quad\quad {{\rm{R}}^{\rm{k}}} = {{\mathop \sum \nolimits_{{\rm{i = 1}}}^{\rm{N}} {\rm{y}}_{\rm{i}}^{\rm{k}} \cdot {\hat {\rm{y}}}_{\rm{i}}^{\rm{k}}} \over {\mathop \sum \nolimits_{{\rm{i = 1}}}^{\rm{N}} {\rm{y}}_{\rm{i}}^{\rm{k}}}}$$\end{document}</tex-math>
        </disp-formula>
      </p>
      <list list-type="roman-lower">
        <list-item>
          <p>MiF</p>
        </list-item>
      </list>
      <p>MiF is utilized to evaluate the system performance at the micro level of labels. In MiF, the distribution of each label is taken into consideration; the labels with larger counts exert more influence on the final results during the calculation. MiF can be computed by the harmonic mean of MiP and MiR, as follows:
<disp-formula id="M0011"><label>(11)</label><tex-math notation="LaTeX" id="M0011-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${\rm{MiF = }}{{{\rm{2}}\! \cdot\! {\rm{MiP}}\! \cdot\! {\rm{MiR}}} \over {{\rm{MiP\! +\! MiR}}}}$$\end{document}</tex-math></disp-formula></p>
      <p>where</p>
      <p>
        <disp-formula id="UM0004">
          <tex-math notation="LaTeX" id="UM0004-Latex">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${\rm{MiP}} = {{\mathop \sum \nolimits_{{\rm{K = 1}}}^{\rm{K}} \mathop \sum \nolimits_{{\rm{i = 1}}}^{\rm{N}} {\rm{y}}_{\rm{i}}^{\rm{k}} \cdot {\hat y}_{\rm{i}}^{\rm{k}}} \over {\mathop \sum \nolimits_{{\rm{K = 1}}}^{\rm{K}} \mathop \sum \nolimits_{{\rm{i = 1}}}^{\rm{N}} {\hat y}_{\rm{i}}^{\rm{k}}}} \quad\quad {\rm{MiR}} = {{\mathop \sum \nolimits_{{\rm{K = 1}}}^{\rm{K}} \mathop \sum \nolimits_{{\rm{i = 1}}}^{\rm{N}} {\rm{y}}_{\rm{i}}^{\rm{k}} \cdot {\hat y}_{\rm{i}}^{\rm{k}}} \over {\mathop \sum \nolimits_{{\rm{K = 1}}}^{\rm{K}} \mathop \sum \nolimits_{{\rm{i = 1}}}^{\rm{N}} {\rm{y}}_{\rm{i}}^{\rm{k}}}}$$\end{document}</tex-math>
        </disp-formula>
      </p>
    </sec>
    <sec id="s6-s2">
      <title>Experimental settings</title>
      <p>In our experiments, all texts of articles and additional biomedical knowledge are converted into lower cases before being fed into the downstream deep neural networks. In case some article texts might exceed the length limitations of pretrained models, the overlong texts of biomedical articles are truncated to substitute the original ones. In our experiments, the maximum length of input texts is fixed to 512, and the training batch size is set to 50. As multiple state-of-the-art pretrained models are explored in LCEL, all parameters follow their default settings during the model initialization. In the training phase, the AdamW optimizer (<xref rid="R38" ref-type="bibr">38</xref>) is adopted to minimize the training loss, and the learning rates are kept identically for all models with the value of 2e-5.</p>
      <p>Regarding the loss function of ASL, since we are more interested in emphasizing the contributions of positive samples, we empirically set γ<sub>−</sub> &gt; γ<sub>+</sub> to explicitly minimize the weights of negative samples. In particular, γ<sub>−</sub> and γ<sub>+</sub> are separately assigned to 1 and 0. To simplify the experimentations, the hyper-parameter of the probability margin <italic toggle="yes">m</italic> follows the default settings as mentioned in the study by Ben-Baruch <italic toggle="yes">et al</italic>. (<xref rid="R14" ref-type="bibr">14</xref>), which equals 0.05. In terms of data augmentation, an additional 96 804 COVID-19 relevant articles are collected from the online hub (<xref rid="R11" ref-type="bibr">11</xref>) on 25 January 2022, excluding the overlaps with the original LitCovid dataset.</p>
      <p>The total number of fine-tuning steps for each pretrained model is set to 16 000, and each checkpoint per 500 training steps is reserved for further evaluation and integration. The best-performed checkpoints of each pretrained model will be ensembled for the final topic prediction.</p>
    </sec>
    <sec id="s6-s3">
      <title>System performance on the development dataset</title>
      <p>Our experimental results on the LitCovid development dataset are presented in the following order:</p>
      <list list-type="roman-lower">
        <list-item>
          <p>Evaluation with different biomedical knowledge on the LitCovid development dataset.</p>
        </list-item>
        <list-item>
          <p>Evaluation with ASL and the performance comparison with BCE loss.</p>
        </list-item>
        <list-item>
          <p>Evaluation with data augmentation and the overall comparison with different training policies.</p>
        </list-item>
      </list>
      <sec id="s6-s3-s1">
        <title>System performance with different knowledge features</title>
        <p>Following the previous work (<xref rid="R13" ref-type="bibr">13</xref>), three distinctive kinds of additional biomedical knowledge are proposed to enhance the feature representations for LCEL. To investigate the importance of the proposed biomedical knowledge, a detailed feature combination study is performed on the LitCovid development dataset, trying to reveal their different influences. Since seven advanced pretrained models are proposed for the ensemble learning of LCEL, the naive yet effective pretrained model of PubMedBERT (<xref rid="R28" ref-type="bibr">28</xref>) is selected to simplify the experimental comparison. The feature combination study follows the same training scenario described in the study by Gu <italic toggle="yes">et al</italic>. (<xref rid="R13" ref-type="bibr">13</xref>), i.e. it only utilizes the default LitCovid training dataset as well as BCE loss for fine-tuning. <xref rid="T4" ref-type="table">Table 4</xref> depicts the details of the knowledge combination experiments, in which the best scores are highlighted in boldface. It is worth mentioning that all experiments rely on the fundamental texts (i.e. titles and abstracts) of the input articles, which are available for all kinds of trials.</p>
        <table-wrap position="float" id="T4">
          <label>Table 4.</label>
          <caption>
            <p>The knowledge combination experiments on the development dataset</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th valign="bottom" align="left" rowspan="1" colspan="1">Additional biomedical knowledge</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">MaP (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">MaR (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">MaF (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">MiP (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">MiR (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">MiF (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">EBP (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">EBR (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">EBF (%)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">Text + Keywords + MeSH + Journal name</td>
                <td align="left" rowspan="1" colspan="1">84.71</td>
                <td align="left" rowspan="1" colspan="1">87.25</td>
                <td align="left" rowspan="1" colspan="1">85.93</td>
                <td align="left" rowspan="1" colspan="1">89.24</td>
                <td align="left" rowspan="1" colspan="1">90.96</td>
                <td align="left" rowspan="1" colspan="1">90.09</td>
                <td align="left" rowspan="1" colspan="1">92.02</td>
                <td align="left" rowspan="1" colspan="1">93.20</td>
                <td align="left" rowspan="1" colspan="1">92.61</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Text + Keywords + MeSH</td>
                <td align="left" rowspan="1" colspan="1">85.73</td>
                <td align="left" rowspan="1" colspan="1">86.34</td>
                <td align="left" rowspan="1" colspan="1">86.02</td>
                <td align="left" rowspan="1" colspan="1">90.19</td>
                <td align="left" rowspan="1" colspan="1">90.12</td>
                <td align="left" rowspan="1" colspan="1">90.16</td>
                <td align="left" rowspan="1" colspan="1">92.37</td>
                <td align="left" rowspan="1" colspan="1">92.58</td>
                <td align="left" rowspan="1" colspan="1">92.47</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Text + Keywords</td>
                <td align="left" rowspan="1" colspan="1">85.80</td>
                <td align="left" rowspan="1" colspan="1">85.79</td>
                <td align="left" rowspan="1" colspan="1">85.76</td>
                <td align="left" rowspan="1" colspan="1">90.34</td>
                <td align="left" rowspan="1" colspan="1">89.69</td>
                <td align="left" rowspan="1" colspan="1">90.01</td>
                <td align="left" rowspan="1" colspan="1">92.37</td>
                <td align="left" rowspan="1" colspan="1">92.26</td>
                <td align="left" rowspan="1" colspan="1">92.31</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Text</td>
                <td align="left" rowspan="1" colspan="1">85.14</td>
                <td align="left" rowspan="1" colspan="1">84.17</td>
                <td align="left" rowspan="1" colspan="1">84.60</td>
                <td align="left" rowspan="1" colspan="1">90.10</td>
                <td align="left" rowspan="1" colspan="1">89.22</td>
                <td align="left" rowspan="1" colspan="1">89.66</td>
                <td align="left" rowspan="1" colspan="1">92.09</td>
                <td align="left" rowspan="1" colspan="1">91.83</td>
                <td align="left" rowspan="1" colspan="1">91.96</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>It is observed from <xref rid="T4" ref-type="table">Table 4</xref> that, by merely using the contextual information of titles and abstracts, PubMedBERT (<xref rid="R28" ref-type="bibr">28</xref>) is able to achieve an MaF as high as 84.60%, an MiF as high as 89.66% and an EBF as high as 91.96%, respectively. This indicates that the contextual information of titles and abstracts inherently contains crucial clues for COVID-19 multi-label classification, and the pretrained model can effectively capture and represent such useful information. When successively combing the contexts with the biomedical knowledge of keywords and MeSH terms, the performance is further improved and reaches an MaF of 86.02%, an MiF of 90.16% and an EBF of 92.47%, respectively. This suggests that these distinctive kinds of biomedical knowledge can significantly provide supplementary semantic information for COVID-19–relevant topics, which have been manually refined and interpreted by authors and curators. Interestingly, with further combing of the knowledge of journal names, the model is able to obtain the highest EBF of 92.61%, while losing slight performance in MaF and MiF. This indicates that the journal name does bring certain background knowledge to the pretrained model; however, such biomedical knowledge is too general to help the model improve overall. Although there are some slight losses in MaF and MiF, the knowledge of journal names still helps the model perform consistently better than the one that exclusively uses the contexts of titles and abstracts. In this regard, we take all biomedical knowledge of keywords, MeSH terms and journal names as the default features for all pretrained models during our experimentation.</p>
      </sec>
      <sec id="s6-s3-s2">
        <title>System performance with ASL</title>
        <p>As ASL is proposed for better control of the contributions from both positive and negative samples, a fair comparison between ASL and BCE is conducted to better understand the difference and to demonstrate the influence of the asymmetric focusing mechanism. Similar to the aforementioned feature combination, the single model of PubMedBERT (<xref rid="R28" ref-type="bibr">28</xref>) on the LitCovid development dataset is evaluated to simplify the experimental comparison. <xref rid="T5" ref-type="table">Table 5</xref> reports the detailed performance of each label and their averaged results in micro, macro and example-based measures, respectively.</p>
        <table-wrap position="float" id="T5">
          <label>Table 5.</label>
          <caption>
            <p>The comparison of different loss functions</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th valign="bottom" align="left" rowspan="1" colspan="1">Loss</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">Labels</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">P (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">R (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">F1 (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">Count</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">BCE</td>
                <td align="left" rowspan="1" colspan="1">Treatment</td>
                <td align="left" rowspan="1" colspan="1">89.74</td>
                <td align="left" rowspan="1" colspan="1">90.76</td>
                <td align="left" rowspan="1" colspan="1">90.25</td>
                <td align="left" rowspan="1" colspan="1">2207</td>
              </tr>
              <tr>
                <td rowspan="9" align="left" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">Diagnosis</td>
                <td align="left" rowspan="1" colspan="1">85.27</td>
                <td align="left" rowspan="1" colspan="1">90.62</td>
                <td align="left" rowspan="1" colspan="1">87.86</td>
                <td align="left" rowspan="1" colspan="1">1546</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Prevention</td>
                <td align="left" rowspan="1" colspan="1">94.06</td>
                <td align="left" rowspan="1" colspan="1">95.09</td>
                <td align="left" rowspan="1" colspan="1">94.58</td>
                <td align="left" rowspan="1" colspan="1">2750</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Mechanism</td>
                <td align="left" rowspan="1" colspan="1">88.86</td>
                <td align="left" rowspan="1" colspan="1">86.21</td>
                <td align="left" rowspan="1" colspan="1">87.51</td>
                <td align="left" rowspan="1" colspan="1">1073</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Transmission</td>
                <td align="left" rowspan="1" colspan="1">70.22</td>
                <td align="left" rowspan="1" colspan="1">74.61</td>
                <td align="left" rowspan="1" colspan="1">72.35</td>
                <td align="left" rowspan="1" colspan="1">256</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Epidemic Forecasting</td>
                <td align="left" rowspan="1" colspan="1">74.16</td>
                <td align="left" rowspan="1" colspan="1">80.73</td>
                <td align="left" rowspan="1" colspan="1">77.31</td>
                <td align="left" rowspan="1" colspan="1">192</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Case Report</td>
                <td align="left" rowspan="1" colspan="1">90.67</td>
                <td align="left" rowspan="1" colspan="1">92.74</td>
                <td align="left" rowspan="1" colspan="1">91.69</td>
                <td align="left" rowspan="1" colspan="1">482</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">macro avg</td>
                <td align="left" rowspan="1" colspan="1">84.71</td>
                <td align="left" rowspan="1" colspan="1">87.25</td>
                <td align="left" rowspan="1" colspan="1">85.93</td>
                <td align="left" rowspan="1" colspan="1">8506</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">micro avg</td>
                <td align="left" rowspan="1" colspan="1">89.24</td>
                <td align="left" rowspan="1" colspan="1">90.96</td>
                <td align="left" rowspan="1" colspan="1">90.09</td>
                <td align="left" rowspan="1" colspan="1">8506</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">example avg</td>
                <td align="left" rowspan="1" colspan="1">92.02</td>
                <td align="left" rowspan="1" colspan="1">93.20</td>
                <td align="left" rowspan="1" colspan="1">92.61</td>
                <td align="left" rowspan="1" colspan="1">6239</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">ASL</td>
                <td align="left" rowspan="1" colspan="1">Treatment</td>
                <td align="left" rowspan="1" colspan="1">89.08</td>
                <td align="left" rowspan="1" colspan="1">91.30</td>
                <td align="left" rowspan="1" colspan="1">90.18</td>
                <td align="left" rowspan="1" colspan="1">2207</td>
              </tr>
              <tr>
                <td rowspan="9" align="left" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">Diagnosis</td>
                <td align="left" rowspan="1" colspan="1">87.94</td>
                <td align="left" rowspan="1" colspan="1">90.56</td>
                <td align="left" rowspan="1" colspan="1">89.23</td>
                <td align="left" rowspan="1" colspan="1">1546</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Prevention</td>
                <td align="left" rowspan="1" colspan="1">93.92</td>
                <td align="left" rowspan="1" colspan="1">96.11</td>
                <td align="left" rowspan="1" colspan="1">95.00</td>
                <td align="left" rowspan="1" colspan="1">2750</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Mechanism</td>
                <td align="left" rowspan="1" colspan="1">88.39</td>
                <td align="left" rowspan="1" colspan="1">87.23</td>
                <td align="left" rowspan="1" colspan="1">87.80</td>
                <td align="left" rowspan="1" colspan="1">1073</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Transmission</td>
                <td align="left" rowspan="1" colspan="1">70.00</td>
                <td align="left" rowspan="1" colspan="1">68.36</td>
                <td align="left" rowspan="1" colspan="1">69.17</td>
                <td align="left" rowspan="1" colspan="1">256</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Epidemic Forecasting</td>
                <td align="left" rowspan="1" colspan="1">76.88</td>
                <td align="left" rowspan="1" colspan="1">79.69</td>
                <td align="left" rowspan="1" colspan="1">78.26</td>
                <td align="left" rowspan="1" colspan="1">192</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Case Report</td>
                <td align="left" rowspan="1" colspan="1">92.23</td>
                <td align="left" rowspan="1" colspan="1">91.08</td>
                <td align="left" rowspan="1" colspan="1">91.65</td>
                <td align="left" rowspan="1" colspan="1">482</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">macro avg</td>
                <td align="left" rowspan="1" colspan="1">85.49</td>
                <td align="left" rowspan="1" colspan="1">86.33</td>
                <td align="left" rowspan="1" colspan="1">85.90</td>
                <td align="left" rowspan="1" colspan="1">8506</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">micro avg</td>
                <td align="left" rowspan="1" colspan="1">89.70</td>
                <td align="left" rowspan="1" colspan="1">91.24</td>
                <td align="left" rowspan="1" colspan="1">90.47</td>
                <td align="left" rowspan="1" colspan="1">8506</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">example avg</td>
                <td align="left" rowspan="1" colspan="1">92.43</td>
                <td align="left" rowspan="1" colspan="1">93.43</td>
                <td align="left" rowspan="1" colspan="1">92.93</td>
                <td align="left" rowspan="1" colspan="1">6239</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>In <xref rid="T5" ref-type="table">Table 5</xref>, it is noticeable that when adopting the conventional BCE loss, the best performance of PubMedBERT (<xref rid="R28" ref-type="bibr">28</xref>), on average, achieves an MaF of 85.93%, an MiF of 90.09% and an EBF of 92.61%. This indicates that the conventional BCE loss is universal and robust enough to handle the multi-label classification problem, due to its equivalent weight estimation over different label distributions. In contrast, when using ASL, the performance surpasses the model with BCE loss by improvements of 0.38 units in MiF and 0.32 units in EBF. This suggests that ASL enables the model to decouple the modulations of positive and negative samples, and the asymmetric focusing mechanism helps the model to understand positive samples better. However, when applying ASL, the model suffers from a slight decline in MaF with 0.03 units lower than the one with BCE. This implies that, although the asymmetric focusing mechanism concentrates more on the low-distributional positive samples, the aggressive adjustments of the weight manipulation might harm the importance of certain labels.</p>
        <p>Specifically, by adopting BCE, the prediction of Prevention achieves the highest F1 score of 94.58%. The performance scores of Treatment, Diagnosis, Mechanism and Case Report are relatively close. Compared to the above-mentioned labels, predictions of Epidemic Forecasting and Transmission perform the worst. This is likely due to the label imbalance described in Section <xref rid="s4" ref-type="sec">Dataset</xref>, which implies that with fewer class examples, the model faces more difficulties during prediction. Benefiting from the asymmetric focusing mechanism, ASL successfully helps the model pay more attention to the labels of Diagnosis, Prevention, Mechanism and Epidemic Forecasting, gaining better performance in the multi-label classification.</p>
        <p>To understand the impacts of the exponential decay factors in the asymmetric focusing mechanism, we combine γ<sub>+</sub> and γ<sub>−</sub> with different values to verify their influences. Note that for each pair of decay factors, we fine-tune the pretrained model and save multiple checkpoints for evaluation as described in the Section <xref rid="s6-s2" ref-type="sec">Experimental Settings</xref>. <xref rid="F3" ref-type="fig">Figure 3</xref> illustrates the boxplots of the standardized five-number summary for all checkpoints of pretrained models when using different settings of γ<sub>+</sub> and γ<sub>−</sub>. Specifically, the minimum, the maximum, the sample median and the first and third quartiles are depicted in <xref rid="F3" ref-type="fig">Figure 3</xref>. During the comparison, we set γ<sub>+</sub> with the range from 0 to 2 and γ<sub>−</sub> with the range from 0 to 3, respectively. When tuning γ<sub>+</sub> and γ-, all other parameters of pretrained models remain the same.</p>
        <fig position="float" id="F3" fig-type="figure">
          <label>Figure 3.</label>
          <caption>
            <p>The comparison of ASL with different hyper-parameters.</p>
          </caption>
          <graphic xlink:href="baac103f3" position="float"/>
        </fig>
        <p>It is observed from <xref rid="F3" ref-type="fig">Figure 3</xref> that in all experimental trials, the best-performing checkpoint is gained when adopting the combination of the decay factors with γ<sub>+</sub> = 0 and γ- = 1 (i.e. ASL_0_1). This can be explained by the advantages of the asymmetric focusing mechanism, which focuses more on the positive samples while attenuating the importance of negative ones. Interestingly, when simply applying γ<sub>+</sub> = 0 and γ- = 0 (i.e. ASL_0_0), the best checkpoint of the model also exhibits comparable performance, which implies that the equal treatment of both positive and negative samples is as effective as the inherent assumption of BCE.</p>
        <p>However, when γ<sub>+</sub> is fixed to 0 and γ- is above 1, the pretrained models perform much worse. This is likely due to excessive downweighting of γ-, which may lead to too much disregard for negative samples, losing the necessary semantic information for the models. It is also noticeable that as the asymmetric decay weight γ- becomes higher, the prediction variance of the model also increases. This may further support the importance of keeping modest magnitudes of the contributions from both positive and negative samples. Moreover, allowing γ<sub>+</sub> &gt; 0, all the pretrained models achieve suboptimal performance, demonstrating that too much attenuation on the positive samples cannot provide more meaningful clues for further improvement.</p>
      </sec>
      <sec id="s6-s3-s3">
        <title>System performance with data augmentation</title>
        <p>Likewise, data augmentation is another approach proposed in LCEL that aims at benefiting the representation capabilities of pretrained models. To investigate the importance of corresponding contributions of data augmentation, we experiment with different training policies and compare their results, as seen in <xref rid="T6" ref-type="table">Table 6</xref>. One of the key claims is that data augmentation is able to provide meaningful background information that is crucial for COVID-19 multi-label classification. To verify the assumption, <xref rid="T6" ref-type="table">Table 6</xref> exhibits the details of the experiments with different training policies.</p>
        <table-wrap position="float" id="T6">
          <label>Table 6.</label>
          <caption>
            <p>The overall comparison of data augmentation with different loss functions</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
              <col align="left" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th valign="bottom" align="left" rowspan="1" colspan="1">Policy</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">Model</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">MaP (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">MaR (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">MaF (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">MiP (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">MiR (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">MiF (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">EBP (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">EBR (%)</th>
                <th valign="bottom" align="left" rowspan="1" colspan="1">EBF (%)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">Train_Def + BCE</td>
                <td align="left" rowspan="1" colspan="1">PubMedBERT</td>
                <td align="left" rowspan="1" colspan="1">84.71</td>
                <td align="left" rowspan="1" colspan="1">87.25</td>
                <td align="left" rowspan="1" colspan="1">85.93</td>
                <td align="left" rowspan="1" colspan="1">89.24</td>
                <td align="left" rowspan="1" colspan="1">90.96</td>
                <td align="left" rowspan="1" colspan="1">90.09</td>
                <td align="left" rowspan="1" colspan="1">92.02</td>
                <td align="left" rowspan="1" colspan="1">93.20</td>
                <td align="left" rowspan="1" colspan="1">92.61</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">CovidBERT</td>
                <td align="left" rowspan="1" colspan="1">84.65</td>
                <td align="left" rowspan="1" colspan="1">85.23</td>
                <td align="left" rowspan="1" colspan="1">84.86</td>
                <td align="left" rowspan="1" colspan="1">88.51</td>
                <td align="left" rowspan="1" colspan="1">90.45</td>
                <td align="left" rowspan="1" colspan="1">89.47</td>
                <td align="left" rowspan="1" colspan="1">91.30</td>
                <td align="left" rowspan="1" colspan="1">92.68</td>
                <td align="left" rowspan="1" colspan="1">91.98</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioBERT-Large</td>
                <td align="left" rowspan="1" colspan="1">85.20</td>
                <td align="left" rowspan="1" colspan="1">78.01</td>
                <td align="left" rowspan="1" colspan="1">80.71</td>
                <td align="left" rowspan="1" colspan="1">88.39</td>
                <td align="left" rowspan="1" colspan="1">87.54</td>
                <td align="left" rowspan="1" colspan="1">87.96</td>
                <td align="left" rowspan="1" colspan="1">90.34</td>
                <td align="left" rowspan="1" colspan="1">90.23</td>
                <td align="left" rowspan="1" colspan="1">90.28</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioBERT-Base</td>
                <td align="left" rowspan="1" colspan="1">84.12</td>
                <td align="left" rowspan="1" colspan="1">82.09</td>
                <td align="left" rowspan="1" colspan="1">82.71</td>
                <td align="left" rowspan="1" colspan="1">88.56</td>
                <td align="left" rowspan="1" colspan="1">88.28</td>
                <td align="left" rowspan="1" colspan="1">88.42</td>
                <td align="left" rowspan="1" colspan="1">91.02</td>
                <td align="left" rowspan="1" colspan="1">90.87</td>
                <td align="left" rowspan="1" colspan="1">90.94</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioM-ELECTRA</td>
                <td align="left" rowspan="1" colspan="1">86.45</td>
                <td align="left" rowspan="1" colspan="1">83.76</td>
                <td align="left" rowspan="1" colspan="1">85.02</td>
                <td align="left" rowspan="1" colspan="1">89.68</td>
                <td align="left" rowspan="1" colspan="1">90.04</td>
                <td align="left" rowspan="1" colspan="1">89.86</td>
                <td align="left" rowspan="1" colspan="1">92.32</td>
                <td align="left" rowspan="1" colspan="1">92.60</td>
                <td align="left" rowspan="1" colspan="1">92.46</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioELECTRA</td>
                <td align="left" rowspan="1" colspan="1">83.34</td>
                <td align="left" rowspan="1" colspan="1">85.77</td>
                <td align="left" rowspan="1" colspan="1">84.48</td>
                <td align="left" rowspan="1" colspan="1">88.59</td>
                <td align="left" rowspan="1" colspan="1">90.81</td>
                <td align="left" rowspan="1" colspan="1">89.68</td>
                <td align="left" rowspan="1" colspan="1">91.68</td>
                <td align="left" rowspan="1" colspan="1">93.09</td>
                <td align="left" rowspan="1" colspan="1">92.38</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioMed_RoBERTa</td>
                <td align="left" rowspan="1" colspan="1">85.18</td>
                <td align="left" rowspan="1" colspan="1">85.48</td>
                <td align="left" rowspan="1" colspan="1">85.32</td>
                <td align="left" rowspan="1" colspan="1">89.52</td>
                <td align="left" rowspan="1" colspan="1">90.87</td>
                <td align="left" rowspan="1" colspan="1">90.19</td>
                <td align="left" rowspan="1" colspan="1">92.19</td>
                <td align="left" rowspan="1" colspan="1">93.12</td>
                <td align="left" rowspan="1" colspan="1">92.65</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Train_Def + ASL</td>
                <td align="left" rowspan="1" colspan="1">PubMedBERT</td>
                <td align="left" rowspan="1" colspan="1">85.49</td>
                <td align="left" rowspan="1" colspan="1">86.33</td>
                <td align="left" rowspan="1" colspan="1">85.90</td>
                <td align="left" rowspan="1" colspan="1">89.70</td>
                <td align="left" rowspan="1" colspan="1">91.24</td>
                <td align="left" rowspan="1" colspan="1">90.47</td>
                <td align="left" rowspan="1" colspan="1">92.43</td>
                <td align="left" rowspan="1" colspan="1">93.43</td>
                <td align="left" rowspan="1" colspan="1">92.93</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">CovidBERT</td>
                <td align="left" rowspan="1" colspan="1">84.57</td>
                <td align="left" rowspan="1" colspan="1">85.17</td>
                <td align="left" rowspan="1" colspan="1">84.83</td>
                <td align="left" rowspan="1" colspan="1">88.95</td>
                <td align="left" rowspan="1" colspan="1">90.50</td>
                <td align="left" rowspan="1" colspan="1">89.72</td>
                <td align="left" rowspan="1" colspan="1">91.59</td>
                <td align="left" rowspan="1" colspan="1">92.81</td>
                <td align="left" rowspan="1" colspan="1">92.20</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioBERT-Large</td>
                <td align="left" rowspan="1" colspan="1">82.63</td>
                <td align="left" rowspan="1" colspan="1">81.58</td>
                <td align="left" rowspan="1" colspan="1">82.06</td>
                <td align="left" rowspan="1" colspan="1">88.11</td>
                <td align="left" rowspan="1" colspan="1">87.64</td>
                <td align="left" rowspan="1" colspan="1">87.88</td>
                <td align="left" rowspan="1" colspan="1">90.54</td>
                <td align="left" rowspan="1" colspan="1">90.61</td>
                <td align="left" rowspan="1" colspan="1">90.57</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioBERT-Base</td>
                <td align="left" rowspan="1" colspan="1">85.87</td>
                <td align="left" rowspan="1" colspan="1">86.32</td>
                <td align="left" rowspan="1" colspan="1">86.07</td>
                <td align="left" rowspan="1" colspan="1">90.27</td>
                <td align="left" rowspan="1" colspan="1">90.62</td>
                <td align="left" rowspan="1" colspan="1">90.44</td>
                <td align="left" rowspan="1" colspan="1">92.65</td>
                <td align="left" rowspan="1" colspan="1">93.04</td>
                <td align="left" rowspan="1" colspan="1">92.84</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioM-ELECTRA</td>
                <td align="left" rowspan="1" colspan="1">83.73</td>
                <td align="left" rowspan="1" colspan="1">87.06</td>
                <td align="left" rowspan="1" colspan="1">85.34</td>
                <td align="left" rowspan="1" colspan="1">88.92</td>
                <td align="left" rowspan="1" colspan="1">91.89</td>
                <td align="left" rowspan="1" colspan="1">90.38</td>
                <td align="left" rowspan="1" colspan="1">91.95</td>
                <td align="left" rowspan="1" colspan="1">93.92</td>
                <td align="left" rowspan="1" colspan="1">92.92</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioELECTRA</td>
                <td align="left" rowspan="1" colspan="1">83.67</td>
                <td align="left" rowspan="1" colspan="1">87.43</td>
                <td align="left" rowspan="1" colspan="1">85.47</td>
                <td align="left" rowspan="1" colspan="1">88.61</td>
                <td align="left" rowspan="1" colspan="1">91.49</td>
                <td align="left" rowspan="1" colspan="1">90.03</td>
                <td align="left" rowspan="1" colspan="1">91.70</td>
                <td align="left" rowspan="1" colspan="1">93.50</td>
                <td align="left" rowspan="1" colspan="1">92.59</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioMed_RoBERTa</td>
                <td align="left" rowspan="1" colspan="1">84.32</td>
                <td align="left" rowspan="1" colspan="1">87.27</td>
                <td align="left" rowspan="1" colspan="1">85.75</td>
                <td align="left" rowspan="1" colspan="1">88.75</td>
                <td align="left" rowspan="1" colspan="1">91.68</td>
                <td align="left" rowspan="1" colspan="1">90.19</td>
                <td align="left" rowspan="1" colspan="1">91.76</td>
                <td align="left" rowspan="1" colspan="1">93.69</td>
                <td align="left" rowspan="1" colspan="1">92.71</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Train _Aug + BCE</td>
                <td align="left" rowspan="1" colspan="1">PubMedBERT</td>
                <td align="left" rowspan="1" colspan="1">86.47</td>
                <td align="left" rowspan="1" colspan="1">86.62</td>
                <td align="left" rowspan="1" colspan="1">86.51</td>
                <td align="left" rowspan="1" colspan="1">90.38</td>
                <td align="left" rowspan="1" colspan="1">91.58</td>
                <td align="left" rowspan="1" colspan="1">90.98</td>
                <td align="left" rowspan="1" colspan="1">92.88</td>
                <td align="left" rowspan="1" colspan="1">93.74</td>
                <td align="left" rowspan="1" colspan="1">93.31</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">CovidBERT</td>
                <td align="left" rowspan="1" colspan="1">85.26</td>
                <td align="left" rowspan="1" colspan="1">86.62</td>
                <td align="left" rowspan="1" colspan="1">85.62</td>
                <td align="left" rowspan="1" colspan="1">89.74</td>
                <td align="left" rowspan="1" colspan="1">91.36</td>
                <td align="left" rowspan="1" colspan="1">90.54</td>
                <td align="left" rowspan="1" colspan="1">92.45</td>
                <td align="left" rowspan="1" colspan="1">93.48</td>
                <td align="left" rowspan="1" colspan="1">92.96</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioBERT-Large</td>
                <td align="left" rowspan="1" colspan="1">85.88</td>
                <td align="left" rowspan="1" colspan="1">81.02</td>
                <td align="left" rowspan="1" colspan="1">83.33</td>
                <td align="left" rowspan="1" colspan="1">90.54</td>
                <td align="left" rowspan="1" colspan="1">87.56</td>
                <td align="left" rowspan="1" colspan="1">89.03</td>
                <td align="left" rowspan="1" colspan="1">92.13</td>
                <td align="left" rowspan="1" colspan="1">90.64</td>
                <td align="left" rowspan="1" colspan="1">91.38</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioBERT-Base</td>
                <td align="left" rowspan="1" colspan="1">88.16</td>
                <td align="left" rowspan="1" colspan="1">85.14</td>
                <td align="left" rowspan="1" colspan="1">86.45</td>
                <td align="left" rowspan="1" colspan="1">90.98</td>
                <td align="left" rowspan="1" colspan="1">91.46</td>
                <td align="left" rowspan="1" colspan="1">91.22</td>
                <td align="left" rowspan="1" colspan="1">93.04</td>
                <td align="left" rowspan="1" colspan="1">93.49</td>
                <td align="left" rowspan="1" colspan="1">93.26</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioM-ELECTRA</td>
                <td align="left" rowspan="1" colspan="1">87.94</td>
                <td align="left" rowspan="1" colspan="1">85.39</td>
                <td align="left" rowspan="1" colspan="1">86.52</td>
                <td align="left" rowspan="1" colspan="1">90.70</td>
                <td align="left" rowspan="1" colspan="1">91.24</td>
                <td align="left" rowspan="1" colspan="1">90.97</td>
                <td align="left" rowspan="1" colspan="1">92.88</td>
                <td align="left" rowspan="1" colspan="1">93.43</td>
                <td align="left" rowspan="1" colspan="1">93.15</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioELECTRA</td>
                <td align="left" rowspan="1" colspan="1">84.73</td>
                <td align="left" rowspan="1" colspan="1">89.24</td>
                <td align="left" rowspan="1" colspan="1">86.83</td>
                <td align="left" rowspan="1" colspan="1">89.70</td>
                <td align="left" rowspan="1" colspan="1">92.05</td>
                <td align="left" rowspan="1" colspan="1">90.86</td>
                <td align="left" rowspan="1" colspan="1">92.21</td>
                <td align="left" rowspan="1" colspan="1">94.04</td>
                <td align="left" rowspan="1" colspan="1">93.12</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioMed_RoBERTa</td>
                <td align="left" rowspan="1" colspan="1">89.10</td>
                <td align="left" rowspan="1" colspan="1">83.62</td>
                <td align="left" rowspan="1" colspan="1">86.11</td>
                <td align="left" rowspan="1" colspan="1">91.76</td>
                <td align="left" rowspan="1" colspan="1">90.21</td>
                <td align="left" rowspan="1" colspan="1">90.98</td>
                <td align="left" rowspan="1" colspan="1">93.40</td>
                <td align="left" rowspan="1" colspan="1">92.54</td>
                <td align="left" rowspan="1" colspan="1">92.97</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Train <italic toggle="yes">_</italic>Aug + ASL</td>
                <td align="left" rowspan="1" colspan="1">PubMedBERT</td>
                <td align="left" rowspan="1" colspan="1">84.86</td>
                <td align="left" rowspan="1" colspan="1">87.93</td>
                <td align="left" rowspan="1" colspan="1">86.36</td>
                <td align="left" rowspan="1" colspan="1">89.75</td>
                <td align="left" rowspan="1" colspan="1">92.45</td>
                <td align="left" rowspan="1" colspan="1">91.08</td>
                <td align="left" rowspan="1" colspan="1">92.54</td>
                <td align="left" rowspan="1" colspan="1">94.34</td>
                <td align="left" rowspan="1" colspan="1">93.43</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">CovidBERT</td>
                <td align="left" rowspan="1" colspan="1">84.98</td>
                <td align="left" rowspan="1" colspan="1">87.10</td>
                <td align="left" rowspan="1" colspan="1">86.01</td>
                <td align="left" rowspan="1" colspan="1">89.68</td>
                <td align="left" rowspan="1" colspan="1">91.74</td>
                <td align="left" rowspan="1" colspan="1">90.70</td>
                <td align="left" rowspan="1" colspan="1">92.41</td>
                <td align="left" rowspan="1" colspan="1">93.79</td>
                <td align="left" rowspan="1" colspan="1">93.09</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioBERT-Large</td>
                <td align="left" rowspan="1" colspan="1">82.59</td>
                <td align="left" rowspan="1" colspan="1">86.73</td>
                <td align="left" rowspan="1" colspan="1">84.52</td>
                <td align="left" rowspan="1" colspan="1">87.27</td>
                <td align="left" rowspan="1" colspan="1">91.24</td>
                <td align="left" rowspan="1" colspan="1">89.21</td>
                <td align="left" rowspan="1" colspan="1">90.58</td>
                <td align="left" rowspan="1" colspan="1">93.28</td>
                <td align="left" rowspan="1" colspan="1">91.91</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioBERT-Base</td>
                <td align="left" rowspan="1" colspan="1">85.02</td>
                <td align="left" rowspan="1" colspan="1">88.11</td>
                <td align="left" rowspan="1" colspan="1">86.51</td>
                <td align="left" rowspan="1" colspan="1">89.69</td>
                <td align="left" rowspan="1" colspan="1">92.24</td>
                <td align="left" rowspan="1" colspan="1">90.95</td>
                <td align="left" rowspan="1" colspan="1">92.50</td>
                <td align="left" rowspan="1" colspan="1">94.13</td>
                <td align="left" rowspan="1" colspan="1">93.31</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioM-ELECTRA</td>
                <td align="left" rowspan="1" colspan="1">84.90</td>
                <td align="left" rowspan="1" colspan="1">88.56</td>
                <td align="left" rowspan="1" colspan="1">86.55</td>
                <td align="left" rowspan="1" colspan="1">89.80</td>
                <td align="left" rowspan="1" colspan="1">92.36</td>
                <td align="left" rowspan="1" colspan="1">91.06</td>
                <td align="left" rowspan="1" colspan="1">92.62</td>
                <td align="left" rowspan="1" colspan="1">94.16</td>
                <td align="left" rowspan="1" colspan="1">93.38</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioELECTRA</td>
                <td align="left" rowspan="1" colspan="1">84.92</td>
                <td align="left" rowspan="1" colspan="1">87.88</td>
                <td align="left" rowspan="1" colspan="1">86.36</td>
                <td align="left" rowspan="1" colspan="1">89.22</td>
                <td align="left" rowspan="1" colspan="1">92.65</td>
                <td align="left" rowspan="1" colspan="1">90.90</td>
                <td align="left" rowspan="1" colspan="1">92.12</td>
                <td align="left" rowspan="1" colspan="1">94.45</td>
                <td align="left" rowspan="1" colspan="1">93.27</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">BioMed_RoBERTa</td>
                <td align="left" rowspan="1" colspan="1">84.36</td>
                <td align="left" rowspan="1" colspan="1">89.25</td>
                <td align="left" rowspan="1" colspan="1">86.67</td>
                <td align="left" rowspan="1" colspan="1">87.90</td>
                <td align="left" rowspan="1" colspan="1">93.82</td>
                <td align="left" rowspan="1" colspan="1">90.76</td>
                <td align="left" rowspan="1" colspan="1">91.52</td>
                <td align="left" rowspan="1" colspan="1">95.31</td>
                <td align="left" rowspan="1" colspan="1">93.38</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">LCEL¬ (Train <italic toggle="yes">_</italic>Aug + ASL)</td>
                <td align="left" rowspan="1" colspan="1">Ensembled</td>
                <td align="left" rowspan="1" colspan="1">86.37</td>
                <td align="left" rowspan="1" colspan="1">88.45</td>
                <td align="left" rowspan="1" colspan="1">87.40</td>
                <td align="left" rowspan="1" colspan="1">90.53</td>
                <td align="left" rowspan="1" colspan="1">93.00</td>
                <td align="left" rowspan="1" colspan="1">91.75</td>
                <td align="left" rowspan="1" colspan="1">93.08</td>
                <td align="left" rowspan="1" colspan="1">94.76</td>
                <td align="left" rowspan="1" colspan="1">93.91</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>In <xref rid="T6" ref-type="table">Table 6</xref>, Train_Def stands for the models trained only using the LitCovid training dataset, while Train_Aug means the models trained with data augmentation. It is worth noticing that the training policy of ‘Train_Def + BCE’ is identical to the previous work (<xref rid="R13" ref-type="bibr">13</xref>), which was one of the top-ranked systems during the LitCovid online competition. In contrast, ‘Train_Aug + ASL’ stands for the training policy proposed for LCEL.</p>
        <p>As is shown in <xref rid="T6" ref-type="table">Table 6</xref>, for the training policy of ‘Train_Def + BCE’, it can be observed that all models have competitive performances with only slight differences due to their powerful feature representation abilities. This indicates that all pretrained models with biomedical knowledge can provide robust COVID-19–specific feature representations, which benefit the ultimate multi-label classification performance. In particular, PubMedBERT (<xref rid="R28" ref-type="bibr">28</xref>) acquires the highest MaF of 85.93%, while BioMed_RoBERTa (<xref rid="R33" ref-type="bibr">33</xref>) reports the best performance with an MiF of 90.19% and an EBF of 92.65%.</p>
        <p>Moreover, compared with ‘Train_Def + BCE’, the training policy of ‘Train_Def + ASL’ consistently improves the performance of BioBERT-Base (<xref rid="R30" ref-type="bibr">30</xref>), BioM-ELECTRA (<xref rid="R31" ref-type="bibr">31</xref>), BioELECTRA (<xref rid="R32" ref-type="bibr">32</xref>) and BioMed-RoBERTa (<xref rid="R33" ref-type="bibr">33</xref>). This suggests that the ASL enables the models to decouple the impacts of positive and negative samples and helps the models focus more on the positive ones, which benefits the overall multi-label classification. Although there are some slight declines in the MaF of PubMedBERT (<xref rid="R28" ref-type="bibr">28</xref>) and CovidBERT (<xref rid="R29" ref-type="bibr">29</xref>), and the MiF of BioBERT-Large (<xref rid="R30" ref-type="bibr">30</xref>), the other F-measures of these pretrained models are still boosted due to ASL.</p>
        <p>In contrast, when adopting data augmentation, the performance of the training policies based on Train_Aug significantly outperforms Train_Def. This indicates that inadequate training data make it difficult to learn the essential semantic representations, while data augmentation addresses the training deficiency effectively, which enables an overall improvement of the models. Regarding ‘Train_Aug + BCE’ and ‘Train_Aug + ASL’, both policies rival each other and exhibit competitive performance. This implies that a large number of external training data guarantee abundant priori semantic information, which provides a solid foundation for learning capability. Once adapted to the COVID-19 domain, the additional semantic information can help the pretrained models understand COVID-19–relevant topics better. After integrating all the fine-tuned pretrained models under the policy of ‘Train_Aug + ASL’, LCEL is able to obtain consistent superiority in all label-level and instance-based level F-measures, resulting in the highest MaF of 87.40%, MiF of 91.75% and EBF of 93.91%. In a word, the experimental results show the effectiveness of the proposed LCEL method, due to the efficient aggregation of multiple classifiers and ASL function.</p>
      </sec>
    </sec>
    <sec id="s6-s4">
      <title>System performance on the test dataset</title>
      <p>In the following section, a comprehensive comparison between the state-of-the-art systems (<xref rid="R13" ref-type="bibr">13</xref>, <xref rid="R22" ref-type="bibr">22–24</xref>, <xref rid="R39" ref-type="bibr">39</xref>) and LCEL is performed on the BioCreative VII LitCovid test dataset. Since there were up to 80 different valid predictions submitted to the challenge (<xref rid="R11" ref-type="bibr">11</xref>) during the online competition, for a fair comparison, the organizers implemented an official baseline system that utilized a shallow embedding-based machine learning approach, namely ML-Net (<xref rid="R39" ref-type="bibr">39</xref>). <xref rid="T7" ref-type="table">Table 7</xref> reports the official statistics of all submissions as well as the overall system comparison. The highest scores of F-measures are boldfaced in <xref rid="T7" ref-type="table">Table 7</xref>.</p>
      <table-wrap position="float" id="T7">
        <label>Table 7.</label>
        <caption>
          <p>The comparison of system performance on the test dataset</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th valign="bottom" align="left" rowspan="1" colspan="1">Team submission stats</th>
              <th valign="bottom" align="left" rowspan="1" colspan="1">MaP (%)</th>
              <th valign="bottom" align="left" rowspan="1" colspan="1">MaR (%)</th>
              <th valign="bottom" align="left" rowspan="1" colspan="1">MaF (%)</th>
              <th valign="bottom" align="left" rowspan="1" colspan="1">MiP (%)</th>
              <th valign="bottom" align="left" rowspan="1" colspan="1">MiR (%)</th>
              <th valign="bottom" align="left" rowspan="1" colspan="1">MiF (%)</th>
              <th valign="bottom" align="left" rowspan="1" colspan="1">EBP (%)</th>
              <th valign="bottom" align="left" rowspan="1" colspan="1">EBR (%)</th>
              <th valign="bottom" align="left" rowspan="1" colspan="1">EBF (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">Mean</td>
              <td align="left" rowspan="1" colspan="1">86.70</td>
              <td align="left" rowspan="1" colspan="1">80.12</td>
              <td align="left" rowspan="1" colspan="1">81.91</td>
              <td align="left" rowspan="1" colspan="1">89.67</td>
              <td align="left" rowspan="1" colspan="1">86.24</td>
              <td align="left" rowspan="1" colspan="1">87.78</td>
              <td align="left" rowspan="1" colspan="1">89.85</td>
              <td align="left" rowspan="1" colspan="1">88.87</td>
              <td align="left" rowspan="1" colspan="1">89.31</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Std</td>
              <td align="left" rowspan="1" colspan="1">6.09</td>
              <td align="left" rowspan="1" colspan="1">7.94</td>
              <td align="left" rowspan="1" colspan="1">7.01</td>
              <td align="left" rowspan="1" colspan="1">5.41</td>
              <td align="left" rowspan="1" colspan="1">4.82</td>
              <td align="left" rowspan="1" colspan="1">4.29</td>
              <td align="left" rowspan="1" colspan="1">5.21</td>
              <td align="left" rowspan="1" colspan="1">4.51</td>
              <td align="left" rowspan="1" colspan="1">4.60</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Q1</td>
              <td align="left" rowspan="1" colspan="1">84.63</td>
              <td align="left" rowspan="1" colspan="1">75.45</td>
              <td align="left" rowspan="1" colspan="1">76.51</td>
              <td align="left" rowspan="1" colspan="1">88.03</td>
              <td align="left" rowspan="1" colspan="1">84.52</td>
              <td align="left" rowspan="1" colspan="1">85.41</td>
              <td align="left" rowspan="1" colspan="1">86.99</td>
              <td align="left" rowspan="1" colspan="1">86.19</td>
              <td align="left" rowspan="1" colspan="1">86.68</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Median</td>
              <td align="left" rowspan="1" colspan="1">88.35</td>
              <td align="left" rowspan="1" colspan="1">83.83</td>
              <td align="left" rowspan="1" colspan="1">85.27</td>
              <td align="left" rowspan="1" colspan="1">91.08</td>
              <td align="left" rowspan="1" colspan="1">88.43</td>
              <td align="left" rowspan="1" colspan="1">89.25</td>
              <td align="left" rowspan="1" colspan="1">91.88</td>
              <td align="left" rowspan="1" colspan="1">90.97</td>
              <td align="left" rowspan="1" colspan="1">91.32</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Q3</td>
              <td align="left" rowspan="1" colspan="1">90.79</td>
              <td align="left" rowspan="1" colspan="1">85.55</td>
              <td align="left" rowspan="1" colspan="1">86.70</td>
              <td align="left" rowspan="1" colspan="1">92.51</td>
              <td align="left" rowspan="1" colspan="1">89.64</td>
              <td align="left" rowspan="1" colspan="1">90.83</td>
              <td align="left" rowspan="1" colspan="1">93.53</td>
              <td align="left" rowspan="1" colspan="1">91.92</td>
              <td align="left" rowspan="1" colspan="1">92.54</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Baseline (ML-Net)</td>
              <td align="left" rowspan="1" colspan="1">83.64</td>
              <td align="left" rowspan="1" colspan="1">73.09</td>
              <td align="left" rowspan="1" colspan="1">76.55</td>
              <td align="left" rowspan="1" colspan="1">87.56</td>
              <td align="left" rowspan="1" colspan="1">81.42</td>
              <td align="left" rowspan="1" colspan="1">84.37</td>
              <td align="left" rowspan="1" colspan="1">88.49</td>
              <td align="left" rowspan="1" colspan="1">85.14</td>
              <td align="left" rowspan="1" colspan="1">86.78</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Bioformer</td>
              <td align="left" rowspan="1" colspan="1">90.38</td>
              <td align="left" rowspan="1" colspan="1">88.23</td>
              <td align="left" rowspan="1" colspan="1">88.75</td>
              <td align="left" rowspan="1" colspan="1">93.67</td>
              <td align="left" rowspan="1" colspan="1">90.02</td>
              <td align="left" rowspan="1" colspan="1">91.81</td>
              <td align="left" rowspan="1" colspan="1">94.14</td>
              <td align="left" rowspan="1" colspan="1">92.56</td>
              <td align="left" rowspan="1" colspan="1">93.34</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DUT914</td>
              <td align="left" rowspan="1" colspan="1">87.78</td>
              <td align="left" rowspan="1" colspan="1">88.30</td>
              <td align="left" rowspan="1" colspan="1">87.60</td>
              <td align="left" rowspan="1" colspan="1">91.34</td>
              <td align="left" rowspan="1" colspan="1">92.17</td>
              <td align="left" rowspan="1" colspan="1">91.75</td>
              <td align="left" rowspan="1" colspan="1">93.50</td>
              <td align="left" rowspan="1" colspan="1">94.38</td>
              <td align="left" rowspan="1" colspan="1">93.94</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DonutNLP</td>
              <td align="left" rowspan="1" colspan="1">91.52</td>
              <td align="left" rowspan="1" colspan="1">85.66</td>
              <td align="left" rowspan="1" colspan="1">87.54</td>
              <td align="left" rowspan="1" colspan="1">93.43</td>
              <td align="left" rowspan="1" colspan="1">90.10</td>
              <td align="left" rowspan="1" colspan="1">91.74</td>
              <td align="left" rowspan="1" colspan="1">94.40</td>
              <td align="left" rowspan="1" colspan="1">92.54</td>
              <td align="left" rowspan="1" colspan="1">93.46</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">PolyU_CBSNLP</td>
              <td align="left" rowspan="1" colspan="1">91.39</td>
              <td align="left" rowspan="1" colspan="1">85.34</td>
              <td align="left" rowspan="1" colspan="1">87.49</td>
              <td align="left" rowspan="1" colspan="1">92.52</td>
              <td align="left" rowspan="1" colspan="1">90.29</td>
              <td align="left" rowspan="1" colspan="1">91.39</td>
              <td align="left" rowspan="1" colspan="1">93.78</td>
              <td align="left" rowspan="1" colspan="1">92.64</td>
              <td align="left" rowspan="1" colspan="1">93.21</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">LCEL</td>
              <td align="left" rowspan="1" colspan="1">89.79</td>
              <td align="left" rowspan="1" colspan="1">92.53</td>
              <td align="left" rowspan="1" colspan="1">90.94</td>
              <td align="left" rowspan="1" colspan="1">91.38</td>
              <td align="left" rowspan="1" colspan="1">94.75</td>
              <td align="left" rowspan="1" colspan="1">93.03</td>
              <td align="left" rowspan="1" colspan="1">93.49</td>
              <td align="left" rowspan="1" colspan="1">96.09</td>
              <td align="left" rowspan="1" colspan="1">94.77</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>As shown in <xref rid="T7" ref-type="table">Table 7</xref>, the official baseline system ML-Net (<xref rid="R39" ref-type="bibr">39</xref>) reaches decent achievements with an MaF of 76.55%, an MiF of 84.37% and an EBF of 86.78%. The baseline performance is quite close to the Q1 statistics for all three F-measures, suggesting that ∼75% of the team submissions have more promising results than the official baseline method. In contrast, the average MaF, MiF and EBF of all submissions are as high as 81.91%, 87.78% and 89.31%, respectively, all of which are better than the baseline scores. However, although most submissions outperform the baseline system, there are still relatively large standard deviations among the submissions, with 7.01% to MaF, 4.82% to MiF and 4.60% to EBF, respectively.</p>
      <p>Note that all four of the top-performing systems developed during the online competition, i.e. Bioformer (<xref rid="R24" ref-type="bibr">24</xref>), DonutNLP (<xref rid="R23" ref-type="bibr">23</xref>), DUT914 (<xref rid="R22" ref-type="bibr">22</xref>) and PolyU_CBSNLP (<xref rid="R13" ref-type="bibr">13</xref>), consistently achieved top-ranked performance in all three F-measures. Interestingly, all state-of-the-art systems (<xref rid="R13" ref-type="bibr">13</xref>, <xref rid="R22" ref-type="bibr">22–24</xref>), more or less, adopted ensemble learning technologies. Specifically, Bioformer (<xref rid="R24" ref-type="bibr">24</xref>) investigated multiple pretrained models including PubMedBERT (<xref rid="R28" ref-type="bibr">28</xref>) and BioBERT (<xref rid="R30" ref-type="bibr">30</xref>) for the LitCovid multi-label classification problem. To enhance the representation abilities, Bioformer (<xref rid="R24" ref-type="bibr">24</xref>) further proposed to exploit a larger external dataset to fine-tune the pretrained models and achieved the best performance on MaF and MiF with scores of 88.75% and 91.81%, respectively. Similarly, DUT914 (<xref rid="R22" ref-type="bibr">22</xref>) proposed to merge the multiple feature representations from different pretrained models of CovidBERT (<xref rid="R29" ref-type="bibr">29</xref>) and BioBERT (<xref rid="R30" ref-type="bibr">30</xref>) to capture the crucial semantic clues for the LitCovid Track. Due to the feature enrichment, DUT914 (<xref rid="R22" ref-type="bibr">22</xref>) obtained the best EBF of 93.94%, surpassing Bioformer (<xref rid="R24" ref-type="bibr">24</xref>) by 0.6 units. DonutNLP (<xref rid="R23" ref-type="bibr">23</xref>) was another top-ranked system during the online competition, which utilized a voting-based ensemble learning method to integrate multiple BioBERT (<xref rid="R30" ref-type="bibr">30</xref>) models. Although DonutNLP (<xref rid="R23" ref-type="bibr">23</xref>) did not achieve a performance as high as Bioformer (<xref rid="R24" ref-type="bibr">24</xref>) and DUT914 (<xref rid="R22" ref-type="bibr">22</xref>), it still acquired a comparable performance of MaF, MiF and EBF, with scores of 87.54%, 91.74% and 93.46%, respectively. Likewise, our previous work PolyU_CBSNLP (<xref rid="R13" ref-type="bibr">13</xref>) also proposed to ensemble multiple pretrained models to tackle the challenging task. However, different from DonutNLP (<xref rid="R23" ref-type="bibr">23</xref>), the pretrained models with heterogeneous architectures were mainly taken into consideration in PolyU_CBSNLP (<xref rid="R13" ref-type="bibr">13</xref>), and in total, seven advanced pretrained models were adopted accordingly. Although our previous work (<xref rid="R13" ref-type="bibr">13</xref>) did not outperform the top systems, it still rivaled these systems and reached promising results with an MaF of 87.49%, an MiF of 91.39% and an EBF of 93.21%.</p>
      <p>For the LCEL model, since there were seven different transformer-like pretrained models to be ensembled, during the training phase, only the models that performed the best on the development dataset were reserved for further integration. Compared with the above-mentioned state-of-the-art systems (<xref rid="R13" ref-type="bibr">13</xref>, <xref rid="R22" ref-type="bibr">22–24</xref>, <xref rid="R39" ref-type="bibr">39</xref>), LCEL consistently exhibits an overwhelming superiority in all F-measures, resulting in an MaF of 90.94, an MiF of 93.03 and an EBF of 94.77, respectively. Despite slightly lower precision, LCEL significantly improves the performance in all recall-based measures. This suggests the effectiveness of our proposed ensemble learning method. On the one hand, by adopting additional biomedical knowledge and data augmentation, LCEL is able to capture the supplementary semantic aspects related to COVID-19. On the other hand, benefiting from ASL, LCEL efficiently addresses the imbalanced label distribution, emphasizing more contributions to the positive samples. The experimental results illustrate the efficacy of the proposed ensemble learning method, which might lay the preliminary foundation for research in the COVID-19 domain.</p>
    </sec>
    <sec id="s6-s5">
      <title>Error analysis</title>
      <p>To investigate the challenging issues in practice and provide insights for future work, we analyzed the errors in detail and grouped the main possible reasons as follows:</p>
      <sec id="s6-s5-s1">
        <title>Implicit language expression</title>
        <p>The proposed LCEL model aims to grasp critical semantic clues from literature contexts; however, in some cases, if there is a lack of such explicit evidence clearly expressed in the input texts, it would be difficult for LCEL to determine the final topics. For instance, in the article PMID:34202160, our LCEL cannot recognize the true topic of ‘Treatment’as there are no such explicit semantic clues to support that topic.</p>
      </sec>
      <sec id="s6-s5-s2">
        <title>Contextual misunderstandings</title>
        <p>Since the topic prediction of our proposed ensemble learning method largely relies on the contextual information provided by the input literature, sometimes, certain meaningful and remarkable words or phrases will result in misunderstandings of LCEL. For instance, in the article PMID:34291812, the main content of the literature describes treatments related to a COVID-19 infection; however, as the remarkably indicative word ‘prevention’ explicitly occurs in the title, our LCEL still outputs the label of ‘Prevention’ incorrectly.</p>
      </sec>
      <sec id="s6-s5-s3">
        <title>Information deficiency</title>
        <p>As pretrained models always impose constraints on the length of input texts, the overlong articles will be truncated before being fed into the downstream deep neural networks. However, some information would be inevitably discarded during this aggressive preprocessing. This in turn could cause unexpected difficulties for the LCEL model to recommend labels. In this study, around 17.8% of articles were truncated and 15.1% of the informative text is inevitably lost during the process due to the fixed sequence length of 512. For instance, during the preprocessing, the text of the article PMID:34227364 is shortened and some crucial information is dropped in the process, leading to the failure of recognizing the correct topic of ‘Mechanism’.</p>
      </sec>
      <sec id="s6-s5-s4">
        <title>Predicting bias</title>
        <p>Despite applying ASL to tackle the imbalanced label distribution problem, LCEL is still prone to pay more attention to the dominant topics aggressively while disregarding the tail ones conservatively. For instance, the article PMID:34205856 carries relatively short texts with no more than 30 words in its title and abstract. In this article, even though there is no such sign of ‘Treatment’, our LCEL model still recommends that topic, incorrectly.</p>
      </sec>
      <sec id="s6-s5-s5">
        <title>Inconsistent annotation</title>
        <p>In our experiments, some results show that parts of the false-positive COVID-19 topics identified by LCEL might be true and perhaps should be annotated in the LitCovid corpus. Taking the articles PMID:34338124 and PMID:34208057 into consideration, our LCEL model recommends the topic of ‘Case Report’ for both of them; however, even if some strongly indicative words (e.g. case, report, etc.) occur multiple times in the titles and the abstracts, the label is not annotated as the ground-truth answer. This is probably because of the inherent annotation disagreements of the LitCovid biocuration.</p>
      </sec>
    </sec>
  </sec>
  <sec id="s7">
    <title>Conclusion and future work</title>
    <p>This research proposed a novel ensemble learning of LCEL for COVID-19 multi-label classification, which integrated multiple powerful biomedical pretrained models. Specifically, seven advanced pretrained models with heterogeneous architectures were selected for ensemble learning. To enhance the representation abilities of deep neural models, additional biomedical knowledge and data augmentation strategies were exploited to fully utilize the semantic expressions. In light of the imbalanced label distribution, a novel ASL function was introduced to the LCEL model, which explicitly adjusted the negative–positive importance by assigning different exponential decay factors. Benefiting from ASL, the proposed model was able to dynamically decouple the modulations of the positive and negative samples during the training phase and focused more on the positive samples, while decreasing the contribution of negative ones. The experimental results on the LitCovid dataset achieved state-of-the-art performance, demonstrating the effectiveness of our proposed method.</p>
    <p>Our research on the LitCovid dataset has exhibited promising results for the COVID-19 multi-label classification research. In future work, we will develop more advanced deep neural models with richer semantic features and sophisticated ensemble techniques to improve the current system for better performance.</p>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
  </ack>
  <sec sec-type="data-availability" id="s8">
    <title>Availability of data and materials</title>
    <p>The datasets underlying this article are available in the BioCreative VII LitCovid Track at <ext-link xlink:href="https://biocreative.bioinformatics.udel.edu/tasks/biocreative-vii/track-5/" ext-link-type="uri">https://biocreative.bioinformatics.udel.edu/tasks/biocreative-vii/track-5/</ext-link>. The codes of the proposed LCEL model are available at <ext-link xlink:href="https://github.com/JHnlp/LCEL" ext-link-type="uri">https://github.com/JHnlp/LCEL</ext-link>.</p>
  </sec>
  <sec id="s9">
    <title>Author contributions</title>
    <p>J.G. and E.C. conceived the study; J.G. performed the data collection, training, prediction and analysis; J.G. and X.W. redesigned the experiment and data analysis; J.G., E.C., L.Q., G.Z. and C.H. co-wrote the paper and are responsible for various sections of theoretical interpretations, respectively. All authors contributed to the revised and approved the final manuscript.</p>
  </sec>
  <sec id="s10">
    <title>Funding</title>
    <p>This research is supported by the research grants of The Hong Kong Polytechnic University Projects (#1-W182, #G-YW4H) and the National Natural Science Foundation of China (#61976147).</p>
  </sec>
  <sec id="s11">
    <title>Conflict of interest.</title>
    <p>None declared.</p>
  </sec>
  <sec id="s12">
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </sec>
  <sec id="s13">
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="R1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>L.L.</given-names></string-name>, <string-name><surname>Lo</surname><given-names>K.</given-names></string-name>, <string-name><surname>Chandrasekhar</surname><given-names>Y.</given-names></string-name></person-group><etal>et al.</etal> (<year>2020</year>) <article-title>CORD-19: the COVID-19 Open Research Dataset</article-title>. <source><italic toggle="yes">ArXiv Preprint</italic></source>, arXiv:2004.10706.</mixed-citation>
    </ref>
    <ref id="R2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esteva</surname><given-names>A.</given-names></string-name>, <string-name><surname>Anuprit</surname><given-names>K.</given-names></string-name>, <string-name><surname>Romain</surname><given-names>P.</given-names></string-name></person-group><etal>et al.</etal> (<year>2020</year>) <article-title>Co-search: COVID-19 information retrieval with semantic search, question answering, and abstractive summarization</article-title>. <source><italic toggle="yes">ArXiv Preprint</italic></source>, arXiv:2006.09595.</mixed-citation>
    </ref>
    <ref id="R3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Allot</surname><given-names>A.</given-names></string-name> and <string-name><surname>Lu</surname><given-names>Z.</given-names></string-name></person-group> (<year>2021</year>) <article-title>LitCovid: an open database of COVID-19 literature</article-title>. <source><italic toggle="yes">Nucleic Acids Research</italic></source>, <volume>49</volume>, <fpage>D1534</fpage>–<lpage>D1540</lpage>.<pub-id pub-id-type="pmid">33166392</pub-id></mixed-citation>
    </ref>
    <ref id="R4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Allot</surname><given-names>A.</given-names></string-name> and <string-name><surname>Lu</surname><given-names>Z.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Keep up with the latest coronavirus research</article-title>. <source><italic toggle="yes">Nature</italic></source>, <volume>579</volume>, <page-range>193</page-range>.</mixed-citation>
    </ref>
    <ref id="R5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Santus</surname><given-names>E.</given-names></string-name>, <string-name><surname>Marino</surname><given-names>N.</given-names></string-name>, <string-name><surname>Cirillo</surname><given-names>D.</given-names></string-name></person-group><etal>et al.</etal> (<year>2021</year>) <article-title>Artificial intelligence-aided precision medicine for COVID-19: strategic areas of research and development</article-title>. <source><italic toggle="yes">Journal of Medical Internet Research</italic></source>, <volume>23</volume>, <page-range>e22453</page-range>.</mixed-citation>
    </ref>
    <ref id="R6">
      <label>6.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Nentidis</surname><given-names>A.</given-names></string-name>, <string-name><surname>Krithara</surname><given-names>A.</given-names></string-name>, <string-name><surname>Bougiatiotis</surname><given-names>K.</given-names></string-name></person-group><etal>et al.</etal> (<year>2020</year>) <article-title>Overview of BioASQ 2020: the eighth BioASQ challenge on large-scale biomedical semantic indexing and question answering</article-title>. In: <italic toggle="yes">CLEF 2020</italic>. <publisher-name>Springer, Cham</publisher-name>, <conf-loc>Greece</conf-loc>, pp. <fpage>194</fpage>–<lpage>214</lpage>.</mixed-citation>
    </ref>
    <ref id="R7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>K.</given-names></string-name>, <string-name><surname>Peng</surname><given-names>S.</given-names></string-name>, <string-name><surname>Wu</surname><given-names>J.</given-names></string-name></person-group><etal>et al.</etal> (<year>2015</year>) <article-title>MeSHLabeler: improving the accuracy of large-scale MeSH indexing by integrating diverse evidence</article-title>. <source><italic toggle="yes">Bioinformatics</italic></source>, <volume>31</volume>, <fpage>i339</fpage>–<lpage>i347</lpage>.<pub-id pub-id-type="pmid">26072501</pub-id></mixed-citation>
    </ref>
    <ref id="R8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gu</surname><given-names>J.</given-names></string-name>, <string-name><surname>Qian</surname><given-names>L.</given-names></string-name> and <string-name><surname>Zhou</surname><given-names>G.</given-names></string-name></person-group> (<year>2016</year>) <article-title>Chemical-induced disease relation extraction with various linguistic features</article-title>. <source><italic toggle="yes">Database</italic></source>, <volume>2016</volume>, <page-range>baw042</page-range>.</mixed-citation>
    </ref>
    <ref id="R9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gu</surname><given-names>J.</given-names></string-name>, <string-name><surname>Sun</surname><given-names>F.</given-names></string-name>, <string-name><surname>Qian</surname><given-names>L.</given-names></string-name></person-group><etal>et al.</etal> (<year>2017</year>) <article-title>Chemical-induced disease relation extraction via convolutional neural network</article-title>. <source><italic toggle="yes">Database</italic></source>, <volume>2017</volume>, <page-range>bax024</page-range>.</mixed-citation>
    </ref>
    <ref id="R10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gu</surname><given-names>J.</given-names></string-name>, <string-name><surname>Sun</surname><given-names>F.</given-names></string-name>, <string-name><surname>Qian</surname><given-names>L.</given-names></string-name></person-group><etal>et al.</etal> (<year>2019</year>) <article-title>Chemical-induced disease relation extraction via attention-based distant supervision</article-title>. <source><italic toggle="yes">BMC Bioinformatics</italic></source>, <volume>20</volume>, <page-range>403</page-range>.</mixed-citation>
    </ref>
    <ref id="R11">
      <label>11.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Allot</surname><given-names>A.</given-names></string-name>, <string-name><surname>Leaman</surname><given-names>R.</given-names></string-name></person-group><etal>et al.</etal> (<year>2021</year>) <article-title>Overview of the BioCreative VII LitCovid Track: multi-label topic classification for COVID-19 literature annotation</article-title>. In: <italic toggle="yes">Proceedings of the seventh BioCreative challenge evaluation workshop</italic>. <publisher-name>BioCreative</publisher-name>, <conf-loc>Cecilia Arighi, University of Delaware, USA</conf-loc>, pp. <fpage>266</fpage>–<lpage>271</lpage>.</mixed-citation>
    </ref>
    <ref id="R12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Allot</surname><given-names>A.</given-names></string-name>, <string-name><surname>Leaman</surname><given-names>R.</given-names></string-name></person-group><etal>et al.</etal> (<year>2022</year>) <article-title>Multi-label classification for biomedical literature: an overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations</article-title>. <source><italic toggle="yes">Database</italic></source>, <volume>2022</volume>, <page-range>baac069</page-range>.</mixed-citation>
    </ref>
    <ref id="R13">
      <label>13.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Gu</surname><given-names>J.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X.</given-names></string-name>, <string-name><surname>Chersoni</surname><given-names>E.</given-names></string-name></person-group><etal>et al.</etal> (<year>2021</year>) <article-title>Team PolyU-CBSNLP at BioCreative-VII Litcovid Track: ensemble learning for COVID-19 multilabel classification</article-title>. In: <italic toggle="yes">Proceedings of the Seventh BioCreative Challenge Evaluation Workshop</italic>. <publisher-name>BioCreative</publisher-name>, <conf-loc>Cecilia Arighi, University of Delaware, USA</conf-loc>, pp. <fpage>326</fpage>–<lpage>331</lpage>.</mixed-citation>
    </ref>
    <ref id="R14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ben-Baruch</surname><given-names>E.</given-names></string-name>, <string-name><surname>Ridnik</surname><given-names>T.</given-names></string-name>, <string-name><surname>Zamir</surname><given-names>N.</given-names></string-name></person-group><etal>et al.</etal> (<year>2020</year>) <article-title>Asymmetric loss for multi-label classification</article-title>. <source><italic toggle="yes">ArXiv Preprint</italic></source>, arXiv:2009.14119.</mixed-citation>
    </ref>
    <ref id="R15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aronson</surname><given-names>A.</given-names></string-name>, <string-name><surname>Mork</surname><given-names>J.</given-names></string-name>, <string-name><surname>Gay</surname><given-names>C.</given-names></string-name></person-group><etal>et al.</etal> (<year>2004</year>) <article-title>The NLM indexing initiative’s medical text indexer</article-title>. <source><italic toggle="yes">Medinfo</italic></source>, <volume>107</volume>, <fpage>268</fpage>–<lpage>272</lpage>.</mixed-citation>
    </ref>
    <ref id="R16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dai</surname><given-names>S.</given-names></string-name>, <string-name><surname>You</surname><given-names>R.</given-names></string-name>, <string-name><surname>Lu</surname><given-names>Z.</given-names></string-name></person-group><etal>et al.</etal> (<year>2020</year>) <article-title>FullMeSH: improving large-scale MeSH indexing with full text</article-title>. <source><italic toggle="yes">Bioinformatics</italic></source>, <volume>36</volume>, <fpage>1533</fpage>–<lpage>1541</lpage>.<pub-id pub-id-type="pmid">31596475</pub-id></mixed-citation>
    </ref>
    <ref id="R17">
      <label>17.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Jin</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Dhingra</surname><given-names>B.</given-names></string-name>, <string-name><surname>Cohen</surname><given-names>W.</given-names></string-name></person-group><etal>et al.</etal> (<year>2018</year>) <article-title>AttentionMesh: simple, effective and interpretable automatic mesh indexer</article-title>. In: <italic toggle="yes">Proceedings of the 6th BioASQ Workshop A Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering</italic>. <publisher-name>Association for Computational Linguistics</publisher-name>, <conf-loc>Brussels, Belgium</conf-loc>, pp. <fpage>47</fpage>–<lpage>56</lpage>.</mixed-citation>
    </ref>
    <ref id="R18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xun</surname><given-names>G.</given-names></string-name>, <string-name><surname>Jha</surname><given-names>K.</given-names></string-name>, <string-name><surname>Yuan</surname><given-names>Y.</given-names></string-name></person-group><etal>et al.</etal> (<year>2019</year>) <article-title>MeSHProbeNet: a self-attentive probe net for MeSH indexing</article-title>. <source><italic toggle="yes">Bioinformatics</italic></source>, <volume>35</volume>, <fpage>3794</fpage>–<lpage>3802</lpage>.<pub-id pub-id-type="pmid">30851089</pub-id></mixed-citation>
    </ref>
    <ref id="R19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xun</surname><given-names>G.</given-names></string-name>, <string-name><surname>Jha</surname><given-names>K.</given-names></string-name> and <string-name><surname>Aidong</surname><given-names>Z.</given-names></string-name></person-group> (<year>2020</year>) <article-title>MeSHProbeNet-P: improving large-scale MeSH indexing with personalizable MeSH probes</article-title>. <source><italic toggle="yes">ACM Transactions on Knowledge Discovery from Data</italic></source>, <volume>15</volume>, <fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation>
    </ref>
    <ref id="R20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lipscomb</surname><given-names>C.</given-names></string-name></person-group> (<year>2000</year>) <article-title>Medical subject headings (MeSH)</article-title>. <source><italic toggle="yes">Bull Med Libr Assoc.</italic></source>, <volume>88</volume>, <fpage>265</fpage>–<lpage>266</lpage>.<pub-id pub-id-type="pmid">10928714</pub-id></mixed-citation>
    </ref>
    <ref id="R21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anastasios</surname><given-names>N.</given-names></string-name>, <string-name><surname>Georgios</surname><given-names>K.</given-names></string-name>, <string-name><surname>Eirini</surname><given-names>V.</given-names></string-name></person-group><etal>et al.</etal> (<year>2021</year>) <article-title>Overview of BioASQ 2021: the ninth BioASQ challenge on large-scale biomedical semantic indexing and question answering</article-title>. <source><italic toggle="yes">ArXiv Preprint</italic></source>, arXiv:2106.14885.</mixed-citation>
    </ref>
    <ref id="R22">
      <label>22.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Tang</surname><given-names>W.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>J.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>H.</given-names></string-name></person-group><etal>et al.</etal> (<year>2021</year>) <article-title>Team DUT914 at BioCreative VII Litcovid Track: a BioBERT-based feature enhancement approach</article-title>. In: <italic toggle="yes">Proceedings of the Seventh BioCreative Challenge Evaluation Workshop</italic>. <publisher-name>BioCreative</publisher-name>, <conf-loc>Cecilia Arighi, University of Delaware, USA</conf-loc>, pp. <fpage>292</fpage>–<lpage>294</lpage>.</mixed-citation>
    </ref>
    <ref id="R23">
      <label>23.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>S.</given-names></string-name>, <string-name><surname>Chiu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Yeh</surname><given-names>W.</given-names></string-name></person-group><etal>et al.</etal> (<year>2021</year>) <article-title>Team DonutNLP at BioCreativeVII Litcovid Track: multi-label topic classification for COVID-19 literature annotation using the BERT-based ensemble learning approach</article-title>. In: <italic toggle="yes">Proceedings of the Seventh BioCreative Challenge Evaluation Workshop</italic>. <publisher-name>BioCreative</publisher-name>, <conf-loc>Cecilia Arighi, University of Delaware, USA</conf-loc>, pp. <fpage>289</fpage>–<lpage>291</lpage>.</mixed-citation>
    </ref>
    <ref id="R24">
      <label>24.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Fang</surname><given-names>L.</given-names></string-name> and <string-name><surname>Wang</surname><given-names>K.</given-names></string-name></person-group>, (<year>2021</year>) <article-title>Team Bioformer at BioCreative VII LitCovid Track: multic-label topic classification for COVID-19 literature with a compact BERT model</article-title>. In: <italic toggle="yes">Proceedings of the seventh BioCreative challenge evaluation workshop</italic>. <publisher-name>BioCreative</publisher-name>, <conf-loc>Cecilia Arighi, University of Delaware, USA</conf-loc>, pp. <fpage>272</fpage>–<lpage>274</lpage>.</mixed-citation>
    </ref>
    <ref id="R25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kemal</surname><given-names>O.</given-names></string-name>, <string-name><surname>Baris</surname><given-names>C.</given-names></string-name>, <string-name><surname>Sinan</surname><given-names>K.</given-names></string-name></person-group><etal>et al.</etal> (<year>2021</year>) <article-title>Imbalance problems in object detection: a review</article-title>. <source><italic toggle="yes">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</italic></source>, <volume>43</volume>, <fpage>3388</fpage>–<lpage>3415</lpage>.<pub-id pub-id-type="pmid">32191882</pub-id></mixed-citation>
    </ref>
    <ref id="R26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>T.</given-names></string-name>, <string-name><surname>Goyal</surname><given-names>P.</given-names></string-name>, <string-name><surname>Girshick</surname><given-names>R.</given-names></string-name></person-group><etal>et al.</etal> (<year>2017</year>) <article-title>Focal loss for dense object detection</article-title>. <source><italic toggle="yes">IEEE Transactions on Pattern Analysis and Machine Intelligence</italic></source>, <volume>42</volume>, <fpage>318</fpage>–<lpage>327</lpage>.</mixed-citation>
    </ref>
    <ref id="R27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sagi</surname><given-names>O.</given-names></string-name> and <string-name><surname>Rokach</surname><given-names>L.</given-names></string-name></person-group> (<year>2018</year>) <article-title>Ensemble learning: a survey</article-title>. <source><italic toggle="yes">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</italic></source>, <volume>8</volume>, <page-range>e1249</page-range>.</mixed-citation>
    </ref>
    <ref id="R28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Tinn</surname><given-names>R.</given-names></string-name>, <string-name><surname>Cheng</surname><given-names>H.</given-names></string-name></person-group><etal>et al.</etal> (<year>2022</year>) <article-title>Domain-specific language model pretraining for biomedical natural language processing</article-title>. <source><italic toggle="yes">ACM Transactions on Computing for Healthcare</italic></source>, <volume>3</volume>, <fpage>1</fpage>–<lpage>23</lpage>.</mixed-citation>
    </ref>
    <ref id="R29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hebbar</surname><given-names>S.</given-names></string-name> and <string-name><surname>Xie</surname><given-names>Y.</given-names></string-name></person-group> (<year>2021</year>) <article-title>CovidBERT-Biomedical Relation Extraction for Covid-19</article-title>. <source><italic toggle="yes">The International FLAIRS Conference Proceedings</italic></source>, <volume>34</volume>. doi: <pub-id pub-id-type="doi">10.32473/flairs.v34i1.128488</pub-id>.</mixed-citation>
    </ref>
    <ref id="R30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname><given-names>J.</given-names></string-name>, <string-name><surname>Yoon</surname><given-names>W.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>S.</given-names></string-name></person-group><etal>et al.</etal> (<year>2020</year>) <article-title>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</article-title>. <source><italic toggle="yes">Bioinformatics</italic></source>, <volume>36</volume>, <fpage>1234</fpage>–<lpage>1240</lpage>.<pub-id pub-id-type="pmid">31501885</pub-id></mixed-citation>
    </ref>
    <ref id="R31">
      <label>31.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Alrowili</surname><given-names>S.</given-names></string-name> and <string-name><surname>Vijay-Shanker</surname><given-names>K.</given-names></string-name></person-group> (<year>2021</year>) <article-title>BioM-transformers: building large biomedical language models with BERT, ALBERT and ELECTRA</article-title>. In: <italic toggle="yes">Proceedings of the 20th Workshop on Biomedical Language Processing</italic>. <publisher-name>Association for Computational Linguistics</publisher-name>, pp. <fpage>221</fpage>–<lpage>227</lpage>.</mixed-citation>
    </ref>
    <ref id="R32">
      <label>32.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kanakarajan</surname><given-names>K.</given-names></string-name>, <string-name><surname>Kundumani</surname><given-names>B.</given-names></string-name> and <string-name><surname>Sankarasubbu</surname><given-names>M.</given-names></string-name></person-group> (<year>2021</year>) <article-title>BioELECTRA: pretrained biomedical text encoder using discriminators</article-title>. In: <italic toggle="yes">Proceedings of the 20th Workshop on Biomedical Language Processing</italic>. <publisher-name>Association for Computational Linguistics</publisher-name>, pp. <fpage>143</fpage>–<lpage>154</lpage>.</mixed-citation>
    </ref>
    <ref id="R33">
      <label>33.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><string-name><surname>Gururangan</surname><given-names>S.</given-names></string-name>, <string-name><surname>Marasović</surname><given-names>A.</given-names></string-name>, <string-name><surname>Swayamdipta</surname><given-names>S.</given-names></string-name></person-group><etal>et al.</etal> (<year>2020</year>) <article-title>Don’t stop pretraining: adapt language models to domains and tasks</article-title>. In: <source><italic toggle="yes">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</italic></source>. <publisher-name>Association for Computational Linguistics</publisher-name>, <conf-loc>Seattle, Washington, USA</conf-loc>, pp. <fpage>8342</fpage>–<lpage>8360</lpage>.</mixed-citation>
    </ref>
    <ref id="R34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Devlin</surname><given-names>J.</given-names></string-name>, <string-name><surname>Chang</surname><given-names>M.W.</given-names></string-name>, <string-name><surname>Lee</surname><given-names>K.</given-names></string-name></person-group><etal>et al.</etal> (<year>2018</year>) <article-title>BERT: pre-training of deep bidirectional transformers for language understanding</article-title>. <source><italic toggle="yes">ArXiv Preprint</italic></source>, arXiv:1810.04805.</mixed-citation>
    </ref>
    <ref id="R35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clark</surname><given-names>K.</given-names></string-name>, <string-name><surname>Luong</surname><given-names>M.T.</given-names></string-name>, <string-name><surname>Le</surname><given-names>Q.V.</given-names></string-name></person-group><etal>et al.</etal> (<year>2020</year>) <article-title>Electra: pre-training text encoders as discriminators rather than generators</article-title>. <source><italic toggle="yes">ArXiv Preprint</italic></source>, arXiv:2003.10555.</mixed-citation>
    </ref>
    <ref id="R36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Ott</surname><given-names>M.</given-names></string-name>, <string-name><surname>Goyal</surname><given-names>N.</given-names></string-name></person-group><etal>et al.</etal> (<year>2019</year>) <article-title>RoBERTa: a robustly optimized BERT pretraining approach</article-title>. <source><italic toggle="yes">ArXiv Preprint</italic></source>, arXiv:1907.11692.</mixed-citation>
    </ref>
    <ref id="R37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Collobert</surname><given-names>R.</given-names></string-name>, <string-name><surname>Weston</surname><given-names>J.</given-names></string-name>, <string-name><surname>Bottou</surname><given-names>L.</given-names></string-name></person-group><etal>et al.</etal> (<year>2011</year>) <article-title>Natural language processing (almost) from scratch</article-title>. <source><italic toggle="yes">Journal of Machine Learning Research</italic></source>, <volume>12</volume>, <fpage>2493</fpage>–<lpage>2537</lpage>.</mixed-citation>
    </ref>
    <ref id="R38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Loshchilov</surname><given-names>I.</given-names></string-name> and <string-name><surname>Hutter</surname><given-names>F.</given-names></string-name></person-group> (<year>2017</year>) <article-title>Decoupled weight decay regularization</article-title>. <source><italic toggle="yes">ArXiv Preprint</italic></source>, arXiv:1711.05101.</mixed-citation>
    </ref>
    <ref id="R39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Du</surname><given-names>J.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Peng</surname><given-names>Y.</given-names></string-name></person-group><etal>et al.</etal> (<year>2019</year>) <article-title>ML-Net: multi-label classification of biomedical texts with deep neural networks</article-title>. <source><italic toggle="yes">Journal of the American Medical Informatics Association</italic></source>, <volume>26</volume>, <fpage>1279</fpage>–<lpage>1285</lpage>.<pub-id pub-id-type="pmid">31233120</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
