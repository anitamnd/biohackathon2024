<?DTDIdentifier.IdentifierValue article.dtd?>
<?DTDIdentifier.IdentifierType system?>
<?SourceDTD.DTDName article.dtd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName bmc2nlmx2.xsl?>
<?ConverterInfo.Version 2?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">3116489</article-id>
    <article-id pub-id-type="publisher-id">1471-2105-12-180</article-id>
    <article-id pub-id-type="pmid">21599902</article-id>
    <article-id pub-id-type="doi">10.1186/1471-2105-12-180</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>A Simple Approach to Ranking Differentially Expressed Gene Expression Time Courses through Gaussian Process Regression</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes" id="A1">
        <name>
          <surname>Kalaitzis</surname>
          <given-names>Alfredo A</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>A.Kalaitzis@sheffield.ac.uk</email>
      </contrib>
      <contrib contrib-type="author" corresp="yes" id="A2">
        <name>
          <surname>Lawrence</surname>
          <given-names>Neil D</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>N.Lawrence@dcs.shef.ac.uk</email>
      </contrib>
    </contrib-group>
    <aff id="I1"><label>1</label>The Sheffield Institute for Translational Neuroscience, 385A Glossop Road, Sheffield, S10 2HQ, UK</aff>
    <pub-date pub-type="collection">
      <year>2011</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>5</month>
      <year>2011</year>
    </pub-date>
    <volume>12</volume>
    <fpage>180</fpage>
    <lpage>180</lpage>
    <history>
      <date date-type="received">
        <day>18</day>
        <month>1</month>
        <year>2011</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>5</month>
        <year>2011</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright Â©2011 Kalaitzis and Lawrence; licensee BioMed Central Ltd.</copyright-statement>
      <copyright-year>2011</copyright-year>
      <copyright-holder>Kalaitzis and Lawrence; licensee BioMed Central Ltd.</copyright-holder>
      <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0">http://creativecommons.org/licenses/by/2.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="http://www.biomedcentral.com/1471-2105/12/180"/>
    <abstract>
      <sec>
        <title>Background</title>
        <p>The analysis of gene expression from time series underpins many biological studies. Two basic forms of analysis recur for data of this type: removing inactive (quiet) genes from the study and determining which genes are differentially expressed. Often these analysis stages are applied disregarding the fact that the data is drawn from a time series. In this paper we propose a simple model for accounting for the underlying temporal nature of the data based on a Gaussian process.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>We review Gaussian process (GP) regression for estimating the continuous trajectories underlying in gene expression time-series. We present a simple approach which can be used to filter quiet genes, or for the case of time series in the form of expression ratios, quantify differential expression. We assess via ROC curves the rankings produced by our regression framework and compare them to a recently proposed hierarchical Bayesian model for the analysis of gene expression time-series (BATS). We compare on both simulated and experimental data showing that the proposed approach considerably outperforms the current state of the art.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>Gaussian processes offer an attractive trade-off between efficiency and usability for the analysis of microarray time series. The Gaussian process framework offers a natural way of handling biological replicates and missing values and provides confidence intervals along the estimated curves of gene expression. Therefore, we believe Gaussian processes should be a standard tool in the analysis of gene expression time series.</p>
      </sec>
    </abstract>
  </article-meta>
</front>
<body>
  <sec>
    <title>Background</title>
    <p>Gene expression profiles give a snapshot of mRNA concentration levels as encoded by the genes of an organism under given experimental conditions. Early studies of this data often focused on a single point in time which biologists assumed to be critical along the gene regulation process after the perturbation. However, the <italic>static </italic>nature of such experiments severely restricts the inferences that can be made about the underlying dynamical system.</p>
    <p>With the decreasing cost of gene expression microarrays time series experiments have become commonplace giving a far broader picture of the gene regulation process. Such time series are often irregularly sampled and may involve differing numbers of replicates at each time point [<xref ref-type="bibr" rid="B1">1</xref>]. The experimental conditions under which gene expression measurements are taken cannot be perfectly controlled leading the signals of interest to be corrupted by noise, either of biological origin or arising through the measurement process.</p>
    <p>Primary analysis of gene expression profiles is often dominated by methods targeted at <italic>static </italic>experiments, i.e. gene expression measured on a single time-point, that treat time as an additional experimental factor [<xref ref-type="bibr" rid="B1">1</xref>-<xref ref-type="bibr" rid="B6">6</xref>]. However, were possible, it would seem sensible to consider methods that can account for the special nature of time course data. Such methods can take advantage of the particular statistical constraints that are imposed on data that is naturally ordered [<xref ref-type="bibr" rid="B7">7</xref>-<xref ref-type="bibr" rid="B12">12</xref>].</p>
    <p>The analysis of gene expression microarray time-series has been a stepping stone to important problems in systems biology such as the genome-wide identification of direct targets of transcription factors [<xref ref-type="bibr" rid="B13">13</xref>,<xref ref-type="bibr" rid="B14">14</xref>] and the full reconstruction of gene regulatory networks [<xref ref-type="bibr" rid="B15">15</xref>,<xref ref-type="bibr" rid="B16">16</xref>]. A more comprehensive review on the motivations and methods of analysis of time-course gene expression data can be found in [<xref ref-type="bibr" rid="B17">17</xref>].</p>
    <sec>
      <title>Testing for Expression</title>
      <p>A primary stage of analysis is to characterize the activity of each gene in an experiment. Removing inactive or <italic>quiet </italic>genes (genes which show negligible changes in mRNA concentration levels in response to treatments/perturbations) allows the focus to dwell on genes that have responded to treatment. We can consider two experimental set ups. Firstly, we may be attempting to measure the absolute level of gene expression (for example using Affymetrix GeneChip microarrays). In this case a quiet gene would be one whose expression level is indistinguishable from noise. Alternatively, we might be may be hybridizing two samples to the same array and quantifying the ratio of the expression levels. Here a quiet gene would be one which is showing a similar response in both hybridized samples. In either case we consider such expression profiles will consist principally of <italic>noise</italic>. Removing such genes will often have benign effects later in the processing pipeline. However, mistaken removal of profiles can clearly compromise any further downstream analysis. If the temporal nature of the data is ignored, our ability to detect such phenomena can be severely compromised. An example can be seen in Figure <xref ref-type="fig" rid="F1">1</xref>, where the temporal information is removed from an experimental profile by randomly reordering its expression samples. Disregarding the temporal correlation between measurements, hinders our ability to assess the profile due to critical inherent traits of the signal being lost such as the speed and scale of variation.</p>
      <fig id="F1" position="float">
        <label>Figure 1</label>
        <caption>
          <p><bold>Temporal information removed from the profile of gene Cyp1b1 in the experimental mouse data</bold>. <bold>(a) </bold>The centred profile of the gene <italic>Cyp1b1 </italic>(probeID 1416612_at in the <italic>GSE10562 </italic>dataset). The blue crosses represent zero-mean hybridised gene expression in time of measurement (log2 ratios between treatment and control). <bold>(b) </bold>The same profile with its timepoints randomised.</p>
        </caption>
        <graphic xlink:href="1471-2105-12-180-1"/>
      </fig>
      <p>Failure to capture the signal in a profile, irrespective of the amount of embedded noise, may be partially due to <italic>temporal aggregation </italic>effects, meaning that the coarse sampling of gene expression or the sampling rates do not match the natural rates of change in mRNA concentrations [<xref ref-type="bibr" rid="B18">18</xref>]. For these reasons, the classification scheme of differential expression in this paper is focused on reaching a high <italic>true positive rate </italic>(TPR, <italic>sensitivity </italic>or <italic>recall </italic>) and is to serve as a pre-processing tool prior to more involved analysis of time-course microarray data. In this work we distinguish between <italic>two-sample </italic>testing and experiments where <italic>control </italic>and <italic>treated </italic>cases are directly-hybridized on the microarray (For brevity, we shall refer to experiments with such setups as <italic>one-sample testing</italic>). The <italic>two-sample </italic>setup is a common experimental setup in which two groups of sample replicates are used [<xref ref-type="bibr" rid="B13">13</xref>,<xref ref-type="bibr" rid="B19">19</xref>]; one being under the treatment effect of interest and the other being the control group, so to recover the most active genes under a treatment one may be interested in testing for the statistical significance of a treated profile being differentially expressed with respect to its control counterpart. Other studies use data from a <italic>one-sample </italic>setup [<xref ref-type="bibr" rid="B11">11</xref>,<xref ref-type="bibr" rid="B12">12</xref>], in which the <italic>control </italic>and <italic>treated </italic>cases are directly hybridized on a microarray and the measurements are normalized log fold-changes between the two output channels of the microarray [<xref ref-type="bibr" rid="B20">20</xref>], so the analogous goal is to test for the statistical significance of having a non-zero signal.</p>
      <p>A recent significant contribution in regards to the estimation and ranking of differential expression of time-series in a <italic>one-sample </italic>setup is a hierarchical Bayesian model for the analysis of gene expression time-series (BATS) [<xref ref-type="bibr" rid="B11">11</xref>,<xref ref-type="bibr" rid="B12">12</xref>] which offers fast computations through exact equations of Bayesian inference, but makes a considerable number of prior biological assumptions to accomplish this (cf. <bold>Simulated data</bold>).</p>
    </sec>
    <sec>
      <title>Gene Expression Analysis with Gaussian Processes</title>
      <p><italic>Gaussian processes</italic>(GP) [<xref ref-type="bibr" rid="B21">21</xref>,<xref ref-type="bibr" rid="B22">22</xref>] offer an easy to implement approach to quantifying the true signal and noise embedded in a gene expression time-series, and thus allow us to rank the differential expression of the gene profile. A Gaussian process is the natural generalisation of a multivariate Gaussian distribution to a Gaussian distribution over <italic>a specific family of functions </italic>-- a family defined by a <italic>covariance function </italic>or <italic>kernel</italic>, i.e. a metric of similarity between data-points (Roughly speaking, if we also view a function as a vector with an infinite number of components, then that function can be represented as a point in an infinite-dimensional <italic>space of a specific family of functions </italic>and a Gaussian process as an infinite-dimensional Gaussian distribution over that space).</p>
      <p>In the context of expression trajectory estimation, a Gaussian process coupled with the <italic>squared-exponential </italic>covariance function (or <italic>radial basis function</italic>, RBF) -- a standard covariance function used in regression tasks -- makes the reasonable assumption that the underlying true signal in a profile is a <italic>smooth </italic>function [<xref ref-type="bibr" rid="B23">23</xref>], i.e. a function with an infinite degree of differentiability. This property endows the GP with a large degree of flexibility in capturing the underlying signals without imposing strong modeling assumptions (e.g. number of basis functions) but may also erroneously pick up spurious patterns (false positives) should the time-course profiles suffer from temporal aggregation. From a generative viewpoint, the profiles are assumed to have been corrupted by additive white Gaussian noise. This property makes the GP an attractive tool for bootstrapping simulated biological replicates [<xref ref-type="bibr" rid="B24">24</xref>].</p>
      <p>In a different context, Gaussian process priors have been used for modeling transcriptional regulation. For example in [<xref ref-type="bibr" rid="B25">25</xref>], while using the time-course expression of a-priori known direct targets (genes) of a transcription-factor, the authors went one step further and inferred the concentration rates of the transcription-factor protein itself and [<xref ref-type="bibr" rid="B26">26</xref>] extended the same model for the case of regulatory repression. The ever-lingering issue of outliers in time series is still critical, but is not addressed here as there is significant literature on this issue in the context of GP regression, which is complementary to this work.</p>
      <p>For example [<xref ref-type="bibr" rid="B19">19</xref>,<xref ref-type="bibr" rid="B27">27</xref>] developed a probabilistic model using Gaussian processes with a robust noise model specialised for two-sample testing to detect <italic>intervals </italic>of differential expression, whereas the present work optionally focuses on <italic>one-sample </italic>testing, to rank the differential expression and ultimately detect <italic>quiet/active </italic>genes. Other examples can also be easily applied; [<xref ref-type="bibr" rid="B28">28</xref>] use a Student-<italic>t </italic>distribution as the robust noise model in the regression framework along with variational approximations to make inference tractable, and [<xref ref-type="bibr" rid="B29">29</xref>] employ a Student-<italic>t </italic>observation model with Laplace approximations for inference. The standard GP regression framework is straightforward to use here with a limited need for manual tweaking of a few hyper-parameters. We describe the GP framework, as used here for regression, in more detail in the <bold>Methods </bold>section.</p>
    </sec>
  </sec>
  <sec>
    <title>Results and Discussion</title>
    <p>We apply standard Gaussian process (GP) regression and the Bayesian hierarchical model for the analysis of time-series (BATS) on two in-silico datasets simulated by BATS and GPs, and on one experimental dataset coming from a study on primary mouse keratinocytes with an induced activation of the TRP63 transcription factor, for which a reverse-engineering algorithm was developed (TSNI: time-series network identification) to infer the direct targets of TRP63 [<xref ref-type="bibr" rid="B13">13</xref>].</p>
    <p>We assume that each gene expression profile can be categorized as either quiet or differentially expressed. We consider algorithms that provide a rank ordering of the profiles according to which is most likely to be non-quiet (or differentially expressed). Given ground truth we can then evaluate the quality of such a ranking and compare different algorithms. We make use of <italic>receiver operating characteristic </italic>curves (ROC curves) to evaluate the algorithms. These curves plot the <italic>false positive rate </italic>on the horizontal axis, versus the <italic>true positive rate </italic>on the vertical axis; i.e. the percentage of the total negatives (non-differentially expressed profiles) erroneously classified as positives (differentially expressed) versus the percentage of the total positives correctly classified as positives.</p>
    <p>From the output of each model a ranking of differential expression is produced and assessed with ROC curves to quantify how well in accordance to each of the three ground truths (BATS-sampled, GP-sampled, TSNI-experimental) the method performs. The BATS model can employ three different noise models, where the marginal distribution of the error is assumed to be either Gaussian, Student-<italic>t </italic>or double exponential respectively. For the following comparisons we plot four ROC curves, one for each noise model of BATS and one for the GP. We demonstrate that the ranking of the GP framework outperforms that of BATS with respect to the TSNI ranking on the experimental data and on GP-sampled profiles.</p>
    <sec>
      <title>Simulated data</title>
      <p>The first set of in-silico profiles are simulated by the BATS software <ext-link ext-link-type="uri" xlink:href="http://www.na.iac.cnr.it/bats/">http://www.na.iac.cnr.it/bats/</ext-link> in accordance to the guidelines given in [<xref ref-type="bibr" rid="B12">12</xref>]. In BATS [<xref ref-type="bibr" rid="B11">11</xref>] each time-course profile is assumed to be generated by a function expanded in an orthonormal basis (Legendre or Fourier) plus noise. The number of bases and their coefficients, are estimated with analytic computations in a fully Bayesian manner. Thus the global estimand for every gene expression trajectory is the linear combination of some number of bases whose coefficients are estimated by a posterior distribution. In addition, the BATS framework allows various types of non-Gaussian noise models.</p>
      <sec>
        <title>BATS simulation</title>
        <p>We reproduce one instantiation of the simulations performed in [<xref ref-type="bibr" rid="B11">11</xref>]; specifically, three sets of <italic>N </italic>= 8000 profiles, of <italic>n </italic>= 11 timepoints and <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i1.gif"/></inline-formula> replicates, for <italic>i </italic>= 1; ..., <italic>N</italic>, <italic>j </italic>= 1, ..., <italic>n </italic>except <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i2.gif"/></inline-formula> according to the model defined in [11, sec. 2.2]. In each of the three sets of profiles, 600 out of 8000 are randomly chosen to be differentially expressed (labeled as "1" in the ground truth) and simulated as a sum of an orthonormal basis of Legendre polynomials with additive i.i.d.(identically and independently distributed) noise.</p>
        <p>The other 7400 non-differentially expressed profiles (labeled as "0" in the ground truth) are essentially zero functions with additive i.i.d. noise. The three simulated datasets are induced with different kinds of i.i.d. noise; respectively, Gaussian N(0, <italic>Ï </italic><sup>2</sup>), Student-<italic>t </italic>distributed with 5(T(5)) and 3 (T(3)) degrees of freedom. Figure <xref ref-type="fig" rid="F2">2(a, b, c)</xref> illustrates the comparison on the BATS-sampled data with all three kinds of induced noise.</p>
        <fig id="F2" position="float">
          <label>Figure 2</label>
          <caption>
            <p><bold>GP vs. BATS on simulated data</bold>. ROC curves for the GP and BATS methods on data simulated by BATS induced with <bold>(a) </bold>Gaussian noise, <bold>(b) </bold>Student's-<italic>t </italic>with 5 degrees of freedom, (c) Student's-<italic>t </italic>with 3 degrees of freedom; and on <bold>(d) </bold>data simulated by Gaussian processes. Each panel depicts one ROC curve for the GP method and three for BATS, each using a different noise model indicated by the subscript in the legend ("G" for Gaussian, "T" for Student's-<italic>t </italic>and "DE" for double exponential marginal distributions of error), followed by the area under the corresponding curve (AUC).</p>
          </caption>
          <graphic xlink:href="1471-2105-12-180-2"/>
        </fig>
      </sec>
      <sec>
        <title>GP simulation</title>
        <p>In a similar setup, the second in-silico dataset consists of 8000 profiles sampled from Gaussian processes, with the same number of replicates and time-points, among which 600 were setup as differentially expressed. To generate a differentially expressed profile, each of the <italic>hyperparameters </italic>of the RBF covariance function, namely the <italic>characteristic lengthscale, signal variance </italic>and <italic>noise variance </italic>(cf. <bold>Methods</bold>) is sampled from separate Gamma distributions. The three Gamma distributions are fitted to sets of their corresponding hyperparameters, which are observed for the true positive profiles under a near zero FPR during the first test on BATS-generated profiles. In this way, we attempt to resemble the behaviour of the BATS-sampled profiles. Table <xref ref-type="table" rid="T1">1</xref> lists the parameters of the three fitted Gamma distributions.</p>
        <table-wrap id="T1" position="float">
          <label>Table 1</label>
          <caption>
            <p>Parameters of the Gamma distributions for sampling the RBF-hyperparameters.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th/>
                <th/>
                <th align="center" colspan="2">Sampling Gamma distribution <italic>Î</italic>(<italic>a</italic>, <italic>b</italic>)</th>
              </tr>
              <tr>
                <th/>
                <th/>
                <th colspan="2">
                  <hr/>
                </th>
              </tr>
              <tr>
                <th/>
                <th/>
                <th align="center"><italic>a </italic>(scale)</th>
                <th align="center"><italic>b </italic>(shape)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">Sampled<break/>RBF-<break/>Hyperparameters</td>
                <td align="center">â<sup>2 </sup>(characteristic lengthscale)</td>
                <td align="center">1.4</td>
                <td align="center">5.7</td>
              </tr>
              <tr>
                <td/>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td/>
                <td align="center"><inline-formula><inline-graphic xlink:href="1471-2105-12-180-i3.gif"/></inline-formula>(signal variance)</td>
                <td align="center">2.76</td>
                <td align="center">0.2</td>
              </tr>
              <tr>
                <td/>
                <td colspan="3">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td/>
                <td align="center"><inline-formula><inline-graphic xlink:href="1471-2105-12-180-i4.gif"/></inline-formula> (noise variance)</td>
                <td align="center">23</td>
                <td align="center">0.008</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <p>These are the parameters of the Gamma distributions from which we sample the RBF- hyperparameters. For example, the characteristic lengthscale is sampled from a Gamma with scale 1.4 and shape 5.7. The hyperparameters are then used in the RBF covariance function to sample/simulate a profile from the Gaussian process.</p>
          </table-wrap-foot>
        </table-wrap>
        <p>The other 7400 non-differentially expressed profiles are simply zero functions with additive white Gaussian noise of variance equal to the sum of two samples from the Gamma distribution for the <italic>signal variance </italic>and the <italic>noise variance</italic>. This addition serves to create a non-differentiated profile of comparative scale to the differentiated ones, but nonetheless of completely random nature. Figure <xref ref-type="fig" rid="F2">2(d)</xref> illustrates the comparison on the GP-sampled data.</p>
      </sec>
    </sec>
    <sec>
      <title>Experimental data</title>
      <p>We apply the standard GP regression framework and BATS on an experimental dataset coming from a study on primary mouse keratinocytes with an induced activation of the TRP63 transcription factor (GEO-accession number [GEOdataset:GSE10562]), where a reverse-engineering algorithm was developed (TSNI: time-series network identification) to infer the direct targets of TRP63 [<xref ref-type="bibr" rid="B13">13</xref>]. In that study, 786 out of 22690 gene reporters were chosen based on the area under their curves, and ranked by TSNI according to the probability of belonging to direct targets of TRP63. The ranking list was published in a supplementary file available for download</p>
      <p>(genome.cshlp.org/content/suppl/2008/05/05/gr.073601.107.DC1/DellaGatta_SupTable1.xls) and used here as a <italic>noisy ground truth</italic>. We pre-process the data with the robust multi-array average (RMA) expression measure [<xref ref-type="bibr" rid="B30">30</xref>], implemented in the "affy" R-package.</p>
      <p>We label the top 100 position of the TSNI ranking as "1" in the ground truth as they are the most likely to be direct targets of the TRP63 transcription factor and because the <italic>binding scores </italic>(computed as the sum of -log2 of <italic>p</italic>-values of all TRP63-binding regions identified by ChIP-chip experiments) are most densely distributed amongst the first 100 positions, see Figure <xref ref-type="fig" rid="F3">3</xref>. Furthermore, in [<xref ref-type="bibr" rid="B13">13</xref>] these 100 positions were further validated by gene set enrichment analysis (GSEA) [<xref ref-type="bibr" rid="B31">31</xref>] to check if their up/down regulation patterns were correlated to genes that respond to TRP63 knock-downs in general. In summary, <italic>"the top 100 TSNI ranked transcripts are significantly enriched for the strongest binding sites" </italic>[<xref ref-type="bibr" rid="B13">13</xref>]. Figure <xref ref-type="fig" rid="F4">4</xref> illustrates the comparison on the experimental data.</p>
      <fig id="F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p><bold>Distribution of binding scores along the TSNI ranking</bold>. By inspection, the distribution of the binding scores is mostly dense along the first 100 positions of the TSNI ranking. The authors in [<xref ref-type="bibr" rid="B13">13</xref>] only selected the top 100 genes and the bottom 200 genes to search for binding sites and thus showed that the top 100 genes have more binding sites than the bottom 200 genes.</p>
        </caption>
        <graphic xlink:href="1471-2105-12-180-3"/>
      </fig>
      <fig id="F4" position="float">
        <label>Figure 4</label>
        <caption>
          <p><bold>GP vs. BATS on experimental data</bold>. ROC curves for the GP and BATS methods on experimental data from [<xref ref-type="bibr" rid="B13">13</xref>]. As in Figure 2, one ROC curve and the area under it (AUC) are depicted for the GP method and three for BATS, each using a different noise model indicated by the subscript in the legend. <bold>(a) </bold>Ground truth consists of 22690 labels among which only the 786 profiles chosen to be ranked by TSNI (based on the area under their curves) are labeled as "1", cf. <bold>Experimental data</bold>. <bold>(b) </bold>Same number of labels; here only the top 100 profiles ranked by TSNI are labeled as "1".</p>
        </caption>
        <graphic xlink:href="1471-2105-12-180-4"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>Discussion</title>
    <p>On BATS-sampled data, Figure <xref ref-type="fig" rid="F2">2(a, b, c)</xref>, we observe that the change in the induced noise is barely noticeable in regards to the performances of both methods and that BATS maintains its stable supremacy over the GP framework. This performance gap is partially due to the lack of a robust noise model for the GP (cf. <bold>Conclusions</bold>). Furthermore, there is a modeling bias in the underlying functions of the simulated profiles, which contain a finite small degree of differentiability (maximum degree of Legendre polynomial is 6). This puts the GP in a disadvantaged position as it models for (smooth) infinitely differentiable functions when its covariance function is a <italic>squared exponential</italic>. Consequently, for this simulated dataset the GP is more susceptible to capturing spurious patterns as they are more likely to lie within its modeling range, whereas for BATS modeling the polynomials with a limited degree acts as a safeguard against spurious patterns, most of which vary rapidly in time.</p>
    <p>On GP-sampled data, Figure <xref ref-type="fig" rid="F2">2(d)</xref>, we observe the reversal of the performance gap in favor of the GP framework while its performance is almost unaffected. The GP is still prone to non-differentially expressed profiles with spurious patterns and differentially expressed profiles with excessive noise. However, the limited polynomial degree of BATS proves to be inadequate for many of the GP-sampled functions and the two BATS variants with robust noise models (BATS<italic><sub>T</sub></italic>, BATS<italic><sub>DE</sub></italic>) only alleviate the problem slightly. In Figure <xref ref-type="fig" rid="F4">4</xref> we observe the GP outperforming the Gaussian noise variant of BATS (BATS<italic><sub>G</sub></italic>) by a similar degree as in Figure <xref ref-type="fig" rid="F2">2(d)</xref>. The experimental data are much more complex and apparently the robust BATS variants now offer no increase in performance. Since the ground truth focuses on the 100 most differentially expressed genes with respect to the induction of the TRP63 transcription factor, then these results indicate that the GP method of ranking presented here indeed highlights differentially expressed genes and that it naturally features an attractive degree of robustness against different kinds of noise.</p>
  </sec>
  <sec>
    <title>Conclusions</title>
    <p>We presented an approach to estimating the continuous trajectory of gene expression time-series from microarray data through <italic>Gaussian process </italic>(GP) regression and ranking the differential expression of each profile via a log-ratio of marginal likelihoods of two GPs, each one representing the hypothesis of differential and non-differential expression respectively. We compared our method to a recent Bayesian hierarchical model (BATS) via ROC curves on data simulated by BATS and GPs and experimental data. Each evaluation was made on the basis of matched percentages to a ground truth - a binary vector which labeled the profiles in a dataset as differentially expressed or not. The experimental data were taken from a previous study on primary mouse keratinocytes and the top 100 genes of its ranking were used here as the noisy ground truth for the purposes of assessment. The GP framework significantly outperformed BATS on experimental and GP-sampled data and the results showed that standard GP regression can be regarded as a serious competitor in evaluating the continuous trajectories of gene expression and ranking its differential expression.</p>
    <p>This ranking scheme presented here is reminiscent of the work in [<xref ref-type="bibr" rid="B19">19</xref>] on <italic>two-sample </italic>data (separate time-course profiles for each treatment), where the two competing hypotheses are represented in a graphical model of two different generative models connected with a <italic>gating </italic>scheme; one where the two profiles of the gene reporter are assumed to be generated by two different GPs, and thus the gene is <italic>differentially expressed </italic>across the two treatments, and one where the two profiles are assumed to be generated by the same GP, and thus the gene is <italic>non-differentially expressed</italic>. The gating network serves to <italic>switch </italic>between the two generative models, in time, to detect <italic>intervals </italic>of differential expression and thus allow biologists to draw conclusions about the propagation of a perturbation in a gene regulatory network. Instead, the issue presented in this paper is more basic and so is the methodology to deal with it. However, we note that the robust mechanisms against outliers used in [<xref ref-type="bibr" rid="B19">19</xref>,<xref ref-type="bibr" rid="B28">28</xref>,<xref ref-type="bibr" rid="B29">29</xref>] are complementary to this work and one should not hesitate to incorporate one into a framework similar to ours. Practicalities aside, this paper also introduces additional proof that Gaussian processes, naturally and without much engineering, fit to the analysis of gene expression time-series and that simplicity can still be preferred over the ever-increasing -- but sometimes necessary -- complexity of hierarchical Bayesian frameworks.</p>
    <sec>
      <title>Future work</title>
      <p>A natural next step would be to add a robust noise mechanism in our framework. In this regard, fine examples can be found in [<xref ref-type="bibr" rid="B19">19</xref>,<xref ref-type="bibr" rid="B28">28</xref>,<xref ref-type="bibr" rid="B29">29</xref>]. Finally, an interesting biological question is about the potential periodicity of the underlying signal in a gene expression profile. In this regard a different of kind stationary covariance function, the <italic>periodic </italic>covariance function [<xref ref-type="bibr" rid="B22">22</xref>], can fit a time-series generated by an periodic process and thus its lengthscale hyperparameter can be interpreted as its cycle.</p>
    </sec>
  </sec>
  <sec sec-type="methods">
    <title>Methods</title>
    <p>As we mentioned earlier, analysing time-course microarray data by means of Gaussian process (GP) regression is not a new idea (cf. <bold>Background</bold>). In this section we review the methodology to estimating the continuous trajectory of a gene expression by GP regression and subsequently describe a likelihood-ratio approach to ranking the differential expression of its profile. The following content is based on the key components of GP theory as described in [<xref ref-type="bibr" rid="B21">21</xref>,<xref ref-type="bibr" rid="B22">22</xref>].</p>
    <sec>
      <title>The Gaussian process model</title>
      <p>The idea is to treat trajectory estimation given the observations (gene expression time-series) as an interpolation problem on functions of one dimension. By assuming the observations have Gaussian-distributed noise, the computations for prediction become tractable and involve only the manipulation of linear algebra rules.</p>
      <sec>
        <title>A finite parametric model</title>
        <p>We begin the derivation of the GP regression model by defining a standard <italic>linear regression </italic>model (a more concrete example of such a model is for <italic>Ï </italic>= (1, <italic>x</italic>, <italic>x</italic><sup>2</sup>)<sup>â¤</sup>, i.e. a line mapped to a quadratic curve)<disp-formula id="bmcM1"><label>(1)</label><graphic xlink:href="1471-2105-12-180-i5.gif"/></disp-formula></p>
        <p>where gene expression measurements in time <bold>y </bold>= {<italic>y<sub>n</sub></italic>}<sub><italic>n </italic>= 1..<italic>N </italic></sub>are contaminated with white Gaussian noise and the inputs (of time) are mapped to a feature space <bold>Î¦ </bold>= {<italic>Ï </italic>(<italic>x<sub>n</sub></italic>)<sup>â¤</sup>}<sub><italic>n </italic>= 1..<italic>N</italic></sub>. Furthermore, if we assume the noise to be i.i.d. (identically and independently distributed) as a Gaussian with zero mean and variance <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i4.gif"/></inline-formula><disp-formula id="bmcM2"><label>(2)</label><graphic xlink:href="1471-2105-12-180-i6.gif"/></disp-formula></p>
        <p>then the probability density of the observations given the inputs and parameters (<italic>data likelihood</italic>) is Gaussian-distributed<disp-formula id="bmcM3"><label>(3)</label><graphic xlink:href="1471-2105-12-180-i7.gif"/></disp-formula></p>
        <p>Where <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i8.gif"/></inline-formula>.</p>
      </sec>
      <sec>
        <title>Introducing Bayesian methodology</title>
        <p>Now turning to <italic>Bayesian linear regression</italic>, we wish to encode our initial beliefs about the parameters <bold>w </bold>by specifying a zero mean, isotropic Gaussian distribution as a <italic>prior </italic>over the parameters<disp-formula id="bmcM4"><label>(4)</label><graphic xlink:href="1471-2105-12-180-i9.gif"/></disp-formula></p>
        <p>By integrating the product of the <italic>likelihood Ã prior </italic>with respect to the parameters, we get the <italic>marginal likelihood</italic><disp-formula id="bmcM5"><label>(5)</label><graphic xlink:href="1471-2105-12-180-i10.gif"/></disp-formula></p>
        <p>which is jointly Gaussian. Hence the <italic>mean </italic>and <italic>covariance </italic>of the <italic>marginal </italic>are<disp-formula id="bmcM6"><label>(6)</label><graphic xlink:href="1471-2105-12-180-i11.gif"/></disp-formula><disp-formula id="bmcM7"><label>(7)</label><graphic xlink:href="1471-2105-12-180-i12.gif"/></disp-formula><disp-formula id="bmcM8"><label>(8)</label><graphic xlink:href="1471-2105-12-180-i13.gif"/></disp-formula></p>
        <p>By computing the <italic>marginal likelihood </italic>in eq. (8), we can <italic>compare </italic>or <italic>rank </italic>different models, without fear of <italic>overfitting </italic>on the data, or having to explicitly apply a <italic>regulariser </italic>to the <italic>likelihood; </italic>the <italic>marginal likelihood </italic>implicitly <italic>penalises </italic>too complex models [21, sec. 5.4].</p>
        <p>Notice in eq. (7) how the structure of the covariance implies that choosing a different feature space Î¦ results in a different <bold>K</bold><italic><sub>y</sub></italic>. Whatever <bold>K</bold><italic><sub>y </sub></italic>is, it must satisfy the following requirements to be a valid covariance matrix of the GP:</p>
        <p>â¢ <bold>Kolmogorov consistency</bold>, which is satisfied when <italic>K<sub>ij </sub></italic>= <italic>K</italic>(<italic>x<sub>i</sub></italic>, <italic>x<sub>j</sub></italic>) for some <italic>covariance function K</italic>, such that all possible <bold>K </bold>are <italic>positive semidefinite </italic>(<bold>y<sup>â¤ </sup>Ky </bold>â¥ 0).</p>
        <p>â¢ <bold>Exchangeability</bold>, which is satisfied when the data are i.i.d.. It means that the order in which the data become available has no impact on the <italic>marginal distribution</italic>, hence there is no need to hold out data from the training set for <italic>validation </italic>purposes (for measuring generalisation errors, etc.).</p>
      </sec>
      <sec>
        <title>Definition of a Gaussian process</title>
        <p>More formally, <italic>a Gaussian process is a stochastic process (or collection of random variables) over a feature space, such that the distribution p </italic>(<italic>y</italic>(<italic>x</italic><sub>1</sub><italic>)</italic>, <italic>y</italic>(<italic>x</italic><sub>2</sub>),..., <italic>y(x<sub>n</sub></italic>)) <italic>of a function y(x), for any finite set of points </italic>{<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, ..., <italic>x<sub>n</sub></italic>} <italic>mapped to that space, is Gaussian, and such that any of these Gaussian distributions is Kolmogorov consistent</italic>.</p>
        <p>If we remove the <italic>noise term </italic><inline-formula><inline-graphic xlink:href="1471-2105-12-180-i4.gif"/></inline-formula><bold>I </bold>from <bold>K</bold><italic><sub>y </sub></italic>in eq. (7) we can have noiseless predictions of <italic>f</italic>(<italic>x</italic>) rather than <italic>y</italic>(<italic>x</italic>) = <italic>f</italic>(<italic>x</italic>) + Îµ. However, when dealing with finite parameter spaces <bold>K</bold><italic><sub>f </sub></italic>may be <italic>ill-conditioned </italic>(cf. sec. <italic>SE derivation</italic>), so the <italic>noise term </italic>guarantees that <bold>K</bold><italic><sub>y </sub></italic>will have <italic>full rank </italic>(and an inverse). Having said that, we now formulate the GP <italic>prior </italic>over the <italic>latent </italic>function values <italic>f </italic>by rewriting eq. (8) as<disp-formula id="bmcM9"><label>(9)</label><graphic xlink:href="1471-2105-12-180-i14.gif"/></disp-formula></p>
        <p>where the <italic>mean function </italic>(usually defined as the zero function) and the <italic>covariance function </italic>respectively are<disp-formula id="bmcM10"><label>(10)</label><graphic xlink:href="1471-2105-12-180-i15.gif"/></disp-formula><disp-formula id="bmcM11"><label>(11)</label><graphic xlink:href="1471-2105-12-180-i16.gif"/></disp-formula></p>
      </sec>
    </sec>
    <sec>
      <title>The squared-exponential kernel</title>
      <p>In this paper we only use the univariate version of the squared-exponential (SE) kernel. But before embarking on its analysis, the reader should be aware of the existing wide variety of kernel families, and potential combinations of them. A comprehensive review of the literature on covariance functions is found in [21, chap. 4].</p>
      <sec>
        <title>Derivation and interpretation of the SE kernel</title>
        <p>In the GP definition section we mentioned the possibility of an <italic>ill-conditioned </italic>covariance matrix. In the case of a finite parametric model (as in eq. (1)), <bold>K<sub>f </sub></bold>can have at most as many non-zero eigenvalues as the number of parameters in the model. Hence for any problem of any given size, the matrix is non-invertible. Ensuring <bold>K</bold><italic><sub>f </sub></italic>is not ill-conditioned, involves adding the diagonal noise term to the covariance. In an infinite-dimensional feature space, one would not have to worry about this issue as the features are integrated out and the covariance between datapoints is no longer expressed in terms of the features but by a <italic>covariance function</italic>. As demonstrated in [22, sec.45.3] and [21, sec.4.2.1], with an example of a one-dimensional dataset, we express the covariance matrix <bold>K</bold><italic><sub>f </sub></italic>in terms of the features <bold>Î¦</bold><disp-formula id="bmcM12"><label>(12)</label><graphic xlink:href="1471-2105-12-180-i17.gif"/></disp-formula></p>
        <p>then by considering a feature space defined by <italic>radial basis functions </italic>and integrating with respect to their centers <italic>h</italic>, eq. (12) becomes<disp-formula id="bmcM13"><label>(13)</label><graphic xlink:href="1471-2105-12-180-i18.gif"/></disp-formula></p>
        <p>where one ends up with a smooth (infinitely differentiable) function on an infinite-dimensional space of (radial basis function) features. Taking the constant out front as a <italic>signal variance </italic><inline-formula><inline-graphic xlink:href="1471-2105-12-180-i3.gif"/></inline-formula> and squaring the exponential gives rise to the standard form of the <italic>univariate squared-exponential </italic>(SE) covariance function. The <italic>noisy univariate </italic>SE kernel -- the one used in this paper is<disp-formula id="bmcM14"><label>(14)</label><graphic xlink:href="1471-2105-12-180-i19.gif"/></disp-formula></p>
        <p>The SE is a <italic>stationary </italic>kernel, i.e. it is a function of <italic>d </italic>= <italic>x<sub>i </sub></italic>- <italic>x<sub>j </sub></italic>which makes it <italic>translation invariant </italic>in time. <italic>Î´<sub>ij </sub></italic>is the <italic>Kronecker delta </italic>function which is unity when <italic>i </italic>= <italic>j </italic>and zero otherwise and <italic>l</italic><sup>2 </sup>is the <italic>characteristic lengthscale </italic>which specifies the distance beyond which any two inputs (<italic>x<sub>i</sub></italic>, <italic>x<sub>j</sub></italic>) become uncorrelated. In effect, the lengthscale <italic>l</italic><sup>2 </sup>governs the amount that <italic>f </italic>varies along the input (time). A small lengthscale means that <italic>f </italic>varies rapidly along time, and a very large lengthscale means that <italic>f </italic>behaves almost as a constant function, see Figure <xref ref-type="fig" rid="F5">5</xref>. This parameterisation of the SE kernel becomes very powerful when combined with <italic>hyperparameter adaptation</italic>, as described in a following section. Other adapted hyperparameters include the <italic>signal variance </italic><inline-formula><inline-graphic xlink:href="1471-2105-12-180-i3.gif"/></inline-formula> which is a vertical scale of function variation and the <italic>noise variance </italic><inline-formula><inline-graphic xlink:href="1471-2105-12-180-i4.gif"/></inline-formula> (introduced in eq. (2)) which is not a hyperparameter of the SE itself, but unless we consider it as a constant in the <italic>noisy </italic>case, its adaptation can give different explanations about the latent function that generates the data.</p>
        <fig id="F5" position="float">
          <label>Figure 5</label>
          <caption>
            <p><bold>Gaussian process fit on expression profile of gene Cyp1b1 in the experimental mouse data</bold>. Figure 5: A GP fitted on the centred profile of the gene <italic>Cyp1b1 </italic>(probeID 1416612_at in the <italic>GSE10562 </italic>dataset) with different settings of the lengthscale hyperparameter â<sup>2</sup>. The blue crosses represent zero-mean hybridised gene expression in time (log2 ratios between treatment and control) and the shaded area indicates the point-wise mean plus/minus two times the standard deviation (95% confidence region). <bold>(a) </bold>Mean function is constant as â<sup>2 </sup>â â (0 inverse lengthscale in eq. (14)) and all of the observed data variance is attributed to noise (<inline-formula><inline-graphic xlink:href="1471-2105-12-180-i4.gif"/></inline-formula>). <bold>(b) </bold>The lengthscale is manually set to a local-optimum large value (â<sup>2 </sup>= 30) and thus the mean function roughly fits the data-points. The observed data variance is equally attributed to signal (<inline-formula><inline-graphic xlink:href="1471-2105-12-180-i3.gif"/></inline-formula>) and noise (<inline-formula><inline-graphic xlink:href="1471-2105-12-180-i4.gif"/></inline-formula>). Consequently, the GP features high uncertainty in its predictive curve. <bold>(c) </bold>The lengthscale is manually set to a local-optimum small value (â<sup>2 </sup>= 15.6) and thus the mean function tighly fits the data-points with high certainty. The interpretation from the covariance function in this case is that the profile contains a minimal amount of noise and that most of the observed data variance is attributed to the underlying signal (<inline-formula><inline-graphic xlink:href="1471-2105-12-180-i3.gif"/></inline-formula>). <bold>(d) </bold>The contour of the corresponding LML function plotted by an exhaustive search of â<sup>2 </sup>and SNR values. The two main local-optima are indicated by the green dots and a third optimum that corresponds to the 1st panel appears almost as flat in the contour and its vicinity encompasses the whole lengthscale axis for very small values of SNR (i.e. given that SNR â 0, the lengthscale is trivial).</p>
          </caption>
          <graphic xlink:href="1471-2105-12-180-5"/>
        </fig>
        <p>One can also combine covariance functions as long as they are <italic>positive-definite</italic>. Examples of valid combined covariance functions include the <italic>sum </italic>and <italic>convolution </italic>of two covariance functions. In fact, eq. (14) is a combined sum of the SE kernel with the covariance function of <italic>isotropic Gaussian noise</italic>.</p>
      </sec>
    </sec>
    <sec>
      <title>Gaussian process prediction</title>
      <p>To interpolate the trajectory of gene expression at non-sampled time-points, as illustrated in Figure <xref ref-type="fig" rid="F5">5</xref>, we infer a function value <italic>f</italic><sub>* </sub>at a new input (non-sampled time-point) <italic>x</italic><sub>*</sub>, given the knowledge of function estimates <bold>f </bold>at known time-points <bold>x</bold>. The joint distribution <italic>p</italic>(<italic>f</italic><sub>*</sub>, <bold>f </bold>) is Gaussian, hence the conditional distribution <italic>p</italic>(<italic>f</italic><sub>*</sub>| <bold>f </bold>) is also Gaussian. In this section we consider predictions using noisy observations; we know the noise is Gaussian too, so the noisy conditional distribution does not differ. By Bayes' rule<disp-formula id="bmcM15"><label>(15)</label><graphic xlink:href="1471-2105-12-180-i20.gif"/></disp-formula></p>
      <p>where the Gaussian process prior over the noisy observations is<disp-formula id="bmcM16"><label>(16)</label><graphic xlink:href="1471-2105-12-180-i21.gif"/></disp-formula></p>
      <sec>
        <title>Predictive equations for GP regression</title>
        <p>We start by defining the <italic>mean function </italic>and the <italic>covariance </italic>between a new time-point x<sub>* </sub>and each of the <italic>i<sup>th </sup></italic>known time-points, where <italic>i </italic>= 1..<italic>N</italic><disp-formula id="bmcM17"><label>(17)</label><graphic xlink:href="1471-2105-12-180-i22.gif"/></disp-formula><disp-formula id="bmcM18"><label>(18)</label><graphic xlink:href="1471-2105-12-180-i23.gif"/></disp-formula></p>
        <p>For every new time-point a new vector k<sub>* </sub>is <italic>concatenated </italic>as an additional row and column to the covariance matrix <bold>K</bold><italic><sub>C </sub></italic>to give<disp-formula id="bmcM19"><label>(19)</label><graphic xlink:href="1471-2105-12-180-i24.gif"/></disp-formula></p>
        <p>where <italic>C = N..N</italic><sub>* </sub>is incremented with every new k<sub>* </sub>added to <bold>K</bold><italic><sub>C</sub></italic>. By considering a zero mean function and eq. (19), the joint distribution <italic>p</italic>(<italic>f</italic>*, y) from eq. (15) can be computed<disp-formula id="bmcM20"><label>(20)</label><graphic xlink:href="1471-2105-12-180-i25.gif"/></disp-formula></p>
        <p>Finally, to derive the <italic>predictive mean </italic>and <italic>covariance </italic>of the posterior distribution from eq. (15) we use the Gaussian identities presented in [21, sec.A.2]. These are the predictive equations for <italic>GP regression </italic>of a single new time-point<disp-formula id="bmcM21"><label>(21)</label><graphic xlink:href="1471-2105-12-180-i26.gif"/></disp-formula><disp-formula id="bmcM22"><label>(22)</label><graphic xlink:href="1471-2105-12-180-i27.gif"/></disp-formula><disp-formula id="bmcM23"><label>(23)</label><graphic xlink:href="1471-2105-12-180-i28.gif"/></disp-formula></p>
        <p>and <italic>K<sub>f </sub></italic>= <italic>K<sub>f </sub></italic>(<bold>x</bold>, <bold>x</bold>). These equations can be generalised easily for the prediction of function values at multiple new time-points by augmenting <bold>k</bold><sub>* </sub>with more columns and <italic>k</italic>(<bold>x</bold>*, <bold>x</bold>*) with more components, one for each new time-point <bold>x</bold>*.</p>
      </sec>
    </sec>
    <sec>
      <title>Hyperparameter learning</title>
      <p>Given the SE covariance function, one can learn the hyperparameters from the data by optimising the log-marginal likelihood function of the GP. In general, a non-parametric model such as the GP can employ a variety of kernel families whose hyperparameters can be adapted with respect to the underlying intensity and frequency of the local signal structure, and interpolate it in a probabilistic fashion (i.e. while quantifying the uncertainty of prediction). The SE kernel allows one to give intuitive interpretations of the adapted hyperparameters, especially for one-dimensional data such as a gene expression time-series, see Figure <xref ref-type="fig" rid="F5">5</xref> for interpretations of various local-optima.</p>
      <sec>
        <title>Optimising the marginal likelihood</title>
        <p>In the context of GP models the marginal likelihood results from the marginalisation over function values <bold>f</bold><disp-formula id="bmcM24"><label>(24)</label><graphic xlink:href="1471-2105-12-180-i29.gif"/></disp-formula></p>
        <p>where the <italic>GP prior p</italic>(<bold>f|x</bold>) is given in eq. (9) and the likelihood is a factorised Gaussian <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i30.gif"/></inline-formula>. The integral can be evaluated analytically [21, sec. A.2] to give the <italic>log-marginal likelihood </italic>(LML, it is common practice to take the log of the antiderivative for the sake of numerical stability, as it yields a sum instead of a product)<disp-formula id="bmcM25"><label>(25)</label><graphic xlink:href="1471-2105-12-180-i31.gif"/></disp-formula></p>
        <p>We notice that the marginal here is explicitly conditioned on <italic>Î¸ </italic>(<italic>hyperparameters</italic>) to emphasise that it is a function of the hyperparameters through <bold>K</bold><italic><sub>f</sub></italic>. To optimise the marginal likelihood we take the partial derivatives of the LML with respect to the hyperparameters<disp-formula id="bmcM26"><label>(26)</label><graphic xlink:href="1471-2105-12-180-i32.gif"/></disp-formula></p>
        <p>We use <italic>scaled conjugate gradients </italic>[<xref ref-type="bibr" rid="B32">32</xref>] -- a standard optimisation scheme -- to maximise the LML.</p>
      </sec>
    </sec>
    <sec>
      <title>Ranking with likelihood-ratios</title>
      <p>Alternatively, one may choose to go "fully Bayesian" by placing a <italic>hyper-prior </italic>over the hyperparameters <italic>p</italic>(<bold><italic>Î¸ </italic></bold>|<inline-formula><inline-graphic xlink:href="1471-2105-12-180-i33.gif"/></inline-formula>), where <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i33.gif"/></inline-formula> represents some type of model, and compute a posterior over hyperparameters<disp-formula id="bmcM27"><label>(27)</label><graphic xlink:href="1471-2105-12-180-i34.gif"/></disp-formula></p>
      <p>based on some initial beliefs, such as the functions having large lengthscales, and optimise the marginal likelihood so that the optimum lengthscale tends to a large value, unless there is evidence to the contrary. Depending on the model <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i33.gif"/></inline-formula>, the integral in eq. (27) may be analytically intractable and thus one has to resort to approximating this quantity [<xref ref-type="bibr" rid="B33">33</xref>] (e.g. Laplace approximation) or using <italic>Markov Chain Monte Carlo </italic>(MCMC) methods to sample from the posterior distribution [<xref ref-type="bibr" rid="B34">34</xref>].</p>
      <p>In the case where one is using different types of models (e.g. with different number of hyperparameters), a Bayesian-standard way of comparing between such two models is through Bayes factors [<xref ref-type="bibr" rid="B11">11</xref>,<xref ref-type="bibr" rid="B19">19</xref>,<xref ref-type="bibr" rid="B23">23</xref>] -- ratios of the <italic>integral </italic>quantities in eq. (27)<disp-formula id="bmcM28"><label>(28)</label><graphic xlink:href="1471-2105-12-180-i35.gif"/></disp-formula></p>
      <p>where the models <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i33.gif"/></inline-formula> usually represent two different hypotheses, namely <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i36.gif"/></inline-formula> - the profile has a significant underlying signal and thus it is truly differentially expressed and <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i37.gif"/></inline-formula> - there is no underlying signal in the profile and the observed gene expression is just the effect of random noise. The ranking is based on how likely <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i36.gif"/></inline-formula> in comparison to <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i37.gif"/></inline-formula>, given a profile.</p>
      <p>In this paper we present a much simpler -- but effective to the task -- approach to ranking the differential expression of a profile. Instead of integrating out the hyperparameters, we approximate the Bayes factor with a log-ratio of marginal likelihoods (cf. eq. (25))<disp-formula id="bmcM29"><label>(29)</label><graphic xlink:href="1471-2105-12-180-i38.gif"/></disp-formula></p>
      <p>with each LML being a function of different instantiations of <bold><italic>Î¸</italic></bold>. We still maintain hypotheses <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i36.gif"/></inline-formula> and <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i37.gif"/></inline-formula> that represent the same notions explained above, but in our case they differ simply by configurations of <bold><italic>Î¸</italic></bold>. Specifically, on <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i36.gif"/></inline-formula> the hyperparameters are <bold>fixed </bold>to <bold><italic>Î¸</italic></bold><sub>1 </sub>= (â, 0; var(<bold>y</bold>))<sup>â¤ </sup>to encode a function constant in time (<italic>l </italic><sup>2 </sup>â â), with no underlying signal <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i39.gif"/></inline-formula>, which generates a time-series with a variance that can be solely explained by noise <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i40.gif"/></inline-formula>. Analogously, on <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i37.gif"/></inline-formula> the hyperparameters <bold><italic>Î¸</italic></bold><sub>2 </sub>are initialised to encode a function that fluctuates in accordance to a typical significant profile (e.g. â<sup>2 </sup>= 20), with a distinct signal variance that solely explains the observed time-series variance <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i41.gif"/></inline-formula> and with no noise <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i42.gif"/></inline-formula>.</p>
      <sec>
        <title>Local optima of the log-marginal likelihood (LML) function</title>
        <p>These two configurations correspond to two points in the three-dimensional function that is the LML, both of which usually lie close to local-optimum solutions. This assumption can be verified, empirically, by exhaustively plotting the LML function for a number of profiles, see Figure <xref ref-type="fig" rid="F5">5</xref>. In case the LML contour differs for some profiles, more initialisation points should be used to ensure convergence to the maximum-likelihood solution. Because the configuration of the second hypothesis (no noise, <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i43.gif"/></inline-formula>) is an extremely unlikely scenario, we let <bold><italic>Î¸</italic></bold><sub>2 </sub>adapt to a given profile by optimising the LML function, as opposed to keeping it fixed like <bold><italic>Î¸</italic></bold><sub>1</sub>.</p>
        <p>In most cases the LML (eq. (25)) is not convex. Multiple optima do not necessarily pose a threat here; depending on the data and as long as they have similar function values, multiple optima present alternative interpretations on the observations. To alleviate the problem of spurious local optimum solutions however, we make the following observation: when we explicitly restrict the signal variance hyperparameter (<inline-formula><inline-graphic xlink:href="1471-2105-12-180-i3.gif"/></inline-formula>) to low values during optimisation, we also implicitly restrict the noise variance hyperparameter (<inline-formula><inline-graphic xlink:href="1471-2105-12-180-i3.gif"/></inline-formula>) to large values. This occurs as the explanation of the observed data variance (var(<bold>y</bold>)) is shared between the signal and noise variance hyperparameters, i.e. <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i44.gif"/></inline-formula>. This dependency allows us to treat the three-dimension optimisation problem as a two-dimension problem, one of lengthscale â <sup>2 </sup>and one of signal-to-noise ratio <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i45.gif"/></inline-formula> without fear of missing out an optima.</p>
        <p>Figure <xref ref-type="fig" rid="F5">5</xref> illustrates the marginal likelihood as a function of the characteristic lengthscale â<sup>2 </sup>and the SNR. It features two local optima, one for a small lengthscale and a high SNR, where the observed data are explained with a relatively complex function and a small noise variance, and one optimum for a large lengthscale and a low SNR, where the data are explained by a simpler function with high noise variance. We also notice that the first optimum has a lower LML. This relates to the algebraic structure of the LML (eq. (25)); the first term (dot product) promotes data fitness and the second term (determinant) penalizes the complexity of the model [21, sec.5.4]. Overall, the LML function of the Gaussian process offers a good fitness-complexity trade-off without the need for additional regularisation. Optionally, one can use multiple initialisation points focusing on different non-infinite lengthscales to deal with the multiple local optima along the lengthscale axis, and pick the best solution (max LML) to represent the <inline-formula><inline-graphic xlink:href="1471-2105-12-180-i36.gif"/></inline-formula> hypothesis in the likelihood-ratio during the ranking stage.</p>
      </sec>
    </sec>
    <sec>
      <title>Source code</title>
      <p>The source code for the GP regression framework is available in MATLAB code <ext-link ext-link-type="uri" xlink:href="http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/gp/">http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/gp/</ext-link> and as a package for the R statistical computing language <ext-link ext-link-type="uri" xlink:href="http://cran.r-project.org/web/packages/gptk/">http://cran.r-project.org/web/packages/gptk/</ext-link>. The routines for the estimation and ranking of the gene expression time-series are available upon request for both languages. The time needed to analyse the 22690 profiles in the experimental data, with only the basic two initialisation points of hyperparameters, is about 30 minutes on a desktop running Ubuntu 10.04 with a dual-core CPU at 2.8 GHz and 3.2 GiB of memory.</p>
    </sec>
  </sec>
  <sec>
    <title>Authors' contributions</title>
    <p>AAK designed and implemented the computational analysis and ranking scheme presented here, assessed the various methods and drafted the manuscript. NDL pre-processed the experimental data and wrote the original Gaussian process toolkit for MATLAB and AAK rewrote it for the R statistical language. Both AAK and NDL participated in interpreting the results and revising the manuscript. All authors read and approved the final manuscript.</p>
  </sec>
</body>
<back>
  <sec>
    <title>Acknowledgements</title>
    <p>The authors would like to thank Diego di Bernardo for his useful feedback on the experimental data. Research was partially supported by a EPSRC Doctoral Training Award, the Department of Neuroscience, University of Sheffield and BBSRC (grant BB/H018123/2).</p>
  </sec>
  <ref-list>
    <ref id="B1">
      <mixed-citation publication-type="journal">
        <name>
          <surname>LÃ¶nnstedt</surname>
          <given-names>I</given-names>
        </name>
        <name>
          <surname>Speed</surname>
          <given-names>TP</given-names>
        </name>
        <article-title>Replicated microarray data</article-title>
        <source>Statistica Sinica</source>
        <year>2002</year>
        <volume>12</volume>
        <fpage>31</fpage>
        <lpage>46</lpage>
      </mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Spellman</surname>
          <given-names>PT</given-names>
        </name>
        <name>
          <surname>Sherlock</surname>
          <given-names>G</given-names>
        </name>
        <name>
          <surname>Zhang</surname>
          <given-names>MQ</given-names>
        </name>
        <name>
          <surname>Iyer</surname>
          <given-names>VR</given-names>
        </name>
        <name>
          <surname>Anders</surname>
          <given-names>K</given-names>
        </name>
        <name>
          <surname>Eisen</surname>
          <given-names>MB</given-names>
        </name>
        <name>
          <surname>Brown</surname>
          <given-names>PO</given-names>
        </name>
        <name>
          <surname>Botstein</surname>
          <given-names>D</given-names>
        </name>
        <name>
          <surname>Futcher</surname>
          <given-names>B</given-names>
        </name>
        <article-title>Comprehensive identification of cell cycle-regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization</article-title>
        <source>Molecular biology of the cell</source>
        <year>1998</year>
        <volume>9</volume>
        <issue>12</issue>
        <fpage>3273</fpage>
        <pub-id pub-id-type="pmid">9843569</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Friedman</surname>
          <given-names>N</given-names>
        </name>
        <name>
          <surname>Linial</surname>
          <given-names>M</given-names>
        </name>
        <name>
          <surname>Nachman</surname>
          <given-names>I</given-names>
        </name>
        <name>
          <surname>Pe'er</surname>
          <given-names>D</given-names>
        </name>
        <article-title>Using Bayesian networks to analyze expression data</article-title>
        <source>Journal of computational biology</source>
        <year>2000</year>
        <volume>7</volume>
        <issue>3-4</issue>
        <fpage>601</fpage>
        <lpage>620</lpage>
        <pub-id pub-id-type="doi">10.1089/106652700750050961</pub-id>
        <pub-id pub-id-type="pmid">11108481</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Dudoit</surname>
          <given-names>S</given-names>
        </name>
        <name>
          <surname>Yang</surname>
          <given-names>YH</given-names>
        </name>
        <name>
          <surname>Callow</surname>
          <given-names>MJ</given-names>
        </name>
        <name>
          <surname>Speed</surname>
          <given-names>TP</given-names>
        </name>
        <article-title>Statistical methods for identifying differentially expressed genes in replicated cDNA microarray experiments</article-title>
        <source>Statistica sinica</source>
        <year>2002</year>
        <volume>12</volume>
        <fpage>111</fpage>
        <lpage>140</lpage>
      </mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Kerr</surname>
          <given-names>MK</given-names>
        </name>
        <name>
          <surname>Martin</surname>
          <given-names>M</given-names>
        </name>
        <name>
          <surname>Churchill</surname>
          <given-names>GA</given-names>
        </name>
        <article-title>Analysis of variance for gene expression microarray data</article-title>
        <source>Journal of Computational Biology</source>
        <year>2000</year>
        <volume>7</volume>
        <issue>6</issue>
        <fpage>819</fpage>
        <lpage>837</lpage>
        <pub-id pub-id-type="doi">10.1089/10665270050514954</pub-id>
        <pub-id pub-id-type="pmid">11382364</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Efron</surname>
          <given-names>B</given-names>
        </name>
        <name>
          <surname>Tibshirani</surname>
          <given-names>R</given-names>
        </name>
        <name>
          <surname>Storey</surname>
          <given-names>JD</given-names>
        </name>
        <name>
          <surname>Tusher</surname>
          <given-names>V</given-names>
        </name>
        <article-title>Empirical Bayes analysis of a microarray experiment</article-title>
        <source>Journal of the American Statistical Association</source>
        <year>2001</year>
        <volume>96</volume>
        <issue>456</issue>
        <fpage>1151</fpage>
        <lpage>1160</lpage>
        <pub-id pub-id-type="doi">10.1198/016214501753382129</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Bar-Joseph</surname>
          <given-names>Z</given-names>
        </name>
        <name>
          <surname>Gerber</surname>
          <given-names>G</given-names>
        </name>
        <name>
          <surname>Simon</surname>
          <given-names>I</given-names>
        </name>
        <name>
          <surname>Gifford</surname>
          <given-names>DK</given-names>
        </name>
        <name>
          <surname>Jaakkola</surname>
          <given-names>TS</given-names>
        </name>
        <article-title>Comparing the continuous representation of time-series expression profiles to identify differentially expressed genes</article-title>
        <source>Proceedings of the National Academy of Sciences of the United States of America</source>
        <year>2003</year>
        <volume>100</volume>
        <issue>18</issue>
        <fpage>10146</fpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1732547100</pub-id>
        <pub-id pub-id-type="pmid">12934016</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Ernst</surname>
          <given-names>J</given-names>
        </name>
        <name>
          <surname>Nau</surname>
          <given-names>G</given-names>
        </name>
        <name>
          <surname>Bar-Joseph</surname>
          <given-names>Z</given-names>
        </name>
        <article-title>Clustering short time series gene expression data</article-title>
        <source>Bioinformatics</source>
        <year>2005</year>
        <volume>21</volume>
        <issue>Suppl 1</issue>
        <fpage>i159</fpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bti1022</pub-id>
        <pub-id pub-id-type="pmid">15961453</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Storey</surname>
          <given-names>JD</given-names>
        </name>
        <name>
          <surname>Xiao</surname>
          <given-names>W</given-names>
        </name>
        <name>
          <surname>Leek</surname>
          <given-names>JT</given-names>
        </name>
        <name>
          <surname>Tompkins</surname>
          <given-names>RG</given-names>
        </name>
        <name>
          <surname>Davis</surname>
          <given-names>RW</given-names>
        </name>
        <article-title>Significance analysis of time course microarray experiments</article-title>
        <source>Proceedings of the National Academy of Sciences of the United States of America</source>
        <year>2005</year>
        <volume>102</volume>
        <issue>36</issue>
        <fpage>12837</fpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0504609102</pub-id>
        <pub-id pub-id-type="pmid">16141318</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Tai</surname>
          <given-names>YC</given-names>
        </name>
        <name>
          <surname>Speed</surname>
          <given-names>TP</given-names>
        </name>
        <article-title>A multivariate empirical Bayes statistic for replicated microarray time course data</article-title>
        <source>The Annals of Statistics</source>
        <year>2006</year>
        <volume>34</volume>
        <issue>5</issue>
        <fpage>2387</fpage>
        <lpage>2412</lpage>
        <pub-id pub-id-type="doi">10.1214/009053606000000759</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Angelini</surname>
          <given-names>C</given-names>
        </name>
        <name>
          <surname>De Canditiis</surname>
          <given-names>D</given-names>
        </name>
        <name>
          <surname>Mutarelli</surname>
          <given-names>M</given-names>
        </name>
        <name>
          <surname>Pensky</surname>
          <given-names>M</given-names>
        </name>
        <article-title>A Bayesian approach to estimation and testing in time-course microarray experiments</article-title>
        <source>Stat Appl Genet Mol Biol</source>
        <year>2007</year>
        <volume>6</volume>
        <fpage>24</fpage>
      </mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Angelini</surname>
          <given-names>C</given-names>
        </name>
        <name>
          <surname>Cutillo</surname>
          <given-names>L</given-names>
        </name>
        <name>
          <surname>De Canditiis</surname>
          <given-names>D</given-names>
        </name>
        <name>
          <surname>Mutarelli</surname>
          <given-names>M</given-names>
        </name>
        <name>
          <surname>Pensky</surname>
          <given-names>M</given-names>
        </name>
        <article-title>BATS: a Bayesian user-friendly software for Analyzing Time Series microarray experiments</article-title>
        <source>BMC bioinformatics</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>415</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-9-415</pub-id>
        <pub-id pub-id-type="pmid">18837969</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Della Gatta</surname>
          <given-names>G</given-names>
        </name>
        <name>
          <surname>Bansal</surname>
          <given-names>M</given-names>
        </name>
        <name>
          <surname>Ambesi-Impiombato</surname>
          <given-names>A</given-names>
        </name>
        <name>
          <surname>Antonini</surname>
          <given-names>D</given-names>
        </name>
        <name>
          <surname>Missero</surname>
          <given-names>C</given-names>
        </name>
        <name>
          <surname>di Bernardo</surname>
          <given-names>D</given-names>
        </name>
        <article-title>Direct targets of the TRP63 transcription factor revealed by a combination of gene expression profiling and reverse engineering</article-title>
        <source>Genome research</source>
        <year>2008</year>
        <volume>18</volume>
        <issue>6</issue>
        <fpage>939</fpage>
        <pub-id pub-id-type="doi">10.1101/gr.073601.107</pub-id>
        <pub-id pub-id-type="pmid">18441228</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Honkela</surname>
          <given-names>A</given-names>
        </name>
        <name>
          <surname>Girardot</surname>
          <given-names>C</given-names>
        </name>
        <name>
          <surname>Gustafson</surname>
          <given-names>EH</given-names>
        </name>
        <name>
          <surname>Liu</surname>
          <given-names>YH</given-names>
        </name>
        <name>
          <surname>Furlong</surname>
          <given-names>EEM</given-names>
        </name>
        <name>
          <surname>Lawrence</surname>
          <given-names>ND</given-names>
        </name>
        <name>
          <surname>Rattray</surname>
          <given-names>M</given-names>
        </name>
        <article-title>Model-based method for transcription factor target identification with limited data</article-title>
        <source>Proceedings of the National Academy of Sciences</source>
        <year>2010</year>
        <volume>107</volume>
        <issue>17</issue>
        <fpage>7793</fpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0914285107</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Bansal</surname>
          <given-names>M</given-names>
        </name>
        <name>
          <surname>Gatta</surname>
          <given-names>GD</given-names>
        </name>
        <name>
          <surname>Di Bernardo</surname>
          <given-names>D</given-names>
        </name>
        <article-title>Inference of gene regulatory networks and compound mode of action from time course gene expression profiles</article-title>
        <source>Bioinformatics</source>
        <year>2006</year>
        <volume>22</volume>
        <issue>7</issue>
        <fpage>815</fpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl003</pub-id>
        <pub-id pub-id-type="pmid">16418235</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Finkenstadt</surname>
          <given-names>B</given-names>
        </name>
        <name>
          <surname>Heron</surname>
          <given-names>EA</given-names>
        </name>
        <name>
          <surname>Komorowski</surname>
          <given-names>M</given-names>
        </name>
        <name>
          <surname>Edwards</surname>
          <given-names>K</given-names>
        </name>
        <name>
          <surname>Tang</surname>
          <given-names>S</given-names>
        </name>
        <name>
          <surname>Harper</surname>
          <given-names>CV</given-names>
        </name>
        <name>
          <surname>Davis</surname>
          <given-names>JRE</given-names>
        </name>
        <name>
          <surname>White</surname>
          <given-names>MRH</given-names>
        </name>
        <name>
          <surname>Millar</surname>
          <given-names>AJ</given-names>
        </name>
        <name>
          <surname>Rand</surname>
          <given-names>DA</given-names>
        </name>
        <article-title>Reconstruction of transcriptional dynamics from gene reporter data using differential equations</article-title>
        <source>Bioinformatics</source>
        <year>2008</year>
        <volume>24</volume>
        <issue>24</issue>
        <fpage>2901</fpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btn562</pub-id>
        <pub-id pub-id-type="pmid">18974172</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Bar-Joseph</surname>
          <given-names>Z</given-names>
        </name>
        <article-title>Analyzing time series gene expression data</article-title>
        <source>Bioinformatics</source>
        <year>2004</year>
        <volume>20</volume>
        <issue>16</issue>
        <fpage>2493</fpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bth283</pub-id>
        <pub-id pub-id-type="pmid">15130923</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Bay</surname>
          <given-names>SD</given-names>
        </name>
        <name>
          <surname>Chrisman</surname>
          <given-names>L</given-names>
        </name>
        <name>
          <surname>Pohorille</surname>
          <given-names>A</given-names>
        </name>
        <name>
          <surname>Shrager</surname>
          <given-names>J</given-names>
        </name>
        <article-title>Temporal aggregation bias and inference of causal regulatory networks</article-title>
        <source>Journal of Computational Biology</source>
        <year>2004</year>
        <volume>11</volume>
        <issue>5</issue>
        <fpage>971</fpage>
        <lpage>985</lpage>
        <pub-id pub-id-type="doi">10.1089/cmb.2004.11.971</pub-id>
        <pub-id pub-id-type="pmid">15700412</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Stegle</surname>
          <given-names>O</given-names>
        </name>
        <name>
          <surname>Denby</surname>
          <given-names>KJ</given-names>
        </name>
        <name>
          <surname>Cooke</surname>
          <given-names>EJ</given-names>
        </name>
        <name>
          <surname>Wild</surname>
          <given-names>DL</given-names>
        </name>
        <name>
          <surname>Ghahramani</surname>
          <given-names>Z</given-names>
        </name>
        <name>
          <surname>Borgwardt</surname>
          <given-names>KM</given-names>
        </name>
        <article-title>A robust Bayesian two-sample test for detecting intervals of differential gene expression in microarray time series</article-title>
        <source>Journal of Computational Biology</source>
        <year>2010</year>
        <volume>17</volume>
        <issue>3</issue>
        <fpage>355</fpage>
        <lpage>367</lpage>
        <pub-id pub-id-type="doi">10.1089/cmb.2009.0175</pub-id>
        <pub-id pub-id-type="pmid">20377450</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Schena</surname>
          <given-names>M</given-names>
        </name>
        <name>
          <surname>Shalon</surname>
          <given-names>D</given-names>
        </name>
        <name>
          <surname>Davis</surname>
          <given-names>RW</given-names>
        </name>
        <name>
          <surname>Brown</surname>
          <given-names>PO</given-names>
        </name>
        <article-title>Quantitative monitoring of gene expression patterns with a complementary DNA microarray</article-title>
        <source>Science</source>
        <year>1995</year>
        <volume>270</volume>
        <issue>5235</issue>
        <fpage>467</fpage>
        <pub-id pub-id-type="doi">10.1126/science.270.5235.467</pub-id>
        <pub-id pub-id-type="pmid">7569999</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="book">
        <name>
          <surname>Rasmussen</surname>
          <given-names>CE</given-names>
        </name>
        <name>
          <surname>Williams</surname>
          <given-names>CKI</given-names>
        </name>
        <source>Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)</source>
        <year>2005</year>
        <publisher-name>The MIT Press</publisher-name>
      </mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="book">
        <name>
          <surname>MacKay</surname>
          <given-names>DJC</given-names>
        </name>
        <article-title>Gaussian Processes</article-title>
        <source>Information theory, inference, and learning algorithms</source>
        <year>2003</year>
        <publisher-name>Cambridge University Press</publisher-name>
        <fpage>535</fpage>
        <lpage>548</lpage>
      </mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Yuan</surname>
          <given-names>M</given-names>
        </name>
        <article-title>Flexible temporal expression profile modelling using the Gaussian process</article-title>
        <source>Computational statistics &amp; data analysis</source>
        <year>2006</year>
        <volume>51</volume>
        <issue>3</issue>
        <fpage>1754</fpage>
        <lpage>1764</lpage>
        <pub-id pub-id-type="doi">10.1016/j.csda.2005.11.017</pub-id>
        <pub-id pub-id-type="pmid">21617730</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Kirk</surname>
          <given-names>PDW</given-names>
        </name>
        <name>
          <surname>Stumpf</surname>
          <given-names>MPH</given-names>
        </name>
        <article-title>Gaussian process regression bootstrapping: exploring the effects of uncertainty in time course data</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <issue>10</issue>
        <fpage>1300</fpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp139</pub-id>
        <pub-id pub-id-type="pmid">19289448</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Lawrence</surname>
          <given-names>ND</given-names>
        </name>
        <name>
          <surname>Sanguinetti</surname>
          <given-names>G</given-names>
        </name>
        <name>
          <surname>Rattray</surname>
          <given-names>M</given-names>
        </name>
        <article-title>Modelling transcriptional regulation using Gaussian processes</article-title>
        <source>Advances in Neural Information Processing Systems</source>
        <year>2007</year>
        <volume>19</volume>
        <fpage>785</fpage>
      </mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Gao</surname>
          <given-names>P</given-names>
        </name>
        <name>
          <surname>Honkela</surname>
          <given-names>A</given-names>
        </name>
        <name>
          <surname>Rattray</surname>
          <given-names>M</given-names>
        </name>
        <name>
          <surname>Lawrence</surname>
          <given-names>ND</given-names>
        </name>
        <article-title>Gaussian process modelling of latent chemical species: applications to inferring transcription factor activities</article-title>
        <source>Bioinformatics</source>
        <year>2008</year>
        <volume>24</volume>
        <issue>16</issue>
        <fpage>i70</fpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btn278</pub-id>
        <pub-id pub-id-type="pmid">18689843</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="other">
        <name>
          <surname>Stegle</surname>
          <given-names>O</given-names>
        </name>
        <name>
          <surname>Denby</surname>
          <given-names>KJ</given-names>
        </name>
        <name>
          <surname>Wild</surname>
          <given-names>L</given-names>
        </name>
        <name>
          <surname>McHattie</surname>
          <given-names>S</given-names>
        </name>
        <name>
          <surname>Meade</surname>
          <given-names>A</given-names>
        </name>
        <name>
          <surname>Ghahramani</surname>
          <given-names>Z</given-names>
        </name>
        <name>
          <surname>Borgwardt</surname>
          <given-names>KM</given-names>
        </name>
        <article-title>Discovering temporal patterns of differential gene expression in microarray time series</article-title>
        <source>In GCB</source>
        <year>2009</year>
        <fpage>133</fpage>
        <lpage>142</lpage>
      </mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Tipping</surname>
          <given-names>ME</given-names>
        </name>
        <name>
          <surname>Lawrence</surname>
          <given-names>ND</given-names>
        </name>
        <article-title>Variational inference for Student-t models: Robust Bayesian interpolation and generalised component analysis</article-title>
        <source>Neurocomputing</source>
        <year>2005</year>
        <volume>69</volume>
        <issue>1-3</issue>
        <fpage>123</fpage>
        <lpage>141</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2005.02.016</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="other">
        <name>
          <surname>Vanhatalo</surname>
          <given-names>J</given-names>
        </name>
        <name>
          <surname>JylÃ¤nki</surname>
          <given-names>P</given-names>
        </name>
        <name>
          <surname>Vehtari</surname>
          <given-names>A</given-names>
        </name>
        <article-title>Gaussian process regression with Student-t likelihood</article-title>
        <source>Neural Information Processing System, Citeseer</source>
        <year>2009</year>
      </mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Irizarry</surname>
          <given-names>RA</given-names>
        </name>
        <name>
          <surname>Hobbs</surname>
          <given-names>B</given-names>
        </name>
        <name>
          <surname>Collin</surname>
          <given-names>F</given-names>
        </name>
        <name>
          <surname>Beazer-Barclay</surname>
          <given-names>YD</given-names>
        </name>
        <name>
          <surname>Antonellis</surname>
          <given-names>KJ</given-names>
        </name>
        <name>
          <surname>Scherf</surname>
          <given-names>U</given-names>
        </name>
        <name>
          <surname>Speed</surname>
          <given-names>TP</given-names>
        </name>
        <article-title>Exploration, normalization, and summaries of high density oligonucleotide array probe level data</article-title>
        <source>Biostatistics</source>
        <year>2003</year>
        <volume>4</volume>
        <issue>2</issue>
        <fpage>249</fpage>
        <pub-id pub-id-type="doi">10.1093/biostatistics/4.2.249</pub-id>
        <pub-id pub-id-type="pmid">12925520</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Subramanian</surname>
          <given-names>A</given-names>
        </name>
        <name>
          <surname>Tamayo</surname>
          <given-names>P</given-names>
        </name>
        <name>
          <surname>Mootha</surname>
          <given-names>VK</given-names>
        </name>
        <name>
          <surname>Mukherjee</surname>
          <given-names>S</given-names>
        </name>
        <name>
          <surname>Ebert</surname>
          <given-names>BL</given-names>
        </name>
        <name>
          <surname>Gillette</surname>
          <given-names>MA</given-names>
        </name>
        <name>
          <surname>Paulovich</surname>
          <given-names>A</given-names>
        </name>
        <name>
          <surname>Pomeroy</surname>
          <given-names>SL</given-names>
        </name>
        <name>
          <surname>Golub</surname>
          <given-names>TR</given-names>
        </name>
        <name>
          <surname>Lander</surname>
          <given-names>ES</given-names>
        </name>
        <name>
          <surname>Mesirov</surname>
          <given-names>JP</given-names>
        </name>
        <article-title>Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles</article-title>
        <source>Proceedings of the National Academy of Sciences</source>
        <year>2005</year>
        <volume>102</volume>
        <issue>43</issue>
        <fpage>15545</fpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0506580102</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal">
        <name>
          <surname>MÃ¶ller</surname>
          <given-names>MF</given-names>
        </name>
        <article-title>A scaled conjugate gradient algorithm for fast supervised learning</article-title>
        <source>Neural networks</source>
        <year>1993</year>
        <volume>6</volume>
        <issue>4</issue>
        <fpage>525</fpage>
        <lpage>533</lpage>
        <pub-id pub-id-type="doi">10.1016/S0893-6080(05)80056-5</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal">
        <name>
          <surname>MacKay</surname>
          <given-names>DJC</given-names>
        </name>
        <article-title>Comparison of approximate methods for handling hyperparameters</article-title>
        <source>Neural Computation</source>
        <year>1999</year>
        <volume>11</volume>
        <issue>5</issue>
        <fpage>1035</fpage>
        <lpage>1068</lpage>
        <pub-id pub-id-type="doi">10.1162/089976699300016331</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="other">
        <name>
          <surname>Neal</surname>
          <given-names>RM</given-names>
        </name>
        <article-title>Monte Carlo implementation of Gaussian process models for Bayesian regression and classification</article-title>
        <source>Arxiv preprint physics/9701026</source>
        <year>1997</year>
      </mixed-citation>
    </ref>
  </ref-list>
</back>
