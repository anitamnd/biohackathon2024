<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5870774</article-id>
    <article-id pub-id-type="pmid">28881966</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btx231</article-id>
    <article-id pub-id-type="publisher-id">btx231</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Ismb/Eccb 2017: The 25th Annual Conference Intelligent Systems for Molecular Biology Held Jointly with the 16th Annual European Conference on Computational Biology, Prague, Czech Republic, July 21–25, 2017</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Function</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DextMP: deep dive into text for predicting moonlighting proteins</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Khan</surname>
          <given-names>Ishita K</given-names>
        </name>
        <xref ref-type="aff" rid="btx231-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bhuiyan</surname>
          <given-names>Mansurul</given-names>
        </name>
        <xref ref-type="aff" rid="btx231-aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kihara</surname>
          <given-names>Daisuke</given-names>
        </name>
        <xref ref-type="aff" rid="btx231-aff1">1</xref>
        <xref ref-type="aff" rid="btx231-aff3">3</xref>
        <xref ref-type="corresp" rid="btx231-cor1"/>
        <!--<email>dkihara@purdue.edu</email>-->
      </contrib>
    </contrib-group>
    <aff id="btx231-aff1"><label>1</label>Department of Computer Science, Purdue University, West Lafayette, IN, USA</aff>
    <aff id="btx231-aff2"><label>2</label>Department of Computer Science, Indiana University-Purdue University Indianapolis (IUPUI), Indianapolis, IN, USA</aff>
    <aff id="btx231-aff3"><label>3</label>Department of Biological Science, Purdue University, West Lafayette, IN, USA</aff>
    <author-notes>
      <corresp id="btx231-cor1">To whom correspondence should be addressed. Email: <email>dkihara@purdue.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>15</day>
      <month>7</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2017-07-12">
      <day>12</day>
      <month>7</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>7</month>
      <year>2017</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>33</volume>
    <issue>14</issue>
    <fpage>i83</fpage>
    <lpage>i91</lpage>
    <permissions>
      <copyright-statement>© The Author 2017. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2017</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btx231.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="SA1">
        <title>Motivation</title>
        <p>Moonlighting proteins (MPs) are an important class of proteins that perform more than one independent cellular function. MPs are gaining more attention in recent years as they are found to play important roles in various systems including disease developments. MPs also have a significant impact in computational function prediction and annotation in databases. Currently MPs are not labeled as such in biological databases even in cases where multiple distinct functions are known for the proteins. In this work, we propose a novel method named DextMP, which predicts whether a protein is a MP or not based on its textual features extracted from scientific literature and the UniProt database.</p>
      </sec>
      <sec id="SA2">
        <title>Results</title>
        <p>DextMP extracts three categories of textual information for a protein: titles, abstracts from literature, and function description in UniProt. Three language models were applied and compared: a state-of-the-art deep unsupervised learning algorithm along with two other language models of different types, Term Frequency-Inverse Document Frequency in the bag-of-words and Latent Dirichlet Allocation in the topic modeling category. Cross-validation results on a dataset of known MPs and non-MPs showed that DextMP successfully predicted MPs with over 91% accuracy with significant improvement over existing MP prediction methods. Lastly, we ran DextMP with the best performing language models and text-based feature combinations on three genomes, human, yeast and <italic>Xenopus laevis</italic>, and found that about 2.5–35% of the proteomes are potential MPs.</p>
      </sec>
      <sec id="SA3">
        <title>Availability and Implementation</title>
        <p>Code available at <ext-link ext-link-type="uri" xlink:href="http://kiharalab.org/DextMP">http://kiharalab.org/DextMP</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">National Science Foundation</named-content>
          <named-content content-type="funder-identifier">10.13039/100000001</named-content>
        </funding-source>
        <award-id>DBI1262189</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">National Institutes of Health</named-content>
          <named-content content-type="funder-identifier">10.13039/100000002</named-content>
        </funding-source>
        <award-id>R01GM097528</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">National Science Foundation</named-content>
          <named-content content-type="funder-identifier">10.13039/100000001</named-content>
        </funding-source>
        <award-id>IIS1319551</award-id>
        <award-id>IOS1127027</award-id>
        <award-id>DMS1614777</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Investigation of function of proteins is a central problem in bioinformatics as it is an essential step for unfolding obscurities of cellular processes. Although a majority of proteins are speculated to perform a single function, over the past decade a significant number of multifunctional, or more popularly called ‘moonlighting’ proteins are emerging into attention in the biology community (<xref rid="btx231-B2" ref-type="bibr">Campbell and Scanes, 1995</xref>; <xref rid="btx231-B17" ref-type="bibr">Jeffery, 1999</xref>; <xref rid="btx231-B42" ref-type="bibr">Weaver, 1998</xref>). Moonlighting proteins (MPs) are defined as proteins that perform multiple independent cellular functions within a single polypeptide chain. Functional diversity of these proteins are not due to gene fusions, multiple domains in the same protein chain, multiple RNA splice variants or proteolytic fragments, families of homologous proteins or pleotropic effects (<xref rid="btx231-B15" ref-type="bibr">Huberts and Vander Klei, 2010</xref>; <xref rid="btx231-B17" ref-type="bibr">Jeffery, 1999</xref>, <xref rid="btx231-B18" ref-type="bibr">2004</xref>; <xref rid="btx231-B29" ref-type="bibr">Mani <italic>et al.</italic>, 2014</xref>). Most prominent examples of MPs are enzymes (<xref rid="btx231-B17" ref-type="bibr">Jeffery, 1999</xref>, <xref rid="btx231-B18" ref-type="bibr">2004</xref>). The first of such findings was in late 1980s, crystallins (<xref rid="btx231-B33" ref-type="bibr">Piatigorsky and Wistow, 1989</xref>; <xref rid="btx231-B44" ref-type="bibr">Wistow and Kim, 1991</xref>), which are structural eye lens proteins that also have enzyme function. Since then, MPs are continued to be found in a wide variety of genomes with diverse cellular functions and molecular mechanisms for switching functions.</p>
    <p>In parallel to serendipitous findings of MPs through experiments, bioinformatics approaches have been applied to characterize MPs in recent years (<xref rid="btx231-B24" ref-type="bibr">Khan and Kihara, 2014</xref>). Existing studies investigated different aspects of MPs such as sequence similarity (<xref rid="btx231-B6" ref-type="bibr">Gomez <italic>et al.</italic>, 2003</xref>; <xref rid="btx231-B26" ref-type="bibr">Khan <italic>et al.</italic>, 2012</xref>), conserved motifs/domains, structural disorder (<xref rid="btx231-B12" ref-type="bibr">Hernández <italic>et al.</italic>, 2011</xref>), and protein-protein interaction (PPI) patterns (<xref rid="btx231-B3" ref-type="bibr">Chapple <italic>et al.</italic>, 2015</xref>; <xref rid="btx231-B5" ref-type="bibr">Gómez <italic>et al.</italic>, 2011</xref>; <xref rid="btx231-B34" ref-type="bibr">Pritykin <italic>et al.</italic>, 2015</xref>). We have recently developed a computational prediction method named MPFit, which predicts MPs and non-MPs using a diverse set of proteomics data (<xref rid="btx231-B25" ref-type="bibr">Khan and Kihara, 2016</xref>). Development of MPFit was based on our previous study where we presented a systematic characterizations of MPs in a computational framework (<xref rid="btx231-B23" ref-type="bibr">Khan <italic>et al.</italic>, 2014</xref>). However, all these existing studies overlook a major resource of information of protein function, i.e. text-based information that underlies in scientific literature and textual description of protein annotation in databases such as UniProt (<xref rid="btx231-B41" ref-type="bibr">UniProt Consortium, 2014</xref>). In most cases MPs are not explicitly labelled in the database with ‘moonlighting’, ‘dual function’, ‘multitasking’, or other related words, even in cases where two distinct functions are known and clearly stated in its database entry. To accommodate the current limited knowledge of MPs, two online repositories of MPs (<xref rid="btx231-B13" ref-type="bibr">Hernández <italic>et al.</italic>, 2014</xref>; <xref rid="btx231-B29" ref-type="bibr">Mani <italic>et al.</italic>, 2014</xref>) were built on expert knowledge with manual curation from literature. This situation convinced us that application of text mining techniques on MP literature would provide a major boost towards automatic MP annotation. In this work we propose a first text mining-based approach for predicting MPs, named DextMP (Deep dive into tEXT for predicting Moonlighting Proteins).</p>
    <p>For the last decade, text mining techniques has been extensively developed to unravel non-trivial knowledge from structured/unstructured text data (<xref rid="btx231-B30" ref-type="bibr">Manning <italic>et al.</italic>, 2008</xref>). Most of the existing works are based on <italic>bag-of-words</italic> that leverages word-related statistics in the text (<xref rid="btx231-B19" ref-type="bibr">Joachims, 1998</xref>). The next generation of text-based feature learning models represent each text with a distribution of latent topics (<xref rid="btx231-B14" ref-type="bibr">Hoffman <italic>et al.</italic>, 2010</xref>). In recent years, unsupervised deep learning-based feature construction has become popular in text mining (<xref rid="btx231-B31" ref-type="bibr">Mikolov <italic>et al.</italic>, 2013</xref>). Such deep-learning-based methods map text into a condensed <italic>d</italic>-dimensional continuous vector space such that semantically similar texts are embedded nearby each other.</p>
    <p>DextMP consists of four logical steps: first, it extracts textual information of proteins from literature (publication titles or abstracts) and functional description in UniProt. Next, it constructs a <italic>k</italic>-dimensional feature vector from each text. In this step, a state-of-the-art deep unsupervised learning algorithm is applied, which is called <italic>paragraph vector</italic> (<xref rid="btx231-B27" ref-type="bibr">Le and Mikolov, 2014</xref>), along with two other widely used language models, Term Frequency-Inverse Document Frequency (TFIDF) in the bag-of-words category (<xref rid="btx231-B30" ref-type="bibr">Manning <italic>et al.</italic>, 2008</xref>) and Latent Dirichlet Allocation (LDA) in the topic modeling category (<xref rid="btx231-B14" ref-type="bibr">Hoffman <italic>et al.</italic>, 2010</xref>). Third, using four machine learning classifiers, a text is classified to MP or to non-MP based on the text features. Finally, prediction made to each literature for a protein is summarized to make a prediction to the protein. Cross-validation results on the dataset of known MPs and non-MPs (control dataset) show that DextMP can successfully predict MPs with over 91% accuracy, with a significant improvement over existing MP prediction methods. Among the different forms of text information, abstracts taken from literature and function description in UniProt showed better performance than the title of literature. Lastly, we ran DextMP with the best performing language models and text-based feature combinations on three genomes, <italic>Saccharomyces cerevisiae</italic> (yeast), <italic>Homo sapiens</italic> (human) and <italic>Xenopus laevis</italic> (African clawed frog), and found that about 2.5–35% of the proteomes are potential MPs.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <p>We first explain text data and features used, then describe learning models of DextMP.</p>
    <sec>
      <title>2.1 Dataset of MPs and non-MPs</title>
      <p>The dataset of MPs and non-MPs (i.e. negative example of moonlighting proteins) were taken from our previous work (<xref rid="btx231-B25" ref-type="bibr">Khan and Kihara, 2016</xref>). The dataset contains 263 MPs selected from a manually curated MP database, MoonProt (<xref rid="btx231-B29" ref-type="bibr">Mani <italic>et al.</italic>, 2014</xref>). Proteins that do not have a UniProt ID were discarded. In addition, five MPs were discarded because they have over 25% sequence identity to other proteins in the dataset. Non-MPs were selected using the following Gene Ontology (GO) function annotation-based criteria developed in our previous works (<xref rid="btx231-B23" ref-type="bibr">Khan <italic>et al.</italic>, 2014</xref>; <xref rid="btx231-B25" ref-type="bibr">Khan and Kihara, 2016</xref>). From the four most dominant genomes in the MP dataset, namely, human (45 MP, 17.1%), <italic>E.coli</italic> (29 MPs, 11%), yeast (23 MPs, 8.7%) and mouse (11 MPs, 4.2%), a protein was selected as a non-MP if a) it has at least eight GO term annotations, b) when GO terms in the Biological Process (BP) category were clustered using the semantic similarity score (<xref rid="btx231-B39" ref-type="bibr">Schlicker <italic>et al.</italic>, 2006</xref>) no more than one cluster was obtained at either the 0.1 threshold or the 0.5 threshold, and c) no more than one cluster of Molecular Function (MF) GO terms at semantic similarity scores of 0.1 and 0.5 were formed. In essence, a protein is considered as a non-MP if it has a sufficient number of GO annotations but they are not functionally diverse. We further ruled out non-MPs that had above 25% sequence identity with another non-MP sequences, and finally selected 162 non-MPs, among which 60 are from human (37.0%), 52 from mouse (32.1%), 34 from yeast (20.9%) and 16 from <italic>E.coli</italic> (9.88%). In summary, 263 MP and 162 non-MP were selected as the control dataset for the DextMP model.</p>
    </sec>
    <sec>
      <title>2.2 Text extraction</title>
      <p>For each of the proteins in the control dataset, we extracted three categories of text information from UniProt: a) the title of each reference paper of the protein entry; b) the abstract of each reference; and c) the summary description of the protein’s function in the FUNCTION field in UniProt. The text data for a) and c) were directly collected from the UniProt data dump (<ext-link ext-link-type="uri" xlink:href="http://www.uniprot.org/downloads">http://www.uniprot.org/downloads</ext-link>), and b) was collected by crawling web links in the PUBLICATION list of the entry. <xref rid="btx231-T1" ref-type="table">Table 1</xref> shows the statistics of the data size. Note that while one protein can have multiple titles and abstracts associated with it, it only has one function description. For 49 MPs, no publication title was found, whereas 105 MPs did not have a hyperlink directed to a publication abstract (<xref rid="btx231-T1" ref-type="table">Table 1</xref>). <xref ref-type="fig" rid="btx231-F1">Figure 1</xref> shows the distribution of the number of abstracts per MP and non-MP in the dataset.</p>
      <fig id="btx231-F1" orientation="portrait" position="float">
        <label>Fig. 1</label>
        <caption>
          <p>Distribution of the number of abstracts per protein. Black, MP; gray, non-MP in the control dataset. The first bar is for 1 and 2 abstracts, next bar is for 3 and 4 and so on</p>
        </caption>
        <graphic xlink:href="btx231f1"/>
      </fig>
      <p>
        <table-wrap id="btx231-T1" orientation="portrait" position="float">
          <label>Table 1.</label>
          <caption>
            <p>Data size for the MP/non-MP dataset</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="(" span="1"/>
              <col valign="top" align="char" char="(" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1">#Proteins</th>
                <th rowspan="1" colspan="1">#Titles<xref ref-type="table-fn" rid="tblfn1"><sup>a</sup></xref></th>
                <th rowspan="1" colspan="1">#Abstracts<xref ref-type="table-fn" rid="tblfn1"><sup>a</sup></xref></th>
                <th rowspan="1" colspan="1">#Functions</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">MP</td>
                <td rowspan="1" colspan="1">263</td>
                <td rowspan="1" colspan="1">2496 (214)</td>
                <td rowspan="1" colspan="1">1450 (158)</td>
                <td rowspan="1" colspan="1">194</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">non-MP</td>
                <td rowspan="1" colspan="1">162</td>
                <td rowspan="1" colspan="1">1665 (162)</td>
                <td rowspan="1" colspan="1">1624 (162)</td>
                <td rowspan="1" colspan="1">162</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn1">
              <label>a</label>
              <p>In the parenthesis the number of proteins is shown for which the text data was found. For example, out of 263 MPs, at least one publication title was found for 214 MPs.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </p>
      <p>Obtained text data underwent three layers of data clean-up. First, redundant literature that appears both in MPs and non-MPs were discarded (this typically happens for papers that describe many proteins, e.g. genome annotation). Second, from each text data, all stop words, punctuations, and special symbols including Greek letters were removed. Finally, stemming and lemmatization were performed (<xref rid="btx231-B30" ref-type="bibr">Manning <italic>et al.</italic>, 2008</xref>) using the nltk package (<xref rid="btx231-B1" ref-type="bibr">Bird, 2006</xref>).</p>
      <p>As an example we briefly describe text data for a MP, phosphoglucose isomerase (PGI) in mouse (UniProt ID: P06745). It primarily acts as an enzyme in the second step of glycolysis. This protein moonlights by acting as a cytokine/growth factor, and causes pre-B cells to mature into antibody secreting cells, supports survival of embryonal neurons, and causes differentiation of some leukemia cell lines. Among 14 references for this protein, one is entitled ‘tumor cell autocrine motility factor is the neuroleukin/phosphohexose isomerase polypeptide’, which implies that this protein is an MP. It becomes apparent if one reads the abstract of this paper (<ext-link ext-link-type="uri" xlink:href="http://www.uniprot.org/citations/8674049">http://www.uniprot.org/citations/8674049</ext-link>) or the function description of the entry, which says ‘besides its role as a glycolytic enzyme, mammalian PGI can function as a tumor-secreted cytokine and an angiogenic factor (AMF) that stimulates endothelial cell motility. PGI is also a neurotrophic factor (Neuroleukin) for spinal and sensory neurons’. Despite this clear knowledge of moonlightness of this protein, the UniProt entry does not use any exact keyword, e.g. moonlighting proteins, multifunction, etc., which clearly indicates that this is an MP.</p>
    </sec>
    <sec>
      <title>2.3 Framework of DextMP</title>
      <p>The overall framework of DextMP is shown in <xref ref-type="fig" rid="btx231-F2">Figure 2</xref>. It is split in two parts, MP/non-MP prediction to a text (the <italic>text prediction model</italic>, the top panel in <xref ref-type="fig" rid="btx231-F2">Fig. 2</xref>) and prediction made to a query protein by combining prediction made for each text of the protein (the bottom panel).</p>
      <fig id="btx231-F2" orientation="portrait" position="float">
        <label>Fig. 2</label>
        <caption>
          <p>Schematic diagram of DextMP. The upper panel shows the text prediction process while the bottom panel is for the prediction model that uses predicted text labels to make the final MP/non-MP classification. P1, Protein 1, CL: Class Label</p>
        </caption>
        <graphic xlink:href="btx231f2"/>
      </fig>
      <p>In the text prediction model, first, three different types of text information, titles, abstracts and function descriptions, are extracted. Then the data clean-up step is carried out. Next, the texts in the dataset are applied to a deep unsupervised feature construction technique (<xref rid="btx231-B27" ref-type="bibr">Le and Mikolov, 2014</xref>), a bag of words model (<xref rid="btx231-B30" ref-type="bibr">Manning <italic>et al.</italic>, 2008</xref>), and topic modeling (<xref rid="btx231-B14" ref-type="bibr">Hoffman <italic>et al.</italic>, 2010</xref>) to construct text features. Finally, machine learning classification algorithms, namely, logistic regression (LR), random forest (RF), Support Vector Machine (SVM) and gradient boosted machine (GBM) (<xref rid="btx231-B32" ref-type="bibr">Pedregosa <italic>et al.</italic>, 2011</xref>) are applied to the learned features to provide a MP/non-MP prediction on each text data.</p>
      <p>Once we have a MP/non-MP class prediction for each text, we use the model shown in bottom panel of <xref ref-type="fig" rid="btx231-F2">Figure 2</xref> to obtain a class prediction for proteins (<italic>protein-level</italic> prediction). Each protein is associated with a certain number of texts (titles/abstracts) that have predicted class labels. To make the final MP/non-MP classification, two heuristics were applied: (i) A majority vote, where we simply take the binary class label votes for the protein using different majority cutoffs, 50%, 70%, 80% and 90%. (ii) A weighted majority vote, where a weight for a text is from the class prediction probability from the text level prediction. The weighted majority vote was applied to three classifiers, LR, RF and GBM. This latter part of DextMP is not applied when function description of proteins was used, since there is only one description for a protein and voting is not needed.</p>
    </sec>
    <sec>
      <title>2.4 Learning features from text</title>
      <p>Here we explain the three language models used for feature construction from text (<xref ref-type="fig" rid="btx231-F2">Fig. 2</xref>, top panel).</p>
      <p>1. Bag-of-words with TFIDF: Given a text corpus (collection of sentences/texts), the bag-of-words model first computes the dictionary that contains all the words in the text corpus. Given a dictionary of size <italic>N</italic>, a text can be represented as an <italic>N</italic>-dimensional real-valued vector with TFIDF values for each word in the dictionary. For a word <italic>w</italic>, TFIDF is be computed as follows: <italic>TFIDF(w) = TF(w) * IDF(w)</italic>, where Term Frequency, <italic>TF(w)</italic> = (number of times word <italic>w</italic> appears in a text)/(total number of words in the text); and Inverse Document Frequency, <italic>IDF(w)</italic> = <italic>log<sub>e</sub></italic>(total number of texts in the corpus/number of texts with word <italic>w</italic>); Intuitively, TFIDF measures the importance of a keyword to a sentence with respect to its entire dictionary corpus.</p>
      <p>2. Topic Modeling with LDA: In principle, the bag-of-words model has two critical limitations: for a large dictionary, the size of the feature vector for each text can be huge, which makes it computationally expensive, and it does not take consideration of the word ordering in a text. To alleviate above two challenges, in topic modeling a text is modeled as a distribution of words for latent topics, where the <italic>number of topics</italic> is a user-defined parameter. Latent Dirichlet Allocation (LDA) is one of the most popular topic modeling algorithms, which uses two Dirichlet-multinomial distributions to model the mappings between documents and topics, and topics and words. We used an open source Python implementation of LDA (<xref rid="btx231-B37" ref-type="bibr">Rurek and Sojka, 2010</xref>).</p>
      <p>3. Unsupervised Deep Language Model, DEEP and PDEEP: As the third language model, we used a deep learning-based unsupervised feature construction algorithm (<xref rid="btx231-B27" ref-type="bibr">Le and Mikolov, 2014</xref>). This model maps texts into a continuous vector space of a dimension <italic>d</italic>, such that semantically similar texts appear close in the space. For a sequence of words <italic>W</italic> = (<italic>w<sub>0</sub></italic>, <italic>w<sub>1</sub></italic>, …, <italic>w<sub>n</sub></italic>), where <italic>w<sub>i</sub></italic> ∈ <italic>D</italic> (<italic>D</italic> is the dictionary), suppose <italic>w<sub>i</sub></italic> is an input word and rest of the words in the dictionary form the context <italic>w<sub>c</sub> = (w<sub>0</sub></italic>, <italic>w<sub>1</sub></italic>, . .<italic>w<sub>i-1</sub></italic>, <italic>w<sub>i+1</sub></italic>, . <italic>w<sub>n</sub></italic><sub>)</sub> . A neural network with a single hidden layer is trained so that for a vector of <italic>D</italic> dimension with 1 for <italic>w<sub>i</sub></italic> and 0 otherwise, the network outputs the conditional probability that the other words co-appear in the neighborhood of <italic>w<sub>i</sub></italic> in a text. Using the conditional probabilities that each word co-appears with <italic>w<sub>i</sub></italic> in the training dataset of texts as known outputs, weights of hidden layers are trained with back-propagation, and the weights of hidden layers are considered as a feature vector of <italic>w<sub>i.</sub></italic></p>
      <p>Mikolov <italic>et al.</italic> extended the above model by modifying the probability expression to <italic>Pr[W|T]</italic>. Here, <italic>T</italic> is the text containing the sequence of words, and can be thought as another word. Similar to the above model along with the task of maximizing the conditional probability, it outputs <italic>d</italic>-dimensional feature vector representation of each text, <italic>T</italic>. We used an open source Python implementation of the ‘paragraph vector’ deep learning model (<xref rid="btx231-B37" ref-type="bibr">Rurek and Sojka, 2010</xref>).</p>
      <p>Using the deep learning method, we computed two models, DEEP and PDEEP (Pre-trained deep learning model). For DEEP, features were constructed on texts from the control dataset of MP and non-MP only. For PDEEP, we used the entire text data from UniProt. Concretely, we extracted a total of 1 060 520 titles and 551 056 function descriptions from the UniProt data dump. Since publication abstracts are not available in the data dump, we omitted PDEEP training for abstracts.</p>
    </sec>
    <sec>
      <title>2.5 Parameter tuning of DextMP</title>
      <p>We used a grid search to determine hyper-parameters for LDA and DEEP. In LDA, the ‘number of topics’ parameter was tuned by a grid search performed between 10 and 100 with a step size of 10 for each text type. In DEEP, we tuned three hyper-parameters: the ‘minimum count’ parameter was tuned within a range of 1–5, ‘window size’ was tuned within 2–8, both with a step size of 1, and ‘dimension size’ was tuned in a range from 20 to 200 with an increasing step size of 20. The parameter ‘minimum count’ indicates the minimum number of texts that the word must appear in, ‘window size’ is the size of the convolution context, and ‘dimension size’ indicates the length of the feature vector representation. For PDEEP, we used the same parameters as DEEP.</p>
      <p>The hyper-parameters associated with the four classifiers of DextMP were also determined using a grid search. For LR and SVM we tuned the regularization, a cost parameter and a kernel function (linear or radial basis function), and used the default values for the other parameters in the models in the sklearn package (<xref rid="btx231-B32" ref-type="bibr">Pedregosa <italic>et al.</italic>, 2011</xref>). For RF and GBM, we tuned the ‘number of trees’ parameter, the ‘learning rate’ parameter for GBM, and used the default for the others.</p>
      <p>We performed a five-fold cross-validation. The dataset was split into five sub-groups, among which three sub-groups were used for training, another sub-group was used for validation, and the last sub-group was used for testing. Given a vector of hyper-parameters for a combination of a model and a classifier, where a model is either LDA/DEEP and a classifier be LR/RF/SVM/GBM, we performed a grid search for the optimal hyper-parameters over the training set, used the validation set to find the best hyper-parameter vector, and ran the optimized model with the hyper-parameter values for the test set to report results. For example, when LDA and LR combination was to be trained, for each of all the combinations of the ‘number of topics’ parameter for LDA and the regularization parameter for LR, model parameters were optimized on a training set that consists of three sub-groups. Once the model was optimized for each of the all hyper-parameter combinations on the training set, the optimized models were tested on the validation set to determine the best hyper-parameter combination and the model optimized under the hyper-parameters. Then, the selected model was tested on the testing set to report the F-score for that parameter setting. Each sub-group was used once for testing. We performed the above procedure for five test sub-groups independently, and finally reported the average F-score computed for the 5 test sub-groups.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Text features of MPs</title>
      <p>To begin with, we browsed abundant words in texts of MPs in the control dataset. In <xref ref-type="fig" rid="btx231-F3">Figure 3</xref>, word clouds of the three categories of texts, publication titles, function descriptions and abstracts, are shown.</p>
      <fig id="btx231-F3" orientation="portrait" position="float">
        <label>Fig. 3</label>
        <caption>
          <p>Word clouds of text information of moonlighting protein dataset. The size of a word in the visualization is proportional to the number of times the word appears in the input text. (<bold>A–C</bold>): titles, function descriptions and abstracts, respectively. The images were generated at <ext-link ext-link-type="uri" xlink:href="http://www.wordle.net/">http://www.wordle.net/</ext-link></p>
        </caption>
        <graphic xlink:href="btx231f3"/>
      </fig>
      <p>From the word clouds, a few points came to light: the words ‘enzyme’, ‘kinase’ and ‘transcription’ appear in all three text types in <xref ref-type="fig" rid="btx231-F3">Figure 3</xref> (red circles). Word counts of ‘enzyme’ in titles, abstracts, UniProt function descriptions are 108/34, 562/107, 89/26, respectively for MP/non-MP. Counts for ‘kinase’ and ‘transcription’ were (102/16, 210/105, 44/9) and (87/59, 431/331, 75/34), respectively, for (titles, abstracts, UniProt function descriptions) of MP/non-MP. This is consistent with previous reports that many MPs were known primarily as enzymes when their secondary function, such as transcription factor, was discovered (<xref rid="btx231-B13" ref-type="bibr">Hernández <italic>et al.</italic>, 2014</xref>; <xref rid="btx231-B16" ref-type="bibr">Jeffery, 2003</xref>; <xref rid="btx231-B25" ref-type="bibr">Khan and Kihara, 2016</xref>; <xref rid="btx231-B29" ref-type="bibr">Mani <italic>et al.</italic>, 2014</xref>). The word ‘ribosome’, which appeared as the top word in <xref ref-type="fig" rid="btx231-F3">Figure 3A</xref> (green circles), also agrees with our previous finding (<xref rid="btx231-B25" ref-type="bibr">Khan and Kihara, 2016</xref>) that predicted MPs were enriched in ribosomal pathways in the KEGG database (<xref rid="btx231-B21" ref-type="bibr">Kanehisa and Goto, 2000</xref>), and found in literature (<xref rid="btx231-B45" ref-type="bibr">Wool, 1996</xref>). Additionally, words that are clear indicators of MPs also appeared, such as ‘bifunctional’ (counts were 21/0, 29/5, 6/0 for MP/non-MP in titles, abstracts, function descriptions, respectively; blue circle in <xref ref-type="fig" rid="btx231-F3">Fig. 3A</xref>) and ‘multifunctional’ (12/0, 19/4, 4/0 for MP/non-MP in titles, abstracts, function descriptions, respectively).</p>
    </sec>
    <sec>
      <title>3.2 DextMP performance on text level prediction</title>
      <p>We now show prediction results of the text-level MP prediction by DextMP on the control dataset (<xref rid="btx231-T2" ref-type="table">Table 2</xref>). A schematic diagram of this part of the DextMP model is described in the top panel of <xref ref-type="fig" rid="btx231-F2">Figure 2</xref> and explained in Section 2.2. Along with the two different deep learning based models (DEEP and PDEEP), we used two other methods in popular language model categories, TFIDF in the ‘bag-of-words’ category and LDA in the ‘topic modelling’ category. For each language model, three forms of text information, titles, abstracts and UniProt function descriptions, were used and compared. Note that the abstracts-PDEEP combination was omitted, as it requires all publication abstracts for the entire protein corpus in UniProt for model training. Since UniProt does not maintain any file dump for publication abstracts, it requires running a web-crawler and downloading abstract texts for all proteins in UniProt, which became computationally very expensive. For learned features by each language model, we further used four classifiers, LR, RF, SVM and GBM to make MP/non-MP classification (shown in right columns of <xref rid="btx231-T2" ref-type="table">Table 2</xref>).
<table-wrap id="btx231-T2" orientation="portrait" position="float"><label>Table 2.</label><caption><p>Summary of the text-level prediction with different combinations of text types, language models and classifiers</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="char" char="." span="1"/></colgroup><thead><tr><th rowspan="2" colspan="1">Text Type</th><th rowspan="2" colspan="1">Language Model</th><th colspan="4" rowspan="1">Classifiers<hr/></th></tr><tr><th rowspan="1" colspan="1">LR</th><th rowspan="1" colspan="1">RF</th><th rowspan="1" colspan="1">SVM</th><th rowspan="1" colspan="1">GBM</th></tr></thead><tbody><tr><td rowspan="4" colspan="1">Titles</td><td rowspan="1" colspan="1">TFIDF</td><td rowspan="1" colspan="1"><bold>0.7774</bold></td><td rowspan="1" colspan="1"><bold>0.7942</bold></td><td rowspan="1" colspan="1"><bold>0.8751</bold></td><td rowspan="1" colspan="1">0.7218</td></tr><tr><td rowspan="1" colspan="1">LDA</td><td rowspan="1" colspan="1">0.6128</td><td rowspan="1" colspan="1">0.6829</td><td rowspan="1" colspan="1">0.6584</td><td rowspan="1" colspan="1">0.7065</td></tr><tr><td rowspan="1" colspan="1">DEEP</td><td rowspan="1" colspan="1">0.7696</td><td rowspan="1" colspan="1">0.7402</td><td rowspan="1" colspan="1">0.8429</td><td rowspan="1" colspan="1"><bold>0.8029</bold></td></tr><tr><td rowspan="1" colspan="1">PDEEP</td><td rowspan="1" colspan="1">0.6262</td><td rowspan="1" colspan="1">0.5482</td><td rowspan="1" colspan="1">0.4836</td><td rowspan="1" colspan="1">0.6445</td></tr><tr><td rowspan="4" colspan="1">Abstracts</td><td rowspan="1" colspan="1">TFIDF</td><td rowspan="1" colspan="1"><bold>0.9220</bold></td><td rowspan="1" colspan="1"><bold>0.8682</bold></td><td rowspan="1" colspan="1"><bold>0.9371</bold></td><td rowspan="1" colspan="1"><bold>0.8396</bold></td></tr><tr><td rowspan="1" colspan="1">LDA</td><td rowspan="1" colspan="1">0.6419</td><td rowspan="1" colspan="1">0.6936</td><td rowspan="1" colspan="1">0.6512</td><td rowspan="1" colspan="1">0.7349</td></tr><tr><td rowspan="1" colspan="1">DEEP</td><td rowspan="1" colspan="1">0.7775</td><td rowspan="1" colspan="1">0.8119</td><td rowspan="1" colspan="1">0.8480</td><td rowspan="1" colspan="1">0.7987</td></tr><tr><td rowspan="1" colspan="1">PDEEP</td><td rowspan="1" colspan="1">–</td><td rowspan="1" colspan="1">–</td><td rowspan="1" colspan="1">–</td><td rowspan="1" colspan="1">–</td></tr><tr><td rowspan="4" colspan="1">Function Descriptions</td><td rowspan="1" colspan="1">TFIDF</td><td rowspan="1" colspan="1">0.7412</td><td rowspan="1" colspan="1">0.7439</td><td rowspan="1" colspan="1">0.7715</td><td rowspan="1" colspan="1">0.6947</td></tr><tr><td rowspan="1" colspan="1">LDA</td><td rowspan="1" colspan="1">0.6128</td><td rowspan="1" colspan="1">0.6829</td><td rowspan="1" colspan="1">0.6582</td><td rowspan="1" colspan="1">0.7065</td></tr><tr><td rowspan="1" colspan="1">DEEP</td><td rowspan="1" colspan="1"><bold>0.8929</bold></td><td rowspan="1" colspan="1"><bold>0.8962</bold></td><td rowspan="1" colspan="1"><bold>0.9184</bold></td><td rowspan="1" colspan="1"><bold>0.8788</bold></td></tr><tr><td rowspan="1" colspan="1">PDEEP</td><td rowspan="1" colspan="1">0.7017</td><td rowspan="1" colspan="1">0.7211</td><td rowspan="1" colspan="1">0.3474</td><td rowspan="1" colspan="1">0.6917</td></tr></tbody></table><table-wrap-foot><fn id="tblfn2"><p>Two-class weighted F-score was reported, where F-score of MP and non-MP was calculated and weighted average of them was taken, where the weights are the number of data points of each class. The values shown are the average of the test sets in the Five-fold cross-validation. LR, Logistic Regression; RF, Random Forest; SVM, Support Vector Machine; GBM, Gradient Boosted Machine.</p></fn></table-wrap-foot></table-wrap></p>
      <p>Among all the text-language_model-classifier combinations tested in <xref rid="btx231-T2" ref-type="table">Table 2</xref>, the highest F-score, 0.9371, was recorded by the combination of TFIDF and SVM when it was applied to literature abstracts (abstracts-TFIDF-SVM). The precision was 0.8920 and the recall was 0.8640. Besides this best combination, seven more combinations showed an F-score over 0.850. Comparing the three text types, abstracts had the highest F-score (0.9371), and UniProt function descriptions was second highest (0.9184), and using titles had the lowest (0.8751).This order was consistent when the average F-score across different model-classifier combinations for each text type was considered: the abstracts again showed the highest value of 0.8053 in comparison to the function descriptions (0.7138) and the titles (0.7141). We further counted which text type showed the highest F-score for combinations of language models (PDEEP was excluded) and classifiers, e.g. TFIDF-LR. Six combinations showed the highest F-score when applied to abstracts, four combinations with function descriptions, and two were best with literature titles. These analyses show that MP detection can be done better by using abstracts or UniProt function descriptions than simply using literature titles.</p>
      <p>Next we compare four language models. In <xref rid="btx231-T2" ref-type="table">Table 2</xref>, for each text type, the best performing language model under four classifiers is highlighted in bold. Surprisingly, that the simple TFIDF worked well with titles and abstracts. This is likely because titles are simpler so that TFIDF can easily capture MP-specific words to make correct predictions. DEEP showed superior performance in the function description category, which implies that complex semantic inter-word relations in the function descriptions require a complex model to correctly identify characteristics of MPs and non-MPs. DEEP clearly outperforms LDA in all three text information categories and all four classifiers, while PDEEP had 3 (out of 8) wins over LDA. PDEEP was built as an extension from DEEP by enlarging its training set to the whole corpus in UniProt. This model showed a lower F-score consistently for both the title and the function description categories. The reason for the lower accuracy of PDEEP is maybe because the training data used for PDEEP is too large and somewhat generalized textual features unique for MPs.</p>
      <p>Comparing the four classifiers, SVM showed the best result in six cases among the eleven different settings (rows in <xref rid="btx231-T2" ref-type="table">Table 2</xref>), GBM won for four cases and RF for one case. LR did not win a single setting.</p>
      <p>We have also tested DextMP’s prediction accuracy when text summaries were used as input. A summary of each abstract and each UniProt function description was computed using a well-known algorithm, TextRank (<xref rid="btx231-B36" ref-type="bibr">Rada and Tarau, 2004</xref>) implemented in a general- purpose python text summarization package, sumy (<ext-link ext-link-type="uri" xlink:href="https://pypi.python.org/pypi/sumy">https://pypi.python.org/pypi/sumy</ext-link>). On average a summary reduced word counts of an abstract from 178.3 to 152.7 and from 172.2 to 145.4 for a function description. Using computed summaries of abstracts, F-score for the TFIDF model with the LR, SVM, GBM and RF classifiers reduced to 0.7937, 0.8043, 0.7259 and 0.7692, respectively. We used the TFIDF model for this comparison because it performed best among the language models used on the original abstract texts (<xref rid="btx231-T2" ref-type="table">Table 2</xref>). For functional descriptions, using summaries also reduced F-score of the DEEP model with the four classifies to 0.8525, 0.8395, 0.7494 and 0.8210, respectively. DEEP was used here since it performed best for function descriptions. The reduction of the F-score was about 5 to 20% relative to when the original texts were used.</p>
      <p>In terms of computational time, DEEP takes substantially more time in training relative to TFIDF and LDA, because the neural network needs to be trained (<xref rid="btx231-T3" ref-type="table">Table 3</xref>). However, since training can be pre-computed using a training dataset and reused later, in practice DEEP does not take much time when applied to predictions of new data relative to the time needed for training. In contrast, although TFIDF takes a short time for training, it takes a substantially longer time in prediction because the size of a feature vector becomes large as more unique words appear, which exceeded 16 000.
<table-wrap id="btx231-T3" orientation="portrait" position="float"><label>Table 3.</label><caption><p>Computational time (seconds)</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/></colgroup><thead><tr><th rowspan="2" colspan="1">Phase</th><th rowspan="2" colspan="1">Text Type</th><th colspan="3" rowspan="1">Language model<hr/></th></tr><tr><th rowspan="1" colspan="1">TFIDF</th><th rowspan="1" colspan="1">LDA</th><th rowspan="1" colspan="1">DEEP</th></tr></thead><tbody><tr><td rowspan="3" colspan="1">Training</td><td rowspan="1" colspan="1">Titles</td><td rowspan="1" colspan="1">5.8*10<sup>−5</sup></td><td rowspan="1" colspan="1">1.0*10<sup>−3</sup></td><td rowspan="1" colspan="1">4.4*10<sup>−1</sup></td></tr><tr><td rowspan="1" colspan="1">Abstracts</td><td rowspan="1" colspan="1">3.3*10<sup>−4</sup></td><td rowspan="1" colspan="1">2.9*10<sup>−3</sup></td><td rowspan="1" colspan="1">9.0*10<sup>−1</sup></td></tr><tr><td rowspan="1" colspan="1">Function Dsc.</td><td rowspan="1" colspan="1">6.3*10<sup>−4</sup></td><td rowspan="1" colspan="1">1.5*10<sup>−2</sup></td><td rowspan="1" colspan="1">1.2</td></tr><tr><td rowspan="3" colspan="1">Feature generation</td><td rowspan="1" colspan="1">Titles</td><td rowspan="1" colspan="1">7.8*10<sup>−4</sup></td><td rowspan="1" colspan="1">3.3*10<sup>−4</sup></td><td rowspan="1" colspan="1">1.8*10<sup>−4</sup></td></tr><tr><td rowspan="1" colspan="1">Abstracts</td><td rowspan="1" colspan="1">3.2*10<sup>−3</sup></td><td rowspan="1" colspan="1">6.6*10<sup>−4</sup></td><td rowspan="1" colspan="1">2.4*10<sup>−4</sup></td></tr><tr><td rowspan="1" colspan="1">Function Dsc.</td><td rowspan="1" colspan="1">2.2*10<sup>−3</sup></td><td rowspan="1" colspan="1">1.0*10<sup>−3</sup></td><td rowspan="1" colspan="1">2.0*10<sup>−4</sup></td></tr><tr><td rowspan="3" colspan="1">Classification</td><td rowspan="1" colspan="1">Titles</td><td rowspan="1" colspan="1">5.1*10<sup>−2</sup></td><td rowspan="1" colspan="1">3.8*10<sup>−3</sup></td><td rowspan="1" colspan="1">9.2*10<sup>−3</sup></td></tr><tr><td rowspan="1" colspan="1">Abstracts</td><td rowspan="1" colspan="1">1.2*10<sup>−1</sup></td><td rowspan="1" colspan="1">4.0*10<sup>−3</sup></td><td rowspan="1" colspan="1">1.3*10<sup>−2</sup></td></tr><tr><td rowspan="1" colspan="1">Function Dsc.</td><td rowspan="1" colspan="1">7.0*10<sup>−2</sup></td><td rowspan="1" colspan="1">5.3*10<sup>−3</sup></td><td rowspan="1" colspan="1">6.1*10<sup>−3</sup></td></tr></tbody></table></table-wrap></p>
      <p>Computational time is classified into three steps of the DextMP algorithm, training, feature generation and classification. Training is the time needed on average for processing a text to compute parameters of a language model. Feature generation is the time needed to compute a feature vector of a text to be classified. Classification is the average time that each classifier took to make a classification to a text.</p>
    </sec>
    <sec>
      <title>3.3 DextMP performance on protein level prediction</title>
      <p>Next, we discuss the performance of DextMP on the final protein-level MP/non-MP classification using predictions made to each text that belongs to proteins. This process is represented in the bottom panel in <xref ref-type="fig" rid="btx231-F2">Figure 2</xref>. When UniProt function descriptions are used, a protein-level prediction is identical to the text-level prediction, because a query protein has only one UniProt description. When titles or abstracts of literature were used as text information, classification labels assigned to texts of a query protein were summarized using a simple majority vote or a weighted majority vote. As mentioned in Section 2.3, for a simple majority vote, four majority cutoffs, 50%, 70%, 80%, 90%, were tested in cross-validation for each combination of (text type)-(language model)-(classifier), and the cutoff that gave the largest F-score in the validation set was chosen and applied to the testing set. In <xref ref-type="fig" rid="btx231-F4">Figure 4</xref> we compared F-scores of protein level classification of the 21 (text type)-(language model)-(classifier) combinations using the simple majority votes and the weighted majority votes. Out of all 44 combinations in <xref rid="btx231-T2" ref-type="table">Table 2</xref>, the 21 combinations used in Figure 4 were LR, RF, and GBM classifiers applied to the title and abstract categories. The function descriptions category was excluded as a text type because it does not need voting. Among the 21 combinations, the simple majority votes showed a larger F-score for 15 cases than the counterpart, although margins are not very large. Therefore, we only show the results with the simple voting for the rest of this work.</p>
      <fig id="btx231-F4" orientation="portrait" position="float">
        <label>Fig. 4</label>
        <caption>
          <p>Protein-level cross-validation F-scores for weighted and non-weighted majority votes. Results for 21 (text type)-(language model)-(classifier) combinations are compared</p>
        </caption>
        <graphic xlink:href="btx231f4"/>
      </fig>
      <p><xref rid="btx231-T4" ref-type="table">Table 4</xref> summarizes F-scores of the protein-level MP prediction. The highest F-score was achieved when function descriptions were used by a combination of DEEP-SVM (0.9184). Note that values for function descriptions are identical to the text-level accuracy (<xref rid="btx231-T2" ref-type="table">Table 2</xref>) because one protein has only one description in UniProt. Comparing with the text-level prediction results in <xref rid="btx231-T2" ref-type="table">Table 2</xref>, a similar order of performance by different setting combinations was observed. However, a difference is that in almost all the cases the text-level accuracy was higher than the protein-level, which indicates the voting step in the protein-level prediction decreased the accuracy. Following the highest F-score combination of function descriptions-DEEP-SVM, the next three top combinations were all with function descriptions, which kept the same values as the text-level prediction, using the DEEP language model. Similar to what was observed in <xref rid="btx231-T2" ref-type="table">Table 2</xref>, TFIDF showed the best results with all the classifiers in the titles category, and also the best with three classifiers in the abstracts category, as highlighted in bold. Precision and recall values were well balanced for the F-score results in <xref rid="btx231-T4" ref-type="table">Table 4</xref>. For example, for the titles-TFIDF-SVM combination, which showed an F-score of 0.8330, precision and recall were 0.8316 and 0.8479, respectively.
<table-wrap id="btx231-T4" orientation="portrait" position="float"><label>Table 4.</label><caption><p>Summary of the protein-level prediction</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/></colgroup><thead><tr><th rowspan="2" colspan="1">Text Type</th><th rowspan="2" colspan="1">Language Model</th><th colspan="4" rowspan="1">Classifiers<hr/></th></tr><tr><th rowspan="1" colspan="1">LR</th><th rowspan="1" colspan="1">RF</th><th rowspan="1" colspan="1">SVM</th><th rowspan="1" colspan="1">GBM</th></tr></thead><tbody><tr><td rowspan="4" colspan="1">Titles</td><td rowspan="1" colspan="1">TFIDF</td><td rowspan="1" colspan="1"><bold>0.7703</bold></td><td rowspan="1" colspan="1"><bold>0.7474</bold></td><td rowspan="1" colspan="1"><bold>0.8330</bold></td><td rowspan="1" colspan="1"><bold>0.6901</bold></td></tr><tr><td rowspan="1" colspan="1">LDA</td><td rowspan="1" colspan="1">0.5654</td><td rowspan="1" colspan="1">0.5723</td><td rowspan="1" colspan="1">0.5836</td><td rowspan="1" colspan="1">0.6227</td></tr><tr><td rowspan="1" colspan="1">DEEP</td><td rowspan="1" colspan="1">0.6651</td><td rowspan="1" colspan="1">0.6698</td><td rowspan="1" colspan="1">0.7557</td><td rowspan="1" colspan="1">0.6826</td></tr><tr><td rowspan="1" colspan="1">PDEEP</td><td rowspan="1" colspan="1">0.6611</td><td rowspan="1" colspan="1">0.5278</td><td rowspan="1" colspan="1">0.4314</td><td rowspan="1" colspan="1">0.6021</td></tr><tr><td rowspan="4" colspan="1">Abstracts</td><td rowspan="1" colspan="1">TFIDF</td><td rowspan="1" colspan="1"><bold>0.8132</bold></td><td rowspan="1" colspan="1"><bold>0.8225</bold></td><td rowspan="1" colspan="1"><bold>0.8208</bold></td><td rowspan="1" colspan="1">0.7833</td></tr><tr><td rowspan="1" colspan="1">LDA</td><td rowspan="1" colspan="1">0.5459</td><td rowspan="1" colspan="1">0.5739</td><td rowspan="1" colspan="1">0.5342</td><td rowspan="1" colspan="1">0.5713</td></tr><tr><td rowspan="1" colspan="1">DEEP</td><td rowspan="1" colspan="1">0.7650</td><td rowspan="1" colspan="1">0.8105</td><td rowspan="1" colspan="1">0.7747</td><td rowspan="1" colspan="1"><bold>0.7909</bold></td></tr><tr><td rowspan="1" colspan="1">PDEEP</td><td rowspan="1" colspan="1">–</td><td rowspan="1" colspan="1">–</td><td rowspan="1" colspan="1">–</td><td rowspan="1" colspan="1">–</td></tr><tr><td rowspan="4" colspan="1">Function Descriptions</td><td rowspan="1" colspan="1">TFIDF</td><td rowspan="1" colspan="1">0.7412</td><td rowspan="1" colspan="1">0.7439</td><td rowspan="1" colspan="1">0.7715</td><td rowspan="1" colspan="1">0.6947</td></tr><tr><td rowspan="1" colspan="1">LDA</td><td rowspan="1" colspan="1">0.6128</td><td rowspan="1" colspan="1">0.6829</td><td rowspan="1" colspan="1">0.6582</td><td rowspan="1" colspan="1">0.7065</td></tr><tr><td rowspan="1" colspan="1">DEEP</td><td rowspan="1" colspan="1"><bold>0.8929</bold></td><td rowspan="1" colspan="1"><bold>0.8962</bold></td><td rowspan="1" colspan="1"><bold>0.9184</bold></td><td rowspan="1" colspan="1"><bold>0.8788</bold></td></tr><tr><td rowspan="1" colspan="1">PDEEP</td><td rowspan="1" colspan="1">0.7017</td><td rowspan="1" colspan="1">0.7211</td><td rowspan="1" colspan="1">0.3474</td><td rowspan="1" colspan="1">0.6917</td></tr></tbody></table><table-wrap-foot><fn id="tblfn3"><p>F-score was reported. The values shown are the average of the test sets in the five-fold cross validation. LR, Logistic Regression; RF, Random Forest; SVM, Support Vector Machine; GBM, Gradient Boosted Machine. For each text type, titles, abstracts and function descriptions, the best performing language model under four classifiers is highlighted in bold.</p></fn></table-wrap-foot></table-wrap></p>
      <p>For comparison, we ran a sequence-based function prediction method, PFP (<xref rid="btx231-B8" ref-type="bibr">Hawkins <italic>et al.</italic>, 2006</xref>, <xref rid="btx231-B10" ref-type="bibr">2009</xref>) on the control dataset and classified the proteins to MP/non-MP based on predicted GO terms. Following the GO term-based MP/non-MP classification performed in our previous study (<xref rid="btx231-B23" ref-type="bibr">Khan <italic>et al.</italic>, 2014</xref>), a protein was classified into MP if more than one GO term in the BP category was predicted with moderate or higher confidence scores (a PFP raw score &gt; 500), and if the GO terms were classified into more than two clusters using the relevance semantic similarity (SS_Rel) score (<xref rid="btx231-B39" ref-type="bibr">Schlicker <italic>et al.</italic>, 2006</xref>) of 0.1 and more than four clusters at SS_Rel of 0.5. This protocol predicted 127/52 MPs/non-MPs correctly out of 263/162 MPs/non-MPs, resulting in an F-score of 0.4472. Thus, DextMP showed a higher accuracy than the function prediction-based results.</p>
      <p>We think the prediction accuracy shown in <xref rid="btx231-T4" ref-type="table">Table 4</xref> is sufficiently high for practical use, particularly for a large scale screening considering that an alternative for finding MPs from text is for someone to read texts one by one.</p>
    </sec>
    <sec>
      <title>3.4 Genome-scale MP prediction using DextMP</title>
      <p>Finally, we applied DextMP for predicting MPs in three genomes. Two genomes, <italic>S. cerevisiae</italic> (yeast) and <italic>H. sapiens</italic> (human), were chosen because MP prediction by MPFit (<xref rid="btx231-B25" ref-type="bibr">Khan and Kihara, 2016</xref>) were previously tested on them, so that we can compare DextMP with MPFit. One more genome, <italic>X.laevis</italic>, was chosen because omics-data, such as gene expression and protein-protein interaction data, are not available for this organism, and thus existing MP prediction methods (<xref rid="btx231-B3" ref-type="bibr">Chapple <italic>et al.</italic>, 2015</xref>; <xref rid="btx231-B5" ref-type="bibr">Gómez <italic>et al.</italic>, 2011</xref>), which rely on omics-data, cannot be used. Therefore DextMP can make unique contributions. Among the eleven settings we tested in <xref rid="btx231-T3" ref-type="table">Table 3</xref>, we used the top two models in the titles category and the top two in the function descriptions category from <xref rid="btx231-T4" ref-type="table">Table 4</xref>, i.e. titles-TFIDF-SVM, titles-TFIDF-LR, function_descriptions-DEEP-RF and function_descriptions-DEEP-SVM, and took the consensus of the four predictions. We did not use abstracts-based methods because abstracts were not directly available at UniProt and were not convenient for a large-scale prediction.</p>
      <p>In our previous work, we developed an omics-data-based MP prediction method, MPFit (<xref rid="btx231-B25" ref-type="bibr">Khan and Kihara, 2016</xref>) and demonstrated that it outperformed two existing methods, one of which uses a target organism’s PPI network (<xref rid="btx231-B3" ref-type="bibr">Chapple <italic>et al.</italic>, 2015</xref>) and another method that is based on GO term annotation of proteins (<xref rid="btx231-B34" ref-type="bibr">Pritykin <italic>et al.</italic>, 2015</xref>). Therefore, in this section, we compare DextMP mainly with MPFit on the yeast and human genomes for which MPFit was applied.</p>
      <p><xref rid="btx231-T5" ref-type="table">Table 5</xref> summarizes predictions to the three genomes. For the yeast genome, out of 6721 proteins, 6500 had both title and function description in UniProt so that DextMP can run on them (coverage 96.73%). Among these proteins, 2316 (34.46% of the entire proteins in the genome) were predicted as MP by DextMP when a consensus of three settings are considered, and 896 (13.33%) if a consensus of the all four settings was considered. In our previous work, MPFit predicted 10.97% of the yeast proteins are MPs, which is similar to the current prediction with the full (four) consensus. Since yeast has 27 known MPs in the MoonProt database (<xref rid="btx231-B29" ref-type="bibr">Mani <italic>et al.</italic>, 2014</xref>), we computed recall based on them. Out of 27 known MPs, 24 and 20 are detected as MPs when a consensus of ≥3 and 4 settings are considered, which give recall value of 0.889 and 0.741, respectively. MPFit recorded a recall of 0.8146 (22 out of 27) in the previous work, which is between the two values in the current work. These two recall values are significantly higher than the GO term-based prediction by <xref rid="btx231-B34" ref-type="bibr">Pritykin <italic>et al.</italic> (2015)</xref>, which was 0.4815. Besides the high recall value, DextMP also has a strong advantage of having a higher coverage than both MPFit and the method by Pritykin <italic>et al.</italic>, because text information is in general more available than omics-data or GO annotations, which the two methods use as input. The coverage for DextMP was 96.73%, while MPFit and Pritykin <italic>et al.</italic> had a coverage of 69.56% and 68.69%, respectively.
<table-wrap id="btx231-T5" orientation="portrait" position="float"><label>Table 5.</label><caption><p>Genome-scale prediction by DextMP</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="center" span="1"/></colgroup><thead><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">Yeast</th><th rowspan="1" colspan="1">Human</th><th rowspan="1" colspan="1"><italic>X.laevis</italic></th></tr></thead><tbody><tr><td rowspan="1" colspan="1"># Proteins</td><td rowspan="1" colspan="1">6721</td><td rowspan="1" colspan="1">20 104</td><td rowspan="1" colspan="1">11 078</td></tr><tr><td rowspan="1" colspan="1">Coverage</td><td rowspan="1" colspan="1">96.73%</td><td rowspan="1" colspan="1">98.06%</td><td rowspan="1" colspan="1">30.54%</td></tr><tr><td rowspan="2" colspan="1"># MPs (%) (vote ≥ 3)</td><td rowspan="1" colspan="1">2316</td><td rowspan="1" colspan="1">4781</td><td rowspan="1" colspan="1">600</td></tr><tr><td rowspan="1" colspan="1">(34.46%)</td><td rowspan="1" colspan="1">(23.78%)</td><td rowspan="1" colspan="1">(5.42%)</td></tr><tr><td rowspan="2" colspan="1"># MPs (%) (vote = 4)</td><td rowspan="1" colspan="1">896</td><td rowspan="1" colspan="1">1682</td><td rowspan="1" colspan="1">279</td></tr><tr><td rowspan="1" colspan="1">(13.33%)</td><td rowspan="1" colspan="1">(8.37%)</td><td rowspan="1" colspan="1">(2.51%)</td></tr><tr><td rowspan="1" colspan="1"># known MPs</td><td rowspan="1" colspan="1">23</td><td rowspan="1" colspan="1">45</td><td rowspan="1" colspan="1">–</td></tr><tr><td rowspan="1" colspan="1">recall (vote ≥ 3)</td><td rowspan="1" colspan="1">0.889</td><td rowspan="1" colspan="1">0.933</td><td rowspan="1" colspan="1">–</td></tr><tr><td rowspan="1" colspan="1">recall (vote = 4)</td><td rowspan="1" colspan="1">0.741</td><td rowspan="1" colspan="1">0.689</td><td rowspan="1" colspan="1">–</td></tr></tbody></table><table-wrap-foot><fn id="tblfn4"><p>Coverage, the percentage of proteins in a genome that have both literature title and function descriptions, so that DextMP can run on them. Two prediction results are shown: the number of predicted MP proteins which are detected by three or more settings (vote ≥ 3) and the number of MPs detected by the all four settings unanimously (vote = 4). <italic>X.laevis</italic> does not have known MPs. The fraction in parentheses was computed for predicted MPs among all the proteins in the genome.</p></fn></table-wrap-foot></table-wrap></p>
      <p>The human genome has a very high coverage of 98.06% (19 713 proteins out of 20, 104 proteins), which have text information and were subject to DextMP’s prediction. This is much higher than the coverage for both MPFit (67.91%), the GO-based method by Pritykin <italic>et al.</italic> (48.08%) and the PPI-based method (<xref rid="btx231-B3" ref-type="bibr">Chapple <italic>et al.</italic>, 2015</xref>) (64.01%). Out of 45 known MPs in human, 42 were predicted correctly by DextMP (recall: 0.9333) when a consensus with three or more settings was considered. With the full consensus of the four settings, 31 MPs were correctly detected (recall: 0.689). These two recall values are higher than the two existing methods, the GO-based method (recall: 0.4889) and the PPI-based method (recall: 0.0667). Our previous method, MPFit, had a recall of 0.7333, which is between the two recall values recorded in the current work. DextMP predicted that 23.78% to 8.37% of human proteins are MPs with the two cutoffs of consensus voting. The lower value, 8.37%, is close to the MPFit’s prediction of 7.82% (<xref rid="btx231-B25" ref-type="bibr">Khan and Kihara, 2016</xref>).</p>
      <p>As discussed above, a major advantage of DextMP is that it solely relies on text information of proteins, unlike the other methods that cannot be applied for proteins that lack experimental studies (e.g. PPI) or well-curated GO term annotations. Capitalizing on this aspect, we ran another genome, <italic>X.laevis</italic> with DextMP as it is not applicable for MPFit or the two other existing prediction methods because the genome lacks experimental studies. For <italic>X.laevis</italic>, out of 11 078 proteins 30.5% have literature information in UniProt. Due to the smaller number of proteins with literature information as compared with human and yeast, the fraction of predicted MPs in <italic>X.laevis</italic>, 2.51-5.42%, seems small, but the fraction against the 11 078 proteins with literature is in accordance with the results for yeast and human.</p>
      <p>We now discuss three case studies where DextMP made correct prediction to known MPs while our previous method, MPFit, failed. The first example is a band 3 anion transport protein in human (UniProt ID: P02730). The primary function of this protein is transportation of inorganic anions across the plasma membrane while the moonlighting function is a scaffold providing binding sites for glycolytic enzymes (<xref rid="btx231-B28" ref-type="bibr">Low <italic>et al.</italic>, 1993</xref>). MPFit failed to predict this protein as an MP because this protein lacks four out of six omics-data features (i.e. PPI, phylogenetic profile, genetic interaction), which MPFit imputes to complete input feature values but apparently it did not work. In contrast, this protein has functional description in UniProt, which clearly depicts its two functions as follows: <italic>functions both as a transporter that mediates electroneutral anion exchange across the cell membrane and as a structural protein</italic>, and <italic>interactions of its cytoplasmic domain with cytoskeletal proteins, glycolytic enzymes, and hemoglobin</italic>. Based on this text, it was easy for DextMP to make a correct MP prediction.</p>
      <p>The second example is protein PHGPx (UniProt ID: P36969) in human. The primary function of this MP is cell protection against membrane lipid peroxidation and cell death while the moonlighting function is the protein’s structural role in mature spermatozoa (<xref rid="btx231-B38" ref-type="bibr">Scheerer <italic>et al.</italic>, 2007</xref>). MPFit could not see characteristics of MPs in this protein’s omics data, because some input features were not available and moreover, an important feature, functional divergence of interacting proteins in its PPI network, was not observed. However, the protein’s functional description in UniProt indicates two functions, <italic>protects cells against membrane lipid peroxidation</italic> and <italic>required for normal sperm development and male fertility</italic>, which resulted in a correct MP prediction by DextMP.</p>
      <p>The last example is gephyrin (UniProt ID: Q9NQX3) in human. This protein anchors transmembrane receptors by connecting membrane proteins to cytoskeleton microtubule binding proteins. Its moonlighting function is biosynthesis of the molybdenum cofactor (<xref rid="btx231-B40" ref-type="bibr">Stallmeyer <italic>et al.</italic>, 1999</xref>). Similar to the previous two examples, this protein lacks several omics-data that are used as features in MPFit. Gene expression data showed that this protein has a similar expression pattern with genes with different functional classes, which is an indicator of an MP, but this information was diluted in combination with other omics-data. On the other hand, it is clear from its functional description that it has two functions: It says <italic>microtubule-associated protein involved in membrane protein-cytoskeleton interactions</italic> related to its first function, and <italic>catalyzes two steps in the biosynthesis of the molybdenum cofactor</italic>, which is related to the second function.</p>
      <p>Lastly, we provide two examples where DextMP made novel MP prediction (i.e. proteins which are not recorded as known MPs in the MoonProt database). The first example is aminoacyl tRNA synthase complex-interacting protein 1 (UniProt ID: Q12904) in human. This protein is a non-catalytic component of the multi-synthase complex, and among other multi-functionalities, it moonlights by binding to tRNA, possesses inflammatory cytokine activity, and is involved in glucose homeostasis, angiogenesis and wound repair (<xref rid="btx231-B7" ref-type="bibr">Han <italic>et al.</italic>, 2006</xref>). The second example is exoribonuclease in yeast (UniProt ID: P22147). According to its function description in UniProt, this is also a multi-functional protein that exhibits several independent functions at different levels of the cellular processes. It is a 5′–3′ exonuclease component of the nonsense-mediated mRNA decay (NMD), and has a role in multiple processes including DNA strand exchange and exonuclease activities, preventing accumulation of potentially harmful truncated proteins, regulating the decay of wild-type mRNAs, degradation of mature tRNA, and defense against viruses, among other functions (<xref rid="btx231-B20" ref-type="bibr">Johnson and Kolodner, 1999</xref>; <xref rid="btx231-B22" ref-type="bibr">Käslin and Heyer, 1994</xref>).</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>We developed DextMP that predicts MPs from text information, which is the first work of this kind. DextMP complements our earlier work, MPFit, which predicts MPs from their omics data-based features. DextMP showed significant improvement of predictions over existing methods. Moreover, it is widely applicable because it only needs the text information of target proteins. Since the study of MPs is still in its early stage, even in cases that proteins are known to have multiple distinct functions, they are not explicitly labeled as MPs in databases. DextMP will be a very useful tool for detecting potential MPs from a vast amount of UniProt entries.</p>
    <p>It would be appropriate to discuss implication and technical nature of the provided genome-scale MP prediction. Since DextMP uses literature information of genes, the quantity and the quality of available literature directly affects to prediction results. A smaller number of MPs were detected in <italic>X. laevis</italic> than yeast and human apparently because only 30.54% of genes in the genome have literature information. It is also noticed that the predicted fraction of MPs in yeast is larger than human, but this result is also at least partly reflecting the fact that yeast has one of the most well studied and annotated genomes as it is a model organism for systems biology. Another technical point to note is that the accuracy of DexMP was confirmed on the control set, where the numbers of positive and negative data are balanced. Since this MP/non-MP distribution is different in genomes from the control set, the accuracy of the genome-scale prediction may be affected by that. Also the negative data used in the control set have unavoidable uncertainty, because non-MPs in the dataset may be found as MPs in the future.</p>
    <p>In <xref rid="btx231-T5" ref-type="table">Table 5</xref>, we provided two MP estimations by using cutoffs of three or four votes. Using the three-vote criterion showed a better recall against known MPs by design, however, it is difficult to determine which estimations should be more trusted since the number of known MPs are currently very limited. Thus these two criteria should be considered as confidence levels of a prediction. The current prediction provides a rough estimates of MPs in genomes, which itself would be informative and useful to gain a large perspective of MPs. Ultimately, literature of all genes in genomes needs to be manually checked to obtain the precise number of MPs, where DextMP’s prediction can help in prioritizing genes to examine.</p>
    <p>The results of the genome-scale prediction nevertheless suggest that MPs are not mere exceptions but common in organisms. This observation triggers various interesting biological questions, for example, how proteins gain moonlighting functions during evolution and biophysical mechanisms that enable a protein to have multiple functions. Correct annotation to proteins with dual functions also affects to functional enrichment analysis (<xref rid="btx231-B43" ref-type="bibr">Wei <italic>et al.</italic>, 2017</xref>), which is commonly used in systems and network biology (<xref rid="btx231-B4" ref-type="bibr">Dotan-Cohen <italic>et al.</italic>, 2009</xref>; <xref rid="btx231-B9" ref-type="bibr">Hawkins <italic>et al.</italic>, 2010</xref>; <xref rid="btx231-B35" ref-type="bibr">Rachlin <italic>et al.</italic>, 2006</xref>). This work is also relevant to computational biologists, particularly those who are working on developing function prediction methods (<xref rid="btx231-B11" ref-type="bibr">Hawkins and Kihara, 2007</xref>), genome annotation, function analysis on networks and curation of functional annotation in databases.</p>
    <p>Overall this work will help our understanding of the multi-functional nature of proteins at the systems level, and will aid in exploring the complex functional interplay of proteins in a cellular process.</p>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors thank Sarah Rodenbeck for proofreading the manuscript.</p>
    <sec>
      <title>Funding</title>
      <p>This work was partly supported by the National Science Foundation (DBI1262189). DK also acknowledged supports from the National Institutes of Health (R01GM097528) and the National Science Foundation (IIS1319551, IOS1127027, DMS1614777).</p>
      <p><italic>Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="btx231-B1">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bird</surname><given-names>S.</given-names></name></person-group> (<year>2006</year>) <chapter-title>NLTK: the natural language toolkit</chapter-title>
<source>COLING/ACL Interact. Present. Sessions</source>, <fpage>69</fpage>–<lpage>72</lpage>.</mixed-citation>
    </ref>
    <ref id="btx231-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Campbell</surname><given-names>R.M.</given-names></name>, <name name-style="western"><surname>Scanes</surname><given-names>C.G.</given-names></name></person-group> (<year>1995</year>) 
<article-title>Endocrine peptides ′moonlighting′ as immune modulators: roles for somatostatin and GH-releasing factor</article-title>. <source>J. Endocrinol</source>., <volume>147</volume>, <fpage>383</fpage>–<lpage>396</lpage>.<pub-id pub-id-type="pmid">8543908</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chapple</surname><given-names>C.E.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Extreme multifunctional proteins identified from a human protein interaction network</article-title>. <source>Nat. Commnun</source>., <volume>6</volume>, <fpage>7412.</fpage></mixed-citation>
    </ref>
    <ref id="btx231-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dotan-Cohen</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>Biological process linkage networks</article-title>. <source>PLoS ONE</source>, <volume>4</volume>, <fpage>e5313.</fpage><pub-id pub-id-type="pmid">19390589</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gómez</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Do protein-protein interaction databases identify moonlighting proteins?</article-title><source>Mol. BioSyst</source>., <volume>7</volume>, <fpage>2379</fpage>–<lpage>2382</lpage>.<pub-id pub-id-type="pmid">21677976</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gomez</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2003</year>) 
<article-title>Do current sequence analysis algorithms disclose multifunctional (moonlighting) proteins?</article-title><source>Bioinformatics</source>, <volume>19</volume>, <fpage>895</fpage>–<lpage>896</lpage>.<pub-id pub-id-type="pmid">12724303</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>J.M.</given-names></name></person-group><etal>et al</etal> (<year>2006</year>) 
<article-title>Structural separation of different extracellular activities in aminoacyl-tRNA synthetase-interacting multi-functional protein, p43/AIMP1</article-title>. <source>Biochem. Biophys. Res. Commun</source>., <volume>342</volume>, <fpage>113</fpage>–<lpage>118</lpage>.<pub-id pub-id-type="pmid">16472771</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hawkins</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2006</year>) 
<article-title>Enhanced automated function prediction using distantly related sequences and contextual association by PFP</article-title>. <source>Protein Sci</source>., <volume>15</volume>, <fpage>1550</fpage>–<lpage>1556</lpage>.<pub-id pub-id-type="pmid">16672240</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hawkins</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Functional enrichment analyses and construction of functional similarity networks with high confidence function prediction by PFP</article-title>. <source>BMC Bioinformatics</source>, <volume>11</volume>, <fpage>265</fpage>–<lpage>286</lpage>.<pub-id pub-id-type="pmid">20482861</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hawkins</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>PFP: Automated prediction of gene ontology functional annotations with confidence scores using protein sequence data</article-title>. <source>Proteins Struct. Funct. Bioinf</source>., <volume>74</volume>, <fpage>566</fpage>–<lpage>582</lpage>.</mixed-citation>
    </ref>
    <ref id="btx231-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hawkins</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Kihara</surname><given-names>D.</given-names></name></person-group> (<year>2007</year>) 
<article-title>Function prediction of uncharacterized proteins</article-title>. <source>J. Bioinf. Comput. Biol</source>., <volume>5</volume>, <fpage>1</fpage>–<lpage>30</lpage>.</mixed-citation>
    </ref>
    <ref id="btx231-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hernández</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Do moonlighting proteins belong to the intrinsically disordered protein class?</article-title><source>J. Proteomics Bioinf</source>., <volume>5</volume>, <fpage>262</fpage>–<lpage>264</lpage>.</mixed-citation>
    </ref>
    <ref id="btx231-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hernández</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>MultitaskProtDB: a database of multitasking proteins</article-title>. <source>Nucleic Acids Res</source>., <volume>42</volume>, <fpage>D517</fpage>–<lpage>D520</lpage>.<pub-id pub-id-type="pmid">24253302</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hoffman</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Online learning for latent dirichlet allocation</article-title>. <source>Adv. Neural Inf. Process. Syst</source>., <volume>23</volume>, <fpage>856</fpage>–<lpage>864</lpage>.</mixed-citation>
    </ref>
    <ref id="btx231-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huberts</surname><given-names>D.H.</given-names></name>, <name name-style="western"><surname>Vander Klei</surname><given-names>I.J.</given-names></name></person-group> (<year>2010</year>) 
<article-title>Moonlighting proteins: an intriguing mode of multitasking</article-title>. <source>Biochim. Biophys. Acta</source>, <volume>1803</volume>, <fpage>520</fpage>–<lpage>525</lpage>.<pub-id pub-id-type="pmid">20144902</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jeffery</surname><given-names>C.J.</given-names></name></person-group> (<year>2003</year>) 
<article-title>Moonlighting proteins: old proteins learning new tricks</article-title>. <source>Trends Genet</source>., <volume>19</volume>, <fpage>415</fpage>–<lpage>417</lpage>.<pub-id pub-id-type="pmid">12902157</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jeffery</surname><given-names>C.</given-names></name></person-group> (<year>1999</year>) 
<article-title>Moonlighting proteins</article-title>. <source>Trends Biochem. Sci</source>., <volume>24</volume>, <fpage>8</fpage>–<lpage>11</lpage>.<pub-id pub-id-type="pmid">10087914</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jeffery</surname><given-names>C.</given-names></name></person-group> (<year>2004</year>) 
<article-title>Moonlighting proteins: complications and implications for proteomics research</article-title>. <source>Drug Discov. Today TARGETS</source>, <volume>3</volume>, <fpage>71</fpage>–<lpage>78</lpage>.</mixed-citation>
    </ref>
    <ref id="btx231-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Joachims</surname><given-names>T.</given-names></name></person-group> (<year>1998</year>) 
<article-title>Text categorization with support vector machines: Learning with many relevant features</article-title>. <source>Eur. Conf. Mach. Learn</source>., <volume>10</volume>, <fpage>137</fpage>–<lpage>142</lpage>.</mixed-citation>
    </ref>
    <ref id="btx231-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Johnson</surname><given-names>A.W.</given-names></name>, <name name-style="western"><surname>Kolodner</surname><given-names>R.D.</given-names></name></person-group> (<year>1999</year>) 
<article-title>Strand exchange protein 1 from Saccharomyces cerevisiae. A novel multifunctional protein that contains DNA strand exchange and exonuclease activities</article-title>. <source>J. Biol. Chem</source>., <volume>266</volume>, <fpage>14046</fpage>–<lpage>14054</lpage>.</mixed-citation>
    </ref>
    <ref id="btx231-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kanehisa</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Goto</surname><given-names>S.</given-names></name></person-group> (<year>2000</year>) 
<article-title>KEGG: Kyoto encyclopedia of genes and genomes</article-title>. <source>Nucleic Acids Res</source>., <volume>28</volume>, <fpage>27</fpage>–<lpage>30</lpage>.<pub-id pub-id-type="pmid">10592173</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Käslin</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Heyer</surname><given-names>W.D.</given-names></name></person-group> (<year>1994</year>) 
<article-title>A multifunctional exonuclease from vegetative Schizosaccharomyces pombe cells exhibiting in vitro strand exchange activity</article-title>. <source>J. Biol. Chem</source>., <volume>269</volume>, <fpage>14094</fpage>–<lpage>14102</lpage>.<pub-id pub-id-type="pmid">8188690</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Genome-scale identification and characterization of moonlighting proteins</article-title>. <source>Biol. Direct</source>., <volume>9</volume>, <fpage>1</fpage>–<lpage>29</lpage>.<pub-id pub-id-type="pmid">24405803</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Kihara</surname><given-names>D.</given-names></name></person-group> (<year>2014</year>) 
<article-title>Computational characterization of moonlighting proteins</article-title>. <source>Biochem. Soc. Trans</source>., <volume>42</volume>, <fpage>1780</fpage>–<lpage>1785</lpage>.<pub-id pub-id-type="pmid">25399606</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Kihara</surname><given-names>D.</given-names></name></person-group> (<year>2016</year>) 
<article-title>Genome-scale prediction of moonlighting proteins using diverse protein association information</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>2281</fpage>–<lpage>2288</lpage>.<pub-id pub-id-type="pmid">27153604</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Evaluation of function predictions by PFP, ESG, and PSI-BLAST for moonlighting proteins</article-title>. <source>BMC Proceedings</source>, <volume>6</volume>, <fpage>S5.</fpage></mixed-citation>
    </ref>
    <ref id="btx231-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Le</surname><given-names>Q.V.</given-names></name>, <name name-style="western"><surname>Mikolov</surname><given-names>T.</given-names></name></person-group> (<year>2014</year>) 
<article-title>Distributed representations of sentences and documents</article-title>. <source>arXiv Preprint</source>, 1405.4053.</mixed-citation>
    </ref>
    <ref id="btx231-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Low</surname><given-names>P.S.</given-names></name></person-group><etal>et al</etal> (<year>1993</year>) 
<article-title>Regulation of glycolysis via reversible enzyme binding to the membrane protein, band 3</article-title>. <source>J. Biol. Chem</source>., <volume>268</volume>, <fpage>14627</fpage>–<lpage>14631</lpage>.<pub-id pub-id-type="pmid">8325839</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mani</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>MoonProt: a database for proteins that are known to moonlight</article-title>. <source>Nucleic Acids Res</source>., <volume>43</volume>, <fpage>D277</fpage>–<lpage>D282</lpage>.<pub-id pub-id-type="pmid">25324305</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Manning</surname><given-names>C.D.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) <source>Introduction to Information Retrieval</source>. 
<publisher-name>Cambridge University Press</publisher-name>, 
<publisher-loc>Cambridge</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btx231-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mikolov</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Distributed representations of words and phrases and their compositionality</article-title>. <source>Adv. Neural Inf. Process. Syst</source>., <volume>26</volume>, <fpage>3111</fpage>–<lpage>3119</lpage>.</mixed-citation>
    </ref>
    <ref id="btx231-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pedregosa</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Scikit-learn: machine learning in Python</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="btx231-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Piatigorsky</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>WistowG</surname><given-names>J.</given-names></name></person-group> (<year>1989</year>) 
<article-title>Enzyme/crystallins: gene sharing as an evolutionary strategy</article-title>. <source>Cell</source>, <volume>57</volume>, <fpage>197</fpage>–<lpage>199</lpage>.<pub-id pub-id-type="pmid">2649248</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pritykin</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Genome-wide detection and analysis of multifunctional genes</article-title>. <source>PLoS Comput. Biol</source>., <volume>11</volume>, <fpage>e1004467.</fpage><pub-id pub-id-type="pmid">26436655</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rachlin</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2006</year>) 
<article-title>Biological context networks: a mosaic view of the interactome</article-title>. <source>Mol. Syst. Biol</source>., <volume>2</volume>, <fpage>66.</fpage><pub-id pub-id-type="pmid">17130868</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B36">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Rada</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Tarau</surname><given-names>P.</given-names></name></person-group> (<year>2004</year>) TextRank: Bringing order into texts. In: <italic>Proceedings of EMNLP, Association for Computational Linguistics, Barcelona, Spain</italic>, pp. <fpage>404</fpage>–<lpage>411</lpage>.</mixed-citation>
    </ref>
    <ref id="btx231-B37">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Rurek</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Sojka</surname><given-names>P.</given-names></name></person-group> (<year>2010</year>) Software Framework for Topic Modelling with Large Corpora. In: <italic>Proceedings of LREC 2010 workshop New Challenges for NLP Frameworks, University of Malta, Valletta, Malta</italic>, pp. <fpage>45</fpage>–<lpage>50</lpage>.</mixed-citation>
    </ref>
    <ref id="btx231-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Scheerer</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>Structural basis for catalytic activity and enzyme polymerization of phospholipid hydroperoxide glutathione peroxidase-4 (GPx4)</article-title>. <source>Biochemistry</source>, <volume>46</volume>, <fpage>9041</fpage>–<lpage>9049</lpage>.<pub-id pub-id-type="pmid">17630701</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schlicker</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2006</year>) 
<article-title>A new measure for functional similarity of gene products based on Gene Ontology</article-title>. <source>BMC Bioinformatics</source>, <volume>7</volume>, <fpage>322.</fpage><pub-id pub-id-type="pmid">16796757</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stallmeyer</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>1999</year>) 
<article-title>The neurotransmitter receptor-anchoring protein gephyrin reconstitutes molybdenum cofactor biosynthesis in bacteria, plants, and mammalian cells</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A</source>., <volume>96</volume>, <fpage>1333</fpage>–<lpage>1338</lpage>.<pub-id pub-id-type="pmid">9990024</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B41">
      <mixed-citation publication-type="journal"><collab>UniProt Consortium</collab>. (<year>2014</year>) 
<article-title>Activities at the Universal Protein Resource (UniProt)</article-title>. <source>Nucleic Acids Res</source>., <volume>42</volume>, <fpage>D191</fpage>–<lpage>D198</lpage>.<pub-id pub-id-type="pmid">24253303</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Weaver</surname><given-names>D.T.</given-names></name></person-group> (<year>1998</year>) 
<article-title>Telomeres: moonlighting by DNA repair proteins</article-title>. <source>Curr. Biol</source>., <volume>8</volume>, <fpage>R492</fpage>–<lpage>R494</lpage>.<pub-id pub-id-type="pmid">9663383</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wei</surname><given-names>Q.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>NaviGO: Interactive tool for visualization and functional similarity and coherence analysis with gene ontology</article-title>. <source>BMC Bioinformatics</source>, <volume>18</volume>, <fpage>177.</fpage><pub-id pub-id-type="pmid">28320317</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wistow</surname><given-names>G.J.</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>H.</given-names></name></person-group> (<year>1991</year>) 
<article-title>Lens protein expression in mammals:taxon-specificity and the recruitment of crystallins</article-title>. <source>J. Mol. Evol</source>., <volume>32</volume>, <fpage>262</fpage>–<lpage>269</lpage>.<pub-id pub-id-type="pmid">1904503</pub-id></mixed-citation>
    </ref>
    <ref id="btx231-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wool</surname><given-names>I.G.</given-names></name></person-group> (<year>1996</year>) 
<article-title>Extraribosomal functions of ribosomal proteins</article-title>. <source>Trends Biochem. Sci</source>., <volume>21</volume>, <fpage>164</fpage>–<lpage>165</lpage>.<pub-id pub-id-type="pmid">8871397</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
