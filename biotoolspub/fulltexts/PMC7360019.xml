<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Stud Health Technol Inform?>
<?submitter-system nihms?>
<?submitter-userid 12156074?>
<?submitter-authority eRA?>
<?submitter-login elizabethl?>
<?submitter-name Elizabeth Lindemann?>
<?domain nihpa?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">9214582</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">21248</journal-id>
    <journal-id journal-id-type="nlm-ta">Stud Health Technol Inform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Stud Health Technol Inform</journal-id>
    <journal-title-group>
      <journal-title>Studies in health technology and informatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0926-9630</issn>
    <issn pub-type="epub">1879-8365</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7360019</article-id>
    <article-id pub-id-type="pmid">31437913</article-id>
    <article-id pub-id-type="doi">10.3233/SHTI190211</article-id>
    <article-id pub-id-type="manuscript">nihpa1604667</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Recurrent Deep Network Models for Clinical NLP Tasks: Use Case with Sentence Boundary Disambiguation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Knoll</surname>
          <given-names>Benjamin C.</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lindemann</surname>
          <given-names>Elizabeth A.</given-names>
        </name>
        <xref ref-type="aff" rid="A2">b</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Albert</surname>
          <given-names>Arian L.</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Melton</surname>
          <given-names>Genevieve B.</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref ref-type="aff" rid="A2">b</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pakhomov</surname>
          <given-names>Serguei V.S.</given-names>
        </name>
        <xref ref-type="aff" rid="A1">a</xref>
        <xref ref-type="aff" rid="A3">c</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>a</label>Institute for Health Informatics, University of Minnesota, Minneapolis, Minnesota, USA</aff>
    <aff id="A2"><label>b</label>Department of Surgery, University of Minnesota, Minneapolis, Minnesota, USA</aff>
    <aff id="A3"><label>c</label>College of Pharmacy, University of Minnesota, Minneapolis, Minnesota, USA</aff>
    <author-notes>
      <corresp id="CR1"><bold>Address for correspondence:</bold> Serguei Pakhomov, PhD, Department of Pharmaceutical Care and Health Systems, College of Pharmacy, University of Minnesota, 5-130 WDH 1332A, 308 Harvard St SE, Minneapolis, MN 55455, <email>Pakh0002@umn.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>19</day>
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <day>21</day>
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>14</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <volume>264</volume>
    <fpage>198</fpage>
    <lpage>202</lpage>
    <!--elocation-id from pubmed: 10.3233/SHTI190211-->
    <permissions>
      <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This article is published online with Open Access by IOS Press and distributed under the terms of the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P2">Although a number of foundational natural language processing (NLP) tasks like text segmentation are considered a simple problem in the general English domain dominated by well-formed text, complexities of clinical documentation lead to poor performance of existing solutions designed for the general English domain. We present an alternative solution that relies on a convolutional neural network layer followed by a bidirectional long short-term memory layer (CNN-Bi-LSTM) for the task of sentence boundary disambiguation and describe an ensemble approach for domain adaptation using two training corpora. Implementations using the Keras neural-networks API are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/NLPIE/clinical-sentences">https://github.com/NLPIE/clinical-sentences</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>Natural Language Processing</kwd>
      <kwd>Machine Learning</kwd>
      <kwd>Neural Networks (Computer)</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <title>Introduction</title>
    <p id="P3">In contrast to general English, clinical notes have significant differences in structure and content. For instance, clinical text often contains units of thought that fit the technical definition of sentences that are not terminated by the standard sentence boundary symbols or any symbols in many cases. Structures such as labels, section headers, text arranged in tables, and lists are examples of clinical text that do not follow general English rules for sentence termination. Furthermore, clinical text contains a disproportionately high number of acronyms, abbreviations, and ordinal numbers frequently decorated with punctuation symbols and containing variable capitalization. Segmentation errors caused by these ambiguities are magnified in downstream processing.</p>
    <p id="P4">Previous research has shown that transfer learning in deep networks can improve generalization to tasks of related problems with small data sets [<xref rid="R1" ref-type="bibr">1</xref>]. Ensemble methods that engage in meta-learning through weighted voting models such as boosting, bagging, and stacking also reduce the generalization error over standard models [<xref rid="R2" ref-type="bibr">2</xref>]. We utilize transfer learning both in the use of word embeddings, and in our method for domain adaptation of models trained on one corpus to a different, but related corpus of clinical text.</p>
    <p id="P5">Sentence boundary disambiguation (SBD), also known as sentence segmentation or sentence boundary detection, is a well-understood and explored problem in the domain of general English text. In well-formed general English text, most sentences are terminated by sentence boundary symbols. Ambiguities caused by acronyms, abbreviations, quotations, and ordinal numbers are handled by further rules, or by statistical methods such as maximum entropy classification of boundaries. Using these methods on general English text results in accuracy performance above 95% [<xref rid="R3" ref-type="bibr">3</xref>]. Relying on sentence boundary symbols for SBD on the entire text of a clinical document leads to errors in detecting non-terminated sentence boundaries [<xref rid="R4" ref-type="bibr">4</xref>]. Although deep learning is the prevailing approach for many machine learning problems, it remains underutilized in clinical applications, and the generalizability of clinical applications using current approaches is limited [<xref rid="R5" ref-type="bibr">5</xref>].</p>
    <p id="P6">In this paper we report on applying deep neural network methods with the use case of sequence labeling to the SBD problem on the entire text of clinical notes with no preprocessing or cleaning. We show that an architecture consisting of word embeddings enriched with character information run through a bi-LSTM have high accuracy in detecting sentence boundaries. We also explore the generalization problem of using a trained model for SBD on previously unseen text using several implementations, including combining training data, resuming training with data from the new corpus, and a stacking method where the hidden layer results of two models trained separately on both corpora are summed before prediction using a shared prediction layer. We show that the stacking method has the lowest generalization error with 96% F1 score for beginning of sentence tags.</p>
  </sec>
  <sec id="S2">
    <title>Methods</title>
    <sec id="S3">
      <title>Sentence Segmentation</title>
      <p id="P7">SBD is often the first step in solving any problem using natural language processing (NLP). Availability of sentence boundaries is necessary both for many general language tasks, such as part-of-speech tagging and parsing, and for domain-specific analytical tasks such as document classification. Errors in sentence detection tend to propagate to many other areas in a system making sentence accuracy critical for any downstream tasks in a text analysis system.</p>
      <p id="P8">In <xref rid="T1" ref-type="table">Table 1</xref>, some examples of text from Fairview Medical Services notes where sentences are not terminated with sentence boundary symbols are shown.</p>
      <p id="P9">There are several existing commonly used implementations of SBD that rely on or expect sentence boundary symbols and perform poorly in their absence. Stanford CoreNLP [<xref rid="R6" ref-type="bibr">6</xref>] provides a rule-based algorithm, which makes decisions based on the results of a tokenizer to disambiguate whether sentence boundary symbols indicate sentence splits. Natural Language Toolkit (NLTK) [<xref rid="R7" ref-type="bibr">7</xref>] implements SBD using a method that combines rules for sentence boundaries with an unsupervised algorithm for the detection of acronyms and abbreviations, a common source of errors in SBD [<xref rid="R8" ref-type="bibr">8</xref>]. The Apache OpenNLP toolkit [<xref rid="R9" ref-type="bibr">9</xref>] and Apache cTAKES [<xref rid="R10" ref-type="bibr">10</xref>] provide SBD based on the maximum entropy method described in Reynar, et. al [<xref rid="R3" ref-type="bibr">3</xref>]. Previous evaluations have looked at the performance of SBD and have noted the difficulty of the task in the domain of clinical notes and have noted the performance issues on non-terminated sentences [<xref rid="R4" ref-type="bibr">4</xref>].</p>
      <p id="P10">Approaches to the NLP problem of sequence tagging are well-suited for the SBD problem—which can be expressed as a tagging task where words beginning sentences are tagged ‘B’ and words internal to sentences are tagged ‘I’. The architecture for sequence tagging involving recurrent neural networks (RNNs) has shown good results when applied to the tasks of part-of-speech (PoS) tagging and named entity recognition (NER) [<xref rid="R11" ref-type="bibr">11</xref>], and improvements were shown when character information is combined with word embeddings via a convolutional neural network (CNN) [<xref rid="R12" ref-type="bibr">12</xref>].</p>
    </sec>
    <sec id="S4">
      <title>Dataset</title>
      <sec id="S5">
        <title>Source Corpora</title>
        <p id="P11">We created two source corpora for the sentence detection task. The first dataset was drawn from the MIMIC-III (Medical Information Mart for Intensive Care) corpus [<xref rid="R13" ref-type="bibr">13</xref>], which is a de-identified corpus of notes associated with 40,000 intensive care unit patients at the Beth Israel Deaconess Medical Center between 2θ01 and 2012. Our MIMIC corpus consisted of 749 randomly sampled notes. A second, target dataset was drawn from the Fairview Health Services (FV) EHR system. We used a stratified sampling strategy, in which we created batches of 56 notes made up of 16 inpatient notes and 40 outpatient notes. The inpatient notes were selected proportional to the distribution of note type (<xref rid="T2" ref-type="table">Table 2</xref>) and the outpatient notes were selected proportional to the distribution of department (<xref rid="T3" ref-type="table">Table 3</xref>). A total of 952 notes from FV were used, 17 complete batches. MIMIC notes were in plaintext while FV notes were converted from RTF to plaintext using the BioMedICUS system [<xref rid="R14" ref-type="bibr">14</xref>]. The MIMIC corpus used in this study contains a total of 315,797 tokens, and the FV corpus contained 415,112 tokens.</p>
      </sec>
      <sec id="S6">
        <title>Manual Annotation of Sentences</title>
        <p id="P12">The manual annotation of sentences was performed in the BRAT Rapid Annotation Tool [<xref rid="R15" ref-type="bibr">15</xref>] by a pair of trained annotators. Annotators were instructed to label all complete thoughts, section headers, item labels, list items, and fragments using a “Sentence” annotation. For any data that was not groupable into sentence-like units (e.g., purely numeric tables, lists of laboratory data, lines of vital signs measures, and metadata tables such as those in header information), or for any other areas of text for which annotators had low confidence in their ability to correctly label sentences, annotators were instructed to use an “Unsure” annotation. After sentences were manually annotated, the documents were tokenized and converted to tagged sequences where ‘B’ was applied to the first token in every sentence, ‘I’ was applied to the rest of the tokens in the sentence, and ‘O’ was applied to all the tokens in the “Unsure” category.</p>
      </sec>
      <sec id="S7">
        <title>Cross-validation structure</title>
        <p id="P13">To evaluate generalization error, a cross-validation structure was used where 100% of the MIMIC data was used for cross-validation with 80% as a training split and 20% as a validation split; for the FV data 50% was used for cross-validation (again using an 80-20 training-validation split) and 50% was held out as an unseen test corpus. For architecture and hyper-parameter tuning, the MIMIC validation split was used. The FV cross-validation set was used for training models alone or augmenting MIMIC-trained models. During training, the validation data was used to provide validation loss as an estimation of generalization error to determine when the model has stopped improving and training could be halted.</p>
      </sec>
    </sec>
    <sec id="S8">
      <title>Model Architecture</title>
      <p id="P14">Words were tokenized according to rules that split whenever any whitespace, any symbols, or any digits are encountered. Words were represented using a 300-dimension word embedding trained using the Facebook fastText software package [<xref rid="R16" ref-type="bibr">16</xref>] on the entire MIMIC-III corpus preprocessed to replace any symbols with spaces, to replace digits with their English names in separate words, i.e. “1.23” to “one two three”, and to lowercase all letters. These word embeddings are enriched by summing with the results of a convolutional neural network (CNN) on 30-dimension character embeddings which are learned during training on the SBD tagging task. The CNN is made up of one convolutional layer with 300 filters, each looking at the sequences of the embeddings of four characters, followed by global max pooling. The results of the CNN function as an adjustment vector to the original word vector for the sentence tagging task. The input of the character CNN is all characters of the word (including symbols and whitespace) along with a context of up to seven characters between the previous word and the word; and up to seven characters between the word and the next word. Special characters were inserted for the end of the previous word, the beginning of the next word, the beginning and end of the word, as well as the beginning and end of the document if those fell into the context. Using all the original characters, including whitespace, allows structural information about the document’s formatting to be used for SBD decisions. After the word representation is constructed the results are batch-normalized before passing to the next layer. The architecture of the word representation layer is shown in <xref rid="F1" ref-type="fig">Figure 1</xref>.</p>
      <p id="P15">To encode contextual word representations, we use a bidirectional long short-term memory (LSTM) layer. LSTM units are iteratively run on time-series data (sequences of words in the case of text), maintaining an internal cell state as it moves from one input to the next. LSTMs are optimized during training to learn what information is important to remember from previous inputs to the cell. A LSTM layer is parameterized by the number of LSTM units, each contributing one output dimension. In a bi-directional LSTM layer, the inputs are run both ways through the layer, with one set of LSTM units responsible for seeing the data in order and one set responsible for seeing the data in reverse. Dropout and recurrent dropout [<xref rid="R17" ref-type="bibr">17</xref>] were used to provide regularization of the learned weights and prevent overfitting. The results of the bi-LSTM layer are batch-normalized before being passed to the inference layer. In <xref rid="F2" ref-type="fig">Figure 2</xref>, computation of a bi-LSTM on time series data is shown, each node labeled LSTM-F and LSTM-R is the same set of units at different points in the time series, and the lines drawn between nodes represent the propagation of internal memory states to the next item in the time series. The outputs from the forward and backward LSTMs are concatenated to a single contextual word representation, an embedding of the word and surrounding words.</p>
      <p id="P16">After the bi-directional LSTM layer, a sigmoid-activated dense NN prediction layer is used on each contextual word embedding to output the log-probability that the word is the beginning of a sentence. Lasso or L1-norm regularization was used on the weights of the prediction layer to prevent overfitting. The complete graph of our architecture is shown in <xref rid="F3" ref-type="fig">Figure 3</xref>.</p>
      <sec id="S9">
        <title>Domain Adaptation</title>
        <p id="P17">In addition to using models trained on each individual corpus, we evaluated three methods for domain adaptation of models trained on MIMIC to the FV hold-out test set. First, we looked at merging the cross-validation data from both corpora. Second, we looked at resuming training of the network trained on the MIMIC cross-validation data with the FV cross-validation data. Third, we looked at using an ensemble stacking method for transfer learning where the hidden-layer contextual word representations of the network trained on MIMIC were summed with the contextual word representations of a new network before the sigmoid dense NN prediction layer. In this architecture the output of the second network functions as corrections to the first network for the FV training data. This stacked network architecture is shown in <xref rid="F4" ref-type="fig">Figure 4</xref>.</p>
      </sec>
    </sec>
    <sec id="S10">
      <title>Training</title>
      <p id="P18">Based on the results of tuning using CV on the MIMIC corpus, we selected the gradient descent variant ADAM (Adaptive Moment Estimation) [<xref rid="R18" ref-type="bibr">18</xref>] as the optimizer of network weights. During training, only models that were improvements on validation loss were saved, and after 5 epochs with no improvement training was terminated. Binary cross-entropy loss was used for training, and loss values were weighted by the ratio between the target tag probability and an equal distribution of tags, shown in the equation in <xref rid="F5" ref-type="fig">Figure 5</xref>. Mini-batching was used for training, sequences of 32 words were batched into groups of 32 for gradient optimization.</p>
    </sec>
  </sec>
  <sec id="S11">
    <title>Results</title>
    <sec id="S12">
      <title>Manually Annotated Corpora</title>
      <p id="P19">On an overlap of 100 MIMIC notes annotated by both annotators, ignoring “Unsure” annotations, the Cohen’s kappa of Sentence annotations was computed as 0.957 using the irr library in R version 3.4.4. The agreement between annotators on “Unsure” annotations was 0.646. After conversion, on the subset of 100 notes labeled by both annotators Cohen’s kappa was 0.71 for all tags and 0.95 after excluding items labeled as ‘O’ by either annotator. <xref rid="T4" ref-type="table">Tables 4</xref> and <xref rid="T5" ref-type="table">5</xref> describe the distribution of ‘B’, ‘I’, and ‘O’ tags after this conversion.</p>
      <p id="P20">In both source corpora, sentences not terminated by sentence boundary symbols are highly prevalent. Section headers and text labels were common and often ended by the colon sentence boundary symbol. <xref rid="T6" ref-type="table">Table 6</xref> shows the quantity of sentences terminated by each symbol.</p>
    </sec>
    <sec id="S13">
      <title>Evaluation of SBD Approaches</title>
      <p id="P21">For our evaluation we ignored all tags that were labeled as ‘O’ both during training and during evaluation. Thus, the recall, precision, and F1 for ‘B’ and ‘I’ are symmetric, every false positive ‘B’ is a false negative ‘I’ and every false negative ‘B’ is a false positive ‘I’. We’ve reported only the ‘B’ scores as they are directly proportional to the overall accuracy of detected sentences. The best architecture and hyper-parameter tuned models from cross validation achieved 98.6% F1 on both ‘B’ and ‘I’ tags on the MIMIC validation set and 99.2% F1 on ‘B’ and ‘I’ tags in the FV validation set.</p>
      <p id="P22">We evaluated our implementation of SBD against the 50% FV hold-out data set (476 notes). In addition to the architecture described above, we evaluated a maximum entropy / logistic regression classifier (listed as LR) using optimization of a sigmoid-activated dense NN layer on an input of 7 characters before, at the beginning, at the end, and following every word. This is an approach like, but not as tuned as individual implementations of maximum entropy SBD. The primary metrics used for evaluation were the precision, recall, and F1-score for the beginning of sentence class tag.</p>
      <p id="P23">In addition to models trained on MIMIC and FV individually, we evaluated three methods to test generalizability against the FV test corpus. The models trained solely against one corpus are listed as “MIMIC” and “FV.” The results of a model trained on a both corpora’s cross-validation set combined are listed as “MIMIC+FV.” The continued training of one network is listed as “MIMIC then FV” and the ensemble model is listed as “Ensemble.” Results of these evaluations are show in <xref rid="T7" ref-type="table">Table 7</xref> with best results in bold.</p>
    </sec>
  </sec>
  <sec id="S14">
    <title>Discussion</title>
    <p id="P24">The complexity, grammatic idiosyncrasies, and domain variability of clinical text lead to significant hurdles in designing and training generalizable models for NLP tasks. This common challenge necessitates the use high-capacity, complex machine learning models such as the deep neural network approach described here. Leveraging transfer learning and domain adaptation, such as the ensemble method used here, is an important tool to regularize models created from smaller domain-specific corpora with data from external corpora. In the SBD task, the clinical-specific structuring of sentences in our target corpus led us to applying these approaches.</p>
    <p id="P25">In all experiments, recall was higher than precision, which can be explained by the class weighting structure. Models are penalized much higher for missing a ‘B’ tag than for replacing an ‘I’ with a ‘B’ tag, leading to models being overeager in splitting sentences. Adaptation of models trained on one corpus to another corpus of text show clear but relatively small losses in performance—we can see that the MIMIC trained model has an F1-score approximately 0.06 lower than the FV trained model.</p>
    <p id="P26">The ensemble method slightly improves the F1 score against the other domain adaptation methods, increasing precision at a slight cost to recall. The ensemble method was the best performing overall with a 0.96 F1 score on B-tags. This F1-score is on par with the 0.957 Cohen’s Kappa inter-rater agreement on the MIMIC data that represents a “ceiling” for performance of SBD algorithms. As shown in the information about our corpora, the FV corpus has different distribution of sentence ‘B’, ‘I’, and ‘O’ tags than the MIMIC corpus, demonstrating that these methods are successful in adapting to a corpus with significant syntactic differences.</p>
    <p id="P27">There is a loss in performance in the continued training method versus the MIMIC+FV method. In this method, weights may not be able to recover from sub-optimal positions for predicting FV data from the training on MIMIC data. The gradient descent may not be able to find a path from the current position of the weights to the more optimal position of weights found by the FV-only and the MIMIC+FV trained models.</p>
  </sec>
  <sec id="S15">
    <title>Conclusions</title>
    <p id="P28">Our study shows that there are improvements in SBD using deep networks over using traditional classification methods, and that these networks can perform well even against different corpora and against corpora with large proportions of sentences that are not terminated by sentence boundary symbols. We’ve also shown that transfer learning approaches for domain adaptation such as the ensemble model have lower generalization error than combining training sets or continued training.</p>
    <sec id="S16">
      <title>Further Work</title>
      <p id="P29">The generalization performance gains from using a two-network ensemble indicate further exploration into meta-learning and ensemble approaches may be fruitful. Furthermore, usage of this or other transfer learning ensemble methods with general-domain English corpora included as training data for base models remains an unexplored possibility.</p>
      <p id="P30">The accuracy of automatically detected sentences can have substantial consequences on downstream components in a processing pipeline. These benefits are significant on face but have not been formally quantified, and these effects are a potential target for future research.</p>
    </sec>
  </sec>
</body>
<back>
  <ack id="S17">
    <title>Acknowledgements</title>
    <p id="P31">We would like to acknowledge Michael Hietpas, one of the annotators for our data. This research was supported in part by NIH/NCATS UL1TR002494, NIH/NCATS U01TR002062, NIH/NIGMS R01GM102282, and AHRQ R01HS022085. The content is solely the responsibility of the authors and does not necessary represent the official views of the National Institutes of Health or the Agency for Healthcare Research and Quality.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>[1]</label>
      <mixed-citation publication-type="journal"><name><surname>Donahue</surname><given-names>J</given-names></name>, <name><surname>Jia</surname><given-names>Y</given-names></name>, <name><surname>Vinyals</surname><given-names>O</given-names></name>, <name><surname>Hoffman</surname><given-names>J</given-names></name>, <name><surname>Zhang</surname><given-names>N</given-names></name>, <name><surname>Tzeng</surname><given-names>E</given-names></name>, and <name><surname>Darrell</surname><given-names>T</given-names></name>, <article-title>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</article-title>, <source>CoRR</source>. <comment>abs/1310.1</comment> (<year>2013</year>). <comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1310.1531">http://arxiv.org/abs/1310.1531</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R2">
      <label>[2]</label>
      <mixed-citation publication-type="journal"><name><surname>Bauer</surname><given-names>E</given-names></name>, and <name><surname>Kohavi</surname><given-names>R</given-names></name>, <article-title>An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants</article-title>, <source>Machine Learning</source><volume>36</volume> (<year>1999</year>), <fpage>105</fpage>–<lpage>139</lpage>. doi:<pub-id pub-id-type="doi">10.1023/A:1007515423169</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R3">
      <label>[3]</label>
      <mixed-citation publication-type="confproc"><name><surname>Reynar</surname><given-names>JC</given-names></name>, and <name><surname>Ratnaparkhi</surname><given-names>A</given-names></name>, <source>A maximum entropy approach to identifying sentence boundaries</source>, in: <conf-name>Proceedings of the Fifth Conference on Applied Natural Language Processing</conf-name>, <year>1997</year>, <fpage>16</fpage>–<lpage>19</lpage>.</mixed-citation>
    </ref>
    <ref id="R4">
      <label>[4]</label>
      <mixed-citation publication-type="web"><name><surname>Griffis</surname><given-names>D</given-names></name>, <name><surname>Shivade</surname><given-names>C</given-names></name>, <name><surname>Fosler-Lussier</surname><given-names>E</given-names></name>, and <name><surname>Lai</surname><given-names>AM</given-names></name>, <article-title>A Quantitative and Qualitative Evaluation of Sentence Boundary Detection for the Clinical Domain</article-title>., <source>AMIA Joint Summits on Translational Science Proceedings. AMIA Joint Summits on Translational Science</source>. <comment>2016</comment> (<year>2016</year>), <fpage>88</fpage>–<lpage>97</lpage>. <comment><ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/27570656">http://www.ncbi.nlm.nih.gov/pubmed/27570656</ext-link></comment> (<date-in-citation>accessed November 15, 2018</date-in-citation>).<pub-id pub-id-type="pmid">27570656</pub-id></mixed-citation>
    </ref>
    <ref id="R5">
      <label>[5]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>L</given-names></name>, <name><surname>Rastegar-Mojarad</surname><given-names>M</given-names></name>, <name><surname>Moon</surname><given-names>S</given-names></name>, <name><surname>Shen</surname><given-names>F</given-names></name>, <name><surname>Afzal</surname><given-names>N</given-names></name>, <name><surname>Liu</surname><given-names>S</given-names></name>, <name><surname>Zeng</surname><given-names>Y</given-names></name>, <name><surname>Mehrabi</surname><given-names>S</given-names></name>, <name><surname>Sohn</surname><given-names>S</given-names></name>, and <name><surname>Liu</surname><given-names>H</given-names></name>, <article-title>Clinical information extraction applications: A literature review</article-title>, <source>Journal of Biomedical Informatics</source><volume>77</volume> (<year>2018</year>), <fpage>34</fpage>–<lpage>49</lpage>. doi:<pub-id pub-id-type="doi">10.1016/J.JBI.2017.11.011</pub-id><comment>.</comment><pub-id pub-id-type="pmid">29162496</pub-id></mixed-citation>
    </ref>
    <ref id="R6">
      <label>[6]</label>
      <mixed-citation publication-type="confproc"><name><surname>Manning</surname><given-names>C</given-names></name>, <name><surname>Surdeanu</surname><given-names>M</given-names></name>, <name><surname>Bauer</surname><given-names>J</given-names></name>, <name><surname>Finkel</surname><given-names>J</given-names></name>, <name><surname>Bethard</surname><given-names>S</given-names></name>, and <name><surname>McClosky</surname><given-names>D</given-names></name>, <source>The Stanford CoreNLP Natural Language Processing Toolkit</source>, <conf-name>Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</conf-name> (<year>2014</year>) <fpage>55</fpage>–<lpage>60</lpage>. doi:<pub-id pub-id-type="doi">10.3115/v1/P14-5010</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R7">
      <label>[7]</label>
      <mixed-citation publication-type="journal"><name><surname>Bird</surname><given-names>S</given-names></name>, <name><surname>Loper</surname><given-names>E</given-names></name>, and <name><surname>Klein</surname><given-names>E</given-names></name>, <article-title>Natural Language Processing with Python</article-title>, <source>O’Reilly Media Inc</source>, <year>2009</year>.</mixed-citation>
    </ref>
    <ref id="R8">
      <label>[8]</label>
      <mixed-citation publication-type="journal"><name><surname>Kiss</surname><given-names>T</given-names></name>, and <name><surname>Strunk</surname><given-names>J</given-names></name>, <article-title>Unsupervised Multilingual Sentence Boundary Detection</article-title>, <source>Computational Linguistics</source><volume>32</volume> (<year>2006</year>), <fpage>485</fpage>–<lpage>525</lpage>. doi:<pub-id pub-id-type="doi">10.1162/coli.2006.32.4.485</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R9">
      <label>[9]</label>
      <mixed-citation publication-type="web"><collab>Apache OpenNLP, (n.d.)</collab>. <comment><ext-link ext-link-type="uri" xlink:href="https://opennlp.apache.org/">https://opennlp.apache.org/</ext-link></comment> (<date-in-citation>accessed November 15, 2018</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="R10">
      <label>[10]</label>
      <mixed-citation publication-type="web"><collab>Apache cTAKES™ - clinical Text Analysis Knowledge Extraction System, (n.d.)</collab>. <comment><ext-link ext-link-type="uri" xlink:href="http://ctakes.apache.org/">http://ctakes.apache.org/</ext-link></comment> (<date-in-citation>accessed November 15, 2018</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="R11">
      <label>[11]</label>
      <mixed-citation publication-type="journal"><name><surname>Huang</surname><given-names>Z</given-names></name>, <name><surname>Xu</surname><given-names>W</given-names></name>, and <name><surname>Yu</surname><given-names>K</given-names></name>, <article-title>Bidirectional LSTM-CRF Models for Sequence Tagging</article-title>, <source>CoRR</source>. <comment>abs/1508.0</comment> (<year>2015</year>). doi:<pub-id pub-id-type="doi">10.18653/v1/P16-1101</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R12">
      <label>[12]</label>
      <mixed-citation publication-type="journal"><name><surname>Ma</surname><given-names>X</given-names></name>, and <name><surname>Hovy</surname><given-names>EH</given-names></name>, <article-title>End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</article-title>, <source>CoRR</source>. <comment>abs/1603.0</comment> (<year>2016</year>). <comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1603.01354">http://arxiv.org/abs/1603.01354</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R13">
      <label>[13]</label>
      <mixed-citation publication-type="journal"><name><surname>Johnson</surname><given-names>AEW</given-names></name>, <name><surname>Pollard</surname><given-names>TJ</given-names></name>, <name><surname>Shen</surname><given-names>L</given-names></name>, <name><surname>Lehman</surname><given-names>LH</given-names></name>, <name><surname>Feng</surname><given-names>M</given-names></name>, <name><surname>Ghassemi</surname><given-names>M</given-names></name>, <name><surname>Moody</surname><given-names>B</given-names></name>, <name><surname>Szolovits</surname><given-names>P</given-names></name>, <name><surname>Anthony Celi</surname><given-names>L</given-names></name>, and <name><surname>Mark</surname><given-names>RG</given-names></name>, <article-title>MIMIC-III, a freely accessible critical care database</article-title>, <source>Scientific Data</source>. <volume>3</volume> (<year>2016</year>) <fpage>160035</fpage>. doi:<pub-id pub-id-type="doi">10.1038/sdata.2016.35</pub-id><comment>.</comment><pub-id pub-id-type="pmid">27219127</pub-id></mixed-citation>
    </ref>
    <ref id="R14">
      <label>[14]</label>
      <mixed-citation publication-type="web"><collab>BioMedICUS, (n.d.)</collab>. <comment><ext-link ext-link-type="uri" xlink:href="https://nlpie.github.io/biomedicus/">https://nlpie.github.io/biomedicus/</ext-link></comment> (<date-in-citation>accessed November 17, 2018</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="R15">
      <label>[15]</label>
      <mixed-citation publication-type="web"><collab>brat rapid annotation tool, (n.d.)</collab>. <comment><ext-link ext-link-type="uri" xlink:href="http://brat.nlplab.org/">http://brat.nlplab.org/</ext-link></comment> (<date-in-citation>accessed November 15, 2018</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="R16">
      <label>[16]</label>
      <mixed-citation publication-type="journal"><name><surname>Bojanowski</surname><given-names>P</given-names></name>, <name><surname>Grave</surname><given-names>E</given-names></name>, <name><surname>Joulin</surname><given-names>A</given-names></name>, and <name><surname>Mikolov</surname><given-names>T</given-names></name>, <article-title>Enriching Word Vectors with Subword Information</article-title>, <source>CoRR</source>. <comment>abs/1607.0</comment> (<year>2016</year>). <comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1607.04606">http://arxiv.org/abs/1607.04606</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R17">
      <label>[17]</label>
      <mixed-citation publication-type="journal"><name><surname>Semeniuta</surname><given-names>S</given-names></name>, <name><surname>Severyn</surname><given-names>A</given-names></name>, and <name><surname>Barth</surname><given-names>E</given-names></name>, <article-title>Recurrent Dropout without Memory Loss</article-title>, <source>CoRR</source>. <comment>abs/1603.0</comment> (<year>2016</year>). <comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1603.05118">http://arxiv.org/abs/1603.05118</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R18">
      <label>[18]</label>
      <mixed-citation publication-type="journal"><name><surname>Kingma</surname><given-names>DP</given-names></name>, and <name><surname>Ba</surname><given-names>J</given-names></name>, <article-title>Adam: A Method for Stochastic Optimization</article-title>, <source>CoRR</source>. <comment>abs/1412.6</comment> (<year>2014</year>). <comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</ext-link>.</comment></mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="F1" orientation="portrait" position="float">
    <label>Figure 1 –</label>
    <caption>
      <p id="P32">Word Representation Layer</p>
    </caption>
    <graphic xlink:href="nihms-1604667-f0001"/>
  </fig>
  <fig id="F2" orientation="portrait" position="float">
    <label>Figure 2 –</label>
    <caption>
      <p id="P33">Bi-directional LSTM</p>
    </caption>
    <graphic xlink:href="nihms-1604667-f0002"/>
  </fig>
  <fig id="F3" orientation="portrait" position="float">
    <label>Figure 3 –</label>
    <caption>
      <p id="P34">Complete Model Graph</p>
    </caption>
    <graphic xlink:href="nihms-1604667-f0003"/>
  </fig>
  <fig id="F4" orientation="portrait" position="float">
    <label>Figure 4 –</label>
    <caption>
      <p id="P35">Ensemble of Two Networks</p>
    </caption>
    <graphic xlink:href="nihms-1604667-f0004"/>
  </fig>
  <fig id="F5" orientation="portrait" position="float">
    <label>Figure 5 –</label>
    <caption>
      <p id="P36">Weighting of classes</p>
    </caption>
    <graphic xlink:href="nihms-1604667-f0005"/>
  </fig>
  <table-wrap id="T1" position="float" orientation="portrait">
    <label>Table 1 –</label>
    <caption>
      <p id="P37">Examples of sentences without termination</p>
    </caption>
    <table frame="hsides" rules="rows">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Text</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">RECOMMENDATIONS FOR MDs/PROVIDERS TO ORDER:</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Recommendations already ordered by Registered Dietitian (RD): Calorie counts reordered</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Diet: dysphagia diet level 2 mechanical, thin liquids, magic cup between meals, Nepro between meals</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Pt reported his appetite is getting better, he likes the supplements</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">(+) No chance of pregnancy C-spine cleared: N/A, no H/O Chronic pain,no other significant disability</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T2" position="float" orientation="portrait">
    <label>Table 2 –</label>
    <caption>
      <p id="P38">Inpatient note types per FV batch</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Note Type</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Number</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Progress Note</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Plan of Care</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">ED Notes</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">8 other note types</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 each</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T3" position="float" orientation="portrait">
    <label>Table 3 –</label>
    <caption>
      <p id="P39">Outpatient note departments per FV batch</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Note Type</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Number</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Family Medicine</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">5</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Internal Medicine</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Pediatrics</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Obstetrics and Gynecology</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Hematology and Oncology</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Urgent Care</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Physical Therapy</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Cardiovascular Disease</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">3</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">13 other departments</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1 each</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T4" position="float" orientation="portrait">
    <label>Table 4 –</label>
    <caption>
      <p id="P40">Distribution of Tags in MIMIC</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Tag</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Count</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Percentage</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">B</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">23,648</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">7.5%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">I</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">200,272</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">63.4%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">O</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">91,877</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">29.1%</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T5" position="float" orientation="portrait">
    <label>Table 5 –</label>
    <caption>
      <p id="P41">Distribution of Tags in FV</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Tag</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Count</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Percentage</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">B</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">43,636</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">10.5%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">I</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">336,018</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">80.9%</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">O</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">35,458</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">8.5%</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T6" position="float" orientation="portrait">
    <label>Table 6 –</label>
    <caption>
      <p id="P42">Sentence Termination Type</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Type</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">MIMIC</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">FV</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Period</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">12,698 (53.7%)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">13,619 (31.2%)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Exclamation Point</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">19</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Question Mark</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">24 (0.1%)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">261 (0.6%)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Semi-colon</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">48 (0.2%)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">10</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Colon</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4,855 (20.5%)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6,180 (14.2%)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Quotation</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">58 (0.1%)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">No symbol</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6,018 (25.4%)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">23,506 (53.8%)</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T7" position="float" orientation="portrait">
    <label>Table 7 –</label>
    <caption>
      <p id="P43">‘B’ Tag Accuracy Against FV Hold-out</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Precision</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Recall</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">F1</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">LR-MIMIC</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.511</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.840</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.636</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">LR-FV</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.650</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.948</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.771</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIMIC</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.829</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.971</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.895</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">FV</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.923</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.991</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.956</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIMIC+FV</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.919</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>0.995</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.956</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">MIMIC then FV</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.910</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.992</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.949</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Ensemble</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>0.933</bold>
          </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.989</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <bold>0.96</bold>
          </td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
