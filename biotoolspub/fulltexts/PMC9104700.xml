<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.0 20120330//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Genome Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">Genome Res</journal-id>
    <journal-id journal-id-type="hwp">genome</journal-id>
    <journal-id journal-id-type="publisher-id">GENOME</journal-id>
    <journal-title-group>
      <journal-title>Genome Research</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1088-9051</issn>
    <issn pub-type="epub">1549-5469</issn>
    <publisher>
      <publisher-name>Cold Spring Harbor Laboratory Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9104700</article-id>
    <article-id pub-id-type="pmid">35396274</article-id>
    <article-id pub-id-type="medline">9509184</article-id>
    <article-id pub-id-type="doi">10.1101/gr.275870.121</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Method</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Chromatin interaction–aware gene regulatory modeling with graph attention networks</article-title>
      <alt-title alt-title-type="left-running">Karbalayghareh et al.</alt-title>
      <alt-title alt-title-type="right-running">GraphReg</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1723-9355</contrib-id>
        <name>
          <surname>Karbalayghareh</surname>
          <given-names>Alireza</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sahin</surname>
          <given-names>Merve</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Leslie</surname>
          <given-names>Christina S.</given-names>
        </name>
      </contrib>
    </contrib-group>
    <aff>Computational and Systems Biology Program, Memorial Sloan Kettering Cancer Center, New York, New York 10065, USA</aff>
    <author-notes>
      <corresp>Corresponding author: <email>cleslie@cbio.mskcc.org</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <volume>32</volume>
    <issue>5</issue>
    <fpage>930</fpage>
    <lpage>944</lpage>
    <history>
      <date date-type="received">
        <day>7</day>
        <month>6</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>5</day>
        <month>4</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>
        <ext-link xlink:href="http://genome.cshlp.org/site/misc/terms.xhtml" ext-link-type="uri">© 2022 Karbalayghareh et al.; Published by Cold Spring Harbor Laboratory Press</ext-link>
      </copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This article, published in <italic>Genome Research</italic>, is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), as described at <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="930.pdf"/>
    <abstract>
      <p>Linking distal enhancers to genes and modeling their impact on target gene expression are longstanding unresolved problems in regulatory genomics and critical for interpreting noncoding genetic variation. Here, we present a new deep learning approach called GraphReg that exploits 3D interactions from chromosome conformation capture assays to predict gene expression from 1D epigenomic data or genomic DNA sequence. By using graph attention networks to exploit the connectivity of distal elements up to 2 Mb away in the genome, GraphReg more faithfully models gene regulation and more accurately predicts gene expression levels than the state-of-the-art deep learning methods for this task. Feature attribution used with GraphReg accurately identifies functional enhancers of genes, as validated by CRISPRi-FlowFISH and TAP-seq assays, outperforming both convolutional neural networks (CNNs) and the recently proposed activity-by-contact model. Sequence-based GraphReg also accurately predicts direct transcription factor (TF) targets as validated by CRISPRi TF knockout experiments via in silico ablation of TF binding motifs. GraphReg therefore represents an important advance in modeling the regulatory impact of epigenomic and sequence elements.</p>
    </abstract>
    <funding-group>
      <award-group id="funding-1">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health </institution>
            <institution-id institution-id-type="doi">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>HG009395</award-id>
        <award-id>HG012103</award-id>
      </award-group>
      <award-group id="funding-2">
        <funding-source>
          <institution-wrap>
            <institution>National Human Genome Research Institute </institution>
            <institution-id institution-id-type="doi">10.13039/100000051</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>HG009395</award-id>
        <award-id>HG012103</award-id>
      </award-group>
      <award-group id="funding-3">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health </institution>
            <institution-id institution-id-type="doi">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>DK128852</award-id>
      </award-group>
      <award-group id="funding-4">
        <funding-source>
          <institution-wrap>
            <institution>National Institute of Diabetes and Digestive and Kidney Diseases </institution>
            <institution-id institution-id-type="doi">10.13039/100000062</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>DK128852</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="15"/>
    </counts>
  </article-meta>
</front>
<body>
  <p>Transcriptional gene regulation involves the binding of transcription factors (TFs) at both promoter and enhancer regions and the physical interaction of these bound complexes via DNA looping. Technological advances in chromosome conformation capture assays such as Hi-C (<xref rid="GR275870KARC14" ref-type="bibr">Lieberman-Aiden et al. 2009</xref>), HiChIP (<xref rid="GR275870KARC16" ref-type="bibr">Mumbach et al. 2016</xref>), Micro-C (<xref rid="GR275870KARC13" ref-type="bibr">Krietenstein et al. 2020</xref>), and HiCAR (<xref rid="GR275870KARC29" ref-type="bibr">Wei et al. 2022</xref>) provide high-resolution data on 3D chromatin interactions, including regulatory interactions between enhancers and promoters. Three-dimensional interaction data sets combined with traditional 1D epigenomic profiles, including chromatin accessibility (DNase-seq and ATAC-seq) (<xref rid="GR275870KARC6" ref-type="bibr">Buenrostro et al. 2015</xref>) and histone modifications (via ChIP-seq and CUT&amp;RUN) (<xref rid="GR275870KARC24" ref-type="bibr">Skene and Henikoff 2017</xref>), together map the chromatin state and connectivity of regulatory elements and should provide rich training data for predictive gene regulatory models, including models that also incorporate the underlying genomic DNA sequence. Ultimately, such models could be used to infer the regulatory function of noncoding genetic variants in a cell type of interest.</p>
  <p>Here, we propose a gene regulatory modeling approach called GraphReg that integrates 1D epigenomic data (Epi-GraphReg) or both epigenomic and DNA sequence data (Seq-GraphReg) with 3D chromatin interaction data from HiChIP, Hi-C, Micro-C, or HiCAR via a graph neural network to predict gene expression. The 1D input data can include any standard epigenomic assays such as histone modification ChIP-seq, transcription factor ChIP-seq, or chromatin accessibility from DNase-seq or ATAC-seq.</p>
  <p>GraphReg models use convolutional neural network (CNN) layers to learn local representations from 1D inputs, followed by graph attention network (GAT) layers to propagate these representations over the 3D interaction graph, to predict gene expression (CAGE-seq) across genomic positions (bins). GraphReg is trained to predict CAGE-seq (<xref rid="GR275870KARC21" ref-type="bibr">Shiraki et al. 2003</xref>), a tag-based protocol for gene expression measurement and transcription start site (TSS) mapping, because it quantifies promoter output and does not depend on transcript length.</p>
  <p>Our motivation for proposing GraphReg is twofold. First, we aim to improve the accuracy of predictive gene regulatory models by leveraging 3D genomic architecture to incorporate distal enhancer elements, and we show that GraphReg outperforms baseline CNN models for prediction of CAGE output. Second, we seek to interpret the model to assess the functional importance of distal enhancers for regulation of specific target genes. To this end, we use feature attribution methods on trained GraphReg models and show that we obtain a better ranking of functional enhancers as validated by CRISPRi-FlowFISH (<xref rid="GR275870KARC8" ref-type="bibr">Fulco et al. 2019</xref>) and TAP-seq (<xref rid="GR275870KARC20" ref-type="bibr">Schraivogel et al. 2020</xref>) than baseline CNN models and the recently proposed activity-by-contact (ABC) model (<xref rid="GR275870KARC8" ref-type="bibr">Fulco et al. 2019</xref>). Finally, we show that Seq-GraphReg models capture meaningful TF binding signals through in silico motif ablation experiments, which accurately predict direct TF targets as validated by CRISPRi-based TF perturbation experiments.</p>
  <sec sec-type="results" id="s1">
    <title>Results</title>
    <sec id="s1a">
      <title>GraphReg: a deep graph neural network model for interaction-aware gene expression prediction</title>
      <p>In our experiments, we used a minimal set of 1D epigenomic data relevant to gene regulation: DNase-seq as a measure of chromatin accessibility, H3K4me3 ChIP-seq for promoter activity, and H3K27ac ChIP-seq for enhancer activity. For 3D interaction data, we have used a variety of chromatin conformation assays, such as Hi-C, H3K27ac HiChIP, Micro-C, and HiCAR. Because Epi-GraphReg is trained on 1D epigenomic and 3D interaction input data in a given cell type, to predict gene expression output in the same cell type, it learns a regulatory model that can generalize to other contexts. That is, given cell type–specific input data in a new cell type, the trained Epi-GraphReg model can predict gene expression in that cell type. In this sense, the Epi-GraphReg model is cell type–agnostic. Seq-GraphReg uses DNA sequence as the input and performs multitask learning, predicting DNase, H3K4me3, and H3K27ac in the CNN block and predicting CAGE-seq after the GAT block. Seq-GraphReg is therefore a cell type–restricted model because it learns the TF binding motifs specific to the training cell type and consequently cannot generalize to another cell type. However, Seq-GraphReg can capture cell type–specific TF binding motifs in enhancer and promoter regions and potentially predict the impact of DNA sequence alterations on target gene expression.</p>
      <p>We bin the epigenomic data (H3K4me3, K3K27ac, and DNase) at 100-bp resolution. We process the 5-kb resolution Hi-C/HiChIP/Micro-C/HiCAR contact matrices with the HiC-DC+ package (<xref rid="GR275870KARC7" ref-type="bibr">Carty et al. 2017</xref>; <xref rid="GR275870KARC19" ref-type="bibr">Sahin et al. 2021</xref>) to identify significant interactions of genomic distance up to 2 Mb, and we use the 3D interactions satisfying three different false discovery rates (FDR) of 0.1, 0.01, and 0.001 to define a graph on 5-kb genomic bins. We process 3D interaction data up to 2 Mb for two main reasons. First, the read coverage of 3D assays become sparser after 2 Mb, making it difficult to robustly find significant interactions. Second, the majority of known functional enhancers of genes reside within 2 Mb of the TSS. For consistency with the 3D interaction data, we also bin CAGE-seq at 5-kb resolution. In each batch of training, we extract 6-Mb genomic regions and use the corresponding DNA sequences, epigenomic data, interaction graphs from 3D data, and the corresponding CAGE-seq as training examples for the model. For the next batch, we shift the entire 6-Mb region by 2 Mb and repeat this to cover the training chromosomes.</p>
      <p>For Epi-GraphReg (<xref rid="GR275870KARF1" ref-type="fig">Fig. 1</xref>A; <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S1</ext-link>), each epigenomic signal track is treated as a different channel and fed to 1D CNN layers followed by max-pooling and ReLU activation functions. This produces latent local representations at 5-kb resolution, which serve as the node features for the interaction graphs derived from 3D data. We then use several graph attention network (GAT) layers to propagate these local representations over the interaction graphs, so that promoter regions are influenced by the representations of their candidate distal enhancers. Finally, we predict the CAGE values (promoter activities) by a fully connected (FC) layer.</p>
      <fig position="float" id="GR275870KARF1">
        <label>Figure 1.</label>
        <caption>
          <p>A schematic overview of GraphReg models. (<italic>A</italic>) The Epi-GraphReg model uses 1D epigenomic data, such as H3K4me3 and H3K27ac ChIP-seq and DNase-seq (or ATAC-seq) to learn local features of genomic bins via convolutional neural networks, and then propagates these features over adjacency graphs extracted from Hi-C/HiChIP contact matrices using graph attention networks to predict gene expression (CAGE-seq) across genomic bins. (<italic>B</italic>) The Seq-GraphReg model uses DNA sequence as input and, after some convolutional and dilated convolutional layers, predicts epigenomic data. This helps to learn useful latent representations of genomic DNA sequences that are then passed to the graph attention networks to be integrated over the adjacency graphs derived from Hi-C/HiChIP contact matrices and to predict gene expression values (CAGE-seq). (<italic>C</italic>) A 6-Mb genomic region (11 Mb–17 Mb) of Chr 19 showing input and output signals and predictions in K562 cells, including epigenomic data (H3K4me3, H3K27ac, DNase), CAGE, HiChIP interaction graph, and predicted CAGE values for GraphReg and CNN models. Training and evaluations of the models are performed in the dashed middle 2 Mb (here 13 Mb–15 Mb) region so that all genes can see the effects of their distal enhancers up to 2 Mb.</p>
        </caption>
        <graphic xlink:href="930f01" position="float"/>
      </fig>
      <p>For Seq-GraphReg (<xref rid="GR275870KARF1" ref-type="fig">Fig. 1</xref>B; <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S1</ext-link>), we feed one-hot-coded genomic DNA sequences to a series of 1D CNN, max-pooling, and ReLU layers, similar to Epi-GraphReg. The resulting bottleneck representations are then supplied to two blocks corresponding to different prediction tasks: a GAT block, similar to the Epi-GraphReg model, uses several GAT layers followed by a FC layer to predict the CAGE values; and a dilated CNN block, containing several dilated CNN layers whose dilated rate is multiplied by two each layer, to predict the 1D epigenomic data. Dilated CNNs have been used previously to increase the receptive field of CNN layers in deep learning models that predict epigenomic or expression data from DNA sequence, such as Basenji (<xref rid="GR275870KARC11" ref-type="bibr">Kelley et al. 2018</xref>), BPNet (<xref rid="GR275870KARC4" ref-type="bibr">Avsec et al. 2021b</xref>), ExPecto (<xref rid="GR275870KARC32" ref-type="bibr">Zhou et al. 2018</xref>), and Xpresso (<xref rid="GR275870KARC1" ref-type="bibr">Agarwal and Shendure 2020</xref>). Therefore, Seq-GraphReg follows a multitask learning approach to find more meaningful bottleneck representations to provide to the GAT block.</p>
      <p>GATs have an advantage over other graph neural networks (GNN) in that they learn to weight edges in the graph from node features. In this way, GATs weigh enhancer–enhancer (E–E) and enhancer–promoter (E–P) interactions, based on the features learned in the promoter and enhancer bins, to predict CAGE values more accurately. However, non-attention-based GNNs such as graph convolutional networks (GCN) (<xref rid="GR275870KARC12" ref-type="bibr">Kipf and Welling 2017</xref>; <xref rid="GR275870KARC5" ref-type="bibr">Bigness et al. 2022</xref>) fail to learn the importance of individual interactions. It has been shown that GATs outperform GCNs in other machine learning contexts as well (<xref rid="GR275870KARC28" ref-type="bibr">Veličković et al. 2018</xref>). We use different attention heads (<xref rid="GR275870KARF1" ref-type="fig">Fig. 1</xref>A,B; Methods) in each GAT layer to enhance the flexibility of the model to learn distinct E–E and E–P interaction weights and to improve the prediction accuracy by integrating them together. <xref rid="GR275870KARF1" ref-type="fig">Figure 1</xref>C shows an example of a 6-Mb region (11 Mb–17 Mb) of Chromosome 19 in K562 cells with the corresponding true CAGE output, 1D epigenomic inputs, H3K27ac HiChIP interaction graph (FDR = 0.1), and the predicted CAGE signals using Epi-GraphReg and Seq-GraphReg and their CNN counterparts, which we denote as Epi-CNN and Seq-CNN, respectively. The dashed middle 2-Mb region in <xref rid="GR275870KARF1" ref-type="fig">Figure 1</xref>C indicates where we compute the loss in training and evaluate CAGE predictions. The inputs and outputs of Epi-CNN and Seq-CNN models are the same as those of Epi-GraphReg and Seq-GraphReg, respectively, with the exception that they use dilated CNN layers instead of GAT layers and do not use any 3D interaction data (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S1</ext-link>). We can train Seq-GraphReg in an end-to-end or separate manner. In separate training, instead of multitask learning, we first predict epigenomic data from DNA sequence and then feed the bottleneck representation to the GAT layers to predict CAGE expression levels. Because CNN models to predict epigenomic signals can use smaller window sizes (100 kb instead of 6 Mb), more filters can be used given the same GPU memory resources, yielding a better bottleneck representation that consequently leads to improvement in CAGE prediction. However, this prediction improvement comes at the cost of losing access to fast backpropagation-based saliency scores at base-pair resolution (Methods).</p>
      <p>To assess the performance of GraphReg on different graphs, we ran it using four different 3D assays—Hi-C, HiChIP, Micro-C, and HiCAR—with three different FDR cutoffs (from HiC-DC+) 0.1, 0.01, and 0.001. <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figure S2</ext-link> shows the degree distributions of the genes using different graphs in four cell lines: K562, GM12878, hESC, and mESC. Some example graphs corresponding to a subset of Chromosome 1 for these cell types are shown in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figures S3 and S4</ext-link>, showing how various 3D assays and FDR cutoffs yield different graph topologies.</p>
    </sec>
    <sec id="s1b">
      <title>GraphReg accurately predicts gene expression by using 3D interaction data</title>
      <p>We trained GraphReg models for three ENCODE human cell lines (GM12878, K562, hESC) and for mouse embryonic stem cells (mESC), for which complete 1D epigenomic data and 3D interaction data are available. To evaluate, we performed cross-validation experiments in which we held out two chromosomes for testing, used two chromosomes for validation, and trained on all remaining autosomal chromosomes (Methods). Although our model predicts the CAGE signal at all genomic bins, we focused first on predictions for GENCODE-annotated TSS bins of protein coding genes, where the CAGE signal can be non-zero. <xref rid="GR275870KARF2" ref-type="fig">Figure 2</xref>, A and B, and <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figures S5–S11</ext-link> show the CAGE prediction results for GM12878, K562, hESC, and mESC in which predictions for test chromosomes across runs are concatenated together to obtain global performance results (20 test chromosomes over 10 runs). We computed the negative log-likelihood values (<italic>NLL</italic>), based on our loss function for the Poisson distribution, of the predicted CAGE signals for three gene sets: all genes (All); expressed genes, defined as CAGE signal ≥ 5 (Expressed); and expressed genes with at least one E–P interaction (Interacting). We also reported Pearson's correlation (<italic>R</italic>) of log-normalized predicted and true CAGE values (<xref rid="GR275870KARF2" ref-type="fig">Fig. 2</xref>A,B; <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figs. S5–S11</ext-link>). We also compared Seq-GraphReg and Seq-CNN with Basenji on K562 and GM12878 (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S12</ext-link>). In all cases, GraphReg models outperform the corresponding CNN models, achieving higher <italic>R</italic> and lower <italic>NLL</italic> (<xref rid="GR275870KARF2" ref-type="fig">Fig. 2</xref>A,B; <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figs. S5–S11</ext-link>). After restricting to expressed or interacting genes, the problem gets harder, and the prediction improvement of GraphReg over CNN models increases. Epigenome-based models (Epi-GraphReg and Epi-CNN) have higher prediction accuracy than the sequence-based models (Seq-GraphReg and Seq-CNN) (<xref rid="GR275870KARF2" ref-type="fig">Fig. 2</xref>A,B; <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figs. S5–S11</ext-link>) because the epigenomic data are highly correlated with the CAGE output. To plot the box plots in <xref rid="GR275870KARF2" ref-type="fig">Figure 2</xref> and assess statistical significance, we randomly sampled 2000 predicted genes (in each category, All, Expressed, and Interacting) 50 times, computed the mean of 2000 <italic>NLL</italic>s for both GraphReg and CNN, and performed a Wilcoxon signed-rank test to see if the <italic>NLL</italic>s are significantly smaller for GraphReg than CNN. For both epigenome-based (<xref rid="GR275870KARF2" ref-type="fig">Fig. 2</xref>A) and sequence-based (<xref rid="GR275870KARF2" ref-type="fig">Fig. 2</xref>B) models, GraphReg has significantly smaller <italic>NLL</italic> than CNN (Wilcoxon signed-rank test), demonstrating more accurate predictions for GraphReg.</p>
      <fig position="float" id="GR275870KARF2">
        <label>Figure 2.</label>
        <caption>
          <p>GraphReg models outperform their CNN counterparts for gene expression prediction. (<italic>A</italic>,<italic>B</italic>) Negative log-likelihood (<italic>NLL</italic>, lower is better) between true and predicted CAGE signals of epigenome-based (<italic>A</italic>) and sequence-based (<italic>B</italic>) GraphReg and CNN models over 50 random selections of 2000 predicted genes from test chromosomes concatenated from 10 cross-validation experiments with different training, test, and validation chromosomes. Box plots show the distributions of <italic>NLL</italic> in GM12878, K562, hESC, and mESC for three gene sets: all genes, expressed genes (CAGE signal ≥ 5), and expressed genes with at least one 3D interaction (interacting). The 3D data used in Epi-GraphReg (<italic>A</italic>) for each cell type is as follows: Hi-C (FDR = 0.001) for GM12878, H3K27ac HiChIP (FDR = 0.01) for K562, Micro-C (FDR = 0.1) for hESC, and H3K27ac HiChIP (FDR = 0.1) for mESC. The 3D data used for Seq-GraphReg (<italic>B</italic>) is H3K27ac HiChIP (FDR = 0.1) for GM12878, K562, and mESC, and Micro-C (FDR = 0.1) for hESC. Example scatterplots of all predicted test genes that are expressed (CAGE ≥ 5) are plotted for GM12878 in epigenome-based models (<italic>A</italic>) and K562 in sequence-based models (<italic>B</italic>), where the genes are color-coded by the number of 3D interactions <italic>n</italic>. The sequenced-based models have been trained separately (and not using dilated CNN) for K562 and end-to-end for GM12878, hESC, and mESC. (<italic>C</italic>) Box plots show mean squared error (<italic>MSE</italic>) of the true and predicted log-fold gene expression changes between GM12878 and K562 in 50 random selections of 2000 predicted genes from test chromosomes concatenated from 10 cross-validation experiments with different training, test, and validation chromosomes. The sets All, Expressed, and Interacting denote the intersections of such sets in GM12878 and K562. Both Epi- and Seq-GraphReg models have better prediction accuracy than their CNN counterparts. The scatterplots of the true log-fold gene expression changes and the log-fold changes derived from the predicted CAGE values by Seq-GraphReg and Seq-CNN, between GM12878 and K562, are shown for expressed genes (CAGE ≥ 5 in both K562 and GM12878). TSS bins are color-coded by the minimum number of 3D interactions in GM12878 and K562 (<italic>m</italic>). Seq-GraphReg has higher <italic>R</italic> and lower <italic>MSE</italic> than Seq-CNN. (<italic>D</italic>) Epi-GraphReg models show higher cell-to-cell generalization capability than Epi-CNN models. Box plots show the distributions of <italic>NLL</italic> on the test cell type (K562 or GM12878) when trained on the other cell type over 50 random selections of 2000 predicted genes from test chromosomes of the test cell concatenated from 10 cross-validation experiments with different training, test, and validation chromosomes in the training cell. The models are evaluated on the same test chromosomes in the unseen test cell. HiChIP (FDR = 0.1) is used for both cells. The generalization of Epi-GraphReg from K562 to GM12878 is significantly better (<italic>P</italic> &lt; 10<sup>−4</sup>, Wilcoxon signed-rank test) than Epi-CNN in all gene sets. The generalization of Epi-GraphReg from GM12878 to K562 is significantly better (<italic>P</italic> &lt; 10<sup>−4</sup>, Wilcoxon signed-rank test) than Epi-CNN in expressed and interacting genes. The scatterplots of all predicted test genes that are expressed (CAGE ≥ 5) are plotted when trained on K562 and tested on GM12878.</p>
        </caption>
        <graphic xlink:href="930f02" position="float"/>
      </fig>
      <p>We also examined scatterplots of log-normalized [log<sub>2</sub>(<italic>x</italic> + 1)] predicted versus true CAGE values of GraphReg and CNN models across the expressed genes in mESC, hESC, K562, and GM12878 (<xref rid="GR275870KARF2" ref-type="fig">Fig. 2</xref>A,B; <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figs. S5–S11</ext-link>). Here, TSS bins are color-coded by their number of 3D interactions (<italic>n</italic>). An example of Epi-GraphReg and Epi-CNN on GM12878 using Hi-C (FDR = 0.001) is shown in <xref rid="GR275870KARF2" ref-type="fig">Figure 2</xref>A, and an example of Seq-GraphReg and Seq-CNN on K562 using HiChIP (FDR = 0.1) with separate training and no dilated layers is shown in <xref rid="GR275870KARF2" ref-type="fig">Figure 2</xref>B. For epigenome-based models in <xref rid="GR275870KARF2" ref-type="fig">Figure 2</xref>A, Epi-GraphReg yields <italic>R</italic> = 0.654 and <italic>NLL</italic> = 292.34 on expressed genes, outperforming Epi-CNN with <italic>R</italic> = 0.63 and <italic>NLL</italic> = 333.10. For sequence-based models in <xref rid="GR275870KARF2" ref-type="fig">Figure 2</xref>B, Seq-GraphReg yields <italic>R</italic> = 0.494 and <italic>NLL</italic> = 464.77 on expressed genes, outperforming Seq-CNN with <italic>R</italic> = 0.43 and <italic>NLL</italic> = 582.34. More scatterplots for different GraphReg models can be found in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figures S6–S11</ext-link>. Analogous to our previous notion of gene regulatory complexity (<xref rid="GR275870KARC9" ref-type="bibr">González et al. 2015</xref>), we refer to genes with <italic>n</italic> = 0 as “simple genes” and those with <italic>n</italic> &gt; 0 (at least one 3D interaction) as “complex genes.” We hypothesized that by exploiting 3D interaction data, GraphReg would better model the regulation of complex genes. This fact can be easily seen in scatterplots shown in <xref rid="GR275870KARF2" ref-type="fig">Figure 2</xref>B.</p>
      <p>Notably, the extent of improvement of Seq-GraphReg over Seq-CNN is usually higher than that of the Epi-GraphReg over Epi-CNN (<xref rid="GR275870KARF2" ref-type="fig">Fig. 2</xref>A,B; <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figs. S5–S11</ext-link>), suggesting the importance of 3D interaction data for integrating distal regulatory elements and improving prediction accuracy when predicting from DNA sequence, especially for more complex genes with many E–P interactions (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figs. S10, S11</ext-link>). In particular, Seq-CNN models predict lower values for some highly expressed genes with many 3D interactions, whereas Seq-GraphReg can more accurately predict these higher expression values (<xref rid="GR275870KARF2" ref-type="fig">Fig. 2</xref>B; <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figs. S10, S11</ext-link>). The effects of 3D assay (Hi-C, HiChIP, Micro-C, HiCAR) and different FDR levels (0.1, 0.01, 0.001, corresponding to different noise levels in the graphs) for Epi-GraphReg are shown in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figures S5–S9</ext-link>.</p>
      <p>GraphReg's improved performance over CNNs can also be witnessed by calculating the log-fold change between CAGE predictions for two different cell types (<xref rid="GR275870KARF2" ref-type="fig">Fig. 2</xref>C) and by comparing to true log-fold changes using mean squared error (<italic>MSE</italic>). Evaluation of predicted log-fold changes in GM12878 versus K562 on held-out chromosomes in 10 cross-validation runs (<xref rid="GR275870KARF2" ref-type="fig">Fig. 2</xref>C) confirmed the improvement of Epi-GraphReg and Seq-GraphReg over Epi-CNN and Seq-CNN, respectively. We see in <xref rid="GR275870KARF2" ref-type="fig">Figure 2</xref>C that <italic>MSE</italic>s of GraphReg are significantly smaller than CNN (Wilcoxon signed-rank test), and the performance improvements are higher for Seq-GraphReg versus Seq-CNN. <xref rid="GR275870KARF2" ref-type="fig">Figure 2</xref>C shows the scatterplots of predicted versus true log-fold change in TSS bins on held-out chromosomes for expressed genes (CAGE ≥ 5 in both GM12878 and K562), color-coded by the minimum number of 3D interactions over the two cell types (<italic>m</italic>). For Seq-GraphReg, HiChIP (FDR = 0.1) is used in both GM12878 and K562. For Epi-GraphReg, Hi-C (FDR = 0.001) and HiChIP (FDR = 0.01) are used in GM12878 and K562, respectively. As shown in scatterplots of <xref rid="GR275870KARF2" ref-type="fig">Figure 2</xref>C, Seq-GraphReg achieves <italic>R</italic> = 0.285 and <italic>MSE</italic> = 2.61 in expressed genes, compared to very poor <italic>R</italic> = 0.03 and <italic>MSE</italic> = 3.9 for Seq-CNN. Scatterplots of true versus predicted log-fold change by Epi-GraphReg and Epi-CNN are shown in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figure S13</ext-link>, with <italic>R</italic> = 0.561 and <italic>MSE</italic> = 1.94 for Epi-GraphReg and <italic>R</italic> = 0.535 and <italic>MSE</italic> = 2.08 for Epi-CNN. Overall, these results confirm that using 3D chromatin interactions in GraphReg leads to improved gene expression prediction.</p>
      <p>We investigated how well the cell type–agnostic Epi-GraphReg and Epi-CNN models can generalize from one cell type to another. <xref rid="GR275870KARF2" ref-type="fig">Figure 2</xref>D shows the box plots of loss (<italic>NLL</italic>) when we train on one cell type (K562 or GM12878) and test on the other, using 10 cross-validation runs with distinct sets of validation chromosomes (held out from training and used to assess performance in the test cell type). Epi-GraphReg significantly outperforms Epi-CNN (<italic>P</italic> &lt; 10<sup>−4</sup>, Wilcoxon signed-rank test) for generalization from K562 to GM12878 and vice versa in all categories except one: all genes from GM12878 to K562. Example scatterplots for these cross-cell-type and cross-chromosome generalization tasks show the improvement of Epi-GraphReg over Epi-CNN (<xref rid="GR275870KARF2" ref-type="fig">Fig. 2</xref>D): when training on K562 with HiChIP (FDR = 0.1) and testing on GM12878 with HiChIP (FDR = 0.1), Epi-GraphReg attains <italic>R</italic> = 0.607 and <italic>NLL</italic> = 353.22, whereas Epi-CNN yields <italic>R</italic> = 0.578 and <italic>NLL</italic> = 389.52 for expressed genes. The effect of 3D assay choice and FDR values for cross-cell-type generalization is examined in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figures S14–S16</ext-link>, where we see that the best and most robust generalization happens when we use H3K27ac HiChIP in both train and test cell types or HiChIP in train and Hi-C in test cell types, and the worst and least robust generalization performance occurs when we use Hi-C in train and HiChIP in test cell types. One reason for this phenomenon could be that HiChIP interactions, which are enriched for enhancer–promoter interactions, are more generalizable between cell types than Hi-C interactions, which depend strongly on the depth of coverage and library complexity and may be dominated by structural interactions in lower coverage data sets. Similarly to K562 → GM12878, when training on GM12878 with HiChIP (FDR = 0.001) and testing on K562 with HiChIP (FDR = 0.1), Epi-GraphReg attains <italic>R</italic> = 0.563 and <italic>NLL</italic> = 389.71 compared to <italic>R</italic> = 0.585 and <italic>NLL</italic> = 425.91 for Epi-CNN on expressed genes (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S16</ext-link>).</p>
      <p>Normalization of epigenomic tracks is very critical in cross-cell-type experiments because sequencing depths can create undesired batch effects and prevent good generalization to the unseen cell types. For the results shown in <xref rid="GR275870KARF2" ref-type="fig">Figure 2</xref>D and <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figures S14–S16</ext-link>, we used DESeq-like normalization between K562 and GM12878 for each pair of epigenomic tracks. We also tried another normalization called reads per genomic coverage (RPGC) or 1× normalization from deepTools (<xref rid="GR275870KARC18" ref-type="bibr">Ramírez et al. 2016</xref>), and we evaluated the generalization performance across three cell lines: K562, GM12878, and hESC. <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figures S17–S22</ext-link> show the generalization performance from cell type <italic>x</italic> to the cell type <italic>y</italic> (<italic>x</italic> → <italic>y</italic>), where <italic>x</italic> and <italic>y</italic> (<italic>x</italic> ≠ <italic>y</italic>) could be any of K562, GM12878, or hESC. We used Hi-C and HiChIP graphs in K562 and GM12878, and Micro-C and HiCAR graphs in hESC, all with the FDR of 0.1. Overall, the generalization of Epi-GraphReg is better than Epi-CNN in terms of having lower <italic>NLL</italic> and higher <italic>R</italic>. However, the generalization performance varies using different 3D assays in train and test cell types. In each <italic>x</italic> → <italic>y</italic> experiment, the scatterplots for one of the best performing pairs of 3D assays are shown in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figures S17–S22</ext-link>.</p>
      <p>We also wanted to determine the classes of genes for which GraphReg's predictions are better than those of CNN models and thus understand the genes for which 3D information gives the most benefit. To this end, we defined a simple metric called <italic>Delta</italic><sub><italic>NLL</italic></sub> = <italic>NLL</italic><sub><italic>CNN</italic></sub> − <italic>NLL</italic><sub><italic>GraphReg</italic></sub>. If <italic>Delta</italic><sub><italic>NLL</italic></sub> &gt; 0 for a gene, it means that GraphReg's prediction is better than CNN for that gene, and if <italic>Delta</italic><sub><italic>NLL</italic></sub> &lt; 0 for a gene, it means that CNN's prediction is better than GraphReg for that gene. We produced scatterplots of <italic>Delta</italic><sub><italic>NLL</italic></sub> versus the number of interactions for all expressed genes in all four cell types—GM12878, K562, hESC, and mESC—and for both sequence-based (Seq-GraphReg and Seq-CNN) and epigenome-based (Epi-GraphReg and Epi-CNN) models (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figs. S6–S11</ext-link>, <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">S15–S22</ext-link>). The majority of genes have positive <italic>Delta</italic><sub><italic>NLL</italic></sub>, especially the ones with non-zero interactions. Furthermore, the number of genes with positive <italic>Delta</italic><sub><italic>NLL</italic></sub> is higher in sequenced-based models, showing the greater advantage of using 3D information and GraphReg when predicting from DNA sequence. We have also listed in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figures S23–S30</ext-link> the top genes for which either GraphReg or CNN predicts better.</p>
    </sec>
    <sec id="s1c">
      <title>GraphReg accurately identifies functional enhancers of genes</title>
      <p>Thanks to the feature attribution methods for machine learning models—for example, saliency maps (gradient-by-input), DeepLIFT (<xref rid="GR275870KARC22" ref-type="bibr">Shrikumar et al. 2017</xref>), DeepSHAP (<xref rid="GR275870KARC15" ref-type="bibr">Lundberg and Lee 2017</xref>), and Integrated Gradients (<xref rid="GR275870KARC25" ref-type="bibr">Sundararajan et al. 2017</xref>)—it is possible to derive the important input features for the prediction of a specific output. Because GraphReg allows each gene to be influenced by its potential enhancers through interactions in the 3D graph, we hypothesized that feature attribution analysis would allow the identification of functional distal enhancers, that is, those that contribute to the regulation of target genes. CRISPRi-FlowFISH (<xref rid="GR275870KARC8" ref-type="bibr">Fulco et al. 2019</xref>) is a recent enhancer screening approach that uses KRAB-dCas9 interference with candidate enhancer regions in pooled fashion and RNA FISH against a gene of interest as readout; this enables estimation of the effect size of perturbing each candidate enhancer on target gene expression. The developers of CRISPRi-FlowFISH introduced a score called activity-by-contact (ABC) for finding and ranking enhancers of a gene, which is considered the current state-of-the-art for this problem. The ABC score is the product of “activity,” defined as the geometric mean of DNase and H3K27ac signal in each candidate enhancer region, and “contact,” defined by the KR-normalized Hi-C signal between the target promoter and the candidate enhancer, normalized so that the ABC scores of all candidate enhancers up to 5 Mb from the gene sum to one.</p>
      <p>To determine functional enhancers for genes, we used two feature attribution methods: the saliency map (gradient-by-input) and DeepSHAP (<xref rid="GR275870KARC15" ref-type="bibr">Lundberg and Lee 2017</xref>). For the epigenome-based models (Epi-GraphReg and Epi-CNN), as the inputs are binned at 100-bp resolution, both saliency and DeepSHAP scores are computed at 100-bp resolution as well. We used all-zero data as the reference signal for DeepSHAP so that it can evaluate the importance of peak regions in the input tracks. For the sequence-based models (Seq-GraphReg and Seq-CNN), the feature attribution methods can provide scores at nucleotide resolution. However, for enhancer scoring and validation with FlowFISH analysis, as suggested in Basenji (<xref rid="GR275870KARC11" ref-type="bibr">Kelley et al. 2018</xref>), we derived the saliency maps as the dot product of the 100-bp bin representations (bottleneck) and the gradient of the model prediction for the gene with respect to those bin representations. <xref rid="GR275870KARF3" ref-type="fig">Figure 3</xref>A shows the distribution of area under the precision-recall curve (auPR) values for 19 genes from the K562 FlowFISH data set with more than 10 candidate enhancers for all models. The overall precision-recall curve across 2574 enhancer-gene (E-G) pairs for all 19 genes is shown in <xref rid="GR275870KARF3" ref-type="fig">Figure 3</xref>B. HiChIP (FDR = 0.1) was used for GraphReg in these experiments. These results confirm that feature attribution applied to GraphReg models more accurately identifies the functional enhancers of the genes from a pool of candidate enhancers than the ABC score or feature attribution on the corresponding CNN models (<xref rid="GR275870KARF3" ref-type="fig">Fig. 3</xref>A,B). The highest auPR among GraphReg models was 0.4236 (Epi-GraphReg with saliency), outperforming the best CNN model (auPR 0.366 for Epi-CNN with saliency) and ABC (auPR 0.364).</p>
      <fig position="float" id="GR275870KARF3">
        <label>Figure 3.</label>
        <caption>
          <p>GraphReg models more accurately identify functional enhancers of genes. (<italic>A</italic>) Distribution of the area under the precision-recall curve (auPR) for 19 genes in K562 cells based on CRISPRi-FlowFISH data. GraphReg models have higher median of auPR than both CNN and activity-by-contact (ABC) models. (<italic>B</italic>) Precision-recall curves of the GraphReg, CNN, and ABC models for identifying enhancers of 19 genes screened by CRISPRi-FlowFISH. (<italic>C</italic>) Distribution of auPR for 35 genes in K562 cells based on TAP-seq data. GraphReg models have higher median of auPR than both CNN and ABC models. (<italic>D</italic>) Precision-recall curves for the GraphReg, CNN, and ABC models for identifying functional enhancers of 35 genes as determined by TAP-seq. (<italic>E</italic>) <italic>MYC</italic> locus (2.5 Mb) on Chr 8 with epigenomic data, true CAGE, predicted CAGE using GraphReg and CNN models, HiChIP interaction graph, and the saliency maps of the GraphReg and CNN models, all in K562 cells. Experimental CRISPRi-FlowFISH results and ABC values are also shown for <italic>MYC</italic>. Feature attribution shows that GraphReg models exploit HiChIP interaction graphs to find the distal enhancers, whereas CNN models find only promoter-proximal enhancers. Green and red boxes show true positives and false negatives, respectively. CNN models miss the distal enhancers and consequently lead to false negatives in very distal regions.</p>
        </caption>
        <graphic xlink:href="930f03" position="float"/>
      </fig>
      <p>To assess the ability of models to find distal enhancers, we examined the example of <italic>MYC</italic> and considered only candidate enhancers more than 10 kb from TSS. There are 200 such candidate distal enhancer-gene (DE-G) pairs for <italic>MYC</italic>, including eight true functional enhancers. Precision-recall analysis for <italic>MYC</italic> (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S31A</ext-link>) found the highest auPR of GraphReg models to be 0.576 (Epi-GraphReg with DeepSHAP), strongly outperforming the best CNN model (auPR 0.0792, Epi-CNN with DeepSHAP) as well as ABC (auPR 0.3567). Visualization of the feature attribution scores gives insight into how 3D information allows GraphReg to access distal regulatory information that dilated CNNs cannot exploit, despite their large receptive field. <xref rid="GR275870KARF3" ref-type="fig">Figure 3</xref>E shows a 2.5 Mb genomic region containing the <italic>MYC</italic> locus, with the corresponding epigenomic data, HiChIP graph edges, and true and predicted CAGE signals by GraphReg and CNN models in K562 cells, together with DeepSHAP and saliency scores of the four models for <italic>MYC</italic>, and ABC scores and FlowFISH results for <italic>MYC</italic>. Large negative scores in the FlowFISH track indicate true functional enhancers for <italic>MYC</italic>. Feature attribution tracks in <xref rid="GR275870KARF3" ref-type="fig">Figure 3</xref>E show that GraphReg models are able to capture the most distal enhancers of <italic>MYC</italic> (green boxes), about 2 Mb away from its TSS, but the CNN models fail to capture these enhancers and produce false negatives (red boxes).</p>
      <p>We further evaluated GraphReg using recent chromosome-wide enhancer screening data from targeted perturb-seq (TAP-seq) for Chromosomes 8 and 11 in K562 cells (<xref rid="GR275870KARC20" ref-type="bibr">Schraivogel et al. 2020</xref>). We restricted to the 35 genes with at least one functional enhancer, considering all screened enhancers whose distance from the target gene TSS is &lt;2 Mb. <xref rid="GR275870KARF3" ref-type="fig">Figure 3</xref>C shows the distribution of auPR for these 35 genes. Similar to CRISPRi-FlowFISH results (<xref rid="GR275870KARF3" ref-type="fig">Fig. 3</xref>A), GraphReg models have better median auPR than ABC scores or the CNN models using either DeepSHAP or saliency as the feature attribution method. Pooling all the E-G pairs of these 35 genes (4423 E-G pairs in total), precision-recall analysis (<xref rid="GR275870KARF3" ref-type="fig">Fig. 3</xref>D) shows that the highest auPR for the GraphReg models is 0.435 (Epi-GraphReg with saliency), outperforming the best CNN model (auPR 0.3936) and ABC (auPR 0.395).</p>
      <p>As another example of distal enhancer discovery in the TAP-seq data set, we considered screened enhancers more than 20 kb from TSS of <italic>IFITM1</italic> (79 distal DE-G pairs, five true functional enhancers). Precision-recall analysis for <italic>IFITM1</italic> shows that the highest auPR among the GraphReg models is 0.894 (Epi-GraphReg with saliency), compared to 0.635 for the best CNN model (Epi-CNN with DeepSHAP) and 0.751 for ABC (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S31B</ext-link>). We also plotted a 250-kb genomic region containing the <italic>IFITM1</italic> locus, with the corresponding K562 epigenomic data, HiChIP graph edges, true and predicted CAGE signals by GraphReg and CNN models, feature attribution scores for <italic>IFITM1</italic>, ABC scores for <italic>IFITM1</italic>, and TAP-seq results for <italic>IFITM1</italic> (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S31C</ext-link>). Large negative scores in the TAP-seq track indicate true functional enhancers of <italic>IFITM1</italic>. Here, all GraphReg, CNN, and ABC models are able to capture the distal enhancers of <italic>IFITM1</italic> (green boxes), but the CNN models also capture many nonfunctional enhancers as false positives (red boxes).</p>
    </sec>
    <sec id="s1d">
      <title>Seq-GraphReg predicts the direct targets of transcription factor perturbations via motif ablation</title>
      <p>To assess whether Seq-GraphReg captures meaningful transcription factor (TF) motif information, we asked whether we could predict differential gene expression in TF perturbation experiments by applying our model to genomic sequences where the TF's binding motif had been ablated. We downloaded RNA-seq data for 51 CRISPRi TF knockout (KO) experiments in K562 cells from ENCODE and retained 29 experiments for which at least 200 genes were significantly down-regulated. Using two trained models each for Seq-GraphReg and Seq-CNN, we predicted gene expression using wild-type genomic sequences and sequences in which hits of a given TF motif had been zeroed out, then performed differential expression analysis on these values to predict the 100 most down-regulated target genes for the corresponding TF KO experiment (Methods). To have confident true labels, we restricted this analysis to genes that have true significant differential expression (up- or down-regulated) in the real respective TF KO experiments and that also have a wild-type CAGE value of at least 20.</p>
      <p><xref rid="GR275870KARF4" ref-type="fig">Figure 4</xref>A shows the distributions of true mean logFC of the 100 predicted target genes over all TFs for the Seq-GraphReg and Seq-CNN models. As a baseline, we report the distribution of true mean logFC over all significantly differential genes (<italic>P</italic><sub><italic>adj</italic></sub> &lt; 0.05). This baseline actually shows the average performance of a random algorithm that chooses 100 genes randomly out of all significantly differential genes. We also show the corresponding distributions after restricting to predicted genes with <italic>n</italic> ≥ 5, where <italic>n</italic> denotes the number of 3D interactions. In both settings, Seq-GraphReg's predicted target genes are significantly more down-regulated than those of Seq-CNN's or than baseline performance (Wilcoxon signed-rank test on distributions of mean logFC values). <xref rid="GR275870KARF4" ref-type="fig">Figure 4</xref>B shows the heatmaps of the true mean logFC of the top 100 predicted genes by Seq-GraphReg and Seq-CNN for each TF, for <italic>n</italic> ≥ 0 and <italic>n</italic> ≥ 5. For the majority of TFs, the mean logFC of Seq-GraphReg's predicted targets is more negative than that of Seq-CNN's predicted targets, validating the improved performance of Seq-GraphReg for identifying true down-regulated genes. <xref rid="GR275870KARF4" ref-type="fig">Figure 4</xref>C shows the distributions of the precision (fraction of true significantly down-regulated genes among the 100 predicted genes) for all TFs for Seq-GraphReg and Seq-CNN, for <italic>n</italic> ≥ 0 and <italic>n</italic> ≥ 5. As baseline, we report the fraction of significantly down-regulated genes (<italic>P</italic><sub><italic>adj</italic></sub> &lt; 0.05 and logFC &lt; 0) over all significantly differential genes (<italic>P</italic><sub><italic>adj</italic></sub> &lt; 0.05), for each TF. This baseline shows the average performance of a random algorithm. Again, the precision values are always highest for Seq-GraphReg and significantly greater than for Seq-CNN and baseline (Wilcoxon signed-rank test). <xref rid="GR275870KARF4" ref-type="fig">Figure 4</xref>D shows this data in heatmap form, confirming that for the majority of TFs, the precision for Seq-GraphReg is higher than for Seq-CNN.</p>
      <fig position="float" id="GR275870KARF4">
        <label>Figure 4.</label>
        <caption>
          <p>Seq-GraphReg accurately predicts the regulatory effects of transcription factor knockouts by in silico motif ablation. (<italic>A</italic>) Distributions of true mean logFC (over 100 predicted target genes) over all TFs for Seq-GraphReg, Seq-CNN, and baseline for <italic>n</italic> ≥ 0 and <italic>n</italic> ≥ 5, where <italic>n</italic> denotes the number of enhancer–promoter (E–P) interactions. The median of true mean logFCs for predicted genes by Seq-GraphReg is negative, and the distribution is significantly more down-regulated than that of Seq-CNN and baseline (Wilcoxon signed-rank test). (<italic>B</italic>) Heatmaps of the true mean logFC of the top 100 predicted genes by Seq-GraphReg and Seq-CNN for each TF, for <italic>n</italic> ≥ 0 and <italic>n</italic> ≥ 5. For the majority of TFs, the mean logFC of Seq-GraphReg's predicted targets is more negative than that of Seq-CNN's targets. (<italic>C</italic>) Distributions of precision values (fraction of true significantly down-regulated genes among 100 predicted genes) of all TFs for Seq-GraphReg, Seq-CNN, and baseline, for <italic>n</italic> ≥ 0 and <italic>n</italic> ≥ 5. The precision is always highest in Seq-GraphReg and significantly greater than for Seq-CNN and baseline (Wilcoxon signed-rank test). (<italic>D</italic>) Heatmaps of precision values (fraction of true significantly down-regulated genes among the top 100 predicted genes) for Seq-GraphReg and Seq-CNN for each TF and for <italic>n</italic> ≥ 0 and <italic>n</italic> ≥ 5. For the majority of TFs, the precision of Seq-GraphReg is higher than Seq-CNN. (<italic>E</italic>) A visual example of the effect of <italic>JUND</italic> KO on the gene <italic>TCF3</italic>. JUND motif hits around <italic>TCF3</italic> are plotted in blue bars. The promoter of <italic>TCF3</italic> is indicated by green lines, and two distal enhancers A (1.08 Mb downstream) and B (1.29 Mb upstream) of the gene <italic>TCF3</italic> by red lines. The interactions of enhancers A and B with the promoter of <italic>TCF3</italic> in HiChIP graph are marked by blue circles. In silico mutagenesis (ISM) is performed in 100-bp regions of enhancers A and B, each centered at a JUND motif, and the ISM heatmaps are shown. The heatmaps show the difference in predictions (mutated − reference) after applying a mutation at each nucleotide. The heatmaps around the JUND motif in both enhancers A and B are blueish, indicating the importance of JUND motif in these regions for <italic>TCF3</italic> expression prediction. The base-level representations of ISM scores are the negative summation of all four scores (only three are non-zero) at each nucleotide.</p>
        </caption>
        <graphic xlink:href="930f04" position="float"/>
      </fig>
      <p><xref rid="GR275870KARF4" ref-type="fig">Figure 4</xref>E depicts a visual example of predicting the effect of <italic>JUND</italic> KO on the gene <italic>TCF3</italic> in K562. <italic>TCF3</italic> is among the top 20 predicted genes of Seq-GraphReg for the JUND TF (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S32A</ext-link>), and CRISPRi KO of <italic>JUND</italic> leads to significant down-regulation of <italic>TCF3</italic>. <xref rid="GR275870KARF4" ref-type="fig">Figure 4</xref>E shows JUND motif hits (blue bars) around <italic>TCF3</italic>, indicating the motifs ablated for in silico prediction, along with JUND ChIP-seq data in K562 cells, demonstrating that the motif hits indeed decrease in JUND ChIP-seq peaks. To see if Seq-GraphReg can identify the JUND motifs in distal enhancers, we did the following. First, we chose two distal enhancers (indicated in <xref rid="GR275870KARF4" ref-type="fig">Fig. 4</xref>E by A and B) of the gene <italic>TCF3</italic> with direct (one-hop) enhancer–promoter interactions in the HiChIP graph. Enhancers with direct interactions will have a bigger effect size on gene expression upon mutation. We also visualize graph heatmaps (adjacency matrices) instead of loops to facilitate viewing the interactions. Second, we used in silico mutagenesis (ISM) for feature attribution. We performed ISM on Seq-GraphReg trained with fast Fourier transform (FFT) loss to get more meaningful and interpretable motifs. We have borrowed the idea of adding FFT loss from a recent study (<xref rid="GR275870KARC26" ref-type="bibr">Tseng et al. 2020</xref>). We used FFT in the separate training scheme of Seq-GraphReg instead of end-to-end training owing to GPU memory limitation. <xref rid="GR275870KARF4" ref-type="fig">Figure 4</xref>E shows in silico mutagenesis results for the two distal enhancer regions A (1.08 Mb downstream) and B (1.29 Mb upstream) of the gene <italic>TCF3</italic>, whose interactions with the <italic>TCF3</italic> promoter are marked by blue circles in the HiChIP graph. The promoter of <italic>TCF3</italic> is specified by a green line, and the enhancers A and B are specified by red lines. ISM is performed in 100-bp regions of enhancers A and B, each centered at a JUND motif. The ISM heatmaps show the difference in predictions (mutated – reference) after applying a mutation at each nucleotide. The heatmaps around the JUND motif in both enhancers A and B are blueish, meaning that ISM identifies the importance of the JUND motif in these regions for target gene expression prediction. In other words, mutations in the JUND motif in distal enhancers A and B lead to a reduction in predicted expression, showing the positive effect of these distal JUND motifs on the expression of <italic>TCF3</italic>. The base-level representations of ISM scores are the negative summation of all four scores (only three are non-zero) at each nucleotide, which identify the JUND motifs.</p>
      <p>Overall, these results show that Seq-GraphReg models can better capture the regulatory relationships of TFs and genes than Seq-CNN models. This validates the primary motivation for our model—namely, that TFs can regulate their target genes by binding distal enhancers that loop to target promoters, and that these regulatory effects cannot be effectively learned without 3D interaction data. GraphReg is therefore the model of choice to capture complex gene regulatory relationships. To further show the potential of Seq-GraphReg for decoding gene regulation, we looked at the feature attributions of the 100 best predicted complex genes (having high numbers of E–P interactions) in GM12878, K562, and hESC and identified distal TF motifs that play a role in regulating those genes. <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figures S33–S38</ext-link> show the most enriched distal (at least 20 kb away from the TSS) TF motifs and their contributions in regulating their target genes in each cell type (Methods). Such motif analysis results are interesting in that they provide insights about the interplay between the distal TF motifs and their target genes. Although these results are at this stage just predictions by the Seq-GraphReg model, it would be interesting to pursue experimental validation by deletion of the identified distal TF motifs and assessing the resulting effect on target gene expression. We hope to perform such validation experiments in other biologically relevant systems in the near future.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s2">
    <title>Discussion</title>
    <p>Until now, machine learning models for the genome-wide prediction of gene expression in a given cell state, or gene expression changes in a cell state transition, have largely relied on 1D epigenomic data and/or genomic DNA sequence without using 3D genomic architecture. Prior models include regularized linear regression approaches (<xref rid="GR275870KARC9" ref-type="bibr">González et al. 2015</xref>; <xref rid="GR275870KARC17" ref-type="bibr">Osmanbeyoglu et al. 2019</xref>) as well as more recent work using convolutional neural networks (<xref rid="GR275870KARC23" ref-type="bibr">Singh et al. 2016</xref>; <xref rid="GR275870KARC11" ref-type="bibr">Kelley et al. 2018</xref>; <xref rid="GR275870KARC32" ref-type="bibr">Zhou et al. 2018</xref>; <xref rid="GR275870KARC1" ref-type="bibr">Agarwal and Shendure 2020</xref>; <xref rid="GR275870KARC3" ref-type="bibr">Avsec et al. 2021a</xref>). However, previous linear models used a fixed assignment of regulatory elements to genes without incorporating 3D interaction data, whereas current deep learning models consider relatively local features (such as promoters and at most nearby enhancers) and therefore cannot capture the impact of distal regulatory elements, which can be 1 Mb or farther away from gene promoters. Two previous studies tried to use 3D data to predict gene expression (<xref rid="GR275870KARC31" ref-type="bibr">Zeng et al. 2019</xref>; <xref rid="GR275870KARC5" ref-type="bibr">Bigness et al. 2022</xref>) but did not address important aspects of modeling gene regulation. The most directly relevant one, GC-MERGE (<xref rid="GR275870KARC5" ref-type="bibr">Bigness et al. 2022</xref>), uses histone modifications and Hi-C data to predict gene expression (RNA-seq) using graph convolutional networks (GCN); however, this model does not provide any new insights about regulation of genes, such as finding functional enhancers or revealing the regulatory role of TF binding motifs. Our proposed method, GraphReg, is the most comprehensive and versatile model that is capable of revealing mechanistic information on gene regulation. We have provided the properties of previous deep learning models for gene expression prediction in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Table S1</ext-link>. The comparisons of GraphReg with these methods in terms of Pearson's correlation (R) of predicted versus true gene expression values are provided in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Tables S2, S3</ext-link>, and <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figure S12</ext-link>.</p>
    <p>We have shown that GraphReg more effectively models the impact of distal enhancers on gene expression than 1D dilated CNNs. Although dilated CNNs have a large receptive field, our feature attribution analyses show that only relatively promoter-proximal information influences gene expression prediction in these models. In contrast, GraphReg exploits 3D chromatin interactions to access distal information up to 2 Mb from the gene promoter. In other words, GraphReg adds inductive bias from biology to the deep predictive model, which converts a black-box model attending to everywhere to a more focused model attending to relevant regions.</p>
    <p>An alternative attention-based methodology for learning long-range interactions in sequential input is the transformer, a model that provides state-of-the-art performance in machine translation and other natural language processing tasks (<xref rid="GR275870KARC27" ref-type="bibr">Vaswani et al. 2017</xref>). A recent work, Enformer (<xref rid="GR275870KARC3" ref-type="bibr">Avsec et al. 2021a</xref>), introduced a transformer model for the prediction of CAGE as well as epigenomic tracks from genomic sequence. Currently, however, this transformer-based architecture can integrate information up to 100 kb away in the genome, an order of magnitude less than GraphReg, while requiring considerable computational resources to train. GraphReg's use of 3D interaction data through graph attention networks therefore provides an efficient and biologically well-motivated means to encode distal regulation.</p>
    <p>We showed that Seq-GraphReg meaningfully encodes TF motif binding information by performing motif ablation experiments, in which predicted differential gene expression based on ablated versus wild-type genomic sequences recovered true regulatory effects as measured by CRISPRi TF knockout experiments. A longer-term goal of this work is to model the impact of distal regulatory variants on gene expression, extending efforts to predict the regulatory impact of promoter-proximal genetic variants (<xref rid="GR275870KARC32" ref-type="bibr">Zhou et al. 2018</xref>). We anticipate that it will be critical to train such models using true genetic sequence variation and expression in disease-relevant cellular contexts as these data become available.</p>
    <p>Although we currently use GraphReg to predict bulk CAGE-seq counts with Poisson loss, other transcriptomic assays could be explored in future work. For example, suitably processed nascent transcription assays such as GRO-seq would yield a quantification of promoter activity as well as enhancer RNA expression and could potentially be used to train GraphReg models. We also envision extensions to single-cell multi-omic data sets—training at the pseudo-bulk level where signals are aggregated over cell clusters or metacells, or even at the single-cell level. Here, (aggregated) scRNA-seq gives a tag-based output signal similar to CAGE-seq. Therefore, we anticipate that GraphReg models will have broad applicability for interpreting the function of epigenomic and genomic variation on gene expression.</p>
  </sec>
  <sec sec-type="methods" id="s3">
    <title>Methods</title>
    <sec id="s3a">
      <title>Epigenomic data processing</title>
      <p>We use bigWig coverage tracks for epigenomic data. We acquired these tracks in three ways: (1) by downloading the processed bigWig files from the ENCODE portal; (2) by using the <italic>bam_cov.py</italic> script from Basenji (<xref rid="GR275870KARC11" ref-type="bibr">Kelley et al. 2018</xref>), which reads in the BAM alignment file and generates the bigWig files; and (3) by using the <italic>bamCoverage</italic> command from deepTools (<xref rid="GR275870KARC18" ref-type="bibr">Ramírez et al. 2016</xref>), which reads in a BAM alignment file and returns the bigWig file using different normalization schemes. We used 1× normalization or reads per genome coverage (RPGC), which normalizes the coverage tracks by sequencing depth. For cross-cell-type analyses in which we train on one cell type and test on another one, the normalization of epigenomic coverage tracks is critical to avoid any undesired batch effects. We used two different normalizations for this purpose: (1) RPGC, and (2) DESeq-like normalization. We bin the epigenomic tracks at 100 bp, get the coverage in each bin, and apply log-normalization using the function log<sub>2</sub>(<italic>x</italic> + 1). This log-normalization is performed for the epigenome-based models in which the epigenomic data are inputs to the models.</p>
    </sec>
    <sec id="s3b">
      <title>CNN layers for learning local representations of 1D data</title>
      <p>The inputs to the first layer of the Epi-GraphReg (Epi-CNN) and Seq-GraphReg (Seq-CNN) models are 1D epigenomic data and genomic DNA sequence, respectively. Regardless of the 1D input type, we use several CNN layers followed by ReLU activation, BatchNorm, dropout, and max-pooling to learn local representations for 5-kb bins of the genome (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S1</ext-link>). We consider genomic regions of 6 Mb in length as our input; hence, we have vectors of size <italic>N</italic> = 1200, representing 5-kb bins over the 6-Mb region, for CAGE values. We bin the epigenomic data at 100-bp resolution; therefore, the length of each epigenomic input vector would be 60,000. In the GraphReg models, we define the set of local representations by <italic>H</italic> = {<italic>h</italic><sub>1</sub>, <italic>h</italic><sub>2</sub>, …, <italic>h</italic><sub><italic>N</italic></sub>}, where <italic>h</italic><sub><italic>i</italic></sub> ∈ ℝ<sup><italic>F</italic></sup> and <italic>F</italic> = 128 is the channel number of the last CNN layer just before the GAT block. Then <italic>H</italic> is given to the GAT block, which is a type of graph neural network (GNN) (<xref rid="GR275870KARC12" ref-type="bibr">Kipf and Welling 2017</xref>; <xref rid="GR275870KARC28" ref-type="bibr">Veličković et al. 2018</xref>; <xref rid="GR275870KARC30" ref-type="bibr">Xu et al. 2019</xref>), where <italic>h</italic><sub><italic>i</italic></sub> is the node feature of node <italic>i</italic>. To make the comparisons fair, in Epi-CNN and Seq-CNN models we use eight layers of dilated CNNs with residual connections (<xref rid="GR275870KARC11" ref-type="bibr">Kelley et al. 2018</xref>; <xref rid="GR275870KARC4" ref-type="bibr">Avsec et al. 2021b</xref>) to increase the receptive fields up to 2.5 Mb upstream and downstream from gene TSSs, where the dilation rate is multiplied by 2 at each layer. In the sequence-based models (Seq-GraphReg and Seq-CNN) we also predict epigenomic data to guide the models to learn informative local motif representations; after the first five CNN layers, where the resolution is 100 bp, we use six layers of dilated CNNs with residual connections to predict three epigenomic assays, H3K4me3, H3K27ac, and DNase, at 100-bp resolution (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S1</ext-link>). The learned local features just before six dilated CNN layers are called bottleneck representations that are passed through some CNN layers with max-pooling to bring the resolution to 5 kb and then are given to GAT block in Seq-GraphReg (and eight layers of dilated CNN in Seq-CNN) to predict the CAGE (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S1</ext-link>).</p>
    </sec>
    <sec id="s3c">
      <title>Graph attention networks for integration of local features via 3D interaction data</title>
      <p>We use the graphs extracted from 3D data (Hi-C, HiChIP, Micro-C, HiCAR) to capture gene regulatory interactions and predict gene expression (CAGE-seq). We processed 3D data sets at 5 kb resolution using HiC-DC+ (<xref rid="GR275870KARC7" ref-type="bibr">Carty et al. 2017</xref>; <xref rid="GR275870KARC19" ref-type="bibr">Sahin et al. 2021</xref>) and kept only the significant interactions with three significance levels of FDR less than 0.1, 0.01, and 0.001. The nodes in the extracted graphs therefore represent 5-kb genomic bins, and the edges indicate the most significant interactions between bins. Each 6-Mb genomic input region thus corresponds to a graph with <italic>N</italic> = 1200 nodes whose features <italic>H</italic> = {<italic>h</italic><sub>1</sub>, <italic>h</italic><sub>2</sub>, …, <italic>h</italic><sub><italic>N</italic></sub>} have been learned in the previous CNN block.</p>
      <p>The graph attention network (GAT) block receives a graph <italic>G</italic> = (<italic>V</italic>, <italic>E</italic>) and a set of node features <inline-formula id="il1"><mml:math id="IL1" display="inline" overflow="scroll"><mml:msup><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mo fence="false">{</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:mo>…</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo fence="false">}</mml:mo></mml:math></inline-formula> from the previous layer <italic>t</italic> and outputs an updated set of node features <inline-formula id="il2"><mml:math id="IL2" display="inline" overflow="scroll"><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo fence="false">{</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:mo>…</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo fence="false">}</mml:mo></mml:math></inline-formula>. Each GAT layer uses two weight matrices: <inline-formula id="il3"><mml:math id="IL3" display="inline" overflow="scroll"><mml:msubsup><mml:mi>W</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> for promoters (or self-nodes) and <inline-formula id="il4"><mml:math id="IL4" display="inline" overflow="scroll"><mml:msubsup><mml:mi>W</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> for enhancers (or neighbor nodes), where <italic>F</italic><sup>′</sup> is the number of output features in each GAT layer. The importance of distinct enhancers on a promoter might be different. In the case of GCNs, all enhancers are treated as of equal importance because all the E–P weights are the same. However, by using GAT, we give the model the ability to adjust the weights of different enhancers based on their importance for the task of predicting expression of a specific gene. Therefore, the GAT formulation makes more biological sense for our problem. Here, unlike previous graph neural networks (<xref rid="GR275870KARC28" ref-type="bibr">Veličković et al. 2018</xref>), we do not include self-loops in the graph <italic>G</italic>. We have decoupled self-loops and neighbor-loops because of the differing role of promoters and enhancers in the model. By only including the neighbor-loops in the graph <italic>G</italic>, the model focuses on distal enhancers that cannot be captured in local models such as CNNs.</p>
      <p>We define the self-attention mechanism from node <italic>j</italic> to node <italic>i</italic> at layer <italic>t</italic> as<disp-formula id="GR275870KARM1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:msubsup><mml:mtext>β</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>
where <inline-formula id="il5"><mml:math id="IL5" display="inline" overflow="scroll"><mml:msubsup><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula id="il6"><mml:math id="IL6" display="inline" overflow="scroll"><mml:msubsup><mml:mi>a</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math></inline-formula> are two weight vectors, <italic>σ</italic>(.) is the sigmoid function, <inline-formula id="il7"><mml:math id="IL7" display="inline" overflow="scroll"><mml:msubsup><mml:mtext>β</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>∈</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the attention weight from node <italic>j</italic> to node <italic>i</italic> at layer <italic>t</italic>, and <italic>T</italic> denotes the transpose of a vector. By using a sigmoid function instead of a conventional softmax function, we give extra freedom to the model to discard irrelevant and nonenhancer interactions. We also account for the cardinality of the nodes by defining <inline-formula id="il8"><mml:math id="IL8" display="inline" overflow="scroll"><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msqrt><mml:mo fence="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo fence="false">|</mml:mo></mml:msqrt><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:math></inline-formula>, where <inline-formula id="il9"><mml:math id="IL9" display="inline" overflow="scroll"><mml:msup><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula id="il10"><mml:math id="IL10" display="inline" overflow="scroll"><mml:msup><mml:mi>b</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math></inline-formula> are two weight vectors. Finally, we define the updates of the node features at the next layer as<disp-formula id="GR275870KARM2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>∘</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mtext>β</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>
where <italic>f</italic> is a nonlinearity function and ° is the element-wise product, <inline-formula id="il11"><mml:math id="IL11" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the set of neighbors of node <italic>i</italic> (not including itself), and <inline-formula id="il12"><mml:math id="IL12" display="inline" overflow="scroll"><mml:mo fence="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo fence="false">|</mml:mo></mml:math></inline-formula> is the number of neighbors of node <italic>i</italic>. We use an exponential linear unit (ELU) for <italic>f</italic>. As in <xref rid="GR275870KARC28" ref-type="bibr">Veličković et al. (2018)</xref>, we use <italic>K</italic> heads and concatenate the features as<disp-formula id="GR275870KARM3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo fence="false">|</mml:mo><mml:msubsup><mml:mo fence="false">|</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>∘</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mrow><mml:mo>∈</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:msubsup><mml:mtext>β</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="il13"><mml:math id="IL13" display="inline" overflow="scroll"><mml:msubsup><mml:mo fence="false">‖</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:math></inline-formula> means concatenation of <italic>K</italic> independent heads. Therefore, <inline-formula id="il14"><mml:math id="IL14" display="inline" overflow="scroll"><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> will have <italic>KF</italic><sup>′</sup> features for <italic>i</italic> ∈ {1, …, <italic>N</italic>} and <italic>t</italic> ∈ {1, …, <italic>L</italic>}, where <italic>L</italic> is the number of GAT layers.</p>
      <p>In Equation (<xref rid="GR275870KARM3" ref-type="disp-formula">3</xref>), the updated feature <inline-formula id="il15"><mml:math id="IL15" display="inline" overflow="scroll"><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> at bin <italic>i</italic> is also a function of the number of its neighbors, namely, <inline-formula id="il16"><mml:math id="IL16" display="inline" overflow="scroll"><mml:mo fence="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo fence="false">|</mml:mo></mml:math></inline-formula>. The impact of node cardinality on its feature is nontrivial by adding <inline-formula id="il17"><mml:math id="IL17" display="inline" overflow="scroll"><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> in Equation (<xref rid="GR275870KARM3" ref-type="disp-formula">3</xref>). We mentioned that Epi-GraphReg models are cell type–agnostic, meaning that they can be generalized from one cell type to another one. When we want to train the Epi-GraphReg models on some cell types and test them on other unseen cell types, this might lead to performance degradation if the distributions of node cardinality of graphs in the train and test cells are different. This could happen for many reasons, including the library sizes or FDR levels of Hi-C, HiChIP, Micro-C, or HiCAR data from which we extract the graphs. For example, in our experiments, we noticed this discrepancy between the graphs of GM12878 and K562 cells. To solve this and improve the generalization performance, we normalize the attention weights of each node <italic>i</italic> such that the summation of all the attention weights becomes one and we remove the cardinality parameter <inline-formula id="il18"><mml:math id="IL18" display="inline" overflow="scroll"><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. As such, the normalized updates of GAT layers can be written as<disp-formula id="GR275870KARM4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>
where <inline-formula id="il19"><mml:math id="IL19" display="inline" overflow="scroll"><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:msubsup><mml:mtext>β</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> is the attention coefficient of node <italic>i</italic> (or promoter coefficient) and <inline-formula id="il20"><mml:math id="IL20" display="inline" overflow="scroll"><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mtext>β</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:msubsup><mml:mtext>β</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> is the attention coefficient from node <italic>j</italic> to node <italic>i</italic> (or enhancer coefficient). We see that <italic>η</italic><sub><italic>ij</italic></sub> ∈ [0, 1] for all <italic>i</italic>, <italic>j</italic> ∈ {1, …, <italic>N</italic>} and all the attention coefficients of each node <italic>i</italic> sum up to one: <inline-formula id="il21"><mml:math id="IL21" display="inline" overflow="scroll"><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∪</mml:mo><mml:mo fence="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo fence="false">}</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Similar to Equation (<xref rid="GR275870KARM3" ref-type="disp-formula">3</xref>), when having <italic>K</italic> heads, the normalized updates would be<disp-formula id="GR275870KARM5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo fence="false">|</mml:mo><mml:msubsup><mml:mo fence="false">|</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:mi>j</mml:mi><mml:mrow><mml:mo>∈</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>h</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>
Because the second formulation in Equation (<xref rid="GR275870KARM5" ref-type="disp-formula">5</xref>) is more robust, it is used as the default choice for the GraphReg models.</p>
    </sec>
    <sec id="s3d">
      <title>Poisson regression and loss function</title>
      <p>After the GAT layers in GraphReg models, the last layer before the predictions is a width-1 CNN layer (equivalent to a fully connected layer) with exponential nonlinearity to predict the CAGE value at each bin. As the CAGE values are counts, we used Poisson regression, meaning that the expected value of each (CAGE) output given the inputs is the mean of a Poisson distribution.</p>
      <p>For the Epi-GraphReg model, we let <italic>X</italic> denote the 1D epigenomic inputs for a 6-Mb region, <italic>G</italic> the corresponding graph, <italic>Y</italic> = [<italic>y</italic><sub>1</sub>, …, <italic>y</italic><sub><italic>N</italic></sub>] the observed CAGE signal across 5-kb bins, and <inline-formula id="il22"><mml:math id="IL22" display="inline" overflow="scroll"><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>θ</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> the predicted CAGE signal for the bin <italic>i</italic>, where <italic>θ</italic> represents parameters of the model. Now we assume that <inline-formula id="il23"><mml:math id="IL23" display="inline" overflow="scroll"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo fence="false">|</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>∼</mml:mo><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula>, where <inline-formula id="il24"><mml:math id="IL24" display="inline" overflow="scroll"><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>θ</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula id="il25"><mml:math id="IL25" display="inline" overflow="scroll"><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mi>Y</mml:mi><mml:mo fence="false">|</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula>. Hence, the loss function is the negative log-likelihood of the Poisson distribution,<disp-formula id="GR275870KARM6"><label>(6)</label><mml:math id="M6" display="block" overflow="scroll"><mml:msub><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>3</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mrow><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:munderover><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>θ</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>θ</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
where Γ(.) is the gamma function. We train our model for the middle one-third region (2 Mb or middle <italic>N</italic>/3 bins) instead of the whole 6-Mb region (all <italic>N</italic> bins) in each batch. Because we include 3D interactions of genomic distance up to 2 Mb, promoters in the middle 2-Mb region can see the effects of distal enhancers in the full 6-Mb input region. In the test phase we also restrict to predictions on the middle 2-Mb regions. We shift input regions by 2 Mb across genome so there is no overlap between predictions in two different batches. Depending on the number of GAT layers used, we can see the effects of enhancers via multi-hop interactions up to 4 Mb away from the promoters. If we have <italic>L</italic> GAT layers, message passing is done <italic>L</italic> times over the graph, and the promoters will see the effects of enhancers up to <italic>L</italic> hops.</p>
      <p>For the Seq-GraphReg model, we also predict the 1D epigenomic data with a resolution of 100 bp after the dilated CNN layers from DNA sequence. Here, we let <italic>X</italic> denote the DNA sequence of the 6-Mb input region, <italic>G</italic> the corresponding graph, <inline-formula id="il26"><mml:math id="IL26" display="inline" overflow="scroll"><mml:msup><mml:mi>Y</mml:mi><mml:mi>e</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mn>1</mml:mn><mml:mi>e</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:mo>…</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mi mathvariant="normal">′</mml:mi></mml:msup></mml:mrow><mml:mi>e</mml:mi></mml:msubsup><mml:mo>]</mml:mo></mml:math></inline-formula> the observed 100-bp epigenomic inputs (over <italic>N</italic><sup>′</sup> = 60, 000 bins), and <inline-formula id="il27"><mml:math id="IL27" display="inline" overflow="scroll"><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> the predicted epigenomic signal <italic>e</italic> for the bin <italic>i</italic>. The loss for each epigenomic signal <italic>e</italic> is given by<disp-formula id="GR275870KARM7"><label>(7)</label><mml:math id="M7" display="block" overflow="scroll"><mml:msubsup><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>e</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munderover><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi></mml:msubsup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
and the CAGE loss <inline-formula id="il28"><mml:math id="IL28" display="inline" overflow="scroll"><mml:msubsup><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, similar to Equation (<xref rid="GR275870KARM6" ref-type="disp-formula">6</xref>), can be written as<disp-formula id="GR275870KARM8"><label>(8)</label><mml:math id="M8" display="block" overflow="scroll"><mml:msubsup><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>3</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mrow><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:munderover><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>θ</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>θ</mml:mi></mml:msubsup><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
where <inline-formula id="il29"><mml:math id="IL29" display="inline" overflow="scroll"><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> is the bottleneck representation just before dilated CNN layers (shown by a star <inline-formula id="il029"><mml:math id="IL30" display="inline" overflow="scroll"><mml:msup><mml:mo>[</mml:mo><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:math></inline-formula> in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S1</ext-link>) at 100-bp resolution. The final loss function to be minimized is the convex combination of the CAGE loss and the epigenomic losses,<disp-formula id="GR275870KARM9"><label>(9)</label><mml:math id="M9" display="block" overflow="scroll"><mml:msub><mml:mi>L</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:msubsup><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:munderover><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>3</mml:mn></mml:munderover><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>e</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:math></disp-formula>
where <italic>λ</italic> is a hyperparameter to control the weights of the CAGE loss versus the epigenomic losses. We use a sigmoid-like schedule for <italic>λ</italic> given by <inline-formula id="il30"><mml:math id="IL31" display="inline" overflow="scroll"><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.4em"/><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>−</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, which starts with a small value <italic>λ</italic> = 0.01 and approaches a value close to one at higher epochs. This schedule lets the model first learn meaningful motif representations from DNA sequences to predict the epigenomic data and then puts higher weights on predicting the CAGE signals from those motif representations at 100-bp resolution. <italic>NLL</italic> in all figures refers to <italic>L</italic><sub><italic>E</italic></sub> and <inline-formula id="il31"><mml:math id="IL32" display="inline" overflow="scroll"><mml:msubsup><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> for epigenome-based and sequence-based models, respectively.</p>
    </sec>
    <sec id="s3e">
      <title>End-to-end versus separate training of Seq-GraphReg</title>
      <p>Seq-GraphReg as shown in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figure S1</ext-link> is an end-to-end model with two main tasks: (1) epigenomic tracks are predicted from DNA sequences; and (2) CAGE is predicted from the bottleneck representation <inline-formula id="il0029"><mml:math id="IL33" display="inline" overflow="scroll"><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> learned in the first task. For end-to-end training, we need to load the entire 6-Mb DNA sequence to the GPU memory, and this can limit our capability to use more CNN filters in the first layers to learn a richer bottleneck representation. Therefore, the overall CAGE prediction might be sacrificed because of GPU memory limitations. To resolve this problem, we propose another scheme for training Seq-GraphReg, which we call separate training. The separate training works as follows. First, we perform task (1) to predict the epigenomic data from DNA sequences of length 100 kb. Then, we freeze the learned parameters of the CNN layers and get the bottleneck representations <inline-formula id="il00029"><mml:math id="IL34" display="inline" overflow="scroll"><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula>, which are at 100-bp resolution. Finally, we use <inline-formula id="il2900"><mml:math id="IL35" display="inline" overflow="scroll"><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> as the input to the second block (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S1</ext-link>) to predict the CAGE values. The second block has several CNN and max-pool layers to bring the resolution to 5 kb, and then it is given to three GAT layers to predict the CAGE values.</p>
      <p>We have shown that separate training on K562 outperforms end-to-end training in terms of CAGE prediction performance. The reason for the improvement is that in separate training, we use 100-kb DNA sequences (instead of 6 Mb), so we do not exhaust GPU memory and thus can use more CNN filters to learn richer bottleneck representations that consequently lead to superior gene expression prediction. However, this prediction improvement comes at the cost of losing base-pair level saliency scores via backpropagation. The reason for this is that to get the gradient of the output CAGE with respect to the input DNA sequence via backpropagation, we need an end-to-end model from input to the output. Nevertheless, we can use perturbation-based feature attribution methods such as in silico mutagenesis (ISM) for the separately trained Seq-GraphReg models (as was performed and is shown in <xref rid="GR275870KARF4" ref-type="fig">Fig. 4</xref>E). However, backpropagation-based feature attribution methods, such as gradient-by-input and DeepSHAP, are much faster than ISM. Overall, there is a trade-off between better prediction performance (achieved by separate training) and having fast backpropagation-based feature attribution capability (achieved by end-to-end training).</p>
    </sec>
    <sec id="s3f">
      <title>Dilated CNN layers and FFT loss in Seq-GraphReg</title>
      <p>In separate training of Seq-GraphReg, we also studied the effects of using dilated CNN layers and also adding FFT loss to get better motif interpretations. In task (1) of separate training, where we predict epigenomic data from DNA sequence, we removed the residual dilated CNN layers and instead used three CNN layers after the bottleneck <inline-formula id="il32"><mml:math id="IL36" display="inline" overflow="scroll"><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> for each task of predicting H3K4me3, H3K27ac, and DNase. We observed that not using dilated layers in fact helped prediction performance in K562 (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S11</ext-link>). This is partly because when we do not use dilated layers, the CNN filters can focus more on local peak regions and learn better motif representations.</p>
      <p>Because our goal is not only prediction but also interpretation of base-pair feature attribution methods, we used the idea of adding FFT loss to the prediction loss (<italic>NLL</italic>) at training, as suggested in <xref rid="GR275870KARC26" ref-type="bibr">Tseng et al. (2020)</xref>. Here, we explain how we used FFT loss in separate training of Seq-GraphReg models. In task (1), let <inline-formula id="il33"><mml:math id="IL37" display="inline" overflow="scroll"><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> be the gradient of bottleneck <inline-formula id="il34"><mml:math id="IL38" display="inline" overflow="scroll"><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> with respect to the input DNA sequence <italic>X</italic> and summed over 4 bases as our attribution vector. Thus, the length of the vector <inline-formula id="il35"><mml:math id="IL39" display="inline" overflow="scroll"><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> is the same as that of <italic>X</italic>. Let <italic>a</italic> denote the magnitudes of the positive-frequency Fourier components of (a slightly smoothed version of) <inline-formula id="il36"><mml:math id="IL40" display="inline" overflow="scroll"><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula>, and <inline-formula id="il37"><mml:math id="IL41" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> denote the <italic>i</italic>th component of <inline-formula id="il38"><mml:math id="IL42" display="inline" overflow="scroll"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mi>a</mml:mi><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> (smaller <italic>i</italic> corresponds to lower frequencies). Similar to <xref rid="GR275870KARC26" ref-type="bibr">Tseng et al. (2020)</xref>, we penalize the high-frequency components as<disp-formula id="GR275870KARM10"><label>(10)</label><mml:math id="M10" display="block" overflow="scroll"><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:mo movablelimits="false">∑</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:munder><mml:mo>⁡</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula>
where <italic>w</italic><sub><italic>i</italic></sub> = 1 for <italic>i</italic> ≤ <italic>T</italic>, and <inline-formula id="il39"><mml:math id="IL43" display="inline" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> for <italic>i</italic> &gt; <italic>T</italic>. The total loss to be trained is <inline-formula id="il40"><mml:math id="IL44" display="inline" overflow="scroll"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>3</mml:mn></mml:munderover><mml:msubsup><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>e</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, where <inline-formula id="il41"><mml:math id="IL45" display="inline" overflow="scroll"><mml:msubsup><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>e</mml:mi></mml:msubsup></mml:math></inline-formula> is the prediction loss (<italic>NLL</italic>) of the epigenomic data <italic>e</italic>, <inline-formula id="il42"><mml:math id="IL46" display="inline" overflow="scroll"><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:math></inline-formula> is attribution prior, and <italic>γ</italic> is a positive scalar to control the contribution of attribution loss. We have used <italic>s</italic> = 0.2 and <italic>γ</italic> = 1 as suggested by <xref rid="GR275870KARC26" ref-type="bibr">Tseng et al. (2020)</xref>. We have applied ISM to the Seq-GraphReg models trained separately without dilation and with FFT loss and observed that relevant TF motifs can be identified in distal enhancer regions (<xref rid="GR275870KARF4" ref-type="fig">Fig. 4</xref>E).</p>
    </sec>
    <sec id="s3g">
      <title>Model architecture and training</title>
      <p>For Epi-GraphReg and Seq-GraphReg, we used respectively two and three GAT layers with four heads in each layer (<ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Fig. S1</ext-link>). For Epi-CNN and Seq-CNN we used eight dilated CNN layers to increase their receptive field up to 2.5 Mb. For human cell types GM12878, K562, and hESC, we held out Chromosomes <italic>i</italic> and <italic>i</italic> + 10 for validation and Chromosomes <italic>i</italic> + 1 and <italic>i</italic> + 11 for test, for <italic>i</italic> = 1, …, 10, and trained on all remaining chromosomes except X and Y. For mouse cell type mESC, we held out Chromosomes <italic>i</italic> and (<italic>i</italic> + 10 mod 20) + sign(<italic>i</italic> ≥ 10) for validation and Chromosomes <italic>i</italic> + 1 and (<italic>i</italic> + 11 mod 20) + sign(<italic>i</italic> ≥ 9) for test, for <italic>i</italic> = 1, …, 10, and trained on all remaining chromosomes except X and Y.</p>
    </sec>
    <sec id="s3h">
      <title>TF knockout analysis and motif ablation experiments</title>
      <p>For RNA-seq data sets from CRISPRi TF KO experiments, we performed differential gene expression analyses using DESeq2 (<xref rid="GR275870KARC2" ref-type="bibr">Anders and Huber 2010</xref>) between control and TF KO K562 cells for each TF. We only considered TFs whose KO led to significant down-regulation (adjusted <italic>P</italic> &lt; 0.05) of at least 200 genes to have robust statistics. To assess the ability of Seq-GraphReg and Seq-CNN to predict the down-regulation of genes as direct effects of TF KO, we performed an in silico KO of each TF by zeroing out all the motifs for the TF in 6-Mb DNA sequences encompassing candidate target genes. Then we used the models to predict the change in gene expression caused as direct effects of each TF KO. Similar to the true KO experiments, we used two trained models each for Seq-GraphReg and Seq-CNN as two replicates to predict control and in silico KO expression levels. We then performed differential expression analysis using DESeq2 to get the predicted logFC (log-fold change) and adjusted <italic>P</italic>-values. We ranked the significantly regulated genes (adjusted <italic>P</italic> &lt; 0.05) with true CAGE values at least 20 based on predicted logFC and used the 100 genes with the largest negative predicted logFC as the predicted target genes for each model.</p>
    </sec>
    <sec id="s3i">
      <title>Seq-GraphReg predicts how distal TF motifs regulate their target genes</title>
      <p>We first looked at our gene expression predictions using Seq-GraphReg in GM12878, K562, and hESC, and we selected the top 100 accurately predicted genes using the following criteria: <italic>NLL</italic> &lt; 300, <italic>Delta</italic><sub><italic>NLL</italic></sub> &gt; 300, CAGE ≥ 100, and <italic>n</italic> ≥ <italic>n</italic><sub><italic>EP</italic></sub>, where <italic>n</italic> is the number of enhancer–promoter interactions. Because GM12878 and K562 have denser graphs, we used <italic>n</italic><sub><italic>EP</italic></sub> = 10 in GM12878 and K562. Because hESC has sparser graphs, we used <italic>n</italic><sub><italic>EP</italic></sub> = 5 in hESC. Then we performed feature attribution for all those genes using gradient-by-input, which gives us base-pair saliency scores for all the nucleotides in the enhancer regions of those genes. High saliency sequence regions correspond to our candidate motifs for each gene.</p>
      <p>To find the candidate motifs we screened the enhancer regions of each gene using a window of length 20 bp (because the usual length of TF motifs is in the range 7–20 bp) and summed the saliency scores in each window. We shifted this window by steps of 10 bp to not miss motifs that might appear on the borders of the window. This process produces 500 windows (scores) in each enhancer bin of length 5 kb. If a gene has <italic>n</italic> enhancer–promoter interactions, there will be 500<italic>n</italic> windows for that gene. For each gene, we kept the windows that have the highest 5% absolute scores as the motif candidates for that gene, giving 25<italic>n</italic> windows with the highest absolute scores. To have a control set, we also retained for each gene 25<italic>n</italic> windows with the lowest non-zero absolute scores. To study the effects of distal motifs on gene regulation, we kept 25<italic>n</italic> sequences of length 20 bp in both high and low saliency regions that have 1D distance of at least 20 kb from the TSS of their target genes.</p>
      <p>We used FIMO software (<xref rid="GR275870KARC10" ref-type="bibr">Grant et al. 2011</xref>) to scan these 20-bp sequences and find motif matches to the CIS-BP motif database. We kept all matches with <italic>P</italic>-values of at least 10<sup>−4</sup> (default FIMO parameters) for both high and low saliency regions. We also restricted to motifs whose TF is expressed in the cell type of interest, meaning that CAGE ≥ 50 for each retained TF. <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figure S33A</ext-link> shows the number of motifs in high and low saliency regions in GM12878. The listed motifs in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figure S33A</ext-link> are the ones that are significantly enriched in high saliency regions compared to low saliency ones, meaning that the adjusted <italic>P</italic>-values are less than 0.01. Here, we have used Fisher's exact test to derive the <italic>P</italic>-values and Benjamini–Hochberg (BH) to adjust for multiple hypothesis testing. The adjusted <italic>P</italic>-values for these motifs are shown in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figure S34A</ext-link>. We also counted the occurrences of each motif for each gene individually in both high and low saliency regions, as shown in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figure S33B</ext-link>. Then for each gene-motif pair, we performed Fisher's exact test to see if each motif is enriched in high saliency regions for the gene. The resulting <italic>P</italic>-values are shown in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figure S34B</ext-link>, where we see the association of TF motifs with their target genes. Results in <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figure S34B</ext-link> suggest that combinations of distal TF motifs play a role in regulating the expression of target genes. Another important observation from <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figure S34B</ext-link> is that some of the TF motifs are universal and play a role in almost all the genes, but some of the motifs are more gene-specific and act only on the selected genes.</p>
      <p><ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figures S35–S38</ext-link> show similar motif analysis results for K562 and hESC. Three-dimensional data used by Seq-GraphReg for GM12878 and K562 is HiChIP (FDR = 0.1) and for hESC is Micro-C (FDR = 0.1). <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Figures S36 and S38</ext-link> show the enrichment of distal TF motifs for the best predicted genes in K562 and hESC, respectively.</p>
    </sec>
    <sec id="s4">
      <title>Data sets</title>
      <p>We downloaded the epigenomic and CAGE data of GM12878, K562, hESC, and mESC cells from ENCODE (<uri xlink:href="https://www.encodeproject.org/">https://www.encodeproject.org/</uri>) portal with the following accession numbers: K562 CAGE: ENCFF623BZZ and ENCFF902UHF; K562 DNase-seq: ENCFF826DJP; K562 H3K4me3: ENCFF689TMV, ENCFF915MJO (RPGC), and ENCFF367FNL (RPGC); K562 H3K27ac: ENCFF010PHG, ENCFF384ZZM (RPGC), and ENCFF070PWH (RPGC); K562 JUND TF ChIP-seq: ENCFF709JGL; GM12878 CAGE: ENCFF915EIJ and ENCFF990KLZ; GM12878 DNase-seq: ENCFF775ZJX and ENCFF783ZLL; GM12878 H3K4me3: ENCFF818GNV, ENCFF342CXS (RPGC), and ENCFF818UQV (RPGC); GM12878 H3K27ac: ENCFF180LKW, ENCFF197QHX (RPGC), and ENCFF882PRP (RPGC); hESC (H1) CAGE: ENCFF525HJR and ENCFF521WGM; hESC (H1) DNase-seq: ENCFF131HMO and ENCFF546PJU (RPGC); hESC (H1) H3K4me3: ENCFF760NUN, ENCFF151CFM (RPGC), and ENCFF765FFD (RPGC); hESC (H1) H3K27ac: ENCFF919FBG, ENCFF860ABR (RPGC), and ENCFF693IFG (RPGC); mESC (ES-E14) DNase-seq: ENCFF754ILF and ENCFF785XJZ; mESC (ES-E14) H3K4me3: ENCFF240MDV; mESC (ES-E14) H3K27ac: ENCFF163HEV. We downloaded mESC CAGE data from the NCBI Gene Expression Omnibus (GEO; <uri xlink:href="https://www.ncbi.nlm.nih.gov/geo/">https://www.ncbi.nlm.nih.gov/geo/</uri>) with the accession numbers GSM3852792, GSM3852793, and GSM3852794. We got the H3K27ac HiChIP data of K562, GM12878, and mESC from GEO with the accession numbers GSE101498, GSE101498, and GSE113339, respectively. We obtained Hi-C data for K562 and GM12878 from GEO with accession number GSE63525. We obtained Micro-C data for hESC (H1) from the 4D Nucleome Data Portal (4DN; <uri xlink:href="https://data.4dnucleome.org">https://data.4dnucleome.org</uri>) with accession number of 4DNFI2TK7L2F. We obtained HiCAR data for hESC (H1) from GEO with accession number GSE162819. We got the CRISPRi-FlowFISH enhancer validation data set for K562 from <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Table 6a</ext-link> of <xref rid="GR275870KARC8" ref-type="bibr">Fulco et al. (2019)</xref>. We got the CRISPRi TAP-seq enhancer validation data set for K562 (Chromosomes 8 and 11) from <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Tables 2 and 3</ext-link> of <xref rid="GR275870KARC20" ref-type="bibr">Schraivogel et al. (2020)</xref>. We acquired ABC enhancer predictions of the K562 (at Chromosomes 8 and 11) from <uri xlink:href="https://osf.io/f2uvz/">https://osf.io/f2uvz/</uri>. We downloaded CRISPRi RNA-seq (TF knockout) experiments of 51 TFs (plus two controls) in K562 from ENCODE with the accession numbers ENCSR439NIR, ENCSR447WYJ, ENCSR490XKT, ENCSR895NMN, ENCSR747UPR, ENCSR045BVL, ENCSR658NCC, ENCSR685TOO, ENCSR157JAK, ENCSR290RLW, ENCSR849RWU, ENCSR177JPA, ENCSR434ZAM, ENCSR467RQJ, ENCSR179XMY, ENCSR231XYT, ENCSR569WHC, ENCSR612JVF, ENCSR844UQZ, ENCSR466OEJ, ENCSR156EPV, ENCSR991TCB, ENCSR559VHS, ENCSR109KMO, ENCSR539CHL, ENCSR153WDD, ENCSR046XJC, ENCSR692NVT, ENCSR932JIP, ENCSR315EHR, ENCSR612NHI, ENCSR143YTV, ENCSR997FLI, ENCSR588MYR, ENCSR366ONT, ENCSR708MPN, ENCSR747ZDR, ENCSR521VAG, ENCSR045LNG, ENCSR199FFQ, ENCSR208KNW, ENCSR004ZBD, ENCSR975MRK, ENCSR966USM, ENCSR496MYK, ENCSR866PAO, ENCSR205DTM, ENCSR212QDE, ENCSR349LHQ, ENCSR043UEE, ENCSR785ATE, ENCSR095PIC, ENCSR016WFQ.</p>
    </sec>
    <sec id="s3j">
      <title>Software availability</title>
      <p>The code to preprocess data, train models, and perform the analyses in the paper, as well as all the trained models, are available at GitHub (<uri xlink:href="https://github.com/karbalayghareh/GraphReg">https://github.com/karbalayghareh/GraphReg</uri>) and as <ext-link xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.275870.121/-/DC1" ext-link-type="uri">Supplemental Code</ext-link>.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="PMC_1" content-type="local-data">
      <caption>
        <title>Supplemental Material</title>
      </caption>
      <media mimetype="text" mime-subtype="html" xlink:href="supp_32_5_930__DC1.html"/>
      <media xlink:role="associated-file" mimetype="application" mime-subtype="pdf" xlink:href="supp_gr.275870.121_Supplemental_Material.pdf"/>
      <media xlink:role="associated-file" mimetype="application" mime-subtype="x-zip-compressed" xlink:href="supp_gr.275870.121_Supplemental_Code.zip"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>This work was supported by the National Institutes of Health/National Human Genome Research Institute (NIH/NHGRI) U01 awards HG009395 and HG012103 and National Institutes of Health/National Institute of Diabetes and Digestive and Kidney Diseases (NIH/NIDDK) U01 award DK128852.</p>
    <p><italic>Author contributions:</italic> A.K. developed the machine learning models, performed all computational experiments, and co-wrote the manuscript. M.S. performed analysis on HiChIP data sets. C.S.L. supervised the research and co-wrote the manuscript.</p>
  </ack>
  <fn-group>
    <fn fn-type="supplementary-material">
      <p>[Supplemental material is available for this article.]</p>
    </fn>
    <fn>
      <p>Article published online before print. Article, supplemental material, and publication date are at <ext-link xlink:href="https://www.genome.org/cgi/doi/10.1101/gr.275870.121" ext-link-type="uri">https://www.genome.org/cgi/doi/10.1101/gr.275870.121</ext-link>.</p>
    </fn>
    <fn>
      <p>Freely available online through the <italic>Genome Research</italic> Open Access option.</p>
    </fn>
  </fn-group>
  <sec id="s5">
    <title>Competing interest statement</title>
    <p>The authors declare no competing interests.</p>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="GR275870KARC1">
      <mixed-citation publication-type="journal"><string-name><surname>Agarwal</surname><given-names>V</given-names></string-name>, <string-name><surname>Shendure</surname><given-names>J</given-names></string-name>. <year>2020</year>. <article-title>Predicting mRNA abundance directly from genomic sequence using deep convolutional neural networks</article-title>. <source>Cell Rep</source><volume>31</volume>: <fpage>107663</fpage>. <pub-id pub-id-type="doi">10.1016/j.celrep.2020.107663</pub-id><pub-id pub-id-type="pmid">32433972</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC2">
      <mixed-citation publication-type="journal"><string-name><surname>Anders</surname><given-names>S</given-names></string-name>, <string-name><surname>Huber</surname><given-names>W</given-names></string-name>. <year>2010</year>. <article-title>Differential expression analysis for sequence count data</article-title>. <source>Genome Biol</source><volume>11</volume>: <fpage>R106</fpage>. <pub-id pub-id-type="doi">10.1186/gb-2010-11-10-r106</pub-id><pub-id pub-id-type="pmid">20979621</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC3">
      <mixed-citation publication-type="journal"><string-name><surname>Avsec</surname><given-names>Ž</given-names></string-name>, <string-name><surname>Agarwal</surname><given-names>V</given-names></string-name>, <string-name><surname>Visentin</surname><given-names>D</given-names></string-name>, <string-name><surname>Ledsam</surname><given-names>JR</given-names></string-name>, <string-name><surname>Grabska-Barwinska</surname><given-names>A</given-names></string-name>, <string-name><surname>Taylor</surname><given-names>KR</given-names></string-name>, <string-name><surname>Assael</surname><given-names>Y</given-names></string-name>, <string-name><surname>Jumper</surname><given-names>J</given-names></string-name>, <string-name><surname>Kohli</surname><given-names>P</given-names></string-name>, <string-name><surname>Kelley</surname><given-names>DR</given-names></string-name>. <year>2021a</year>. <article-title>Effective gene expression prediction from sequence by integrating long-range interactions</article-title>. <source>Nat Methods</source><volume>18</volume>: <fpage>1196</fpage>–<lpage>1203</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-021-01252-x</pub-id><pub-id pub-id-type="pmid">34608324</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC4">
      <mixed-citation publication-type="journal"><string-name><surname>Avsec</surname><given-names>Ž</given-names></string-name>, <string-name><surname>Weilert</surname><given-names>M</given-names></string-name>, <string-name><surname>Shrikumar</surname><given-names>A</given-names></string-name>, <string-name><surname>Krueger</surname><given-names>S</given-names></string-name>, <string-name><surname>Alexandari</surname><given-names>A</given-names></string-name>, <string-name><surname>Dalal</surname><given-names>K</given-names></string-name>, <string-name><surname>Fropf</surname><given-names>R</given-names></string-name>, <string-name><surname>McAnany</surname><given-names>C</given-names></string-name>, <string-name><surname>Gagneur</surname><given-names>J</given-names></string-name>, <string-name><surname>Kundaje</surname><given-names>A</given-names></string-name>, <etal/><year>2021b</year>. <article-title>Base-resolution models of transcription-factor binding reveal soft motif syntax</article-title>. <source>Nat Genet</source><volume>53</volume>: <fpage>354</fpage>–<lpage>366</lpage>. <pub-id pub-id-type="doi">10.1038/s41588-021-00782-6</pub-id><pub-id pub-id-type="pmid">33603233</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC5">
      <mixed-citation publication-type="journal"><string-name><surname>Bigness</surname><given-names>J</given-names></string-name>, <string-name><surname>Loinaz</surname><given-names>X</given-names></string-name>, <string-name><surname>Patel</surname><given-names>S</given-names></string-name>, <string-name><surname>Larschan</surname><given-names>E</given-names></string-name>, <string-name><surname>Singh</surname><given-names>R</given-names></string-name>. <year>2022</year>. <article-title>Integrating long-range regulatory interactions to predict gene expression using graph convolutional networks</article-title>. <source>J Comput Biol</source><pub-id pub-id-type="doi">10.1089/cmb.2021.0316</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC6">
      <mixed-citation publication-type="journal"><string-name><surname>Buenrostro</surname><given-names>JD</given-names></string-name>, <string-name><surname>Wu</surname><given-names>B</given-names></string-name>, <string-name><surname>Chang</surname><given-names>HY</given-names></string-name>, <string-name><surname>Greenleaf</surname><given-names>WJ</given-names></string-name>. <year>2015</year>. <article-title>ATAC-seq: a method for assaying chromatin accessibility genome-wide</article-title>. <source>Curr Protoc Mol Biol</source><volume>109</volume>: <fpage>21</fpage>–<lpage>29</lpage>. <pub-id pub-id-type="doi">10.1002/0471142727.mb2129s109</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC7">
      <mixed-citation publication-type="journal"><string-name><surname>Carty</surname><given-names>M</given-names></string-name>, <string-name><surname>Zamparo</surname><given-names>L</given-names></string-name>, <string-name><surname>Sahin</surname><given-names>M</given-names></string-name>, <string-name><surname>González</surname><given-names>A</given-names></string-name>, <string-name><surname>Pelossof</surname><given-names>R</given-names></string-name>, <string-name><surname>Elemento</surname><given-names>O</given-names></string-name>, <string-name><surname>Leslie</surname><given-names>CS</given-names></string-name>. <year>2017</year>. <article-title>An integrated model for detecting significant chromatin interactions from high-resolution Hi-C data</article-title>. <source>Nat Commun</source><volume>8</volume>: <fpage>15454</fpage>. <pub-id pub-id-type="doi">10.1038/ncomms15454</pub-id><pub-id pub-id-type="pmid">28513628</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC8">
      <mixed-citation publication-type="journal"><string-name><surname>Fulco</surname><given-names>CP</given-names></string-name>, <string-name><surname>Nasser</surname><given-names>J</given-names></string-name>, <string-name><surname>Jones</surname><given-names>TR</given-names></string-name>, <string-name><surname>Munson</surname><given-names>G</given-names></string-name>, <string-name><surname>Bergman</surname><given-names>DT</given-names></string-name>, <string-name><surname>Subramanian</surname><given-names>V</given-names></string-name>, <string-name><surname>Grossman</surname><given-names>SR</given-names></string-name>, <string-name><surname>Anyoha</surname><given-names>R</given-names></string-name>, <string-name><surname>Doughty</surname><given-names>BR</given-names></string-name>, <string-name><surname>Patwardhan</surname><given-names>TA</given-names></string-name>, <etal/><year>2019</year>. <article-title>Activity-by-contact model of enhancer–promoter regulation from thousands of CRISPR perturbations</article-title>. <source>Nat Genet</source><volume>51</volume>: <fpage>1664</fpage>–<lpage>1669</lpage>. <pub-id pub-id-type="doi">10.1038/s41588-019-0538-0</pub-id><pub-id pub-id-type="pmid">31784727</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC9">
      <mixed-citation publication-type="journal"><string-name><surname>González</surname><given-names>AJ</given-names></string-name>, <string-name><surname>Setty</surname><given-names>M</given-names></string-name>, <string-name><surname>Leslie</surname><given-names>CS</given-names></string-name>. <year>2015</year>. <article-title>Early enhancer establishment and regulatory locus complexity shape transcriptional programs in hematopoietic differentiation</article-title>. <source>Nat Genet</source><volume>47</volume>: <fpage>1249</fpage>–<lpage>1259</lpage>. <pub-id pub-id-type="doi">10.1038/ng.3402</pub-id><pub-id pub-id-type="pmid">26390058</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC10">
      <mixed-citation publication-type="journal"><string-name><surname>Grant</surname><given-names>CE</given-names></string-name>, <string-name><surname>Bailey</surname><given-names>TL</given-names></string-name>, <string-name><surname>Noble</surname><given-names>WS</given-names></string-name>. <year>2011</year>. <article-title>FIMO: scanning for occurrences of a given motif</article-title>. <source>Bioinformatics</source><volume>27</volume>: <fpage>1017</fpage>–<lpage>1018</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btr064</pub-id><pub-id pub-id-type="pmid">21330290</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC11">
      <mixed-citation publication-type="journal"><string-name><surname>Kelley</surname><given-names>DR</given-names></string-name>, <string-name><surname>Reshef</surname><given-names>YA</given-names></string-name>, <string-name><surname>Bileschi</surname><given-names>M</given-names></string-name>, <string-name><surname>Belanger</surname><given-names>D</given-names></string-name>, <string-name><surname>McLean</surname><given-names>CY</given-names></string-name>, <string-name><surname>Snoek</surname><given-names>J</given-names></string-name>. <year>2018</year>. <article-title>Sequential regulatory activity prediction across chromosomes with convolutional neural networks</article-title>. <source>Genome Res</source><volume>28</volume>: <fpage>739</fpage>–<lpage>750</lpage>. <pub-id pub-id-type="doi">10.1101/gr.227819.117</pub-id><pub-id pub-id-type="pmid">29588361</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC12">
      <mixed-citation publication-type="confproc"><string-name><surname>Kipf</surname><given-names>TN</given-names></string-name>, <string-name><surname>Welling</surname><given-names>M</given-names></string-name>. <year>2017</year>. <article-title>Semi</article-title><article-title>-supervised classification with graph convolutional networks</article-title>. In <conf-name>5th International Conference on Learning Representations, ICLR 2017</conf-name>, Toulon, France.</mixed-citation>
    </ref>
    <ref id="GR275870KARC13">
      <mixed-citation publication-type="journal"><string-name><surname>Krietenstein</surname><given-names>N</given-names></string-name>, <string-name><surname>Abraham</surname><given-names>S</given-names></string-name>, <string-name><surname>Venev</surname><given-names>SV</given-names></string-name>, <string-name><surname>Abdennur</surname><given-names>N</given-names></string-name>, <string-name><surname>Gibcus</surname><given-names>J</given-names></string-name>, <string-name><surname>Hsieh</surname><given-names>THS</given-names></string-name>, <string-name><surname>Parsi</surname><given-names>KM</given-names></string-name>, <string-name><surname>Yang</surname><given-names>L</given-names></string-name>, <string-name><surname>Maehr</surname><given-names>R</given-names></string-name>, <string-name><surname>Mirny</surname><given-names>LA</given-names></string-name>, <etal/><year>2020</year>. <article-title>Ultrastructural details of mammalian chromosome architecture</article-title>. <source>Mol Cell</source><volume>78</volume>: <fpage>554</fpage>–<lpage>565.e7</lpage>. <pub-id pub-id-type="doi">10.1016/j.molcel.2020.03.003</pub-id><pub-id pub-id-type="pmid">32213324</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC14">
      <mixed-citation publication-type="journal"><string-name><surname>Lieberman-Aiden</surname><given-names>E</given-names></string-name>, <string-name><surname>Van Berkum</surname><given-names>NL</given-names></string-name>, <string-name><surname>Williams</surname><given-names>L</given-names></string-name>, <string-name><surname>Imakaev</surname><given-names>M</given-names></string-name>, <string-name><surname>Ragoczy</surname><given-names>T</given-names></string-name>, <string-name><surname>Telling</surname><given-names>A</given-names></string-name>, <string-name><surname>Amit</surname><given-names>I</given-names></string-name>, <string-name><surname>Lajoie</surname><given-names>BR</given-names></string-name>, <string-name><surname>Sabo</surname><given-names>PJ</given-names></string-name>, <string-name><surname>Dorschner</surname><given-names>MO</given-names></string-name>, <etal/><year>2009</year>. <article-title>Comprehensive mapping of long-range interactions reveals folding principles of the human genome</article-title>. <source>Science</source><volume>326</volume>: <fpage>289</fpage>–<lpage>293</lpage>. <pub-id pub-id-type="doi">10.1126/science.1181369</pub-id><pub-id pub-id-type="pmid">19815776</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC15">
      <mixed-citation publication-type="confproc"><string-name><surname>Lundberg</surname><given-names>SM</given-names></string-name>, <string-name><surname>Lee</surname><given-names>SI</given-names></string-name>. <year>2017</year>. <article-title>A</article-title><article-title>unified approach to interpreting model predictions</article-title>. In <conf-name>Advances in Neural Information Processing Systems 30 (NIPS 2017)</conf-name>, Long Beach, CA, pp. <fpage>4765</fpage>–<lpage>4774</lpage>.</mixed-citation>
    </ref>
    <ref id="GR275870KARC16">
      <mixed-citation publication-type="journal"><string-name><surname>Mumbach</surname><given-names>MR</given-names></string-name>, <string-name><surname>Rubin</surname><given-names>AJ</given-names></string-name>, <string-name><surname>Flynn</surname><given-names>RA</given-names></string-name>, <string-name><surname>Dai</surname><given-names>C</given-names></string-name>, <string-name><surname>Khavari</surname><given-names>PA</given-names></string-name>, <string-name><surname>Greenleaf</surname><given-names>WJ</given-names></string-name>, <string-name><surname>Chang</surname><given-names>HY</given-names></string-name>. <year>2016</year>. <article-title>HiChIP: efficient and sensitive analysis of protein-directed genome architecture</article-title>. <source>Nat Methods</source><volume>13</volume>: <fpage>919</fpage>–<lpage>922</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.3999</pub-id><pub-id pub-id-type="pmid">27643841</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC17">
      <mixed-citation publication-type="journal"><string-name><surname>Osmanbeyoglu</surname><given-names>HU</given-names></string-name>, <string-name><surname>Shimizu</surname><given-names>F</given-names></string-name>, <string-name><surname>Rynne-Vidal</surname><given-names>A</given-names></string-name>, <string-name><surname>Alonso-Curbelo</surname><given-names>D</given-names></string-name>, <string-name><surname>Chen</surname><given-names>HA</given-names></string-name>, <string-name><surname>Wen</surname><given-names>HY</given-names></string-name>, <string-name><surname>Yeung</surname><given-names>TL</given-names></string-name>, <string-name><surname>Jelinic</surname><given-names>P</given-names></string-name>, <string-name><surname>Razavi</surname><given-names>P</given-names></string-name>, <string-name><surname>Lowe</surname><given-names>SW</given-names></string-name>, <etal/><year>2019</year>. <article-title>Chromatin-informed inference of transcriptional programs in gynecologic and basal breast cancers</article-title>. <source>Nat Commun</source><volume>10</volume>: <fpage>4369</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-12291-6</pub-id><pub-id pub-id-type="pmid">31554806</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC18">
      <mixed-citation publication-type="journal"><string-name><surname>Ramírez</surname><given-names>F</given-names></string-name>, <string-name><surname>Ryan</surname><given-names>DP</given-names></string-name>, <string-name><surname>Grüning</surname><given-names>B</given-names></string-name>, <string-name><surname>Bhardwaj</surname><given-names>V</given-names></string-name>, <string-name><surname>Kilpert</surname><given-names>F</given-names></string-name>, <string-name><surname>Richter</surname><given-names>AS</given-names></string-name>, <string-name><surname>Heyne</surname><given-names>S</given-names></string-name>, <string-name><surname>Dündar</surname><given-names>F</given-names></string-name>, <string-name><surname>Manke</surname><given-names>T</given-names></string-name>. <year>2016</year>. <article-title>deepTools2: a next generation web server for deep-sequencing data analysis</article-title>. <source>Nucleic Acids Res</source><volume>44</volume>: <fpage>W160</fpage>–<lpage>W165</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gkw257</pub-id><pub-id pub-id-type="pmid">27079975</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC19">
      <mixed-citation publication-type="journal"><string-name><surname>Sahin</surname><given-names>M</given-names></string-name>, <string-name><surname>Wong</surname><given-names>W</given-names></string-name>, <string-name><surname>Zhan</surname><given-names>Y</given-names></string-name>, <string-name><surname>Van Deynze</surname><given-names>K</given-names></string-name>, <string-name><surname>Koche</surname><given-names>R</given-names></string-name>, <string-name><surname>Leslie</surname><given-names>CS</given-names></string-name>. <year>2021</year>. <article-title>HiC-DC+ enables systematic 3D interaction calls and differential analysis for Hi-C and HiChIP</article-title>. <source>Nat Commun</source><volume>12</volume>: <fpage>3366</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-021-23749-x</pub-id><pub-id pub-id-type="pmid">34099725</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC20">
      <mixed-citation publication-type="journal"><string-name><surname>Schraivogel</surname><given-names>D</given-names></string-name>, <string-name><surname>Gschwind</surname><given-names>AR</given-names></string-name>, <string-name><surname>Milbank</surname><given-names>JH</given-names></string-name>, <string-name><surname>Leonce</surname><given-names>DR</given-names></string-name>, <string-name><surname>Jakob</surname><given-names>P</given-names></string-name>, <string-name><surname>Mathur</surname><given-names>L</given-names></string-name>, <string-name><surname>Korbel</surname><given-names>JO</given-names></string-name>, <string-name><surname>Merten</surname><given-names>CA</given-names></string-name>, <string-name><surname>Velten</surname><given-names>L</given-names></string-name>, <string-name><surname>Steinmetz</surname><given-names>LM</given-names></string-name>. <year>2020</year>. <article-title>Targeted perturb-seq enables genome-scale genetic screens in single cells</article-title>. <source>Nat Methods</source><volume>17</volume>: <fpage>629</fpage>–<lpage>635</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-020-0837-5</pub-id><pub-id pub-id-type="pmid">32483332</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC21">
      <mixed-citation publication-type="journal"><string-name><surname>Shiraki</surname><given-names>T</given-names></string-name>, <string-name><surname>Kondo</surname><given-names>S</given-names></string-name>, <string-name><surname>Katayama</surname><given-names>S</given-names></string-name>, <string-name><surname>Waki</surname><given-names>K</given-names></string-name>, <string-name><surname>Kasukawa</surname><given-names>T</given-names></string-name>, <string-name><surname>Kawaji</surname><given-names>H</given-names></string-name>, <string-name><surname>Kodzius</surname><given-names>R</given-names></string-name>, <string-name><surname>Watahiki</surname><given-names>A</given-names></string-name>, <string-name><surname>Nakamura</surname><given-names>M</given-names></string-name>, <string-name><surname>Arakawa</surname><given-names>T</given-names></string-name>, <etal/><year>2003</year>. <article-title>Cap analysis gene expression for high-throughput analysis of transcriptional starting point and identification of promoter usage</article-title>. <source>Proc Natl Acad Sci</source><volume>100</volume>: <fpage>15776</fpage>–<lpage>15781</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.2136655100</pub-id><pub-id pub-id-type="pmid">14663149</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC22">
      <mixed-citation publication-type="confproc"><string-name><surname>Shrikumar</surname><given-names>A</given-names></string-name>, <string-name><surname>Greenside</surname><given-names>P</given-names></string-name>, <string-name><surname>Kundaje</surname><given-names>A</given-names></string-name>. <year>2017</year>. <article-title>Learning important features through propagating activation differences</article-title>. In <conf-name>Proceedings of the 34th International Conference on Machine Learning</conf-name>, <conf-loc>Sydney, Australia</conf-loc>. <italic>PMLR</italic><volume>70</volume>: <fpage>3145</fpage>–<lpage>3153</lpage>. <uri xlink:href="https://proceedings.mlr.press/v70/shrikumar17a.html">https://proceedings.mlr.press/v70/shrikumar17a.html</uri></mixed-citation>
    </ref>
    <ref id="GR275870KARC23">
      <mixed-citation publication-type="journal"><string-name><surname>Singh</surname><given-names>R</given-names></string-name>, <string-name><surname>Lanchantin</surname><given-names>J</given-names></string-name>, <string-name><surname>Robins</surname><given-names>G</given-names></string-name>, <string-name><surname>Qi</surname><given-names>Y</given-names></string-name>. <year>2016</year>. <article-title>DeepChrome: deep-learning for predicting gene expression from histone modifications</article-title>. <source>Bioinformatics</source><volume>32</volume>: <fpage>i639</fpage>–<lpage>i648</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btw427</pub-id><pub-id pub-id-type="pmid">27587684</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC24">
      <mixed-citation publication-type="journal"><string-name><surname>Skene</surname><given-names>PJ</given-names></string-name>, <string-name><surname>Henikoff</surname><given-names>S</given-names></string-name>. <year>2017</year>. <article-title>An efficient targeted nuclease strategy for high-resolution mapping of DNA binding sites</article-title>. <source>eLife</source><volume>6</volume>: <fpage>e21856</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.21856</pub-id><pub-id pub-id-type="pmid">28079019</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC25">
      <mixed-citation publication-type="confproc"><string-name><surname>Sundararajan</surname><given-names>M</given-names></string-name>, <string-name><surname>Taly</surname><given-names>A</given-names></string-name>, <string-name><surname>Yan</surname><given-names>Q</given-names></string-name>. <year>2017</year>. <article-title>Axiomatic attribution for deep networks</article-title>. In <conf-name>Proceedings of the 34th International Conference on Machine Learning</conf-name>, <conf-loc>Sydney, Australia</conf-loc>. <italic>PMLR</italic><volume>70</volume>: <fpage>3319</fpage>–<lpage>3328</lpage>. <uri xlink:href="https://proceedings.mlr.press/v70/sundararajan17a.html">https://proceedings.mlr.press/v70/sundararajan17a.html</uri></mixed-citation>
    </ref>
    <ref id="GR275870KARC26">
      <mixed-citation publication-type="confproc"><string-name><surname>Tseng</surname><given-names>A</given-names></string-name>, <string-name><surname>Shrikumar</surname><given-names>A</given-names></string-name>, <string-name><surname>Kundaje</surname><given-names>A</given-names></string-name>. <year>2020</year>. <article-title>Fourier-transform-based attribution priors improve the interpretability and stability of deep learning models for genomics</article-title>. In <conf-name>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</conf-name>, pp. <fpage>1913</fpage>–<lpage>1923</lpage>.</mixed-citation>
    </ref>
    <ref id="GR275870KARC27">
      <mixed-citation publication-type="confproc"><string-name><surname>Vaswani</surname><given-names>A</given-names></string-name>, <string-name><surname>Shazeer</surname><given-names>N</given-names></string-name>, <string-name><surname>Parmar</surname><given-names>N</given-names></string-name>, <string-name><surname>Uszkoreit</surname><given-names>J</given-names></string-name>, <string-name><surname>Jones</surname><given-names>L</given-names></string-name>, <string-name><surname>Gomez</surname><given-names>AN</given-names></string-name>, <string-name><surname>Kaiser</surname><given-names>L</given-names></string-name>, <string-name><surname>Polosukhin</surname><given-names>I</given-names></string-name>. <year>2017</year>. <article-title>Attention is all you need</article-title>. In <conf-name>Advances in Neural Information Processing Systems 30 (NIPS 2017)</conf-name>, <conf-loc>Long Beach, CA</conf-loc>, pp. <fpage>5998</fpage>–<lpage>6008</lpage>.</mixed-citation>
    </ref>
    <ref id="GR275870KARC28">
      <mixed-citation publication-type="confproc"><string-name><surname>Veličković</surname><given-names>P</given-names></string-name>, <string-name><surname>Cucurull</surname><given-names>G</given-names></string-name>, <string-name><surname>Casanova</surname><given-names>A</given-names></string-name>, <string-name><surname>Romero</surname><given-names>A</given-names></string-name>, <string-name><surname>Lio</surname><given-names>P</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y</given-names></string-name>. <year>2018</year>. <article-title>Graph attention networks</article-title>. In <conf-name>6th International Conference on Learning Representations, ICLR 2018</conf-name>, <conf-loc>Vancouver, BC</conf-loc>.</mixed-citation>
    </ref>
    <ref id="GR275870KARC29">
      <mixed-citation publication-type="journal"><string-name><surname>Wei</surname><given-names>X</given-names></string-name>, <string-name><surname>Xiang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Peters</surname><given-names>DT</given-names></string-name>, <string-name><surname>Marius</surname><given-names>C</given-names></string-name>, <string-name><surname>Sun</surname><given-names>T</given-names></string-name>, <string-name><surname>Shan</surname><given-names>R</given-names></string-name>, <string-name><surname>Ou</surname><given-names>J</given-names></string-name>, <string-name><surname>Lin</surname><given-names>X</given-names></string-name>, <string-name><surname>Yue</surname><given-names>F</given-names></string-name>, <string-name><surname>Li</surname><given-names>W</given-names></string-name>, <etal/><year>2022</year>. <article-title>HiCAR is a robust and sensitive method to analyze open-chromatin-associated genome organization</article-title>. <source>Mol Cell</source><volume>82</volume>: <fpage>1225</fpage>–<lpage>1238.e6</lpage>. <pub-id pub-id-type="doi">10.1016/j.molcel.2022.01.023</pub-id><pub-id pub-id-type="pmid">35196517</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC30">
      <mixed-citation publication-type="confproc"><string-name><surname>Xu</surname><given-names>K</given-names></string-name>, <string-name><surname>Hu</surname><given-names>W</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J</given-names></string-name>, <string-name><surname>Jegelka</surname><given-names>S</given-names></string-name>. <year>2019</year>. <article-title>How powerful are graph neural networks?</article-title> In <conf-name>International Conference on Learning Representations, ICLR 2019</conf-name>, <conf-loc>New Orleans, LA</conf-loc>.</mixed-citation>
    </ref>
    <ref id="GR275870KARC31">
      <mixed-citation publication-type="journal"><string-name><surname>Zeng</surname><given-names>W</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Jiang</surname><given-names>R</given-names></string-name>. <year>2019</year>. <article-title>Integrating distal and proximal information to predict gene expression via a densely connected convolutional neural network</article-title>. <source>Bioinformatics</source><volume>36</volume>: <fpage>496</fpage>–<lpage>503</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btz562</pub-id></mixed-citation>
    </ref>
    <ref id="GR275870KARC32">
      <mixed-citation publication-type="journal"><string-name><surname>Zhou</surname><given-names>J</given-names></string-name>, <string-name><surname>Theesfeld</surname><given-names>CL</given-names></string-name>, <string-name><surname>Yao</surname><given-names>K</given-names></string-name>, <string-name><surname>Chen</surname><given-names>KM</given-names></string-name>, <string-name><surname>Wong</surname><given-names>AK</given-names></string-name>, <string-name><surname>Troyanskaya</surname><given-names>OG</given-names></string-name>. <year>2018</year>. <article-title>Deep learning sequence-based ab initio prediction of variant effects on expression and disease risk</article-title>. <source>Nat Genet</source><volume>50</volume>: <fpage>1171</fpage>–<lpage>1179</lpage>. <pub-id pub-id-type="doi">10.1038/s41588-018-0160-6</pub-id><pub-id pub-id-type="pmid">30013180</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
