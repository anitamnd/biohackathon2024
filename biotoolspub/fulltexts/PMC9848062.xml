<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9848062</article-id>
    <article-id pub-id-type="pmid">36416124</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac731</article-id>
    <article-id pub-id-type="publisher-id">btac731</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Systems Biology</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Perceiver CPI: a nested cross-attention network for compound–protein interaction prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7448-535X</contrib-id>
        <name>
          <surname>Nguyen</surname>
          <given-names>Ngoc-Quang</given-names>
        </name>
        <aff><institution>Department of Computer Science and Engineering, Korea University</institution>, Seoul 02841, <country country="KR">Republic of Korea</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jang</surname>
          <given-names>Gwanghoon</given-names>
        </name>
        <aff><institution>Department of Computer Science and Engineering, Korea University</institution>, Seoul 02841, <country country="KR">Republic of Korea</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kim</surname>
          <given-names>Hajung</given-names>
        </name>
        <aff><institution>Interdisciplinary Graduate Program in Bioinformatics, Korea University</institution>, Seoul 02841, <country country="KR">Republic of Korea</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6798-9106</contrib-id>
        <name>
          <surname>Kang</surname>
          <given-names>Jaewoo</given-names>
        </name>
        <aff><institution>Department of Computer Science and Engineering, Korea University</institution>, Seoul 02841, <country country="KR">Republic of Korea</country></aff>
        <aff><institution>Interdisciplinary Graduate Program in Bioinformatics, Korea University</institution>, Seoul 02841, <country country="KR">Republic of Korea</country></aff>
        <aff><institution>AIGEN Sciences</institution>, Seoul 04778, <country country="KR">Republic of Korea</country></aff>
        <xref rid="btac731-cor1" ref-type="corresp"/>
        <!--kangj@korea.ac.kr-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Borgwardt</surname>
          <given-names>Karsten</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btac731-cor1">To whom correspondence should be addressed. <email>kangj@korea.ac.kr</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-11-23">
      <day>23</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>23</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <volume>39</volume>
    <issue>1</issue>
    <elocation-id>btac731</elocation-id>
    <history>
      <date date-type="received">
        <day>31</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>18</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>04</day>
        <month>11</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>29</day>
        <month>12</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac731.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Compound–protein interaction (CPI) plays an essential role in drug discovery and is performed via expensive molecular docking simulations. Many artificial intelligence-based approaches have been proposed in this regard. Recently, two types of models have accomplished promising results in exploiting molecular information: graph convolutional neural networks that construct a learned molecular representation from a graph structure (atoms and bonds), and neural networks that can be applied to compute on descriptors or fingerprints of molecules. However, the superiority of one method over the other is yet to be determined. Modern studies have endeavored to aggregate information that is extracted from compounds and proteins to form the CPI task. Nonetheless, these approaches have used a simple concatenation to combine them, which cannot fully capture the interaction between such information.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We propose the Perceiver CPI network, which adopts a cross-attention mechanism to improve the learning ability of the representation of drug and target interactions and exploits the rich information obtained from extended-connectivity fingerprints to improve the performance. We evaluated Perceiver CPI on three main datasets, Davis, KIBA and Metz, to compare the performance of our proposed model with that of state-of-the-art methods. The proposed method achieved satisfactory performance and exhibited significant improvements over previous approaches in all experiments.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Perceiver CPI is available at <ext-link xlink:href="https://github.com/dmis-lab/PerceiverCPI" ext-link-type="uri">https://github.com/dmis-lab/PerceiverCPI</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Research Foundation of Korea</institution>
            <institution-id institution-id-type="DOI">10.13039/501100003725</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NRF-2020R1A2C3010638</award-id>
        <award-id>NRF-2014M3C9A3063541</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Ministry of Health &amp; Welfare, Republic of Korea</institution>
          </institution-wrap>
        </funding-source>
        <award-id>HR20C0021</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Drug development is a high-cost low-efficient process. New drug approval typically requires 10–15 years and costs 2.8 billion dollars on an average (<xref rid="btac731-B39" ref-type="bibr">Wouters <italic toggle="yes">et al.</italic>, 2020</xref>). Various approaches based on artificial intelligence have been introduced to alleviate this problem. In recent years, traditional machine learning (ML) algorithms have been deployed to model the prediction of the interaction between compounds and proteins as a binary classification problem (<xref rid="btac731-B2" ref-type="bibr">Bahi and Batouche, 2021</xref>). However, binding affinity, which indicates the interaction strength of the drug–target pair, is a continuum value; hence, considering compound–protein interaction (CPI) as a regression problem is both effective and sufficient.</p>
    <p>Through the binding mechanism, drugs can have a positive or negative influence on the functions carried out by proteins, which affect the targeted disease conditions (<xref rid="btac731-B41" ref-type="bibr">You <italic toggle="yes">et al.</italic>, 2018</xref>). Understanding drug–target binding affinity makes it possible to identify candidate drugs that can inhibit or stimulate a given protein. Researchers have attempted to exploit meaningful information from given proteins and compounds. Notably, in terms of protein information extraction, most previous approaches consider the protein sequence as a plain text and then use a 1D convolutional neural network (1DCNN) with different methods of protein sequence numbering. Nevertheless, two types of models have shown excellent performance in terms of obtaining information from chemical compounds: deep neural networks (DNNs) such as a multiple layer perceptron (MLP) neural network, and 1DCNN performing on descriptors or fingerprints, and graph neural networks (GNNs) and their variants for extracting knowledge from a graph-structured dataset (<xref rid="btac731-B40" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2019</xref>).</p>
    <p>With respect to the first approach for molecular descriptors, <xref rid="btac731-B30" ref-type="bibr">Öztürk <italic toggle="yes">et al.</italic> (2018)</xref> proposed DeepDTA, which adopts two 1DCNNs to perform on raw sequences and the simplified molecular-input line-entry system (SMILES) (<xref rid="btac731-B38" ref-type="bibr">Weininger, 1988</xref>) as one-hot vectors. Using 1DCNN, the authors aimed to extract local residues and atomic features to predict binding affinity. DeepConv-DTI (<xref rid="btac731-B21" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2019</xref>) followed a similar idea of DeepDTA by introducing a deep learning (DL) model to predict CPIs using raw protein sequences with Morgan/circular fingerprints (<xref rid="btac731-B27" ref-type="bibr">Morgan, 1965</xref>) as a compound representation. They used a 1DCNN on entire sequences of proteins to capture local residue patterns, while applying MLP neural network on molecular fingerprints to get drug features. Subsequently, Lee <italic toggle="yes">et al.</italic> concatenated aforementioned features, then transmitted them to a fully connected layer and predicted the property.</p>
    <p>Regarding the second method, GNNs that follow a neighborhood aggregation scheme have become increasingly popular for graph-structured data (<xref rid="btac731-B33" ref-type="bibr">Scarselli <italic toggle="yes">et al.</italic>, 2009</xref>). Numerous variants of GNN models have been proposed to achieve state-of-the-art (SOTA) performances in graph-based tasks in various fields of deep learning. Aware of the strength of GNNs, <xref rid="btac731-B29" ref-type="bibr">Nguyen <italic toggle="yes">et al.</italic> (2021)</xref> converted a compound representation into a graph represented by nodes (atoms) and edges (bonds); they, then, used four types of GNNs, graph convolutional networks (GCNs), graph attention networks (GATs), graph isomorphism networks (GINs) and a combination of GCNs and GATs, to capture molecular information. The knowledge extracted from atoms and bonds was then concatenated with the output of three 1DCNNs, which were used to learn different levels of abstract features from raw protein sequences.</p>
    <p>Transformers (<xref rid="btac731-B36" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic>, 2017</xref>) have shown a good performance in many AI fields, such as computer vision and natural language processing. Inspired by their potential to capture features between two sequences, <xref rid="btac731-B6" ref-type="bibr">Chen <italic toggle="yes">et al.</italic> (2020)</xref> proposed TransformerCPI, which is based on the architecture of an autoregressive encoder–decoder, using a combination of multiheaded attention and positional feed-forward to perform the CPI task. In their approach, molecular graphs were propagated to a <italic toggle="yes">GCN</italic> to obtain atomic features. Meanwhile, protein sequences were converted into sequential representations by separating a protein sequence into an overlapping 3-g amino acid sequence. Then, all words were translated into real-valued embeddings using the pretraining approach. The output was processed through 1DCNNs to obtain the final representation of the protein. Subsequently, these two representations were combined using a modified self-attention mechanism followed by MLPs. Motivated by the effectiveness of the self-attention mechanism, HyperattentionDTI was created (<xref rid="btac731-B43" ref-type="bibr">Zhao <italic toggle="yes">et al.</italic>, 2022</xref>). The model was designed to input both compounds and proteins as plain sequences to two stacked 1DCNNs. In contrast to previous attention-based models, HyperattentionDTI inferred an attention vector by using a Sigmoid activation function rather than using a Softmax activation function.</p>
    <p>The drawbacks of the existing approaches can be summarized as follows:
</p>
    <list list-type="order">
      <list-item>
        <p>Because molecular descriptor vectors or fingerprints [such as extended-connectivity fingerprint (ECFP)] contain useful chemical knowledge from the start, the use of molecular fingerprints and molecular descriptors might lead to a better performance than using complex graphs on small datasets. However, owing to the representation’s simplification, models deploying them may underfit larger datasets.</p>
      </list-item>
      <list-item>
        <p>On the other hand, GNNs must always learn a meaningful chemical space embedding from scratch. In addition, because of the global pooling step, which is simply chosen as the sum or average of all atomic features, over-smoothing and information loss are also crucial issues for GNNs.</p>
      </list-item>
      <list-item>
        <p>Integration of the compound network’s and protein network’s representation is often performed by a simple concatenation, which is practically unsuitable for revealing the relationship between these molecules in practice.</p>
      </list-item>
      <list-item>
        <p>Obtaining informative messages from protein sequences is a focus of research not only in CPI tasks but also in the general bioinformatics. Most current approaches consider protein sequences as plain texts, which cannot sufficiently reveal the real 3D structures of proteins.</p>
      </list-item>
    </list>
    <p>In this study, we developed Perceiver CPI, a deep-learning model that addresses three of the abovementioned challenges (1, 2 and 3). Our approach is mainly inspired by that of Perceiver IO (<xref rid="btac731-B16" ref-type="bibr">Jaegle <italic toggle="yes">et al.</italic>, 2021b</xref>) and a directed message-passing neural network (D-MPNN) (<xref rid="btac731-B40" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2019</xref>). The contributions of this study are summarized as follows:
</p>
    <list list-type="bullet">
      <list-item>
        <p>To avoid over-smoothing and information loss problems, we propose a novel method to enrich the representation of compounds by combining the information from both ECFPs and graph information.</p>
      </list-item>
      <list-item>
        <p>To the best of our knowledge, Perceiver CPI is the first approach to use nested cross-attention for capturing the relations between protein and molecule representations.</p>
      </list-item>
      <list-item>
        <p>Experimental results show that Perceiver CPI can achieve SOTA performance in novel pair and novel compound settings, and is competitive or slightly better than the baseline models in a novel protein setting.</p>
      </list-item>
    </list>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Feature encoding</title>
      <sec>
        <title>2.1.1 Compound information encoding</title>
        <p>Unlike previous approaches, which have commonly used either ECFP or molecular graph information constructed from SMILES, our proposed approach adopts ECFP to enrich the information extracted from the compound using D-MPNN (<xref rid="btac731-B40" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2019</xref>). More specifically, we represent a molecule <italic toggle="yes">s</italic> using two forms:
</p>
        <list list-type="bullet">
          <list-item>
            <p>A Morgan/circular fingerprint vector <italic toggle="yes">M<sub>s</sub></italic> as a binary vector, which indicates the existence of specific substructures. The Morgan algorithm searches each atom of the molecule and obtains all possible paths through the atom with a specific radius. Then, each unique path is hashed into a number based on a maximum of bit number.</p>
          </list-item>
          <list-item>
            <p>A molecular graph <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, which represents the interactions between a set of atoms <italic toggle="yes">A</italic> by a set of bonds <italic toggle="yes">B</italic>.</p>
          </list-item>
        </list>
        <p>We then process <italic toggle="yes">M<sub>s</sub></italic> and <italic toggle="yes">G<sub>s</sub></italic> through a MLP neural network and D-MPNN, respectively. Owing to its ability to approximate any continuous mapping, the MLP neural network is used to capture complex non-linear relationship features from <italic toggle="yes">M<sub>s</sub></italic> to yield <italic toggle="yes">O<sub>Ms</sub></italic> as the output. D-MPNN operates on hidden states <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>w</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and messages <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>w</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> associated with directed edges (bonds) instead of messages associated with vertices (atoms). Each bond in the graph has a hidden state (i.e. feature vector) that contains atomic features (atomic number, number of bonds for each atom, formal charge, chirality, number of bonded hydrogens, hybridization, aromaticity and atomic mass) and bond features [bond type (single/double/triple/aromatic), conjugation, ring membership and stereochemistry] (<xref rid="btac731-B34" ref-type="bibr">Stokes <italic toggle="yes">et al.</italic>, 2020</xref>). For each bond <italic toggle="yes">B<sub>vw</sub></italic>, we aggregate the function of the hidden states of all arriving neighboring bonds with the hidden state <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>w</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> itself. Then, the hidden state of edge <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>w</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is updated using the obtained message and the previous hidden state of the atom. In other words, the hidden state of bond <italic toggle="yes">B<sub>vw</sub></italic> is obtained by updating the old hidden state with the newly obtained message. The corresponding message-passing update equations from atom <italic toggle="yes">A<sub>v</sub></italic> to atom <italic toggle="yes">A<sub>w</sub></italic> are as follows:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>∖</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi mathvariant="italic">erage</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>w</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">x</italic> is the feature of the corresponding atom <italic toggle="yes">A</italic>, <italic toggle="yes">f<sub>t</sub></italic> is a MLP layer.</p>
        <p>Specifically, in the message-passing phase, all messages arriving at bond <italic toggle="yes">B<sub>vw</sub></italic> are aggregated using a permutation-invariant aggregation function <italic toggle="yes">Average</italic>. Rather than using the summation function as suggested by the original D-MPNN, which caused model instability when training on small datasets, we adopt the average function <italic toggle="yes">Average</italic> to help the model update gradually. The aggregated representation is then combined with the existing hidden state via the MLP <italic toggle="yes">f<sub>t</sub></italic>, resulting in an updated node feature vector <italic toggle="yes">h<sub>vw</sub></italic>. Notably, all hidden states are initially set to <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>w</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mo>φ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">concat</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (with <italic toggle="yes">e<sub>vw</sub></italic> as the feature of bond <italic toggle="yes">B<sub>vw</sub></italic>, <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mo>φ</mml:mo></mml:math></inline-formula> is ReLU activation function). The main idea behind the message-passing technique is to prevent the distortion of messages between atoms. For example, the message from <italic toggle="yes">A<sub>n</sub></italic> to <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> will only be propagated to <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> in the next iteration, whereas, in a conventional MPNN, it will be sent to node <italic toggle="yes">A<sub>n</sub></italic>, creating an unnecessary loop in the message-passing process. In the readout phase, we use one more average function to construct a final representation <italic toggle="yes">O<sub>Gs</sub></italic>.</p>
        <p>Finally, after having two outputs <italic toggle="yes">O<sub>Ms</sub></italic> from MLP net and <italic toggle="yes">O<sub>Gs</sub></italic> from D-MPNN, we combine this information by adopting a cross-attention mechanism:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>Q</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="monospace">Attention</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="monospace">energy</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="italic">Softmax</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">Com</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="italic">CrossAttention</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="monospace">Attention</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="monospace">energy</mml:mi><mml:mo>∗</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">Q</italic> is created from the output of the Morgan fingerprint MLP <italic toggle="yes">O<sub>Ms</sub></italic>, and <italic toggle="yes">K</italic> and <italic toggle="yes">V</italic> are generated from the output <italic toggle="yes">O<sub>Gs</sub></italic> of the D-MPNN by the projection functions <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></inline-formula> (where <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mi mathvariant="normal">w</mml:mi></mml:math></inline-formula> and <italic toggle="yes">b</italic> are weight and bias, respectively). <italic toggle="yes">C</italic> and <italic toggle="yes">d</italic> are the embedding dimensions and number of heads, respectively. <xref rid="btac731-F1" ref-type="fig">Figure 1</xref> visualizes the attention module. Note that, <italic toggle="yes">O<sub>Ms</sub></italic> and <italic toggle="yes">O<sub>Gs</sub></italic> are 1D arrays. In our experiments, one more projection function <italic toggle="yes">f</italic><sub>0</sub> was used for a dimension reduction purpose at the end of the block. We found that a single-head cross-attention outperformed other multi-head cross-attentions.</p>
        <fig position="float" id="btac731-F1">
          <label>Fig. 1.</label>
          <caption>
            <p>Demonstration of attention blocks. While the self-attention block accepts inputs from only single source, cross-attention blocks receive information from two sources</p>
          </caption>
          <graphic xlink:href="btac731f1" position="float"/>
        </fig>
        <p>Using two modalities, we provide multiple views from compounds to the model; hence, Perceiver CPI is able to learn comprehensive patterns precisely. The ECFP provides information on the existence of substructures, whereas the graph representation considers the carrying knowledge that shows how they connect to one another.</p>
      </sec>
      <sec>
        <title>2.1.2 Protein information encoding</title>
        <p>The protein <italic toggle="yes">t</italic> was encoded using the tasks assessing protein embeddings (TAPE) tokenizer, where the initial feature of each residue was represented by the corresponding number following the UniRep Vocabulary (<xref rid="btac731-B31" ref-type="bibr">Rao <italic toggle="yes">et al.</italic>, 2019</xref>). We used this one-hot encoding scheme for protein sequences, mainly because it is the simplest method to construct a unified representation (UniRep), which is broadly applicable and generalized to unseen regions of sequence space (<xref rid="btac731-B1" ref-type="bibr">Alley <italic toggle="yes">et al.</italic>, 2019</xref>). The input is zero-padded to ensure that the number of output features remains fixed and then propagated into the blocks of 1DCNNs. Finally, we obtain the final output features. To help the model learn more deeply, we use the skip connection type to gradually change the weight of the network (<xref rid="btac731-B11" ref-type="bibr">He <italic toggle="yes">et al.</italic>, 2016</xref>). Skip connections suggest skipping some of the layers in the neural network and feeding the output of one layer as the input to the next layers, thereby ensuring feature reusability to avoid the shattered gradient problem. The shattered gradient problem occurs in DNNs when the gradients resemble white noise and negatively impact the training (<xref rid="btac731-B3" ref-type="bibr">Balduzzi <italic toggle="yes">et al.</italic>, 2017</xref>). Residual connection resolves this by introducing a spatial structure to the gradients, thus stabilizing the training process. Eventually, the output of the 1D convolution block can be expressed as follows:</p>
        <boxed-text id="btac731-BOX1" position="float">
          <label>Algorithm 1</label>
          <caption>
            <p>An algorithm for residual block of 1DCNN</p>
          </caption>
          <p><bold>Require:</bold> <italic toggle="yes">M</italic>, <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mo>←</mml:mo><mml:mo> </mml:mo><mml:mtext>Conv</mml:mtext><mml:mn>1</mml:mn><mml:mi mathvariant="normal">D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>λ</mml:mo></mml:mrow></mml:math></inline-formula></p>
          <p>
            <bold> Result:</bold>
            <inline-formula id="IE14">
              <mml:math id="IM14" display="inline" overflow="scroll">
                <mml:mrow>
                  <mml:mi mathvariant="italic">Pro</mml:mi>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                    <mml:mi>t</mml:mi>
                  </mml:msub>
                  <mml:mo>←</mml:mo>
                  <mml:mi>e</mml:mi>
                  <mml:mi>m</mml:mi>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>b</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi mathvariant="italic">out</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </p>
          <p>
            <bold>for</bold>
            <italic toggle="yes">M</italic>
            <bold>do</bold>
          </p>
          <p><bold> </bold><inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mtext>LN</mml:mtext></mml:mrow></mml:math></inline-formula> (<inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:mtext>Conv</mml:mtext><mml:mn>1</mml:mn><mml:mi mathvariant="normal">D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> + <italic toggle="yes">emb<sub>in</sub></italic> * <italic toggle="yes">λ</italic>);</p>
          <p> <inline-formula id="E5"><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mtext>GLU</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>;</p>
          <p>
            <bold>end for</bold>
          </p>
        </boxed-text>
        <p>where <italic toggle="yes">M</italic> is the number of 1DCNN layers, <bold><sub><italic toggle="yes">λ</italic></sub></bold> is fixed to isolate the effect of scaling and <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:mtext>LN</mml:mtext></mml:mrow></mml:math></inline-formula> is a layer normalization function. <italic toggle="yes">emb</italic> represents the protein embedding with initialization <italic toggle="yes">emb</italic><sub>0</sub>. Motivated by transformers, a combination of normalization and skip connection is observed to be helpful in facilitating the model’s capacity to learn of the model to protein information. Furthermore, the use of <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:mtext>LN</mml:mtext></mml:mrow></mml:math></inline-formula> is intended to normalize the distributions of intermediate layers that might mitigate the gradient malformation to enable smoother gradients, faster training and better generalization accuracy. In particular, we use gated linear unit activation, a finite context approach through stacked convolutions, which can efficiently extract information from a sequence because it allows parallelization over sequential token features (<xref rid="btac731-B7" ref-type="bibr">Dauphin <italic toggle="yes">et al.</italic>, 2017</xref>).</p>
      </sec>
    </sec>
    <sec>
      <title>2.2 Compound–protein interaction</title>
      <p>After obtaining two output representations from the three-element networks, we need to precisely integrate them to ultimately teach the model to capture valuable information that reveals CPI properties. The effective fusion of these multiple input sources is becoming increasingly important, as these multi-modality features have been shown to generate highly accurate performances in various tasks. A significant fusion method synergistically combines the two modalities and guarantees that the resultant product reflects the binding features of the input modalities (<xref rid="btac731-B26" ref-type="bibr">Mohla <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac731-B5" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2021</xref>). Inspired by Perceiver IO and Perceiver (<xref rid="btac731-B15" ref-type="bibr">Jaegle <italic toggle="yes">et al.</italic>, 2021a</xref>,<xref rid="btac731-B16" ref-type="bibr">b</xref>), we propose a novel method that leverages a highly asymmetric attention mechanism to distill compound information iteratively and then structure the final interaction representation using the protein information from a single cross-attention module. In the cross-attention block, we aim to force the model to capture patterns that show the effect of information from the compound on the protein information. In other words, we use this method because we primarily intend to determine how the protein reacts with the compound. More characteristically, after representing a compound, we process the compound latents by applying a series of self-attention modules to refine the compound representation. Finally, we combine <italic toggle="yes">Comp<sub>s</sub></italic> and <italic toggle="yes">Prot<sub>t</sub></italic> by applying a cross-attention module that maps latent arrays to the protein representation. The final interaction representation can be expressed as follows:
<disp-formula id="E6"><label>(5)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>Q</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">Com</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">Pro</mml:mi><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>V</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">Pro</mml:mi><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E7"><label>(6)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">Interactio</mml:mi><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="italic">CrossAttention</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>Using the cross-attention mechanism, we can model the semantic relevance between the protein and compound features, thus drawing attention to significant interaction information and benefiting the binding affinity prediction task. The cross-attention module generates cross-attention energy (also known as an attention map), which is then used to weight the feature map to achieve informative and discriminative feature representation. Moreover, the computation and memory complexity of generating attention energy in cross-attention are linear rather than quadratic, making the entire process more efficient.</p>
    </sec>
    <sec>
      <title>2.3 Loss function and optimizer</title>
      <p>In our experiment, we used the mean squared error (MSE) loss function with LAMB optimizer, which stands for ‘layer-wise adaptive moments optimizer for batch training’ (<xref rid="btac731-B42" ref-type="bibr">You <italic toggle="yes">et al.</italic>, 2019</xref>). As can be seen, the training may become unstable if this ratio is too high. However, the weights do not change rapidly enough if the ratio is too small. Using the trust ratio, LAMB enables the model to be more confident in each step and scale much larger batch sizes without causing divergence. The hyperparameters in our neural network are searched using Bayesian optimization algorithms.</p>
    </sec>
    <sec>
      <title>2.4 Benchmark datasets</title>
      <p>To compare SOTA models with the proposed Perceiver CPI model and analyze its performance, we used three well-known benchmark datasets. To make use of the complementary information captured by the various bioactivity types, including dissociation constant (<italic toggle="yes">K<sub>d</sub></italic>), inhibition constant (<italic toggle="yes">K<sub>i</sub></italic>) or the half maximal inhibitory concentration (<italic toggle="yes">IC</italic><sub>50</sub>), <xref rid="btac731-B35" ref-type="bibr">Tang <italic toggle="yes">et al.</italic> (2014)</xref> introduced a model-based integration approach called KIBA to generate an integrated drug–target bioactivity matrix. KIBA scores were created to optimize the consistency of the three measurements. The Davis dataset contains the interactions of 68 kinase inhibitors with 442 kinases covering &gt;80% of the human catalytic protein kinome without missing interactions (<xref rid="btac731-B8" ref-type="bibr">Davis <italic toggle="yes">et al.</italic>, 2011</xref>). The original unit of the dataset is <italic toggle="yes">K<sub>d</sub></italic> values; however, normalization of the label helps improve the performance. Hence, log transformation was applied to scale the label in the smaller range <italic toggle="yes">pK<sub>d</sub></italic> = <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mn>1</mml:mn><mml:mi>e</mml:mi><mml:mn>9</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Specifically, we used KIBA and Davis from the open-source software named DeepPurpose (<xref rid="btac731-B12" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2021</xref>). For Metz data, <xref rid="btac731-B24" ref-type="bibr">Metz <italic toggle="yes">et al.</italic> (2011)</xref> presented a critical statistical analysis of kinomics screening data across 170 different protein kinases and establishing rigorous criteria. The PDBbind dataset contained 16 151 interactions (<xref rid="btac731-B37" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2005</xref>). After filtering and processing to qualify the dataset, 6689 unique pairs were retained (<xref rid="btac731-B22" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2020</xref>). <xref rid="btac731-T1" ref-type="table">Table 1</xref> shows the summary statistics for all datasets.</p>
      <table-wrap position="float" id="btac731-T1">
        <label>Table 1.</label>
        <caption>
          <p>Statistics of the benchmark datasets</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Dataset</th>
              <th align="center" rowspan="1" colspan="1">Proteins</th>
              <th align="center" rowspan="1" colspan="1">Drugs</th>
              <th align="center" rowspan="1" colspan="1">Interactions</th>
              <th align="center" rowspan="1" colspan="1">Density (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Davis (<xref rid="btac731-B8" ref-type="bibr">Davis <italic toggle="yes">et al.</italic>, 2011</xref>)</td>
              <td rowspan="1" colspan="1">442</td>
              <td rowspan="1" colspan="1">68</td>
              <td rowspan="1" colspan="1">30 056</td>
              <td align="center" rowspan="1" colspan="1">100</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">KIBA (<xref rid="btac731-B35" ref-type="bibr">Tang <italic toggle="yes">et al.</italic>, 2014</xref>)</td>
              <td rowspan="1" colspan="1">229</td>
              <td rowspan="1" colspan="1">2068</td>
              <td rowspan="1" colspan="1">117 657</td>
              <td rowspan="1" colspan="1">24.84</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Metz (<xref rid="btac731-B24" ref-type="bibr">Metz <italic toggle="yes">et al.</italic>, 2011</xref>)</td>
              <td rowspan="1" colspan="1">170</td>
              <td rowspan="1" colspan="1">1423</td>
              <td rowspan="1" colspan="1">35 259</td>
              <td rowspan="1" colspan="1">14.57</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PDBbind (<xref rid="btac731-B37" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2005</xref>)</td>
              <td rowspan="1" colspan="1">2079</td>
              <td rowspan="1" colspan="1">5535</td>
              <td rowspan="1" colspan="1">6989</td>
              <td rowspan="1" colspan="1">0.06</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Furthermore, the density of all four datasets is shown in <xref rid="btac731-F2" ref-type="fig">Figure 2</xref>. We employed kernel density estimation, a fundamental data smoothing problem where inferences about the population are made based on a finite data sample to reveal the dataset density. <xref rid="btac731-F2" ref-type="fig">Figure 2</xref> indicates that almost all the Davis dataset binding affinity values were highly concentrated around five. In particular, 69.64% of the Davis dataset had affinity binding values of five, whereas 71.96% of the KIBA dataset were in the range from 11.1 to 12. Due to the skew distribution of Davis dataset and KIBA dataset, we empirically forced the model to perform a larger weight update for data points, which did not belong to the high density area 10 times larger than the others. For instance, in Davis dataset, the data points have binding affinity in the range from 0 to 5 were discounted by 0.5, while the out-ranged data points multiplied by 5. Conversely, the Metz and PDBbind datasets exhibited well-balanced distributions with fewer outliers than the others; however, the sparsity of these datasets is extremely high.</p>
      <fig position="float" id="btac731-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>Visualization of benchmark datasets with kernel density estimation</p>
        </caption>
        <graphic xlink:href="btac731f2" position="float"/>
      </fig>
      <p>For a fair comparison, we also used the GPCR classification dataset to evaluate the enrichment factor (EF) from the Directory of Useful Decoys-Enhanced database (DUD-E) database. EF is used to show the performance of the model in finding true positives throughout the background database compared to random selection (<xref rid="btac731-B13" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2006</xref>). Moreover, EF reveals the concentration of the annotated ligands among the top-scoring compounds compared to their concentrations throughout the entire dataset. For instance, the enrichment factor at 1% is the percentage of ligands found when 1% of decoys were found (<xref rid="btac731-B18" ref-type="bibr">Jain, 2008</xref>). Decoys from the DUD-E database were extracted from the ZINC database and were commercially available compounds for virtual screening (<xref rid="btac731-B28" ref-type="bibr">Mysinger <italic toggle="yes">et al.</italic>, 2012</xref>). The 2D-similarity between the active compounds and decoy compounds are measured by calculating the Tanimoto distance. The statistics of the GPCR dataset are shown in <xref rid="btac731-T2" ref-type="table">Table 2</xref> for training obtained from TransformerCPI, which was extracted from the GLASS database (<xref rid="btac731-B4" ref-type="bibr">Chan <italic toggle="yes">et al.</italic>, 2015</xref>). The dataset provides experimentally validated GPCR–ligand associations. A threshold of 6.0 was set to divide the original dataset into positive and negative sets. Finally, <xref rid="btac731-T3" ref-type="table">Table 3</xref> presents the test sets collected from the GPCR and Diverse subsets in the DUD-E database.</p>
      <table-wrap position="float" id="btac731-T2">
        <label>Table 2.</label>
        <caption>
          <p>Statistic of GPCR dataset</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Proteins</th>
              <th rowspan="1" colspan="1">Compounds</th>
              <th rowspan="1" colspan="1">Positive pairs</th>
              <th rowspan="1" colspan="1">Negative pairs</th>
              <th rowspan="1" colspan="1">Density (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">356</td>
              <td rowspan="1" colspan="1">5359</td>
              <td rowspan="1" colspan="1">7989</td>
              <td rowspan="1" colspan="1">7354</td>
              <td rowspan="1" colspan="1">0.8</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <table-wrap position="float" id="btac731-T3">
        <label>Table 3.</label>
        <caption>
          <p>Statistics of GPCR and diverse subsets from DUD-E database</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Subset</th>
              <th rowspan="1" colspan="1">Number of target</th>
              <th rowspan="1" colspan="1">Actives</th>
              <th rowspan="1" colspan="1">Decoys</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">GPCR subset</td>
              <td rowspan="1" colspan="1">5</td>
              <td rowspan="1" colspan="1">1480</td>
              <td rowspan="1" colspan="1">99 856</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Diverse subset</td>
              <td rowspan="1" colspan="1">7</td>
              <td rowspan="1" colspan="1">1759</td>
              <td rowspan="1" colspan="1">107 591</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
  </sec>
  <sec>
    <title>3 Experimental results and discussion</title>
    <sec>
      <title>3.1 Experimental results</title>
      <p><bold>Model conversion:</bold> As mentioned above, we considered the CPI task as a regression problem. Nevertheless, only a few methods have used a similar concept; we transformed binary classification models, such as TransformerCPI, DeepconvDTI and HyperattentionDTI into regression models by modifying their final layers. To maintain the original performance, the output of the last layer was transformed to a single perceptron node, and the loss function was altered to the MSE loss function.</p>
      <p><bold>Experimental procedure:</bold> Owing to the density of the Davis dataset and because the dataset approximately covers 80% of the human catalytic protein kinomes, we decided to perform three experiments: novel pair setting, novel compound setting and novel protein setting; only novel pair setting was applied on the KIBA and Metz datasets. Finally, the PDBbind dataset and GPCR dataset with the GPCR and the Diverse subsets from the DUD-E database were used for an additional analysis. To calculate the similarities, the protein similarity is measured by the percentage of the number of aligned amino acids over the total length (in Perceiver CPI, the length of the proteins was fixed to 500). Meanwhile, the compound similarity was calculated using the Tanimoto similarity function.
</p>
      <list list-type="bullet">
        <list-item>
          <p>Novel pair (Davis, KIBA and Metz): There were no overlaps between the training and test datasets. Neither the training compound nor the training protein appeared in the test set.</p>
        </list-item>
        <list-item>
          <p>Novel-hard pair (Davis): There were no overlaps between the training and test datasets. The testing interactions were highly selective for similarities less than 0.3 by comparing to training interactions.</p>
        </list-item>
        <list-item>
          <p>Novel compound (Davis): There were no intersections of compounds in the training set and compounds in the test set.</p>
        </list-item>
        <list-item>
          <p>Novel protein (Davis): There were no intersections of proteins in the training set and proteins in the test set.</p>
        </list-item>
        <list-item>
          <p>Cross-domain experiment (Davis and PDBbind): There were no overlaps between the training and test datasets. We trained the model with the Davis dataset and tested it with the PDBbind dataset.</p>
        </list-item>
        <list-item>
          <p>Enrichment factor analysis <italic toggle="yes">[</italic>GPCR, GPCR subset (DUD-E dataset), Diverse subset (DUD-E dataset)]: There were no overlaps between the training and test datasets. We trained the model with the GPCR dataset and tested it with subsets from the DUD-E dataset (the duplicated target ‘CXCR4’ was removed from the Diverse subset).</p>
        </list-item>
      </list>
      <p><bold>Evaluation metric:</bold> To evaluate the performance on the regression task, we used the mean squared error (MSE) metric to measure the performance of the models and the concordance index (CI) metric to evaluate the proportion of concordant prediction pairs per the total number of label pairs, which tells us whether the predicted binding affinity values of two random drug–target pairs were predicted in the same order as their truth values. In the enrichment factor analysis, we adopted an EF score at fraction 1% (EF<inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) to show the performance of the models in determining the annotated ligands among the top binding affinity compounds and the Boltzmann-enhanced discrimination of the receiver operating characteristic score to focus more on early enrichment with <italic toggle="yes">α </italic>= 80.5 (BEDROC<inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mo>α</mml:mo><mml:mo>=</mml:mo><mml:mn>80.5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>).</p>
      <p><bold>Cross-validation:</bold> We applied five-fold cross-validation to calculate the performance of baseline models and Perceiver CPI in four experiments: novel pair, novel-hard pair, novel compound, novel protein. The validation set was taken arbitrarily from the training set following the ratio training: validation = 80%: 20% for all experiments.</p>
      <p><xref rid="btac731-T4" ref-type="table">Table 4</xref> compares the performance of Perceiver CPI with five SOTA deep-learning baseline models for the three types of separations. Regarding the two principal tasks (novel pair and novel compound), Perceiver CPI showed remarkable performances. With reference to the novel pair setting, our proposed model achieved an MSE of 0.463(±0.013) and CI of 0.638(±0.028), whereas the competitors performed poorly. In the novel compound experiment, Perceiver CPI reached the lowest MSE (0.378(±0.010)) and had the highest value in CI (0.726(±0.017)). We discovered no significant difference between the previous approaches and our model in the novel protein tasks. Nonetheless, Perceiver CPI performed better than the others in terms of MSE and was competitive with the first-placed model in terms of CI metric. As shown in <xref rid="btac731-T5" ref-type="table">Table 5</xref>, in the most challenging setting, when the test set was significantly different from what the model trained on, the proposed method outperformed baseline compactors by providing precise predictions, resulting in the lowest MSE. In practice, the number of proteins is finite, and most of them will eventually be annotated, which means that the CPI task is mainly about finding a new compound with existing proteins in the real world. Interestingly, our model was also more stable than the others as indicated by its lower standard deviation among the validations.</p>
      <table-wrap position="float" id="btac731-T4">
        <label>Table 4.</label>
        <caption>
          <p>Comparison of the proposed method with SOTA model in terms of three settings from the Davis dataset with 5-fold cross-validation</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th colspan="2" align="center" rowspan="1">Novel pair<hr/></th>
              <th colspan="2" align="center" rowspan="1">Novel compound<hr/></th>
              <th colspan="2" align="center" rowspan="1">Novel protein<hr/></th>
            </tr>
            <tr>
              <th align="center" rowspan="1" colspan="1"/>
              <th align="center" rowspan="1" colspan="1">MSE</th>
              <th align="center" rowspan="1" colspan="1">CI</th>
              <th align="center" rowspan="1" colspan="1">MSE</th>
              <th align="center" rowspan="1" colspan="1">CI</th>
              <th align="center" rowspan="1" colspan="1">MSE</th>
              <th align="center" rowspan="1" colspan="1">CI</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">DeepDTA (<xref rid="btac731-B30" ref-type="bibr">Öztürk <italic toggle="yes">et al.</italic>, 2018</xref>)</td>
              <td rowspan="1" colspan="1">0.631(±0.059)</td>
              <td rowspan="1" colspan="1">0.533(±0.027)</td>
              <td rowspan="1" colspan="1">0.482(±0.034)</td>
              <td rowspan="1" colspan="1">0.613(±0.029)</td>
              <td rowspan="1" colspan="1">0.701(±0.045)</td>
              <td rowspan="1" colspan="1">
                <bold>0.759(±0.015)</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepConvDTI (<xref rid="btac731-B21" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2019</xref>)</td>
              <td rowspan="1" colspan="1">0.598(±0.057)</td>
              <td rowspan="1" colspan="1">0.546(±0.043)</td>
              <td rowspan="1" colspan="1">0.512(±0.046)</td>
              <td rowspan="1" colspan="1">0.681(±0.012)</td>
              <td rowspan="1" colspan="1">0.789(±0.109)</td>
              <td rowspan="1" colspan="1">0.714(±0.034)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">TransformerCPI (<xref rid="btac731-B6" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020</xref>)</td>
              <td rowspan="1" colspan="1">0.549(±0.038)</td>
              <td rowspan="1" colspan="1">0.490(±0.032)</td>
              <td rowspan="1" colspan="1">0.522(±0.027)</td>
              <td rowspan="1" colspan="1">0.592(±0.026)</td>
              <td rowspan="1" colspan="1">0.708(±0.032)</td>
              <td rowspan="1" colspan="1">0.676(±0.005)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GraphDTA (GINs) (<xref rid="btac731-B29" ref-type="bibr">Nguyen <italic toggle="yes">et al.</italic>, 2021</xref>)</td>
              <td rowspan="1" colspan="1">0.846(±0.058)</td>
              <td rowspan="1" colspan="1">0.459(±0.032)</td>
              <td rowspan="1" colspan="1">0.452(±0.051)</td>
              <td rowspan="1" colspan="1">0.670(±0.018)</td>
              <td rowspan="1" colspan="1">0.970(±0.061)</td>
              <td rowspan="1" colspan="1">0.660(±0.016)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HyperattentionDTI (<xref rid="btac731-B43" ref-type="bibr">Zhao <italic toggle="yes">et al.</italic>, 2022</xref>)</td>
              <td rowspan="1" colspan="1">0.671(±0.045)</td>
              <td rowspan="1" colspan="1">0.517(±0.013)</td>
              <td rowspan="1" colspan="1">0.506(±0.015)</td>
              <td rowspan="1" colspan="1">0.578(±0.019)</td>
              <td rowspan="1" colspan="1">0.784(±0.063)</td>
              <td rowspan="1" colspan="1">0.674(±0.020)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Perceiver CPI (ours)</td>
              <td rowspan="1" colspan="1">
                <bold>0.463(±0.013)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.638(±0.028)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.378(±0.010)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.726(±0.017)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.667(±0.018)</bold>
              </td>
              <td rowspan="1" colspan="1">0.758(±0.010)</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic toggle="yes">Note</italic>: The metrics are MSE (the lower, the better) and CI (the higher, the better) (± standard deviation)</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap position="float" id="btac731-T5">
        <label>Table 5.</label>
        <caption>
          <p>Comparison of Perceiver CPI and other SOTA competitors on novel-hard pair setting</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th align="center" rowspan="1" colspan="1">MSE</th>
              <th align="center" rowspan="1" colspan="1">CI</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">DeepDTA (<xref rid="btac731-B30" ref-type="bibr">Öztürk <italic toggle="yes">et al.</italic>, 2018</xref>)</td>
              <td rowspan="1" colspan="1">0.948(±0.218)</td>
              <td rowspan="1" colspan="1">0.565(±0.040)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepConvDTI (<xref rid="btac731-B21" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2019</xref>)</td>
              <td rowspan="1" colspan="1">0.768(±0.290)</td>
              <td rowspan="1" colspan="1">0.571(±0.052)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">TransformerCPI (<xref rid="btac731-B6" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020</xref>)</td>
              <td rowspan="1" colspan="1">0.806(±0.254)</td>
              <td rowspan="1" colspan="1">0.508(±0.071)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GraphDTA (GINs) (<xref rid="btac731-B29" ref-type="bibr">Nguyen <italic toggle="yes">et al.</italic>, 2021</xref>)</td>
              <td rowspan="1" colspan="1">0.931(±0.314)</td>
              <td rowspan="1" colspan="1">0.542(±0.070)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HyperattentionDTI (<xref rid="btac731-B43" ref-type="bibr">Zhao <italic toggle="yes">et al.</italic>, 2022</xref>)</td>
              <td rowspan="1" colspan="1">0.873(±0.246)</td>
              <td rowspan="1" colspan="1">0.600(±0.049)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Perceiver CPI (ours)</td>
              <td rowspan="1" colspan="1">
                <bold>0.701(±0.244)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.609(±0.072)</bold>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Considering the most challenging setting, the novel pair split settings with KIBA and Metz datasets, as shown in <xref rid="btac731-T6" ref-type="table">Table 6</xref>, DeepConvDTI achieved inferior performance using ECFP representation for compounds. Although ECFP captures useful information for CPI prediction, owing to its simplicity, the knowledge from compounds is still not fully used. Therefore, instead of using the ECFP independently, a combination of ECFP and graph representations of the compound was utilized to further improve performance. The two datasets contained many missing interactions, resulting in the underperformance of all models. In the KIBA dataset, Perceiver CPI attained a lower MSE than the baseline by 0.028 and a higher CI. In particular, it was extremely difficult to obtain correct predictions using the Metz dataset, which has a 14.57% density.</p>
      <table-wrap position="float" id="btac731-T6">
        <label>Table 6.</label>
        <caption>
          <p>Comparison of Perceiver CPI performance to SOTA baseline models in novel pair task from on KIBA and Metz datasets</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th colspan="2" align="center" rowspan="1">KIBA<hr/></th>
              <th colspan="2" align="center" rowspan="1">Metz<hr/></th>
            </tr>
            <tr>
              <th align="center" rowspan="1" colspan="1"/>
              <th align="center" rowspan="1" colspan="1">MSE</th>
              <th align="center" rowspan="1" colspan="1">CI</th>
              <th align="center" rowspan="1" colspan="1">MSE</th>
              <th align="center" rowspan="1" colspan="1">CI</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">DeepDTA (<xref rid="btac731-B30" ref-type="bibr">Öztürk <italic toggle="yes">et al.</italic>, 2018</xref>)</td>
              <td rowspan="1" colspan="1">0.668(±0.055)</td>
              <td rowspan="1" colspan="1">0.600(±0.011)</td>
              <td rowspan="1" colspan="1">0.781(±0.060)</td>
              <td rowspan="1" colspan="1">0.627(±0.011)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepConvDTI (<xref rid="btac731-B21" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2019</xref>)</td>
              <td rowspan="1" colspan="1">0.550(±0.009)</td>
              <td rowspan="1" colspan="1">0.635(±0.007)</td>
              <td rowspan="1" colspan="1">0.703(±0.027)</td>
              <td rowspan="1" colspan="1">0.671(±0.016)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">TransformerCPI (<xref rid="btac731-B6" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020</xref>)</td>
              <td rowspan="1" colspan="1">0.630(±0.057)</td>
              <td rowspan="1" colspan="1">0.563(±0.014)</td>
              <td rowspan="1" colspan="1">1.081(±0.125)</td>
              <td rowspan="1" colspan="1">0.557(±0.016)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GraphDTA (GINs) (<xref rid="btac731-B29" ref-type="bibr">Nguyen <italic toggle="yes">et al.</italic>, 2021</xref>)</td>
              <td rowspan="1" colspan="1">0.698(±0.042)</td>
              <td rowspan="1" colspan="1">0.591(±0.013)</td>
              <td rowspan="1" colspan="1">1.232(±0.094)</td>
              <td rowspan="1" colspan="1">0.615(±0.010)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HyperattentionDTI (<xref rid="btac731-B43" ref-type="bibr">Zhao <italic toggle="yes">et al.</italic>, 2022</xref>)</td>
              <td rowspan="1" colspan="1">1.022(±0.062)</td>
              <td rowspan="1" colspan="1">0.590(±0.015)</td>
              <td rowspan="1" colspan="1">1.064(±0.080)</td>
              <td rowspan="1" colspan="1">0.630(±0.013)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Perceiver CPI (ours)</td>
              <td rowspan="1" colspan="1">
                <bold>0.522(±0.010)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.638(±0.013)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.658(±0.016)</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.675(±0.012)</bold>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Moreover, we performed a cross-domain experiment to determine the adaptability of our method to an unseen domain dataset. We chose two datasets (Davis and PDBbind) owing to their overlapping properties and measurements. First, we eliminated all overlapping interactions from the PDBbind dataset to the Davis dataset. Second, while we divided the Davis dataset into training and validation sets at a ratio of 80%:20%, the processed PDBbind dataset was used as a test set. The results in <xref rid="btac731-T7" ref-type="table">Table 7</xref> show that Perceiver CPI significantly outperformed the baselines. The proposed approach achieved a higher performance on CI metrics than SOTA models, while exhibited a lower MSE. In other words, Perceiver CPI provides more precise predictions than the compared models. In particular, all models, including ours, performed poorly in the cross-domain experiment because of the quantity and quality of the training dataset.</p>
      <table-wrap position="float" id="btac731-T7">
        <label>Table 7.</label>
        <caption>
          <p>Results of the cross-domain experiment (trained on Davis and tested on PDBbind)</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th align="center" rowspan="1" colspan="1">MSE</th>
              <th align="center" rowspan="1" colspan="1">CI</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">DeepDTA (<xref rid="btac731-B30" ref-type="bibr">Öztürk <italic toggle="yes">et al.</italic>, 2018</xref>)</td>
              <td rowspan="1" colspan="1">4.716</td>
              <td rowspan="1" colspan="1">0.500</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepConvDTI (<xref rid="btac731-B21" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2019</xref>)</td>
              <td rowspan="1" colspan="1">5.400</td>
              <td rowspan="1" colspan="1">0.477</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">TransformerCPI (<xref rid="btac731-B6" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020</xref>)</td>
              <td rowspan="1" colspan="1">4.962</td>
              <td rowspan="1" colspan="1">0.497</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GraphDTA (GINs) (<xref rid="btac731-B29" ref-type="bibr">Nguyen <italic toggle="yes">et al.</italic>, 2021</xref>)</td>
              <td rowspan="1" colspan="1">6.323</td>
              <td rowspan="1" colspan="1">0.516</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HyperattentionCPI (<xref rid="btac731-B43" ref-type="bibr">Zhao <italic toggle="yes">et al.</italic>, 2022</xref>)</td>
              <td rowspan="1" colspan="1">5.946</td>
              <td rowspan="1" colspan="1">0.410</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Perceiver CPI (ours)</td>
              <td rowspan="1" colspan="1">
                <bold>4.612</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.532</bold>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>We tested the model and other classifiers and five docking-based programs [Gold (<xref rid="btac731-B19" ref-type="bibr">Jones <italic toggle="yes">et al.</italic>, 1997</xref>), Glide (<xref rid="btac731-B9" ref-type="bibr">Friesner <italic toggle="yes">et al.</italic>, 2004</xref>), Surflex (<xref rid="btac731-B17" ref-type="bibr">Jain, 2003</xref>), FlexX (<xref rid="btac731-B32" ref-type="bibr">Rarey <italic toggle="yes">et al.</italic>, 1996</xref>) and Blaster (<xref rid="btac731-B14" ref-type="bibr">Irwin <italic toggle="yes">et al.</italic>, 2009</xref>)] on subsets from the DUD-E database. We converted Perceiver CPI architecture into a classifier by changing the loss function from MSELoss to CrossEntropyLoss, as well as by transforming the last layer into a sigmoid function. Ligand enrichment among top-ranking hits is an important criterion for molecular docking and drug–target interactions. <xref rid="btac731-T8" ref-type="table">Table 8</xref> reveals that Perceiver CPI achieved a better performance for multiple targets in an EF<inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and BEDROC<inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mo>α</mml:mo><mml:mo>=</mml:mo><mml:mn>80.5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> than the other deep-learning models. However, the docking-based method outperformed the data-driven method for most protein targets on both metrics (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S7</xref>). Perhaps the combination of the two good methods might lead to an excellent performance. Moreover, the accumulation of extended datasets may enhance the predictions of the ML/DL models.</p>
      <table-wrap position="float" id="btac731-T8">
        <label>Table 8.</label>
        <caption>
          <p>Enrichment factor analysis results for subsets in the DUD-E database (UP: EF<inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mn>1</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, DOWN: BEDROC<inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mo>α</mml:mo><mml:mo>=</mml:mo><mml:mn>80.5</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>)</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Family</th>
              <th align="center" rowspan="1" colspan="1">DeepConvDTI</th>
              <th align="center" rowspan="1" colspan="1">TransformerCPI</th>
              <th align="center" rowspan="1" colspan="1">HyperattentionDTI</th>
              <th align="center" rowspan="1" colspan="1">Perceiver CPI (ours)</th>
              <th align="center" rowspan="1" colspan="1">Gold</th>
              <th align="center" rowspan="1" colspan="1">Glide</th>
              <th align="center" rowspan="1" colspan="1">Surflex</th>
              <th align="center" rowspan="1" colspan="1">FlexX</th>
              <th align="center" rowspan="1" colspan="1">Blaster</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">GPCR (DUD-E)</td>
              <td rowspan="1" colspan="1">9.728(±11.534)</td>
              <td rowspan="1" colspan="1">0.814(±1.178)</td>
              <td rowspan="1" colspan="1">3.982(±3.119)</td>
              <td rowspan="1" colspan="1">
                <bold>16.366(±15.921)</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">N/a</td>
              <td align="center" rowspan="1" colspan="1">N/a</td>
              <td align="center" rowspan="1" colspan="1">N/a</td>
              <td align="center" rowspan="1" colspan="1">N/a</td>
              <td rowspan="1" colspan="1">11.8(±8.136)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">(DUD-E)</td>
              <td rowspan="1" colspan="1">0.152(±0.174)</td>
              <td rowspan="1" colspan="1">0.018(±0.040)</td>
              <td rowspan="1" colspan="1">0.071(±0.058)</td>
              <td rowspan="1" colspan="1">0.236(±0.177)</td>
              <td rowspan="1" colspan="1">
                <bold>0.282(±0.154)</bold>
              </td>
              <td rowspan="1" colspan="1">0.198(±0.205)</td>
              <td rowspan="1" colspan="1">0.284(±0.098)</td>
              <td rowspan="1" colspan="1">0.156(±0.135)</td>
              <td align="center" rowspan="1" colspan="1">N/a</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Diverse</td>
              <td rowspan="1" colspan="1">0.292(±0.774)</td>
              <td rowspan="1" colspan="1">0.922(±0.819)</td>
              <td rowspan="1" colspan="1">1.075(0.876)</td>
              <td rowspan="1" colspan="1">1.88(±1.297)</td>
              <td align="center" rowspan="1" colspan="1">N/a</td>
              <td align="center" rowspan="1" colspan="1">N/a</td>
              <td align="center" rowspan="1" colspan="1">N/a</td>
              <td align="center" rowspan="1" colspan="1">N/a</td>
              <td rowspan="1" colspan="1">
                <bold>13.571(±12.908)</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">(DUD-E)</td>
              <td rowspan="1" colspan="1">0.005(±0.015)</td>
              <td rowspan="1" colspan="1">0.021(0.016)</td>
              <td rowspan="1" colspan="1">0.023(±0.018)</td>
              <td rowspan="1" colspan="1">0.031(±0.022)</td>
              <td rowspan="1" colspan="1">
                <bold>0.295(±0.180)</bold>
              </td>
              <td rowspan="1" colspan="1">0.258(±0.170)</td>
              <td rowspan="1" colspan="1">0.118(±0.093)</td>
              <td rowspan="1" colspan="1">0.104(±0.059)</td>
              <td align="center" rowspan="1" colspan="1">N/a</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>In summary, the proposed Perceiver CPI achieves a competitive or better performance than SOTA deep-learning baselines in all settings, due to the fact that our model adopts the strength of a attention mechanism to dynamically adjust the features of drugs and proteins in different combinations.</p>
    </sec>
    <sec>
      <title>3.2 Discussion</title>
      <sec>
        <title>3.2.1 Difference between perceiver CPI and perceiver IO</title>
        <p>Perceiver IO is an updated version of Perceiver, which uses an asymmetric attention mechanism to accept input information into a tight latent space. Subsequently, the output of Perceiver is merged with the query system using an additional cross-attention. The key insight is to produce each output by attending to the latent array using a specific output query associated with that output. Therefore, the target of Perceiver IO is the input compound. However, the purpose of Perceiver CPI is to seek a change in the protein caused by the effect of a compound; hence, our target is the input protein. As shown in <xref rid="btac731-F3" ref-type="fig">Figure 3</xref>, we take the key (K) and value (V) from the protein information, contrary to Perceiver IO, which considers the protein information as an output query array (Q). Besides, empirical experiment results with the original structure of Perceiver IO showed poorer performance on the CPI task when compared to Perceiver CPI.</p>
        <fig position="float" id="btac731-F3">
          <label>Fig. 3.</label>
          <caption>
            <p>Overview of perceiver CPI. The model is a combination of three-element neural networks that take information from three sources: molecular graphs, Morgan fingerprints for constructing compound patterns and protein sequences for extracting protein knowledge</p>
          </caption>
          <graphic xlink:href="btac731f3" position="float"/>
        </fig>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion and future work</title>
    <p>In this study, we deployed cross-attention mechanisms to address the CPI task. We proposed a novel attention mechanism to not only enrich the information extracted from a compound using ECFP knowledge but also to capture CPI information effectively. The proposed Perceiver CPI model exhibited a significantly improved performance on three benchmark datasets when compared with SOTA baselines in terms of MSE and CI.</p>
    <p>Although Perceiver CPI has demonstrated excellent performance, much work remains to improve the performance of CPI prediction tasks in the future.
</p>
    <list list-type="bullet">
      <list-item>
        <p>Finding and extracting meaningful features from proteins remains a difficult but worthwhile task. For instance, AlphaFold2 from DeepMind can be used to predict the 3D structure of proteins (<xref rid="btac731-B20" ref-type="bibr">Jumper <italic toggle="yes">et al.</italic>, 2021</xref>).</p>
      </list-item>
      <list-item>
        <p>The information taken from compounds can still be cultivated more profitably, such as by using the META-Learning method to construct a better representation from small datasets.</p>
      </list-item>
      <list-item>
        <p>Utilizing information from 3D structures produced from SMILES, as GeoMol attempts to do, is also a promising method because of its high information capacity (<xref rid="btac731-B10" ref-type="bibr">Ganea <italic toggle="yes">et al.</italic>, 2021</xref>).</p>
      </list-item>
      <list-item>
        <p>Adopting the transfer learning method for individual neural networks (compound and protein networks) to generate improved representations from the beginning with the help of prior knowledge should also be considered.</p>
      </list-item>
      <list-item>
        <p>The interpretability of Perceiver CPI is limited by the dimensionality reduction of MLP from the hidden state update process in the message-passing step and from the attention blocks. Addressing such useful features would form a valuable part of future work.</p>
      </list-item>
    </list>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btac731_Supplementary_Data</label>
      <media xlink:href="btac731_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We sincerely thank Dr. Sungjoon Park (Department of Medicine, University of California San Diego, La Jolla, CA, USA) for assistance with the methodology discussion and experimental design comments, which greatly improved the quality of our work. We greatly thank the anonymous reviewers for their careful of reading of our manuscript and their many insightful comments and suggestions.</p>
  </ack>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>Our study used open-access datasets, and the data-related links are available in the Data availability section in the supplementary document.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the National Research Foundation of Korea [NRF-2020R1A2C3010638, NRF-2014M3C9A3063541]; the Ministry of Health &amp; Welfare, Republic of Korea [HR20C0021]; and the ICT Creative Consilience program [IITP-2021-0-01819] supervised by the IITP (Institute for Information &amp; communications Technology Planning &amp; Evaluation).</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btac731-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alley</surname><given-names>E.C.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title>. <source>Nat. Methods</source>, <volume>16</volume>, <fpage>1315</fpage>–<lpage>1322</lpage>.<pub-id pub-id-type="pmid">31636460</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bahi</surname><given-names>M.</given-names></string-name>, <string-name><surname>Batouche</surname><given-names>M.</given-names></string-name></person-group> (<year>2021</year>) <article-title>Convolutional neural network with stacked autoencoders for predicting drug-target interaction and binding affinity</article-title>. <source>Int. J. Data Mining Model. Manag</source>., <volume>13</volume>, <fpage>81</fpage>–<lpage>113</lpage>.</mixed-citation>
    </ref>
    <ref id="btac731-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Balduzzi</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) The shattered gradients problem: if resnets are the answer, then what is the question? In: <italic toggle="yes">Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia</italic>. pp. <fpage>342</fpage>–<lpage>350</lpage>. PMLR.</mixed-citation>
    </ref>
    <ref id="btac731-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chan</surname><given-names>W.K.</given-names></string-name></person-group><etal>et al</etal> (<year>2015</year>) <article-title>GLASS: a comprehensive database for experimentally validated GPCR-ligand associations</article-title>. <source>Bioinformatics</source>, <volume>31</volume>, <fpage>3035</fpage>–<lpage>3042</lpage>.<pub-id pub-id-type="pmid">25971743</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>C.-F.R.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) CrossViT: cross-attention multi-scale vision transformer for image classification. In: <italic toggle="yes">ICCV 2021 took place virtually from October 11 to October 17, 2021</italic>. pp. <fpage>357</fpage>–<lpage>366</lpage>.</mixed-citation>
    </ref>
    <ref id="btac731-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>L.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>TransformerCPI: improving compound–protein interaction prediction by sequence-based deep learning with self-attention mechanism and label reversal experiments</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>4406</fpage>–<lpage>4414</lpage>.<pub-id pub-id-type="pmid">32428219</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Dauphin</surname><given-names>Y.N.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) Language modeling with gated convolutional networks. In: <italic toggle="yes">Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia.</italic> pp. <fpage>933</fpage>–<lpage>941</lpage>. PMLR.</mixed-citation>
    </ref>
    <ref id="btac731-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Davis</surname><given-names>M.I.</given-names></string-name></person-group><etal>et al</etal> (<year>2011</year>) <article-title>Comprehensive analysis of kinase inhibitor selectivity</article-title>. <source>Nat. Biotechnol</source>., <volume>29</volume>, <fpage>1046</fpage>–<lpage>1051</lpage>.<pub-id pub-id-type="pmid">22037378</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friesner</surname><given-names>R.A.</given-names></string-name></person-group><etal>et al</etal> (<year>2004</year>) <article-title>Glide: a new approach for rapid, accurate docking and scoring. 1. method and assessment of docking accuracy</article-title>. <source>J. Med. Chem</source>., <volume>47</volume>, <fpage>1739</fpage>–<lpage>1749</lpage>.<pub-id pub-id-type="pmid">15027865</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ganea</surname><given-names>O.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) GeoMol: torsional geometric generation of molecular 3d conformer ensembles. In: <italic toggle="yes">Advances in Neural Information Processing Systems, Virtual</italic>, Vol. 34.</mixed-citation>
    </ref>
    <ref id="btac731-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>He</surname><given-names>K.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) Deep residual learning for image recognition. In: <italic toggle="yes">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, Nevada, USA.</italic> pp. <fpage>770</fpage>–<lpage>778</lpage>.</mixed-citation>
    </ref>
    <ref id="btac731-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>K.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>DeepPurpose: a deep learning library for drug–target interaction prediction</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>5545</fpage>–<lpage>5547</lpage>.<pub-id pub-id-type="pmid">33275143</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>N.</given-names></string-name></person-group><etal>et al</etal> (<year>2006</year>) <article-title>Benchmarking sets for molecular docking</article-title>. <source>J. Med. Chem</source>., <volume>49</volume>, <fpage>6789</fpage>–<lpage>6801</lpage>.<pub-id pub-id-type="pmid">17154509</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Irwin</surname><given-names>J.J.</given-names></string-name></person-group><etal>et al</etal> (<year>2009</year>) <article-title>Automated docking screens: a feasibility study</article-title>. <source>J. Med. Chem</source>., <volume>52</volume>, <fpage>5712</fpage>–<lpage>5720</lpage>.<pub-id pub-id-type="pmid">19719084</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Jaegle</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2021a</year>) Perceiver: general perception with iterative attention. In: <italic toggle="yes">IInternational Conference on Machine Learning, 18-24 July 2021, Virtual.</italic> pp. <fpage>4651</fpage>–<lpage>4664</lpage>. PMLR.</mixed-citation>
    </ref>
    <ref id="btac731-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Jaegle</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2021b</year>) Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs. In: I<italic toggle="yes">nternational Conference on Learning Representations</italic>.</mixed-citation>
    </ref>
    <ref id="btac731-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jain</surname><given-names>A.N.</given-names></string-name></person-group> (<year>2003</year>) <article-title>Surflex: fully automatic flexible molecular docking using a molecular similarity-based search engine</article-title>. <source>J. Med. Chem</source>., <volume>46</volume>, <fpage>499</fpage>–<lpage>511</lpage>.<pub-id pub-id-type="pmid">12570372</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jain</surname><given-names>A.N.</given-names></string-name></person-group> (<year>2008</year>) <article-title>Bias, reporting, and sharing: computational evaluations of docking methods</article-title>. <source>J. Comput. Aided Mol. Des</source>., <volume>22</volume>, <fpage>201</fpage>–<lpage>212</lpage>.<pub-id pub-id-type="pmid">18075713</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jones</surname><given-names>G.</given-names></string-name></person-group><etal>et al</etal> (<year>1997</year>) <article-title>Development and validation of a genetic algorithm for flexible docking</article-title>. <source>J. Mol. Biol</source>., <volume>267</volume>, <fpage>727</fpage>–<lpage>748</lpage>.<pub-id pub-id-type="pmid">9126849</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jumper</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Highly accurate protein structure prediction with alphafold</article-title>. <source>Nature</source>, <volume>596</volume>, <fpage>583</fpage>–<lpage>589</lpage>.<pub-id pub-id-type="pmid">34265844</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname><given-names>I.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>DeepConv-DTI: prediction of drug-target interactions via deep learning with convolution on protein sequences</article-title>. <source>PLoS Comput. Biol</source>., <volume>15</volume>, <fpage>e1007129</fpage>.<pub-id pub-id-type="pmid">31199797</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>MONN: a multi-objective neural network for predicting compound-protein interactions and affinities</article-title>. <source>Cell Systems</source>, <volume>10</volume>, <fpage>308</fpage>–<lpage>322.e11</lpage>.</mixed-citation>
    </ref>
    <ref id="btac731-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Metz</surname><given-names>J.T.</given-names></string-name></person-group><etal>et al</etal> (<year>2011</year>) <article-title>Navigating the kinome</article-title>. <source>Nat. Chem. Biol</source>., <volume>7</volume>, <fpage>200</fpage>–<lpage>202</lpage>.<pub-id pub-id-type="pmid">21336281</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B26">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mohla</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) FusAtNet: dual attention based spectrospatial multimodal fusion network for hyperspectral and LiDAR classification. In: <italic toggle="yes">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, CVPR 2020, virtually</italic>. pp. <fpage>92</fpage>–<lpage>93</lpage>.</mixed-citation>
    </ref>
    <ref id="btac731-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morgan</surname><given-names>H.L.</given-names></string-name></person-group> (<year>1965</year>) <article-title>The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service</article-title>. <source>J. Chem. Doc</source>., <volume>5</volume>, <fpage>107</fpage>–<lpage>113</lpage>.</mixed-citation>
    </ref>
    <ref id="btac731-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mysinger</surname><given-names>M.M.</given-names></string-name></person-group><etal>et al</etal> (<year>2012</year>) <article-title>Directory of useful decoys, enhanced (DUD-E): better ligands and decoys for better benchmarking</article-title>. <source>J. Med. Chem</source>., <volume>55</volume>, <fpage>6582</fpage>–<lpage>6594</lpage>.<pub-id pub-id-type="pmid">22716043</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nguyen</surname><given-names>T.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>GraphDTA: predicting drug–target binding affinity with graph neural networks</article-title>. <source>Bioinformatics</source>, <volume>37</volume>, <fpage>1140</fpage>–<lpage>1147</lpage>.<pub-id pub-id-type="pmid">33119053</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Öztürk</surname><given-names>H.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>DeepDTA: deep drug–target binding affinity prediction</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>i821</fpage>–<lpage>i829</lpage>.<pub-id pub-id-type="pmid">30423097</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rao</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Evaluating protein transfer learning with tape</article-title>. In; <italic toggle="yes">Advances in Neural Information Processing Systems, Vancouver Convention Center, Vancouver Canada</italic>, Vol. <volume>32</volume>.</mixed-citation>
    </ref>
    <ref id="btac731-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rarey</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>1996</year>) <article-title>A fast flexible docking method using an incremental construction algorithm</article-title>. <source>J. Mol. Biol</source>., <volume>261</volume>, <fpage>470</fpage>–<lpage>489</lpage>.<pub-id pub-id-type="pmid">8780787</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Scarselli</surname><given-names>F.</given-names></string-name></person-group><etal>et al</etal> (<year>2009</year>) <article-title>The graph neural network model</article-title>. <source>IEEE Trans. Neural Netw</source>., <volume>20</volume>, <fpage>61</fpage>–<lpage>80</lpage>.<pub-id pub-id-type="pmid">19068426</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stokes</surname><given-names>J.M.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>A deep learning approach to antibiotic discovery</article-title>. <source>Cell</source>, <volume>180</volume>, <fpage>688</fpage>–<lpage>702.e13</lpage>.<pub-id pub-id-type="pmid">32084340</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tang</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2014</year>) <article-title>Making sense of large-scale kinase inhibitor bioactivity data sets: a comparative and integrative analysis</article-title>. <source>J. Chem. Inf. Model</source>., <volume>54</volume>, <fpage>735</fpage>–<lpage>743</lpage>.<pub-id pub-id-type="pmid">24521231</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>Attention is all you need. In: </article-title><italic toggle="yes">Advances in Neural Information Processing Systems, Long Beach Convention Center (300 E Ocean Blvd, Long Beach, CA 90802, United States)</italic>, Vol. <volume>30</volume>.</mixed-citation>
    </ref>
    <ref id="btac731-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2005</year>) <article-title>The PDBbind database: methodologies and updates</article-title>. <source>J. Med. Chem</source>., <volume>48</volume>, <fpage>4111</fpage>–<lpage>4119</lpage>.<pub-id pub-id-type="pmid">15943484</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weininger</surname><given-names>D.</given-names></string-name></person-group> (<year>1988</year>) <article-title>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules</article-title>. <source>J. Chem. Inf. Model</source>., <volume>28</volume>, <fpage>31</fpage>–<lpage>36</lpage>.</mixed-citation>
    </ref>
    <ref id="btac731-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wouters</surname><given-names>O.J.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Research and development costs of new drugs—reply</article-title>. <source>JAMA</source>, <volume>324</volume>, <fpage>518</fpage>–<lpage>518</lpage>.</mixed-citation>
    </ref>
    <ref id="btac731-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>K.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Analyzing learned molecular representations for property prediction</article-title>. <source>J. Chem. Inf. Model</source>., <volume>59</volume>, <fpage>3370</fpage>–<lpage>3388</lpage>.<pub-id pub-id-type="pmid">31361484</pub-id></mixed-citation>
    </ref>
    <ref id="btac731-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>You</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Graph convolutional policy network for goal-directed molecular graph generation</article-title>. In: <italic toggle="yes">Advances in Neural Information Processing Systems, Palais des Congrès de Montréal, Montréal Canada</italic>, Vol. <volume>31</volume>.</mixed-citation>
    </ref>
    <ref id="btac731-B42">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>You</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) Large batch optimization for deep learning: training Bert in 76 minutes. In: <italic toggle="yes">International Conference on Learning Representations, Virtual Conference.</italic></mixed-citation>
    </ref>
    <ref id="btac731-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhao</surname><given-names>Q.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>HyperAttentionDTI: improving drug–protein interaction prediction by sequence-based deep learning with attention mechanism</article-title>. <source>Bioinformatics</source>, <volume>38</volume>, <fpage>655</fpage>–<lpage>662</lpage>.<pub-id pub-id-type="pmid">34664614</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
