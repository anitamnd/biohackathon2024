<?properties open_access?>
<?subarticle pbio.3000712.r001?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS Biol</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS Biol</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-id journal-id-type="pmc">plosbiol</journal-id>
    <journal-title-group>
      <journal-title>PLoS Biology</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1544-9173</issn>
    <issn pub-type="epub">1545-7885</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7360024</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pbio.3000712</article-id>
    <article-id pub-id-type="publisher-id">PBIOLOGY-D-19-03624</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methods and Resources</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Developmental Biology</subject>
          <subj-group>
            <subject>Life Cycles</subject>
            <subj-group>
              <subject>Larvae</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Human Factors Engineering</subject>
          <subj-group>
            <subject>Man-Computer Interface</subject>
            <subj-group>
              <subject>Virtual Reality</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Architecture</subject>
          <subj-group>
            <subject>User Interfaces</subject>
            <subj-group>
              <subject>Virtual Reality</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Physics</subject>
          <subj-group>
            <subject>Electromagnetic Radiation</subject>
            <subj-group>
              <subject>Light</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Animal Studies</subject>
          <subj-group>
            <subject>Experimental Organism Systems</subject>
            <subj-group>
              <subject>Model Organisms</subject>
              <subj-group>
                <subject>Zebrafish</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Model Organisms</subject>
          <subj-group>
            <subject>Zebrafish</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Animal Studies</subject>
          <subj-group>
            <subject>Experimental Organism Systems</subject>
            <subj-group>
              <subject>Animal Models</subject>
              <subj-group>
                <subject>Zebrafish</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Organisms</subject>
          <subj-group>
            <subject>Eukaryota</subject>
            <subj-group>
              <subject>Animals</subject>
              <subj-group>
                <subject>Vertebrates</subject>
                <subj-group>
                  <subject>Fish</subject>
                  <subj-group>
                    <subject>Osteichthyes</subject>
                    <subj-group>
                      <subject>Zebrafish</subject>
                    </subj-group>
                  </subj-group>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Animal Studies</subject>
          <subj-group>
            <subject>Experimental Organism Systems</subject>
            <subj-group>
              <subject>Model Organisms</subject>
              <subj-group>
                <subject>Drosophila Melanogaster</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Model Organisms</subject>
          <subj-group>
            <subject>Drosophila Melanogaster</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Animal Studies</subject>
          <subj-group>
            <subject>Experimental Organism Systems</subject>
            <subj-group>
              <subject>Animal Models</subject>
              <subj-group>
                <subject>Drosophila Melanogaster</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Organisms</subject>
          <subj-group>
            <subject>Eukaryota</subject>
            <subj-group>
              <subject>Animals</subject>
              <subj-group>
                <subject>Invertebrates</subject>
                <subj-group>
                  <subject>Arthropoda</subject>
                  <subj-group>
                    <subject>Insects</subject>
                    <subj-group>
                      <subject>Drosophila</subject>
                      <subj-group>
                        <subject>Drosophila Melanogaster</subject>
                      </subj-group>
                    </subj-group>
                  </subj-group>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Electronics</subject>
          <subj-group>
            <subject>Diodes</subject>
            <subj-group>
              <subject>Light Emitting Diodes</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Brain Mapping</subject>
            <subj-group>
              <subject>Optogenetics</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Bioassays and Physiological Analysis</subject>
          <subj-group>
            <subject>Neurophysiological Analysis</subject>
            <subj-group>
              <subject>Optogenetics</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Physiology</subject>
          <subj-group>
            <subject>Biological Locomotion</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Physiology</subject>
          <subj-group>
            <subject>Biological Locomotion</subject>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PiVR: An affordable and versatile closed-loop platform to study unrestrained sensorimotor behavior</article-title>
      <alt-title alt-title-type="running-head">A virtual reality system to assist neural circuit analysis</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7570-0162</contrib-id>
        <name>
          <surname>Tadres</surname>
          <given-names>David</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Conceptualization</role>
        <role content-type="https://casrai.org/credit/">Data curation</role>
        <role content-type="https://casrai.org/credit/">Formal analysis</role>
        <role content-type="https://casrai.org/credit/">Investigation</role>
        <role content-type="https://casrai.org/credit/">Methodology</role>
        <role content-type="https://casrai.org/credit/">Resources</role>
        <role content-type="https://casrai.org/credit/">Software</role>
        <role content-type="https://casrai.org/credit/">Validation</role>
        <role content-type="https://casrai.org/credit/">Visualization</role>
        <role content-type="https://casrai.org/credit/">Writing – original draft</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff003">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2267-0262</contrib-id>
        <name>
          <surname>Louis</surname>
          <given-names>Matthieu</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Conceptualization</role>
        <role content-type="https://casrai.org/credit/">Funding acquisition</role>
        <role content-type="https://casrai.org/credit/">Investigation</role>
        <role content-type="https://casrai.org/credit/">Project administration</role>
        <role content-type="https://casrai.org/credit/">Supervision</role>
        <role content-type="https://casrai.org/credit/">Writing – original draft</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff004">
          <sup>4</sup>
        </xref>
        <xref ref-type="corresp" rid="cor001">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Department of Molecular, Cellular, and Developmental Biology, University of California, Santa Barbara, Santa Barbara, California, United States of America</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Neuroscience Research Institute, University of California, Santa Barbara, Santa Barbara, California, United States of America</addr-line>
    </aff>
    <aff id="aff003">
      <label>3</label>
      <addr-line>Institute of Molecular Life Sciences, University of Zurich, Zurich, Switzerland</addr-line>
    </aff>
    <aff id="aff004">
      <label>4</label>
      <addr-line>Department of Physics, University of California, Santa Barbara, Santa Barbara, California, United States of America</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Baden</surname>
          <given-names>Tom</given-names>
        </name>
        <role>Academic Editor</role>
        <xref ref-type="aff" rid="edit1"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>University of Sussex, UNITED KINGDOM</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>mlouis@ucsb.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>14</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>18</volume>
    <issue>7</issue>
    <elocation-id>e3000712</elocation-id>
    <history>
      <date date-type="received">
        <day>16</day>
        <month>12</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>9</day>
        <month>6</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2020 Tadres, Louis</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Tadres, Louis</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pbio.3000712.pdf"/>
    <abstract>
      <p>Tools enabling closed-loop experiments are crucial to delineate causal relationships between the activity of genetically labeled neurons and specific behaviors. We developed the Raspberry Pi Virtual Reality (PiVR) system to conduct closed-loop optogenetic stimulation of neural functions in unrestrained animals. PiVR is an experimental platform that operates at high temporal resolution (70 Hz) with low latencies (&lt;30 milliseconds), while being affordable (&lt;US$500) and easy to build (&lt;6 hours). Through extensive documentation, this tool was designed to be accessible to a wide public, from high school students to professional researchers studying systems neuroscience. We illustrate the functionality of PiVR by focusing on sensory navigation in response to gradients of chemicals (chemotaxis) and light (phototaxis). We show how <italic>Drosophila</italic> adult flies perform negative chemotaxis by modulating their locomotor speed to avoid locations associated with optogenetically evoked bitter taste. In <italic>Drosophila</italic> larvae, we use innate positive chemotaxis to compare behavior elicited by real- and virtual-odor gradients. Finally, we examine how positive phototaxis emerges in zebrafish larvae from the modulation of turning maneuvers to orient in virtual white-light gradients. Besides its application to study chemotaxis and phototaxis, PiVR is a versatile tool designed to bolster efforts to map and to functionally characterize neural circuits.</p>
    </abstract>
    <abstract abstract-type="toc">
      <p>Unravelling the logic of neural circuits underlying behavior requires precise control of sensory inputs based on the animal's behavior. This article presents Raspberry Pi Virtual Reality (PiVR) as an affordable tool designed to enable both academic labs and citizen-science projects to conduct such functional analysis by immersing freely-moving animals in virtual realities.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
            <institution>National Science Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>R25GM067110</award-id>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution>National Institute of Health</institution>
        </funding-source>
        <award-id>R01-NS113048</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2267-0262</contrib-id>
          <name>
            <surname>Louis</surname>
            <given-names>Matthieu</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award003">
        <funding-source>
          <institution>National Science Foundation</institution>
        </funding-source>
        <award-id>PHY-1748958</award-id>
      </award-group>
      <award-group id="award004">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000936</institution-id>
            <institution>Gordon and Betty Moore Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2919.01</award-id>
      </award-group>
      <award-group id="award005">
        <funding-source>
          <institution>National Science Foundation</institution>
        </funding-source>
        <award-id>IOS1523125</award-id>
      </award-group>
      <funding-statement>This work was funded by the National Institute for Health (RO1-NS113048-01) and by the University of California, Santa Barbara (startup funds). This work was also supported by the National Science Foundation under Grant No. NSF PHY-1748958, Grant No. IOS-1523125, IH Grant No. R25GM067110, and the Gordon and Betty Moore Foundation Grant No. 2919.01. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="0"/>
      <page-count count="25"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>All data files and scripts are available from the Dryad database (accession DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25349/D9ZK50">https://doi.org/10.25349/D9ZK50</ext-link>).</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>All data files and scripts are available from the Dryad database (accession DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25349/D9ZK50">https://doi.org/10.25349/D9ZK50</ext-link>).</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>Since the advent of molecular tools to map and manipulate the activity of genetically targeted neurons [<xref rid="pbio.3000712.ref001" ref-type="bibr">1</xref>,<xref rid="pbio.3000712.ref002" ref-type="bibr">2</xref>], a major goal of systems neuroscience has been to unravel the neural computations underlying sensorimotor transformation [<xref rid="pbio.3000712.ref003" ref-type="bibr">3</xref>,<xref rid="pbio.3000712.ref004" ref-type="bibr">4</xref>]. Because of the probabilistic nature of behavior [<xref rid="pbio.3000712.ref005" ref-type="bibr">5</xref>–<xref rid="pbio.3000712.ref007" ref-type="bibr">7</xref>], probing sensorimotor functions requires stimulating an animal with reproducible patterns of sensory input that can be conditioned by the behavioral history of the animal. These conditions can be achieved by immersing animals in virtual realities [<xref rid="pbio.3000712.ref008" ref-type="bibr">8</xref>].</p>
    <p>A virtual reality paradigm consists of a simulated sensory environment perceived by an animal and updated based on a readout of its behavior. Historically, virtual realities have been introduced to study optomotor behavior in tethered flies and bees [<xref rid="pbio.3000712.ref009" ref-type="bibr">9</xref>,<xref rid="pbio.3000712.ref010" ref-type="bibr">10</xref>]. For several decades, sophisticated computer-controlled methods have been developed to produce ever-more-realistic immersive environments. In FreemoVR, freely moving flies avoid collisions with fictive tridimensional obstacles, and zebrafish engage in social interactions with artificial peers [<xref rid="pbio.3000712.ref011" ref-type="bibr">11</xref>]. Spatial learning has been studied in tethered flies moving on a treadmill in 2D environments filled with geometrical objects projected on a visual display [<xref rid="pbio.3000712.ref012" ref-type="bibr">12</xref>]. The same technology has been used to record the neural activity of mice exploring a virtual space [<xref rid="pbio.3000712.ref013" ref-type="bibr">13</xref>].</p>
    <p>Immobilized zebrafish larvae have been studied while hunting virtual prey [<xref rid="pbio.3000712.ref014" ref-type="bibr">14</xref>], adapting their motor responses to fictive changes in intensity of water flow speed [<xref rid="pbio.3000712.ref015" ref-type="bibr">15</xref>] and by virtually aligning themselves with a visual moving stimulus [<xref rid="pbio.3000712.ref016" ref-type="bibr">16</xref>]. Although virtual realities were initially engineered to study visual behavior, they have been generalized to other sensory modalities, such as touch and olfaction. In a treadmill system, navigation has been studied in mice directed by localized stimulations of their whiskers [<xref rid="pbio.3000712.ref017" ref-type="bibr">17</xref>]. The combination of closed-loop tracking and optogenetic stimulation of genetically targeted sensory neurons has enabled a quantitative analysis of chemotaxis in freely moving <italic>Caenorhabditis elegans</italic> and in <italic>Drosophila</italic> larvae immersed in virtual-odor gradients [<xref rid="pbio.3000712.ref018" ref-type="bibr">18</xref>,<xref rid="pbio.3000712.ref019" ref-type="bibr">19</xref>].</p>
    <p>Virtual reality assays aim to reproduce the natural feedback that binds behavior to sensation [<xref rid="pbio.3000712.ref008" ref-type="bibr">8</xref>]. First, the behavior of an animal must be accurately classified in real time. In tethered flying flies, wing beat patterns have been used to deduce turning maneuvers [<xref rid="pbio.3000712.ref020" ref-type="bibr">20</xref>]. Likewise, the movement of a tethered walking fly or a mouse can be inferred from the rotation of the spherical trackball of a treadmill [<xref rid="pbio.3000712.ref013" ref-type="bibr">13</xref>,<xref rid="pbio.3000712.ref021" ref-type="bibr">21</xref>]. In immobilized zebrafish larvae, recordings from the motor neuron axons have been used to infer intended forward swims and turns [<xref rid="pbio.3000712.ref016" ref-type="bibr">16</xref>]. In freely moving <italic>C</italic>. <italic>elegans</italic> and <italic>Drosophila</italic> larvae, the posture of an animal and the position of specific body parts—the head and tail, for instance—have been tracked during motion in 2D arenas [<xref rid="pbio.3000712.ref018" ref-type="bibr">18</xref>,<xref rid="pbio.3000712.ref019" ref-type="bibr">19</xref>]. Variables related to the behavioral readout—most commonly, the spatial coordinates—are then mapped onto a virtual sensory landscape to update the stimulus intensity [<xref rid="pbio.3000712.ref013" ref-type="bibr">13</xref>]. A similar methodology that enables the tracking of individual sensory organs (left and right eyes) has been recently proposed as an open-source package dedicated to zebrafish larvae [<xref rid="pbio.3000712.ref022" ref-type="bibr">22</xref>].</p>
    <p>The effectiveness of the virtual reality paradigm is determined by the overall temporal delay between the animal’s behavior and the update of the stimulus. The shorter this delay, the more authentic the virtual reality is perceived. As a result, the methodology deployed to create efficient virtual realities with closed-loop tracking relies on advanced technology that makes behavioral setups costly and often difficult to adopt by nonspecialists. There is a scope to complement the existing collection of sophisticated assays with tools that are affordable, easily built, and accessible to most laboratories. The fly “ethoscope” proposes a hardware solution that exploits 3D printing to study the behavior of adult flies [<xref rid="pbio.3000712.ref023" ref-type="bibr">23</xref>]. In this system, tracking and behavioral classification are implemented by a portable and low-cost computer, the Raspberry Pi. Although this system can implement a feedback loop between real-time behavioral tracking and stimulus delivery (e.g., the physical rotation of an assay to disrupt sleep), it was not conceived to create refined virtual reality environments using optogenetic stimulations.</p>
    <p>Here, we present the Raspberry Pi Virtual Reality (PiVR) platform enabling the presentation of virtual realities to freely moving small animals. This closed-loop tracker was designed to be accessible to a wide range of researchers by keeping the construction costs low and by maintaining the basic operations simple and customizable to suit the specificities of new experiments. We benchmark the performance of PiVR by studying navigation behavior in the <italic>Drosophila</italic> larva. We then reveal how adult flies adapt their speed of locomotion to avoid areas associated with the activation of bitter-sensing neurons. Finally, we show that zebrafish larvae approach a virtual-light source by modifying their turn angle in response to temporal changes in light intensity.</p>
  </sec>
  <sec sec-type="results" id="sec002">
    <title>Results</title>
    <sec id="sec003">
      <title>PiVR permits high-performance closed-loop tracking and optogenetic stimulations</title>
      <p>The PiVR system enables high-resolution, optogenetic, closed-loop experiments with small, freely moving animals. In its standard configuration, PiVR is composed of a behavioral arena, a camera, a Raspberry Pi microcomputer, a light-emitting diode (LED) controller, and a touch screen (<xref ref-type="fig" rid="pbio.3000712.g001">Fig 1A</xref>). The platform is controlled via a user-friendly graphical interface (<xref ref-type="fig" rid="pbio.3000712.g001">Fig 1B</xref>). Given that PiVR does not require the use of an external computer, the material for one setup amounts to less than US$500, with the construction costs decreasing to about US$350 when several units are built in parallel (<xref ref-type="supplementary-material" rid="pbio.3000712.s017">S1 Table</xref>). As shown in <xref ref-type="supplementary-material" rid="pbio.3000712.s018">S2 Table</xref>, PiVR is significantly cheaper than published alternatives that are capable of operating at equally high frame rates [<xref rid="pbio.3000712.ref011" ref-type="bibr">11</xref>,<xref rid="pbio.3000712.ref022" ref-type="bibr">22</xref>]. In spite of its affordability, PiVR runs a customized software (<xref ref-type="supplementary-material" rid="pbio.3000712.s001">S1</xref>–<xref ref-type="supplementary-material" rid="pbio.3000712.s006">S6</xref> Figs) that automatically identifies semitransparent animals such as <italic>Drosophila</italic> larvae behaving in a static background (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2</xref>) and that monitors movement at a frame rate sufficient to accurately track rapidly moving animals such as walking adult flies and zebrafish larvae (Figs <xref ref-type="fig" rid="pbio.3000712.g003">3</xref> and <xref ref-type="fig" rid="pbio.3000712.g004">4</xref>).</p>
      <fig id="pbio.3000712.g001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.3000712.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>Virtual realities created by PiVR.</title>
          <p>(A) Picture of the standard PiVR setup. The animal is placed on the light diffuser and illuminated from below using infrared LEDs and recorded from above. The Raspberry Pi computer and the LED controller are attached to the touch screen, which permits the user to interface with the PiVR setup. (B) Screenshot of the GUI while running a virtual reality experiment. The GUI has been designed to be intuitive and easy to use while presenting all important experimental parameters that can be modified. (C) Virtual realities are created by updating the intensity of a homogeneous light background based on the current position of a tracked animal mapped onto a predefined landscape shown at the center. (Center) Predefined virtual gradient with a Gaussian geometry. (Left) Trajectory of an unconstrained animal moving in the physical arena. (Right) The graph indicates the time course of the light intensity experienced by the animal during the trajectory displayed in the left panel. Depending on the position of the animal in the virtual-light gradient, the LEDs are turned off (t = 1) or turned on at an intermediate (t = 2) or maximum intensity (t = 3). GUI, graphical user interface; LED, light-emitting diode; PiVR, Raspberry Pi Virtual Reality.</p>
        </caption>
        <graphic xlink:href="pbio.3000712.g001"/>
      </fig>
      <fig id="pbio.3000712.g002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.3000712.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Benchmarking PiVR performance by eliciting larval chemotaxis in virtual-odor gradients.</title>
          <p>(A) <italic>Drosophila</italic> larva with a pair of single <italic>Or42a</italic>-functional OSNs (red dots). Illustration of the identification of different body parts of a moving larva by PiVR. (B) Illustrative trajectory of a larva in a Gaussian virtual-odor gradient elicited by light stimulation. Arrowheads and numbers indicate lateral head movements (casts), and the time points are congruent with the arrowheads shown in (C). Panel D shows the behavior of <italic>Drosophila</italic> larvae directed by <italic>Or42a</italic> OSNs in a gradient of IAA (green color). (E–F) Behavior of larvae expressing the light-gated ion channel CsChrimson in the <italic>Or42a</italic> OSNs evoking a virtual-odor gradient. In panel E, the virtual-odor gradient (red) has a geometry similar to the real-odor gradient (green) presented in (D). The “volcano” virtual-odor landscape presented in (F) highlights that the information conveyed by the <italic>Or42a</italic> OSN alone is sufficient for larvae to chemotax with high accuracy along the rim of the gradient. Thick lines in panels Diii, Eiii, and Fiii indicate the median distances to the source, and the light traces indicate individual trials. All data used to create this figure are available from <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25349/D9ZK50">https://doi.org/10.25349/D9ZK50</ext-link>. IAA, isoamyl acetate; LED, light-emitting diode; Or42a, odorant receptor 42a; OSN, olfactory sensory neuron; PiVR, Raspberry Pi Virtual Reality; Sens. exp., sensory experience; VR, virtual reality.</p>
        </caption>
        <graphic xlink:href="pbio.3000712.g002"/>
      </fig>
      <fig id="pbio.3000712.g003" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.3000712.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Adult fruit flies avoid activation of bitter-sensing neurons by modulating their locomotion speed.</title>
          <p>(A) Adult <italic>Drosophila</italic> expressing CsChrimson in <italic>Gr66a</italic> bitter-sensing neurons (red circles). Illustration of the identification of different body parts of a moving fly by PiVR. (B) Illustrative trajectories of flies in a virtual checkerboard pattern and the corresponding ethogram. Flies were behaving in a petri dish. (D) The ethogram reports the time spent by individual animals (rows) in the dark (white) and lit (red) squares. Panel C displays a quantification of the avoidance of virtual bitter taste through a preference index: <inline-formula id="pbio.3000712.e001"><alternatives><graphic id="pbio.3000712.e001g" xlink:href="pbio.3000712.e001"/><mml:math id="M1"><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">F</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">F</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>, where T is the time spent on the ON or OFF quadrants (Mann–Whitney U test, <italic>p &lt;</italic> 0.001). (E) Median locomotion speeds of individual animals as a function of the exposure to light. (F) Quantification of locomotion speeds across experimental conditions (Dunn’s multiple comparisons test, different letters indicate at least <italic>p &lt;</italic> 0.01). Statistical procedures are detailed in the Methods section. Statistical significances are indicated with lowercase letters. All data used to create this figure are available from <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25349/D9ZK50">https://doi.org/10.25349/D9ZK50</ext-link>. Gr66a, gustatory receptor 66a; PiVR, Raspberry Pi Virtual Reality.</p>
        </caption>
        <graphic xlink:href="pbio.3000712.g003"/>
      </fig>
      <fig id="pbio.3000712.g004" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pbio.3000712.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Zebrafish larvae adapt their turn dynamics to stay close to a virtual-light source.</title>
          <p>(A) Illustration of the identification of a moving zebrafish larva by PiVR. (B) Illustrative trajectory of a zebrafish larva in a virtual-light gradient having a Gaussian geometry. Panel C displays the time course of the speed and the white-light intensity during that trajectory shown in panel B. Yellow vertical lines indicate automatically detected bouts. (D) Trajectories of 13 fish tested in virtual-light gradient (left) and 11 fish in control (right). Red circles indicate 10-, 20-, 30-, and 40-mm distances to the center of the virtual-light source (see <xref ref-type="sec" rid="sec011">Methods</xref>). (E) Thick lines indicate the time courses of the median distances to the virtual-light source. The light lines indicate individual trials. (F) Illustration of the discretization of a trajectory segment into bouts: yellow vertical lines indicate the position at which the animal stops, reorients, and starts the next bout. Black dashed lines indicate movement of the fish. The turn angle (θ) and change in light intensity (ΔI) are calculated for every pair of consecutive bouts (see <xref ref-type="sec" rid="sec011">Methods</xref>). The bottom of panel F illustrates a swim bout oriented up-gradient (purple, ΔI &gt; 0) and down-gradient (green, ΔI &lt; 0). (G) Relationship between θ and I during the previous bout (independent two-sample <italic>t</italic> test, different letters indicate <italic>p &lt;</italic> 0.001). (H) Turn angles θ of the virtual reality condition are grouped according to negative (green) and positive (magenta) intensity experienced in the previous bout (<italic>t</italic> test for paired samples, different letters indicate at least <italic>p &lt;</italic> 0.05). (I) The turn index (β) is calculated from the average reorientation accuracy (β<sub>i</sub>) of the animal relative to the virtual-light source at the onset of each swim bout. (J) Turn index (β) as a function of stimulus intensity (Mann–Whitney U test, all groups <italic>p &gt;</italic> 0.05). All reported statistical significances are Bonferroni corrected and indicated with lowercase letters. Statistical procedures are detailed in the Methods section. All data used to create this figure are available from <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25349/D9ZK50">https://doi.org/10.25349/D9ZK50</ext-link>. PiVR, Raspberry Pi Virtual Reality; VR, virtual reality.</p>
        </caption>
        <graphic xlink:href="pbio.3000712.g004"/>
      </fig>
      <p>We characterized the overall latency of PiVR in two ways. First, by measuring the following three parameters: (1) image acquisition time, (2) image processing time, and (3) the time taken for commands issued by the Raspberry Pi to be actuated by the LED hardware (<xref ref-type="supplementary-material" rid="pbio.3000712.s001">S1A–S1D Fig</xref>). We find that the image processing time is the main time-consuming step. Secondly, we measured the total latency between the moment an image starts being recorded and the update of the intensity of the LED system based only on the analysis of that image. We find that the total latency is shorter than 30 milliseconds (<xref ref-type="supplementary-material" rid="pbio.3000712.s001">S1E–S1G Fig</xref>). Thus, PiVR is suited to perform online tracking of small animals at a frame rate of up to 70 Hz with a lag shorter than three frames and an accuracy suitable to create virtual olfactory realities in <italic>Drosophila</italic> larvae [<xref rid="pbio.3000712.ref018" ref-type="bibr">18</xref>] and virtual visual realities in walking adult flies [<xref rid="pbio.3000712.ref021" ref-type="bibr">21</xref>].</p>
      <p>The PiVR software implements image acquisition, object tracking, and the update of background illumination for optogenetic stimulation. It is free, fully open-source, and written in the programming language Python. At the beginning of each experiment, an autodetect algorithm separates the moving object—the animal—from the background (<xref ref-type="supplementary-material" rid="pbio.3000712.s003">S3 Fig</xref>). During the rest of the experiment, the tracking algorithm operates based on a principle of local background subtraction to achieve high frame rates (<xref ref-type="supplementary-material" rid="pbio.3000712.s004">S4 Fig</xref>). Besides locating the position of the animal’s centroid, PiVR uses a Hungarian algorithm to tell apart the head from the tail positions (<xref ref-type="supplementary-material" rid="pbio.3000712.s005">S5 Fig</xref>). For applications involving off-line tracking with a separate software [<xref rid="pbio.3000712.ref024" ref-type="bibr">24</xref>,<xref rid="pbio.3000712.ref025" ref-type="bibr">25</xref>], the online tracking module of PiVR can be disabled to record videos at 90 Hz in an open-loop mode.</p>
      <p>PiVR has been designed to create virtual realities by updating the intensity of a homogeneous stimulation backlight based on the current position of a tracked animal (<xref ref-type="fig" rid="pbio.3000712.g001">Fig 1C</xref>, left panel) relative to a preset landscape (<xref ref-type="fig" rid="pbio.3000712.g001">Fig 1C</xref>, middle panel, <xref ref-type="supplementary-material" rid="pbio.3000712.s009">S1 Movie</xref>). Virtual sensory realities are generated by optogenetically activating sensory neurons of the peripheral nervous system [<xref rid="pbio.3000712.ref001" ref-type="bibr">1</xref>]. In the present study, we focus on applications involving CsChrimson because the red activation spectrum of this light-gated ion channel is largely invisible to <italic>Drosophila</italic> [<xref rid="pbio.3000712.ref026" ref-type="bibr">26</xref>]. Depending on the light-intensity range necessary to stimulate specific neurons, PiVR features a light pad emitting stimulation light at low-to-medium (2 μW/mm<sup>2</sup>) or high (22 μW/mm<sup>2</sup>) intensities (<xref ref-type="supplementary-material" rid="pbio.3000712.s002">S2C and S2D Fig</xref>). A key advantage of the closed-loop methodology of PiVR is that it permits the creation of virtual realities with arbitrary properties free of the physical constraints of real stimuli. In <xref ref-type="fig" rid="pbio.3000712.g002">Fig 2Fi</xref>, we illustrate the use of PiVR by immersing <italic>Drosophila</italic> larvae in a virtual-odor gradient that has the shape of a volcano—a geometry that challenges the sensorimotor responses of larvae in a predictable way [<xref rid="pbio.3000712.ref018" ref-type="bibr">18</xref>].</p>
      <p>The possibility to 3D print components that once required sophisticated machining has empowered the “maker movement” in our scientific community [<xref rid="pbio.3000712.ref027" ref-type="bibr">27</xref>]. Inspired by this philosophy, PiVR is built from hardware parts that are 3D printed. Thus, the modular design of the setup can be readily adapted to accommodate the experimental needs of traditional model organisms (larvae, adult flies, and zebrafish) as well as less conventional small animals (<xref ref-type="supplementary-material" rid="pbio.3000712.s002">S2</xref> and <xref ref-type="supplementary-material" rid="pbio.3000712.s006">S6</xref> Figs). For example, we adapted PiVR to acquire movies of 10 fruit fly larvae simultaneously with an image quality sufficient to permit off-line tracking of multiple-animal tracking with the idtracker.ai software [<xref rid="pbio.3000712.ref025" ref-type="bibr">25</xref>]. To achieve this, we modified the design of the arena to allow illumination from the side instead of bottom (<xref ref-type="supplementary-material" rid="pbio.3000712.s002">S2B Fig</xref>). This adaptation of the illumination setup was necessary to enhance contrast in the appearance of individual larvae for idtracker.ai to detect idiosyncratic differences between larvae (<xref ref-type="supplementary-material" rid="pbio.3000712.s002">S2Bi Fig</xref>). The versatility of PiVR was also illustrated by tracking various arthropods and a vertebrate with diverse body structures and locomotor properties (Figs <xref ref-type="fig" rid="pbio.3000712.g002">2</xref>–<xref ref-type="fig" rid="pbio.3000712.g004">4</xref>, <xref ref-type="supplementary-material" rid="pbio.3000712.s006">S6 Fig</xref>).</p>
    </sec>
    <sec id="sec004">
      <title>Benchmarking PiVR performances by eliciting larval chemotaxis in virtual-odor gradients</title>
      <p>To benchmark the performances of PiVR, we turned to the navigation behavior evoked by odor gradients (chemotaxis) in the <italic>Drosophila</italic> larva [<xref rid="pbio.3000712.ref028" ref-type="bibr">28</xref>,<xref rid="pbio.3000712.ref029" ref-type="bibr">29</xref>]. Larval chemotaxis relies on a set of well-characterized sensorimotor rules [<xref rid="pbio.3000712.ref030" ref-type="bibr">30</xref>]. To ascend an attractive odor gradient, larvae modulate the alternation of relatively straight runs and reorientation maneuvers (turns). Stops are predominantly triggered when the larva undergoes negative changes in odor concentration during down-gradient runs. Following a stop, turning is directed toward the gradient through an active sampling process that involves lateral head movements (head casts). A key advantage of the larva as a model organism for chemotaxis is that robust orientation responses can be directed by a functionally reduced olfactory system. The odorant receptor gene 42a (<italic>Or42a</italic>) is expressed in a pair of bilaterally symmetric olfactory sensory neurons (OSNs) [<xref rid="pbio.3000712.ref031" ref-type="bibr">31</xref>,<xref rid="pbio.3000712.ref032" ref-type="bibr">32</xref>] that are sufficient to direct chemotaxis [<xref rid="pbio.3000712.ref033" ref-type="bibr">33</xref>]. We exploited this property to compare reorientation performances elicited by real- and virtual-odor stimulations of the <italic>Or42a</italic> OSNs.</p>
      <p>We started by applying the computer-vision algorithm of PiVR to track larvae with a single functional <italic>Or42a</italic>-expressing OSN (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2A</xref>). Individual animals were introduced in a rectangular arena comprising a gradient of isoamyl acetate (IAA) at its center (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2D</xref>). Once exposed to the odor gradient, <italic>Or42a</italic>-functional larvae quickly identified the position of the odor source, and they remained in the source’s vicinity (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2D</xref> and <xref ref-type="supplementary-material" rid="pbio.3000712.s010">S2 Movie</xref>). The trajectories of consecutively tested larvae were analyzed by quantifying the time course of the distance between the head of the larva and the center of the odor source. The navigation of the <italic>Or42a</italic>-functional larvae yielded an average distance to the source significantly lower than that observed in the presence of the solvent alone (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2Diii</xref>). This result is consistent with the behavior of wild-type larvae in response to attractive odors [<xref rid="pbio.3000712.ref034" ref-type="bibr">34</xref>]. It establishes that PiVR can automatically detect and accurately track animals in real time.</p>
      <p>Next, we tested the ability of PiVR to create virtual olfactory realities by optogenetically stimulating the larval olfactory system. In past work, robust chemotaxis was elicited in light gradients by expressing the blue-light-gated ion channel channelrhodopsin in the <italic>Or42a</italic>-expressing OSN of blind larvae [<xref rid="pbio.3000712.ref018" ref-type="bibr">18</xref>]. In these experiments, the light stimulus was delivered as a point of LED light kept focused on the larva. Using a closed-loop paradigm, the intensity of the LED light was updated at a rate of 30 Hz, based on the position of the larva’s head mapped onto a landscapes predefined by the user [<xref rid="pbio.3000712.ref018" ref-type="bibr">18</xref>]. PiVR was built on the same principle with the following modifications: (1) the spatial resolution of PiVR was reduced because the field of view of the camera captures the whole arena and not just the larva, (2) optogenetic stimulation was achieved through homogeneous background illumination instead of a light spot that must follow the larva, and (3) we favored the red-light-gated ion channel CsChrimson [<xref rid="pbio.3000712.ref026" ref-type="bibr">26</xref>] over channelrhodopsin to minimize the innate photophobic response of larvae to the blue-light range [<xref rid="pbio.3000712.ref035" ref-type="bibr">35</xref>].</p>
      <p>The simplified hardware design of PiVR produced precise tracking of the head position of a larva exposed to a fictive light gradient (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2B</xref>). The spatiotemporal resolution of the tracking is illustrated in <xref ref-type="fig" rid="pbio.3000712.g002">Fig 2C</xref>, in which surges in head speed are associated with scanning movements of the head on a timescale shorter than 500 milliseconds [<xref rid="pbio.3000712.ref036" ref-type="bibr">36</xref>]. Head “casts” induced transient changes in light intensity <italic>I</italic>(<italic>t</italic>) [<xref rid="pbio.3000712.ref018" ref-type="bibr">18</xref>]. These changes in stimulus intensity correspond to spikes in the relative sensory experience of the larva (<inline-formula id="pbio.3000712.e002"><alternatives><graphic xlink:href="pbio.3000712.e002.jpg" id="pbio.3000712.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mfrac><mml:mo>*</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>) (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2C</xref>, arrowheads in purple trace) [<xref rid="pbio.3000712.ref030" ref-type="bibr">30</xref>]. The tracking resolution of PiVR enabled recording periodic patterns in tail speed (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2C</xref>, top green trace) that reflect consecutive cycles of acceleration/deceleration during forward peristalsis [<xref rid="pbio.3000712.ref037" ref-type="bibr">37</xref>]. <italic>Or42a</italic>-functional larvae displayed strong chemotaxis in response to a point-source virtual-odor gradient (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2E</xref> and <xref ref-type="supplementary-material" rid="pbio.3000712.s011">S3 Movie</xref>) with a level of attraction comparable to the behavior evoked by a real-odor gradient (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2D</xref>). Moreover, PiVR recapitulated the meandering trajectories along the rim of a volcano-shaped virtual-odor gradient (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2F</xref> and <xref ref-type="supplementary-material" rid="pbio.3000712.s012">S4 Movie</xref>) [<xref rid="pbio.3000712.ref018" ref-type="bibr">18</xref>]. Together, the results of <xref ref-type="fig" rid="pbio.3000712.g002">Fig 2</xref> and <xref ref-type="supplementary-material" rid="pbio.3000712.s007">S7 Fig</xref> validate that PiVR has the tracking accuracy and closed-loop performances necessary to elicit genuine navigation in virtual-odor gradients.</p>
    </sec>
    <sec id="sec005">
      <title>Adult flies avoid activation of bitter-sensing neurons by modulating their locomotion speed</title>
      <p>After having established the capability of PiVR to create virtual realities in <italic>Drosophila</italic> larvae, we sought to generalize the application of this tool to other small model organisms. Adult <italic>Drosophila</italic> are covered by a thick and opaque cuticle. Consequently, activating light-gated ion channels expressed in the sensory neurons of adult flies requires higher light intensities than in semitransparent larvae [<xref rid="pbio.3000712.ref006" ref-type="bibr">6</xref>,<xref rid="pbio.3000712.ref026" ref-type="bibr">26</xref>]. The background illumination system of PiVR was modified to deliver light intensities as high as 50 μW/mm<sup>2</sup> to penetrate the adult-fly cuticle [<xref rid="pbio.3000712.ref038" ref-type="bibr">38</xref>]. Despite a 10-fold increase in locomotion speed between adult flies and larvae (peak speed of 12 mm/s and 1.6 mm/s, respectively), PiVR accurately monitored the motion of adult fruit flies for the entire duration of 5-minute trials (<xref ref-type="fig" rid="pbio.3000712.g003">Fig 3A and 3B</xref>). We turn to gustation to test the ability of PiVR to evoke orientation behavior in adult flies stimulated by virtual chemical gradients.</p>
      <p><italic>Drosophila</italic> demonstrates innate strong aversion to bitter taste [<xref rid="pbio.3000712.ref039" ref-type="bibr">39</xref>]. This behavior is mediated by a set of sensory neurons expressing the gustatory receptor gene 66a (<italic>Gr66a</italic>). Optogenetic activation of the <italic>Gr66a</italic>-expressing neurons alone is sufficient to elicit aversive responses [<xref rid="pbio.3000712.ref038" ref-type="bibr">38</xref>]. Using the closed-loop tracking capabilities of PiVR (<xref ref-type="fig" rid="pbio.3000712.g003">Fig 3A</xref>), we examined taste-driven responses of flies expressing the red-light-gated ion channel CsChrimson in their <italic>Gr66a</italic>-expressing neurons. Because we reasoned that navigation in response to taste might be less directed than navigation in response to airborne odors, we presented flies with a 2D landscape emulating a checkerboard (<xref ref-type="fig" rid="pbio.3000712.g003">Fig 3B</xref>). In this virtual checkerboard, quadrants were associated with either virtual bitter taste (light “ON”) or no taste (light “OFF”). Flies adapted their motion to avoid squares paired with virtual bitter taste (<xref ref-type="fig" rid="pbio.3000712.g003">Fig 3B–3D</xref> and <xref ref-type="supplementary-material" rid="pbio.3000712.s013">S5 Movie</xref>). This result generalized the field of application of PiVR to fast-moving small animals, such as walking adult flies.</p>
      <p>To determine how flies actively avoid being exposed to bitter-tasting squares, we interrogated the spatial trajectories recorded by PiVR (<xref ref-type="fig" rid="pbio.3000712.g003">Fig 3B</xref>) and correlated stimulus input with behavioral output [<xref rid="pbio.3000712.ref040" ref-type="bibr">40</xref>]. This quantitative analysis of the relationship between stimulus dynamics and behavior highlighted that flies modulate their locomotion speed in response to excitation of their bitter-tasting neurons. When flies were located in a lit square eliciting virtual bitter taste, they moved significantly faster than when located in a dark square with no bitter taste. When flies encountered sensory relief in a dark square, they frequently stopped (<xref ref-type="fig" rid="pbio.3000712.g003">Fig 3E and 3F</xref>). In summary, our results establish that PiVR is suitable to track and immerse adult flies in virtual sensory realities. Moreover, computational quantification of behavioral data produced by PiVR suggests that flies avoid bitter tastes by modulating their locomotion speed to avoid staying exposed to virtual bitter taste in the illuminated squares [<xref rid="pbio.3000712.ref041" ref-type="bibr">41</xref>,<xref rid="pbio.3000712.ref042" ref-type="bibr">42</xref>]. The contribution of other orientation mechanisms that integrate spatial information is left to be examined in future work. For instance, it is possible that flies implement directed turns upon their entry in a bitter-tasting quadrant (<xref ref-type="fig" rid="pbio.3000712.g003">Fig 3B</xref> and <xref ref-type="supplementary-material" rid="pbio.3000712.s013">S5 Movie</xref>).</p>
    </sec>
    <sec id="sec006">
      <title>Zebrafish larvae adapt their turn amplitude to stay close to a virtual-light source</title>
      <p>Because of its transparency and amenability to molecular genetics, the zebrafish <italic>Danio rerio</italic> has emerged as a tractable model system to study how sensory representations and sensorimotor transformations arise from the activity in neural ensembles in vertebrates [<xref rid="pbio.3000712.ref043" ref-type="bibr">43</xref>,<xref rid="pbio.3000712.ref044" ref-type="bibr">44</xref>]. Zebrafish are innately attracted by real sources of white light [<xref rid="pbio.3000712.ref045" ref-type="bibr">45</xref>]. Here, we show that PiVR is suitable to study the organization of orientation behavior of zebrafish larvae immersed in a virtual 2D light gradient (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4A and 4B</xref>). Already at the larval stage, individuals are capable of staying confined to virtual disks of white light [<xref rid="pbio.3000712.ref046" ref-type="bibr">46</xref>]. In spite of the fact that 5-days-postfertilization (dpf) zebrafish larvae stay in constant motion, tracking with PiVR established that larvae have the navigational capabilities to stay near the peak of a virtual white-light gradient (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4B</xref> and <xref ref-type="supplementary-material" rid="pbio.3000712.s014">S6 Movie</xref>) by constantly returning to this position for the duration of the entire trial (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4D and 4E</xref>, <xref ref-type="supplementary-material" rid="pbio.3000712.s008">S8A Fig</xref>).</p>
      <p>The elementary motor patterns (actions) underlying the behavioral repertoire of zebrafish larvae can be decomposed into stops and slow and rapid swims [<xref rid="pbio.3000712.ref047" ref-type="bibr">47</xref>]. By analyzing the time series of the centroid speed recorded by PiVR, we observed periodic increases in swim speed (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4C</xref>). These episodes correspond to bursts, or “bouts,” of swim [<xref rid="pbio.3000712.ref048" ref-type="bibr">48</xref>]. To examine the orientation strategy used by zebrafish larvae, we discretized trajectories into bouts. As described in previous work [<xref rid="pbio.3000712.ref047" ref-type="bibr">47</xref>], each bout was found to be approximately straight (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4B</xref>, cyan segments comprised by the circles). At low light intensities, significant reorientation occurred between consecutive swim bouts compared with the controls (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4G</xref>).</p>
      <p>Given that the virtual landscape produced by PiVR resulted from a temporal update of the intensity of isotropic light stimulation, we can rule out the detection of binocular differences in stimulus intensity [<xref rid="pbio.3000712.ref049" ref-type="bibr">49</xref>]. Thus, reorientation in the virtual-light landscape of <xref ref-type="fig" rid="pbio.3000712.g004">Fig 4</xref> could only result from the detection of temporal changes in light intensity. Using the data set recorded with PiVR, we conducted a correlative analysis between the stimulus history and the reorientation maneuvers to define the visual features eliciting an increase in turning at low light intensity. More specifically, we analyzed the turn angle as a function of the light intensity for bouts associated with positive (up-gradient) and negative (down-gradient) changes in light intensity. When zebrafish larvae moved up-gradient, the turn angle was not modulated by the absolute intensity of the stimulus (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4H</xref>, purple). By contrast, the turn rate increased for swim bouts oriented down-gradient at low stimulus intensity (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4H</xref>, green). Therefore, we conclude that the rate of turning of zebrafish larvae is determined by a combination of the absolute light intensity and the sign of change in stimulus intensity.</p>
      <p>Given the ability of zebrafish larvae to efficiently return to the peak of a light gradient (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4D</xref>), we asked whether zebrafish larvae can also bias their turns toward the light source. To this end, we defined the turn index (β) to quantify the percentage of turns directed toward the light gradient (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4I</xref> and Methods). This metric leads us to conclude that zebrafish larvae do not bias their turns toward the source more often than the control (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4J</xref>). In summary, our results demonstrate that PiVR can track zebrafish larvae at a sufficiently high spatiotemporal resolution to characterize individual swim bouts. We show that zebrafish achieve positive phototaxis by increasing the amplitude of their turns when they are moving down-gradient and experiencing low light intensities. This result is consistent with a recent in-depth study of the sensorimotor strategy controlling zebrafish phototaxis in a virtual reality paradigm [<xref rid="pbio.3000712.ref050" ref-type="bibr">50</xref>]. We conclude that phototaxis in zebrafish is at least partially controlled by an increase in the amplitude of turns when two conditions are met: (1) the animal must detect a negative change in light intensity, and (2) the absolute light intensity must be low enough. The combination of the previous two conditions appears sufficient to generate positive phototaxis, even in the absence of turning biases toward the light gradient.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec007">
    <title>Discussion</title>
    <p>Anyone seeking to unravel the neural logic underlying a navigation process—whether it is the response of a cell to a morphogen gradient or the flight of a seabird guided by the Earth’s magnetic field—faces the need to characterize the basic orientation strategy before speculating about its molecular and cellular underpinnings. PiVR is a versatile closed-loop experimental platform devised to create light-based virtual sensory realities by tracking the motion of unconstrained small animals subjected to a predefined model of the sensory environment (<xref ref-type="fig" rid="pbio.3000712.g001">Fig 1</xref>). It was created to assist the study of orientation behavior and neural circuit functions by scholars who might not have extensive background in programming or instrumentation to customize existing tools.</p>
    <sec id="sec008">
      <title>An experimental platform that is inexpensive and customizable</title>
      <p>Prior to the commercialization of consumer-oriented 3D printers and cheap microcomputers such as the Raspberry Pi, virtual reality paradigms necessitated using custom setups that cost several thousand or even one hundred thousand dollars [<xref rid="pbio.3000712.ref011" ref-type="bibr">11</xref>,<xref rid="pbio.3000712.ref013" ref-type="bibr">13</xref>,<xref rid="pbio.3000712.ref018" ref-type="bibr">18</xref>,<xref rid="pbio.3000712.ref019" ref-type="bibr">19</xref>]. PiVR is an affordable (&lt; US$500) alternative that enables laboratories without advanced technical expertise to conduct high-throughput virtual reality experiments. The procedure to build PiVR is visually illustrated in <xref ref-type="supplementary-material" rid="pbio.3000712.s015">S7 Movie</xref>. All construction steps are intended to be tractable by virtually any users who have access to a soldering station and a 3D printer. A detailed step-by-step protocol is available on a dedicated website (<ext-link ext-link-type="uri" xlink:href="http://www.pivr.org/">www.pivr.org</ext-link>).</p>
      <p>The creation of realistic immersive virtual realities critically depends on the update frequency and the update latency of the closed-loop system. The maximal update frequency corresponds to the maximally sustained frame rate that the system can support. The update latency is the latency between an action of the tested subject and the implementation of a change in the virtual reality environment. In normal working conditions, PiVR can be used with an update frequency of up to 70 Hz with a latency below 30 milliseconds (<xref ref-type="supplementary-material" rid="pbio.3000712.s002">S2 Fig</xref>). These characteristics are similar to those routinely used to test optomotor responses with visual display streaming during walking behavior in insects [<xref rid="pbio.3000712.ref021" ref-type="bibr">21</xref>], thereby ensuring the suitability of PiVR for a wide range of applications.</p>
      <p>Different optogenetic tools require excitation at different wavelengths ranging from blue to deep red [<xref rid="pbio.3000712.ref026" ref-type="bibr">26</xref>,<xref rid="pbio.3000712.ref051" ref-type="bibr">51</xref>,<xref rid="pbio.3000712.ref052" ref-type="bibr">52</xref>]. The modularity of PiVR enables the experimenter to customize the illumination system to any wavelength range. Additionally, different animals demand distinct levels of light intensities to ensure adequate light penetration in transparent and opaque tissues. Although 5 μW/mm<sup>2</sup> of red light had been used to activate neurons located in the leg segments of adult flies with CsChrimson [<xref rid="pbio.3000712.ref038" ref-type="bibr">38</xref>], 1 μW/mm<sup>2</sup> is enough to activate OSNs of the semitransparent <italic>Drosophila</italic> larva (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2</xref>). In its standard version, PiVR can emit red-light intensities as high as 2 μW/mm<sup>2</sup> and white-light intensities up to 6,800 Lux—a range sufficient for most applications in transparent animals (Figs <xref ref-type="fig" rid="pbio.3000712.g002">2</xref> and <xref ref-type="fig" rid="pbio.3000712.g004">4</xref>). For animals with an opaque cuticle, we devised a higher-power version of the backlight illumination system that delivers intensities up to 22 μW/mm<sup>2</sup> (525 nm) and 50 μW/mm<sup>2</sup> (625 nm) (<xref ref-type="fig" rid="pbio.3000712.g003">Fig 3</xref> and <xref ref-type="supplementary-material" rid="pbio.3000712.s001">S1D Fig</xref>). Given that an illumination of 50 μW/mm<sup>2</sup> is approaching the LED eye safety limits (International Electrotechnical Commission: 62471), it is unlikely that experimenters will want to exceed this range for common applications in the lab.</p>
    </sec>
    <sec id="sec009">
      <title>Exploring orientation behavior through virtual reality paradigms</title>
      <p>By capitalizing on the latest development of molecular genetics and bioengineering [<xref rid="pbio.3000712.ref001" ref-type="bibr">1</xref>], virtual sensory stimuli can be created by expressing optogenetic tools in targeted neurons of the peripheral nervous system of an animal. In the present study, PiVR was used to immerse <italic>Drosophila</italic> in virtual chemosensory gradients (Figs <xref ref-type="fig" rid="pbio.3000712.g002">2</xref> and <xref ref-type="fig" rid="pbio.3000712.g003">3</xref>). In a first application, we stimulated one OSN of <italic>Drosophila</italic> larvae with CsChrimson. The attractive search behavior elicited by a single source of a real odor (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2Di</xref>) was reproduced in an exponential light gradient (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2Ei</xref>). To reveal the precision with which larvae orient their turns toward the gradient, larvae were tested in a virtual-odor landscape with a volcano shape (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2Fi</xref>).</p>
      <p>Adult flies move approximately 10 times faster than larvae. We established the ability of PiVR to immerse freely moving flies in a virtual bitter-taste gradient (<xref ref-type="fig" rid="pbio.3000712.g003">Fig 3B</xref>). Unlike the attractive responses elicited by appetitive virtual odors in the larva (positive chemotaxis), bitter taste produced strong aversive behavior (<xref ref-type="supplementary-material" rid="pbio.3000712.s013">S5 Movie</xref>). A correlative analysis of data recorded with PiVR revealed that the aversive behavior of adult flies is at least partly conditioned by a modulation of the animal’s locomotor speed: random search is enhanced upon detection of (virtual) bitter, whereas locomotion is drastically reduced upon sensory relief. As illustrated in <xref ref-type="fig" rid="pbio.3000712.g003">Fig 3</xref>, PiVR offers a framework to explore the existence of other mechanisms contributing to the orientation of flies experiencing taste gradients. This approach adds to earlier studies of the effects of optogenetic stimulation of the bitter taste system on spatial navigation [<xref rid="pbio.3000712.ref038" ref-type="bibr">38</xref>] and feeding behavior [<xref rid="pbio.3000712.ref053" ref-type="bibr">53</xref>].</p>
      <p>Zebrafish move through discrete swim bouts. The loop time of PiVR was sufficiently short to rise to the tracking challenge posed by the discrete nature of fish locomotion (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4B</xref>). Genuine phototactic behavior was elicited in zebrafish larvae for several minutes based on pure temporal changes in light intensity devoid of binocular differences and a panoramic component. By synthetically recreating naturalistic landscapes [<xref rid="pbio.3000712.ref045" ref-type="bibr">45</xref>], the behavior recorded by PiVR in Gaussian light gradients (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4</xref>) complements previous studies featuring the use of discrete light disks [<xref rid="pbio.3000712.ref046" ref-type="bibr">46</xref>] and local asymmetric stimulations [<xref rid="pbio.3000712.ref054" ref-type="bibr">54</xref>]. A correlative analysis of the sensory input and the behavioral output corroborated the idea that positive phototaxis in fish can emerge from a modulation of the turn rate by the detected changes in light intensity (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4G and 4H</xref>) without necessarily involving a turning bias toward the light gradient (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4J</xref>). This orientation strategy shares similarities with the nondirectional increase in locomotor activity (“dark photokinesis”) that follows a sudden loss of illumination [<xref rid="pbio.3000712.ref055" ref-type="bibr">55</xref>,<xref rid="pbio.3000712.ref056" ref-type="bibr">56</xref>]. Taken together, our results establish that PiVR is suitable to conduct a detailed analysis of the sensorimotor rules directing attractive and aversive orientation behavior in small animals with distinct body plans and locomotor properties.</p>
    </sec>
    <sec id="sec010">
      <title>Outlook</title>
      <p>Although this study focused on sensory navigation, PiVR is equally suited to investigate neural circuits [<xref rid="pbio.3000712.ref004" ref-type="bibr">4</xref>] through optogenetic manipulations. To determine the connectivity and function of circuit elements, one typically performs acute functional manipulations during behavior. PiVR permits the time-dependent or behavior-dependent presentation of light stimuli to produce controlled gain of functions. The use of multiple setups in parallel is ideal to increase the throughput of behavioral screens—a budget of US$2,000 is sufficient to build more than five setups. If the experimenter wishes to define custom stimulation rules—triggering a light flash whenever an animal stops moving, for instance—this rule can be readily implemented by PiVR on single animals. For patterns of light stimulation that do not depend on the behavior of an animal—stimulations with a regular series of brief light pulses, for instance—groups of animals can be recorded at the same time. In its standard configuration (<xref ref-type="fig" rid="pbio.3000712.g001">Fig 1A</xref>), the resolution of videos recorded with PiVR is sufficient to achieve individual tracking of group behavior through off-line analysis with specialized algorithms such as idtracker.ai [<xref rid="pbio.3000712.ref025" ref-type="bibr">25</xref>] (<xref ref-type="supplementary-material" rid="pbio.3000712.s002">S2Bi Fig</xref>). Even for small animals such as flies, videos of surprisingly good quality can be recorded by outfitting the charge-coupled device (CCD) camera of PiVR with appropriate optics (<xref ref-type="supplementary-material" rid="pbio.3000712.s016">S8 Movie</xref>).</p>
      <p>Until recently, systems neuroscientists had to design and build their own setup to examine the function of neural circuits, or they had to adapt existing systems that were often expensive and complex. Fortunately, our field has benefited from the publication of a series of customizable tools to design and conduct behavioral analysis. The characteristics of the most representative tools are reviewed in <xref ref-type="supplementary-material" rid="pbio.3000712.s018">S2 Table</xref>. The ethoscope is a cost-efficient solution based on the use of a Raspberry Pi computer [<xref rid="pbio.3000712.ref023" ref-type="bibr">23</xref>], but it was not designed to create virtual realities. Several other packages can be used to track animals in real time on an external computer platform. For instance, Bonsai is an open-source visual programming framework for the acquisition and online processing of data streams to facilitate the prototyping of integrated behavioral experiments [<xref rid="pbio.3000712.ref057" ref-type="bibr">57</xref>]. FreemoVR can produce impressive tridimensional virtual visual realities [<xref rid="pbio.3000712.ref011" ref-type="bibr">11</xref>]. Stytra is a powerful open-source software package designed to carry out behavioral experiments specifically in zebrafish with real-time tracking capabilities [<xref rid="pbio.3000712.ref022" ref-type="bibr">22</xref>]. As illustrated in comparisons of <xref ref-type="supplementary-material" rid="pbio.3000712.s018">S2 Table</xref>, PiVR complements these tools by proposing a versatile solution to carry out closed-loop tracking with low-latency performance. One limitation of PiVR is that it produces purely homogenous temporal changes in light intensity without any spatial component. Because of its low production costs, the simplicity of its hardware design, and detailed documentation, PiVR can be readily assembled by any laboratory or group of high school students having access to a 3D printer. For these reasons, PiVR represents a tool of choice to make light-based virtual reality experiments accessible to experimentalists who might not be technically inclined.</p>
      <p>We anticipate that the performance (<xref ref-type="supplementary-material" rid="pbio.3000712.s001">S1 Fig</xref>) and the resolution of PiVR (Methods) will keep improving in the future. Historically, a new and faster version of the Raspberry Pi computer has been released every 2 years. In the near future, the image processing time of PiVR might decrease to just a few milliseconds, pushing the frequency to well above 70 Hz. Following the parallel development of transgenic techniques in nontraditional genetic model systems, it should be possible to capitalize on the use of optogenetic tools in virtually any species. Although PiVR was developed for animals measuring no more than a few centimeters, it should be easily scalable to accommodate experiments with larger animals such as mice and rats. Together with FlyPi [<xref rid="pbio.3000712.ref058" ref-type="bibr">58</xref>] and the ethoscope [<xref rid="pbio.3000712.ref023" ref-type="bibr">23</xref>], PiVR represents a low-barrier technology that should empower many labs to characterize new behavioral phenotypes and study neural circuit functions with minimal investment in time and research funds.</p>
    </sec>
  </sec>
  <sec sec-type="materials|methods" id="sec011">
    <title>Methods</title>
    <sec id="sec012">
      <title>Hardware design</title>
      <p>We designed all 3D-printed parts using 3D Builder (Microsoft Corporation). An Ultimaker 3 (Ultimaker, Geldermalsen, the Netherlands) with 0.8-mm print cores was used to print all parts. We used 2.85-mm PLA (B01EKFVAEU) as building material and 2.85-mm PVA (HY-PVA-300-NAT) as support material. The STL files were converted to gCode using Ultimaker’s Cura software (<ext-link ext-link-type="uri" xlink:href="https://ultimaker.com/en/products/ultimaker-cura-software">https://ultimaker.com/en/products/ultimaker-cura-software</ext-link>). The printed circuit boards (PCBs) were designed using Fritzing (<ext-link ext-link-type="uri" xlink:href="http://fritzing.org/home/">http://fritzing.org/home/</ext-link>) and printed by AISLER BV (Lemiers, the Netherlands).</p>
    </sec>
    <sec id="sec013">
      <title>Hardware parts</title>
      <p>Raspberry Pi components were bought from Newark element14 (Chicago, United States) and Adafruit Industries (New York, US). The 850-nm long-pass filter was bought from Edmond Optics (Barrington, US). Other electronics components were obtained from Mouser Electronics (Mansfield, US), Digi-Key Electronics (Thief River Falls, US), and Amazon (Seattle, US). Hardware was obtained from McMaster (Elmhurst, US). A complete bill of materials is available in <xref ref-type="supplementary-material" rid="pbio.3000712.s017">S1 Table</xref>. Updates will be available on <ext-link ext-link-type="uri" xlink:href="http://www.pivr.org/">www.pivr.org</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/louislab/pivr_publication">https://gitlab.com/louislab/pivr_publication</ext-link>.</p>
    </sec>
    <sec id="sec014">
      <title>Building and using PiVR</title>
      <p>Detailed instructions on how to build a PiVR and how to use it can be found in <xref ref-type="supplementary-material" rid="pbio.3000712.s019">S1 HTML</xref>. Any updates will be available on <ext-link ext-link-type="uri" xlink:href="http://www.pivr.org/">www.pivr.org</ext-link>.</p>
    </sec>
    <sec id="sec015">
      <title>Data analysis and statistics</title>
      <p>All data and scripts have been deposited as a data package on Dryad: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25349/D9ZK50">https://doi.org/10.25349/D9ZK50</ext-link> [<xref rid="pbio.3000712.ref059" ref-type="bibr">59</xref>]. Data analysis was performed using custom written analysis codes, which are bundled with the data package. In addition, the data analysis scripts are available from <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/LouisLab/PiVR">https://gitlab.com/LouisLab/PiVR</ext-link>.</p>
    </sec>
    <sec id="sec016">
      <title>Latency measurements of PiVR</title>
      <p>To estimate the time it takes between an animal performing an action and PiVR presenting the appropriate light stimulus, the following elements were taken into account: (1) image acquisition time, (2) image processing and VR calculation latency, and (3) software-to-hardware latency. To measure image acquisition time (<xref ref-type="supplementary-material" rid="pbio.3000712.s001">S1B Fig</xref>), the camera was allowed to set optimal exposure at each light intensity before measuring the shutter speed. To measure image processing and VR calculation latency (<xref ref-type="supplementary-material" rid="pbio.3000712.s001">S1Ci Fig</xref>), time was measured using the non-real-time Python time.time() function, which is documented to have uncertainty in the order of a few microseconds. To confirm these measurements, we also recorded the timestamps given by the real-time graphical processing unit (GPU) as reported in <xref ref-type="supplementary-material" rid="pbio.3000712.s001">S1Cii Fig</xref>. Together, these measurements show that although image processing time has a median around 8–10 milliseconds, there are a few frames for which the image processing time takes longer than 20 milliseconds, which, at high frame rates, leads to frames being skipped or dropped (arrows in <xref ref-type="supplementary-material" rid="pbio.3000712.s001">S1Ci Fig</xref> and <xref ref-type="supplementary-material" rid="pbio.3000712.s001">S1Cii Fig</xref>). Finally, to measure the software-to-hardware latency, we measured how long it takes to turn the general-purpose input/output (GPIO) pins ON and OFF during an experiment (<xref ref-type="supplementary-material" rid="pbio.3000712.s001">S1D Fig</xref>). The pins are connected to a transistor with rise and fall times in the order of microseconds (<ext-link ext-link-type="uri" xlink:href="https://eu.mouser.com/datasheet/2/308/FQP30N06L-1306227.pdf">https://eu.mouser.com/datasheet/2/308/FQP30N06L-1306227.pdf</ext-link>). LEDs tend to have a latency in the order of 100 ns. To confirm these measurements, we used PiVR to measure total time between movement in the field of view of the camera and the LED being turned on. The standard tracking software was modified to operate according to the following rules: (1) turn the LED “ON” at the 50th frame (and multiples thereof) and (2) compare pixel intensity of a fixed region of interest of the image to the previous image. If the value is above a threshold, count the frame as “flash.” Turn the LED OFF again. (3) If the LED was turned OFF in the previous frame, turn it ON again. The complete code can be found on the PiVR GitLab repository on branch “LED_flash_test.” This paradigm allowed us to quantify the maximal latency between animal behavior on the arena and the update of the LED. We found that the LED was always turned ON while the next image was collected as the time to detection depended on the observed location in the frame (<xref ref-type="supplementary-material" rid="pbio.3000712.s001">S1E and S1G Fig</xref>). Therefore, the maximal latency is &lt;30 milliseconds.</p>
    </sec>
    <sec id="sec017">
      <title>Video recording performance of PiVR</title>
      <p>When PiVR is used as a video recorder, the resolution (px) limits the maximal frame rate (fps): at 640 × 480 px, the frame rate can be set up to 90 fps, at 1,296 × 972 up to 42 fps, at 1,920 × 1,080 up to 30 fps, and at 2,592 × 1,944 a frame rate up to 15 fps can be used. PiVR is compatible with a wide variety of M12 lenses, allowing for high-quality video recordings, depending on the experimental needs (<xref ref-type="supplementary-material" rid="pbio.3000712.s016">S8 Movie</xref>).</p>
    </sec>
    <sec id="sec018">
      <title>Fruit fly larval experiments</title>
      <p>For the experiments using fruit fly larvae (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2</xref>), animals were raised on standard cornmeal medium at 22°C on a 12-hour day/night cycle. Third instar larvae were placed in 15% sucrose for 20–120 minutes prior to the experiment. Experiments were conducted on 2% agarose (Genesee, 20–102). In the experiments described in <xref ref-type="fig" rid="pbio.3000712.g002">Fig 2</xref>, the arena was a 100-mm-diameter petri dish (Fisher Scientific, FB0875712).</p>
      <p>For the experiments featuring a real-odor gradient, IAA (Sigma Aldrich, 306967–100 ML) was diluted in paraffin oil (Sigma Aldrich, 18512-1L) to produce a 1 M solution. A single source of the odor dilution was tested in larvae with the following genotype: <italic>w;Or42a-Gal4;UAS-Orco</italic>,<italic>Orco</italic><sup><italic>-/-</italic></sup>. The control consisted of solvent (paraffin oil) devoid of odor. For the virtual reality experiments, the following genotype was used: <italic>w</italic>;<italic>Or42a-Gal4</italic>,<italic>UAS-CsChrimson;UAS-Orco</italic>,<italic>Orco</italic><sup><italic>-/-</italic></sup>. In <xref ref-type="fig" rid="pbio.3000712.g002">Fig 2</xref>, the same genotype was used in controls, but the tests were conducted without any light stimulations. Larvae expressing CsChrimson were grown in complete darkness in 0.5 M all-<italic>trans</italic> retinal (R2500, MilliporeSigma, MO, USA).</p>
    </sec>
    <sec id="sec019">
      <title>Adult fruit fly experiments</title>
      <p>Male flies with the <italic>Gr66a</italic>-Gal4 transgene (Bloomington stock number: 57670) [<xref rid="pbio.3000712.ref060" ref-type="bibr">60</xref>] were crossed to virgin females carrying <italic>20xUAS-CsChrimson-mVenus</italic> [<xref rid="pbio.3000712.ref026" ref-type="bibr">26</xref>] integrated into the <italic>attP40</italic> landing site. The flies were grown in complete darkness on standard cornmeal medium with 0.5M all-<italic>trans</italic> retinal at 25°C. Female flies between 1 and 7 days after eclosion were selected after putting the vial on ice for a few seconds. The experiment was conducted in a 100-mm-diameter petri dish (Fisher Scientific, FB0875712) under a white-light condition.</p>
    </sec>
    <sec id="sec020">
      <title>Zebrafish larva experiments</title>
      <p>In the experiments shown in <xref ref-type="fig" rid="pbio.3000712.g004">Fig 4</xref>, we used AB <italic>casper</italic> [<xref rid="pbio.3000712.ref061" ref-type="bibr">61</xref>] as parents. Only pigmented larvae were used for the experiments. The larvae were reared at 28.5°C and a 14:10 light cycle. The experiments were run at 26°C. The arena was a 100-mm petri dish (Fisherbrand, 08-757-12PK). All ambient light was blocked. The maximum white-light intensity provided by PiVR was measured to be approximately 6,800 Lux (Extech Instruments Light Meter 401025).</p>
    </sec>
    <sec id="sec021">
      <title>Data analysis and statistical procedures</title>
      <p>All data analysis was performed using Python. The scripts used to create the plots shown in the figures (including all the data necessary to recreate the plots) can be found at (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25349/D9ZK50">https://doi.org/10.25349/D9ZK50</ext-link>). Generally, data sets were tested for normal distribution (Lilliefors test) and for homogeneity of variance (Levene’s test) [<xref rid="pbio.3000712.ref062" ref-type="bibr">62</xref>]. Depending on the result, the parametric <italic>t</italic> test or the nonparametric Mann–Whitney U rank-sum test was used. To compare multiple groups, either Bonferroni correction was applied after comparing multiple groups, or Dunn’s test was applied [<xref rid="pbio.3000712.ref062" ref-type="bibr">62</xref>]. Below, information about the analysis and the applied statistical tests throughout the manuscript are separately addressed for each figure. To estimate peak movement speed of different animals, the median of the 90th percentile of maximum speed per experiment was calculated.</p>
      <sec id="sec022">
        <title>Data analysis of <xref ref-type="fig" rid="pbio.3000712.g002">Fig 2</xref></title>
        <p>To calculate movement speed, the <italic>x</italic> and <italic>y</italic> coordinates were first filtered using a triangular rolling filter with a window size equal to the frame rate (30 fps) divided by the high bound on the speed of the animal (1 mm/s) times the pixel-per-millimeter value of the experiment. Depending on the exact distance between camera and the arena (pixel-per-millimeter value), the window size of the filter was typically 0.3 seconds. Speed was calculated using Euclidian distance. The time series of the speed was smoothened using a triangular rolling filter with a window size of 1 second. To calculate the sensory experience, the stimulus intensity time course was filtered using a boxcar rolling filter with window size 1 second (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2C</xref>). To calculate the distance to source for the IAA gradient, the source location was manually defined using the PiVR software. In the Gaussian virtual-odor gradient, the coordinates with the maximum intensity value was defined as the source. In the volcano-shaped virtual gradient, the circle with the highest values was defined as the nearest source to the animal (<xref ref-type="fig" rid="pbio.3000712.g002">Fig 2F</xref>). At 4 minutes into the experiment, the distance to source between the experimental and control condition was compared. As the data were not normally distributed (Lilliefors test), Mann–Whitney U test was used (<xref ref-type="supplementary-material" rid="pbio.3000712.s007">S7A–S7C Fig</xref>).</p>
      </sec>
      <sec id="sec023">
        <title>Data analysis of <xref ref-type="fig" rid="pbio.3000712.g003">Fig 3</xref></title>
        <p>Each experiment lasted 5 minutes. The preference index for each animal was calculated by subtracting the time spent by an animal in the squares without light from the time spent in the squares with light (squares eliciting virtual bitter taste). This subtraction was then divided by the total experimental time (<xref ref-type="fig" rid="pbio.3000712.g003">Fig 3C</xref>). Mann–Whitney U test was used to compare preference between genotypes because the distribution of the preference indices was not normally distributed (Lilliefors test). Speed was calculated as described in <xref ref-type="fig" rid="pbio.3000712.g002">Fig 2</xref>: first, the <italic>x</italic> and <italic>y</italic> coordinates were smoothened using a triangular rolling filter with a window size equal to the frame rate (30) divided by the approximate walking speed of flies (approximately 5 mm/s) times the pixel-per-millimeter ratio of the experiment, which was usually 0.06–0.09 seconds. Locomotion speed was calculated by using the Euclidian distance. The time series of the speed itself was filtered using a triangular rolling filter with a window size equal to the frame rate (30 fps). To test for statistical difference in speed between genotypes and the light on and off condition, Dunn’s multiple comparison test was implemented through the scikit-posthocs library of Python.</p>
      </sec>
      <sec id="sec024">
        <title>Data analysis of <xref ref-type="fig" rid="pbio.3000712.g004">Fig 4</xref></title>
        <p>Each experiment lasted 5 minutes. Each trial started with the animal facing the virtual-light source. To focus the analysis on animals that demonstrated significant movement during the experiment, the final data set was based on trials fulfilling the following criteria: (1) an animal had to move at least 20 mm over the course of the experiment, (2) only movements above 1 mm per frame were recorded (due to camera/detection noise, the centroid in resting animals can move), and (3) the animal must have been tracked for at least 3 out of 5 minutes.</p>
        <p>Locomotion speed was calculated by first smoothening the <italic>x</italic> and <italic>y</italic> centroid coordinates with a half-triangular rolling filter with the window size of 1 second. The speed was calculated using Euclidian distance and filtered again using a 0.3-second triangular rolling filter. Swim bouts were identified from the time course of the movement speed by using the “find_peaks” function of the scipy library [<xref rid="pbio.3000712.ref063" ref-type="bibr">63</xref>] with the following parameters: a minimum speed of 2.5 mm/s and a minimum time between two consecutive peaks (bouts) of five frames (0.165 seconds) (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4C</xref>). The same filter was applied (half-triangular rolling filter with window size of 1 second) to calculate the distance to source. Distance to the maximum value of the virtual reality landscape was calculated. To calculate the distance to source of the controls, the same virtual reality was assigned relative to the animal starting position, and the distance to this simulated landscape was calculated (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4E</xref>). In <xref ref-type="supplementary-material" rid="pbio.3000712.s008">S8A Fig</xref>, the distance to source was compared across experimental conditions at 4 minutes into the experiment. We used Mann–Whitney U test to compare the distance to source because the variance between samples could not be assumed to be equal (Levene’s test) (<xref ref-type="supplementary-material" rid="pbio.3000712.s008">S8A Fig</xref>). Bouts (see above) were then used to discretize the trajectory. The reorientation angle θ was calculated for each pair of consecutive bouts by comparing the animal’s coordinates 0.66 seconds (approximate duration of a bout) before and after the local peak in locomotor speed. To filter out occasional (&lt;10%) misdetection of the animal’s reflection once it was located near the wall of the petri dish, we only considered bouts with a reorientation angle θ smaller than 135° (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4F</xref>). The light intensity experienced by the animal was used to bin the reorientation angles into four groups, shown in <xref ref-type="fig" rid="pbio.3000712.g004">Fig 4G and 4H</xref>. To compare the control versus experimental condition of the four light intensity bins, Student <italic>t</italic> test was used after checking for normality (Lilliefors test) and for equality of variance (Levene’s test). Bonferroni correction was used to correct for multiple comparisons (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4G</xref>).</p>
        <p>To compare reorientation (turn) angles θ associated with positive and negative visual sensory experiences, the change in light intensity experienced during the previous bouts (ΔI) was used to split the experimental data in two groups: ΔI &gt; 0 (up-gradient) and ΔI &lt; 0 (down-gradient). The data were binned according to stimulus intensity as in <xref ref-type="fig" rid="pbio.3000712.g004">Fig 4G</xref>. To compare turn angle in the two groups, Student <italic>t</italic> test for paired samples was used after checking for normality (Lilliefors test) and for equality of variance (Levene’s test). Bonferroni correction was used to adjust for multiple comparisons (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4E</xref>).</p>
        <p>The turn index β was calculated based on the angular difference α between the heading of the animal at a given bout and the bearing with respect to the white-light source. This angular difference α indicates whether the animal approaches the source (angles near 0°) or swims away from it (angles near 180°). We defined 90° as the boundary between swimming toward and swimming away from the source. The turn index β was then defined by counting the number of bouts toward the source and by subtracting from it the number of bouts away from the source normalized by the total number of bouts. A turn index was calculated for each trial. We compared each group using the Mann–Whitney U test because the data were not normally distributed (Lilliefors test). Bonferroni correction was used to adjust for multiple testing (<xref ref-type="fig" rid="pbio.3000712.g004">Fig 4J</xref>). To bin the reorientation (turn) angle θ according to distance to source, the distance was calculated for each bout and binned in four groups according to <xref ref-type="supplementary-material" rid="pbio.3000712.s008">S8C Fig</xref>. We compared each group using Mann–Whitney U test because the data were not always normally distributed (Lilliefors test). Bonferroni correction was used to adjust for multiple comparisons.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="supplementary-material" id="sec025">
    <title>Supporting information</title>
    <supplementary-material content-type="local-data" id="pbio.3000712.s001">
      <label>S1 Fig</label>
      <caption>
        <title>Timing performance of PiVR.</title>
        <p>(A) Illustration of the three parameters measured to estimate overall loop time and latency. (B) Dependence of image acquisition time on the infrared background illumination strength. (Ci) To measure image processing and VR computation time, the non-real-time python 3.5 time.time() function is used. At 50, 60, and 70 fps some frames (6/5,598, 25/7,198 and 16/8,398, respectively) take longer than the period of the frame rate, which leads to dropped frames. (Cii) To confirm these measurements, we also recorded timestamps of the images assigned by the real-time clock of the GPU. (D) To estimate the software-to-hardware latency during a full update cycle of the tracker, we measured the time between the GPIO pin being instructed to turn ON and the GPIO pin reporting being turned ON. (E) Cameras with a rolling shutter take images by reading out lines of pixels from top to bottom (E, left). This can be illustrated in a simple <italic>x</italic>/<italic>y</italic> plot (E, right). To estimate maximum latency between the animal position and the update of the LED intensity, we used an LED flash paradigm while PiVR was tracking a dummy object at 70 Hz. Our latency results depend on the ROI associated with the location of the dummy object. When PiVR tracks an ROI located at the top of the frame (Fi, left), it detects the LED only two frames later (Fii, left). By contrast, when PiVR tracks an ROI located at the bottom of the frame (Fi, right), it detects the LED in the next possible frame (Fii, right). If PiVR tracks an ROI in the center of the image (Fi, center), it either detects the LED during the next frame or two frames later. We conclude that the LED is being turned ON while the next image is being formed (1.34 milliseconds). For a full LED ON–LED OFF–LED ON sequence, we find that there are three frames between the LED being turned ON when PiVR tracks the top of the image (Fiii, left), whereas it takes two frames when PiVR tracks the bottom of the image (Fiii, right). Because the time between two light flashes contains the frame during which the LED is turned OFF, the image acquisition and processing time corresponds to one or two frames. This is summarized in the timeline of panel G. Taken together, this shows that the time necessary to detect the movement of an animal and to update the LED intensity takes a maximum of two frames plus the time necessary to take the picture, which amounts to a maximum of 30 milliseconds at a frame rate of 70 Hz. All data used to create these plots are available from <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25349/D9ZK50">https://doi.org/10.25349/D9ZK50</ext-link>. GPIO, general-purpose input/output; GPU, graphical processing unit; LED, light-emitting diode; PiVR, Raspberry Pi Virtual Reality; ROI, region of interest; VR, virtual reality.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pbio.3000712.s001.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s002">
      <label>S2 Fig</label>
      <caption>
        <title>Modularity of PiVR (hardware).</title>
        <p>(A) PiVR is highly modular. The standard version (shown in <xref ref-type="fig" rid="pbio.3000712.g001">Fig 1A</xref>) can easily be adapted to allow for side (or top) illumination using the same LED controller system. If high-light intensities are needed, a high-power LED controller can be used in combination with high-power LEDs. Panel B shows an example of a side illuminator. Side illumination increases contrast on the surface of the animal. (Bi) This side illuminator was used to collect videos with sufficient detail for idtracker.ai [<xref rid="pbio.3000712.ref025" ref-type="bibr">25</xref>] to track 10 fruit fly larvae while retaining their identity. (C) The standard PiVR illuminator consists of at least two different 12-V LED strips: one for background illumination (850 nm) and another color (here, 625 nm) to stimulate the optogenetic tool of choice. (D) The high-power stimulation arena uses an LED controller that keeps current to high-power LEDs constant and can drive a maximum of 12 high-power LEDs. (Ci and Di) The intensity of the stimulation light was measured using a Photodiode (Thorlabs Inc. S130VC). Each pixel is 2 cm<sup>2</sup>. The black circles indicate petri dishes used as behavioral arenas. LED, light-emitting diode; PiVR, Raspberry Pi Virtual Reality.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pbio.3000712.s002.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s003">
      <label>S3 Fig</label>
      <caption>
        <title>Flow diagram of the automatic animal detection and background reconstruction.</title>
        <p>(A) After placing the animal and pressing “start tracking” on the graphical user interface, the software will grab the first image. All images are immediately filtered using a Gaussian kernel with a sigma depending on the size of the animal (user defined) to reduce camera noise. (B) Then a second image is taken. The mean of the images taken so far is being calculated. (C) The current image (in this example, the second image of the experiment) is then subtracted from the mean image shown in panel B. (D) The histogram of the subtracted image shows most gray scale values to be 0. For the region where the animal has moved since the first frame, the pixel values are negative (magenta). For the region that the animal has left since the first frame, the pixel values are positive (green). (E) The threshold value is calculated based on the histogram: it is the mean of the image subtracted by 4 (optimal value defined by trial and error). (F) The threshold value is used to binarize the subtracted image shown in panel C. If there is no or more than one blob with a minimal area (defined by user in animal parameters file), the loop restarts at step (B). (G) If there is exactly one blob, an area around the blob (defined by user in animal parameters file) is defined as the current region of interest. (H) The region of interest is the area where movement has been detected. The algorithm will now restrict the search for the animal to this region. (I) The histogram of this small area of the first image (A) shows that the few pixel defining the animal are distinct from the background. (J) To find the optimal local threshold for binarizing the image, the threshold is adjusted if more than one blob is detected (top, green arrow). As soon as only one blob with the characteristics of the animal (defined by user in animal parameters file) has been detected, the local threshold value is set, and the shape of the identified animal in the first frame is saved (bottom). (K) Using the local threshold, each new image is binarized and then subtracted from the first image as shown in panel (J, bottom). If the identical blob that was detected in panel J (bottom) is found in any of the new subtracted binary images (cyan arrow), the animal is considered as having left its original position, and the algorithm continues. (L) The region occupied by the animal is then copied from the latest image and (M) pasted into the first image. (N) The resulting image does not contain the animal and will be used as the background image for the tracking algorithm (<xref ref-type="supplementary-material" rid="pbio.3000712.s004">S4 Fig</xref>). All data used to create these plots are available from <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25349/D9ZK50">https://doi.org/10.25349/D9ZK50</ext-link>.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pbio.3000712.s003.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s004">
      <label>S4 Fig</label>
      <caption>
        <title>Flow diagram of the animal tracking algorithm.</title>
        <p>(A) At the start of the experiment, the ROI is defined during animal detection (<xref ref-type="supplementary-material" rid="pbio.3000712.s003">S3G Fig</xref>). During the experiment, the current ROI is defined using the previous frame. The ROI of the current image (C) is then subtracted from the ROI of the background (B). The fact that the tracking algorithm only considers a subsample of the image is central to the temporal performances (short processing time) of PiVR. (D) In the resulting image, the animal clearly stands out relative to the background. (E) The histogram of the image indicates that whereas the background consists mostly of values around 0, the animal has pixel intensity values that are negative. (F) The threshold is defined as being three standard deviations away from the mean (G). This threshold is used to binarize the subtracted ROI. The largest blob with animal characteristics (defined by animal parameters) is defined to be the animal. (H) The image of the detected animal is saved, and the next ROI is designated (defined by animal parameters). All data used to create these plots are available from <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25349/D9ZK50">https://doi.org/10.25349/D9ZK50</ext-link>. PiVR, Raspberry Pi Virtual Reality; ROI, region of interest.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pbio.3000712.s004.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s005">
      <label>S5 Fig</label>
      <caption>
        <title>Head/tail classification using a Hungarian algorithm.</title>
        <p>(A) During tracking, head/tail classification starts with the binarized image (<xref ref-type="supplementary-material" rid="pbio.3000712.s004">S4G Fig</xref>). (B) The binary image is used to calculate the morphological skeleton, which in turn is used to identify the two endpoints, one of which must be the head and the other the tail. (C) The Euclidian distance between the tail position in the previous frame and each of the endpoints is calculated. If the tail was not defined in the previous frame, the centroid position is used instead. (D) Whichever endpoint has less distance is defined as the tail (here <italic>v</italic>). The other endpoint is defined as the head.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pbio.3000712.s005.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s006">
      <label>S6 Fig</label>
      <caption>
        <title>Capability of PiVR to track a wide variety of small animals.</title>
        <p>PiVR is able to detect, track, and assign head and tail positions to a variety of invertebrate species with different body plans: (A) kelp fly, (B) jumping spider, (C) firefly, and (D) pill bug. PiVR, Raspberry Pi Virtual Reality.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pbio.3000712.s006.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s007">
      <label>S7 Fig</label>
      <caption>
        <title>Distance to real- and virtual-odor sources in larval chemotaxis to real- and virtual-odor gradients.</title>
        <p>(A) Distance to real-odor source (isoamyl acetate, <italic>n</italic> = 30) and the solvent (paraffin oil, <italic>n</italic> = 30), (B) between the Gaussian-shaped virtual-odor reality (<italic>n</italic> = 31) and the control (<italic>n</italic> = 26), and (C) the distance to the local maximum (rim of the volcano, <italic>n</italic> = 29) and the control (<italic>n</italic> = 26). Time point is 4 minutes into the experiment (Mann–Whitney U test, <italic>p &lt;</italic> 0.001). All data used to create these plots are available from <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25349/D9ZK50">https://doi.org/10.25349/D9ZK50</ext-link>.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pbio.3000712.s007.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s008">
      <label>S8 Fig</label>
      <caption>
        <title>Zebrafish larvae in virtual-light source.</title>
        <p>(A) Distance to virtual-light source of control (black) and experimental condition (red) at 4 minutes into the experiment. (B) Relationship between turn angle θ and distance to the virtual-light source (Mann–Whitney U test, different letters indicate <italic>p &lt;</italic> 0.01, <italic>n</italic> = 11 and 13). All reported <italic>p</italic>-values are Bonferroni corrected. All data used to create these plots are available from <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25349/D9ZK50">https://doi.org/10.25349/D9ZK50</ext-link>.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pbio.3000712.s008.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s009">
      <label>S1 Movie</label>
      <caption>
        <title>Illustration of homogenous illumination creating a virtual checkerboard reality.</title>
        <p>The behavior of a freely moving fly is shown in the petri dish (left), in the virtual checkerboard recorded by PiVR (middle). The corresponding time course of the homogenous illumination intensity is shown in the (right) panel. PiVR, Raspberry Pi Virtual Reality.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pbio.3000712.s009.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s010">
      <label>S2 Movie</label>
      <caption>
        <title>Sample trajectory of a <italic>Drosophila</italic> larva expressing <italic>Orco</italic> only in the <italic>Or42a</italic> olfactory sensory neuron behaving in a quasistatic isoamyl acetate gradient.</title>
        <p>The odor gradient was reconstructed for visualization and an estimation of the experienced odor intensity as described before [<xref rid="pbio.3000712.ref033" ref-type="bibr">33</xref>] (see also <xref ref-type="sec" rid="sec011">Methods</xref> section). Or42a, odorant receptor 42a.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pbio.3000712.s010.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s011">
      <label>S3 Movie</label>
      <caption>
        <title>Illustrative trajectory of a <italic>Drosophila</italic> larva expressing the optogenetic tool CsChrimson in the <italic>Or42a</italic> olfactory sensory neuron behaving in a Gaussian-shaped virtual-odor reality.</title>
        <p>Or42a, odorant receptor 42a.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pbio.3000712.s011.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s012">
      <label>S4 Movie</label>
      <caption>
        <title>Illustrative trajectory of a <italic>Drosophila</italic> larva expressing the optogenetic tool CsChrimson in the Or42a olfactory sensory neuron in a volcano-shaped virtual-odor reality.</title>
        <p>Or42a, odorant receptor 42a.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pbio.3000712.s012.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s013">
      <label>S5 Movie</label>
      <caption>
        <title>Illustrative trajectory of an adult <italic>Drosophila</italic> expressing the optogenetic tool CsChrimson in the <italic>Gr66a</italic> bitter-sensing neurons in a checkerboard-shaped virtual gustatory reality.</title>
        <p>Gr66a, gustatory receptor 66a.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pbio.3000712.s013.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s014">
      <label>S6 Movie</label>
      <caption>
        <title>Illustrative trajectory of a zebrafish larva in a virtual visual reality.</title>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pbio.3000712.s014.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s015">
      <label>S7 Movie</label>
      <caption>
        <title>Time-lapse video illustrating the production and assembly of a PiVR setup ending with the start of an experiment.</title>
        <p>For a detailed step-by-step protocol, please visit <ext-link ext-link-type="uri" xlink:href="http://www.pivr.org/">www.pivr.org</ext-link>. PiVR, Raspberry Pi Virtual Reality.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pbio.3000712.s015.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s016">
      <label>S8 Movie</label>
      <caption>
        <title>Video sequences of an adult fly in a dish recorded with PiVR outfitted with three different lenses.</title>
        <p>Comparison of the image quality between the standard 3.6-mm, F1.8 lens that comes with the Raspberry Pi camera; a 6-mm, F1.8 lens (US$25, PT-0618MP); and a higher-magnification 16-mm, F1.8 lens (US$21, PT-1618MP). PiVR, Raspberry Pi Virtual Reality.</p>
        <p>(MP4)</p>
      </caption>
      <media xlink:href="pbio.3000712.s016.mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s017">
      <label>S1 Table</label>
      <caption>
        <title>Bill of materials for the standard version of PiVR.</title>
        <p>PiVR, Raspberry Pi Virtual Reality.</p>
        <p>(CSV)</p>
      </caption>
      <media xlink:href="pbio.3000712.s017.csv">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s018">
      <label>S2 Table</label>
      <caption>
        <title>Comparative analysis of prevalent tracking systems used for behavioral analysis.</title>
        <p>(1) Optogenetic activation in closed-loop experiments. FreemoVR and Stytra are designed to present complex 3D and 2D visual stimuli, respectively. (2) Maximal frequency of the closed-loop stimulus. (3) Maximal time between an action of the tracked animal and the update of the hardware presenting the virtual reality. (4) Most tracking algorithms monitor the position of the centroid. Some tools will automatically detect other features of the tracked animal. (5) Some tracking algorithms are designed to specifically identify animals with a stereotypic shape, size, and movement. Other tracking algorithms are more flexible. (6) Assessed based on published information. (7) Bonsai was used in the optoPAD system, which uses optogenetics [<xref rid="pbio.3000712.ref053" ref-type="bibr">53</xref>]. The information presented in this comparative table relies on the following publications: FlyPi [<xref rid="pbio.3000712.ref058" ref-type="bibr">58</xref>]; Ethoscope [<xref rid="pbio.3000712.ref023" ref-type="bibr">23</xref>]; FreemoVR [<xref rid="pbio.3000712.ref011" ref-type="bibr">11</xref>]; Bonsai [<xref rid="pbio.3000712.ref057" ref-type="bibr">57</xref>]; PiVR, present manuscript. PiVR, Pi Raspberry Virtual Reality.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pbio.3000712.s018.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pbio.3000712.s019">
      <label>S1 HTML</label>
      <caption>
        <title>Copy of the <ext-link ext-link-type="uri" xlink:href="http://www.pivr.org/">www.pivr.org</ext-link> website, which includes information on how to build a standard PiVR setup.</title>
        <p>PiVR, Raspberry Pi Virtual Reality.</p>
        <p>(ZIP)</p>
      </caption>
      <media xlink:href="pbio.3000712.s019.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>We thank Ellie Heckscher, Primoz Ravbar, and Andrew Straw for comments on the manuscript. We are grateful to Ajinkya Deogade for work performed during the initial phase of the project (development of a do-it-yourself open-loop tracker based on FlyPi), for creating a preliminary draft of 3D-printed parts, for measuring the white-light intensities used in <xref ref-type="fig" rid="pbio.3000712.g004">Fig 4</xref>, and for discussions. We thank Tanya Tabachnik for technical advice about the assay construction. We are grateful to Stella Glasauer for collecting the kelp fly tracked in <xref ref-type="supplementary-material" rid="pbio.3000712.s006">S6A Fig</xref>, for taking the picture of the spider in <xref ref-type="supplementary-material" rid="pbio.3000712.s006">S6B Fig</xref>, for designing the PiVR logo, and for comments on the manuscript. We are in debt to Tyler Sizemore for collecting the firefly and pill bug tracked in <xref ref-type="supplementary-material" rid="pbio.3000712.s006">S6 Fig</xref>. We thank Igor Siwanowicz for providing the pictures of the firefly and pill bug of <xref ref-type="supplementary-material" rid="pbio.3000712.s006">S6 Fig</xref>. We are grateful to Minoru Koyama and Jared Rouchard for rearing and providing the fish used in <xref ref-type="fig" rid="pbio.3000712.g004">Fig 4</xref>. The development and optimization of PiVR greatly benefited from feedback received during the Drosophila Neurobiology: Genes, Circuits &amp; Behavior Summer School in 2017, 2018, and 2019 at the Cold Spring Harbor Laboratory.</p>
  </ack>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>CCD</term>
        <def>
          <p>charge-coupled device</p>
        </def>
      </def-item>
      <def-item>
        <term>dpf</term>
        <def>
          <p>days postfertilization</p>
        </def>
      </def-item>
      <def-item>
        <term>Gr66a</term>
        <def>
          <p>gustatory receptor 66a</p>
        </def>
      </def-item>
      <def-item>
        <term>IAA</term>
        <def>
          <p>isoamyl acetate</p>
        </def>
      </def-item>
      <def-item>
        <term>LED</term>
        <def>
          <p>light-emitting diode</p>
        </def>
      </def-item>
      <def-item>
        <term>OSN</term>
        <def>
          <p>olfactory sensory neuron</p>
        </def>
      </def-item>
      <def-item>
        <term>PiVR</term>
        <def>
          <p>Raspberry Pi Virtual Reality</p>
        </def>
      </def-item>
      <def-item>
        <term>Or42a</term>
        <def>
          <p>odorant receptor 42a</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ref-list>
    <title>References</title>
    <ref id="pbio.3000712.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Simpson</surname><given-names>JH</given-names></name>, <name><surname>Looger</surname><given-names>LL</given-names></name>. <article-title>Functional imaging and optogenetics in Drosophila</article-title>. <source>Genetics</source>. <year>2018</year>;<volume>208</volume>(<issue>4</issue>):<fpage>1291</fpage>–<lpage>309</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.117.300228</pub-id><?supplied-pmid 29618589?><pub-id pub-id-type="pmid">29618589</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Venken</surname><given-names>KJ</given-names></name>, <name><surname>Simpson</surname><given-names>JH</given-names></name>, <name><surname>Bellen</surname><given-names>HJ</given-names></name>. <article-title>Genetic manipulation of genes and cells in the nervous system of the fruit fly</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>72</volume>(<issue>2</issue>):<fpage>202</fpage>–<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2011.09.021</pub-id><?supplied-pmid 22017985?><pub-id pub-id-type="pmid">22017985</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Kristan</surname><given-names>WB</given-names></name>. <article-title>Neuronal decision-making circuits</article-title>. <source>Current Biology</source>. <year>2008</year>;<volume>18</volume>(<issue>19</issue>):<fpage>R928</fpage>–<lpage>R32</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2008.07.081</pub-id><?supplied-pmid 18957243?><pub-id pub-id-type="pmid">18957243</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Olsen</surname><given-names>SR</given-names></name>, <name><surname>Wilson</surname><given-names>RI</given-names></name>. <article-title>Cracking neural circuits in a tiny brain: new approaches for understanding the neural circuitry of Drosophila</article-title>. <source>Trends in neurosciences</source>. <year>2008</year>;<volume>31</volume>(<issue>10</issue>):<fpage>512</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1016/j.tins.2008.07.006</pub-id><?supplied-pmid 18775572?><pub-id pub-id-type="pmid">18775572</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Gordus</surname><given-names>A</given-names></name>, <name><surname>Pokala</surname><given-names>N</given-names></name>, <name><surname>Levy</surname><given-names>S</given-names></name>, <name><surname>Flavell</surname><given-names>SW</given-names></name>, <name><surname>Bargmann</surname><given-names>CI</given-names></name>. <article-title>Feedback from network states generates variability in a probabilistic olfactory circuit</article-title>. <source>Cell</source>. <year>2015</year>;<volume>161</volume>(<issue>2</issue>):<fpage>215</fpage>–<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2015.02.018</pub-id><?supplied-pmid 25772698?><pub-id pub-id-type="pmid">25772698</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Inagaki</surname><given-names>HK</given-names></name>, <name><surname>Jung</surname><given-names>Y</given-names></name>, <name><surname>Hoopfer</surname><given-names>ED</given-names></name>, <name><surname>Wong</surname><given-names>AM</given-names></name>, <name><surname>Mishra</surname><given-names>N</given-names></name>, <name><surname>Lin</surname><given-names>JY</given-names></name>, <etal>et al</etal><article-title>Optogenetic control of Drosophila using a red-shifted channelrhodopsin reveals experience-dependent influences on courtship</article-title>. <source>Nature methods</source>. <year>2014</year>;<volume>11</volume>(<issue>3</issue>):<fpage>325</fpage><pub-id pub-id-type="doi">10.1038/nmeth.2765</pub-id><?supplied-pmid 24363022?><pub-id pub-id-type="pmid">24363022</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Mueller</surname><given-names>JM</given-names></name>, <name><surname>Ravbar</surname><given-names>P</given-names></name>, <name><surname>Simpson</surname><given-names>JH</given-names></name>, <name><surname>Carlson</surname><given-names>JM</given-names></name>. <article-title>Drosophila melanogaster grooming possesses syntax with distinct rules at different temporal scales</article-title>. <source>PLoS Comput Biol</source>. <year>2019</year>;<volume>15</volume>(<issue>6</issue>):<fpage>e1007105</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007105</pub-id><?supplied-pmid 31242178?><pub-id pub-id-type="pmid">31242178</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Dombeck</surname><given-names>DA</given-names></name>, <name><surname>Reiser</surname><given-names>MB</given-names></name>. <article-title>Real neuroscience in virtual worlds</article-title>. <source>Current opinion in neurobiology</source>. <year>2012</year>;<volume>22</volume>(<issue>1</issue>):<fpage>3</fpage>–<lpage>10</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2011.10.015</pub-id><?supplied-pmid 22138559?><pub-id pub-id-type="pmid">22138559</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>von Holst</surname><given-names>E</given-names></name>, <name><surname>Mittelstaedt</surname><given-names>H</given-names></name>. <article-title>The principle of reafference: Interactions between the central nervous system and the peripheral organs</article-title>. <source>Perceptual processing: Stimulus equivalence and pattern recognition</source>. <year>1971</year>:<fpage>41</fpage>–<lpage>72</lpage>.</mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Srinivasan</surname><given-names>M</given-names></name>, <name><surname>Zhang</surname><given-names>S</given-names></name>, <name><surname>Lehrer</surname><given-names>M</given-names></name>, <name><surname>Collett</surname><given-names>T</given-names></name>. <article-title>Honeybee navigation en route to the goal: visual flight control and odometry</article-title>. <source>Journal of Experimental Biology</source>. <year>1996</year>;<volume>199</volume>(<issue>1</issue>):<fpage>237</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">9317712</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Stowers</surname><given-names>JR</given-names></name>, <name><surname>Hofbauer</surname><given-names>M</given-names></name>, <name><surname>Bastien</surname><given-names>R</given-names></name>, <name><surname>Griessner</surname><given-names>J</given-names></name>, <name><surname>Higgins</surname><given-names>P</given-names></name>, <name><surname>Farooqui</surname><given-names>S</given-names></name>, <etal>et al</etal><article-title>Virtual reality for freely moving animals</article-title>. <source>Nature methods</source>. <year>2017</year>;<volume>14</volume>(<issue>10</issue>):<fpage>995</fpage><pub-id pub-id-type="doi">10.1038/nmeth.4399</pub-id><?supplied-pmid 28825703?><pub-id pub-id-type="pmid">28825703</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Haberkern</surname><given-names>H</given-names></name>, <name><surname>Basnak</surname><given-names>MA</given-names></name>, <name><surname>Ahanonu</surname><given-names>B</given-names></name>, <name><surname>Schauder</surname><given-names>D</given-names></name>, <name><surname>Cohen</surname><given-names>JD</given-names></name>, <name><surname>Bolstad</surname><given-names>M</given-names></name>, <etal>et al</etal><article-title>Visually guided behavior and optogenetically induced learning in head-fixed flies exploring a virtual landscape</article-title>. <source>Current Biology</source>. <year>2019</year>;<volume>29</volume>(<issue>10</issue>):<fpage>1647</fpage>–<lpage>59.e8</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2019.04.033</pub-id><?supplied-pmid 31056392?><pub-id pub-id-type="pmid">31056392</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Dombeck</surname><given-names>DA</given-names></name>, <name><surname>Harvey</surname><given-names>CD</given-names></name>, <name><surname>Tian</surname><given-names>L</given-names></name>, <name><surname>Looger</surname><given-names>LL</given-names></name>, <name><surname>Tank</surname><given-names>DW</given-names></name>. <article-title>Functional imaging of hippocampal place cells at cellular resolution during virtual navigation</article-title>. <source>Nature neuroscience</source>. <year>2010</year>;<volume>13</volume>(<issue>11</issue>):<fpage>1433</fpage><pub-id pub-id-type="doi">10.1038/nn.2648</pub-id><?supplied-pmid 20890294?><pub-id pub-id-type="pmid">20890294</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Bianco</surname><given-names>IH</given-names></name>, <name><surname>Engert</surname><given-names>F</given-names></name>. <article-title>Visuomotor transformations underlying hunting behavior in zebrafish</article-title>. <source>Current biology</source>. <year>2015</year>;<volume>25</volume>(<issue>7</issue>):<fpage>831</fpage>–<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2015.01.042</pub-id><?supplied-pmid 25754638?><pub-id pub-id-type="pmid">25754638</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Ahrens</surname><given-names>MB</given-names></name>, <name><surname>Li</surname><given-names>JM</given-names></name>, <name><surname>Orger</surname><given-names>MB</given-names></name>, <name><surname>Robson</surname><given-names>DN</given-names></name>, <name><surname>Schier</surname><given-names>AF</given-names></name>, <name><surname>Engert</surname><given-names>F</given-names></name>, <etal>et al</etal><article-title>Brain-wide neuronal dynamics during motor adaptation in zebrafish</article-title>. <source>Nature</source>. <year>2012</year>;<volume>485</volume>(<issue>7399</issue>):<fpage>471</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1038/nature11057</pub-id><?supplied-pmid 22622571?><pub-id pub-id-type="pmid">22622571</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Ahrens</surname><given-names>MB</given-names></name>, <name><surname>Huang</surname><given-names>K-H</given-names></name>, <name><surname>Narayan</surname><given-names>S</given-names></name>, <name><surname>Mensh</surname><given-names>BD</given-names></name>, <name><surname>Engert</surname><given-names>F</given-names></name>. <article-title>Two-photon calcium imaging during fictive navigation in virtual environments</article-title>. <source>Frontiers in neural circuits</source>. <year>2013</year>;<volume>7</volume>:<fpage>104</fpage><pub-id pub-id-type="doi">10.3389/fncir.2013.00104</pub-id><?supplied-pmid 23761738?><pub-id pub-id-type="pmid">23761738</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Sofroniew</surname><given-names>NJ</given-names></name>, <name><surname>Cohen</surname><given-names>JD</given-names></name>, <name><surname>Lee</surname><given-names>AK</given-names></name>, <name><surname>Svoboda</surname><given-names>K</given-names></name>. <article-title>Natural whisker-guided behavior by head-fixed mice in tactile virtual reality</article-title>. <source>Journal of Neuroscience</source>. <year>2014</year>;<volume>34</volume>(<issue>29</issue>):<fpage>9537</fpage>–<lpage>50</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0712-14.2014</pub-id><?supplied-pmid 25031397?><pub-id pub-id-type="pmid">25031397</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Schulze</surname><given-names>A</given-names></name>, <name><surname>Gomez-Marin</surname><given-names>A</given-names></name>, <name><surname>Rajendran</surname><given-names>VG</given-names></name>, <name><surname>Lott</surname><given-names>G</given-names></name>, <name><surname>Musy</surname><given-names>M</given-names></name>, <name><surname>Ahammad</surname><given-names>P</given-names></name>, <etal>et al</etal><article-title>Dynamical feature extraction at the sensory periphery guides chemotaxis</article-title>. <source>Elife</source>. <year>2015</year>;<volume>4</volume>:<fpage>e06694</fpage>.</mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Kocabas</surname><given-names>A</given-names></name>, <name><surname>Shen</surname><given-names>C-H</given-names></name>, <name><surname>Guo</surname><given-names>ZV</given-names></name>, <name><surname>Ramanathan</surname><given-names>S</given-names></name>. <article-title>Controlling interneuron activity in Caenorhabditis elegans to evoke chemotactic behaviour</article-title>. <source>Nature</source>. <year>2012</year>;<volume>490</volume>(<issue>7419</issue>):<fpage>273</fpage><pub-id pub-id-type="doi">10.1038/nature11431</pub-id><?supplied-pmid 23000898?><pub-id pub-id-type="pmid">23000898</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Reiser</surname><given-names>MB</given-names></name>, <name><surname>Dickinson</surname><given-names>MH</given-names></name>. <article-title>A modular display system for insect behavioral neuroscience</article-title>. <source>Journal of neuroscience methods</source>. <year>2008</year>;<volume>167</volume>(<issue>2</issue>):<fpage>127</fpage>–<lpage>39</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.07.019</pub-id><?supplied-pmid 17854905?><pub-id pub-id-type="pmid">17854905</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Seelig</surname><given-names>JD</given-names></name>, <name><surname>Chiappe</surname><given-names>ME</given-names></name>, <name><surname>Lott</surname><given-names>GK</given-names></name>, <name><surname>Dutta</surname><given-names>A</given-names></name>, <name><surname>Osborne</surname><given-names>JE</given-names></name>, <name><surname>Reiser</surname><given-names>MB</given-names></name>, <etal>et al</etal><article-title>Two-photon calcium imaging from head-fixed Drosophila during optomotor walking behavior</article-title>. <source>Nature methods</source>. <year>2010</year>;<volume>7</volume>(<issue>7</issue>):<fpage>535</fpage><pub-id pub-id-type="doi">10.1038/nmeth.1468</pub-id><?supplied-pmid 20526346?><pub-id pub-id-type="pmid">20526346</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref022">
      <label>22</label>
      <mixed-citation publication-type="journal"><name><surname>Štih</surname><given-names>V</given-names></name>, <name><surname>Petrucco</surname><given-names>L</given-names></name>, <name><surname>Kist</surname><given-names>AM</given-names></name>, <name><surname>Portugues</surname><given-names>R</given-names></name>. <article-title>Stytra: an open-source, integrated system for stimulation, tracking and closed-loop behavioral experiments</article-title>. <source>PLoS Comput Biol</source>. <year>2019</year>;<volume>15</volume>(<issue>4</issue>):<fpage>e1006699</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006699</pub-id><?supplied-pmid 30958870?><pub-id pub-id-type="pmid">30958870</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Geissmann</surname><given-names>Q</given-names></name>, <name><surname>Rodriguez</surname><given-names>LG</given-names></name>, <name><surname>Beckwith</surname><given-names>EJ</given-names></name>, <name><surname>French</surname><given-names>AS</given-names></name>, <name><surname>Jamasb</surname><given-names>AR</given-names></name>, <name><surname>Gilestro</surname><given-names>GF</given-names></name>. <article-title>Ethoscopes: An open platform for high-throughput ethomics</article-title>. <source>PLoS Biol</source>. <year>2017</year>;<volume>15</volume>(<issue>10</issue>):<fpage>e2003026</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.2003026</pub-id><?supplied-pmid 29049280?><pub-id pub-id-type="pmid">29049280</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Branson</surname><given-names>K</given-names></name>, <name><surname>Robie</surname><given-names>AA</given-names></name>, <name><surname>Bender</surname><given-names>J</given-names></name>, <name><surname>Perona</surname><given-names>P</given-names></name>, <name><surname>Dickinson</surname><given-names>MH</given-names></name>. <article-title>High-throughput ethomics in large groups of Drosophila</article-title>. <source>Nature methods</source>. <year>2009</year>;<volume>6</volume>(<issue>6</issue>):<fpage>451</fpage><pub-id pub-id-type="doi">10.1038/nmeth.1328</pub-id><?supplied-pmid 19412169?><pub-id pub-id-type="pmid">19412169</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Romero-Ferrero</surname><given-names>F</given-names></name>, <name><surname>Bergomi</surname><given-names>MG</given-names></name>, <name><surname>Hinz</surname><given-names>RC</given-names></name>, <name><surname>Heras</surname><given-names>FJ</given-names></name>, <name><surname>de Polavieja</surname><given-names>GG</given-names></name>. <article-title>idtracker. ai: tracking all individuals in small or large collectives of unmarked animals</article-title>. <source>Nature methods</source>. <year>2019</year>;<volume>16</volume>(<issue>2</issue>):<fpage>179</fpage><pub-id pub-id-type="doi">10.1038/s41592-018-0295-5</pub-id><?supplied-pmid 30643215?><pub-id pub-id-type="pmid">30643215</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Klapoetke</surname><given-names>NC</given-names></name>, <name><surname>Murata</surname><given-names>Y</given-names></name>, <name><surname>Kim</surname><given-names>SS</given-names></name>, <name><surname>Pulver</surname><given-names>SR</given-names></name>, <name><surname>Birdsey-Benson</surname><given-names>A</given-names></name>, <name><surname>Cho</surname><given-names>YK</given-names></name>, <etal>et al</etal><article-title>Independent optical excitation of distinct neural populations</article-title>. <source>Nature methods</source>. <year>2014</year>;<volume>11</volume>(<issue>3</issue>):<fpage>338</fpage><pub-id pub-id-type="doi">10.1038/nmeth.2836</pub-id><?supplied-pmid 24509633?><pub-id pub-id-type="pmid">24509633</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Baden</surname><given-names>T</given-names></name>, <name><surname>Chagas</surname><given-names>AM</given-names></name>, <name><surname>Gage</surname><given-names>G</given-names></name>, <name><surname>Marzullo</surname><given-names>T</given-names></name>, <name><surname>Prieto-Godino</surname><given-names>LL</given-names></name>, <name><surname>Euler</surname><given-names>T</given-names></name>. <article-title>Open Labware: 3-D printing your own lab equipment</article-title>. <source>PLoS Biol</source>. <year>2015</year>;<volume>13</volume>(<issue>3</issue>):<fpage>e1002086</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.1002086</pub-id><?supplied-pmid 25794301?><pub-id pub-id-type="pmid">25794301</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Gershow</surname><given-names>M</given-names></name>, <name><surname>Berck</surname><given-names>M</given-names></name>, <name><surname>Mathew</surname><given-names>D</given-names></name>, <name><surname>Luo</surname><given-names>L</given-names></name>, <name><surname>Kane</surname><given-names>EA</given-names></name>, <name><surname>Carlson</surname><given-names>JR</given-names></name>, <etal>et al</etal><article-title>Controlling airborne cues to study small animal navigation</article-title>. <source>Nature methods</source>. <year>2012</year>;<volume>9</volume>(<issue>3</issue>):<fpage>290</fpage><pub-id pub-id-type="doi">10.1038/nmeth.1853</pub-id><?supplied-pmid 22245808?><pub-id pub-id-type="pmid">22245808</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref029">
      <label>29</label>
      <mixed-citation publication-type="journal"><name><surname>Gomez-Marin</surname><given-names>A</given-names></name>, <name><surname>Stephens</surname><given-names>GJ</given-names></name>, <name><surname>Louis</surname><given-names>M</given-names></name>. <article-title>Active sampling and decision making in Drosophila chemotaxis</article-title>. <source>Nat Commun</source>. <year>2011</year>;<volume>2</volume>:<fpage>441</fpage> Epub 2011/08/25. <pub-id pub-id-type="doi">10.1038/ncomms1455</pub-id>
<?supplied-pmid 21863008?><pub-id pub-id-type="pmid">21863008</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref030">
      <label>30</label>
      <mixed-citation publication-type="journal"><name><surname>Gomez-Marin</surname><given-names>A</given-names></name>, <name><surname>Louis</surname><given-names>M</given-names></name>. <article-title>Active sensation during orientation behavior in the Drosophila larva: more sense than luck</article-title>. <source>Current opinion in neurobiology</source>. <year>2012</year>;<volume>22</volume>(<issue>2</issue>):<fpage>208</fpage>–<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2011.11.008</pub-id><?supplied-pmid 22169055?><pub-id pub-id-type="pmid">22169055</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref031">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>Fishilevich</surname><given-names>E</given-names></name>, <name><surname>Domingos</surname><given-names>AI</given-names></name>, <name><surname>Asahina</surname><given-names>K</given-names></name>, <name><surname>Naef</surname><given-names>F</given-names></name>, <name><surname>Vosshall</surname><given-names>LB</given-names></name>, <name><surname>Louis</surname><given-names>M</given-names></name>. <article-title>Chemotaxis behavior mediated by single larval olfactory neurons in Drosophila</article-title>. <source>Curr Biol</source>. <year>2005</year>;<volume>15</volume>(<issue>23</issue>):<fpage>2086</fpage>–<lpage>96</lpage>. Epub 2005/12/08. <pub-id pub-id-type="doi">10.1016/j.cub.2005.11.016</pub-id> .<?supplied-pmid 16332533?><pub-id pub-id-type="pmid">16332533</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref032">
      <label>32</label>
      <mixed-citation publication-type="journal"><name><surname>Kreher</surname><given-names>SA</given-names></name>, <name><surname>Kwon</surname><given-names>JY</given-names></name>, <name><surname>Carlson</surname><given-names>JR</given-names></name>. <article-title>The molecular basis of odor coding in the Drosophila larva</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>46</volume>(<issue>3</issue>):<fpage>445</fpage>–<lpage>56</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2005.04.007</pub-id><?supplied-pmid 15882644?><pub-id pub-id-type="pmid">15882644</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref033">
      <label>33</label>
      <mixed-citation publication-type="journal"><name><surname>Louis</surname><given-names>M</given-names></name>, <name><surname>Huber</surname><given-names>T</given-names></name>, <name><surname>Benton</surname><given-names>R</given-names></name>, <name><surname>Sakmar</surname><given-names>TP</given-names></name>, <name><surname>Vosshall</surname><given-names>LB</given-names></name>. <article-title>Bilateral olfactory sensory input enhances chemotaxis behavior</article-title>. <source>Nat Neurosci</source>. <year>2008</year>;<volume>11</volume>(<issue>2</issue>):<fpage>187</fpage>–<lpage>99</lpage>. Epub 2007/12/25. <pub-id pub-id-type="doi">10.1038/nn2031</pub-id> .<?supplied-pmid 18157126?><pub-id pub-id-type="pmid">18157126</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>Tastekin</surname><given-names>I</given-names></name>, <name><surname>Khandelwal</surname><given-names>A</given-names></name>, <name><surname>Tadres</surname><given-names>D</given-names></name>, <name><surname>Fessner</surname><given-names>ND</given-names></name>, <name><surname>Truman</surname><given-names>JW</given-names></name>, <name><surname>Zlatic</surname><given-names>M</given-names></name>, <etal>et al</etal><article-title>Sensorimotor pathway controlling stopping behavior during chemotaxis in the Drosophila melanogaster larva</article-title>. <source>Elife</source>. <year>2018</year>;<volume>7</volume>:<fpage>e38740</fpage><pub-id pub-id-type="doi">10.7554/eLife.38740</pub-id><?supplied-pmid 30465650?><pub-id pub-id-type="pmid">30465650</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref035">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Humberg</surname><given-names>T-H</given-names></name>, <name><surname>Bruegger</surname><given-names>P</given-names></name>, <name><surname>Afonso</surname><given-names>B</given-names></name>, <name><surname>Zlatic</surname><given-names>M</given-names></name>, <name><surname>Truman</surname><given-names>JW</given-names></name>, <name><surname>Gershow</surname><given-names>M</given-names></name>, <etal>et al</etal><article-title>Dedicated photoreceptor pathways in Drosophila larvae mediate navigation by processing either spatial or temporal cues</article-title>. <source>Nature communications</source>. <year>2018</year>;<volume>9</volume>(<issue>1</issue>):<fpage>1260</fpage><pub-id pub-id-type="doi">10.1038/s41467-018-03520-5</pub-id><?supplied-pmid 29593252?><pub-id pub-id-type="pmid">29593252</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref036">
      <label>36</label>
      <mixed-citation publication-type="journal"><name><surname>Gomez-Marin</surname><given-names>A</given-names></name>, <name><surname>Louis</surname><given-names>M</given-names></name>. <article-title>Multilevel control of run orientation in Drosophila larval chemotaxis</article-title>. <source>Frontiers in behavioral neuroscience</source>. <year>2014</year>;<volume>8</volume>:<fpage>38</fpage><pub-id pub-id-type="doi">10.3389/fnbeh.2014.00038</pub-id><?supplied-pmid 24592220?><pub-id pub-id-type="pmid">24592220</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref037">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Heckscher</surname><given-names>ES</given-names></name>, <name><surname>Lockery</surname><given-names>SR</given-names></name>, <name><surname>Doe</surname><given-names>CQ</given-names></name>. <article-title>Characterization of Drosophila larval crawling at the level of organism, segment, and somatic body wall musculature</article-title>. <source>Journal of Neuroscience</source>. <year>2012</year>;<volume>32</volume>(<issue>36</issue>):<fpage>12460</fpage>–<lpage>71</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0222-12.2012</pub-id><?supplied-pmid 22956837?><pub-id pub-id-type="pmid">22956837</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref038">
      <label>38</label>
      <mixed-citation publication-type="journal"><name><surname>Shao</surname><given-names>L</given-names></name>, <name><surname>Saver</surname><given-names>M</given-names></name>, <name><surname>Chung</surname><given-names>P</given-names></name>, <name><surname>Ren</surname><given-names>Q</given-names></name>, <name><surname>Lee</surname><given-names>T</given-names></name>, <name><surname>Kent</surname><given-names>CF</given-names></name>, <etal>et al</etal><article-title>Dissection of the <italic>Drosophila</italic> neuropeptide F circuit using a high-throughput two-choice assay</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2017</year>;<volume>114</volume>(<issue>38</issue>):<fpage>E8091</fpage>–<lpage>E9</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1710552114</pub-id><?supplied-pmid 28874527?><pub-id pub-id-type="pmid">28874527</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref039">
      <label>39</label>
      <mixed-citation publication-type="journal"><name><surname>Moon</surname><given-names>SJ</given-names></name>, <name><surname>Köttgen</surname><given-names>M</given-names></name>, <name><surname>Jiao</surname><given-names>Y</given-names></name>, <name><surname>Xu</surname><given-names>H</given-names></name>, <name><surname>Montell</surname><given-names>C</given-names></name>. <article-title>A taste receptor required for the caffeine response in vivo</article-title>. <source>Current biology</source>. <year>2006</year>;<volume>16</volume>(<issue>18</issue>):<fpage>1812</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2006.07.024</pub-id><?supplied-pmid 16979558?><pub-id pub-id-type="pmid">16979558</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref040">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Benhamou</surname><given-names>S</given-names></name>, <name><surname>Bovet</surname><given-names>P</given-names></name>. <article-title>Distinguishing between elementary orientation mechanisms by means of path analysis</article-title>. <source>Animal Behaviour</source>. <year>1992</year>;<volume>43</volume>(<issue>3</issue>):<fpage>371</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref041">
      <label>41</label>
      <mixed-citation publication-type="journal"><name><surname>Bell</surname><given-names>WJ</given-names></name>, <name><surname>Tobin</surname><given-names>TR</given-names></name>. <article-title>Chemo‐orientation</article-title>. <source>Biological Reviews</source>. <year>1982</year>;<volume>57</volume>(<issue>2</issue>):<fpage>219</fpage>–<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref042">
      <label>42</label>
      <mixed-citation publication-type="book"><name><surname>Fraenkel</surname><given-names>GS</given-names></name>, <name><surname>Gunn</surname><given-names>DL</given-names></name>. <source>The Orientation of Animals</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Dover Publications</publisher-name>; <year>1961</year>.</mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref043">
      <label>43</label>
      <mixed-citation publication-type="journal"><name><surname>Ahrens</surname><given-names>MB</given-names></name>, <name><surname>Engert</surname><given-names>F</given-names></name>. <article-title>Large-scale imaging in small brains</article-title>. <source>Current opinion in neurobiology</source>. <year>2015</year>;<volume>32</volume>:<fpage>78</fpage>–<lpage>86</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2015.01.007</pub-id><?supplied-pmid 25636154?><pub-id pub-id-type="pmid">25636154</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref044">
      <label>44</label>
      <mixed-citation publication-type="journal"><name><surname>Friedrich</surname><given-names>RW</given-names></name>. <article-title>Neuronal computations in the olfactory system of zebrafish</article-title>. <source>Annual review of neuroscience</source>. <year>2013</year>;<volume>36</volume>:<fpage>383</fpage>–<lpage>402</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150504</pub-id><?supplied-pmid 23725002?><pub-id pub-id-type="pmid">23725002</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref045">
      <label>45</label>
      <mixed-citation publication-type="journal"><name><surname>Burgess</surname><given-names>HA</given-names></name>, <name><surname>Schoch</surname><given-names>H</given-names></name>, <name><surname>Granato</surname><given-names>M</given-names></name>. <article-title>Distinct retinal pathways drive spatial orientation behaviors in zebrafish navigation</article-title>. <source>Current biology</source>. <year>2010</year>;<volume>20</volume>(<issue>4</issue>):<fpage>381</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2010.01.022</pub-id><?supplied-pmid 20153194?><pub-id pub-id-type="pmid">20153194</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref046">
      <label>46</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>X</given-names></name>, <name><surname>Engert</surname><given-names>F</given-names></name>. <article-title>Navigational strategies underlying phototaxis in larval zebrafish</article-title>. <source>Frontiers in systems neuroscience</source>. <year>2014</year>;<volume>8</volume>:<fpage>39</fpage><pub-id pub-id-type="doi">10.3389/fnsys.2014.00039</pub-id><?supplied-pmid 24723859?><pub-id pub-id-type="pmid">24723859</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref047">
      <label>47</label>
      <mixed-citation publication-type="journal"><name><surname>Budick</surname><given-names>SA</given-names></name>, <name><surname>O'Malley</surname><given-names>DM</given-names></name>. <article-title>Locomotor repertoire of the larval zebrafish: swimming, turning and prey capture</article-title>. <source>Journal of Experimental Biology</source>. <year>2000</year>;<volume>203</volume>(<issue>17</issue>):<fpage>2565</fpage>–<lpage>79</lpage>.<pub-id pub-id-type="pmid">10934000</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref048">
      <label>48</label>
      <mixed-citation publication-type="journal"><name><surname>Kalueff</surname><given-names>AV</given-names></name>, <name><surname>Gebhardt</surname><given-names>M</given-names></name>, <name><surname>Stewart</surname><given-names>AM</given-names></name>, <name><surname>Cachat</surname><given-names>JM</given-names></name>, <name><surname>Brimmer</surname><given-names>M</given-names></name>, <name><surname>Chawla</surname><given-names>JS</given-names></name>, <etal>et al</etal><article-title>Towards a comprehensive catalog of zebrafish behavior 1.0 and beyond</article-title>. <source>Zebrafish</source>. <year>2013</year>;<volume>10</volume>(<issue>1</issue>):<fpage>70</fpage>–<lpage>86</lpage>. <pub-id pub-id-type="doi">10.1089/zeb.2012.0861</pub-id><?supplied-pmid 23590400?><pub-id pub-id-type="pmid">23590400</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref049">
      <label>49</label>
      <mixed-citation publication-type="journal"><name><surname>Orger</surname><given-names>MB</given-names></name>, <name><surname>de Polavieja</surname><given-names>GG</given-names></name>. <article-title>Zebrafish behavior: opportunities and challenges</article-title>. <source>Annual review of neuroscience</source>. <year>2017</year>;<volume>40</volume>:<fpage>125</fpage>–<lpage>47</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-neuro-071714-033857</pub-id><?supplied-pmid 28375767?><pub-id pub-id-type="pmid">28375767</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref050">
      <label>50</label>
      <mixed-citation publication-type="journal"><name><surname>Karpenko</surname><given-names>S</given-names></name>, <name><surname>Wolf</surname><given-names>S</given-names></name>, <name><surname>Lafaye</surname><given-names>J</given-names></name>, <name><surname>Le Goc</surname><given-names>G</given-names></name>, <name><surname>Panier</surname><given-names>T</given-names></name>, <name><surname>Bormuth</surname><given-names>V</given-names></name>, <etal>et al</etal><article-title>From behavior to circuit modeling of light-seeking navigation in zebrafish larvae</article-title>. <source>eLife</source>. <year>2020</year>;<volume>9</volume>.</mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref051">
      <label>51</label>
      <mixed-citation publication-type="journal"><name><surname>Boyden</surname><given-names>ES</given-names></name>, <name><surname>Zhang</surname><given-names>F</given-names></name>, <name><surname>Bamberg</surname><given-names>E</given-names></name>, <name><surname>Nagel</surname><given-names>G</given-names></name>, <name><surname>Deisseroth</surname><given-names>K</given-names></name>. <article-title>Millisecond-timescale, genetically targeted optical control of neural activity</article-title>. <source>Nature neuroscience</source>. <year>2005</year>;<volume>8</volume>(<issue>9</issue>):<fpage>1263</fpage><pub-id pub-id-type="doi">10.1038/nn1525</pub-id><?supplied-pmid 16116447?><pub-id pub-id-type="pmid">16116447</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref052">
      <label>52</label>
      <mixed-citation publication-type="journal"><name><surname>Mauss</surname><given-names>AS</given-names></name>, <name><surname>Busch</surname><given-names>C</given-names></name>, <name><surname>Borst</surname><given-names>A</given-names></name>. <article-title>Optogenetic neuronal silencing in Drosophila during visual processing</article-title>. <source>Scientific reports</source>. <year>2017</year>;<volume>7</volume>(<issue>1</issue>):<fpage>13823</fpage><pub-id pub-id-type="doi">10.1038/s41598-017-14076-7</pub-id><?supplied-pmid 29061981?><pub-id pub-id-type="pmid">29061981</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref053">
      <label>53</label>
      <mixed-citation publication-type="journal"><name><surname>Moreira</surname><given-names>J-M</given-names></name>, <name><surname>Itskov</surname><given-names>PM</given-names></name>, <name><surname>Goldschmidt</surname><given-names>D</given-names></name>, <name><surname>Baltazar</surname><given-names>C</given-names></name>, <name><surname>Steck</surname><given-names>K</given-names></name>, <name><surname>Tastekin</surname><given-names>I</given-names></name>, <etal>et al</etal><article-title>optoPAD, a closed-loop optogenetics system to study the circuit basis of feeding behaviors</article-title>. <source>Elife</source>. <year>2019</year>;<volume>8</volume>:<fpage>e43924</fpage><pub-id pub-id-type="doi">10.7554/eLife.43924</pub-id><?supplied-pmid 31226244?><pub-id pub-id-type="pmid">31226244</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref054">
      <label>54</label>
      <mixed-citation publication-type="journal"><name><surname>Karpenko</surname><given-names>S</given-names></name>, <name><surname>Wolf</surname><given-names>S</given-names></name>, <name><surname>Lafaye</surname><given-names>J</given-names></name>, <name><surname>Le Goc</surname><given-names>G</given-names></name>, <name><surname>Panier</surname><given-names>T</given-names></name>, <name><surname>Bormuth</surname><given-names>V</given-names></name>, <name><surname>Candelier</surname><given-names>R</given-names></name>, <name><surname>Debrégeas</surname><given-names>G</given-names></name>. <article-title>From behavior to circuit modeling of light-seeking navigation in zebrafish larvae</article-title>. <source>Elife.</source><year>2020</year><month>1</month><day>2</day>;<volume>9</volume>:<fpage>e52882</fpage><pub-id pub-id-type="doi">10.7554/eLife.52882</pub-id><?supplied-pmid 31895038?><pub-id pub-id-type="pmid">31895038</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref055">
      <label>55</label>
      <mixed-citation publication-type="journal"><name><surname>Fernandes</surname><given-names>AM</given-names></name>, <name><surname>Fero</surname><given-names>K</given-names></name>, <name><surname>Arrenberg</surname><given-names>AB</given-names></name>, <name><surname>Bergeron</surname><given-names>SA</given-names></name>, <name><surname>Driever</surname><given-names>W</given-names></name>, <name><surname>Burgess</surname><given-names>HA</given-names></name>. <article-title>Deep brain photoreceptors control light-seeking behavior in zebrafish larvae</article-title>. <source>Current Biology</source>. <year>2012</year>;<volume>22</volume>(<issue>21</issue>):<fpage>2042</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2012.08.016</pub-id><?supplied-pmid 23000151?><pub-id pub-id-type="pmid">23000151</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref056">
      <label>56</label>
      <mixed-citation publication-type="journal"><name><surname>Horstick</surname><given-names>EJ</given-names></name>, <name><surname>Bayleyen</surname><given-names>Y</given-names></name>, <name><surname>Sinclair</surname><given-names>JL</given-names></name>, <name><surname>Burgess</surname><given-names>HA</given-names></name>. <article-title>Search strategy is regulated by somatostatin signaling and deep brain photoreceptors in zebrafish</article-title>. <source>BMC biology</source>. <year>2017</year>;<volume>15</volume>(<issue>1</issue>):<fpage>4</fpage><pub-id pub-id-type="doi">10.1186/s12915-016-0346-2</pub-id><?supplied-pmid 28122559?><pub-id pub-id-type="pmid">28122559</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref057">
      <label>57</label>
      <mixed-citation publication-type="journal"><name><surname>Lopes</surname><given-names>G</given-names></name>, <name><surname>Bonacchi</surname><given-names>N</given-names></name>, <name><surname>Frazão</surname><given-names>J</given-names></name>, <name><surname>Neto</surname><given-names>JP</given-names></name>, <name><surname>Atallah</surname><given-names>BV</given-names></name>, <name><surname>Soares</surname><given-names>S</given-names></name>, <etal>et al</etal><article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title>. <source>Frontiers in neuroinformatics</source>. <year>2015</year>;<volume>9</volume>:<fpage>7</fpage><pub-id pub-id-type="doi">10.3389/fninf.2015.00007</pub-id><?supplied-pmid 25904861?><pub-id pub-id-type="pmid">25904861</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref058">
      <label>58</label>
      <mixed-citation publication-type="journal"><name><surname>Chagas</surname><given-names>AM</given-names></name>, <name><surname>Prieto-Godino</surname><given-names>LL</given-names></name>, <name><surname>Arrenberg</surname><given-names>AB</given-names></name>, <name><surname>Baden</surname><given-names>T</given-names></name>. <article-title>The€ 100 lab: A 3D-printable open-source platform for fluorescence microscopy, optogenetics, and accurate temperature control during behaviour of zebrafish, Drosophila, and Caenorhabditis elegans</article-title>. <source>PLoS Biol</source>. <year>2017</year>;<volume>15</volume>(<issue>7</issue>):<fpage>e2002702</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.2002702</pub-id><?supplied-pmid 28719603?><pub-id pub-id-type="pmid">28719603</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref059">
      <label>59</label>
      <mixed-citation publication-type="journal"><name><surname>Tadres</surname><given-names>D</given-names></name>, <name><surname>Louis</surname><given-names>M</given-names></name>. <article-title>Data from PiVR: an affordable and versatile closed-loop platform to study unrestrained sensorimotor behavior</article-title>. <year>2020</year><source>Dryad Digital Repository</source>. Available from: <pub-id pub-id-type="doi">10.25349/D9ZK50</pub-id>.</mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref060">
      <label>60</label>
      <mixed-citation publication-type="journal"><name><surname>Kwon</surname><given-names>JY</given-names></name>, <name><surname>Dahanukar</surname><given-names>A</given-names></name>, <name><surname>Weiss</surname><given-names>LA</given-names></name>, <name><surname>Carlson</surname><given-names>JR</given-names></name>. <article-title>Molecular and cellular organization of the taste system in the Drosophila larva</article-title>. <source>Journal of Neuroscience</source>. <year>2011</year>;<volume>31</volume>(<issue>43</issue>):<fpage>15300</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3363-11.2011</pub-id><?supplied-pmid 22031876?><pub-id pub-id-type="pmid">22031876</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref061">
      <label>61</label>
      <mixed-citation publication-type="journal"><name><surname>White</surname><given-names>RM</given-names></name>, <name><surname>Sessa</surname><given-names>A</given-names></name>, <name><surname>Burke</surname><given-names>C</given-names></name>, <name><surname>Bowman</surname><given-names>T</given-names></name>, <name><surname>LeBlanc</surname><given-names>J</given-names></name>, <name><surname>Ceol</surname><given-names>C</given-names></name>, <etal>et al</etal><article-title>Transparent adult zebrafish as a tool for in vivo transplantation analysis</article-title>. <source>Cell stem cell</source>. <year>2008</year>;<volume>2</volume>(<issue>2</issue>):<fpage>183</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/j.stem.2007.11.002</pub-id><?supplied-pmid 18371439?><pub-id pub-id-type="pmid">18371439</pub-id></mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref062">
      <label>62</label>
      <mixed-citation publication-type="journal"><name><surname>Zar</surname><given-names>JH</given-names></name>. <article-title>Biostatistical analysis</article-title>. <source>Pearson Education India</source>; <year>1999</year>.</mixed-citation>
    </ref>
    <ref id="pbio.3000712.ref063">
      <label>63</label>
      <mixed-citation publication-type="journal"><name><surname>Oliphant</surname><given-names>TE</given-names></name>. <article-title>Python for Scientific Computing</article-title>. <source>Computing in Science &amp; Engineering</source>. <year>2007</year>;<volume>9</volume>(<issue>3</issue>):<fpage>10</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1109/MCSE.2007.58</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<sub-article id="pbio.3000712.r001" article-type="editor-report">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pbio.3000712.r001</article-id>
    <title-group>
      <article-title>Decision Letter 0</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Alvarez-Garcia</surname>
          <given-names>Ines</given-names>
        </name>
        <role>Senior Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2020 Ines Alvarez-Garcia</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Ines Alvarez-Garcia</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj001" ext-link-type="doi" xlink:href="10.1371/journal.pbio.3000712" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>0</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">19 Dec 2019</named-content>
    </p>
    <p>Dear Matthieu, </p>
    <p>Thank you for submitting your manuscript entitled "PiVR: an affordable and versatile closed-loop platform to study unrestrained sensorimotor behavior" for consideration as a Methods and Resources by PLOS Biology.</p>
    <p>Your manuscript has now been evaluated by the PLOS Biology editorial staff as well as by an academic editor with relevant expertise and I am writing to let you know that we would like to send your submission out for external peer review.</p>
    <p>However, before we can send your manuscript to reviewers, we need you to complete your submission by providing the metadata that is required for full assessment. To this end, please login to Editorial Manager where you will find the paper in the 'Submissions Needing Revisions' folder on your homepage. Please click 'Revise Submission' from the Action Links and complete all additional questions in the submission questionnaire.</p>
    <p>Please re-submit your manuscript within two working days, i.e. by Dec 23 2019 11:59PM.</p>
    <p>Login to Editorial Manager here: <ext-link ext-link-type="uri" xlink:href="https://www.editorialmanager.com/pbiology">https://www.editorialmanager.com/pbiology</ext-link>
</p>
    <p>During resubmission, you will be invited to opt-in to posting your pre-review manuscript as a bioRxiv preprint. Visit <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosbiology/s/preprints">http://journals.plos.org/plosbiology/s/preprints</ext-link> for full details. If you consent to posting your current manuscript as a preprint, please upload a single Preprint PDF when you re-submit. </p>
    <p>Once your full submission is complete, your paper will undergo a series of checks in preparation for peer review. Once your manuscript has passed all checks it will be sent out for review. </p>
    <p>***Please be aware that, due to the voluntary nature of our reviewers and academic editors, manuscripts may be subject to delays due to their limited availability during the holiday season. Please also note that the journal office will be closed entirely 21st- 29th December inclusive, and 1st January 2020. Thank you for your patience.***</p>
    <p>Feel free to email us at <email>plosbiology@plos.org</email> if you have any queries relating to your submission.</p>
    <p>Kind regards,</p>
    <p>Ines</p>
    <p>--</p>
    <p>Ines Alvarez-Garcia, PhD</p>
    <p>Senior Editor</p>
    <p>PLOS Biology</p>
    <p>Carlyle House, Carlyle Road</p>
    <p>Cambridge, CB4 3DN</p>
    <p>+44 1223–442810</p>
  </body>
</sub-article>
<sub-article id="pbio.3000712.r002" article-type="aggregated-review-documents">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pbio.3000712.r002</article-id>
    <title-group>
      <article-title>Decision Letter 1</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Alvarez-Garcia</surname>
          <given-names>Ines</given-names>
        </name>
        <role>Senior Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2020 Ines Alvarez-Garcia</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Ines Alvarez-Garcia</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj002" ext-link-type="doi" xlink:href="10.1371/journal.pbio.3000712" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">31 Jan 2020</named-content>
    </p>
    <p>Dear Matthieu,</p>
    <p>Thank you very much for submitting your manuscript "PiVR: an affordable and versatile closed-loop platform to study unrestrained sensorimotor behavior" for consideration as a Methods and Resources at PLOS Biology. Thank you also for your patience as we completed our editorial process, and please accept my apologies for the delay in providing you with our decision. Your manuscript has been evaluated by the PLOS Biology editors, an Academic Editor with relevant expertise, and by three independent reviewers.</p>
    <p>As you will see, the reviewers feel that the system you have developed is novel and useful for the scientific community, however they also raise several issues that would need to be addressed before we can consider your manuscript further for publication. You should make a comprehensive comparison with other methods already available (such as Bonsai, Stytra or FlyPI) and state clearly the advantages and limitations of PiVR. In addition, you should streamline the text and follow the reviewers’ suggestions to improve the structure of the manuscript.</p>
    <p>In light of the reviews (attached below), we are pleased to offer you the opportunity to address the [comments/remaining points] from the reviewers in a revised version that we anticipate should not take you very long. We will then assess your revised manuscript and your response to the reviewers' comments and we may consult the reviewers again.</p>
    <p>We expect to receive your revised manuscript within 1 month.</p>
    <p>Please email us (<email>plosbiology@plos.org</email>) if you have any questions or concerns, or would like to request an extension. At this stage, your manuscript remains formally under active consideration at our journal; please notify us by email if you do not intend to submit a revision so that we may end consideration of the manuscript at PLOS Biology.</p>
    <p>**IMPORTANT - SUBMITTING YOUR REVISION**</p>
    <p>Your revisions should address the specific points made by each reviewer. Please submit the following files along with your revised manuscript:</p>
    <p>1. A 'Response to Reviewers' file - this should detail your responses to the editorial requests, present a point-by-point response to all of the reviewers' comments, and indicate the changes made to the manuscript. </p>
    <p>*NOTE: In your point by point response to the reviewers, please provide the full context of each review. Do not selectively quote paragraphs or sentences to reply to. The entire set of reviewer comments should be present in full and each specific point should be responded to individually.</p>
    <p>You should also cite any additional relevant literature that has been published since the original submission and mention any additional citations in your response. </p>
    <p>2. In addition to a clean copy of the manuscript, please also upload a 'track-changes' version of your manuscript that specifies the edits made. This should be uploaded as a "Related" file type. </p>
    <p>*Resubmission Checklist*</p>
    <p>When you are ready to resubmit your revised manuscript, please refer to this resubmission checklist: <ext-link ext-link-type="uri" xlink:href="https://plos.io/Biology_Checklist">https://plos.io/Biology_Checklist</ext-link></p>
    <p>To submit a revised version of your manuscript, please go to <ext-link ext-link-type="uri" xlink:href="https://www.editorialmanager.com/pbiology/">https://www.editorialmanager.com/pbiology/</ext-link> and log in as an Author. Click the link labelled 'Submissions Needing Revision' where you will find your submission record. </p>
    <p>Please make sure to read the following important policies and guidelines while preparing your revision:</p>
    <p>*Published Peer Review*</p>
    <p>Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out. Please see here for more details:</p>
    <p>
      <ext-link ext-link-type="uri" xlink:href="https://blogs.plos.org/plos/2019/05/plos-journals-now-open-for-published-peer-review/">https://blogs.plos.org/plos/2019/05/plos-journals-now-open-for-published-peer-review/</ext-link>
    </p>
    <p>*PLOS Data Policy*</p>
    <p>Please note that as a condition of publication PLOS' data policy (<ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosbiology/s/data-availability">http://journals.plos.org/plosbiology/s/data-availability</ext-link>) requires that you make available all data used to draw the conclusions arrived at in your manuscript. If you have not already done so, you must include any data used in your manuscript either in appropriate repositories, within the body of the manuscript, or as supporting information (N.B. this includes any numerical values that were used to generate graphs, histograms etc.). For an example see here: <ext-link ext-link-type="uri" xlink:href="http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5">http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5</ext-link></p>
    <p>*Protocols deposition*</p>
    <p>To enhance the reproducibility of your results, we recommend that if applicable you deposit your laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. For instructions see: <ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosbiology/s/submission-guidelines#loc-materials-and-methods">https://journals.plos.org/plosbiology/s/submission-guidelines#loc-materials-and-methods</ext-link>
</p>
    <p>Thank you again for your submission to our journal. We hope that our editorial process has been constructive thus far, and we welcome your feedback at any time. Please don't hesitate to contact us if you have any questions or comments.</p>
    <p>Sincerely,</p>
    <p>Ines</p>
    <p>--</p>
    <p>Ines Alvarez-Garcia, PhD</p>
    <p>Senior Editor</p>
    <p>PLOS Biology</p>
    <p>Carlyle House, Carlyle Road</p>
    <p>Cambridge, CB4 3DN</p>
    <p>+44 1223–442810</p>
    <p>-----------------------------------------------------</p>
    <p>Reviewers’ comments</p>
    <p>Rev. 1:</p>
    <p>The present study by Tadres and Louis presents a useful and affordable open-source virtual reality setup based on the Raspberry Pi system. The authors present solid data using virtual chemotaxis and phototaxis experiments, confirming that their system works accurately for such purposes. I find this work valuable to the community, especially for the effort to design systems that can be implemented at low cost as well as for teaching platforms.</p>
    <p>I have a few comments that I believe would improve the presentation and message of this manuscript and one concern regarding the proposed speed of the system.</p>
    <p>First, I have my doubts that the overall close-loop reaction time is correct. The shutter speeds and the image processing time were measured. However, due to the limitations of the bus, there is always a lag between image acquisition and the start of image processing. I have no experience with the Raspberry Pie system, but I have encountered these limitations in top of the line computing systems. Thus, I would test this directly. My suggestion is to use their system at maximal acquisition speed, image short LED flash, and use their computing platform to detect this flash and turn the same LED on for a second time. By looking at the number of frames between the two flashes, the authors could test the speed of the entire system. Although the sampling rate will be limited by the acquisition speed of the camera, the authors will be able to prove if their proposed 20ms processing time is accurate. This is important since the authors correctly claim that "the shorter the delay, the more authentic the virtual reality is perceived." I don't see issues with the experiments presented, but new users will be warned in case the system is slower than the speeds required for their purposes.</p>
    <p>My second suggestion is regarding the current structure. The main claim of this paper is to have a VR system that is affordable and reliable; not the experimental results used to benchmark the system. Thus, I would remove the experimental sections from the discussion (Defining that nature of taste-driven responses in adult flies; Exploring the ability of zebrafish [larvae] to orient with minimal graded visual inputs; Exploring the ability of the fruit fly….) and merged them in a minimal form to the experimental results.</p>
    <p>Finally, although I truly appreciate and are convinced that the system is useful for particular experiments, it won't be for all. There are limitations on the FOV, camera resolution, speed, etc., that make it indispensable for many labs to engineering their systems. Thus, I would recommend adding this perspective to the discussion. I believe that this will be helpful for labs that thanks to the affordable design will start doing these kinds of VR experiments, but don't have the experience to judge all details.</p>
    <p>Minor:</p>
    <p>I would disagree with the first sentence: "Behavior emerges from the conversion of sensory input into motor output". Organisms are not automata, e.g.; there is a whole world of internal states that modulate behavior. Please rephrase.</p>
    <p>I would also make a stronger point on why VR systems are important (line 41). You don't need a VR system to stimulate an animal repeatedly. VR systems allow combining the stimulus to the behavior, enabling a precise exploration of response dynamics to sensory stimuli dependent on particular behavioral conditions.</p>
    <p>- The paper sometimes uses colloquial terms, e.g., "crack the neuronal code," etc. I would change that language.</p>
    <p>- Hz and frames per second are used, I would stick to Hz.</p>
    <p>- Reference, e.g., page 213: Schulze, Gomez-Marin (17) -&gt; Schulze et al. (17). Also line 307.</p>
    <p>- Get rid of popular in line 220 and 256. Model organisms already imply that they are used widely.</p>
    <p>- Line 600 - 608, it is not clear that the (Ci-iii) refers to the supplemental figure.</p>
    <p>- Line 195, "field of the view", remove the.</p>
    <p>Rev. 2:</p>
    <p>In this manuscript, Tadres &amp; Louis present PiVR, a novel hardware designed to perform closed-loop light-based stimulations on small animals. The authors present data based on optogenetic activation of genetically targeted neurons in Drosophila larvae and imagos; they also present a proof of principle using visible light on zebrafish larvae.</p>
    <p>PiVR is certainly an interesting device and I can see it becoming popular in the field of Drosophila neuroscience, especially among people interested in developing new paradigms of learning.</p>
    <p>The device has some clear strengths and some limitations. The strengths are discussed appropriately and fairly. The limitations not so much and it may be useful to have a more rounded discussion of both so that readers can immediately recognise whether this tool is appropriate for their uses.</p>
    <p>Amongst the strengths I would count:</p>
    <p>1. The documentation is outstanding.</p>
    <p>2. The machine is relatively inexpensive.</p>
    <p>3. The basic usage of the machine seems to be easy to implement</p>
    <p>Amongst the weaknesses:</p>
    <p>1. Some of the proposed usages of PiVR are suboptimal. It is stressed several times in the manuscript that PiVR can be used to acquire videos "offline", for them to be tracked by a different software (such as tracker.ai). I doubt this setup can compete, in terms of resolution, speed and even cost with the simpler solution of having an industrial CCD connected to an existing computer.</p>
    <p>2. While, in principle, the R in VR stands for any kind of Reality, in fact it is commonly associated to complex visual representations such as projections of objects, patterns, scenarios. PiVR is limited to providing different intensity of lights and therefore its main use-case is going to be optogenetics.</p>
    <p>3. It is not clear to me why a reader should prefer PiVR over other already existing alternatives and it may be useful to stress out, perhaps in a table, pros and cons of PiVR vs the "competition". FlyPI , BONSAI, FreeMO-VR are all alternative products with functionalities that overlap the ones of PiVR - the manuscript would benefit from a more straight comparison with them.</p>
    <p>I do not have specific comments on the manuscript, besides the fact that I found it perhaps too long and repetitive. I think the strength of this tool is that it does one thing and it does nicely but all this gets somehow lost in a manuscript so discursive and repetitive. The discussion, in particular, is not well focused.</p>
    <p>The figures are well presented and clear but perhaps not very focused. figures 2 and 3 read more like a demonstration that "optogenetics works". It would have been more useful to focus on PiVR versatility and show several different use cases rather than only those two very basic ones. Somehow the focus of the figures is optogenetics, not PiVR.</p>
    <p>I also had troubles understanding part of figure 4 (F-J) and almost all of figure 5. Perhaps the concepts of slow vs fast dynamic VR can be explained in greater detail with a cartoon and examples of why and how an experimenter may want to use slow or fast dynamic VR could be provided.</p>
    <p>Rev. 3:</p>
    <p>This paper describes an open, low-cost platform to perform closed-loop behavioral experiments. The intention is to make such experiments more accessible or scalable by removing barriers of cost and development. The utility of the system is thoroughly demonstrated through example experiments in larval and adult Drosophila and zebrafish, which are analyzed to reveal new biological insights.</p>
    <p>The resources described in the paper are well designed and described and should be straightforward to apply to many current experimental questions. However, there are several existing platforms with overlapping aims and many of them are not cited or discussed in this paper. Some of these allow much more experimental flexibility than the generation of virtual environments through modulation of a one-dimensional signal that is used in the current paper. Therefore, it is not clear that the method fulfills the criteria required for a resource in PLOS Biology, in that it would enable experiments that were not possible using existing methods. In particular, it would be helpful to cite the following two resources and discuss them in comparison with the PIVR system:</p>
    <p>1) Stytra</p>
    <p>
      <ext-link ext-link-type="uri" xlink:href="http://www.portugueslab.com/pages/resources.html">http://www.portugueslab.com/pages/resources.html</ext-link>
    </p>
    <p>Stytra is a python-based package, which can be used for animal tracking and closed-loop presentation of stimuli. Implementations with low-cost hardware are possible. Štih V*, Petrucco L*, Kist AM, Portugues R (2019)</p>
    <p>Stytra: An open-source, integrated system for stimulation, tracking and closed-loop behavioral experiments. PLOS Computational Biology, doi:10.1371/journal.pcbi.1006699</p>
    <p>2) Bonsai.</p>
    <p>
      <ext-link ext-link-type="uri" xlink:href="http://www.kampff-lab.org/bonsai">http://www.kampff-lab.org/bonsai</ext-link>
    </p>
    <p>Bonsai is a software framework that is widely used for designing closed-loop behavior experiments, and can be used in conjunction with low cost hardware.</p>
    <p>Bonsai: An event-based framework for processing and controlling data streams. Lopes G, Bonacchi N, Frazão J, Neto JP, Atallah BV, Soares S, Moreira L, Matias S, Itskov PM, Correia PA, Medina RE, Calcaterra L, Dreosti E, Paton JJ, Kampff AR. Frontiers in Neuroinformatics. 2015; 9:7.</p>
    <p>Some minor issues:</p>
    <p>1) In the introductory discussion on virtual reality in neuroscience, it could be worth citing the productive use of virtual reality in work in zebrafish, in the context of motor adaptation, prey capture and social behavior.</p>
    <p>2) The closed-loop experiments based on optogenetic stimulation of the gustatory system in flies have technical similarities with experiments in the following paper, which might be appropriate to cite:</p>
    <p>Moreira, J.-M., Itskov, P. M., Goldschmidt, D., Baltazar, C., Steck, K., Tastekin, I., et al. (2019). optoPAD, a closed-loop optogenetics system to study the circuit basis of feeding behaviors. eLife, 8. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.7554/eLife.43924">http://doi.org/10.7554/eLife.43924</ext-link></p>
    <p>3) The terminology used in the zebrafish swimming experiments in not aligned to common usage in the literature, and could be confusing. Individual bursts of movement of the larvae are typically called 'bouts' , and the word 'scoots' is used to refer to a particular type of bout, where the tail oscillations are mostly confined to the caudal region, and which propels the larva forward at a slow speed, without major reorientation or head yaw (also often called 'Slow swims'). Routine turns, by contrast, start with a larger tail movement that reorients the larva, which can be followed by a propulsive phase. Therefore what are called 'scoots' in this paper may encompass both 'scoots' and 'turns' in the terminology of other studies.</p>
    <p>4) In the discussion of phototaxis in zebrafish, there are some other papers that perhaps are relevant to cite, because they either describe virtual reality based assays for phototaxis, or discuss the choice of turns vs scoots and the direction of turns. Ahrens, M. B., Huang, K. H., Narayan, S., Mensh, B. D., &amp; Engert, F. (2013). Frontiers in Neural Circuits, 7, 104.</p>
    <p>Fernandes, A. M., Fero, K., Arrenberg, A. B., Bergeron, S. A., Driever, W., &amp; Burgess, H. A. (2012). Current Biology, 22(21), 2042-2047. Horstick, E. J., Bayleyen, Y., Sinclair, J. L., &amp; Burgess, H. A. (2017). BMC Biology, 1-16.</p>
  </body>
</sub-article>
<sub-article id="pbio.3000712.r003" article-type="author-comment">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pbio.3000712.r003</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 1</article-title>
    </title-group>
    <related-article id="rel-obj003" ext-link-type="doi" xlink:href="10.1371/journal.pbio.3000712" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>2</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">9 Mar 2020</named-content>
    </p>
    <supplementary-material content-type="local-data" id="pbio.3000712.s020">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">200306__PointResponse_v2.pdf</named-content></p>
      </caption>
      <media xlink:href="pbio.3000712.s020.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article id="pbio.3000712.r004" article-type="editor-report">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pbio.3000712.r004</article-id>
    <title-group>
      <article-title>Decision Letter 2</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Alvarez-Garcia</surname>
          <given-names>Ines</given-names>
        </name>
        <role>Senior Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2020 Ines Alvarez-Garcia</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Ines Alvarez-Garcia</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj004" ext-link-type="doi" xlink:href="10.1371/journal.pbio.3000712" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>2</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">20 Mar 2020</named-content>
    </p>
    <p>Dear Matthieu,</p>
    <p>Thank you for submitting your revised Methods and Resources entitled "PiVR: an affordable and versatile closed-loop platform to study unrestrained sensorimotor behavior" for publication in PLOS Biology. I have now discussed the revision with the team of editors and obtained advice from the original Academic Editor. </p>
    <p>We're delighted to let you know that we're now editorially satisfied with your manuscript. However before we can formally accept your paper and consider it "in press", we also need to ensure that your article conforms to our guidelines. A member of our team will be in touch shortly with a set of requests. As we can't proceed until these requirements are met, your swift response will help prevent delays to publication. Please also make sure to address the data and other policy-related requests noted at the end of this email.</p>
    <p>*Copyediting*</p>
    <p>Upon acceptance of your article, your final files will be copyedited and typeset into the final PDF. While you will have an opportunity to review these files as proofs, PLOS will only permit corrections to spelling or significant scientific errors. Therefore, please take this final revision time to assess and make any remaining major changes to your manuscript.</p>
    <p>NOTE: If Supporting Information files are included with your article, note that these are not copyedited and will be published as they are submitted. Please ensure that these files are legible and of high quality (at least 300 dpi) in an easily accessible file format. For this reason, please be aware that any references listed in an SI file will not be indexed. For more information, see our Supporting Information guidelines:</p>
    <p>
      <ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosbiology/s/supporting-information">https://journals.plos.org/plosbiology/s/supporting-information</ext-link>
    </p>
    <p>*Published Peer Review History*</p>
    <p>Please note that you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out. Please see here for more details:</p>
    <p>
      <ext-link ext-link-type="uri" xlink:href="https://blogs.plos.org/plos/2019/05/plos-journals-now-open-for-published-peer-review/">https://blogs.plos.org/plos/2019/05/plos-journals-now-open-for-published-peer-review/</ext-link>
    </p>
    <p>*Early Version*</p>
    <p>Please note that an uncorrected proof of your manuscript will be published online ahead of the final version, unless you opted out when submitting your manuscript. If, for any reason, you do not want an earlier version of your manuscript published online, uncheck the box. Should you, your institution's press office or the journal office choose to press release your paper, you will automatically be opted out of early publication. We ask that you notify us as soon as possible if you or your institution is planning to press release the article.</p>
    <p>*Protocols deposition*</p>
    <p>To enhance the reproducibility of your results, we recommend that if applicable you deposit your laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. For instructions see: <ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/plosbiology/s/submission-guidelines#loc-materials-and-methods">https://journals.plos.org/plosbiology/s/submission-guidelines#loc-materials-and-methods</ext-link>
</p>
    <p>*Submitting Your Revision*</p>
    <p>To submit your revision, please go to <ext-link ext-link-type="uri" xlink:href="https://www.editorialmanager.com/pbiology/">https://www.editorialmanager.com/pbiology/</ext-link> and log in as an Author. Click the link labelled 'Submissions Needing Revision' to find your submission record. Your revised submission must include a cover letter, a Response to Reviewers file that provides a detailed response to the reviewers' comments (if applicable), and a track-changes file indicating any changes that you have made to the manuscript. </p>
    <p>Please do not hesitate to contact me should you have any questions.</p>
    <p>Best wishes,</p>
    <p>Ines</p>
    <p>--</p>
    <p>Ines Alvarez-Garcia, PhD</p>
    <p>Senior Editor</p>
    <p>PLOS Biology</p>
    <p>Carlyle House, Carlyle Road</p>
    <p>Cambridge, CB4 3DN</p>
    <p>+44 1223–442810</p>
    <p>------------------------------------------------------------------------</p>
    <p>DATA POLICY:</p>
    <p>You may be aware of the PLOS Data Policy, which requires that all data be made available without restriction: <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosbiology/s/data-availability">http://journals.plos.org/plosbiology/s/data-availability</ext-link>. I can see that you have deposited your data in Driad (DOI: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25349/D9ZK50">https://doi.org/10.25349/D9ZK50</ext-link>), but the link doesn't seem to be active and I can't check if the data we need before the manuscript enters production is available. Please either activate it or follow the instructions stated below.</p>
    <p>Note that we do not require all raw data (for more information, please also see this editorial: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.1001797">http://dx.doi.org/10.1371/journal.pbio.1001797</ext-link>). Rather, we ask that all individual quantitative observations that underlie the data summarized in the figures and results of your paper be made available in one of the following forms:</p>
    <p>1) Supplementary files (e.g., excel). Please ensure that all data files are uploaded as 'Supporting Information' and are invariably referred to (in the manuscript, figure legends, and the Description field when uploading your files) using the following format verbatim: S1 Data, S2 Data, etc. Multiple panels of a single or even several figures can be included as multiple sheets in one excel file that is saved using exactly the following convention: S1_Data.xlsx (using an underscore).</p>
    <p>2) Deposition in a publicly available repository. Please also provide the accession code or a reviewer link so that we may view your data before publication. </p>
    <p>Regardless of the method selected, please ensure that you provide the individual numerical values that underlie the summary data displayed in the following figure panels as they are essential for readers to assess your analysis and to reproduce it:</p>
    <p>Fig. 3C, F; Fig. 4G, H, J; Fig. S1B, C, D; Fig. S3E, D, I, J; Fig. S4E, F; Fig. S7A, B, C and Fig. S8A, B</p>
    <p>NOTE: the numerical data provided should include all replicates AND the way in which the plotted mean and errors were derived (it should not present only the mean/average values).</p>
    <p>Please also ensure that figure legends in your manuscript include information on where the underlying data can be found, and ensure your supplemental data file/s has a legend.</p>
    <p>Please ensure that your Data Statement in the submission system accurately describes WHERE YOUR DATA CAN BE FOUND.</p>
  </body>
</sub-article>
<sub-article id="pbio.3000712.r005" article-type="author-comment">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pbio.3000712.r005</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 2</article-title>
    </title-group>
    <related-article id="rel-obj005" ext-link-type="doi" xlink:href="10.1371/journal.pbio.3000712" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>3</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">14 May 2020</named-content>
    </p>
    <supplementary-material content-type="local-data" id="pbio.3000712.s021">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">Point-by-point_answer_final.pdf</named-content></p>
      </caption>
      <media xlink:href="pbio.3000712.s021.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article id="pbio.3000712.r006" article-type="editor-report">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pbio.3000712.r006</article-id>
    <title-group>
      <article-title>Decision Letter 3</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Alvarez-Garcia</surname>
          <given-names>Ines</given-names>
        </name>
        <role>Senior Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2020 Ines Alvarez-Garcia</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Ines Alvarez-Garcia</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj006" ext-link-type="doi" xlink:href="10.1371/journal.pbio.3000712" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>3</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">9 Jun 2020</named-content>
    </p>
    <p>Dear Dr Louis,</p>
    <p>On behalf of my colleagues and the Academic Editor, Tom Baden, I am pleased to inform you that we will be delighted to publish your Methods and Resources in PLOS Biology. </p>
    <p>The files will now enter our production system. You will receive a copyedited version of the manuscript, along with your figures for a final review. You will be given two business days to review and approve the copyedit. Then, within a week, you will receive a PDF proof of your typeset article. You will have two days to review the PDF and make any final corrections. If there is a chance that you'll be unavailable during the copy editing/proof review period, please provide us with contact details of one of the other authors whom you nominate to handle these stages on your behalf. This will ensure that any requested corrections reach the production department in time for publication.</p>
    <p>Early Version</p>
    <p>The version of your manuscript submitted at the copyedit stage will be posted online ahead of the final proof version, unless you have already opted out of the process. The date of the early version will be your article's publication date. The final article will be published to the same URL, and all versions of the paper will be accessible to readers.</p>
    <p>PRESS </p>
    <p>We frequently collaborate with press offices. If your institution or institutions have a press office, please notify them about your upcoming paper at this point, to enable them to help maximise its impact. If the press office is planning to promote your findings, we would be grateful if they could coordinate with <email>biologypress@plos.org</email>. If you have not yet opted out of the early version process, we ask that you notify us immediately of any press plans so that we may do so on your behalf.</p>
    <p>We also ask that you take this opportunity to read our Embargo Policy regarding the discussion, promotion and media coverage of work that is yet to be published by PLOS. As your manuscript is not yet published, it is bound by the conditions of our Embargo Policy. Please be aware that this policy is in place both to ensure that any press coverage of your article is fully substantiated and to provide a direct link between such coverage and the published work. For full details of our Embargo Policy, please visit <ext-link ext-link-type="uri" xlink:href="http://www.plos.org/about/media-inquiries/embargo-policy/">http://www.plos.org/about/media-inquiries/embargo-policy/</ext-link>.</p>
    <p>Thank you again for submitting your manuscript to PLOS Biology and for your support of Open Access publishing. Please do not hesitate to contact me if I can provide any assistance during the production process.</p>
    <p>Kind regards,</p>
    <p>Alice Musson</p>
    <p>Publishing Editor, </p>
    <p>PLOS Biology</p>
    <p>on behalf of</p>
    <p>Ines Alvarez-Garcia,</p>
    <p>Senior Editor</p>
    <p>PLOS Biology</p>
  </body>
</sub-article>
