<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neurosci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neurosci</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neurosci.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroscience</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1662-4548</issn>
    <issn pub-type="epub">1662-453X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6788344</article-id>
    <article-id pub-id-type="doi">10.3389/fnins.2019.01053</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Quantifying Neurodegenerative Progression With DeepSymNet, an End-to-End Data-Driven Approach</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Pena</surname>
          <given-names>Danilo</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/766567/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Barman</surname>
          <given-names>Arko</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/781854/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Suescun</surname>
          <given-names>Jessika</given-names>
        </name>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/606426/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jiang</surname>
          <given-names>Xiaoqian</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schiess</surname>
          <given-names>Mya C.</given-names>
        </name>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/815612/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Giancardo</surname>
          <given-names>Luca</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff4">
          <sup>4</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/585231/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <collab>the Alzheimer's Disease Neuroimaging Initiative</collab>
        <xref ref-type="author-notes" rid="fn002">
          <sup>†</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>School of Biomedical Informatics, University of Texas Health Science Center at Houston (UTHealth)</institution>, <addr-line>Houston, TX</addr-line>, <country>United States</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Center for Precision Health, UTHealth</institution>, <addr-line>Houston, TX</addr-line>, <country>United States</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Department of Neurology, McGovern Medical School, UTHealth</institution>, <addr-line>Houston, TX</addr-line>, <country>United States</country></aff>
    <aff id="aff4"><sup>4</sup><institution>Center for Precision Health, UTHealth Diagnostic and Interventional Imaging, McGovern Medical School, UTHealth Institute for Stroke and Cerebrovascular Diseases, UTHealth</institution>, <addr-line>Houston, TX</addr-line>, <country>United States</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Mackenzie W. Mathis, Harvard University, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Blake A. Richards, Montreal Neurological Institute, Mcgill University, Canada; Bing Zhang, Nanjing Drum Tower Hospital, China</p>
      </fn>
      <corresp id="c001">*Correspondence: Luca Giancardo <email>luca.giancardo@uth.tmc.edu</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Brain Imaging Methods, a section of the journal Frontiers in Neuroscience</p>
      </fn>
      <fn fn-type="other" id="fn002">
        <p>†Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (<ext-link ext-link-type="uri" xlink:href="http://adni.loni.usc.edu">adni.loni.usc.edu</ext-link>). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: <ext-link ext-link-type="uri" xlink:href="http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf">http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf</ext-link></p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>04</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>13</volume>
    <elocation-id>1053</elocation-id>
    <history>
      <date date-type="received">
        <day>04</day>
        <month>7</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>19</day>
        <month>9</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2019 Pena, Barman, Suescun, Jiang, Schiess, Giancardo and the Alzheimer's Disease Neuroimaging Initiative.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Pena, Barman, Suescun, Jiang, Schiess, Giancardo and the Alzheimer's Disease Neuroimaging Initiative</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Alzheimer's disease (AD) is the most common neurodegenerative disorder worldwide and is one of the leading sources of morbidity and mortality in the aging population. There is a long preclinical period followed by mild cognitive impairment (MCI). Clinical diagnosis and the rate of decline is variable. Progression monitoring remains a challenge in AD, and it is imperative to create better tools to quantify this progression. Brain magnetic resonance imaging (MRI) is commonly used for patient assessment. However, current approaches for analysis require strong a priori assumptions about regions of interest used and complex preprocessing pipelines including computationally expensive non-linear registrations and iterative surface deformations. These preprocessing steps are composed of many stacked processing layers. Any error or bias in an upstream layer will be propagated throughout the pipeline. Failures or biases in the non-linear subject registration and the subjective choice of atlases of specific regions are common in medical neuroimaging analysis and may hinder the translation of many approaches to the clinical practice. Here we propose a data-driven method based on an extension of a deep learning architecture, DeepSymNet, that identifies longitudinal changes without relying on prior brain regions of interest, an atlas, or non-linear registration steps. Our approach is trained end-to-end and learns how a patient's brain structure dynamically changes between two-time points directly from the raw voxels. We compare our approach with Freesurfer longitudinal pipelines and voxel-based methods using the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Our model can identify AD progression with comparable results to existing Freesurfer longitudinal pipelines without the need of predefined regions of interest, non-rigid registration algorithms, or iterative surface deformation at a fraction of the processing time. When compared to other voxel-based methods which share some of the same benefits, our model showed a statistically significant performance improvement. Additionally, we show that our model can differentiate between healthy subjects and patients with MCI. The model's decision was investigated using the epsilon layer-wise propagation algorithm. We found that the predictions were driven by the pallidum, putamen, and the superior temporal gyrus. Our novel longitudinal based, deep learning approach has the potential to diagnose patients earlier and enable new computational tools to monitor neurodegeneration in clinical practice.</p>
    </abstract>
    <kwd-group>
      <kwd>Alzheimer's disease</kwd>
      <kwd>deep learning</kwd>
      <kwd>magnetic resonance imaging</kwd>
      <kwd>progression</kwd>
      <kwd>biomarkers</kwd>
      <kwd>longitudinal</kwd>
      <kwd>ADNI</kwd>
    </kwd-group>
    <counts>
      <fig-count count="8"/>
      <table-count count="6"/>
      <equation-count count="4"/>
      <ref-count count="55"/>
      <page-count count="14"/>
      <word-count count="9786"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Introduction</title>
    <p>Alzheimer's disease (AD) is the leading cause of dementia globally (50–75%) and is distinguished by a progressive cognitive decline (Lane et al., <xref rid="B25" ref-type="bibr">2018</xref>). Currently, 5.8 million Americans suffer AD, and by 2050, this number will rise to 14 million (Alzheimer's Association, <xref rid="B2" ref-type="bibr">2016</xref>). Criteria for the diagnosis of probable AD is based on subjective clinical assessments (Pfeffer et al., <xref rid="B38" ref-type="bibr">1982</xref>; Marshall et al., <xref rid="B32" ref-type="bibr">2015</xref>). There are multiple treatments available that can ameliorate some of the symptoms, but none of these drugs alter the course of the disease, and inevitably the dementia progresses in all patients. Neuroprotective and other disease-modifying therapies are under active development, however, to demonstrate their efficacy, sensitive, and reproducible metrics to measure disease progression are urgently needed, particularly at the earliest stage of the disease when therapies are more likely to slow the neurodegenerative progression (Aisen et al., <xref rid="B1" ref-type="bibr">2017</xref>).</p>
    <p>MRI based biomarkers for AD have been widely studied. Multiple groups have used imaging data to understand how regional brain atrophy, connectivity, or physical proximity can serve as biomarkers for dementia (Lillemark et al., <xref rid="B28" ref-type="bibr">2014</xref>) or how these can be used to develop deep learning networks for AD classification (Litjens et al., <xref rid="B29" ref-type="bibr">2017</xref>). These techniques are being developed to improve the accuracy of and to provide a quantitative, data-driven approach for AD disease diagnosis (Weiner et al., <xref rid="B51" ref-type="bibr">2017</xref>).</p>
    <p>In the past decade, researchers explored many avenues of the AD classification problem using machine learning. Over the years, there have been extensive reviews summarizing the state of the art of these methods (Rathore et al., <xref rid="B39" ref-type="bibr">2017</xref>; Pellegrini et al., <xref rid="B35" ref-type="bibr">2018</xref>). Recently, some approaches include one imaging modality (typically MRI) (Long et al., <xref rid="B30" ref-type="bibr">2017</xref>), multiple imaging modalities data (Zhang et al., <xref rid="B55" ref-type="bibr">2011</xref>, <xref rid="B54" ref-type="bibr">2012</xref>), brain connectivity (de Vos et al., <xref rid="B10" ref-type="bibr">2018</xref>), and genetic data (Peng et al., <xref rid="B37" ref-type="bibr">2016</xref>). The most commonly used machine learning models are support vector machines, though there is definitely diversity in techniques (Pellegrini et al., <xref rid="B35" ref-type="bibr">2018</xref>). Many of these papers focus on AD vs. CN classification, but studies are also looking at other classification tasks such as CN vs. MCI (Samper-González et al., <xref rid="B44" ref-type="bibr">2018</xref>). All these methods are used for diagnosing a patient based on a model that was trained on cross-sectional data.</p>
    <p>There is a growing interest in using machine learning to understand disease progression, and this is made possible by the available datasets for neurodegenerative disorders (Marcus et al., <xref rid="B31" ref-type="bibr">2010</xref>). Researchers have used this longitudinal data to create brain development trajectories used to predict the risk of developing AD (Lawrence et al., <xref rid="B26" ref-type="bibr">2017</xref>), to develop clinical symptom trajectories (Bhagwat et al., <xref rid="B7" ref-type="bibr">2018</xref>) to extract essential brain features in MCI classification (Huang et al., <xref rid="B19" ref-type="bibr">2017</xref>; Sun et al., <xref rid="B47" ref-type="bibr">2017</xref>), and to investigate different stages of AD progression from a multi-modal imaging standpoint (Gray et al., <xref rid="B16" ref-type="bibr">2012</xref>; Rodrigues et al., <xref rid="B42" ref-type="bibr">2014</xref>; Nozadi et al., <xref rid="B33" ref-type="bibr">2018</xref>). Similar work in Parkinson's disease used longitudinal connectome data as a marker for neurodegenerative progression (Peña-Nogales et al., <xref rid="B36" ref-type="bibr">2018</xref>). In areas such as genetics, longitudinal studies have proved beneficial by revealing more single nucleotide polymorphism phenotype associations than cross-sectional studies in AD research (Xu et al., <xref rid="B52" ref-type="bibr">2014</xref>). Despite the progress, existing methods use a priori hypotheses and feature engineering in their neuroimaging processing pipeline. A typical example would be the computation of the volumetric changes on a predefined number of brain areas which are used as an input to a machine learning or statistical model. This approach has two main limitations: (1), it is bound to the a-priori selection of specific brain areas, making it impractical to model disease progressions that are not fully understood; (2), any error in the estimation of the brain areas metrics would negatively influence the machine learning or statistical model (i.e., garbage in, garbage out).</p>
    <p>To overcome the limitation of engineered features, recent studies have used the concept of feature extraction through deep learning techniques. This allows researchers to automatically extract image representations from the raw voxels specific to the outcome needed. Studies have used deep learning to tune convolutional neural networks (CNNs) on MRI images (Backstrom et al., <xref rid="B5" ref-type="bibr">2018</xref>) and to process multi-modal information including genetic and neuropsychological data (Spasov et al., <xref rid="B46" ref-type="bibr">2019</xref>). Further, others have used deep learning to complete other related tasks like segmentation and brain parcellation (Li et al., <xref rid="B27" ref-type="bibr">2017</xref>; Gibson et al., <xref rid="B14" ref-type="bibr">2018</xref>). ADNI-based machine and deep learning reviews are being written as a result (Weiner et al., <xref rid="B51" ref-type="bibr">2017</xref>). However, we are not aware of end-to-end feature learning approaches to measure longitudinal changes that does not require pre-defined brain areas or region of interests for training.</p>
    <p>In this work, we propose to use a DeepSymNet-based model, a novel end-to-end deep learning architecture, to identify longitudinal neurodegenerative progression between structural MRI images with minimal preprocessing at two-time points. We adapt the DeepSymNet architecture presented by Barman et al. (<xref rid="B6" ref-type="bibr">2019</xref>) to identify structural brain differences by learning time-sensitive representation on a subject-level. The imaging preprocessing pipeline required by the architecture does not use a priori brain regions or non-rigid registration algorithms making the process more robust by having fewer steps throughout the pipeline and more efficient in terms of time required to generate hypotheses and computational time than common longitudinal processing pipelines such as Freesurfer. This work has four main contributions: (1) a new neuroimaging pipeline to measure neurodegenerative progression that does not require pre-defined brain areas or region of interests for training, (2) comparable classification performance when compared with existing Freesurfer and voxel-based longitudinal pipelines for AD-relevant progression, (3) higher computational efficiency and generalizability to external dataset of mild cognitive impairment (MCI) subjects, and (4) analysis of the brain areas that drive the model's decision. This manuscript's code can be found at <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/lgianca/longitudinal-deepsymnet">https://gitlab.com/lgianca/longitudinal-deepsymnet</ext-link>.</p>
  </sec>
  <sec sec-type="materials and methods" id="s2">
    <title>Materials and Methods</title>
    <p>Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (<ext-link ext-link-type="uri" xlink:href="https://www.adni.loni.usc.edu">adni.loni.usc.edu</ext-link>) in November 2018. The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD). The ADNI data were downloaded in November 2018 (<ext-link ext-link-type="uri" xlink:href="https://ida.loni.usc.edu/login.jsp">https://ida.loni.usc.edu/login.jsp</ext-link>). The data were then processed according to the Brain Imaging Data Structure (BIDS) format (Gorgolewski et al., <xref rid="B15" ref-type="bibr">2016</xref>). This structure allows researchers to organize their neuroimaging-related data in a concise way that allows groups to access a growing number of computational tools and pipelines compatible with the BIDS format.</p>
    <p>In this study, we included AD, MCI, and CN subjects from ADNI who had at least two T1-weighted brain images at least 6 months apart from ADNI1, ADNIGO, and ADNI2, and this resulted in 971 patients. When patients who had more than two imaging time points, we chose the first and last sessions. The two sessions are referred to as “session 1” and “session 2” throughout the paper. Further, all the patients chosen stayed within their disease phenotype at and between the two imaging sessions and successfully passed the image preprocessing pipeline. As shown in <xref rid="T1" ref-type="table">Table 1</xref>, after the above criteria were applied, we had a total of 632 subjects between the three groups. A Kruskal-Wallis test was performed on the cohort demographics. The statistical test succeeded to reject the null hypothesis that the samples originate from the same distribution for the sex and time between sessions variables. In our comparative analysis described later, we corrected for these potential confounders.</p>
    <table-wrap id="T1" position="float">
      <label>Table 1</label>
      <caption>
        <p>Demographics and time between imaging sessions of the AD, CN, and MCI patients used in this study (one standard deviation–s.d.).</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th rowspan="1" colspan="1"/>
            <th valign="top" align="center" rowspan="1" colspan="1">
              <bold>AD</bold>
            </th>
            <th valign="top" align="center" rowspan="1" colspan="1">
              <bold>CN</bold>
            </th>
            <th valign="top" align="center" rowspan="1" colspan="1">
              <bold>MCI</bold>
            </th>
            <th valign="top" align="center" rowspan="1" colspan="1">
              <bold><italic>p</italic>-value</bold>
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Number of patients</td>
            <td valign="top" align="center" rowspan="1" colspan="1">212</td>
            <td valign="top" align="center" rowspan="1" colspan="1">270</td>
            <td valign="top" align="center" rowspan="1" colspan="1">150</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">First session age, years [mean (s.d.)]</td>
            <td valign="top" align="center" rowspan="1" colspan="1">74.8 (7.7)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">74.7 (5.9)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">75.3 (7.3)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">0.381</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Time between sessions, years [mean (s.d.)]</td>
            <td valign="top" align="center" rowspan="1" colspan="1">1.6 (0.6)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">4.4 (2.6)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">3.4 (2.8)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">&lt;0.001</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Sex [male, n (%)]</td>
            <td valign="top" align="center" rowspan="1" colspan="1">108 (50.9%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">138 (51.1%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">99 (66%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">&lt;0.01</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Race/Ethnicity [n (%)] -</td>
            <td valign="top" align="center" rowspan="1" colspan="1">0 (0%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">0 (0%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">0 (0%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">0.403</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">American Indian/Alaskan Native,</td>
            <td valign="top" align="center" rowspan="1" colspan="1">4 (1.9%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">6 (2.2%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">3 (2.0%)</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Asian</td>
            <td valign="top" align="center" rowspan="1" colspan="1">0 (0%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">0 (0%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">0 (0%)</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Native Hawaiian/Other Pacific Islander</td>
            <td valign="top" align="center" rowspan="1" colspan="1">12 (5.6%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">20 (7.4%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">8 (5.3%)</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Black/African American</td>
            <td valign="top" align="center" rowspan="1" colspan="1">193 (91.1%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">242 (89.6%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">139 (92.7%)</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">White, More than one race</td>
            <td valign="top" align="center" rowspan="1" colspan="1">3 (1.4%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">2 (0.8%)</td>
            <td valign="top" align="center" rowspan="1" colspan="1">0 (0%)</td>
            <td rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Unknown</td>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
            <td rowspan="1" colspan="1"/>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>We will divide the methods and techniques we used into four main topics: (1) overviews of the Freesurfer-based, voxel-based, and DeepSymNet pipelines, (2) experimental design, (3) computational time analysis, (4) progression generalizability tests, and (5) confounding variable adjustment.</p>
    <p>In the three sections, we provide an overview of the Freesurfer longitudinal pipeline, a description of the feature sets used for machine learning, and their accompanying experiments. Second, we describe the DeepSymNet-based pipeline, the experiments conducted, and the relevant brain area analysis. Third, we discuss the voxel-based machine learning method. Next, we describe comparison experiments for the above pipelines where we evaluated the computational time requirements. We then apply the models on an external MCI cohort to test the generalizability of the different pipelines and to identify progression patterns on this cohort at risk of developing AD. Finally, we describe how we evaluate the effect of potential confounders in the models.</p>
    <sec>
      <title>Data Preprocessing and Feature Set Creation Using Freesurfer Pipelines</title>
      <p>As seen in <xref ref-type="fig" rid="F1">Figure 1</xref> below, Freesurfer extracts the brain region volumes using multiple predefined (and time intensive) steps such as within-subject template creation, atlas registration, non-linear transformations, surface inflating. Here, we aimed to compare the model performance using two different Freesurfer version's atlas-based, longitudinal pipelines. The pipeline features will be referred to as the first feature set (FS 1) and the second feature set (FS 2), respectively. For FS 1, the “University of California San Francisco's Longitudinal Freesurfer (5.1) All Available Base Image [ADNIGO, 2]” file downloaded from the ADNI website. This independent and external source was critical to ensure that our results were comparable with already published research. The FS 2 set was taken from the output of the Freesurfer (6.0) longitudinal pipeline. A full description of the Freesurfer longitudinal pipeline is beyond the scope of this paper, and we would like to refer readers to the original publications for more information (Reuter et al., <xref rid="B40" ref-type="bibr">2010</xref>, <xref rid="B41" ref-type="bibr">2012</xref>; Iglesias et al., <xref rid="B20" ref-type="bibr">2016</xref>). These steps are typically what researchers use in neuroimaging pipelines.</p>
      <fig id="F1" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Overview of <bold>(A)</bold> atlas-based and non-linear registration imaging preprocessing pipeline, <bold>(B)</bold> voxel-based methods, and <bold>(C)</bold> DeepSymNet-based pipeline implementation. Note that all processes involve rigid registration to align the patient's brains longitudinally.</p>
        </caption>
        <graphic xlink:href="fnins-13-01053-g0001"/>
      </fig>
      <p>The patients between the two datasets (FS 1 and FS 2) were matched using the patient's RID (specific to ADNI's protocol) and by the closest visit date. A total of 93 subjects matched between the two datasets. For each patient, the structural MRI variables output from the pipelines were used as a feature vector. The FS 2 regional volume data came from the subcortical segmentation file (aseg.stats) and white matter parcellation files (wmparc.stats) uniquely using the region sizes. The FS 1 pipeline took data from both the header and body of the segmentation files in addition to additional statistics data from the parcellation files (e.g., thickness average and standard deviation, and surface area) which resulted in a larger feature set. For both pipelines, the same cortical and subcortical regions were investigated. Finally, the FS 2 pipeline was compared to the DeepSymNet pipeline using both the limited cohort of 93 patients and the full cohort.</p>
    </sec>
    <sec>
      <title>DeepSymNet-Based Pipeline Overview</title>
      <sec>
        <title>Data Preprocessing</title>
        <p>For the DeepSymNet preprocessing implementation, a simple longitudinal pipeline that used skull stripping, normalization, and a patient-specific alignment (<xref ref-type="fig" rid="F1">Figure 1</xref>). This pipeline has several advantages for longitudinal studies. First, it uses rigid registration, which decreases computational and time costs associated with imaging pipelines. Second, there is no dependence on predefined brain regions, which enables us to take a more data-driven approach to understand progression. Third, the pipeline finds the best space to register the individual patient to which reduces the overall noise and bias that occurs when comparing samples between each other. The output of the MRI images was 182 × 218 × 182 with a resolution of 1 × 1 × 1 mm<sup>3</sup>. Empty voxels outside of the brain were deleted from the image and not used in the subsequent analyses.</p>
      </sec>
      <sec>
        <title>DeepSymNet Architecture Overview</title>
        <p>The Deep Symmetry-Sensitive Convolutional Neural Network (DeepSymNet) architecture (<xref ref-type="fig" rid="F2">Figure 2</xref>) used in this study was inspired from a model designed to identify spatial symmetries in brain angiograms (Barman et al., <xref rid="B6" ref-type="bibr">2019</xref>). We applied this architecture to identify changes through time as opposed to spatial symmetries. This enables the architecture to directly learn a representation sensitive to intra-patients changes, rather than model the complex inter-patient heterogeneity and measure it over time. The model receives as input two brains at different time points, an initial step that learns a common representation between the two time points by 3-dimensional (3-D) Inception modules with weight sharing, which is then followed by a merge layer where the output of the filters is subtracted from one another, then, another set of 3-D Inception modules learn a representation sensitive to change. Finally, a max pooling and fully connected layer estimate the likelihood of a disease-relevant progression.</p>
        <fig id="F2" position="float">
          <label>Figure 2</label>
          <caption>
            <p>Deep Symmetry-Sensitive Convolutional Neural Network (DeepSymNet) architecture overview. Longitudinal images go through a Siamese network composed of 3-D Inception modules that share weights. These outputs are passed through a L-1 merge layer that computes differences between these two sessions. Then, this is passed through a final 3-D Inception layer to learn from these differences. Lastly, these outputs are flattened and put through a dense layer for the final AD-progression prediction.</p>
          </caption>
          <graphic xlink:href="fnins-13-01053-g0002"/>
        </fig>
        <p>As AD is characterized by structural brain degeneration, a model like the one described has the potential of identifying the structural differences or progression patterns between the two MRI acquisition regardless of the brain appearance at during the first imaging session.</p>
      </sec>
      <sec>
        <title>DeepSymNet Architecture Detailed</title>
        <p>Here, we summarize the different components of DeepSymNet and walk the readers through our specific design.</p>
        <sec>
          <title>Shared weights</title>
          <p>Each of the two images is fed into identical neural networks before they are merged together. Additionally, these identical neural networks share weights, allowing them to learn the same patterns in the two images. This architecture allows the network to learn complex differences between the two imaging timepoints and encode information specific to that visit. Further, this part of the network can learn asymmetric patterns as neurodegeneration may differ between right and left hemispheres.</p>
        </sec>
        <sec>
          <title>Inception module</title>
          <p>The Inception modules are composed of multiresolution 3-D convolutional filters that learn to represent the T1 images at different time points according to a loss function. It should be noted that these Inception modules are the 3-D extension to what is presented by Szegedy et al. (<xref rid="B48" ref-type="bibr">2015</xref>), and the modules are not the same full Inception network architecture. The Inception module concatenates several parallel convolutional layers. It can be thought of as a mini network within a larger network. Since MRI images are 3-D images, 3-D convolutional filters are used within the Inception modules. For this application, the Inception module consisted of 1 × 1 × 1, 3 × 3 × 3, and 5 × 5 × 5 convolutions, followed by concatenation and a max pooling layer. These different convolution sizes enable the network to learn features at different scales, which could allow for more complex pattern recognition. Sixty-four filters have been used in the Inception modules and the network uses rectified linear unit (ReLU) activations.</p>
        </sec>
        <sec>
          <title>Merge layer</title>
          <p>An L-1 difference was then used to combine the learned convolutional features from both sessions. This encodes critical information about the structural brain changes between the timepoints, and this information is passed through another Inception layer.</p>
        </sec>
        <sec>
          <title>Fully connected layers</title>
          <p>After the merge and Inception layers, the difference between the two images is transformed into a feature vector. This vector is then passed through a fully connected layer, which creates a linear combination of the filter outputs from the penultimate layer of the network. A SoftMax operation is used as the activation function in the last layer for AD-progression prediction.</p>
          <p>The DeepSymNet network was not designed to be fully translational invariant like the classical Inception Network (Szegedy et al., <xref rid="B48" ref-type="bibr">2015</xref>). Rather, the algorithm is designed to be insensitive to the inevitable small registration inaccuracies between the two timepoints that may be present due to the rigid registration step. This is achieved through the aforementioned max pooling layers. Overall, this network is relatively shallow compared to some of the deep networks commonly reported in literature. The multiresolution filters within the inception modules in addition to the L-1 merge layer allow for complex image representation through space and time.</p>
        </sec>
      </sec>
      <sec>
        <title>Implementation</title>
        <p>The experiments were completed with Python (3.6.8). The DeepSymNet was implemented with Keras (2.2.4) with TensorFlow (1.12.0) as the backend. From a hardware perspective, we used Nvidia's Tesla V100 graphics cards with 32 GB RAM. The training times for each epoch varied depending on the number of model parameters, and these training lengths could range from 9 to 400 s. Each fold had 150 epochs, and a batch size of four was used. Early stopping based on a lack of improvement in validation loss for 30 epochs was also employed to reduce unnecessary computational cost. Binary cross-entropy was used as the loss function, and the Adam optimizer was used (Kingma and Ba, <xref rid="B23" ref-type="bibr">2015</xref>).</p>
      </sec>
      <sec>
        <title>Brain Region Relevance Analysis</title>
        <p>In order to understand which voxels contributed to the model's predictions, the epsilon layer-wise relevance propagation (ϵ-LRP) method was used to analyze the contributions individual voxels on the final prediction (Bach et al., <xref rid="B4" ref-type="bibr">2015</xref>). In summary, the ϵ-LRP method decomposes the output of deep learning architectures, on a sample level, into relevance scores in a backward fashion layer-by-layer. These scores can be projected on the pixel or voxel level of the input, which in our case is the full brain MRI, and we used a heatmap to visualize the relative magnitude of the scores. The ϵ-LRP implementation of the open-source package DeepExplain (<ext-link ext-link-type="uri" xlink:href="https://github.com/marcoancona/DeepExplain">https://github.com/marcoancona/DeepExplain</ext-link>) was used (Ancona and Gross, <xref rid="B3" ref-type="bibr">2017</xref>).</p>
        <p>To create this visualization that demonstrated which parts of the brain were important for DeepSymNet's predictions on AD and CN subjects, we started with the chosen model after the hyperparameter tuning test discussed in the next section. We computed the subject-level relevance with the ϵ-LRP method for both imaging sessions, and we ensured that the voxel magnitudes used were from the test set subjects. For each <italic>n</italic>-th patient, we added the absolute value of all the patients' voxel magnitude, <italic>m</italic>, for both sessions together into a single brain volume, <italic>M</italic><sub><italic>N</italic></sub>.</p>
        <disp-formula id="E1">
          <mml:math id="M1">
            <mml:mtable columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi class="textit" mathvariant="italic">M</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi class="textit" mathvariant="italic">N</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo class="textit" mathvariant="italic">=</mml:mo>
                  <mml:mstyle displaystyle="true">
                    <mml:munderover accentunder="false" accent="false">
                      <mml:mrow>
                        <mml:mo>∑</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi class="textit" mathvariant="italic">i</mml:mi>
                        <mml:mo class="textit" mathvariant="italic">=</mml:mo>
                        <mml:mi class="textit" mathvariant="italic">n</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi class="textit" mathvariant="italic">N</mml:mi>
                      </mml:mrow>
                    </mml:munderover>
                  </mml:mstyle>
                  <mml:mo stretchy="false">|</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi class="textit" mathvariant="italic">m</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi class="textit" mathvariant="italic">n</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo stretchy="false">|</mml:mo>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>Where <italic>m</italic> is the voxel in the brain volume after registering to a common space and <italic>N</italic> represents the total number of patients.</p>
        <p>Next, we wanted to determine the relative importance of each brain region using the magnitude of a voxel's relevance. We used the Harvard-Oxford cortical and subcortical maps to segment the brain into interpretable regions (Caviness et al., <xref rid="B8" ref-type="bibr">1996</xref>). We used these respective maps to segment the voxels into different brain regions, <italic>R</italic>. We took the sum of all the voxel's magnitudes <inline-formula><mml:math id="M2"><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, within each region, <italic>r</italic>, and then normalized this summation by the volume, <italic>V</italic><sub><italic>r</italic></sub>, of the respective regions. This would result in the normalized volume magnitude, <italic>M</italic><sub><italic>r</italic></sub>.</p>
        <disp-formula id="E2">
          <mml:math id="M3">
            <mml:mtable columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mstyle mathvariant="italic">
                        <mml:mi>M</mml:mi>
                      </mml:mstyle>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mstyle mathvariant="italic">
                        <mml:mi>r</mml:mi>
                      </mml:mstyle>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>=</mml:mo>
                  </mml:mstyle>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mstyle displaystyle="true">
                        <mml:msubsup>
                          <mml:mrow>
                            <mml:mo>∑</mml:mo>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mstyle mathvariant="italic">
                              <mml:mi>v</mml:mi>
                            </mml:mstyle>
                            <mml:mstyle mathvariant="italic">
                              <mml:mo>=</mml:mo>
                            </mml:mstyle>
                            <mml:mstyle mathvariant="italic">
                              <mml:mi>1</mml:mi>
                            </mml:mstyle>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mstyle mathvariant="italic">
                                  <mml:mi>V</mml:mi>
                                </mml:mstyle>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mstyle mathvariant="italic">
                                  <mml:mi>r</mml:mi>
                                </mml:mstyle>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mrow>
                        </mml:msubsup>
                      </mml:mstyle>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mstyle mathvariant="italic">
                            <mml:mi>m</mml:mi>
                          </mml:mstyle>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mstyle mathvariant="italic">
                            <mml:mi>v</mml:mi>
                          </mml:mstyle>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mstyle mathvariant="italic">
                            <mml:mi>r</mml:mi>
                          </mml:mstyle>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mstyle mathvariant="italic">
                            <mml:mi>V</mml:mi>
                          </mml:mstyle>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mstyle mathvariant="italic">
                            <mml:mi>r</mml:mi>
                          </mml:mstyle>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mtd>
              </mml:mtr>
              <mml:mtr>
                <mml:mtd>
                  <mml:mtext>    </mml:mtext>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>∀</mml:mo>
                    <mml:mi>r</mml:mi>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>=</mml:mo>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>1</mml:mo>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>,</mml:mo>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>2</mml:mo>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>,</mml:mo>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>…</mml:mo>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="italic">
                    <mml:mi>R</mml:mi>
                  </mml:mstyle>
                </mml:mtd>
              </mml:mtr>
              <mml:mtr>
                <mml:mtd>
                  <mml:mtext>    </mml:mtext>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>∀</mml:mo>
                    <mml:mi>v</mml:mi>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>=</mml:mo>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>1</mml:mo>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>,</mml:mo>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>2</mml:mo>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>,</mml:mo>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="italic">
                    <mml:mo>…</mml:mo>
                  </mml:mstyle>
                  <mml:mstyle mathvariant="italic">
                    <mml:mi>V</mml:mi>
                  </mml:mstyle>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>This allowed us to see and interpret the relative progression-related importance of these different regions for the model's decision.</p>
      </sec>
    </sec>
    <sec>
      <title>Voxel-Based Machine Learning</title>
      <p>In addition to the models discussed above, we experimented with a set of general-purpose machine learning models receiving the same input as the DeepSymNet architecture (<xref ref-type="fig" rid="F1">Figure 1B</xref>). To construct the feature vector for these models, the L-1 difference between the two images was calculated, and the resulting 3-D array was flattened into one dimension and given as input to the model. Linear support vector machine and random forest classifiers were chosen as preliminary models. However, in order to avoid any model bias, we also included a strategy that incorporated robust ensemble model construction through meta-learning and Bayesian optimization (Feurer et al., <xref rid="B12" ref-type="bibr">2015</xref>). This strategy was completed using the AutoML library (<ext-link ext-link-type="uri" xlink:href="https://automl.github.io/auto-sklearn/master/">https://automl.github.io/auto-sklearn/master/</ext-link>). The architecture here is essentially optimized over the parameter space to find the “optimal” solution in order to mitigate human negligence, which offers a good baseline for our comparison.</p>
    </sec>
    <sec>
      <title>Experimental Design for Machine Learning Models</title>
      <p>For the FS 1 and FS 2 sets, longitudinal based classification tasks were performed to assess the reliability between the two different pipelines. These tests used an L1-regularized logistic regression as the classifier, and the features (e.g., regional brain volumes) were scaled with respect to their inter-quartile range</p>
      <disp-formula id="E3">
        <mml:math id="M4">
          <mml:mtable columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mstyle mathvariant="italic">
                          <mml:mi>x</mml:mi>
                        </mml:mstyle>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>r</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mstyle mathvariant="italic">
                      <mml:mo>-</mml:mo>
                    </mml:mstyle>
                    <mml:mtext> </mml:mtext>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mstyle mathvariant="italic">
                          <mml:mi>Q</mml:mi>
                        </mml:mstyle>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mstyle mathvariant="italic">
                          <mml:mi>1</mml:mi>
                        </mml:mstyle>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mstyle mathvariant="italic">
                              <mml:mi>x</mml:mi>
                            </mml:mstyle>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>r</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mstyle mathvariant="italic">
                          <mml:mi>Q</mml:mi>
                        </mml:mstyle>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mstyle mathvariant="italic">
                          <mml:mi>3</mml:mi>
                        </mml:mstyle>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mstyle mathvariant="italic">
                              <mml:mi>x</mml:mi>
                            </mml:mstyle>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>r</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                    <mml:mstyle mathvariant="italic">
                      <mml:mo>-</mml:mo>
                    </mml:mstyle>
                    <mml:mtext> </mml:mtext>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mstyle mathvariant="italic">
                          <mml:mi>Q</mml:mi>
                        </mml:mstyle>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mstyle mathvariant="italic">
                          <mml:mi>1</mml:mi>
                        </mml:mstyle>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mstyle mathvariant="italic">
                              <mml:mi>x</mml:mi>
                            </mml:mstyle>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>r</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mstyle mathvariant="italic">
                  <mml:mo>,</mml:mo>
                </mml:mstyle>
                <mml:mstyle mathvariant="italic">
                  <mml:mo>∀</mml:mo>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mstyle mathvariant="italic">
                      <mml:mi>x</mml:mi>
                    </mml:mstyle>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mstyle mathvariant="italic">
                      <mml:mi>r</mml:mi>
                    </mml:mstyle>
                  </mml:mrow>
                </mml:msub>
                <mml:mstyle mathvariant="italic">
                  <mml:mo>,</mml:mo>
                  <mml:mtext> </mml:mtext>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mstyle mathvariant="italic">
                      <mml:mi>r</mml:mi>
                    </mml:mstyle>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mstyle mathvariant="italic">
                      <mml:mi>F</mml:mi>
                      <mml:mi>S</mml:mi>
                    </mml:mstyle>
                  </mml:mrow>
                </mml:msub>
                <mml:mstyle mathvariant="italic">
                  <mml:mo>=</mml:mo>
                </mml:mstyle>
                <mml:mstyle mathvariant="italic">
                  <mml:mi>1</mml:mi>
                </mml:mstyle>
                <mml:mstyle mathvariant="italic">
                  <mml:mo>,</mml:mo>
                  <mml:mtext> </mml:mtext>
                </mml:mstyle>
                <mml:mstyle mathvariant="italic">
                  <mml:mi>2</mml:mi>
                </mml:mstyle>
                <mml:mstyle mathvariant="italic">
                  <mml:mo>…</mml:mo>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mstyle mathvariant="italic">
                      <mml:mi>R</mml:mi>
                    </mml:mstyle>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mstyle mathvariant="italic">
                      <mml:mi>F</mml:mi>
                      <mml:mi>S</mml:mi>
                    </mml:mstyle>
                  </mml:mrow>
                </mml:msub>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where the values, x<sub><italic>r</italic></sub>, of brain region, <italic>r</italic><sub><italic>FS</italic></sub>, are transformed using the quartiles <italic>Q</italic>.</p>
      <p>A 10-fold cross-validation was used where the split between training and test sets were 90% and 10%, respectively. For the longitudinal classification, each patient had two feature vectors (first and last sessions), and the L-1 difference between these vectors was used as the final feature vector. The cross-validation process with random splits was conducted 100 times, and the average classification probability was taken as the average of all of these test set trials. This previously validated method offers more reliable performance on relatively small datasets as it increases the number of cross-validations without decreasing the size of the test set (Pedregosa et al., <xref rid="B34" ref-type="bibr">2011</xref>; Varoquaux et al., <xref rid="B49" ref-type="bibr">2017</xref>). Sensitivity and specificity metrics were calculated by choosing a cutoff point that was the minimum distance from the upper left corner in the Area Under the Receiver Operating Curve (AUC ROC). Finally, to ensure the validity of this process, tests were also conducted solely using either session 1 or session 2 data. The results from these tests were comparable to existing literature that used machine learning approaches to distinguish AD from CN with structural brain regional volumes and the ADNI dataset. Discussion of these results are beyond the scope of this paper.</p>
      <p>For the DeepSymNet pipeline, a single 10-fold cross-validation was used due to computational constraints. Training a cross-validation approach with random splits was not feasible. Each fold contained a train and validation set, and once the fold was completed, the metrics were evaluated on a separate test set. The train, validation, and test set's proportions were 80% and 10%, and 10%, respectively. Learning rate and regularization optimization tests were conducted manually. The voxel-based approaches followed the same 10-fold cross validation and data split schematic as the DeepSymNet. Within each fold, parameters were tuned using the validation set, and the probabilities from the test sets were taken into account.</p>
      <p>The metrics from the models trained on FS 1, FS 2, and the voxel-based methods served as the baseline metrics to compare against the DeepSymNet architecture. We chose these Freesurfer-based models for two reasons: (1) Freesurfer is arguably the most used and tested image preprocessing approach to create a representation from T1-weighted volumes. Additionally, Freesurfer has a tested and well-recognized longitudinal pipeline used by multiple research groups around the globe. (2) To ensure that we were not biased to a specific FS version's representation, we employed two sets of features.</p>
      <p>The metrics used to evaluate the models are the AUC ROC score, balanced accuracy, sensitivity, and specificity. Further testing required the ROC curves to be compared, and these were tested for significance using the DeLong's test (DeLong et al., <xref rid="B11" ref-type="bibr">1988</xref>). The ROC curves' confidence intervals were calculated using the model's predictions with a Monte Carlo resampling method with 1,000 iterations, with 80% of the data per iteration.</p>
    </sec>
    <sec>
      <title>Computational Time Analysis</title>
      <p>Ultimately, we are interested in developing the underlying algorithm enabling a measuring tool for clinicians or researchers who wants to quantify an AD-relevant progression from T1-weighted brain images. Therefore, we tested the time needed to go from the raw brain images to a prediction in order to assess the feasibility of using a similar application in a clinical setting or large clinical trials. We assumed that that all ML models were already trained, as it is standard for ML applications deployment. For these tests, we used a machine with 8 CPU cores and 1 GPU. We ran 450 iterations of each pipeline and computed the mean and standard deviation of the time needed to complete. All parallelization speed-up for the Longitudinal Freesurfer-based pipeline were enabled.</p>
    </sec>
    <sec>
      <title>Generalizability: Detection of “AD-like” Progression Pattern on MCI Cohort</title>
      <p>After the experiments, we wanted to see if the DeepSymNet model could apply the learned progression pattern on an external set of high-risk patients. The final DeepSymNet model was applied to a cohort of MCI patients from the ADNI protocol. Each of the 10-folds' respective best DeepSymNet model was applied to each MCI patient progression so that each patient had 10 prediction probabilities. These probabilities were averaged together for the final probability measurement. These prediction probabilities were then used to construct AUC ROC curves of MCI vs. CN. Note that the control group probabilities were taken from the test set from the 10-fold cross-validation process explained above for the MCI vs. CN AUC ROC curve. In addition, these MCI results from the DeepSymNet were compared against the same method from the logistic regression that used atlas-based registration pipeline outlined above.</p>
    </sec>
    <sec>
      <title>Confounding Variable Adjustment</title>
      <p>Finally, we adjusted the DeepSymNet's probability output for confounding variables through a logistic regression method. The confounders used were the time between the two imaging sessions, gender, and the baseline age at the first imaging session.</p>
      <disp-formula id="E4">
        <mml:math id="M5">
          <mml:mtable columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>l</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>g</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mi>E</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mi>Y</mml:mi>
                  <mml:mo>)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:msup>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>β</mml:mi>
                      <mml:mn>0</mml:mn>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                    <mml:mtext> </mml:mtext>
                    <mml:msup>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mi>β</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo>∗</mml:mo>
                    </mml:msup>
                    <mml:mtext> </mml:mtext>
                    <mml:msub>
                      <mml:mi>X</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                    <mml:mtext> </mml:mtext>
                    <mml:msub>
                      <mml:mi>β</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo>∗</mml:mo>
                </mml:msup>
                <mml:mtext> </mml:mtext>
                <mml:msub>
                  <mml:mi>X</mml:mi>
                  <mml:mn>2</mml:mn>
                </mml:msub>
                <mml:mo>+</mml:mo>
                <mml:mtext> </mml:mtext>
                <mml:msup>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>β</mml:mi>
                      <mml:mn>3</mml:mn>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo>∗</mml:mo>
                </mml:msup>
                <mml:mtext> </mml:mtext>
                <mml:msub>
                  <mml:mi>X</mml:mi>
                  <mml:mn>3</mml:mn>
                </mml:msub>
                <mml:mo>+</mml:mo>
                <mml:mtext> </mml:mtext>
                <mml:msup>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>β</mml:mi>
                      <mml:mn>4</mml:mn>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo>∗</mml:mo>
                </mml:msup>
                <mml:mtext> </mml:mtext>
                <mml:msub>
                  <mml:mi>X</mml:mi>
                  <mml:mn>4</mml:mn>
                </mml:msub>
                <mml:mtext> </mml:mtext>
                <mml:mi>w</mml:mi>
                <mml:mi>h</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>e</mml:mi>
              </mml:mtd>
            </mml:mtr>
            <mml:mtr>
              <mml:mtd>
                <mml:mtext>                </mml:mtext>
                <mml:mi>Y</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mi>S</mml:mi>
                <mml:mi>u</mml:mi>
                <mml:mi>b</mml:mi>
                <mml:mi>j</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>c</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mi>G</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>u</mml:mi>
                <mml:mi>p</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mrow>
                    <mml:mi>e</mml:mi>
                    <mml:mo>.</mml:mo>
                    <mml:mi>g</mml:mi>
                    <mml:mo>.</mml:mo>
                    <mml:mtext> </mml:mtext>
                    <mml:mi>A</mml:mi>
                    <mml:mi>D</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mtext> </mml:mtext>
                    <mml:mi>C</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mtext> </mml:mtext>
                    <mml:mi>M</mml:mi>
                    <mml:mi>C</mml:mi>
                    <mml:mi>I</mml:mi>
                  </mml:mrow>
                  <mml:mo>)</mml:mo>
                </mml:mrow>
                <mml:mtext> </mml:mtext>
              </mml:mtd>
            </mml:mtr>
            <mml:mtr>
              <mml:mtd>
                <mml:mtext>              </mml:mtext>
                <mml:msub>
                  <mml:mi>X</mml:mi>
                  <mml:mn>1</mml:mn>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mi>T</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>m</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mi>b</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>w</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mi>s</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>s</mml:mi>
                <mml:mi>s</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mi>s</mml:mi>
              </mml:mtd>
            </mml:mtr>
            <mml:mtr>
              <mml:mtd>
                <mml:mtext>              </mml:mtext>
                <mml:msub>
                  <mml:mi>X</mml:mi>
                  <mml:mn>2</mml:mn>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mi>G</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mi>d</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>r</mml:mi>
              </mml:mtd>
            </mml:mtr>
            <mml:mtr>
              <mml:mtd>
                <mml:mtext>              </mml:mtext>
                <mml:msub>
                  <mml:mi>X</mml:mi>
                  <mml:mn>3</mml:mn>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mi>A</mml:mi>
                <mml:mi>g</mml:mi>
                <mml:mi>e</mml:mi>
              </mml:mtd>
            </mml:mtr>
            <mml:mtr>
              <mml:mtd>
                <mml:mtext>              </mml:mtext>
                <mml:msub>
                  <mml:mi>X</mml:mi>
                  <mml:mn>4</mml:mn>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mi>D</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>p</mml:mi>
                <mml:mi>S</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mi>m</mml:mi>
                <mml:mi>N</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mi>P</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>b</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>b</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>l</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>y</mml:mi>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>The feature coefficients from the logistic regression model for these variables along with their 95% confidence interval and <italic>p</italic>-value were reported.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>Results</title>
    <p>In this study, we aimed to examine: (1) various models' performance on learning AD-related progression patterns, (2) image resolution and network architecture hyperparameter tests on DeepSymNet performance, (3) the evaluation of the models on an external set of MCI patients, and (4) the influence of the selected confounding variables. The purpose of the first aim was to investigate the advantages of using an end-to-end data-driven approach to understand AD progression. The second aim allows readers to understand how the model behaved with hyperparameter tuning. The third aim validated the learned AD progression-specific pattern from DeepSymNet, and the fourth aimed ensured that these output probabilities were statistically significant.</p>
    <sec>
      <title>Longitudinal Pipeline and Model Comparison</title>
      <p>As seen in <xref rid="T2" ref-type="table">Table 2</xref>, the FS 1 and FS 2 pipelines had a differing number of structural volumetric features due to the reasons described in the Methods section. As seen in <xref ref-type="fig" rid="F3">Figure 3</xref> and <xref rid="T3" ref-type="table">Table 3</xref> below, the DeepSymNet had the highest AUC ROC of all the methods. Of the machine learning voxel-based methods, the random forests approach performed the best. There were statistical differences in the performance between the random forest voxel-based method and the DeepSymNet. However, there was not a significant difference between the DeepSymNet and the model using Feature Set 2. This suggests that a DeepSymNet architecture learns a high-level representation of longitudinal changes that is, at least, as informative as the changes in brain regional volumes and outperforms the voxel-based general-purpose machine learning approaches tested. Finally, as seen in <xref rid="T4" ref-type="table">Table 4</xref>, the preprocessing time was much faster in the DeepSymNet and voxel-based methods vs. the traditional neuroimaging pipelines used in research.</p>
      <table-wrap id="T2" position="float">
        <label>Table 2</label>
        <caption>
          <p>High-level comparison between the two feature sets from different imaging pipelines used to test the robustness of the new method.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Freesurfer version</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Number of samples used for ML pipeline (AD/CN)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Number of features</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Feature set 1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93 (60/39)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">340</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Feature set 2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93 (60/39)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">117</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <fig id="F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>AUC ROC curves comparing performance using a <bold>(A)</bold> data subset allowing for a comparison with samples found in Feature Set 1 (externally computed by UCSF) and the <bold>(B)</bold> full dataset.</p>
        </caption>
        <graphic xlink:href="fnins-13-01053-g0003"/>
      </fig>
      <table-wrap id="T3" position="float">
        <label>Table 3</label>
        <caption>
          <p>Longitudinal models' metric evaluation for AUC ROC, sensitivity, specificity, and balanced accuracy on the full dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Model</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUC ROC</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Sensitivity</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Specificity</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Balanced accuracy</bold>
              </th>
              <th rowspan="1" colspan="1"/>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">FS 2 (<italic>n</italic> = 482)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">71.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.3</td>
              <td valign="top" align="center" rowspan="3" colspan="1">
                <inline-graphic xlink:href="fnins-13-01053-i0001.jpg"/>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DeepSymNet (<italic>n</italic> = 482)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.7</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Voxel-based random forest (<italic>n</italic> = 482)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.2</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>p-values were computed from the DeLong test for correlated ROC curves to reject the null hypothesis that there is no statistical difference between the AUCs</italic>.</p>
          <p>N.S., not significant.</p>
          <p><italic>***p &lt;0.0001</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap id="T4" position="float">
        <label>Table 4</label>
        <caption>
          <p>Time performance for voxel-based pipeline (including DeepSymNet) and Longitudinal Freesurfer-based pipelines to generate an AD-relevant progression metric at inference time.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Pipeline</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>Computation time</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Mean</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Standard deviation</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DeepSymNet and other voxel-based ML Methods</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6.6 min</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.2 min</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Longitudinal Freesurfer-based</td>
              <td valign="top" align="center" rowspan="1" colspan="1">17.06 h</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2.7 h</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>DeepSymNet Hyperparameter Experiments</title>
      <p>A non-exhaustive manual hyperparameter search for the optimal image resolution and configuration of Inception modules was conducted (<xref rid="T5" ref-type="table">Table 5</xref>). These tests were not completed with a grid-search method in order to conserve time and computational power. The resolution changes were varied between 10, 20, 25, 30, and 35 percent and were applied to the original image isotropically. Afterwards, the number of Inception modules before and after the merge layer were varied to find the optimal model.</p>
      <table-wrap id="T5" position="float">
        <label>Table 5</label>
        <caption>
          <p>Detailed view of DeepSymNet model tuning experimental AUC ROC results.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Experiment</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Image resolution</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Module configuration (Before/After)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUC ROC</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Input image resolution</td>
              <td valign="top" align="center" rowspan="1" colspan="1">[10, 20, <bold>25</bold>, 30, 35%]</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.9</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Inception module configuration</td>
              <td valign="top" align="center" rowspan="1" colspan="1">25%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">[<bold>1/1</bold>, 1/2, 2/1, 2/2, 3/3]</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.9</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>Input image resolution and Inception module configuration were examined. The tests were completed in a stepwise fashion as outlined in the table. Bolded values indicate the model chosen for further analysis</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>Though the model with the 35% image resolution had a higher AUC ROC, the model that used 25% image resolution was chosen as the final model for two reasons. This model had fewer parameters (8.6 M vs. 21.2 M), and there was no statistical difference between the two curves. Several tests were conducted where the number of Inception modules before and after the merge layer was changed. As seen in <xref ref-type="fig" rid="F4">Figure 4</xref>, the model that performed the best in this test contained 1 and 1 Inception modules before and after the merge layer, respectively.</p>
      <fig id="F4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>AUC ROC curves for hyperparameter tuning experiments: <bold>(A)</bold> Image resolution experiments. <bold>(B)</bold> Differing Inception module architecture experiments. “Before” and “after” labels represent the number of 3D Inception modules before and after the L-1 layer where the two timepoints are combined together.</p>
        </caption>
        <graphic xlink:href="fnins-13-01053-g0004"/>
      </fig>
    </sec>
    <sec>
      <title>DeepSymNet Output</title>
      <p>The distribution of classification probabilities AD-like progression estimated by the DeepSymNet architecture was visualized in <xref ref-type="fig" rid="F5">Figure 5</xref> below. As expected, the MCI subjects' probabilities were in between the CN and AD groups. This indicates that the MCI group have structural progression patterns that are similar to AD. Additionally, the MCI cohort qualitatively had a smaller interquartile range compared to the other two classes.</p>
      <fig id="F5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Boxplot visualizing the distribution of DeepSymNet's output probabilities for the three classes, where each dot represents the progression probability for one subject. The class probability can be used as an indicator of AD-like longitudinal progression over two timepoints.</p>
        </caption>
        <graphic xlink:href="fnins-13-01053-g0005"/>
      </fig>
    </sec>
    <sec>
      <title>DeepSymNet Brain Region Relevance Analysis</title>
      <p>After the model evaluation was completed, we wanted to understand which regions of the brain were relevant for the model's decision. The brain regions of interest are visualized both globally and based on location (subcortical vs. cortical) in <xref ref-type="fig" rid="F6">Figure 6</xref>. The palladium, white matter, and putamen subcortical regions had the highest overall relevance magnitude across all the subjects and sessions. Of the cortical regions, the superior temporal gyrus cortex anterior division had the highest magnitude. The top five activated regions from the subcortical and cortical areas are summarized in <xref ref-type="fig" rid="F7">Figure 7</xref>. All of the cortical and subcortical regions and their relevance magnitude can be found in the <xref ref-type="supplementary-material" rid="SM1">Supplementary Tables</xref>.</p>
      <fig id="F6" position="float">
        <label>Figure 6</label>
        <caption>
          <p>ε-LRP relevance maps indicating the contribution of each voxel and brain region to the AD-progression classification at group level. <bold>(A)</bold> Voxel-based relevance map; <bold>(B,C)</bold> normalized relevance maps mapped to brain regions; <bold>(B)</bold> sub-cortical <bold>(C)</bold> cortical. Relevance scales help denote the degree of importance for the model's decision.</p>
        </caption>
        <graphic xlink:href="fnins-13-01053-g0006"/>
      </fig>
      <fig id="F7" position="float">
        <label>Figure 7</label>
        <caption>
          <p>Brain regional relevance magnitudes of top five subcortical and cortical region, sorted in descending order. These values were computed by summation of all the activations within the respective region and normalization by regional volume.</p>
        </caption>
        <graphic xlink:href="fnins-13-01053-g0007"/>
      </fig>
    </sec>
    <sec>
      <title>Models Evaluated on an External Set of MCI Patients</title>
      <p>Further, the final DeepSymNet model was assessed on an external set of MCI patients that DeepSymNet was never trained on. As seen in <xref ref-type="fig" rid="F8">Figure 8</xref>, the AUC ROC score for identifying an AD-like progression in the MCI cohort for DeepSymNet was significantly higher than the machine learning models trained on brain regional volumes, i.e., Freesurfer-based, or voxels-based. Additionally, the AUC ROC performance is comparable to the AD vs. CN progression prediction task. This indicates DeepSymNet's ability to generalize an AD-progression specific pattern that is applicable to high-risk patients and that could not be achieved by MRI-based regional volume measures.</p>
      <fig id="F8" position="float">
        <label>Figure 8</label>
        <caption>
          <p>AUC ROC curves for DeepSymNet, Random forests, and logistic regression (using Feature Set 2) for the identification of an AD-like progression in the MCI (<italic>n</italic> = 150) and CN (<italic>n</italic> = 270) groups. The models were not retrained on the MCI group, as the MCI patients were used only at inference time.</p>
        </caption>
        <graphic xlink:href="fnins-13-01053-g0008"/>
      </fig>
    </sec>
    <sec>
      <title>Confounding Variable Adjustment</title>
      <p>Finally, the DeepSymNet's output probabilities for the AD vs. CN and MCI vs. CN prediction tasks were adjusted using logistic regression with confounders. In both tasks, the output probabilities taken from DeepSymNet were statistically significant (<italic>p</italic> &lt; 0.0001) and had the highest coefficients relative to the potential confounding variables (<xref rid="T6" ref-type="table">Table 6</xref>).</p>
      <table-wrap id="T6" position="float">
        <label>Table 6</label>
        <caption>
          <p>Summary of the logistic regression coefficients with associated confidence interval and p-values for DeepSymNet output probability and confounding variables in both classification tasks.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Classification task</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Time between sessions</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Gender</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Baseline age</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>DeepSymNet output probability</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Progression of AD vs. CN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.25 (0.95 to 1.54)<xref ref-type="table-fn" rid="TN1"><sup>***</sup></xref></td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.03 (−0.50 to 0.56)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.07 (−0.08 to −0.05)<xref ref-type="table-fn" rid="TN1"><sup>***</sup></xref></td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.22 (3.01 to 5.44)<xref ref-type="table-fn" rid="TN1"><sup>***</sup></xref></td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Progression of MCI vs. CN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.2 (−0.11 to 0.08)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.41 (−0.89 to −0.06)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">−0.04 (−0.05 to −0.03)<xref ref-type="table-fn" rid="TN1"><sup>***</sup></xref></td>
              <td valign="top" align="center" rowspan="1" colspan="1">6.41 (5.01 to 7.81)<xref ref-type="table-fn" rid="TN1"><sup>***</sup></xref></td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="TN1">
            <label>***</label>
            <p><italic>p &lt;0.0001</italic>.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>Discussion</title>
    <p>In this study, our novel deep learning architecture, DeepSymNet, learned from temporal differences on the individual level to quantify AD progression. The DeepSymNet architecture combines the benefit of distance-based objective functions (which typically require smaller datasets) with prediction error-based objective functions (which lead to higher classification performance). The regions of the brain that drove the model's decision were visually analyzed using epsilon layer-wise relevance propagation methods. In addition, the DeepSymNet pipeline did not use typical image preprocessing steps, predefined brain regions, or non-rigid registration algorithms. These commonly used steps can be a significant source of downstream bias and computational cost. The robustness of our pipeline was benchmarked against pipelines that used atlas-based methods and baseline voxel-based machine learning models. The DeepSymNet architecture and imaging pipeline is disease-agnostic and could be used for other problems that utilize brain imaging for measuring disease progression.</p>
    <p>AD is an ideal case study for this work as there is no current established framework to numerically quantify AD neurodegenerative progression. For clinicians to properly test new disease-modifying drugs, there is a need to develop tools that quantify AD degeneration with commonly used brain imaging scans such as MRI. Many studies have found that AD-relevant changes are visible on the T1-weighted images, and all the AD population's progression will at some point appear. This idea was supplemented with longitudinal data where our model learned from the differences between the two imaging time points.</p>
    <p>There are several advantages to learning from time differences. First, we effectively reduce the risk of outside confounders affecting the experimental results as the individual patient's data is registered to a common space within their own specific longitudinal data. The L-1 difference used between the same subject will also reduce the magnitude of the cross-sectional patient-specific features and will instead magnify the structural differences over time. Lastly, the preprocessing method that considers individual brain morphology is in line with providing individualized precision medicine.</p>
    <p>In order to compare the robustness of the novel longitudinal pipeline, we compared models that used either pre-defined brain region volumes or voxels as to its input against DeepSymNet. DeepSymNet could represent AD-progression comparable to region-based methods and better than voxel-based methods as indicated from the superior performance over voxel-based methods. We believe this improved performance was due to the model learning both representations of the brain and the differences between the two timepoints. Further, our deep learning model was entirely data-driven and had fewer preprocessing steps.</p>
    <p>Two experimental hyperparameter tuning tests were conducted to improve the model performance: image resolution and Inception module architecture. Due to time and computational constraints, the authors explored through a narrow search space for tuning the model. The DeepSymNet model chosen for further analysis used images with 25% resolution and had one Inception module before and after the merge layer. This model had an AUC ROC of 0.84 (0.81–0.87).</p>
    <p>Once tuned, DeepSymNet pipeline was then applied to an external set of MCI patients. From <xref ref-type="fig" rid="F8">Figure 8</xref>, we saw an improved AUC ROC performance MCI vs. CN progression identification (0.84). This indicated that the model was able to generalize AD-specific progression patterns that are also seen in prodromal MCI patients. The DeepSymNet pipeline also achieved a performance similar to the AD vs. CN prediction task. A non-perfect classification was expected as the MCI cohort is a heterogeneous group where not all subjects will develop AD. Additionally, both classification task probabilities were statistically significant after adjusting for confounders. Finally, as seen in <xref ref-type="fig" rid="F5">Figure 5</xref>, there was a clear trend where the MCI probabilities were between the CN and AD probabilities, which may indicate the degree of neurodegenerative progression.</p>
    <p>Once the model experiments were completed, the top activated brain regions were analyzed. Previous AD studies corroborate our subcortical and cortical regional findings. Researchers found significant white matter reductions in AD patients throughout the brain, particularly in the temporal lobe (Guo et al., <xref rid="B17" ref-type="bibr">2010</xref>) and elevated mean diffusivity in precuneus and entorhinal white matter microstructures (Kantarci et al., <xref rid="B22" ref-type="bibr">2017</xref>). The pallidum region was found to have a significant difference in beta-amyloid burden between early and late-onset AD (Youn et al., <xref rid="B53" ref-type="bibr">2017</xref>) and differences in RNA binding protein TDP-43 deposits (Josephs et al., <xref rid="B21" ref-type="bibr">2016</xref>). Finally, researchers using different imaging modalities to discriminate AD patients found the pallidum and putamen to be consistently important (Rondina et al., <xref rid="B43" ref-type="bibr">2018</xref>). Various frontal regions were relevant for the model decision; this might represent an advanced disease stage in the selected population.</p>
    <p>Further, we looked at evidence surrounding our cortical region findings. An AD disease progression timeline analysis found that the superior temporal gyrus anterior division was among the top biomarkers to first become abnormal (Venkatraghavan et al., <xref rid="B50" ref-type="bibr">2019</xref>). Functional connectivity analysis found decreased connectivity in the superior temporal gyrus in dementia patients including AD (Hafkemeijer et al., <xref rid="B18" ref-type="bibr">2015</xref>; Schwab et al., <xref rid="B45" ref-type="bibr">2018</xref>). The middle temporal gyrus has been shown to atrophy significantly in both MCI and AD patients when compared to controls in longitudinal studies (Ghazi et al., <xref rid="B13" ref-type="bibr">2019</xref>) and research that combined multi-modal data types (Convit et al., <xref rid="B9" ref-type="bibr">2000</xref>; Korolev et al., <xref rid="B24" ref-type="bibr">2016</xref>). Finally, Guo et al. found significant gray matter volume reductions in the superior and middle temporal gyrus (2010). These subcortical and cortical structural changes might provide insight into pathophysiology process of AD and potentially serve as biomarkers for identifying those who are at risk of developing AD.</p>
    <p>There were several limitations in this study to note. As stated in the Methods section, the first and last imaging sessions were considered for this study. Many patients that had more than two visits with MRI imaging, which indicates that there is much more data available that could be incorporated into the model. Further, the hyperparameter tuning tests were all conducted manually, in a semi-structured way, and non-exhaustively. Machine learning methods like grid search or random grid search are used to find optimal network parameters. However, due to the computational costs of these methods at training time, they were not employed. Additionally, the time between the two imaging sessions for each patient was not controlled during the sample selection, which could present itself as a confounder. The time between sessions was corrected for and controlled in the statistical analysis (<xref rid="T6" ref-type="table">Table 6</xref>). Finally, a fully external validation of these results in other AD-specific imaging datasets was not conducted.</p>
    <p>Future studies could expand the generalizability of AD classification by using other open-source datasets such as Open Access Series of Imaging Studies (<ext-link ext-link-type="uri" xlink:href="https://www.oasis-brains.org/">https://www.oasis-brains.org/</ext-link>). Also, studies could look at improving the model performance by taking a Bayesian approach for hyperparameter tuning. Other work could make use of our pipeline as a progression phenotype to assess relationship with other data sources such as cerebrospinal/blood biomarkers or genetic data. Finally, newer models such as advances in recurrent neural networks could also incorporate more time points which may provide a richer representation of a patient's progression over time.</p>
  </sec>
  <sec sec-type="conclusions" id="s5">
    <title>Conclusion</title>
    <p>In summary, we implemented a novel pipeline based on a DeepSymNet architecture, that was able to detect an AD progression pattern by learning from structural differences of inter-subject MRI scans at two-time points. The paper's image preprocessing pipeline did not use predefined brain regions or non-rigid registration, which significantly reduced the opportunity for intermediate bias. In addition, the DeepSymNet pipeline was benchmarked against models that used standard imaging pipelines. From the brain region relevance analysis using the ε-LRP algorithm, the pallidum, putamen, and the superior temporal gyrus regions were critical in the model's final decision. Further, the model learned an AD progression pattern that was generalizable on an independent, external set of MCI patients. This architecture has the potential to be applied to multiple other applications where longitudinal changes need to be detected and measured. Finally, our pipeline can be used to improve imaging-based diagnostic systems by reducing time and computational cost.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data Availability Statement</title>
    <p>The datasets generated for this study will not be made publicly available the public must request access from the Alzheimer's Disease Neuroimaging Initiative for access to the imaging data.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>DP led the writing and carried out the computational experiments. AB and LG conceived the idea behind the methodology. LG and XJ reviewed and provided feedback for the methods. JS and MS assisted with the clinical relevance of the paper. All authors contributed to the writing of the final manuscript.</p>
    <sec>
      <title>Conflict of Interest</title>
      <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
    </sec>
  </sec>
</body>
<back>
  <ack>
    <p>Data collection and sharing for this project was funded by the Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). ADNI was funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie, Alzheimer's Association; Alzheimer's Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research &amp; Development, LLC.; Johnson &amp; Johnson Pharmaceutical Research &amp; Development, LLC.; Lumosity; Lundbeck; Merck &amp; Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (<ext-link ext-link-type="uri" xlink:href="http://www.fnih.org">www.fnih.org</ext-link>). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer's Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California.</p>
  </ack>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> This work was supported by the Huffington Research Fund and from MS's Adriana Blood Endowed Chair in Neurology and partially supported by the Cancer Prevention and Research Institute of Texas grant RP 170668. Additionally, one of Titan XP GPUs used for this research was donated by the NVIDIA Corporation.</p>
    </fn>
  </fn-group>
  <sec sec-type="supplementary-material" id="s8">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fnins.2019.01053/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fnins.2019.01053/full#supplementary-material</ext-link></p>
    <supplementary-material content-type="local-data" id="SM1">
      <media xlink:href="Table_1.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aisen</surname><given-names>P. S.</given-names></name><name><surname>Cummings</surname><given-names>J.</given-names></name><name><surname>Jack</surname><given-names>C. R.</given-names></name><name><surname>Morris</surname><given-names>J. C.</given-names></name><name><surname>Sperling</surname><given-names>R.</given-names></name><name><surname>Frölich</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>On the path to 2025: understanding the Alzheimer's disease continuum</article-title>. <source>Alzheimers Res. Ther</source>. <volume>9</volume>:<fpage>60</fpage>. <pub-id pub-id-type="doi">10.1186/s13195-017-0283-5</pub-id><?supplied-pmid 28793924?><pub-id pub-id-type="pmid">28793924</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Alzheimer's Association</collab></person-group> (<year>2016</year>). <article-title>2016 Alzheimer's disease facts and figures</article-title>. <source>Alzheimers Dement</source>. <volume>12</volume>, <fpage>459</fpage>–<lpage>509</lpage>. <pub-id pub-id-type="doi">10.1016/j.jalz.2016.03.001</pub-id><pub-id pub-id-type="pmid">27570871</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ancona</surname><given-names>M.</given-names></name><name><surname>Gross</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>). <article-title>A unified view of gradient-based attribution methods for Deep Neural Networks</article-title>, in <source>Neural Information Processing Systems</source>, <fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bach</surname><given-names>S.</given-names></name><name><surname>Binder</surname><given-names>A.</given-names></name><name><surname>Montavon</surname><given-names>G.</given-names></name><name><surname>Klauschen</surname><given-names>F.</given-names></name><name><surname>Müller</surname><given-names>K.-R.</given-names></name><name><surname>Samek</surname><given-names>W.</given-names></name></person-group> (<year>2015</year>). <article-title>On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</article-title>. <source>PLoS ONE</source>
<volume>10</volume>:<fpage>e0130140</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0130140</pub-id><?supplied-pmid 26161953?><pub-id pub-id-type="pmid">26161953</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Backstrom</surname><given-names>K.</given-names></name><name><surname>Nazari</surname><given-names>M.</given-names></name><name><surname>Gu</surname><given-names>I. Y.-H.</given-names></name><name><surname>Jakola</surname><given-names>A. S.</given-names></name></person-group> (<year>2018</year>). <article-title>An efficient 3D deep convolutional network for Alzheimer's disease diagnosis using MR images</article-title>, in <source>IEEE International Symposium on Biomedical Imaging (ISBI 2018)</source> (<publisher-loc>Washington, DC</publisher-loc>), <fpage>149</fpage>–<lpage>153</lpage>. <pub-id pub-id-type="doi">10.1109/ISBI.2018.8363543</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barman</surname><given-names>A.</given-names></name><name><surname>Inam</surname><given-names>M. E.</given-names></name><name><surname>Lee</surname><given-names>S.</given-names></name><name><surname>Savitz</surname><given-names>S.</given-names></name><name><surname>Sheth</surname><given-names>S.</given-names></name><name><surname>Giancardo</surname><given-names>L.</given-names></name></person-group> (<year>2019</year>). <article-title>Determining ischemic stroke from CT-angiography imaging using symmetry-sensitive convolutional networks</article-title>, in <source>IEEE International Symposium on Biomedical Imaging</source> (<publisher-loc>Venice</publisher-loc>), <fpage>1</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.1109/ISBI.2019.8759475</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhagwat</surname><given-names>N.</given-names></name><name><surname>Viviano</surname><given-names>J. D.</given-names></name><name><surname>Voineskos</surname><given-names>A. N.</given-names></name><name><surname>Chakravarty</surname><given-names>M. M.</given-names></name><name><surname>Initiative</surname><given-names>A. D. N.</given-names></name></person-group> (<year>2018</year>). <article-title>Modeling and prediction of clinical symptom trajectories in Alzheimer's disease using longitudinal data</article-title>. <source>PLoS Comput. Biol.</source>
<volume>14</volume>:<fpage>e1006376</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006376</pub-id><?supplied-pmid 30216352?><pub-id pub-id-type="pmid">30216352</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caviness</surname><given-names>V. S.</given-names></name><name><surname>Meyer</surname><given-names>J.</given-names></name><name><surname>Makris</surname><given-names>N.</given-names></name><name><surname>Kennedy</surname><given-names>D. N.</given-names></name></person-group> (<year>1996</year>). <article-title>MRI-based topographic parcellation of human neocortex: an anatomically specified method with estimate of reliability</article-title>. <source>J. Cogn. Neurosci.</source>
<volume>8</volume>, <fpage>566</fpage>–<lpage>587</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.1996.8.6.566</pub-id><?supplied-pmid 23961985?><pub-id pub-id-type="pmid">23961985</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Convit</surname><given-names>A.</given-names></name><name><surname>de Asis</surname><given-names>J.</given-names></name><name><surname>de Leon</surname><given-names>M.</given-names></name><name><surname>Tarshish</surname><given-names>C.</given-names></name><name><surname>De Santi</surname><given-names>S.</given-names></name><name><surname>Rusinek</surname><given-names>H.</given-names></name></person-group> (<year>2000</year>). <article-title>Atrophy of the medial occipitotemporal, inferior, and middle temporal gyri in non-demented elderly predict decline to Alzheimer's disease</article-title>. <source>Neurobiol. Aging</source>
<volume>21</volume>, <fpage>19</fpage>–<lpage>26</lpage>. <pub-id pub-id-type="doi">10.1016/S0197-4580(99)00107-4</pub-id><?supplied-pmid 10794844?><pub-id pub-id-type="pmid">10794844</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vos</surname><given-names>F.</given-names></name><name><surname>Koini</surname><given-names>M.</given-names></name><name><surname>Schouten</surname><given-names>T. M.</given-names></name><name><surname>Seiler</surname><given-names>S.</given-names></name><name><surname>van der Grond</surname><given-names>J.</given-names></name><name><surname>Lechner</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>A comprehensive analysis of resting state fMRI measures to classify individual patients with Alzheimer's disease</article-title>. <source>NeuroImage</source><volume>167</volume>, <fpage>62</fpage>–<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.11.025</pub-id><?supplied-pmid 29155080?><pub-id pub-id-type="pmid">29155080</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeLong</surname><given-names>E. R.</given-names></name><name><surname>DeLong</surname><given-names>D. M.</given-names></name><name><surname>Clarke-Pearson</surname><given-names>D. L.</given-names></name></person-group> (<year>1988</year>). <article-title>Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach</article-title>. <source>Biometrics</source>
<volume>44</volume>, <fpage>837</fpage>–<lpage>845</lpage>. <pub-id pub-id-type="doi">10.2307/2531595</pub-id><?supplied-pmid 3203132?><pub-id pub-id-type="pmid">3203132</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feurer</surname><given-names>M.</given-names></name><name><surname>Klein</surname><given-names>A.</given-names></name><name><surname>Eggensperger</surname><given-names>K.</given-names></name><name><surname>Springenberg</surname><given-names>J. T.</given-names></name><name><surname>Blum</surname><given-names>M.</given-names></name><name><surname>Hutter</surname><given-names>F.</given-names></name></person-group> (<year>2015</year>). <article-title>Efficient and robust automated machine learning</article-title>, in <source>Advances in Neural Information Processing Systems</source>, <fpage>2962</fpage>–<lpage>2970</lpage>.</mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazi</surname><given-names>M. M.</given-names></name><name><surname>Nielsen</surname><given-names>M.</given-names></name><name><surname>Pai</surname><given-names>A.</given-names></name><name><surname>Cardoso</surname><given-names>M. J.</given-names></name><name><surname>Modat</surname><given-names>M.</given-names></name><name><surname>Ourselin</surname><given-names>S.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Training recurrent neural networks robust to incomplete data: application to Alzheimer's disease progression modeling</article-title>. <source>Med. Image Anal.</source>
<volume>53</volume>, <fpage>39</fpage>–<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2019.01.004</pub-id><pub-id pub-id-type="pmid">30682584</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibson</surname><given-names>E.</given-names></name><name><surname>Li</surname><given-names>W.</given-names></name><name><surname>Sudre</surname><given-names>C.</given-names></name><name><surname>Fidon</surname><given-names>L.</given-names></name><name><surname>Shakir</surname><given-names>D. I.</given-names></name><name><surname>Wang</surname><given-names>G.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>NiftyNet: a deep-learning platform for medical imaging</article-title>. <source>Comput. Methods Programs Biomed.</source><volume>158</volume>, <fpage>113</fpage>–<lpage>122</lpage>. <pub-id pub-id-type="doi">10.1016/j.cmpb.2018.01.025</pub-id><?supplied-pmid 29544777?><pub-id pub-id-type="pmid">29544777</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K. J.</given-names></name><name><surname>Auer</surname><given-names>T.</given-names></name><name><surname>Calhoun</surname><given-names>V. D.</given-names></name><name><surname>Craddock</surname><given-names>R. C.</given-names></name><name><surname>Das</surname><given-names>S.</given-names></name><name><surname>Duff</surname><given-names>E. P.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments</article-title>. <source>Sci. Data</source><volume>3</volume>:<fpage>160044</fpage>. <pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id><?supplied-pmid 27326542?><pub-id pub-id-type="pmid">27326542</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gray</surname><given-names>K. R.</given-names></name><name><surname>Wolz</surname><given-names>R.</given-names></name><name><surname>Heckemann</surname><given-names>R. A.</given-names></name><name><surname>Aljabar</surname><given-names>P.</given-names></name><name><surname>Hammers</surname><given-names>A.</given-names></name><name><surname>Rueckert</surname><given-names>D.</given-names></name></person-group> the Alzheimer's Disease Neuroimaging Initiative (<year>2012</year>). <article-title>Multi-region analysis of longitudinal FDG-PET for the classification of Alzheimer's disease</article-title>. <source>NeuroImage</source>
<volume>60</volume>, <fpage>221</fpage>–<lpage>229</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.12.071</pub-id><?supplied-pmid 22236449?><pub-id pub-id-type="pmid">22236449</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Li</surname><given-names>K.</given-names></name><name><surname>Li</surname><given-names>Z.</given-names></name><name><surname>Qi</surname><given-names>Z.</given-names></name><name><surname>Jin</surname><given-names>Z.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>Voxel-based assessment of gray and white matter volumes in Alzheimer's disease</article-title>. <source>Neurosci. Lett.</source><volume>468</volume>, <fpage>146</fpage>–<lpage>150</lpage>. <pub-id pub-id-type="doi">10.1016/j.neulet.2009.10.086</pub-id><?supplied-pmid 19879920?><pub-id pub-id-type="pmid">19879920</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafkemeijer</surname><given-names>A.</given-names></name><name><surname>Möller</surname><given-names>C.</given-names></name><name><surname>Dopper</surname><given-names>E. G. P.</given-names></name><name><surname>Jiskoot</surname><given-names>L. C.</given-names></name><name><surname>Schouten</surname><given-names>T. M.</given-names></name><name><surname>van Swieten</surname><given-names>J. C.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Resting state functional connectivity differences between behavioral variant frontotemporal dementia and Alzheimer's disease</article-title>. <source>Front. Hum. Neurosci.</source><volume>9</volume>:<fpage>474</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2015.00474</pub-id><?supplied-pmid 26441584?><pub-id pub-id-type="pmid">26441584</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>M.</given-names></name><name><surname>Yang</surname><given-names>W.</given-names></name><name><surname>Feng</surname><given-names>Q.</given-names></name><name><surname>Chen</surname><given-names>W.</given-names></name></person-group> the Alzheimer's Disease Neuroimaging (<year>2017</year>). <article-title>Longitudinal measurement and hierarchical classification framework for the prediction of Alzheimer's disease</article-title>. <source>Nat. Sci. Rep.</source>
<volume>7</volume>, <fpage>1</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1038/srep39880</pub-id><?supplied-pmid 28079104?><pub-id pub-id-type="pmid">28079104</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iglesias</surname><given-names>J. E.</given-names></name><name><surname>Van Leemput</surname><given-names>K.</given-names></name><name><surname>Augustinack</surname><given-names>J.</given-names></name><name><surname>Insausti</surname><given-names>R.</given-names></name><name><surname>Fischl</surname><given-names>B.</given-names></name><name><surname>Reuter</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>). <article-title>Bayesian longitudinal segmentation of hippocampal substructures in brain MRI using subject-specific atlases</article-title>. <source>NeuroImage</source>
<volume>141</volume>, <fpage>542</fpage>–<lpage>555</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.07.020</pub-id><?supplied-pmid 27426838?><pub-id pub-id-type="pmid">27426838</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Josephs</surname><given-names>K. A.</given-names></name><name><surname>Murray</surname><given-names>M. E.</given-names></name><name><surname>Whitwell</surname><given-names>J. L.</given-names></name><name><surname>Tosakulwong</surname><given-names>N.</given-names></name><name><surname>Weigand</surname><given-names>S. D.</given-names></name><name><surname>Petrucelli</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Updated TDP-43 in Alzheimer's disease staging scheme</article-title>. <source>Acta Neuropathol.</source><volume>131</volume>, <fpage>571</fpage>–<lpage>585</lpage>. <pub-id pub-id-type="doi">10.1007/s00401-016-1537-1</pub-id><?supplied-pmid 26810071?><pub-id pub-id-type="pmid">26810071</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kantarci</surname><given-names>K.</given-names></name><name><surname>Murray</surname><given-names>M. E.</given-names></name><name><surname>Schwarz</surname><given-names>C. G.</given-names></name><name><surname>Reid</surname><given-names>R. I.</given-names></name><name><surname>Przybelski</surname><given-names>S. A.</given-names></name><name><surname>Lesnick</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>White-matter integrity on DTI and the pathologic staging of Alzheimer's disease</article-title>. <source>Neurobiol. Aging</source><volume>56</volume>, <fpage>172</fpage>–<lpage>179</lpage>. <pub-id pub-id-type="doi">10.1016/j.neurobiolaging.2017.04.024</pub-id><?supplied-pmid 28552181?><pub-id pub-id-type="pmid">28552181</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D. P.</given-names></name><name><surname>Ba</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>Adam: a method for stochastic optimization</article-title>, in <source>International Conference on Learning Representations</source> (<publisher-loc>San Diego, CA</publisher-loc>), <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Korolev</surname><given-names>I. O.</given-names></name><name><surname>Symonds</surname><given-names>L. L.</given-names></name><name><surname>Bozoki</surname><given-names>A. C.</given-names></name></person-group><article-title>Alzheimer's Disease Neuroimaging Initiative (2016). Predicting progression from mild cognitive impairment to Alzheimer's dementia using clinical, MRI, and plasma biomarkers via probabilistic pattern classification</article-title>. <source>PLoS ONE</source><volume>11</volume>:<fpage>e0138866</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0138866</pub-id><pub-id pub-id-type="pmid">26901338</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lane</surname><given-names>C. A.</given-names></name><name><surname>Hardy</surname><given-names>J.</given-names></name><name><surname>Schott</surname><given-names>J. M.</given-names></name></person-group> (<year>2018</year>). <article-title>Alzheimer's disease</article-title>. <source>Eur. J. Neurol.</source>
<volume>25</volume>, <fpage>59</fpage>–<lpage>70</lpage>. <pub-id pub-id-type="doi">10.1111/ene.13439</pub-id><?supplied-pmid 28872215?><pub-id pub-id-type="pmid">28872215</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawrence</surname><given-names>E.</given-names></name><name><surname>Vegvari</surname><given-names>C.</given-names></name><name><surname>Ower</surname><given-names>A.</given-names></name><name><surname>Hadjichrysanthou</surname><given-names>C.</given-names></name><name><surname>De Wolf</surname><given-names>F.</given-names></name><name><surname>Anderson</surname><given-names>R. M.</given-names></name></person-group> (<year>2017</year>). <article-title>A systematic review of longitudinal studies which measure Alzheimer's disease biomarkers</article-title>. <source>J. Alzheimers Dis.</source>
<volume>59</volume>, <fpage>1359</fpage>–<lpage>1379</lpage>. <pub-id pub-id-type="doi">10.3233/JAD-170261</pub-id><?supplied-pmid 28759968?><pub-id pub-id-type="pmid">28759968</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Q.</given-names></name><name><surname>Wu</surname><given-names>X.</given-names></name><name><surname>Xu</surname><given-names>L.</given-names></name><name><surname>Chen</surname><given-names>K.</given-names></name><name><surname>Yao</surname><given-names>L.</given-names></name><name><surname>Li</surname><given-names>R.</given-names></name></person-group> (<year>2017</year>). <article-title>Multi-modal discriminative dictionary learning for Alzheimer's disease and mild cognitive impairment</article-title>. <source>Comput. Methods Programs Biomed.</source>
<volume>150</volume>, <fpage>1</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1016/j.cmpb.2017.07.003</pub-id><?supplied-pmid 28859825?><pub-id pub-id-type="pmid">28859825</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillemark</surname><given-names>L.</given-names></name><name><surname>Sørensen</surname><given-names>L.</given-names></name><name><surname>Pai</surname><given-names>A.</given-names></name><name><surname>Dam</surname><given-names>E. B.</given-names></name><name><surname>Nielsen</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>). <article-title>Brain region's relative proximity as marker for Alzheimer's disease based on structural MRI</article-title>. <source>BMC Med. Imaging</source>
<volume>14</volume>:<fpage>21</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2342-14-21</pub-id><?supplied-pmid 24889999?><pub-id pub-id-type="pmid">24889999</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litjens</surname><given-names>G.</given-names></name><name><surname>Kooi</surname><given-names>T.</given-names></name><name><surname>Bejnordi</surname><given-names>B. E.</given-names></name><name><surname>Setio</surname><given-names>A. A. A.</given-names></name><name><surname>Ciompi</surname><given-names>F.</given-names></name><name><surname>Ghafoorian</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>A survey on deep learning in medical image analysis</article-title>. <source>Med. Image Anal.</source><volume>42</volume>, <fpage>60</fpage>–<lpage>88</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2017.07.005</pub-id><?supplied-pmid 28778026?><pub-id pub-id-type="pmid">28778026</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>X.</given-names></name><name><surname>Chen</surname><given-names>L.</given-names></name><name><surname>Jiang</surname><given-names>C.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name></person-group><article-title>Alzheimer's Disease Neuroimaging Initiative (2017). Prediction and classification of Alzheimer disease based on quantification of MRI deformation</article-title>. <source>PLoS ONE</source><volume>12</volume>:<fpage>e0173372</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0173372</pub-id><pub-id pub-id-type="pmid">28264071</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marcus</surname><given-names>D. S.</given-names></name><name><surname>Fotenos</surname><given-names>A. F.</given-names></name><name><surname>Csernansky</surname><given-names>J. G.</given-names></name><name><surname>Morris</surname><given-names>J. C.</given-names></name><name><surname>Buckner</surname><given-names>R. L.</given-names></name></person-group> (<year>2010</year>). <article-title>Open access series of imaging studies: longitudinal MRI data in nondemented and demented older adults</article-title>. <source>J. Cogn. Neurosci.</source>
<volume>22</volume>, <fpage>2677</fpage>–<lpage>2684</lpage>. <pub-id pub-id-type="doi">10.1162/jocn.2009.21407</pub-id><?supplied-pmid 19929323?><pub-id pub-id-type="pmid">19929323</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshall</surname><given-names>G. A.</given-names></name><name><surname>Zoller</surname><given-names>A. S.</given-names></name><name><surname>Lorius</surname><given-names>N.</given-names></name><name><surname>Amariglio</surname><given-names>R. E.</given-names></name><name><surname>Locascio</surname><given-names>J. J.</given-names></name><name><surname>Johnson</surname><given-names>K. A.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Functional activities questionnaire items that best discriminate and predict progression from clinically normal to mild cognitive impairment</article-title>. <source>Curr. Alzheimer Res.</source><volume>12</volume>, <fpage>493</fpage>–<lpage>502</lpage>. <pub-id pub-id-type="doi">10.2174/156720501205150526115003</pub-id><?supplied-pmid 26017560?><pub-id pub-id-type="pmid">26017560</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozadi</surname><given-names>S. H.</given-names></name><name><surname>Kadoury</surname><given-names>S.</given-names></name></person-group> the Alzheimer's Disease Neuroimaging Initiative (<year>2018</year>). <article-title>Classification of Alzheimer's and MCI patients from semantically parcelled PET images: a comparison between AV45 and FDG-PET</article-title>. <source>Int. J. Biomed. Imaging</source>
<volume>2018</volume>, <fpage>1</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1155/2018/1247430</pub-id><?supplied-pmid 29736165?><pub-id pub-id-type="pmid">29736165</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F.</given-names></name><name><surname>Varoquaux</surname><given-names>G.</given-names></name><name><surname>Gramfort</surname><given-names>A.</given-names></name><name><surname>Michel</surname><given-names>V.</given-names></name><name><surname>Thirion</surname><given-names>B.</given-names></name><name><surname>Grisel</surname><given-names>O.</given-names></name><etal/></person-group> (<year>2011</year>). <article-title>Scikit-learn: machine learning in python</article-title>. <source>J. Mach. Learn. Res</source>. <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pellegrini</surname><given-names>E.</given-names></name><name><surname>Ballerini</surname><given-names>L.</given-names></name><name><surname>Hernandez</surname><given-names>M. D. C. V.</given-names></name><name><surname>Chappell</surname><given-names>F. M.</given-names></name><name><surname>González-Castro</surname><given-names>V.</given-names></name><name><surname>Anblagan</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Machine learning of neuroimaging for assisted diagnosis of cognitive impairment and dementia: a systematic review</article-title>. <source>Alzheimers Dement.</source><volume>10</volume>, <fpage>519</fpage>–<lpage>535</lpage>. <pub-id pub-id-type="doi">10.1016/j.dadm.2018.07.004</pub-id><?supplied-pmid 30364671?><pub-id pub-id-type="pmid">30364671</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peña-Nogales</surname><given-names>Ó.</given-names></name><name><surname>Ellmore</surname><given-names>T. M.</given-names></name><name><surname>de Luis-García</surname><given-names>R.</given-names></name><name><surname>Suescun</surname><given-names>J.</given-names></name><name><surname>Schiess</surname><given-names>M. C.</given-names></name><name><surname>Giancardo</surname><given-names>L.</given-names></name></person-group> (<year>2018</year>). <article-title>Longitudinal connectomes as a candidate progression marker for prodromal Parkinson's disease</article-title>. <source>Front. Neurosci.</source>
<volume>12</volume>:<fpage>967</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2018.00967</pub-id><?supplied-pmid 30686966?><pub-id pub-id-type="pmid">30686966</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>J.</given-names></name><name><surname>An</surname><given-names>L.</given-names></name><name><surname>Zhu</surname><given-names>X.</given-names></name><name><surname>Jin</surname><given-names>Y.</given-names></name><name><surname>Shen</surname><given-names>D.</given-names></name></person-group> (<year>2016</year>). <article-title>Structured sparse kernel learning for imaging genetics based Alzheimer's disease diagnosis</article-title>. <source>Med. Image Comput. Comput. Assist. Interv.</source>
<volume>9901</volume>, <fpage>70</fpage>–<lpage>78</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-46723-8_9</pub-id><?supplied-pmid 28580458?><pub-id pub-id-type="pmid">28580458</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeffer</surname><given-names>R. I.</given-names></name><name><surname>Kurosaki</surname><given-names>T. T.</given-names></name><name><surname>Harrah</surname><given-names>C. H.</given-names></name><name><surname>Chance</surname><given-names>J. M.</given-names></name><name><surname>Filos</surname><given-names>S.</given-names></name></person-group> (<year>1982</year>). <article-title>Measurement of functional activities in older adults in the community</article-title>. <source>J. Gerontol.</source>
<volume>37</volume>, <fpage>323</fpage>–<lpage>329</lpage>. <pub-id pub-id-type="doi">10.1093/geronj/37.3.323</pub-id><?supplied-pmid 7069156?><pub-id pub-id-type="pmid">7069156</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rathore</surname><given-names>S.</given-names></name><name><surname>Habes</surname><given-names>M.</given-names></name><name><surname>Iftikhar</surname><given-names>M. A.</given-names></name><name><surname>Shacklett</surname><given-names>A.</given-names></name><name><surname>Davatzikos</surname><given-names>C.</given-names></name></person-group> (<year>2017</year>). <article-title>A review on neuroimaging-based classification studies and associated feature extraction methods for Alzheimer's disease and its prodromal stages</article-title>. <source>NeuroImage</source>
<volume>155</volume>, <fpage>530</fpage>–<lpage>548</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.03.057</pub-id><?supplied-pmid 28414186?><pub-id pub-id-type="pmid">28414186</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reuter</surname><given-names>M.</given-names></name><name><surname>Rosas</surname><given-names>H. D.</given-names></name><name><surname>Fischl</surname><given-names>B.</given-names></name></person-group> (<year>2010</year>). <article-title>Highly accurate inverse consistent registration: a robust approach</article-title>. <source>NeuroImage</source>
<volume>53</volume>, <fpage>1181</fpage>–<lpage>1196</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.020</pub-id><?supplied-pmid 20637289?><pub-id pub-id-type="pmid">20637289</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reuter</surname><given-names>M.</given-names></name><name><surname>Schmansky</surname><given-names>N. J.</given-names></name><name><surname>Rosas</surname><given-names>H. D.</given-names></name><name><surname>Fischl</surname><given-names>B.</given-names></name></person-group> (<year>2012</year>). <article-title>Within-subject template estimation for unbiased longitudinal image analysis</article-title>. <source>NeuroImage</source>
<volume>61</volume>, <fpage>1402</fpage>–<lpage>1418</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.084</pub-id><?supplied-pmid 22430496?><pub-id pub-id-type="pmid">22430496</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodrigues</surname><given-names>F.</given-names></name><name><surname>Silveira</surname><given-names>M.</given-names></name></person-group>. <article-title>the Alzheimer's Disease Neuroimaging Initiative. (2014). Longitudinal FDG-PET features for the classification of Alzheimer's disease</article-title>. <source>IEEE Eng. Med. Biol. Soc.</source><volume>2014</volume>, <fpage>1941</fpage>–<lpage>1944</lpage>. <pub-id pub-id-type="doi">10.1109/EMBC.2014.6943992</pub-id><?supplied-pmid 25570360?><pub-id pub-id-type="pmid">25570360</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rondina</surname><given-names>J. M.</given-names></name><name><surname>Ferreira</surname><given-names>L. K.</given-names></name><name><surname>de Souza Duran</surname><given-names>F. L.</given-names></name><name><surname>Kubo</surname><given-names>R.</given-names></name><name><surname>Ono</surname><given-names>C. R.</given-names></name><name><surname>Leite</surname><given-names>C. C.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Selecting the most relevant brain regions to discriminate Alzheimer's disease patients from healthy controls using multiple kernel learning: a comparison across functional and structural imaging modalities and atlases</article-title>. <source>NeuroImage Clin.</source><volume>17</volume>, <fpage>628</fpage>–<lpage>641</lpage>. <pub-id pub-id-type="doi">10.1016/j.nicl.2017.10.026</pub-id><?supplied-pmid 29234599?><pub-id pub-id-type="pmid">29234599</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samper-González</surname><given-names>J.</given-names></name><name><surname>Burgos</surname><given-names>N.</given-names></name><name><surname>Bottani</surname><given-names>S.</given-names></name><name><surname>Fontanella</surname><given-names>S.</given-names></name><name><surname>Lu</surname><given-names>P.</given-names></name><name><surname>Marcoux</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Reproducible evaluation of classification methods in Alzheimer's disease: framework and application to MRI and PET data</article-title>. <source>NeuroImage</source><volume>183</volume>, <fpage>504</fpage>–<lpage>521</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.08.042</pub-id><?supplied-pmid 30130647?><pub-id pub-id-type="pmid">30130647</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwab</surname><given-names>S.</given-names></name><name><surname>Afyouni</surname><given-names>S.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Han</surname><given-names>Z.</given-names></name><name><surname>Guo</surname><given-names>Q.</given-names></name><name><surname>Dierks</surname><given-names>T.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Functional connectivity alterations of the temporal lobe and hippocampus in semantic dementia and Alzheimer's disease</article-title>. <source>BioRxiv</source>
<fpage>1</fpage>–<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1101/322131</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spasov</surname><given-names>S.</given-names></name><name><surname>Passamonti</surname><given-names>L.</given-names></name><name><surname>Duggento</surname><given-names>A.</given-names></name><name><surname>Liò</surname><given-names>P.</given-names></name><name><surname>Toschi</surname><given-names>N.</given-names></name></person-group> (<year>2019</year>). <article-title>A parameter-efficient deep learning approach to predict conversion from mild cognitive impairment to Alzheimer's disease</article-title>. <source>NeuroImage</source>
<volume>189</volume>, <fpage>276</fpage>–<lpage>287</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.01.031</pub-id><?supplied-pmid 30654174?><pub-id pub-id-type="pmid">30654174</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Z.</given-names></name><name><surname>van de Giessen</surname><given-names>M.</given-names></name><name><surname>Lelieveldt</surname><given-names>B. P. F.</given-names></name><name><surname>Staring</surname><given-names>M.</given-names></name></person-group> the Alzheimer's Disease Neuroimaging Initiative (<year>2017</year>). <article-title>Detection of conversion from mild cognitive impairment to Alzheimer's disease using longitudinal brain MRI</article-title>. <source>Front. Neuroinform.</source>
<volume>11</volume>:<fpage>16</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2017.00016</pub-id><?supplied-pmid 28286479?><pub-id pub-id-type="pmid">28286479</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C.</given-names></name><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Jia</surname><given-names>Y.</given-names></name><name><surname>Sermanet</surname><given-names>P.</given-names></name><name><surname>Reed</surname><given-names>S.</given-names></name><name><surname>Anguelov</surname><given-names>D.</given-names></name><etal/></person-group> (<year>2015</year>). <article-title>Going deeper with convolutions</article-title>, in <source>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <pub-id pub-id-type="doi">10.1109/CVPR.2015.7298594</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varoquaux</surname><given-names>G.</given-names></name><name><surname>Raamana</surname><given-names>P. R.</given-names></name><name><surname>Engemann</surname><given-names>D. A.</given-names></name><name><surname>Hoyos-Idrobo</surname><given-names>A.</given-names></name><name><surname>Schwartz</surname><given-names>Y.</given-names></name><name><surname>Thirion</surname><given-names>B.</given-names></name></person-group> (<year>2017</year>). <article-title>Assessing and tuning brain decoders: cross-validation, caveats, and guidelines</article-title>. <source>NeuroImage</source>
<volume>145</volume>, <fpage>166</fpage>–<lpage>179</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.10.038</pub-id><?supplied-pmid 27989847?><pub-id pub-id-type="pmid">27989847</pub-id></mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venkatraghavan</surname><given-names>V.</given-names></name><name><surname>Bron</surname><given-names>E. E.</given-names></name><name><surname>Niessen</surname><given-names>W. J.</given-names></name><name><surname>Klein</surname><given-names>S.</given-names></name></person-group> the Alzheimer's Disease Neuroimaging Initiative (<year>2019</year>). <article-title>Disease progression timeline estimation for Alzheimer's disease using discriminative event based modeling</article-title>. <source>NeuroImage</source>
<volume>186</volume>, <fpage>518</fpage>–<lpage>532</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.11.024</pub-id><?supplied-pmid 30471388?><pub-id pub-id-type="pmid">30471388</pub-id></mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiner</surname><given-names>M. W.</given-names></name><name><surname>Veitch</surname><given-names>D. P.</given-names></name><name><surname>Aisen</surname><given-names>P. S.</given-names></name><name><surname>Beckett</surname><given-names>L. A.</given-names></name><name><surname>Cairns</surname><given-names>N. J.</given-names></name><name><surname>Green</surname><given-names>R. C.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Recent publications from the Alzheimer's disease neuroimaging initiative: reviewing progress toward improved AD clinical trials</article-title>. <source>Alzheimers Dement.</source><volume>13</volume>, <fpage>e1</fpage>–<lpage>e85</lpage>. <pub-id pub-id-type="doi">10.1016/j.jalz.2016.11.007</pub-id><?supplied-pmid 28342697?><pub-id pub-id-type="pmid">28342697</pub-id></mixed-citation>
    </ref>
    <ref id="B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Z.</given-names></name><name><surname>Shen</surname><given-names>X.</given-names></name><name><surname>Pan</surname><given-names>W.</given-names></name></person-group> Initiative the A. D. N. (<year>2014</year>). <article-title>Longitudinal analysis is more powerful than cross-sectional analysis in detecting genetic association with neuroimaging phenotypes</article-title>. <source>PLoS ONE</source>
<volume>9</volume>:<fpage>e102312</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0102312</pub-id><?supplied-pmid 25098835?><pub-id pub-id-type="pmid">25098835</pub-id></mixed-citation>
    </ref>
    <ref id="B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Youn</surname><given-names>Y. C.</given-names></name><name><surname>Jang</surname><given-names>J.-W.</given-names></name><name><surname>Han</surname><given-names>S.-H.</given-names></name><name><surname>Kim</surname><given-names>H.</given-names></name><name><surname>Seok</surname><given-names>J.-W.</given-names></name><name><surname>Byun</surname><given-names>J. S.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>11C-PIB PET imaging reveals that amyloid deposition in cases with early-onset Alzheimer's disease in the absence of known mutations retains higher levels of PIB in the basal ganglia</article-title>. <source>Clin. Interv. Aging</source><volume>12</volume>, <fpage>1041</fpage>–<lpage>1048</lpage>. <pub-id pub-id-type="doi">10.2147/CIA.S132884</pub-id><?supplied-pmid 28721032?><pub-id pub-id-type="pmid">28721032</pub-id></mixed-citation>
    </ref>
    <ref id="B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>D.</given-names></name><name><surname>Shen</surname><given-names>D.</given-names></name></person-group> Initiative A. D. N. (<year>2012</year>). <article-title>Predicting future clinical changes of MCI patients using longitudinal and multimodal biomarkers</article-title>. <source>PLoS ONE</source>
<volume>7</volume>:<fpage>e33182</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0033182</pub-id><?supplied-pmid 22457741?><pub-id pub-id-type="pmid">22457741</pub-id></mixed-citation>
    </ref>
    <ref id="B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>D.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Zhou</surname><given-names>L.</given-names></name><name><surname>Yuan</surname><given-names>H.</given-names></name><name><surname>Shen</surname><given-names>D.</given-names></name></person-group> (<year>2011</year>). <article-title>Multimodal classification of Alzheimer's disease and mild cognitive impairment</article-title>. <source>NeuroImage</source>
<volume>55</volume>, <fpage>856</fpage>–<lpage>867</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.01.008</pub-id><?supplied-pmid 21236349?><pub-id pub-id-type="pmid">21236349</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
