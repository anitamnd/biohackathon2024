<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?subarticle pone.0256503.r001?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLoS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8409663</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-21-18896</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0256503</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Emotions</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Emotions</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Plant Science</subject>
          <subj-group>
            <subject>Plant Anatomy</subject>
            <subj-group>
              <subject>Flower Anatomy</subject>
              <subj-group>
                <subject>Petals</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Data Management</subject>
          <subj-group>
            <subject>Data Visualization</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Cognitive Science</subject>
            <subj-group>
              <subject>Cognitive Psychology</subject>
              <subj-group>
                <subject>Perception</subject>
                <subj-group>
                  <subject>Sensory Perception</subject>
                  <subj-group>
                    <subject>Vision</subject>
                  </subj-group>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Cognitive Psychology</subject>
            <subj-group>
              <subject>Perception</subject>
              <subj-group>
                <subject>Sensory Perception</subject>
                <subj-group>
                  <subject>Vision</subject>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Cognitive Psychology</subject>
            <subj-group>
              <subject>Perception</subject>
              <subj-group>
                <subject>Sensory Perception</subject>
                <subj-group>
                  <subject>Vision</subject>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Sensory Perception</subject>
            <subj-group>
              <subject>Vision</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Linguistics</subject>
          <subj-group>
            <subject>Semantics</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Plant Science</subject>
          <subj-group>
            <subject>Plant Anatomy</subject>
            <subj-group>
              <subject>Flowers</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Emotions</subject>
            <subj-group>
              <subject>Fear</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Emotions</subject>
            <subj-group>
              <subject>Fear</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Data Management</subject>
          <subj-group>
            <subject>Data Visualization</subject>
            <subj-group>
              <subject>Infographics</subject>
              <subj-group>
                <subject>Charts</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PyPlutchik: Visualising and comparing emotion-annotated corpora</article-title>
      <alt-title alt-title-type="running-head">PyPlutchik: Visualising and comparing emotion-annotated corpora</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Semeraro</surname>
          <given-names>Alfonso</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Conceptualization</role>
        <role content-type="https://casrai.org/credit/">Data curation</role>
        <role content-type="https://casrai.org/credit/">Formal analysis</role>
        <role content-type="https://casrai.org/credit/">Methodology</role>
        <role content-type="https://casrai.org/credit/">Software</role>
        <role content-type="https://casrai.org/credit/">Validation</role>
        <role content-type="https://casrai.org/credit/">Visualization</role>
        <role content-type="https://casrai.org/credit/">Writing – original draft</role>
        <xref rid="aff001" ref-type="aff"/>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7666-0318</contrib-id>
        <name>
          <surname>Vilella</surname>
          <given-names>Salvatore</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Conceptualization</role>
        <role content-type="https://casrai.org/credit/">Data curation</role>
        <role content-type="https://casrai.org/credit/">Methodology</role>
        <role content-type="https://casrai.org/credit/">Validation</role>
        <role content-type="https://casrai.org/credit/">Visualization</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ruffo</surname>
          <given-names>Giancarlo</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Conceptualization</role>
        <role content-type="https://casrai.org/credit/">Methodology</role>
        <role content-type="https://casrai.org/credit/">Project administration</role>
        <role content-type="https://casrai.org/credit/">Supervision</role>
        <role content-type="https://casrai.org/credit/">Visualization</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <addr-line>Department of Computer Science, University of Turin, Turin, Italy</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Danforth</surname>
          <given-names>Christopher M.</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>University of Vermont, UNITED STATES</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>alfonso.semeraro@unito.it</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>1</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <volume>16</volume>
    <issue>9</issue>
    <elocation-id>e0256503</elocation-id>
    <history>
      <date date-type="received">
        <day>8</day>
        <month>6</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>6</day>
        <month>8</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 Semeraro et al</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Semeraro et al</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pdf" xlink:href="pone.0256503.pdf"/>
    <abstract>
      <p>The increasing availability of textual corpora and data fetched from social networks is fuelling a huge production of works based on the model proposed by psychologist Robert Plutchik, often referred simply as the “Plutchik Wheel”. Related researches range from annotation tasks description to emotions detection tools. Visualisation of such emotions is traditionally carried out using the most popular layouts, as bar plots or tables, which are however sub-optimal. The classic representation of the Plutchik’s wheel follows the principles of proximity and opposition between pairs of emotions: spatial proximity in this model is also a semantic proximity, as adjacent emotions elicit a complex emotion (a primary dyad) when triggered together; spatial opposition is a semantic opposition as well, as positive emotions are opposite to negative emotions. The most common layouts fail to preserve both features, not to mention the need of visually allowing comparisons between different corpora in a blink of an eye, that is hard with basic design solutions. We introduce PyPlutchik the Pyplutchik package is available as a Github repository (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://github.com/alfonsosemeraro/pyplutchik" ext-link-type="uri">http://github.com/alfonsosemeraro/pyplutchik</ext-link>) or through the installation commands <italic toggle="yes">pip</italic> or <italic toggle="yes">conda</italic>. For any enquiry about usage or installation feel free to contact the corresponding author, a Python module specifically designed for the visualisation of Plutchik’s emotions in texts or in corpora. PyPlutchik draws the Plutchik’s flower with each emotion petal sized after how much that emotion is detected or annotated in the corpus, also representing three degrees of intensity for each of them. Notably, PyPlutchik allows users to display also primary, secondary, tertiary and opposite dyads in a compact, intuitive way. We substantiate our claim that PyPlutchik outperforms other classic visualisations when displaying Plutchik emotions and we showcase a few examples that display our module’s most compelling features.</p>
    </abstract>
    <funding-group>
      <funding-statement>The author(s) received no specific funding for this work.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="15"/>
      <table-count count="1"/>
      <page-count count="24"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>All data is freely available and the plots can be easily reproduced, we do not have any special access to the data. The data is properly referenced in the bibliography, the links are the following: SSEC data: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.romanklinger.de/ssec/" ext-link-type="uri">http://www.romanklinger.de/ssec/</ext-link> Amazon data: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://jmcauley.ucsd.edu/data/amazon/" ext-link-type="uri">https://jmcauley.ucsd.edu/data/amazon/</ext-link> IMDB data: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/omarhanyy/imdb-top-1000" ext-link-type="uri">https://www.kaggle.com/omarhanyy/imdb-top-1000</ext-link> We added the scripts to reproduce the plots to the Github repository.(<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/alfonsosemeraro/pyplutchik" ext-link-type="uri">https://github.com/alfonsosemeraro/pyplutchik</ext-link>).</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>All data is freely available and the plots can be easily reproduced, we do not have any special access to the data. The data is properly referenced in the bibliography, the links are the following: SSEC data: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.romanklinger.de/ssec/" ext-link-type="uri">http://www.romanklinger.de/ssec/</ext-link> Amazon data: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://jmcauley.ucsd.edu/data/amazon/" ext-link-type="uri">https://jmcauley.ucsd.edu/data/amazon/</ext-link> IMDB data: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.kaggle.com/omarhanyy/imdb-top-1000" ext-link-type="uri">https://www.kaggle.com/omarhanyy/imdb-top-1000</ext-link> We added the scripts to reproduce the plots to the Github repository.(<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/alfonsosemeraro/pyplutchik" ext-link-type="uri">https://github.com/alfonsosemeraro/pyplutchik</ext-link>).</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>1 Introduction</title>
    <p>The recent availability of massive textual corpora has enhanced an extensive research over the emotional dimension underlying human-produced texts. Sentences, conversations, posts, tweets and many other pieces of text can be labelled according to a variety of schemes, that refer to as many psychological theoretical frameworks. Such frameworks are commonly divided into <italic toggle="yes">categorical</italic> models [<xref rid="pone.0256503.ref001" ref-type="bibr">1</xref>–<xref rid="pone.0256503.ref004" ref-type="bibr">4</xref>], based on a finite set of labels, and <italic toggle="yes">dimensional</italic> models [<xref rid="pone.0256503.ref005" ref-type="bibr">5</xref>, <xref rid="pone.0256503.ref006" ref-type="bibr">6</xref>], that position data points as continuous values in an N-dimensional vector space of emotions. One of the most famous dimensional models is Russel’s <italic toggle="yes">circumplex model</italic> of emotions [<xref rid="pone.0256503.ref007" ref-type="bibr">7</xref>]. Russel’s model posits that affect states are defined by dimensions that are not independent: emotions can be represented as points in a space whose main orthogonal axes represent their degree of <italic toggle="yes">arousal</italic> and of <italic toggle="yes">pleasure/displeasure</italic>. According to Russel, the full spectrum of emotions can be meaningfully placed along the resulting circumference. Russel is not the first scholar to use a circular layout to represent emotions (for instance, it was already used in [<xref rid="pone.0256503.ref008" ref-type="bibr">8</xref>]): indeed, this kind of <italic toggle="yes">circumplex</italic> representation became very popular over time, since it is suitable to spatially represent emotions on a continuous space.</p>
    <p>The cultural variation of emotions has also been studied. This is a crucial factor to take into account when classifying emotions: intuitively, the categories themselves can radically change depending on the cultural background [<xref rid="pone.0256503.ref009" ref-type="bibr">9</xref>]. This is also valid with respect to how the annotator perceives the emotions in a text and, if the annotated data is then used to train a classifier, it can introduce a bias in the model. The nomenclature of emotions, their meanings and the relations of words to emotion concepts depend on the social frameworks in which we are born and raised: therefore, cultural variation can significantly impact this kind of analysis. In this regard, in [<xref rid="pone.0256503.ref010" ref-type="bibr">10</xref>] the authors estimate emotion semantics across a sample of almost 2500 spoken languages, finding high variability in the meaning of emotion terms, but also evidence of a <italic toggle="yes">universal structure</italic> in the categorization of emotions. Following different methodological approaches, similar results were previously obtained in [<xref rid="pone.0256503.ref011" ref-type="bibr">11</xref>].</p>
    <p>Regardless of their categorical or dimensional nature, the emotional models provide a complex and multifaceted characterisation of emotions, which often necessitates dedicated and innovative ways to visualise them. This is the case of Plutchik’s model of emotions [<xref rid="pone.0256503.ref012" ref-type="bibr">12</xref>], a categorical model based on 8 labels (<italic toggle="yes">Joy</italic>, <italic toggle="yes">Trust</italic>, <italic toggle="yes">Fear</italic>, <italic toggle="yes">Surprise</italic>, <italic toggle="yes">Sadness</italic>, <italic toggle="yes">Disgust</italic>, <italic toggle="yes">Anger</italic> and <italic toggle="yes">Anticipation</italic>). According to the model, emotions are displayed in a flower-shaped representation, famously known as <italic toggle="yes">Plutchik’s wheel</italic>, which has become since then a classic reference in this domain. The model, that displays emotions in this <italic toggle="yes">circumplex</italic>-like representation and that is described in detail in Sec. 2, leverages the disposition of the <italic toggle="yes">petals</italic> around the wheel to highlight the similar (or opposite) flavour of the emotions, as well as how similar emotions, placed in the same “hemisphere” of the wheel, can combine into primary, secondary and tertiary <italic toggle="yes">dyads</italic>, depending on how many petals away they are located on the flower.</p>
    <p>It is clear that such a complex and elaborated solution plays a central role in defining the model itself. Still, as detailed in Sec. 2, many studies that resort to Plutchik’s model display their results using standard data visualisation layouts, such as bar plots, tables, pie charts and scatter plots, most likely due to the lack of an easy, plug-and-play implementation of the Plutchik’s wheel.</p>
    <p>On these premises, we argue that the most common layouts fail to preserve the characterising features of Plutchik’s model, not to mention the need of visually allowing comparisons between different corpora at a glance, that is hard with basic design solutions. We contribute to fill the gap in the data visualisation tools by introducing <italic toggle="yes">PyPlucthik</italic>, a Python module for visualising texts and corpora annotated according to the Plutchik’s model of emotions. Given the preeminence of Python as a programming language in the field of data science and, particularly, in the area of Natural Language Processing (NLP), we believe that the scientific community will benefit from a ready-to-use Python tool to fulfil this particular need. Of course, other packages and libraries may be released for other languages in the future.</p>
    <p>PyPlutchik provides an off-the-shelf Python implementation of the Plutchik’s wheel. Each petal of the flower is sized after the amount of the correspondent emotion in the corpus: the more traces of an emotion are detected in a corpus, the bigger the petal is drawn. Along with the 8 basic emotions, PyPlutchik displays also three degrees of intensity for each emotion (see <xref rid="pone.0256503.t001" ref-type="table">Table 1</xref>).</p>
    <table-wrap position="float" id="pone.0256503.t001">
      <object-id pub-id-type="doi">10.1371/journal.pone.0256503.t001</object-id>
      <label>Table 1</label>
      <caption>
        <title>Plutchik’s 8 basic emotions with 3 degrees of intensity each.</title>
        <p>Emotions are commonly referred as the middle intensity degree ones.</p>
      </caption>
      <alternatives>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.t001" id="pone.0256503.t001g" position="float"/>
        <table frame="hsides" rules="groups" cellspacing="0" style="border-collapse:collapse">
          <colgroup span="1">
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Lower intensity</th>
              <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Emotion</th>
              <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Higher intensity</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" rowspan="1" colspan="1">Annoyance</td>
              <td align="center" rowspan="1" colspan="1">
                <bold>Anger</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">Rage</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Interest</td>
              <td align="center" rowspan="1" colspan="1">
                <bold>Anticipation</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">Vigilance</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Serenity</td>
              <td align="center" rowspan="1" colspan="1">
                <bold>Joy</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">Ecstasy</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Acceptance</td>
              <td align="center" rowspan="1" colspan="1">
                <bold>Trust</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">Admiration</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Apprehension</td>
              <td align="center" rowspan="1" colspan="1">
                <bold>Fear</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">Terror</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Distraction</td>
              <td align="center" rowspan="1" colspan="1">
                <bold>Surprise</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">Amazement</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Pensiveness</td>
              <td align="center" rowspan="1" colspan="1">
                <bold>Sadness</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">Grief</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Boredom</td>
              <td align="center" rowspan="1" colspan="1">
                <bold>Disgust</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">Loathing</td>
            </tr>
          </tbody>
        </table>
      </alternatives>
    </table-wrap>
    <p>PyPlutchik is built on top of Python data visualisation library <italic toggle="yes">matplotlib</italic> [<xref rid="pone.0256503.ref013" ref-type="bibr">13</xref>], and it is fully scriptable, hence it can be used for representing the emotion annotation of single texts (e.g. for a single tweet), as well as of entire corpora (e.g. a collection of tweets), offering a tool for a proper representation of such annotated texts, which at the best of our knowledge was missing. The two-dimensional Plutchik’s wheel is immediately recognisable, but it is a mere qualitative illustration. PyPlutchik introduces a quantitative dimension to this representation, making it a tool suitable for representing how much an emotion is detected in a corpus. The module accepts as an input a score <italic toggle="yes">s</italic><sub><italic toggle="yes">i</italic></sub> ∈ [0, 1] for each of the 24 <italic toggle="yes">i</italic> emotions in the model (8 basics emotions, 3 degrees of intensity each). Please note that, since the same text cannot express two different degrees of the same emotion, the sum of all the scores of the emotions belonging to the same branch must be less than or equal to 1. Each emotion petal is then sized according to this score. In <xref rid="pone.0256503.g001" ref-type="fig">Fig 1</xref> we can see an example of the versatility of the PyPlutchik representation of the annotated emotions: in (i) we see a pseudo-text in which only <italic toggle="yes">Joy</italic>, <italic toggle="yes">Trust</italic> and <italic toggle="yes">Sadness</italic> have been detected; in (ii) for each emotion, the percentage of pseudo-texts in a pseudo-corpus that show that emotion; finally (iii) contains a detail of (ii), where the three degrees of intensity have been annotated separately.</p>
    <fig position="float" id="pone.0256503.g001">
      <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g001</object-id>
      <label>Fig 1</label>
      <caption>
        <p>A three fold showcase of our visualisation tool on synthetic data: (i) a text where only <italic toggle="yes">Joy</italic>, <italic toggle="yes">Trust</italic> and <italic toggle="yes">Sadness</italic> have been detected; (ii) a corpus of many texts. Each petal is sized after the amount of items in the corpus that show that emotion in; (iii) same corpus as (ii), but higher and lower degrees of intensity of each emotion are expressed.</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g001" position="float"/>
    </fig>
    <p>Most importantly, PyPlutchik is respectful of the original spatial and aesthetic features of the wheel of emotions intended by its author. The colour code has been hard-coded in the module, as it is a distinctive feature of the wheel that belongs to collective imagination. Spatial distribution of the emotion is also a standard, non customisable feature, as the displacement of each petal of the flower is non arbitrary, because it reflects a semantic proximity of close emotions, and a semantic contrariety of opposite emotions (see Sec. 2).</p>
    <p>Representing emotions detected in texts can be hard without a proper tool, but it is a need for the many scientists that work on text and emotions. A great variety of newly available digital text has been explored in order to uncover emotional patterns; the necessity of a handy instrument to easily display such information is still unsatisfied.</p>
    <p>In the following sections, after introducing the reader to the topic of emotion models and their applications in corpora annotation, we will focus on the Plutchik’s emotion model and the current state of the art of its representations. A detailed technical explanation of the PyPlutchik module will follow, with several use cases on a wide range of datasets to help substantiating our claim that PyPlutchik outperforms other classic visualisations.</p>
  </sec>
  <sec id="sec002">
    <title>2 Related work</title>
    <sec id="sec003">
      <title>2.1 Visualising textual data</title>
      <p>Visualising quantitative information associated to textual data might not be an easy task, due to “<italic toggle="yes">the categorical nature of text and its high dimensionality, that makes it very challenging to display graphically</italic>” [<xref rid="pone.0256503.ref014" ref-type="bibr">14</xref>]. Several scientific areas leverage visualisations techniques to extract meaning from texts, such as digital humanities [<xref rid="pone.0256503.ref015" ref-type="bibr">15</xref>] or social media analysis [<xref rid="pone.0256503.ref016" ref-type="bibr">16</xref>].</p>
      <p>Textual data visualisations often usually provide tools for literacy and citation analysis; e.g., PhraseNet [<xref rid="pone.0256503.ref017" ref-type="bibr">17</xref>], Word Tree [<xref rid="pone.0256503.ref018" ref-type="bibr">18</xref>], Web Seer <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://hint.fm/seer/" ext-link-type="uri">http://hint.fm/seer/</ext-link>, and Themail [<xref rid="pone.0256503.ref019" ref-type="bibr">19</xref>] introduced many different ways to generate visual overviews of unstructured texts. Many of these projects were connected to <italic toggle="yes">ManyEyes</italic>, that was launched in 2007 by Viégas, Wattenberg, and al. [<xref rid="pone.0256503.ref020" ref-type="bibr">20</xref>] at IBM, and closed in 2015 to be included in IBM Analytics. ManyEyes was designed as a web based community where users (mainly data analysts and visualisation designers) could upload their data to establish conversations with other users. The ambitious goal was to create a social style of data analysis so that visualisations can be tools to create collaboration and carry on discussions.</p>
      <p>Nevertheless, all of these classic visualisation tools did not allow the exploration of more advanced textual semantic features that can be analysed nowadays due to the numerous developments of Natural Language Processing techniques; nowadays, it exists a good number of software suites—such as <italic toggle="yes">Tableau</italic>, <italic toggle="yes">Microsoft PowerBI</italic> or <italic toggle="yes">Datawrapper</italic>, just to mention a few—that give the users a chance to create very interesting, eye-catching and often complex visualizations. They all adopt a graphical user interface, with all the pros and cons that usually come with it: an intuitive and fast way to realise the majority of the most common layouts, but likely less flexible when it comes to create a more personalised visualisation. On the other hand, programming libraries and modules—such as Python’s <italic toggle="yes">Matplotlib</italic> [<xref rid="pone.0256503.ref013" ref-type="bibr">13</xref>], <italic toggle="yes">Plotly</italic> [<xref rid="pone.0256503.ref021" ref-type="bibr">21</xref>] and <italic toggle="yes">Bokeh</italic> [<xref rid="pone.0256503.ref022" ref-type="bibr">22</xref>], or <italic toggle="yes">D3.js</italic> [<xref rid="pone.0256503.ref023" ref-type="bibr">23</xref>] in Javascript allow the users to create freely their own visualisations, though with a much steeper learning curve for those who are not familiar with these technologies.</p>
      <p>The recent advancements in text technologies have enabled researchers and professional analysts with new tools to find more complex patterns in textual data. Algorithms for topic detection, sentiment analysis, stance detection and emotion detection allow us to convert very large amounts of textual data to actionable knowledge; still, the outputs of such algorithms can be too hard to consume if not with an appropriate data visualisation [<xref rid="pone.0256503.ref024" ref-type="bibr">24</xref>]. During the last decade, many works have been carried out to fill this gap in the areas of (hierarchical) topic visualisation [<xref rid="pone.0256503.ref025" ref-type="bibr">25</xref>–<xref rid="pone.0256503.ref028" ref-type="bibr">28</xref>], sentiment visualisation (a comprehensive survey can be found in [<xref rid="pone.0256503.ref029" ref-type="bibr">29</xref>]), online hate speech detection [<xref rid="pone.0256503.ref030" ref-type="bibr">30</xref>], stance detection [<xref rid="pone.0256503.ref031" ref-type="bibr">31</xref>] and many more.</p>
      <p>Our work lies within the domain of visualisation of emotions in texts, as we propose a novel Python implementation of Plutchik’s wheel of emotions.</p>
    </sec>
    <sec id="sec004">
      <title>2.2 Understanding emotions from texts</title>
      <p>In the last few years, many digital text sources such as social media, digital libraries or television transcripts have been exploited for emotion-based analyses. To mention just a few examples, researchers have studied the display of emotions in online social networks like Twitter [<xref rid="pone.0256503.ref032" ref-type="bibr">32</xref>–<xref rid="pone.0256503.ref035" ref-type="bibr">35</xref>] and Facebook [<xref rid="pone.0256503.ref036" ref-type="bibr">36</xref>–<xref rid="pone.0256503.ref038" ref-type="bibr">38</xref>], in literature corpora [<xref rid="pone.0256503.ref039" ref-type="bibr">39</xref>, <xref rid="pone.0256503.ref040" ref-type="bibr">40</xref>], in television conversations [<xref rid="pone.0256503.ref041" ref-type="bibr">41</xref>], in dialogues excerpts from call centres conversations [<xref rid="pone.0256503.ref042" ref-type="bibr">42</xref>], in human-human video conversations [<xref rid="pone.0256503.ref043" ref-type="bibr">43</xref>]. In [<xref rid="pone.0256503.ref044" ref-type="bibr">44</xref>] the authors, after developing a methodology (the so-called TFMN, <italic toggle="yes">textual forma mentis networks</italic>) that exploits complex networks to extract the semantic framework of concepts from texts, resort to the Plutchik’s model to analyse it.</p>
      <p>Among categorical emotion models, Plutchik’s wheel of emotions is one of the most popular. Categorical (or discrete) emotions model root to the works of Paul Ekman [<xref rid="pone.0256503.ref001" ref-type="bibr">1</xref>], who first recognised six basic emotions universal to the human kind (<italic toggle="yes">Anger</italic>, <italic toggle="yes">Disgust</italic>, <italic toggle="yes">Fear</italic>, <italic toggle="yes">Happiness</italic>, <italic toggle="yes">Sadness</italic>, <italic toggle="yes">Surprise</italic>). Although basicality of emotions is debated [<xref rid="pone.0256503.ref045" ref-type="bibr">45</xref>], categorical emotions are very popular in natural language processing research, because of their practicality in annotation. In recent years many other categorical emotion models have been proposed, each with a distinctive set of basic emotions: the model first proposed by James [<xref rid="pone.0256503.ref002" ref-type="bibr">2</xref>] presents 6 basic emotions, Plutchik’s model 8, Izard’s model [<xref rid="pone.0256503.ref003" ref-type="bibr">3</xref>] 12, Lazarus et al. [<xref rid="pone.0256503.ref004" ref-type="bibr">4</xref>] model 15, Ekman’s extended model [<xref rid="pone.0256503.ref046" ref-type="bibr">46</xref>] 18, Cowen et al. [<xref rid="pone.0256503.ref047" ref-type="bibr">47</xref>] 27. Parrott [<xref rid="pone.0256503.ref048" ref-type="bibr">48</xref>] proposed a tree-structured model with 6 basic emotions on a first level, 25 on a second level and more than one hundred on a third level. Susanto et al. in [<xref rid="pone.0256503.ref049" ref-type="bibr">49</xref>] propose a revisited version of the <italic toggle="yes">hourglass of emotions</italic> by Cambria et al. [<xref rid="pone.0256503.ref050" ref-type="bibr">50</xref>], an interesting model that moves from Plutchik’s one by positioning emotions in an hourglass-shaped design.</p>
      <p>However, annotation of big corpora of texts is easier if labels are in a small number, clearly distinct from each other; on the other hand, a categorical classification of complex human emotions into a handful of basic labels may be limiting.</p>
      <p>Plutchik’s model’s popularity is probably due to a peculiar characteristic. In its wheel of emotions, there are 8 basic emotions (<italic toggle="yes">Joy</italic>, <italic toggle="yes">Trust</italic>, <italic toggle="yes">Fear</italic>, <italic toggle="yes">Surprise</italic>, <italic toggle="yes">Sadness</italic>, <italic toggle="yes">Disgust</italic>, <italic toggle="yes">Anger</italic> and <italic toggle="yes">Anticipation</italic>) with three intensity degrees each, as shown in <xref rid="pone.0256503.t001" ref-type="table">Table 1</xref> and in <xref rid="pone.0256503.g002" ref-type="fig">Fig 2</xref>. Even if each emotion is a category on its own, emotions are related each other by their spatial displacement. In fact, four emotions (<italic toggle="yes">Anger</italic>, <italic toggle="yes">Anticipation</italic>, <italic toggle="yes">Joy</italic>, <italic toggle="yes">Trust</italic>) are respectively opposed to the other four (<italic toggle="yes">Fear</italic>, <italic toggle="yes">Surprise</italic>, <italic toggle="yes">Sadness</italic>, <italic toggle="yes">Disgust</italic>); for instance, <italic toggle="yes">Joy</italic> is the opposite of <italic toggle="yes">Sadness</italic>, hence it is displayed symmetrically with respect to the centre of the wheel. When elicited together, two emotions raise a <italic toggle="yes">dyad</italic>, a complex emotion. Dyads are divided into primary (when triggered by two adjacent emotions), secondary (when triggered by two emotions that are 2 petals away), tertiary (when triggered by two emotions that are 3 petals away) and opposite (when triggered by opposite emotions). A comprehensive diagram of emotion combination and elicited dyads is represented in <xref rid="pone.0256503.g003" ref-type="fig">Fig 3</xref>. This mechanism allows to annotate a basic set of only 8 emotions, while triggering eventually up to 28 more complex nuances, that better map the complexity of human emotions. When representing corpora annotated following Plutchik’s model, it is important then to highlight spatial adjacency or spatial opposition of emotions in a graphical way. We will refer to these feature as <italic toggle="yes">semantic proximity</italic> and <italic toggle="yes">semantic opposition</italic> of two emotions.</p>
      <fig position="float" id="pone.0256503.g002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Plutchik’s wheel of emotions.</title>
          <p>Each petal is partitioned in three degrees of intensity, from the most intense (the most internal section) to the least intense (the most external section).</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g002" position="float"/>
      </fig>
      <fig position="float" id="pone.0256503.g003">
        <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Diagram of Plutchik’s dyads.</title>
          <p>When two emotions are elicited together, they trigger the corresponding primary dyad if they are just one petal apart, a secondary dyad if they are two petal distant each other, a tertiary dyad if they are three petal distant, an opposite dyad if they are on the opposite side of the flower.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g003" position="float"/>
      </fig>
      <p>From a data visualisation point of view, PyPlutchik’s closest relatives can be found in bar plots, radar plots and Windrose diagrams. Bar plots correctly display the quantitative representation of categorical data, while radar plots (also known as spider plot) correctly displace elements in a polar coordinate system, close to the original Plutchik’s one. Windrose diagrams combine both advantages, displaying categorical data on a polar coordinate system. PyPlutchik is inspired to this representation, and it adapts this idea to the collective imagination of Plutchik’s wheel of emotion graphical picture.</p>
    </sec>
    <sec id="sec005">
      <title>2.3 Representing Plutchik’s emotions wheel</title>
      <p>If we skim through the related literature, we notice that many papers needed to display the distribution of emotions in a corpus, and without a dedicated tool they all settled for a practical but sub-optimal solution. In some way, each of the following representations does not respect the standard spatial or aesthetic features of Plutchik’s wheel of emotions:</p>
      <list list-type="bullet">
        <list-item>
          <p><bold>Tables</bold>, as used in [<xref rid="pone.0256503.ref051" ref-type="bibr">51</xref>–<xref rid="pone.0256503.ref056" ref-type="bibr">56</xref>]. Tables are a practical way to communicate exact amounts in an unambiguous way. However, tables are not a proper graphical display, so they miss all the features of the original wheel of emotions: there is not a proper colour code and both semantic proximity and semantic opposition are dismantled. Confronted with a plot, texts are harder to read: plots deliver the same information earlier and easier.</p>
        </list-item>
        <list-item>
          <p><bold>Bar plots</bold>, as used in [<xref rid="pone.0256503.ref057" ref-type="bibr">57</xref>–<xref rid="pone.0256503.ref063" ref-type="bibr">63</xref>]. Bar plots are a traditional option to allow numerical comparisons across categories. In this domain, each bar would represent how many times a emotion is shown in a given corpus. However, bar plots are sub-optimal for two reasons. Firstly, the spatial displacement of the bars does not reflect semantic opposition of two emotions, that are opposites in the Plutchik’s wheel. Secondly, Plutchik’s wheel is circular, meaning that there is a semantic proximity between the first and the last of the 8 emotions branches, which is not represented in a bar plot. PyPlutchik preserves both semantic opposition and semantic proximity: the mass distribution of the ink in Fig 14(i and vi), for instance, immediately communicates of a positive corpus, as positive emotions are way more expressed than their opposites.</p>
        </list-item>
        <list-item>
          <p><bold>Pie charts</bold>, as used in [<xref rid="pone.0256503.ref055" ref-type="bibr">55</xref>, <xref rid="pone.0256503.ref064" ref-type="bibr">64</xref>–<xref rid="pone.0256503.ref067" ref-type="bibr">67</xref>]. Pie charts are a better approximation of the Plutchik’s wheel, as they respect the colour code and they almost respect the spatial displacement of emotions. However, the actual displacement may depend on the emotion distribution: with a skewed distribution toward one or two emotions, all the remaining sectors may be shrunk and translated to a different position. Pie charts do not guarantee a correct spatial positioning of each category. There is also an underlying conceptual flaw in pie charts: they do not handle well items annotated with more than one tag, in this case texts annotated with more than one emotion. In a pie chart, the sum of the sectors’ sizes must equal the number of all the items; each sector would count how many items fall into a category. If multiple annotation on the same item are allowed, the overall sum of sectors’ sizes will exceed the number of actual items in the corpus. Null-annotated items, i.e. those without a noticeable emotion within, must be represented as a ninth, neutral sector. PyPlutchik handles multi-annotated and null-annotated items: for instance, <xref rid="pone.0256503.g001" ref-type="fig">Fig 1(ii)</xref> shows a pseudo-corpus where <italic toggle="yes">Anger</italic> and <italic toggle="yes">Disgust</italic> both are valued one, because they appear in 100% of the pseudo-texts within. <xref rid="pone.0256503.g001" ref-type="fig">Fig 1(i)</xref> shows a text with several emotions missing.</p>
        </list-item>
        <list-item>
          <p><bold>Heatmaps</bold>, as used in [<xref rid="pone.0256503.ref068" ref-type="bibr">68</xref>, <xref rid="pone.0256503.ref069" ref-type="bibr">69</xref>]. Both papers coded the intensity of the 8 basic emotions depending on a second variable, respectively time and principal components of a vectorial representation of texts. Although heatmaps naturally fit the idea of an intensity score at the crossroad of two variables, the final display are sub-optimal in both cases, because they fail to preserve both the Plutchik’s wheel’s colour code and spatial displacement of emotions. As described in Sect. 3, PyPlutchik can be easily scripted for reproducing small-multiples. In Sect. 5 we provide an example of a small-multiple, displaying the evolution of the distribution of emotions in a corpus over time.</p>
        </list-item>
        <list-item>
          <p><bold>Scatter plots</bold>, as used in [<xref rid="pone.0256503.ref070" ref-type="bibr">70</xref>]. Scatter plot are intended to display data points in a two- or three-dimensional space, where each axis maps a continuous variable. In [<xref rid="pone.0256503.ref070" ref-type="bibr">70</xref>], x-axis represents the rank of each emotion on each of the three corpora they analyse, thus producing a descending sorting of emotion labels. This choice was probably made in order to have three descending, more readable series of scatters on the plot. However, this representation breaks both the colour code and the spatial displacement of emotions. PyPlutchik can be easily scripted for a side-by-side comparison of more than one corpus (see Sect. 3), allowing readers to immediately grasp high level discrepancies.</p>
        </list-item>
        <list-item>
          <p><bold>Line plots</bold>, as used in [<xref rid="pone.0256503.ref071" ref-type="bibr">71</xref>]. As well as scatter plots, line plots are appropriate for displaying a trend in a two-dimensional space, where each dimension maps a continuous variable. It is not the case of discrete emotions. Authors plotted the distribution of each emotion over time as a separate line. They managed to colour each line with the corresponding colour in the Plutchik’s wheel, reporting the colour code in a separate legend. As stated before in similar cases, this representation breaks the semantic proximity (opposition) of close (opposite) emotions. Again, in Sect. 3 we provide details about how to script PyPlutchik to produce a small-multiple plot, while in Sec. 5 we showcase the distribution of emotions by time on a real corpus.</p>
        </list-item>
        <list-item>
          <p><bold>Radar plots</bold>, as used in [<xref rid="pone.0256503.ref072" ref-type="bibr">72</xref>–<xref rid="pone.0256503.ref074" ref-type="bibr">74</xref>]. Radar plots, a.k.a. Circular Column Graphs or Star Graphs, successfully preserve spatial proximity of emotions. Especially when the radar area is filled with a non-transparent colour, radars correctly distribute more mass where emotions are more expressed, giving to the reader an immediate sense of how shifted a corpus is against a neutral one. However, on a minor note, continuity of lines and shapes do not properly separate each emotion as a discrete objects per se. Furthermore, radars do not naturally reproduce the right colour code. Lastly, radars are not practical to reproduce stacked values, like the three degrees of intensity in <xref rid="pone.0256503.g001" ref-type="fig">Fig 1 (i)</xref>. Of course, all of these minor issues can be solved with an extension of the basic layout, or also adopting a <bold>Nightingale Rose Chart</bold> (also referred as Polar Area Chart or Windrose diagram), as in [<xref rid="pone.0256503.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0256503.ref075" ref-type="bibr">75</xref>]. However, the main drawback with radar plots and derivatives is that semantic opposition is lost, and we do not have a direct way to represents dyads and their occurrences. PyPlutchik, conversely, has been tailored on the original emotion’s wheel, and it naturally represents both semantic proximity and opposition, as well as the occurrences of dyads in our corpora (see Sect. 4).</p>
        </list-item>
      </list>
    </sec>
  </sec>
  <sec id="sec006">
    <title>3 Visualising primary emotions with PyPlutchik</title>
    <p>PyPlutchik is designed to be integrated in data visualisation with the Python library matplotlib. It spans the printable area in a range of [−1.6, 1.6] inches on both axes, taking the space to represent a petal of maximum length 1, plus the outer labels and the inner white circle. Each petal overlaps on one of the 8 axis of the polar coordinate system. Four transversal minor grid lines cross each axis, spaced of 0.2 inches each, making it a visual reference for a quick evaluation of the petal size and for a comparison between non adjacent petals. Outside the range [0, 1], corresponding to each petal, two labels represent the emotion and the associated numerical score. Colour code is strictly hard-coded, following Plutchik’s wheel of emotions classic representation. PyPlutchik can be used either to plot only the 8 basic emotions, or to show the full intensity spectrum of each emotion, assigning three scores for the three intensity levels. In the latter case, each petal is divided into three sections, with colour intensity decreasing from the centre. In both cases PyPlutchik accepts as input a <italic toggle="yes">dict</italic> data structure, with exactly 8 items. Keys must be the 8 basic emotions names. <italic toggle="yes">dict</italic> is a natural Python data structure for representing JSON files, making PyPlutchik an easy choice to display JSONs. In case of basic emotions only, values in the <italic toggle="yes">dict</italic> must be numeric ∈ [0, 1], while in case of intensity degrees they must be presented as an <italic toggle="yes">iterable</italic> of length three, whose entries must sum to maximum 1. Figs <xref rid="pone.0256503.g004" ref-type="fig">4</xref> and <xref rid="pone.0256503.g005" ref-type="fig">5</xref> show how straightforward it is to plug a <italic toggle="yes">dict</italic> into the module to obtain the visualisation. Furthermore, PyPlutchik can be used to display the occurrences of primary, secondary, and tertiary dyads in our corpora. This more advanced feature will be described in Sect. 4.</p>
    <fig position="float" id="pone.0256503.g004">
      <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g004</object-id>
      <label>Fig 4</label>
      <caption>
        <title>Plutchik’s wheel generated by code on the right.</title>
        <p>Each entry in the Python <italic toggle="yes">dict</italic> is a numeric value ∈ [0, 1].</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g004" position="float"/>
    </fig>
    <fig position="float" id="pone.0256503.g005">
      <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g005</object-id>
      <label>Fig 5</label>
      <caption>
        <title>Plutchik’s wheel generated by code on the right.</title>
        <p>Each entry in the Python <italic toggle="yes">dict</italic> is a three-sized array, whose sum must be ∈ [0, 1].</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g005" position="float"/>
    </fig>
    <p>Due to the easy integration with Python basic data structures and the matplotlib library, PyPlutchik is also completely scriptable to display several plots side by side as small-multiple. Default font family is sans-serif, and text is printed with light weight and size 15 by default. However, it is possible to modify these features by the means of the corresponding parameters (see the documentation the Pyplutchik documentation is available in the Github repository at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/alfonsosemeraro/pyplutchik/blob/master/Documentation.md" ext-link-type="uri">https://github.com/alfonsosemeraro/pyplutchik/blob/master/Documentation.md</ext-link> for a comprehensive and always up-to-date explanation of parameters and features of the module). These features can be also changed with standard matplotlib syntax. The polar coordinates beneath petals and the labels outside can be hidden: this feature leaves only the flower on screen, improving visibility of small flowers in small-multiple plots. Also the petals aspect can be modified, by making them thinner or thicker. <xref rid="pone.0256503.g006" ref-type="fig">Fig 6</xref> shows a small-multiple, with hidden polar coordinates and labels, computed on synthetic random data that have been artificially created only for illustrative purposes. Code for this kind of representation is in the code documentation, as well as a short overview on the parameters that make possible to reproduce the aforementioned variations.</p>
    <fig position="float" id="pone.0256503.g006">
      <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g006</object-id>
      <label>Fig 6</label>
      <caption>
        <title>Small-multiple of a series of Plutchik’s wheel built from synthetic data.</title>
        <p>Polar coordinates beneath the flowers and labels around have been hidden to improve the immediate readability of the flowers, resulting in a collection of emotional fingerprints of different corpora.</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g006" position="float"/>
    </fig>
    <p>As a further customisation option, we allow the user to select a set of petals to be highlighted. This selective presentation feature follows a focus-plus-context [<xref rid="pone.0256503.ref076" ref-type="bibr">76</xref>] approach to the need of emphasising those emotions that might be more distinctive, according to the case under consideration. We chose to apply a focus-plus-context visualisation by filling petals’ areas selectively, without adopting other common techniques, as with fish-eye views [<xref rid="pone.0256503.ref077" ref-type="bibr">77</xref>], in order to avoid distortions and to preserve the spatial relations between the petals. It is possible to select a list of main emotions, and to show a colored petal and all three intensity scores for each emotion in the list, while for the others it will display a grey petal and the cumulative scores only, also in grey. We showcase this feature in <xref rid="pone.0256503.g007" ref-type="fig">Fig 7</xref> and in the code documentation.</p>
    <fig position="float" id="pone.0256503.g007">
      <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g007</object-id>
      <label>Fig 7</label>
      <caption>
        <title>A side-by-side comparison between the synthetic plot of <xref rid="pone.0256503.g001" ref-type="fig">Fig 1(iii)</xref> and an almost identical wheel, but with only two emotions highlighted.</title>
        <p>We highlighted and displayed the three intensity scores of <italic toggle="yes">Anticipation</italic> and <italic toggle="yes">Joy</italic>.</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g007" position="float"/>
    </fig>
  </sec>
  <sec id="sec007">
    <title>4 Showing dyads with PyPlutchik</title>
    <p>Dyads are a crucial feature in Plutchik’s model. As explained in Sec. 1, the high flexibility of the model derives also from the spatial disposition of the emotions. Primary emotions can combine with their direct neighbours, forming primary dyads, or with emotions that are two or three petals away, forming respectively secondary and tertiary dyads. Opposite dyads can be formed as well, by combining emotions belonging to opposite petals. This feature dramatically enriches the spectrum of emotions, beyond the primary ones; enriching the spectrum increases the complexity of the models, also contributing to mitigate, at least partially, the phenomenon of cultural variation discussed in Sec. 1. Therefore, a comprehensive visualisation of Plutchik’s model must offer a way to visualise dyads.</p>
    <p>The design of such a feature is non trivial. Indeed, while the flower of primary emotions is inherent to the model itself, no standard design is provided to visualise dyads. For our implementation we decided to stick with the flower-shaped graphics, in order not to deviate too much from the original visualisation philosophy. Examples that show all levels of dyads can be seen in Figs 14 and 15. While the core of the visual remains the same, a few modifications are introduced. In more detail:</p>
    <list list-type="bullet">
      <list-item>
        <p>the radial axes are progressively rotated by 45 degrees in each level, to enhance the spatial shift from primary emotions to dyads;</p>
      </list-item>
      <list-item>
        <p>the petals are two-tone, according to the colours of the primary emotions that define each dyad;</p>
      </list-item>
      <list-item>
        <p>a textual annotation in the center gives an indication of what kind of dyad is represented: “1” for primary dyads, “2” for secondary dyads, “3” for tertiary dyads, “opp.” for opposite dyads.</p>
      </list-item>
      <list-item>
        <p>while the dyads labels all come in the same colour (default is black), an additional circular layer has been added in order to visualise the labels and the colours of the primary emotions that define each dyad.</p>
      </list-item>
    </list>
    <p>This last feature is particularly useful to give the user an immediate sense of the primary emotions involved in the formation of the dyad. <xref rid="pone.0256503.g008" ref-type="fig">Fig 8</xref> provides an example of the wheel produced if the user inputs a <italic toggle="yes">dict</italic> containing primary dyads instead of emotions. PyPlutchik automatically checks for the kind of input wheel and for its coherence: specifically, the module retrieves an error if the input dictionary contains a mix of emotions from different kind of dyads, as they cannot be displayed on the same plot. In <xref rid="pone.0256503.g009" ref-type="fig">Fig 9</xref> we show a representation of basic emotions, primary dyads, secondary dyads, tertiary dyads and opposite dyads, based on synthetic data. This representation easily conveys the full spectrum of emotions and their combinations according to Plutchik’s model, allowing for a quick but in-depth analysis of emotions detected in a corpus.</p>
    <fig position="float" id="pone.0256503.g008">
      <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g008</object-id>
      <label>Fig 8</label>
      <caption>
        <title>Primary dyads’ wheel generated by code on the right.</title>
        <p>Each entry in the Python <italic toggle="yes">dict</italic> is a numeric value ∈ [0, 1].</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g008" position="float"/>
    </fig>
    <fig position="float" id="pone.0256503.g009">
      <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g009</object-id>
      <label>Fig 9</label>
      <caption>
        <title>Representation of emotions and primary, secondary, tertiary and opposite dyads.</title>
        <p>The data displayed is random.</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g009" position="float"/>
    </fig>
  </sec>
  <sec id="sec008">
    <title>5 Case studies</title>
    <p>We now showcase some useful examples of data visualisation using PyPlutchik. We argue that PyPlutchik is more suitable than any other graphical tool to narrate the stories of these examples, because it is the natural conversion of the original qualitative model to a quantitative akin, tailored to visually represent occurrences of emotions and dyads in an annotated corpus.</p>
    <sec id="sec009">
      <title>5.1 Amazon office products reviews</title>
      <p>As a further use case we exploit a dataset of products review on Amazon [<xref rid="pone.0256503.ref078" ref-type="bibr">78</xref>]. This dataset contains almost 142.8 millions reviews spanning May 1996—July 2014. Products are rated by the customers on a 1-5 stars scale, along with a textual review. Emotions in these textual reviews have been annotated using the Python library NRCLex [<xref rid="pone.0256503.ref079" ref-type="bibr">79</xref>], which checks the text against a lexicon for word-emotion associations; we do not have any ambition of scientific accuracy of the results, as this example is meant for showcasing our visualisation layouts.</p>
      <p>In <xref rid="pone.0256503.g010" ref-type="fig">Fig 10</xref> we plot the average emotion scores in a sample of reviews of office products, grouped by the star-ratings. We can sense a trend: moving from left to right, i.e. from low-rates to high-rates products, we see the petals in the top half of the flower slowly growing in size at the expense of the bottom half petals. The decreasing effect is particularly visible in <italic toggle="yes">Fear</italic>, <italic toggle="yes">Anger</italic> and <italic toggle="yes">Disgust</italic>.</p>
      <fig position="float" id="pone.0256503.g010">
        <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g010</object-id>
        <label>Fig 10</label>
        <caption>
          <title>Average emotion scores in a sample of textual reviews of office products on Amazon.</title>
          <p>Rating of products goes from one star (worst) to five (best). On the left, emotions detected in negative reviews (one star), on the right the emotions detected in positive reviews (five star). While positive emotions stay roughly the same, negative emotions such <italic toggle="yes">Anger</italic>, <italic toggle="yes">Disgust</italic> and <italic toggle="yes">Fear</italic> substantially drop as the ratings get higher.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g010" position="float"/>
      </fig>
      <p>This visualisation is effective in communicating the increasing satisfaction of the customers; nevertheless, this improvement is very gradual and can hardly be noticed by comparing to subsequent steps. As we can see from <xref rid="pone.0256503.g011" ref-type="fig">Fig 11(a)</xref>, it is much more evident if we compare one-star-rated products to five-star-rated product reviews. The selective presentation feature of our module (<xref rid="pone.0256503.g011" ref-type="fig">Fig 11(b)</xref>) is a good way to enhance this result: it allows to put emphasis on the desired emotions without losing sight of the others, that are left untouched in their size or shape but are overshadowed, deprived of their coloured fill.</p>
      <fig position="float" id="pone.0256503.g011">
        <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g011</object-id>
        <label>Fig 11</label>
        <caption>
          <title>Focus-plus-context: The selective presentation feature of PyPlutchik allows to put emphasis on some particular emotions, without losing sight of the others; we can compare different subgroups of the same Amazon corpus placing our visualisations side-by-side, and highlighting only <italic toggle="yes">Anger</italic>, <italic toggle="yes">Disgust</italic> and <italic toggle="yes">Fear</italic> petals, to easily spot how these negative emotions are under represented in 5-stars reviews than in 1-star reviews.</title>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g011" position="float"/>
      </fig>
    </sec>
    <sec id="sec010">
      <title>5.2 Emotions in IMDB movie synopses</title>
      <p>In <xref rid="pone.0256503.g012" ref-type="fig">Fig 12</xref> is shown the emotion detected in the short synopses from the top 1000 movies on the popular website IMDB (Internet Movie Data Base). Data is an excerpt of only four genres (namely Romance, Biography, Mystery and Animation) taken from Kaggle [<xref rid="pone.0256503.ref080" ref-type="bibr">80</xref>], and emotions have been annotated again with the Python library NRCLex. As in the previous case, both the dataset and the methodology are flawed for the task: for instance the synopsis of the movie may describe a summary of the main events or of the characters, but with detachment; the library lexicon may not be suited for the movie language domain. However, data here is presented for visualisation purposes only, and not intended as a contribution in the NLP area.</p>
      <fig position="float" id="pone.0256503.g012">
        <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g012</object-id>
        <label>Fig 12</label>
        <caption>
          <title>Emotions in the synopses of the top 1000 movies in the IMDB database, divided by four genres.</title>
          <p>The shapes are immediately distinct from each other, and they return an intuitive graphical representation of each genre’s peculiarities.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g012" position="float"/>
      </fig>
      <p>Romance shows a slight prominence of positive emotions over negative ones, especially over <italic toggle="yes">Disgust</italic>. The figure aside represents the Biography genre, and it is immediately distinctive for the high <italic toggle="yes">Trust</italic> score, other than higher <italic toggle="yes">Fear</italic>, <italic toggle="yes">Sadness</italic> and <italic toggle="yes">Anger</italic> scores. While high <italic toggle="yes">Trust</italic> represents the high admiration for the subject of the biopic, the other scores are in line with Propp’s narration scheme [<xref rid="pone.0256503.ref081" ref-type="bibr">81</xref>], where the initial equilibrium is threatened by a menace the hero is called to solve. A fortiori, Mystery’s genre conveys even more <italic toggle="yes">Anger</italic> and more <italic toggle="yes">Sadness</italic> than Biography, coupled with a higher sense of <italic toggle="yes">Anticipation</italic> and a very high score for <italic toggle="yes">Fear</italic>, as expected. Last, the Animation genre arouses many emotions, both positive and negative, with high levels of <italic toggle="yes">Joy</italic>, <italic toggle="yes">Fear</italic>, <italic toggle="yes">Anticipation</italic> and <italic toggle="yes">Surprise</italic>, as a children cartoon is probably supposed to do. Printed together, these four shapes are immediately distinct from each other, and they return an intuitive graphical representation of each genre’s peculiarities. Shapes are easily recognisable as positive or negative, bigger petals are predominant and petals’ sizes are easy to compare with the aid of the thin grid behind them.</p>
      <p>Data represented in <xref rid="pone.0256503.g012" ref-type="fig">Fig 12</xref> is a larger excerpt of the same IMDB dataset, which covers 21 genres. The whole dataset gives us the chance to show a small-multiple representation without visible coordinates, as described in Sect. 3: we plotted in <xref rid="pone.0256503.g013" ref-type="fig">Fig 13</xref> the most common 20 genres of movies within the top 1000, 5 by row. We hid the grid and the labels, leaving the flower to speak for itself. Data represented this way is not intended to be read with exactness on numbers. Instead, it is intended to be read as an overall overview on the corpus. Peculiarities, outliers and one-of-a-kind shapes catch the eye immediately, and they can be accurately scrutinised later with a dedicated plot that zooms into the details. For instance, the Film-Noir genre contains only a handful of movies, whose synopses are almost always annotated as emotion-heavy. The resulting shape is a clear outlier in this corpus, with extremely high scores on 5 of 8 emotions. Thrillers and Action movies share a similar emotion distribution, while Music and Musical classify for the happiest.</p>
      <fig position="float" id="pone.0256503.g013">
        <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g013</object-id>
        <label>Fig 13</label>
        <caption>
          <title>Emotions in the synopses of the 20 most common movie genres in the IMDB database.</title>
          <p>Coordinates, grids and labels are not visible: this is an overall view of the corpus, meant to showcase general trends and to spot outliers that can be analysed at a later stage, in dedicated plot.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g013" position="float"/>
      </fig>
    </sec>
    <sec id="sec011">
      <title>5.3 Trump or Clinton?</title>
      <p>In Figs <xref rid="pone.0256503.g014" ref-type="fig">14</xref> and <xref rid="pone.0256503.g015" ref-type="fig">15</xref> we visualise the basic emotions and dyads found in tweets in favour and against Donald Trump and Hillary Clinton, the 2016 United States Presidential Elections principal candidates. Data is the training set released for a SemEval 2016 task, namely a corpus of annotated stances, sentiments and emotions in tweets [<xref rid="pone.0256503.ref082" ref-type="bibr">82</xref>]. Each candidate is represented in both plots on a different row, and each row displays five subplots, respectively basic emotions, primary dyads, secondary dyads, tertiary dyads and opposite dyads. Tweets supporting either Trump or Clinton present higher amounts of positive emotions (<xref rid="pone.0256503.g014" ref-type="fig">Fig 14(i) and 14(vi)</xref>), namely <italic toggle="yes">Anticipation</italic>, <italic toggle="yes">Joy</italic> and <italic toggle="yes">Trust</italic>, and from lower to no amounts of negative emotions, especially <italic toggle="yes">Sadness</italic> and <italic toggle="yes">Disgust</italic>. On the contrary, tweets critical of each candidate (<xref rid="pone.0256503.g015" ref-type="fig">Fig 15(i) and 15(vi)</xref>) show high values of <italic toggle="yes">Anger</italic>, coupled with <italic toggle="yes">Disgust</italic>, probably in the form of disapproval.</p>
      <fig position="float" id="pone.0256503.g014">
        <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g014</object-id>
        <label>Fig 14</label>
        <caption>
          <title>Tweets in favour of Donald Trump and Hillary Clinton from the 2016 StanceDetection task in SemEval.</title>
          <p>From left to right: basic emotions, primary dyads, secondary dyads, tertiary dyads and opposite dyads for both candidates (Donald Trump on the first row, Hillary Clinton on the second one). Despite the high amounts of <italic toggle="yes">Anticipation</italic>, <italic toggle="yes">Joy</italic> and <italic toggle="yes">Trust</italic> for both the candidates, which result in similar primary dyads, there is a significant spike on the secondary dyad <italic toggle="yes">Hope</italic> among Trump’s supporters that is not present in Clinton’s supporters.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g014" position="float"/>
      </fig>
      <fig position="float" id="pone.0256503.g015">
        <object-id pub-id-type="doi">10.1371/journal.pone.0256503.g015</object-id>
        <label>Fig 15</label>
        <caption>
          <title>Similarly to <xref rid="pone.0256503.g014" ref-type="fig">Fig 14</xref>, here are shown the emotions captured in the tweets against Donald Trump and Hillary Clinton from the 2016 StanceDetection task in SemEval.</title>
          <p>We see a clear prevalence of negative emotions, particularly <italic toggle="yes">Anger</italic> and <italic toggle="yes">Disgust</italic>. This combination is often expressed together, as can be seen from the primary emotions plots (ii and vii), where there is a spike in <italic toggle="yes">Contempt</italic>.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.g015" position="float"/>
      </fig>
      <p>There are also significant differences between the two candidates. Donald Trump collects higher levels of <italic toggle="yes">Trust</italic> and <italic toggle="yes">Anticipation</italic> from his supporters than Hillary Clinton, possibly meaning higher expectations from his electoral base. Users that are skeptical of Hillary Clinton show more <italic toggle="yes">Disgust</italic> towards her than Donald Trump’s opponents towards him.</p>
      <p>Besides basic emotions, PyPlutchik can display the distribution of dyads as well, as described in Sec. 4. Dyads allow for a deeper understanding of the data. We can see how the tweets against the presidential candidates in <xref rid="pone.0256503.g015" ref-type="fig">Fig 15</xref> are dominated by the negative basic emotion of <italic toggle="yes">Anger</italic>, with an important presence of <italic toggle="yes">Disgust</italic> and <italic toggle="yes">Anticipation</italic> (subplots (i) and (vi)); the dominant primary dyad is therefore the co-occurence of <italic toggle="yes">Anger</italic> and <italic toggle="yes">Disgust</italic> (subplot (ii)), i.e. the primary dyad <italic toggle="yes">Contempt</italic>, but not <italic toggle="yes">Aggressiveness</italic>, the primary dyad formed by <italic toggle="yes">Anger</italic> and <italic toggle="yes">Anticipation</italic>: the latter rarely co-occurs with the other two, which means that expectations and contempt are two independent drives in such tweets. The other dyads are relatively scarcer as we progress on the secondary and tertiary level (subplots (iii)-(v)). The supporting tweets in <xref rid="pone.0256503.g014" ref-type="fig">Fig 14</xref> are characterised by positive emotions, both in the primary flower and in the dyads, with these again being a reflection of the co-occurrence of the most popular primary emotions. Although <italic toggle="yes">Anticipation</italic>, <italic toggle="yes">Joy</italic> and <italic toggle="yes">Trust</italic> are present in different amounts, primary dyads <italic toggle="yes">Optimism</italic> and <italic toggle="yes">Love</italic> do occur in a comparable number of cases (subplot (ii)). Interestingly, the pro-Trump tweets show a remarkable quantity of <italic toggle="yes">Hope</italic>, the secondary dyad that combines <italic toggle="yes">Anticipation</italic> and <italic toggle="yes">Trust</italic>, suggesting that Trump’s supporters expressed towards him all the three dominant basic emotions together more often that Clinton’s supporters did.</p>
      <p>Generally speaking, we notice that there are not many dyads expressed in the tweets. We can ascribe this to many factors: first and foremost, a dataset annotated <italic toggle="yes">only</italic> on primary emotions and not also explicitly on the dyads will naturally show less dyads, since their presence will depend only on the casual co-occurrence of primary emotions. For this reason we resized the petal lengths in <xref rid="pone.0256503.g014" ref-type="fig">Fig 14(ii)–14(v) and 14(vii)–14(x)</xref> with a different scale than (i) and (v), and the same applies for <xref rid="pone.0256503.g015" ref-type="fig">Fig 15</xref>. For instance, <italic toggle="yes">Trust</italic> petal in <xref rid="pone.0256503.g014" ref-type="fig">Fig 14(i)</xref> is almost to the maximum length, with a score equal to 0.63; <italic toggle="yes">Hope</italic> petal in (iii) is also almost at the maximum length, but with a score equal to 0.37. Both figures are therefore designed for a visual comparison of the petals within the same subplot, but not between two different subplots, due to the lack of consistency in the scale. However, this does not concern us, since our current purpose is to showcase the potential of PyPlutchik: with this regard, we note that we were immediately able to track the unexpected presence of the secondary dyad <italic toggle="yes">Hope</italic>, that stood out among the others.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec012">
    <title>6 Conclusion</title>
    <p>The increasing production of studies that explore the emotion patterns of human-produced texts often requires dedicated data visualisation techniques. This is particularly true for those studies that label emotions according to a model such as Plutchik’s one, which is heavily based on the principles of semantic proximity and opposition between pairs of emotions. Indeed, the so-called <italic toggle="yes">Plutchik’s wheel</italic> is inherent to the definition of the model itself, as it provides the perfect visual metaphor that best explains this theoretical framework.</p>
    <p>Nonetheless, by checking the most recent literature it appears evident that too often this aspect is neglected, instead entrusting the visual representation of the results to standard, sub-optimal solutions such as bar charts, tables, pie charts. We believe that this choice does not do justice neither to the said studies, nor to Plutchik’s model itself, and it is mainly due to the lack of an easy, plug-and-play tool to create adequate visuals.</p>
    <p>With this in mind we introduced PyPlutchik, a Python module for the correct visualisation of Plutchik’s emotion traces in texts and corpora. PyPlutchik fills the gap of a suitable tool specifically designed for representing Plutchik’s model of emotions. Most importantly, it goes beyond the mere qualitative display of the Plutchik’s wheel, that lacks a quantitative dimension, allowing the user to display also the quantities related to the emotions detected in the text or in the corpora. Moreover, PyPlutchik goes the extra mile by implementing a new way of visualising primary, secondary, tertiary and opposite dyads, whose presence is a distinctive and essential feature of the Plutchik’s model of emotions.</p>
    <p>This module is built on top of the popular Python library matplotlib, its APIs being written in a matplotlib style. PyPlutchik is designed for an easy plug and play of JSON files, and it is entirely scriptable. It is designed for single plots, pair-wise and group-wise side-by-side comparisons, and small-multiples representations. The original Plutchik’s wheel of emotions displace the 8 basic emotions according to proximity and opposition principles; the same principles are respected in PyPlutchik layout.</p>
    <p>It should be noted that the PyPlutchik module is meant to work only with data that was classified—manually or automatically—according to the Plutchik’s model of emotions. In general, it is never advisable to represent data on a different scheme from the one used for its classification. Most of the models are radically different from each other, both in the underlying assumptions as well as in the categories scheme; even when they are somewhat similar, there might be key differences that should be kept into account. As for the Plutchik model in particular, to the best of our knowledge it does not exist a one-to-one correspondence with any other discrete emotional model, as well as no methods to go from a continuous emotional space to a discrete one. For these reasons, PyPlutchik should be used only with data annotated following the Plutchik’s scheme of annotation.</p>
    <p>As we pointed out, currently there are thousands of empirical works on Plutchik’s model of emotions. Many of these works need a correct representation of the emotions detected or annotated in data. It is our hope that our module will help the scientific community by providing them with an alternative to the sub-optimal representations of Plutchik’s emotion currently available in literature.</p>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pone.0256503.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Ekman</surname><given-names>P</given-names></name>. <article-title>An argument for basic emotions</article-title>. <source><italic toggle="yes">Cognition &amp; emotion</italic></source>, <volume>6</volume>(<issue>3-4</issue>):<fpage>169</fpage>–<lpage>200</lpage>, <year>1992</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/02699939208411068</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref002">
      <label>2</label>
      <mixed-citation publication-type="book"><name><surname>James</surname><given-names>W</given-names></name>. <source><italic toggle="yes">The principles of psychology</italic></source>, <volume>vol. 1</volume>. <publisher-name>Cosimo, Inc</publisher-name>., <year>2007</year>.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Izard</surname><given-names>C. E.</given-names></name>, <name><surname>Libero</surname><given-names>D. Z.</given-names></name>, <name><surname>Putnam</surname><given-names>P.</given-names></name>, and <name><surname>Haynes</surname><given-names>O. M</given-names></name>. <article-title>Stability of emotion experiences and their relations to traits of personality</article-title>. <source><italic toggle="yes">Journal of personality and social psychology</italic></source>, <volume>64</volume>(<issue>5</issue>):<fpage>847</fpage>, <year>1993</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/0022-3514.64.5.847</pub-id><?supplied-pmid 8505713?><pub-id pub-id-type="pmid">8505713</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref004">
      <label>4</label>
      <mixed-citation publication-type="book"><name><surname>Lazarus</surname><given-names>R. S.</given-names></name> and <name><surname>Lazarus</surname><given-names>B. N</given-names></name>. <source><italic toggle="yes">Passion and reason: Making sense of our emotions</italic></source>. <publisher-name>Oxford University Press</publisher-name>, <publisher-loc>USA</publisher-loc>, <year>1994</year>.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Watson</surname><given-names>D.</given-names></name> and <name><surname>Tellegen</surname><given-names>A</given-names></name>. <article-title>Toward a consensual structure of mood</article-title>. <source><italic toggle="yes">Psychological bulletin</italic></source>, <volume>98</volume>(<issue>2</issue>):<fpage>219</fpage>, <year>1985</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/0033-2909.98.2.219</pub-id>
<?supplied-pmid 3901060?><pub-id pub-id-type="pmid">3901060</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref006">
      <label>6</label>
      <mixed-citation publication-type="book"><name><surname>Mehrabian</surname><given-names>A</given-names></name>. <source><italic toggle="yes">Basic dimensions for a general psychological theory: Implications for personality</italic>, <italic toggle="yes">social</italic>, <italic toggle="yes">environmental</italic>, <italic toggle="yes">and developmental studies</italic></source>, <volume>vol. 2</volume>. <publisher-name>Oelgeschlager, Gunn &amp; Hain</publisher-name><publisher-loc>Cambridge, MA</publisher-loc>, <year>1980</year>.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Russell</surname><given-names>J. A</given-names></name>. <article-title>A circumplex model of affect</article-title>. <source><italic toggle="yes">Journal of personality and social psychology</italic></source>, <volume>39</volume>(<issue>6</issue>):<fpage>1161</fpage>, <year>1980</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/h0077714</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Schlosberg</surname><given-names>H</given-names></name>. <article-title>The description of facial expressions in terms of two dimensions</article-title>. <source>Journal of experimental psychology</source>. <year>1952</year>;<volume>44</volume>(<issue>4</issue>):<fpage>229</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/h0055778</pub-id><?supplied-pmid 13000062?><pub-id pub-id-type="pmid">13000062</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref009">
      <label>9</label>
      <mixed-citation publication-type="other">Mohammad SM. Practical and ethical considerations in the effective use of emotion and sentiment lexicons. arXiv preprint arXiv:201103492. 2020.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Jackson</surname><given-names>JC</given-names></name>, <name><surname>Watts</surname><given-names>J</given-names></name>, <name><surname>Henry</surname><given-names>TR</given-names></name>, <name><surname>List</surname><given-names>JM</given-names></name>, <name><surname>Forkel</surname><given-names>R</given-names></name>, <name><surname>Mucha</surname><given-names>PJ</given-names></name>, <etal>et al</etal>. <article-title>Emotion semantics show both cultural variation and universal structure</article-title>. <source>Science</source>. <year>2019</year>;<volume>366</volume>(<issue>6472</issue>):<fpage>1517</fpage>–<lpage>1522</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.aaw8160</pub-id><?supplied-pmid 31857485?><pub-id pub-id-type="pmid">31857485</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Scherer</surname><given-names>KR</given-names></name>, <name><surname>Wallbott</surname><given-names>HG</given-names></name>. <article-title>Evidence for universality and cultural variation of differential emotion response patterning</article-title>. <source>Journal of personality and social psychology</source>. <year>1994</year>;<volume>66</volume>(<issue>2</issue>):<fpage>310</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1037/0022-3514.66.2.310</pub-id><?supplied-pmid 8195988?><pub-id pub-id-type="pmid">8195988</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Plutchik</surname><given-names>R</given-names></name>. <article-title>The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice</article-title>. <source><italic toggle="yes">American scientist</italic></source>, <volume>89</volume>(<issue>4</issue>):<fpage>344</fpage>–<lpage>350</lpage>, <year>2001</year>.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Hunter</surname><given-names>J. D</given-names></name>. <article-title>Matplotlib: A 2d graphics environment</article-title>. <source><italic toggle="yes">Computing in Science &amp; Engineering</italic></source>, <volume>9</volume>(<issue>3</issue>):<fpage>90</fpage>–<lpage>95</lpage>, <year>2007</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref014">
      <label>14</label>
      <mixed-citation publication-type="book"><name><surname>Meirelles</surname><given-names>I</given-names></name>. <source><italic toggle="yes">Design for information: an introduction to the histories</italic>, <italic toggle="yes">theories</italic>, <italic toggle="yes">and best practices behind effective information visualizations</italic></source>. <publisher-name>Rockport publishers</publisher-name>, <year>2013</year>.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Bradley</surname><given-names>A. J.</given-names></name>, <name><surname>El-Assady</surname><given-names>M.</given-names></name>, <name><surname>Coles</surname><given-names>K.</given-names></name>, <name><surname>Alexander</surname><given-names>E.</given-names></name>, <name><surname>Chen</surname><given-names>M.</given-names></name>, <name><surname>Collins</surname><given-names>C.</given-names></name>, <etal>et al</etal>. <article-title>Visualization and the digital humanities</article-title>. <source><italic toggle="yes">IEEE computer graphics and applications</italic></source>, <volume>38</volume>(<issue>6</issue>):<fpage>26</fpage>–<lpage>38</lpage>, <year>2018</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/MCG.2018.2878900</pub-id><?supplied-pmid 30668453?><pub-id pub-id-type="pmid">30668453</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Wu</surname><given-names>Y.</given-names></name>, <name><surname>Cao</surname><given-names>N.</given-names></name>, <name><surname>Gotz</surname><given-names>D.</given-names></name>, <name><surname>Tan</surname><given-names>Y.-P.</given-names></name>, and <name><surname>Keim</surname><given-names>D. A</given-names></name>. <article-title>A survey on visual analytics of social media data</article-title>. <source><italic toggle="yes">IEEE Transactions on Multimedia</italic></source>, <volume>18</volume>(<issue>11</issue>):<fpage>2135</fpage>–<lpage>2148</lpage>, <year>2016</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TMM.2016.2614220</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>van Ham</surname><given-names>F.</given-names></name>, <name><surname>Wattenberg</surname><given-names>M.</given-names></name>, and <name><surname>Viégas</surname><given-names>F. B</given-names></name>. <article-title>Mapping text with phrase nets</article-title>. <source><italic toggle="yes">IEEE Trans. Vis. Comput. Graph</italic>.</source>, <volume>15</volume>(<issue>6</issue>):<fpage>1169</fpage>–<lpage>1176</lpage>, <year>2009</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TVCG.2009.165</pub-id><?supplied-pmid 19834186?><pub-id pub-id-type="pmid">19834186</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Wattenberg</surname><given-names>M.</given-names></name> and <name><surname>Viégas</surname><given-names>F. B</given-names></name>. <article-title>The word tree, an interactive visual concordance</article-title>. <source><italic toggle="yes">IEEE Transactions on Visualization and Computer Graphics</italic></source>, <volume>14</volume>(<issue>6</issue>):<fpage>1221</fpage>–<lpage>1228</lpage>, <month>Nov</month>. <year>2008</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TVCG.2008.172</pub-id>
<?supplied-pmid 18988967?><pub-id pub-id-type="pmid">18988967</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref019">
      <label>19</label>
      <mixed-citation publication-type="other">F. B. Viégas, S. Golder, and J. Donath. Visualizing email content: Portraying relationships from conversational histories. In <italic toggle="yes">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</italic>, CHI’06, pp. 979–988. ACM, New York, NY, USA, 2006. <pub-id pub-id-type="doi">10.1145/1124772.1124919</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Viegas</surname><given-names>F. B.</given-names></name>, <name><surname>Wattenberg</surname><given-names>M.</given-names></name>, <name><surname>van Ham</surname><given-names>F.</given-names></name><name><surname>Kriss</surname><given-names>J.</given-names></name>, and <name><surname>McKeon</surname><given-names>M</given-names></name>. <article-title>ManyEyes: A site for visualization at internet scale</article-title>. <source><italic toggle="yes">IEEE Transactions on Visualization and Computer Graphics</italic></source>, <volume>13</volume>(<issue>6</issue>):<fpage>1121</fpage>–<lpage>1128</lpage>, <month>Nov</month>. <year>2007</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TVCG.2007.70577</pub-id><?supplied-pmid 17968055?><pub-id pub-id-type="pmid">17968055</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref021">
      <label>21</label>
      <mixed-citation publication-type="other">Inc PT. Collaborative data science; 2015. Available from: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://plot.ly" ext-link-type="uri">https://plot.ly</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref022">
      <label>22</label>
      <mixed-citation publication-type="other">Bokeh Development Team. Bokeh: Python library for interactive visualization; 2014. Available from: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.bokeh.pydata.org" ext-link-type="uri">http://www.bokeh.pydata.org</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Bostock</surname><given-names>M</given-names></name>, <name><surname>Ogievetsky</surname><given-names>V</given-names></name>, <name><surname>Heer</surname><given-names>J</given-names></name>. <article-title>D<sup>3</sup> data-driven documents</article-title>. <source>IEEE transactions on visualization and computer graphics</source>. <year>2011</year>;<volume>17</volume>(<issue>12</issue>):<fpage>2301</fpage>–<lpage>2309</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TVCG.2011.185</pub-id><?supplied-pmid 22034350?><pub-id pub-id-type="pmid">22034350</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Dou</surname><given-names>W.</given-names></name> and <name><surname>Liu</surname><given-names>S</given-names></name>. <article-title>Topic-and time-oriented visual text analysis</article-title>. <source><italic toggle="yes">IEEE computer graphics and applications</italic></source>, <volume>36</volume>(<issue>4</issue>):<fpage>8</fpage>–<lpage>13</lpage>, <year>2016</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/MCG.2016.73</pub-id>
<?supplied-pmid 27514029?><pub-id pub-id-type="pmid">27514029</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Cui</surname><given-names>W.</given-names></name>, <name><surname>Liu</surname><given-names>S.</given-names></name>, <name><surname>Wu</surname><given-names>Z.</given-names></name>, and <name><surname>Wei</surname><given-names>H</given-names></name>. <article-title>How hierarchical topics evolve in large text corpora</article-title>. <source><italic toggle="yes">IEEE transactions on visualization and computer graphics</italic></source>, <volume>20</volume>(<issue>12</issue>):<fpage>2281</fpage>–<lpage>2290</lpage>, <year>2014</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TVCG.2014.2346433</pub-id><?supplied-pmid 26356942?><pub-id pub-id-type="pmid">26356942</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>S.</given-names></name>, <name><surname>Zhou</surname><given-names>M. X.</given-names></name>, <name><surname>Pan</surname><given-names>S.</given-names></name>, <name><surname>Song</surname><given-names>Y.</given-names></name>, <name><surname>Qian</surname><given-names>W.</given-names></name>, <name><surname>Cai</surname><given-names>W.</given-names></name>, <etal>et al</etal>. <article-title>Tiara: Interactive, topic-based visual text summarization and analysis</article-title>. <source><italic toggle="yes">ACM Transactions on Intelligent Systems and Technology (TIST)</italic></source>, <volume>3</volume>(<issue>2</issue>):<fpage>1</fpage>–<lpage>28</lpage>, <year>2012</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/2089094.2089101</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>S.</given-names></name>, <name><surname>Yin</surname><given-names>J.</given-names></name>, <name><surname>Wang</surname><given-names>X.</given-names></name>, <name><surname>Cui</surname><given-names>W.</given-names></name>, <name><surname>Cao</surname><given-names>K.</given-names></name>, and <name><surname>Pei</surname><given-names>J</given-names></name>. <article-title>Online visual analytics of text streams</article-title>. <source><italic toggle="yes">IEEE transactions on visualization and computer graphics</italic></source>, <volume>22</volume>(<issue>11</issue>):<fpage>2451</fpage>–<lpage>2466</lpage>, <year>2015</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TVCG.2015.2509990</pub-id><?supplied-pmid 26701787?><pub-id pub-id-type="pmid">26701787</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Cui</surname><given-names>W.</given-names></name>, <name><surname>Liu</surname><given-names>S.</given-names></name>, <name><surname>Tan</surname><given-names>L.</given-names></name>, <name><surname>Shi</surname><given-names>C.</given-names></name>, <name><surname>Song</surname><given-names>Y.</given-names></name>, <name><surname>Gao</surname><given-names>Z.</given-names></name>, <etal>et al</etal>. <article-title>Textflow: Towards better understanding of evolving topics in text</article-title>. <source><italic toggle="yes">IEEE transactions on visualization and computer graphics</italic></source>, <volume>17</volume>(<issue>12</issue>):<fpage>2412</fpage>–<lpage>2421</lpage>, <year>2011</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TVCG.2011.239</pub-id><?supplied-pmid 22034362?><pub-id pub-id-type="pmid">22034362</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref029">
      <label>29</label>
      <mixed-citation publication-type="book"><name><surname>Kucher</surname><given-names>K.</given-names></name>, <name><surname>Paradis</surname><given-names>C.</given-names></name>, and <name><surname>Kerren</surname><given-names>A</given-names></name>. <part-title>The state of the art in sentiment visualization</part-title>. In <source><italic toggle="yes">Computer Graphics Forum</italic></source>, <volume>vol. 37</volume>, pp. <fpage>71</fpage>–<lpage>96</lpage>. <publisher-name>Wiley Online Library</publisher-name>, <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref030">
      <label>30</label>
      <mixed-citation publication-type="other">A. T. Capozzi, V. Patti, G. Ruffo, and C. Bosco. A data viz platform as a support to study, analyze and understand the hate speech phenomenon. In <italic toggle="yes">Proceedings of the 2nd International Conference on Web Studies</italic>, pp. 28–35, 2018.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref031">
      <label>31</label>
      <mixed-citation publication-type="other">J. Chamberlain, U. Kruschwitz, and O. Hoeber. Scalable visualisation of sentiment and stance. In <italic toggle="yes">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</italic>, 2018.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref032">
      <label>32</label>
      <mixed-citation publication-type="other">K. Roberts, M. Roach, J. Johnson, J. Guthrie, and S. Harabagiu. Empatweet: Annotating and detecting emotions on Twitter. <italic toggle="yes">Proc. Language Resources and Evaluation Conf</italic>, 01 2012.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref033">
      <label>33</label>
      <mixed-citation publication-type="other">W. Wang, L. Chen, K. Thirunarayan, and A. P. Sheth. Harnessing Twitter “big data” for automatic emotion identification. In <italic toggle="yes">2012 International Conference on Privacy</italic>, <italic toggle="yes">SeCurity</italic>, <italic toggle="yes">Risk and Trust and 2012 International Conference on Social Computing</italic>, pp. 587–592, 2012. <pub-id pub-id-type="doi">10.1109/SocialCom-PASSAT.2012.119</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>CBalabantaray</surname><given-names>R.</given-names></name>, <name><surname>mohd</surname><given-names>M.</given-names></name>, and <name><surname>Sharma</surname><given-names>N</given-names></name>. <article-title>Multi-class Twitter emotion classification: A new approach</article-title>. <source><italic toggle="yes">International Journal of Applied Information Systems</italic></source>, <volume>4</volume>:<fpage>48</fpage>–<lpage>53</lpage>, <day>09</day><year>2012</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.5120/ijais12-450651</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref035">
      <label>35</label>
      <mixed-citation publication-type="other">M. Stella, M. S. Vitevitch, and F. Botta. Cognitive networks identify the content of english and Italian popular posts about covid-19 vaccines: Anticipation, logistics, conspiracy and loss of trust, 2021.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref036">
      <label>36</label>
      <mixed-citation publication-type="other">D. Preoţiuc-Pietro, H. Schwartz, G. Park, J. Eichstaedt, M. Kern, L. Ungar, et al. Modelling valence and arousal in Facebook posts. pp. 9–15, 01 2016. <pub-id pub-id-type="doi">10.18653/v1/W16-0404</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref037">
      <label>37</label>
      <mixed-citation publication-type="other">M. Burke and M. Develin. Once more with feeling: Supportive responses to social sharing on Facebook. In <italic toggle="yes">Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work &amp; Social Computing</italic>, CSCW’16, p. 1462–1474. Association for Computing Machinery, New York, NY, USA, 2016. <pub-id pub-id-type="doi">10.1145/2818048.2835199</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref038">
      <label>38</label>
      <mixed-citation publication-type="other">F. Rangel, D. I. H. Farías, P. Rosso, and A. Reyes. Emotions and irony per gender in Facebook. 2014.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref039">
      <label>39</label>
      <mixed-citation publication-type="other">E. Kim and R. Klinger. Who feels what and why? annotation of a literature corpus with semantic roles of emotions. In <italic toggle="yes">Proceedings of the 27th International Conference on Computational Linguistics</italic>, pp. 1345–1359. Association for Computational Linguistics, Santa Fe, New Mexico, USA, Aug. 2018.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref040">
      <label>40</label>
      <mixed-citation publication-type="other">V. Lombardo, R. Damiano, C. Battaglino, and A. Pizzo. Automatic annotation of characters’ emotions in stories. pp. 117–129, 11 2015. <pub-id pub-id-type="doi">10.1007/978-3-319-27036-4_11</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref041">
      <label>41</label>
      <mixed-citation publication-type="other">S. Abrilian, L. Devillers, S. Buisine, and J.-C. Martin. Emotv1: Annotation of real-life emotions for the specifications of multimodal affective interfaces. 01 2005.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref042">
      <label>42</label>
      <mixed-citation publication-type="other">L. Vidrascu and L. Devillers. Annotation and detection of blended emotions in real human-human dialogs recorded in a call center. vol. 0, pp. 944–947, 01 2005. <pub-id pub-id-type="doi">10.1109/ICME.2005.1521580</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref043">
      <label>43</label>
      <mixed-citation publication-type="other">S. Abrilian, L. Devillers, and J.-C. Martin. Annotation of emotions in real-life video interviews: Variability between coders. 01 2006.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref044">
      <label>44</label>
      <mixed-citation publication-type="journal"><name><surname>Stella</surname><given-names>M</given-names></name>. <article-title>Text-mining forma mentis networks reconstruct public perception of the STEM gender gap in social media</article-title>. <source>PeerJ Computer Science</source>. <year>2020</year>;<volume>6</volume>:<fpage>e295</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.7717/peerj-cs.295</pub-id><?supplied-pmid 33816946?><pub-id pub-id-type="pmid">33816946</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref045">
      <label>45</label>
      <mixed-citation publication-type="journal"><name><surname>Scarantino</surname><given-names>A.</given-names></name> and <name><surname>Griffiths</surname><given-names>P</given-names></name>. <article-title>Don’t give up on basic emotions</article-title>. <source><italic toggle="yes">Emotion Review</italic></source>, <volume>3</volume>:<fpage>444</fpage>–<lpage>454</lpage>, <day>09</day>
<year>2011</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1177/1754073911410745</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref046">
      <label>46</label>
      <mixed-citation publication-type="journal"><name><surname>Ekman</surname><given-names>P</given-names></name>. <article-title>Basic emotions</article-title>. <source><italic toggle="yes">Handbook of cognition and emotion</italic></source>, <volume>98</volume>(<issue>45-60</issue>):<fpage>16</fpage>, <year>1999</year>.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref047">
      <label>47</label>
      <mixed-citation publication-type="journal"><name><surname>Cowen</surname><given-names>A. S.</given-names></name> and <name><surname>Keltner</surname><given-names>D</given-names></name>. <article-title>Self-report captures 27 distinct categories of emotion bridged by continuous gradients</article-title>. <source><italic toggle="yes">Proceedings of the National Academy of Sciences</italic></source>, <volume>114</volume>(<issue>38</issue>):<fpage>E7900</fpage>–<lpage>E7909</lpage>, <year>2017</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.1702247114</pub-id>
<?supplied-pmid 28874542?><pub-id pub-id-type="pmid">28874542</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref048">
      <label>48</label>
      <mixed-citation publication-type="book"><name><surname>Parrott</surname><given-names>W. G</given-names></name>. <source><italic toggle="yes">Emotions in social psychology: Essential readings</italic></source>. <publisher-name>psychology press</publisher-name>, <year>2001</year>.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref049">
      <label>49</label>
      <mixed-citation publication-type="journal"><name><surname>Susanto</surname><given-names>Y.</given-names></name>, <name><surname>Livingstone</surname><given-names>A. G.</given-names></name>, <name><surname>Ng</surname><given-names>B. C.</given-names></name>, and <name><surname>Cambria</surname><given-names>E</given-names></name>. <article-title>The hourglass model revisited</article-title>. <source><italic toggle="yes">IEEE Intelligent Systems</italic></source>, <volume>35</volume>(<issue>5</issue>):<fpage>96</fpage>–<lpage>102</lpage>, <year>2020</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/MIS.2020.2992799</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref050">
      <label>50</label>
      <mixed-citation publication-type="book"><name><surname>Cambria</surname><given-names>E.</given-names></name>, <name><surname>Livingstone</surname><given-names>A.</given-names></name>, and <name><surname>Hussain</surname><given-names>A</given-names></name>. <part-title>The hourglass of emotions</part-title>. In <source><italic toggle="yes">Cognitive behavioural systems</italic></source>, pp. <fpage>144</fpage>–<lpage>157</lpage>. <publisher-name>Springer</publisher-name>, <year>2012</year>.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref051">
      <label>51</label>
      <mixed-citation publication-type="other">C. Liu, M. Osama, and A. de Andrade. DENS: A dataset for multi-class emotion analysis. <italic toggle="yes">CoRR</italic>, abs/1910.11769, 2019.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref052">
      <label>52</label>
      <mixed-citation publication-type="other">A. Rakhmetullina, D. Trautmann, and G. Groh. Distant supervision for emotion classification task using emoji 2 emotion. 2018.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref053">
      <label>53</label>
      <mixed-citation publication-type="other">R. Sprugnoli. Multiemotions-it: A new dataset for opinion polarity and emotion analysis for Italian. In <italic toggle="yes">7th Italian Conference on Computational Linguistics</italic>, <italic toggle="yes">CLiC-it 2020</italic>, pp. 402–408. Accademia University Press, 2020.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref054">
      <label>54</label>
      <mixed-citation publication-type="other">M. Abdul-Mageed and L. Ungar. EmoNet: Fine-grained emotion detection with gated recurrent neural networks. In <italic toggle="yes">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</italic>, pp. 718–728. Association for Computational Linguistics, Vancouver, Canada, July 2017. <pub-id pub-id-type="doi">10.18653/v1/P17-1067</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref055">
      <label>55</label>
      <mixed-citation publication-type="journal"><name><surname>Kagita</surname><given-names>N</given-names></name>. <article-title>Role of emotions in the FMCG branding and their purchase intentions</article-title>. <source><italic toggle="yes">Vidwat</italic></source>, <volume>11</volume>(<issue>1</issue>):<fpage>24</fpage>–<lpage>28</lpage>, <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref056">
      <label>56</label>
      <mixed-citation publication-type="other">E. Öhman, M. Pàmies, K. Kajava, and J. Tiedemann. Xed: A multilingual dataset for sentiment analysis and emotion detection, 2020.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref057">
      <label>57</label>
      <mixed-citation publication-type="journal"><name><surname>Caluza</surname><given-names>L. J</given-names></name>. <article-title>Deciphering west philippine sea: A plutchik and vader algorithm sentiment analysis</article-title>. <source><italic toggle="yes">Indian Journal of Science and Technology</italic></source>, <volume>11</volume>:<fpage>1</fpage>–<lpage>12</lpage>, <day>12</day><year>2017</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.17485/ijst/2018/v11i47/130980</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref058">
      <label>58</label>
      <mixed-citation publication-type="other">M. A. Mohsin and A. Beltiukov. Summarizing emotions from text using plutchik’s wheel of emotions. 2019.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref059">
      <label>59</label>
      <mixed-citation publication-type="journal"><name><surname>Chesi</surname><given-names>P.</given-names></name>, <name><surname>Marini</surname><given-names>M.</given-names></name>, <name><surname>Mancardi</surname><given-names>G.</given-names></name>, <name><surname>Patti</surname><given-names>F.</given-names></name>, <name><surname>Alivernini</surname><given-names>L.</given-names></name>, <name><surname>Bisecco</surname><given-names>A.</given-names></name>, <etal>et al</etal>. <article-title>Listening to the neurological teams for multiple sclerosis: the smart project</article-title>. <source><italic toggle="yes">Neurological Sciences</italic></source>, <volume>41</volume>, <day>03</day><year>2020</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10072-020-04301-z</pub-id><?supplied-pmid 32198654?><pub-id pub-id-type="pmid">32198654</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref060">
      <label>60</label>
      <mixed-citation publication-type="journal"><name><surname>Balakrishnan</surname><given-names>V.</given-names></name> and <name><surname>Kaur</surname><given-names>W</given-names></name>. <article-title>String-based multinomial naïve bayes for emotion detection among Facebook diabetes community</article-title>. <source><italic toggle="yes">Procedia Computer Science</italic></source>, <volume>159</volume>:<fpage>30</fpage>–<lpage>37</lpage>, <year>2019</year>. Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 23rd International Conference KES2019. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.procs.2019.09.157</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref061">
      <label>61</label>
      <mixed-citation publication-type="other">T. Ulusoy, K. T. Danyluk, and W. J. Willett. Beyond the physical: Examining scale and annotation in virtual reality visualizations. Technical report, Department of Computer Science, University of Calgary, 2018.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref062">
      <label>62</label>
      <mixed-citation publication-type="other">P. Balouchian and H. Foroosh. Context-sensitive single-modality image emotion analysis: A unified architecture from dataset construction to cnn classification. pp. 1932–1936, 10 2018. <pub-id pub-id-type="doi">10.1109/ICIP.2018.8451048</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref063">
      <label>63</label>
      <mixed-citation publication-type="other">H. Rashkin, A. Bosselut, M. Sap, K. Knight, and Y. Choi. Modeling naive psychology of characters in simple commonsense stories. In <italic toggle="yes">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</italic>, pp. 2289–2299. Association for Computational Linguistics, Melbourne, Australia, July 2018. <pub-id pub-id-type="doi">10.18653/v1/P18-1213</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref064">
      <label>64</label>
      <mixed-citation publication-type="journal"><name><surname>Ranco</surname><given-names>G.</given-names></name>, <name><surname>Aleksovski</surname><given-names>D.</given-names></name>, <name><surname>Caldarelli</surname><given-names>G.</given-names></name>, <name><surname>Grčar</surname><given-names>M.</given-names></name>, and <name><surname>Mozetič</surname><given-names>I</given-names></name>. <article-title>The effects of Twitter sentiment on stock price returns</article-title>. <source><italic toggle="yes">PloS one</italic></source>, <volume>10</volume>:<fpage>e0138441</fpage>, <day>09</day><year>2015</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0138441</pub-id><?supplied-pmid 26390434?><pub-id pub-id-type="pmid">26390434</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref065">
      <label>65</label>
      <mixed-citation publication-type="other">K. Kukk. Correlation between emotional tweets and stock prices. 2019.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref066">
      <label>66</label>
      <mixed-citation publication-type="other">O. Zhurakovskaya, L. Steinkamp, K. M. Tymann, and C. Gips. An emotion detection tool composed of established techniques.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref067">
      <label>67</label>
      <mixed-citation publication-type="other">R. Sharma, D. Pandey, S. Zith, and S. Babu. Sentiment analysis of Facebook &amp; Twitter using soft computing. pp. 2457–1016, 08 2020.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref068">
      <label>68</label>
      <mixed-citation publication-type="other">R. Sawhney, H. Joshi, S. Gandhi, and R. R. Shah. A time-aware transformer based model for suicide ideation detection on social media. In <italic toggle="yes">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</italic>, pp. 7685–7697. Association for Computational Linguistics, Online, Nov. 2020. <pub-id pub-id-type="doi">10.18653/v1/2020.emnlp-main.619</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref069">
      <label>69</label>
      <mixed-citation publication-type="other">H. Tanabe, T. Ogawa, T. Kobayashi, and Y. Hayashi. Exploiting narrative context and a priori knowledge of categories in textual emotion classification. In <italic toggle="yes">Proceedings of the 28th International Conference on Computational Linguistics</italic>, pp. 5535–5540. International Committee on Computational Linguistics, Barcelona, Spain (Online), Dec. 2020. <pub-id pub-id-type="doi">10.18653/v1/2020.coling-main.483</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref070">
      <label>70</label>
      <mixed-citation publication-type="other">S. F. Yilmaz, E. B. Kaynak, A. Koç, H. Dibeklioğlu, and S. S. Kozat. Multi-label sentiment analysis on 100 languages with dynamic weighting for label imbalance, 2020.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref071">
      <label>71</label>
      <mixed-citation publication-type="journal"><name><surname>Treceñe</surname><given-names>J. K. D</given-names></name>. <article-title>Delving the sentiments to track emotions in gender issues: A plutchik-based sentiment analysis in students’ learning diaries</article-title>. <source><italic toggle="yes">International Journal of Scientific &amp; Technology Research</italic></source>, <volume>8</volume>:<fpage>1134</fpage>–<lpage>1139</lpage>, <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref072">
      <label>72</label>
      <mixed-citation publication-type="other">N. Bader, O. Mokryn, and J. Lanir. Exploring emotions in online movie reviews for online browsing. In <italic toggle="yes">Proceedings of the 22nd International Conference on Intelligent User Interfaces Companion</italic>, IUI’17 Companion, p. 35–38. Association for Computing Machinery, New York, NY, USA, 2017. <pub-id pub-id-type="doi">10.1145/3030024.3040982</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref073">
      <label>73</label>
      <mixed-citation publication-type="other">H.-Y. Yu and B.-C. Bae. Emotion and sentiment analysis from a film script: A case study. 2018.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref074">
      <label>74</label>
      <mixed-citation publication-type="other">J. Jenkins. Detecting emotional ambiguity in text. 4:55–57, 05 2020. <pub-id pub-id-type="doi">10.15406/mojabb.2020.04.00134</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref075">
      <label>75</label>
      <mixed-citation publication-type="journal"><name><surname>Stella</surname><given-names>M.</given-names></name>, <name><surname>Restocchi</surname><given-names>V.</given-names></name>, and <name><surname>De Deyne</surname><given-names>S</given-names></name>. <article-title>#lockdown: Network-enhanced emotional profiling in the time of covid-19</article-title>. <source><italic toggle="yes">Big Data and Cognitive Computing</italic></source>, <volume>4</volume>(<issue>2</issue>), <year>2020</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/bdcc4020014</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref076">
      <label>76</label>
      <mixed-citation publication-type="journal"><name><surname>Cockburn</surname><given-names>A.</given-names></name>, <name><surname>Karlson</surname><given-names>A.</given-names></name>, and <name><surname>Bederson</surname><given-names>B. B</given-names></name>. <article-title>A review of overview+ detail, zooming, and focus+ context interfaces</article-title>. <source><italic toggle="yes">ACM Computing Surveys (CSUR)</italic></source>, <volume>41</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>31</lpage>, <year>2009</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/1456650.1456652</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref077">
      <label>77</label>
      <mixed-citation publication-type="journal"><name><surname>Furnas</surname><given-names>G. W</given-names></name>. <article-title>Generalized fisheye views</article-title>. <source><italic toggle="yes">Acm Sigchi Bulletin</italic></source>, <volume>17</volume>(<issue>4</issue>):<fpage>16</fpage>–<lpage>23</lpage>, <year>1986</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/22339.22342</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref078">
      <label>78</label>
      <mixed-citation publication-type="other">J. McAuley, C. Targett, Q. Shi, and A. Van Den Hengel. Image-based recommendations on styles and substitutes. In <italic toggle="yes">Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval</italic>, pp. 43–52, 2015.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref079">
      <label>79</label>
      <mixed-citation publication-type="journal"><name><surname>Mohammad</surname><given-names>Saif M</given-names></name> and <name><surname>Turney</surname><given-names>Peter D</given-names></name>. <article-title>Crowdsourcing a word-emotion association lexicon</article-title>. In <source><italic toggle="yes">Computational intelligence</italic></source>, <volume>29</volume>, <issue>3</issue>, p. <fpage>436</fpage>–<lpage>465</lpage>, Wiley Online Library, <year>2013</year>. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm" ext-link-type="uri">http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm</ext-link>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/j.1467-8640.2012.00460.x</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0256503.ref080">
      <label>80</label>
      <mixed-citation publication-type="other">O. Hany. IMDB top 1000 | Kaggle, 2021.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref081">
      <label>81</label>
      <mixed-citation publication-type="book"><name><surname>Propp</surname><given-names>V</given-names></name>. <source><italic toggle="yes">Morphology of the Folktale</italic></source>, <volume>vol. 9</volume>. <publisher-name>University of Texas Press</publisher-name>, <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="pone.0256503.ref082">
      <label>82</label>
      <mixed-citation publication-type="other">S. M. Mohammad, S. Kiritchenko, P. Sobhani, X. Zhu, and C. Cherry. Semeval-2016 task 6: Detecting stance in tweets. In <italic toggle="yes">Proceedings of the International Workshop on Semantic Evaluation</italic>, SemEval’16. San Diego, California, June 2016.</mixed-citation>
    </ref>
  </ref-list>
</back>
<sub-article article-type="aggregated-review-documents" id="pone.0256503.r001" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0256503.r001</article-id>
    <title-group>
      <article-title>Decision Letter 0</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Danforth</surname>
          <given-names>Christopher M.</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2021 Christopher M. Danforth</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Christopher M. Danforth</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="doi" xlink:href="10.1371/journal.pone.0256503" id="rel-obj001" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>0</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">8 Jul 2021</named-content>
    </p>
    <p>PONE-D-21-18896</p>
    <p>PyPlutchik: visualising and comparing emotion-annotated corpora</p>
    <p>PLOS ONE</p>
    <p>Dear Dr. Vilella,</p>
    <p>Thank you for submitting your manuscript to PLOS ONE. After careful consideration, we feel that it has merit but does not fully meet PLOS ONE’s publication criteria as it currently stands. Therefore, we invite you to submit a revised version of the manuscript that addresses the points raised during the review process.</p>
    <p>We had trouble finding a second reviewer, so please respond carefully to the suggestions of the very comprehensive review that we were fortunately able to secure.</p>
    <p>Please submit your revised manuscript by Aug 22 2021 11:59PM. If you will need more time than this to complete your revisions, please reply to this message or contact the journal office at <email>plosone@plos.org</email>. When you're ready to submit your revision, log on to <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.editorialmanager.com/pone/" ext-link-type="uri">https://www.editorialmanager.com/pone/</ext-link> and select the 'Submissions Needing Revision' folder to locate your manuscript file.</p>
    <p>Please include the following items when submitting your revised manuscript:</p>
    <p>
      <list list-type="bullet">
        <list-item>
          <p>A rebuttal letter that responds to each point raised by the academic editor and reviewer(s). You should upload this letter as a separate file labeled 'Response to Reviewers'.</p>
        </list-item>
        <list-item>
          <p>A marked-up copy of your manuscript that highlights changes made to the original version. You should upload this as a separate file labeled 'Revised Manuscript with Track Changes'.</p>
        </list-item>
        <list-item>
          <p>An unmarked version of your revised paper without tracked changes. You should upload this as a separate file labeled 'Manuscript'.</p>
        </list-item>
      </list>
    </p>
    <p>If you would like to make changes to your financial disclosure, please include your updated statement in your cover letter. Guidelines for resubmitting your figure files are available below the reviewer comments at the end of this letter.</p>
    <p>If applicable, we recommend that you deposit your laboratory protocols in protocols.io to enhance the reproducibility of your results. Protocols.io assigns your protocol its own identifier (DOI) so that it can be cited independently in the future. For instructions see: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols" ext-link-type="uri">http://journals.plos.org/plosone/s/submission-guidelines#loc-laboratory-protocols</ext-link>. Additionally, PLOS ONE offers an option for publishing peer-reviewed Lab Protocol articles, which describe protocols hosted on protocols.io. Read more information on sharing protocols at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols" ext-link-type="uri">https://plos.org/protocols?utm_medium=editorial-email&amp;utm_source=authorletters&amp;utm_campaign=protocols</ext-link>.</p>
    <p>We look forward to receiving your revised manuscript.</p>
    <p>Kind regards,</p>
    <p>Christopher M. Danforth</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Journal Requirements:</p>
    <p>When submitting your revision, we need you to address these additional requirements.</p>
    <p>1. Please ensure that your manuscript meets PLOS ONE's style requirements, including those for file naming. The PLOS ONE style templates can be found at </p>
    <p><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=wjVg/PLOSOne_formatting_sample_main_body.pdf</ext-link> and </p>
    <p>
      <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf" ext-link-type="uri">https://journals.plos.org/plosone/s/file?id=ba62/PLOSOne_formatting_sample_title_authors_affiliations.pdf</ext-link>
    </p>
    <p>2. Please review your reference list to ensure that it is complete and correct. If you have cited papers that have been retracted, please include the rationale for doing so in the manuscript text, or remove these references and replace them with relevant current references. Any changes to the reference list should be mentioned in the rebuttal letter that accompanies your revised manuscript. If you need to cite a retracted article, indicate the article’s retracted status in the References list and also include a citation and full reference for the retraction notice.</p>
    <p>[Note: HTML markup is below. Please do not edit.]</p>
    <p>Reviewers' comments:</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <bold>Comments to the Author</bold>
    </p>
    <p>1. Is the manuscript technically sound, and do the data support the conclusions?</p>
    <p>The manuscript must describe a technically sound piece of scientific research with data that supports the conclusions. Experiments must have been conducted rigorously, with appropriate controls, replication, and sample sizes. The conclusions must be drawn appropriately based on the data presented. </p>
    <p>Reviewer #1: Yes</p>
    <p>**********</p>
    <p>2. Has the statistical analysis been performed appropriately and rigorously? </p>
    <p>Reviewer #1: I Don't Know</p>
    <p>**********</p>
    <p>3. Have the authors made all data underlying the findings in their manuscript fully available?</p>
    <p>The <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.plosone.org/static/policies.action#sharing" ext-link-type="uri">PLOS Data policy</ext-link> requires authors to make all data underlying the findings described in their manuscript fully available without restriction, with rare exception (please refer to the Data Availability Statement in the manuscript PDF file). The data should be provided as part of the manuscript or its supporting information, or deposited to a public repository. For example, in addition to summary statistics, the data points behind means, medians and variance measures should be available. If there are restrictions on publicly sharing data—e.g. participant privacy or use of data from a third party—those must be specified.</p>
    <p>Reviewer #1: Yes</p>
    <p>**********</p>
    <p>4. Is the manuscript presented in an intelligible fashion and written in standard English?</p>
    <p>PLOS ONE does not copyedit accepted manuscripts, so the language in submitted articles must be clear, correct, and unambiguous. Any typographical or grammatical errors should be corrected at revision, so please note any specific errors here.</p>
    <p>Reviewer #1: Yes</p>
    <p>**********</p>
    <p>5. Review Comments to the Author</p>
    <p>Please use the space provided to explain your answers to the questions above. You may also include additional comments for the author, including concerns about dual publication, research ethics, or publication ethics. (Please upload your review as an attachment if it exceeds 20,000 characters)</p>
    <p>Reviewer #1: Summary:</p>
    <p>The authors introduce a Python module called PyPlutchik, which is built on top of popular plotting library MatPlotLib and provides an array of functions for visual representation of emotions in a circumplex, inspired by the infographic design by Robert Plutchik often cited in textual affect research. The authors provide ample evidence of the need within the Natural Language Processing community for a code implementation of this historically important graphical representation of affect, and include additional features to their module which show promise for more advanced and novel incarnations of the Plutchik wheel of emotions. In particular, the authors highlight the spatial relationships of the circumplex as integral to representation of semantic adjacencies and dichotomies; expand upon the notion of complex multi-faceted emotional affect, which greatly improves representation of nuance over singular categorizations for large corpora; and provide users with a range of options from simplistic representation to much more complex representations and small multiples, indicating that the authors have anticipated a range of research use cases. The authors demonstrate a steadfast commitment to the legacy of the original Plutchik Wheel (e.g. in choice of color and spatial orientation), but could perhaps benefit from a deeper acknowledgement of the critiques to the circumplex model in the broader research community since its inception. The graphics generated by the module demonstrate a careful attention to detail and commitment to visual clarity by the authors. Overall, the paper excels in its presentation and substance of scientific contribution, but there are a few necessary areas for revision that should be addressed before publication, particularly with the accompanying code and the implementation of dyadic visualization; and some minor suggestions that will likely be addressed during proofreading or are simply recommendations to possibly benefit from rewording for clarity.</p>
    <p>Necessary areas for revision:</p>
    <p>The introduction should include more context on the circumplex model of emotion cited in [52], specifically introducing the phrasing of a ‘circumplex’ to the paper rather than leaving that verbage in the bibliography. “Emotion circumplex” is a very common bigram in this area of research.</p>
    <p>p.3: Citing the number of Google Scholar results is inappropriate for a scholarly journal, as word/phrase frequency is not indicative of importance on its own, and documents included on Google scholar extend beyond peer-reviewed scholarly research. This reviewer is in agreement that this tool is a meaningful and useful contribution to the scientific community, and the specific cited papers’ usage of alternative plots in section 2 is a convincing display of need, but the mention of the search method for finding those papers is unnecessary and the total paper count is an unreliable measure.</p>
    <p>Github repository should include:</p>
    <p>dependencies.txt</p>
    <p>Example code shown on p.7-8</p>
    <p>Some descriptions of implementation detailed in the paper should be removed from the paper and instead included in the module documentation, giving priority to discussion of the visual representation and affect model, rather than minutiae of implementation (this will also help to ‘future-proof’ work, as it is easier to update a Readme in a repository when changes arise in dependencies, than to update the code a published paper)</p>
    <p>Document line 269 (page 7 code line 2):</p>
    <p>from matplotlib.pyplot import plt</p>
    <p>should be import matplotlib.pyplot as plt</p>
    <p>this is a discrepancy between Python 2 and Python 3 wherein Python 3 no longer supports implicit imports</p>
    <p>code line 3: ‘nrow’/‘ncol’ should be pluralized (‘nrows’/‘ncols’) per matplotlib documentation here: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html" ext-link-type="uri">https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html</ext-link></p>
    <p>It should be noted that other NLP methods such as the “valence/arousal” measure do not align precisely with the Plutchik categories. It would appear, then, that using NLP libraries which categorize emotions based on the valence/arousal axes would be visually skewed by PyPlutchik not because of their content, but because of the measure used. This is a critical point to note in the paper and address explicitly.</p>
    <p>Fig 13 / SemEval discussion: Please include code for these plots in the GitHub repository. In particular, the data pipeline from SemEval to the dyad plots is unclear, and makes it difficult to answer this reviewer’s question: Why are all the dyad values so negligibly small? It seems increasingly challenging to evaluate minute differences in affect between Plutchik plots when the maximum value, for example in Fig. 13 (v), is 0.03 and the corresponding plot area takes up only a few pixels. This appears to demonstrate either an issue in the data processing, a need to reconsider scaling when all values are miniscule (just as line plot axes are adjusted in range to fit the data shown, perhaps values should be normalized by the maximum value for all categories prior to plotting), or some combination of both.</p>
    <p>Suggestions:</p>
    <p>It seems that “module” would be a more appropriate term than “library” for all instances of “PyPlutchik Library” in the abstract and paper: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://dev.to/hamza/framework-vs-library-vs-package-vs-module-the-debate-3jpp" ext-link-type="uri">https://dev.to/hamza/framework-vs-library-vs-package-vs-module-the-debate-3jpp</ext-link></p>
    <p>line 49: “This score can be interpreted as a binary flag that represents if emotion i was detected or not” - if i is in a range between 0 and 1, this is not a binary flag. Binary flags are booleans: 0 OR 1, not a float between 0 and 1. This language should be changed such that i corresponds to 8 emotions and Σ ia,b,c (where a,b,c, correspond to the 3 levels of intensity) is less than or equal to one. From the code, it does not appear true that “all emotions in a branch must sum to 1”, only that their sum must be less than or equal to 1.</p>
    <p>Line 62: “for instance, it is respected also in user interfaces displaying Plutchik’s emotions” please cite</p>
    <p>Line 91: “the exploitation of relationships among different artworks” - the phrasing here is confusing. By exploitation do you mean exploration? What is meant by ‘artworks’?</p>
    <p>Line 93: Is it true that ManyEyes supported interactive visualization? This reviewer had not personally used the tool before its discontinuation, but this article specifically mentions lack of interactivity support: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://boostlabs.com/blog/ibms-many-eyes-online-data-visualization-tool/" ext-link-type="uri">https://boostlabs.com/blog/ibms-many-eyes-online-data-visualization-tool/</ext-link></p>
    <p>General question for section 2: Is the inclusion of ManyEyes relevant, given its discontinuation? What about other existing tools for visualization such as Tableau, Plotly, MatPlotLib, Bokeh, Shiny, D3, or DataWrapper, to name a few? Tools currently in use seem like more relevant STAR benchmarks for discussion.</p>
    <p>Section 2: Table 1 would benefit from accompanying representation of the original Plutchik emotion circumplex, e.g. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.researchgate.net/profile/Gerardo-Maupome/publication/258313558/figure/fig1/AS:297350959517696@1447905401049/Plutchiks-wheel-of-emotions-with-basic-emotions-and-derivative-emotions-and-cone-like.png" ext-link-type="uri">https://www.researchgate.net/profile/Gerardo-Maupome/publication/258313558/figure/fig1/AS:297350959517696@1447905401049/Plutchiks-wheel-of-emotions-with-basic-emotions-and-derivative-emotions-and-cone-like.png</ext-link></p>
    <p>Line 141: Simplistic visual representation of variant dyad types would aid reading comprehension</p>
    <p>There is no mention of cultural variation of emotion within the paper; however, the introduction of dyads does in part mitigate this oversight from a technical standpoint, by providing for a more complex description of emotion. Nonetheless, some acknowledgement/discussion of cultural differences in emotion should be included for context.</p>
    <p>Please review citations carefully. A few examples:</p>
    <p>Citation 22: “a ective” → affective</p>
    <p>Citation 5: “Nrclex” → “NRClex” (please include URL)</p>
    <p>Citation 24: “Imdb” → “IMDB” and “kaggle” → “Kaggle”</p>
    <p>29: “fmcg” capitalize</p>
    <p>“facebook”, “twitter”, “Italian” - brands &amp; languages/countries should always be title cased</p>
    <p>66: “Manyeyes” → “ManyEyes”</p>
    <p>etc.</p>
    <p>**********</p>
    <p>6. PLOS authors have the option to publish the peer review history of their article (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://journals.plos.org/plosone/s/editorial-and-peer-review-process#loc-peer-review-history" ext-link-type="uri">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.plos.org/privacy-policy" ext-link-type="uri">Privacy Policy</ext-link>.</p>
    <p>Reviewer #1: No</p>
    <p>[NOTE: If reviewer comments were submitted as an attachment file, they will be attached to this email and accessible via the submission site. Please log into your account, locate the manuscript record, and check for the action link "View Attachments". If this link does not appear, there are no attachment files.]</p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://pacev2.apexcovantage.com/" ext-link-type="uri">https://pacev2.apexcovantage.com/</ext-link>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Registration is free. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email PLOS at <email>figures@plos.org</email>. Please note that Supporting Information files do not need this step.</p>
  </body>
</sub-article>
<sub-article article-type="author-comment" id="pone.0256503.r002">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0256503.r002</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 0</article-title>
    </title-group>
    <related-article xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="doi" xlink:href="10.1371/journal.pone.0256503" id="rel-obj002" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">29 Jul 2021</named-content>
    </p>
    <p>Turin, 26/07/2021</p>
    <p>We are sincerely grateful to the reviewer for such a thorough and spot-on review. Their observations on our work have been deeply appreciated, as they pointed out many mistakes we needed to fix that survived our passes, and gave us a few useful insights that we discussed among us during the revision process. After this round of review, we feel that the overall quality of the manuscript has significantly improved. We also thank the reviewer for the careful checking of the code and for their suggestions. In the following pages we address every remark made by the reviewer, hoping that every answer is on point and clear enough. Of course we are always available for any further discussion or clarification.</p>
    <p>With our best regards,</p>
    <p>Alfonso Semeraro</p>
    <p>Salvatore Vilella</p>
    <p>Giancarlo Ruffo</p>
    <p>Necessary areas for revision:</p>
    <p>-The introduction should include more context on the circumplex model of emotion cited in [52], specifically introducing the phrasing of a ‘circumplex’ to the paper rather than leaving that verbage in the bibliography. “Emotion circumplex” is a very common bigram in this area of research.</p>
    <p>We modified the introduction following the reviewer’s advice: we gave more space to Russel’s circumplex model, as well as to the circular representation of emotions (emotions circumplex) which is typical of many emotional models. Specifically, we added the following sentences:</p>
    <p>“One of the most famous dimensional models is Russel's circumplex model of emotions [54]. Russel's model posits that affect states are defined by dimensions that are not independent: emotions can be represented as points in a space whose main orthogonal axes represent their degree of arousal and of pleasure/displeasure. According to Russel, the full spectrum of emotions can be meaningfully placed along the resulting circumference. Russel is not the first scholar to use a circular layout to represent emotions (for instance, it was already used in [33]): indeed, this kind of circumplex representation became very popular over time, since it is suitable to spatially represent emotions on a continuous space.“ </p>
    <p>We also highlighted that Plutchik’s model adopts a circumplex-like representation: even though it is not a continuous model, it makes use of a circular disposition of emotions.</p>
    <p>-p.3: Citing the number of Google Scholar results is inappropriate for a scholarly journal, as word/phrase frequency is not indicative of importance on its own, and documents included on Google scholar extend beyond peer-reviewed scholarly research. This reviewer is in agreement that this tool is a meaningful and useful contribution to the scientific community, and the specific cited papers’ usage of alternative plots in section 2 is a convincing display of need, but the mention of the search method for finding those papers is unnecessary and the total paper count is an unreliable measure.</p>
    <p>Following the reviewer’s suggestion, we removed “As of today, the query "Plutchik wheel" produces 3480 results on Google Scholar, of which 1620 publications have been submitted after 2017.” from the introduction. We also replaced</p>
    <p>“the first 100 publications after 2017 retrieved by the aforementioned query, we notice that 25 over 100 papers” </p>
    <p>in Section 2 (line 188) with the less specific</p>
    <p>“the related literature, we notice that many papers”.</p>
    <p>-Github repository should include:</p>
    <p>dependencies.txt</p>
    <p>Example code shown on p.7-8</p>
    <p>Some descriptions of implementation detailed in the paper should be removed from the paper and instead included in the module documentation, giving priority to discussion of the visual representation and affect model, rather than minutiae of implementation (this will also help to ‘future-proof’ work, as it is easier to update a Readme in a repository when changes arise in dependencies, than to update the code a published paper)</p>
    <p>We added the requirements.txt file to GitHub repository, as requested. We also wrote a README.md, where we explained the main features of our module, and we displayed code examples and the corresponding plots.</p>
    <p>We agree with the reviewer that most of the implementation details can be hidden from the paper, and moved elsewhere, as they are susceptible to changes in the future.</p>
    <p>We removed the code listing and all the references to any parameter, which are now available in the code documentation, leaving in the paper only a qualitative description of the main features of the module. Documentation is available at</p>
    <p>
      <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/alfonsosemeraro/pyplutchik/blob/master/Documentation.md" ext-link-type="uri">https://github.com/alfonsosemeraro/pyplutchik/blob/master/Documentation.md</ext-link>
    </p>
    <p>We did not remove the references to the Python dict format as an input for the module, and we also kept Fig. 2, Fig. 3 and Fig. 6 (now Fig. 4, Fig. 5 and Fig. 8), including their small code listing that show the dict as the input data. We believe that part to be robust to changes in future implementations. It also advocates for the simplicity of the tool, as it features a familiar data structure and “how straightforward it is to plug a dict into the module to obtain the visualisation”, as we claim on line 291.</p>
    <p>-Document line 269 (page 7 code line 2):</p>
    <p>from matplotlib.pyplot import plt</p>
    <p>should be import matplotlib.pyplot as plt</p>
    <p>this is a discrepancy between Python 2 and Python 3 wherein Python 3 no longer supports implicit imports</p>
    <p>code line 3: ‘nrow’/‘ncol’ should be pluralized (‘nrows’/‘ncols’) per matplotlib documentation here: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html" ext-link-type="uri">https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html</ext-link></p>
    <p>We are really grateful to the reviewer for such a thorough review also of the code snippets, as in this case this one was definitely wrong. We fixed it into the documentation, and removed it from the paper, as per the previous reviewer’s observation. </p>
    <p>-It should be noted that other NLP methods such as the “valence/arousal” measure do not align precisely with the Plutchik categories. It would appear, then, that using NLP libraries which categorize emotions based on the valence/arousal axes would be visually skewed by PyPlutchik not because of their content, but because of the measure used. This is a critical point to note in the paper and address explicitly.</p>
    <p>This is indeed a very important point to address. We designed PyPlutchik to work only with data annotated following Plutchik’s scheme - we never thought about the possibility to use it with other data. We addressed this in the conclusions, by adding the following sentences:</p>
    <p>“It should be noted that the PyPlutchik module is meant to work only with data that was classified - manually or automatically - according to the Plutchik's model of emotions. In general, it is never advisable to represent data on a different scheme from the one used for its classification. Most of the models are radically different from each other, both in the underlying assumptions as well as in the categories scheme; even when they are somewhat similar, there might be key differences that should be kept into account. As for the Plutchik model in particular, to the best of our knowledge it does not exist a one-to-one correspondence with any other discrete emotional model, as well as no methods to go from a continuous emotional space to a discrete one. For these reasons, PyPlutchik should be used only with data annotated following the Plutchik's scheme of annotation.”</p>
    <p>-Fig 13 / SemEval discussion: Please include code for these plots in the GitHub repository. In particular, the data pipeline from SemEval to the dyad plots is unclear, and makes it difficult to answer this reviewer’s question: Why are all the dyad values so negligibly small? It seems increasingly challenging to evaluate minute differences in affect between Plutchik plots when the maximum value, for example in Fig. 13 (v), is 0.03 and the corresponding plot area takes up only a few pixels. This appears to demonstrate either an issue in the data processing, a need to reconsider scaling when all values are miniscule (just as line plot axes are adjusted in range to fit the data shown, perhaps values should be normalized by the maximum value for all categories prior to plotting), or some combination of both.</p>
    <p> The pipeline from the SemEval data to the dyad plots in Fig. 12 and 13 (which now are Fig. 14 and 15) is pretty straightforward: we flagged a dyad to be present in one row of the dataset if both the two basic emotions that compose such a dyad are flagged as present. The reason for such small scores relies on the data itself. </p>
    <p>Our guess is that this corpus was annotated considering the 8 basic emotions only, and no dyads, as they did not appear in the data. This makes the dyads more rare, because they were not specifically intended by the annotators. We experienced this effect in several projects we worked on, where we annotated emotions in texts: when we did not consider dyads in the annotation interface, then dyads happened to be rare in the annotated data. When dyads were encompassed by the annotation interface, annotators used to click on them more often.</p>
    <p>As per the visualization, a parameter for rescaling the petal length on a given maximum is actually present in our module: it is the “normalize” parameter. We used it for both plots, but setting for all subplots the same maximum value (0.65) for consistency reasons: the high scores of the leftmost subplot made the other scores in the other subplots very small, in comparison. </p>
    <p>We now rescaled the dyads subplots with different scales (0.4 and 0.3). Due to the loss of consistency, Fig. 14 and Fig. 15 are not intended to be read for a comparison between subplots, but rather for a better comparison of petals within the same plot. We acknowledge that in most of the subplots a single outlier stands clear above all the other petals, despite our efforts to rescale the figure. However, we believe that the outstanding difference between dyads with high scores and dyads with close-to-zero scores must be preserved and highlighted by the visualization, as it is exactly the message this representation is designed to convey. For instance, petals in Fig. 14 (iv) are barely readable, but we kept the same scale of Fig. 14 (iii), as it serves as a visual comparison between dyads very frequent in the corpus (like hope) and dyads barely detected.</p>
    <p>We added the following disclaimer in line 461:</p>
    <p>“For this reason we resized the petal lengths in Fig. 14 (ii - v) and (vii - x) with a different scale than (i) and (v), and the same applies for Fig. 15. For instance, Trust petal in Fig. 14 (i) is almost to the maximum length, with a score equal to 0.63; Hope petal in (iii) is also almost at the maximum length, but with a score equal to 0.37. Both figures are therefore designed for a visual comparison of the petals within the same subplot, but not between two different subplots, due to the lack of consistency in the scale. However”...</p>
    <p>Suggestions:</p>
    <p>-It seems that “module” would be a more appropriate term than “library” for all instances of “PyPlutchik Library” in the abstract and paper: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://dev.to/hamza/framework-vs-library-vs-package-vs-module-the-debate-3jpp" ext-link-type="uri">https://dev.to/hamza/framework-vs-library-vs-package-vs-module-the-debate-3jpp</ext-link></p>
    <p>We replaced every occurrence of “library” with “module”, when talking about PyPlutchik. We kept the definition of “library” for matplotlib and NRCLex, whose software suite is remarkably bigger than ours.</p>
    <p>-line 49: “This score can be interpreted as a binary flag that represents if emotion i was detected or not” - if i is in a range between 0 and 1, this is not a binary flag. Binary flags are booleans: 0 OR 1, not a float between 0 and 1. This language should be changed such that i corresponds to 8 emotions and Σ ia,b,c (where a,b,c, correspond to the 3 levels of intensity) is less than or equal to one. From the code, it does not appear true that “all emotions in a branch must sum to 1”, only that their sum must be less than or equal to 1.</p>
    <p>The reviewer is right, the actual statement is incorrect. 0 and 1 can be considered binary flags only for a binary annotation of a single text, i.e. if that emotion has been detected or not in a text. When talking about average amounts of an emotion in a corpus, the score is indeed not binary anymore. We simply removed that consideration (line 49), as we think it can only contribute to confusion.</p>
    <p>Also, we replaced </p>
    <p>“all the scores of emotions belonging to the same branch must sum to 1” </p>
    <p>in line 71 with </p>
    <p>“the sum of all the scores of the emotions belonging to the same branch must be less than or equal to 1”</p>
    <p>-Line 62: “for instance, it is respected also in user interfaces displaying Plutchik’s emotions” please cite</p>
    <p> When we wrote this part, we had in mind two interfaces we used in two projects we worked on, where we annotated Plutchik’s emotions in tweets. The first interface appears like a series of colored buttons, with the classic Plutchik’s color-code. The second interface looks like a clickable Plutchik’s flower. Unfortunately, we realized that none of them is currently published.</p>
    <p>We then removed the sentence in line 62, as we are not aware of further annotation interfaces that reproduce such a graphic detail.</p>
    <p>-Line 91: “the exploitation of relationships among different artworks” - the phrasing here is confusing. By exploitation do you mean exploration? What is meant by ‘artworks’?</p>
    <p>We clarified this sentence by changing it to: </p>
    <p>“ManyEyes was designed as a web based community where users (mainly data analysts and visulisation designers) could upload their data to establish conversations with other users.”</p>
    <p>-Line 93: Is it true that ManyEyes supported interactive visualization? This reviewer had not personally used the tool before its discontinuation, but this article specifically mentions lack of interactivity support: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://boostlabs.com/blog/ibms-many-eyes-online-data-visualization-tool/" ext-link-type="uri">https://boostlabs.com/blog/ibms-many-eyes-online-data-visualization-tool/</ext-link></p>
    <p> As explained in this video: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.youtube.com/watch?v=aAYDBZt7Xk0" ext-link-type="uri">https://www.youtube.com/watch?v=aAYDBZt7Xk0</ext-link> ManyEyes allowed interactivity to some extent: it was possible to highlight parts of the figures, to scroll on maps, to refine the visualization. However, the link reported by the reviewer clearly states that interactive visualizations with ManyEyes were not possible. </p>
    <p>We removed such a controversial comment, as we believe it would not add any relevant contribution to the discussion of the current paper.</p>
    <p>-General question for section 2: Is the inclusion of ManyEyes relevant, given its discontinuation? What about other existing tools for visualization such as Tableau, Plotly, MatPlotLib, Bokeh, Shiny, D3, or DataWrapper, to name a few? Tools currently in use seem like more relevant STAR benchmarks for discussion.</p>
    <p>We believe it is not strictly necessary to include ManyEyes: at this point we are not so sure how relevant it is in the community since when it has been included in IBM Watson. We can leave it there for good measure, and we follow the suggestion of the reviewer by mentioning many other tools that are probably highly used by scientists in this research area. We added the following sentence:</p>
    <p>“nowadays, it exists a good number of software suites - such as Tableau, Microsoft PowerBI or Datawrapper, just to mention a few - that give the user a chance to create very interesting, eye-catching and often complex visualizations. They all adopt a graphical user interface, with all the pros and cons that usually come with it: an intuitive and fast way to realise the majority of the most common layouts, but likely less flexibility when it comes to create a more personalised visualisation. On the other hand, programming libraries and modules - such as Python's Matplotlib [25], Plotly [79] and Bokeh [77], or D3.js [78] in Javascript allow the users to create freely their own visualisations, though with a much steeper learning curve for those who are not familiar with these technologies.”</p>
    <p>-Section 2: Table 1 would benefit from accompanying representation of the original Plutchik emotion circumplex, e.g. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.researchgate.net/profile/Gerardo-Maupome/publication/258313558/figure/fig1/AS:297350959517696@1447905401049/Plutchiks-wheel-of-emotions-with-basic-emotions-and-derivative-emotions-and-cone-like.png" ext-link-type="uri">https://www.researchgate.net/profile/Gerardo-Maupome/publication/258313558/figure/fig1/AS:297350959517696@1447905401049/Plutchiks-wheel-of-emotions-with-basic-emotions-and-derivative-emotions-and-cone-like.png</ext-link></p>
    <p> We added what is now Fig. 2, a reproduction of the figure suggested by the reviewer, which we could not claim authorship for.</p>
    <p>-Line 141: Simplistic visual representation of variant dyad types would aid reading comprehension</p>
    <p>We added what is now Fig. 3. We made up this picture in the same fashion of the many diagrams that display Plutchik’s dyads available online. Combination of emotions one, two, three or four petals away into primary, secondary, tertiary and opposite dyads are explained by a small visual example.</p>
    <p>-There is no mention of cultural variation of emotion within the paper; however, the introduction of dyads does in part mitigate this oversight from a technical standpoint, by providing for a more complex description of emotion. Nonetheless, some acknowledgement/discussion of cultural differences in emotion should be included for context.</p>
    <p>We sincerely thank the reviewer for this comment: indeed this is a very important factor that we forgot to mention. We added in the introduction the following sentences to discuss how cultural variation can impact emotion modeling in general:</p>
    <p>“The cultural variation of emotions has also been studied. This is a crucial factor to take into account when classifying emotions: intuitively, the categories themselves can radically change depending on the cultural background [82]. This is also valid with respect to how the annotator perceives the emotions in a text and, if the annotated data is then used to train a classifier, it can introduce a bias in the model. The nomenclature of emotions, their meanings and the relations of words to emotion concepts depend on the social frameworks in which we are born and raised: therefore, cultural variation can significantly impact this kind of analysis. In this regard, in [81] the authors estimate emotion semantics across a sample of almost 2500 spoken languages, finding high variability in the meaning of emotion terms, but also evidence of a universal structure in the categorization of emotions. Following different methodological approaches, similar results were previously obtained in [83]”</p>
    <p>and we also stated that, as the reviewer correctly points out, the presence of dyads in Plutchik’s model partially makes up for this matter. At the beginning of Section 4, we added the sentences:</p>
    <p>“This feature dramatically enriches the spectrum of emotions, beyond the primary ones; enriching the spectrum increases the complexity of the models, also contributing to mitigate, at least partially, the phenomenon of cultural variation discussed in Section 1.”</p>
    <p>-Please review citations carefully. A few examples:</p>
    <p>Citation 22: “a ective” → affective</p>
    <p>Citation 5: “Nrclex” → “NRClex” (please include URL)</p>
    <p>Citation 24: “Imdb” → “IMDB” and “kaggle” → “Kaggle”</p>
    <p>29: “fmcg” capitalize</p>
    <p>“facebook”, “twitter”, “Italian” - brands &amp; languages/countries should always be title cased</p>
    <p>66: “Manyeyes” → “ManyEyes”</p>
    <p>etc.</p>
    <p>We fixed all the typos and errors reported by the reviewer.</p>
    <supplementary-material id="pone.0256503.s001" position="float" content-type="local-data">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">Response to Reviewers.pdf</named-content></p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="pone.0256503.s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article article-type="editor-report" id="pone.0256503.r003" specific-use="decision-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0256503.r003</article-id>
    <title-group>
      <article-title>Decision Letter 1</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Danforth</surname>
          <given-names>Christopher M.</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2021 Christopher M. Danforth</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Christopher M. Danforth</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="doi" xlink:href="10.1371/journal.pone.0256503" id="rel-obj003" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">9 Aug 2021</named-content>
    </p>
    <p>PyPlutchik: visualising and comparing emotion-annotated corpora</p>
    <p>PONE-D-21-18896R1</p>
    <p>Dear Dr. Vilella,</p>
    <p>We’re pleased to inform you that your manuscript has been judged scientifically suitable for publication and will be formally accepted for publication once it meets all outstanding technical requirements.</p>
    <p>Within one week, you’ll receive an e-mail detailing the required amendments. When these have been addressed, you’ll receive a formal acceptance letter and your manuscript will be scheduled for publication.</p>
    <p>An invoice for payment will follow shortly after the formal acceptance. To ensure an efficient process, please log into Editorial Manager at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.editorialmanager.com/pone/" ext-link-type="uri">http://www.editorialmanager.com/pone/</ext-link>, click the 'Update My Information' link at the top of the page, and double check that your user information is up-to-date. If you have any billing related questions, please contact our Author Billing department directly at <email>authorbilling@plos.org</email>.</p>
    <p>If your institution or institutions have a press office, please notify them about your upcoming paper to help maximize its impact. If they’ll be preparing press materials, please inform our press team as soon as possible -- no later than 48 hours after receiving the formal acceptance. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information, please contact <email>onepress@plos.org</email>.</p>
    <p>Kind regards,</p>
    <p>Christopher M. Danforth</p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
    <p>Additional Editor Comments (optional):</p>
    <p>Reviewers' comments:</p>
  </body>
</sub-article>
<sub-article article-type="editor-report" id="pone.0256503.r004" specific-use="acceptance-letter">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pone.0256503.r004</article-id>
    <title-group>
      <article-title>Acceptance letter</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Danforth</surname>
          <given-names>Christopher M.</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2021 Christopher M. Danforth</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Christopher M. Danforth</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="doi" xlink:href="10.1371/journal.pone.0256503" id="rel-obj004" related-article-type="reviewed-article"/>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">23 Aug 2021</named-content>
    </p>
    <p>PONE-D-21-18896R1 </p>
    <p>PyPlutchik: visualising and comparing emotion-annotated corpora </p>
    <p>Dear Dr. Vilella:</p>
    <p>I'm pleased to inform you that your manuscript has been deemed suitable for publication in PLOS ONE. Congratulations! Your manuscript is now with our production department. </p>
    <p>If your institution or institutions have a press office, please let them know about your upcoming paper now to help maximize its impact. If they'll be preparing press materials, please inform our press team within the next 48 hours. Your manuscript will remain under strict press embargo until 2 pm Eastern Time on the date of publication. For more information please contact <email>onepress@plos.org</email>.</p>
    <p>If we can help with anything else, please email us at <email>plosone@plos.org</email>. </p>
    <p>Thank you for submitting your work to PLOS ONE and supporting open access. </p>
    <p>Kind regards, </p>
    <p>PLOS ONE Editorial Office Staff</p>
    <p>on behalf of</p>
    <p>Dr. Christopher M. Danforth </p>
    <p>Academic Editor</p>
    <p>PLOS ONE</p>
  </body>
</sub-article>
