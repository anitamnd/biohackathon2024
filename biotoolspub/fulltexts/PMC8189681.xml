<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8189681</article-id>
    <article-id pub-id-type="pmid">33165508</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa953</article-id>
    <article-id pub-id-type="publisher-id">btaa953</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Sequence Analysis</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Halcyon: an accurate basecaller exploiting an encoder–decoder model with monotonic attention</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Konishi</surname>
          <given-names>Hiroki</given-names>
        </name>
        <aff>
          <institution>Health Intelligence Center</institution>
        </aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yamaguchi</surname>
          <given-names>Rui</given-names>
        </name>
        <aff>
          <institution>Human Genome Center</institution>
        </aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yamaguchi</surname>
          <given-names>Kiyoshi</given-names>
        </name>
        <aff><institution>Advanced Clinical Research Center, Institute of Medical Science, The University of Tokyo</institution>, Tokyo, <country country="JP">Japan</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Furukawa</surname>
          <given-names>Yoichi</given-names>
        </name>
        <aff><institution>Advanced Clinical Research Center, Institute of Medical Science, The University of Tokyo</institution>, Tokyo, <country country="JP">Japan</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2989-308X</contrib-id>
        <name>
          <surname>Imoto</surname>
          <given-names>Seiya</given-names>
        </name>
        <xref rid="btaa953-cor1" ref-type="corresp"/>
        <aff>
          <institution>Health Intelligence Center</institution>
        </aff>
        <aff>
          <institution>Human Genome Center</institution>
        </aff>
        <!--imoto@ims.u-tokyo.ac.jp-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Inanc</surname>
          <given-names>Birol</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btaa953-cor1">To whom correspondence should be addressed. <email>imoto@ims.u-tokyo.ac.jp</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>5</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-12-07">
      <day>07</day>
      <month>12</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>07</day>
      <month>12</month>
      <year>2020</year>
    </pub-date>
    <volume>37</volume>
    <issue>9</issue>
    <fpage>1211</fpage>
    <lpage>1217</lpage>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>2</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>14</day>
        <month>10</month>
        <year>2020</year>
      </date>
      <date date-type="editorial-decision">
        <day>27</day>
        <month>10</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>30</day>
        <month>10</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa953.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>In recent years, nanopore sequencing technology has enabled inexpensive long-read sequencing, which promises reads longer than a few thousand bases. Such long-read sequences contribute to the precise detection of structural variations and accurate haplotype phasing. However, deciphering precise DNA sequences from noisy and complicated nanopore raw signals remains a crucial demand for downstream analyses based on higher-quality nanopore sequencing, although various basecallers have been introduced to date.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>To address this need, we developed a novel basecaller, Halcyon, that incorporates neural-network techniques frequently used in the field of machine translation. Our model employs monotonic-attention mechanisms to learn semantic correspondences between nucleotides and signal levels without any pre-segmentation against input signals. We evaluated performance with a human whole-genome sequencing dataset and demonstrated that Halcyon outperformed existing third-party basecallers and achieved competitive performance against the latest Oxford Nanopore Technologies’ basecallers.</p>
      </sec>
      <sec id="s3">
        <title>Availabilityand implementation</title>
        <p>The source code (halcyon) can be found at <ext-link xlink:href="https://github.com/relastle/halcyon" ext-link-type="uri">https://github.com/relastle/halcyon</ext-link>.</p>
      </sec>
    </abstract>
    <counts>
      <page-count count="5"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Recently, long-read single-molecule sequencing (lengths up to 2.4 Mbp) has been realized by Oxford Nanopore Technologies (ONT) with the introduction of MinION devices (<xref rid="btaa953-B20" ref-type="bibr">Payne <italic toggle="yes">et al.</italic>, 2019</xref>). Nanopore sequencing has been utilized in various applications such as in the detection of structural variation and cytosine methylation, along with metagenome <italic toggle="yes">de novo</italic> assembly (<xref rid="btaa953-B8" ref-type="bibr">Cretu Stancu <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btaa953-B9" ref-type="bibr">De Coster <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btaa953-B11" ref-type="bibr">Gong <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btaa953-B14" ref-type="bibr">Jain <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btaa953-B23" ref-type="bibr">Simpson <italic toggle="yes">et al.</italic>, 2017</xref>). Basecalling, i.e. translation from complex nanopore raw signals into nucleotide sequences, is first performed in nanopore sequencing pipelines. Error-prone basecalling adversely affects the entirety of downstream analyses incorporating nanopore sequencing, and therefore, the development of more accurate basecallers is critical. Although ONT has officially developed several basecallers, the details of their model specifications are not public. Thus, various third-party basecallers based on deep learning have been developed based on different approaches (<xref rid="btaa953-B4" ref-type="bibr">Boža <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btaa953-B24" ref-type="bibr">Stoiber and Brown, 2017</xref>; <xref rid="btaa953-B26" ref-type="bibr">Teng <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btaa953-B27" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2018</xref>). However, the accuracy achieved by these basecallers at the individual read resolution is insufficient [approximately <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:mo>≤</mml:mo><mml:mn>90</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> (<xref rid="btaa953-B28" ref-type="bibr">Wick <italic toggle="yes">et al.</italic>, 2019</xref>)]. Considering the significance of recent studies driven by ONT’s sequencing platform, there is high demand for the development of a more sophisticated basecaller. In turn, sequence data obtained from more accurate basecalling will enable more accurate detection of structural variations and cytosine methylation.</p>
    <p>Almost all neural-network-based basecallers proposed to date are dependent to some extent on the recurrent neural network (RNN) model. The RNN is well-recognized to handle inputs with variable lengths and interpret complicated timestep dependencies of input sequences. Introducing such a technique in basecalling tasks would be reasonable because nanopore raw signals are produced by multiple nucleotides passing through a pore and interpreting such dependencies from complicated raw signals is essential.</p>
    <p>However, a single sequence of RNN cells cannot handle a variable-length output from a given input. In the case of nanopore basecalling, the length of an output nucleotide sequence cannot be determined exactly from the length of the input raw signals. DeepNano (<xref rid="btaa953-B4" ref-type="bibr">Boža <italic toggle="yes">et al.</italic>, 2017</xref>) tackled this problem by dividing input raw signals into ‘events’ such that a single event corresponds to a single nucleotide. Although such an approach can ensure the training of neural networks is simple and intuitively resolve the problem of variable output dimension, the basecalling performance suffers from a bottleneck in the heuristic segmentation of signals to events, which is not exact.</p>
    <p>Alternatively, another neural network technique with the potential to handle variable output dimension is the connectionist temporal classification (CTC) decoder (<xref rid="btaa953-B12" ref-type="bibr">Graves <italic toggle="yes">et al.</italic>, 2006</xref>); it has been used in processing speech signals. This technique was incorporated in the novel third-party basecaller, Chiron (<xref rid="btaa953-B26" ref-type="bibr">Teng <italic toggle="yes">et al.</italic>, 2018</xref>). However, although this technique can resolve the variable output dimension problem and can enable end-to-end learning from input raw signals into nucleotides, the CTC-decoder itself is not a technique proposed in current schemes; this implies that a more state-of-the-art technique would likely be required to boost the basecalling performance.</p>
    <p>In addition, the encoder–decoder model has been frequently used in machine translation (<xref rid="btaa953-B25" ref-type="bibr">Sutskever <italic toggle="yes">et al.</italic>, 2014</xref>). This model has two RNNs, one of which, the encoder, can encode a variable-length input, whereas the other ‘decoder’ RNN can decode a variable-length output from the fixed dimensional encoded features. This model can be trained using matched input and output sequences, without any corresponding semantic information between the local parts of the inputs and the outputs.</p>
    <p>Another essential technique commonly used in sequence-to-sequence learning is an attention mechanism (<xref rid="btaa953-B2" ref-type="bibr">Bahdanau <italic toggle="yes">et al.</italic>, 2015</xref>; <xref rid="btaa953-B19" ref-type="bibr">Luong <italic toggle="yes">et al.</italic>, 2015</xref>). Prior to the emergence of attention mechanisms, an encoder was used to represent the whole input sequence as a fixed-dimensional vector and a decoder started decoding from the vector. This manner of encoding was dependent largely on the end-part of an input sequence, with the decoder being unable to use sufficient information at the beginning of the input, especially when longer input sequences were used. The attention mechanism resolved this problem by representing a variable-length input sequence as a fixed-dimension context vector in each decoding timestep. Each context vector is obtained by weighting all timestep outputs of an encoder, wherein weights are calculated by a simple feed-forward network given all outputs and a current decoder cell state. An encoder–decoder model with attention mechanisms can learn appropriate attention in a backpropagation scheme. Notably, recent sequence-to-sequence models using this mechanism have achieved remarkable performance (<xref rid="btaa953-B5" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btaa953-B6" ref-type="bibr">Chiu <italic toggle="yes">et al.</italic>, 2018</xref>). Moreover, recent studies have shown that the attention mechanism is superior to a conventional CTC decoder-based model even in speech recognition (<xref rid="btaa953-B7" ref-type="bibr">Chorowski <italic toggle="yes">et al.</italic>, 2015</xref>; <xref rid="btaa953-B29" ref-type="bibr">Zeyer et al., 2018</xref>).</p>
    <p>Thus, we decided to develop an improved basecaller, Halcyon, by utilizing an encoder–decoder model incorporating an attention mechanism. Halcyon incorporates a ‘monotonic’ attention mechanism, which enables the decoder to attend from an earlier part to a later part along an input sequence. Although this technique was originally introduced to accelerate decoding speed in inference at the expense of a small decrease in inference speed (<xref rid="btaa953-B21" ref-type="bibr">Raffel <italic toggle="yes">et al.</italic>, 2017</xref>), we incorporated this technique to stabilize the transition of attention, thereby improving basecalling precision.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Deep neural network architecture</title>
      <p>Halcyon combines a novel CNN module and RNN-based encoder and decoder. Whereas the CNN module is based on architectures commonly utilized in the field of image recognition, encoder and decoder modules are based on those used in the field of neural machine translation. The entire network was implemented using TensorFlow (<xref rid="btaa953-B1" ref-type="bibr">Abadi <italic toggle="yes">et al.</italic>, 2016</xref>).</p>
      <sec>
        <label>2.1.1</label>
        <title>Preliminaries</title>
        <p>This study aimed to construct neural networks that directly translate raw input signals measured by a pore into corresponding nucleotide sequences that passed through the pore. Here, an input with a <italic toggle="yes">T</italic>-timestep signal is denoted by <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, and an <italic toggle="yes">N</italic>-base nucleotide sequence is denoted by <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>≤</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is a 4-D vector indicating the probabilities of four nucleotides (A, T, G and C) at position <italic toggle="yes">k</italic>.</p>
      </sec>
      <sec>
        <label>2.1.2</label>
        <title>Inception-block-based CNN module</title>
        <p>Input raw signals are first fed into a CNN module. This module incorporates inception blocks, which are state-of-the-art architectures in the field of computer vision. A single inception block has branches. Each branch has 1 × 1 convolution to prevent the expansion of channel dimensionality and a convolution layer with different widths of filters. Finally, output vectors from these branches are concatenated in a channel axis and fed into the next layer.</p>
        <p>The motivation behind using this module is the need to extract local features of input raw signal and reduce the dimension of the input timestep axis. As the time-complexity of RNN is severely influenced by the timestep dimension, the reduction contributes to high throughput inference.</p>
        <p>Each convolution block consists of a single layer convolution layer with a rectified linear unit (ReLU) activation function, followed by a batch-normalization layer.</p>
        <p>The ReLU activation function is defined as
<disp-formula id="E1"><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mtext>ReLU</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>x</mml:mi><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext>otherwise</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo> </mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>Batch normalization constitutes a technique to accelerate the learning of neural networks by normalizing each layer’s input within a training minibatch (<xref rid="btaa953-B13" ref-type="bibr">Ioffe and Szegedy, 2015</xref>). Given a minibatch output of a single unit <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> (where n is a minibatch size), the batch normalization layer calculates the mean value and variance value within the minibatch as
<disp-formula id="E2"><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mo>μ</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
 <disp-formula id="E3"><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mo>σ</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mo>μ</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and the normalized output as
<disp-formula id="E4"><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mo>μ</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mo>σ</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="normal">ϵ</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>Then, it returns output <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> instead of returning <bold>x</bold>, where <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>γ</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>β</mml:mo></mml:mrow></mml:math></inline-formula>. Here, <italic toggle="yes">ϵ</italic>, <italic toggle="yes">γ</italic> and <italic toggle="yes">β</italic> are parameters specific to the unit and are optimized in a backpropagation scheme. In the test, <italic toggle="yes">μ</italic> and <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mo>σ</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> are set to the average values over those used in training minibatches.</p>
      </sec>
      <sec>
        <label>2.1.3</label>
        <title>Encoder module</title>
        <p>An RNN-based encoder plays an important role in capturing long-time dependencies in the timestep dimension and dealing with the variable lengths of inputs. LSTM is used in Halcyon as an RNN-based architecture. An LSTM layer is characterized by an LSTM cell and its recursive computation.</p>
        <p>The function of a single cell at the timestep of <italic toggle="yes">t</italic> can be formulated as follows:
<disp-formula id="E5"><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
 <disp-formula id="E6"><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">i</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
 <disp-formula id="E7"><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">c</mml:mi><mml:mo stretchy="false">˜</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
 <disp-formula id="E8"><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mi mathvariant="bold">c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">i</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">c</mml:mi><mml:mo stretchy="false">˜</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>
 <disp-formula id="E9"><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
 <disp-formula id="E10"><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>*</mml:mo><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">a</mml:mi><mml:mo>*</mml:mo><mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:math></inline-formula> denotes the Hadamard product between two vectors <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mi mathvariant="bold">a</mml:mi></mml:math></inline-formula> and <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mi mathvariant="bold">b</mml:mi></mml:math></inline-formula>, and <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="bold">b</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> denotes their vector concatenation. <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the input from the previous network at <italic toggle="yes">t</italic>-timestep; in this case, the output of stacked inception blocks. <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the synaptic-weight matrices and <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote bias vectors, all of which are shared among LSTM over all timesteps. <italic toggle="yes">σ</italic> denotes a sigmoid activation function <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Such calculation is conducted recursively along the timestep axis.</p>
        <p>To capture local dependencies in both the forward and backward directions along the timestep axis, bidirectional recurrent neural networks are incorporated (<xref rid="btaa953-B22" ref-type="bibr">Schuster and Paliwal, 1997</xref>) in Halcyon; these networks conduct the same recursive calculation in the backward direction of the timestep axis. In each timestep, an output vector of a forward RNN cell and that of a backward RNN cell are concatenated, and the result is yielded to the next layer.</p>
      </sec>
      <sec>
        <label>2.1.4</label>
        <title>Decoder module using attention mechanisms</title>
        <p>Given encoded features <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the goal is to estimate the target nucleotide probabilities <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>; i.e. to model the conditional probability <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The basic idea of modeling the probability using an LSTM layer can be formulated as
<disp-formula id="E11"><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>}</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo> </mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the fixed-dimensional representation of <bold>X</bold> given by the last hidden state of the encoder LSTM (theoretically, it has all information over an input sequence). We note that an output sequence length <italic toggle="yes">m</italic> cannot be defined by an input sequence length <italic toggle="yes">n</italic> as the number of electrical signal values measured per nucleotide exhibits some variation. We need to introduce an end-of-sequence symbol &lt;EOS&gt; to model output nucleotide sequences with all possible lengths. Here, each conditional probability <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>{</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>}</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is represented by the output of the decoder LSTM at <italic toggle="yes">t</italic>-timestep, a single fully connected layer, and a softmax function. Given the output of the LSTM at the timestep of <italic toggle="yes">t</italic> <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the weight matrix of the fully connected layer <bold>W</bold>, the conditional probability for each nucleotide base is
<disp-formula id="E12"><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>{</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>}</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>l</mml:mi></mml:munderover><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo> </mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">l</italic> denotes the number of output tokens including an end token, and <italic toggle="yes">g<sub>i</sub></italic> denotes the <italic toggle="yes">i</italic>th element of the fully connected output vector <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">g</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
        <p>However, such a model has a problem whereby the fixed-dimensional <bold>v</bold> contains little information for the beginning of an input sequence, with the problem becoming more serious when input sequences are longer. To handle this issue, we introduced attention mechanisms. Each probability of the elements of joint probability is formulated using attention mechanisms as
<disp-formula id="E13"><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a hidden state of decoder LSTM at timestep <italic toggle="yes">i</italic>. The context vector with attention <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is dependent on the previous decoder hidden state <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and all hidden states of encoder LSTM cells <bold>X</bold>. The context vector is defined as the weighted sum of encoder hidden states as
<disp-formula id="E14"><mml:math id="M14" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mo>α</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the weight <italic toggle="yes">α<sub>ij</sub></italic> for each hidden state <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is calculated by the softmax function to scored values as
<disp-formula id="E15"><mml:math id="M15" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mo>α</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where
<disp-formula id="E16"><mml:math id="M16" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>score</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>A score function can be formulated as a simple trainable feed forward network. Among some variations of such score functions, we adopted Luong attention (<xref rid="btaa953-B19" ref-type="bibr">Luong <italic toggle="yes">et al.</italic>, 2015</xref>), in which the score function is defined by <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>score</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">s</mml:mi><mml:mi>t</mml:mi><mml:mi mathvariant="normal">⊤</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> where <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a synaptic weight matrix for the score function and it is shared over all timesteps. The score function calculates the importance of input features <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> when predicting the output in timestep <italic toggle="yes">i</italic>, which enables the decoder to retrieve essential information from all encoded features selectively.</p>
        <p>Further, we adopted a monotonic attention mechanism (<xref rid="btaa953-B21" ref-type="bibr">Raffel <italic toggle="yes">et al.</italic>, 2017</xref>). Monotonic attention is an attention mechanism that restricts the transition of attention in a left-to-right manner, which is suitable for the task of basecalling nanopore sequences. In general, monotonic attention is used to reduce the complexity in decoding; it was incorporated in Halcyon to decode more accurately. A ‘soft’ monotonic attention mechanism was used in both training and inference time.</p>
      </sec>
      <sec>
        <label>2.1.5</label>
        <title>Training and inference decoder</title>
        <p>In a training phase, each decoder cell outputs likelihoods of nucleotides in each timestep, and then, the cell state is passed to the next decoder cell. In this timing, even if the decoder cell infers a wrong nucleotide, a correct nucleotide from a ground truth sequence will be passed to the next cell. Alternatively, in the inference for test data, a decoder cell cannot use the output token of the previous decoder cell, unlike a training decoder. Therefore, an inference decoder cell infers the likely nucleotide given the previous cell state, attended encoder’s features, and the token emitted by the previous cell. However, searching for an optimal nucleotide sequence <bold>Y</bold> that maximizes the conditional probability <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is too computationally expensive because the complexity grows exponentially with the number of nucleotide bases in the inferred sequence. To tackle this problem, a beam search strategy is commonly used, which retains the best <italic toggle="yes">k</italic> decoded paths with the highest probabilities at each timestep; <italic toggle="yes">k</italic> is termed the beam search width. Halcyon incorporated this strategy in the inference, with the beam search width set to 20 in all experiments except for the performance assessment of using different beam widths.</p>
      </sec>
      <sec>
        <label>2.1.6</label>
        <title>Scheduled sampling</title>
        <p>Although each inference decoder cell can only use the previously decoded token, the training decoder cell always uses the token from the ground truth. Such discrepancy is known to produce rapidly accumulated errors in the decoding of inference. To resolve this issue, ‘schedule sampling’ was introduced (<xref rid="btaa953-B3" ref-type="bibr">Bengio <italic toggle="yes">et al.</italic>, 2015</xref>). Scheduled sampling is a technique used in a training phase, and it randomly samples the previously inferred token instead of sampling from the ground truth. Halcyon used this technique in the training phase against a longer input signal (3000 values long) with a sampling ratio of 0.3.</p>
      </sec>
    </sec>
    <sec>
      <title>2.2 Data preparation</title>
      <sec>
        <label>2.2.1</label>
        <title>ONT MinION and Illumina sequencing</title>
        <p>Genomic DNA (#NA18943) used in the HapMap project was purchased from the Coriell Institute (Camden, NJ). For MinION sequencing, a sequencing library was prepared from 1.5 µg of the DNA using Ligation Sequencing Kit 1D (SQK-LSK108; ONT, Oxford, UK) and Library Loading Bead Kit (EXP-LLB001; ONT) according to the manufacturer’s instructions. The library was loaded onto the R9.4 flow cell of the MinION sequencing device (ONT) and sequenced for 48 h. A total of 11 runs of MinION sequencing were conducted. For Illumina sequencing, a sequencing library was prepared from 200 ng of the DNA using the TruSeq Nano DNA Library Prep Kit (Illumina, San Diego, CA). Sequencing was performed with paired-end reads of 101 bp on a HiSeq 2500 platform according to the manufacturer’s instructions (Illumina).</p>
      </sec>
      <sec>
        <label>2.2.2</label>
        <title>Labeling of raw signals</title>
        <p>Taiyaki (v5.1.0), the training models for basecalling Oxford Nanopore reads, was used to obtain labeled sequences. By using Taiyaki, nanopore raw signals are divided into segments, each of which corresponds to one nucleotide. By using such labeled reads, the arbitrary length of signals with a matched nucleotide sequence is easily obtained. We generated labeled signals with a length of 1000 and those with a length of 3000.</p>
      </sec>
      <sec>
        <label>2.2.3</label>
        <title>Training and Validation dataset</title>
        <p>Halcyon was trained and evaluated in a hold-out validation scheme. Unlike other general machine learning problems such as image recognition, it is inappropriate to divide the dataset into a training dataset and a test dataset. Among the obtained nanopore raw reads, some reads are from the same region of a human whole genome sequence. If such sequences exist in both the training and test datasets, correctly evaluating the generalization performance of nanopore basecalling would be impossible because trivial overfitting to patterns of consecutive nucleotide sequences of a human genome would also contribute to an accurate basecalling. Therefore, a training dataset was defined as paired signals and nucleotide sequences aligned to even-numbered chromosomes (i.e. chr2, chr4,…, chr22), and the test dataset as those aligned to odd-numbered chromosomes (i.e. chr1, chr3, …, chr21).</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 Transfer learning against different input lengths</title>
      <p>Halcyon was trained against different lengths of signals in a transfer-learning scheme to train the model against longer inputs effectively because starting with longer inputs might render attention-based training difficult. Therefore, Halcyon was trained against 1000-value-long signals and then against 3000-value-long signals. Such transfer learning was possible because (i) RNN-based encoders and decoders are applicable to inputs and outputs with different lengths, which are attributed to recurrent RNN cells, and (ii) parameters of CNN are fully dependent on the convolution kernels, the parameters of which are shared along the timestep axis.</p>
    </sec>
    <sec>
      <title>2.4 Inference</title>
      <p>In basecalling test data with arbitrary lengths, each set of input current signals was segmented into 3000-value-long signals with 800-value-long overlaps. These segmented reads were basecalled independently and merged into a single nucleotide sequence. In merging neighbor reads, pairwise local alignment against sequences supposed to be overlapped was conducted. A match score of +4, a mismatch penalty of -4.5, and gap/extend penalties of -5/-3 were used in the pairwise alignment.</p>
    </sec>
    <sec>
      <title>2.5 Evaluation</title>
      <p>The performance of Halcyon was compared with that of other existing basecallers with two viewpoints (i) ‘Individual read accuracy’: how accurately can each model basecall an individual sequence, and (ii) ‘SNV detection rate’: how accurately can SNVs be detected using whole basecalled sequences obtained from each model.</p>
      <p>We selected Guppy [v3.6.0], Bonito[v0.1.5], Chiron [v.0.5.1] (<xref rid="btaa953-B26" ref-type="bibr">Teng <italic toggle="yes">et al.</italic>, 2018</xref>) and DeepNano [latest version from <ext-link xlink:href="https://bitbucket.org/vboza/deepnano" ext-link-type="uri">https://bitbucket.org/vboza/deepnano</ext-link>] (<xref rid="btaa953-B4" ref-type="bibr">Boža <italic toggle="yes">et al.</italic>, 2017</xref>) as basecallers for comparison. Guppy and Bonito were selected as basecallers developed by ONT officially, and the others were selected as third-party basecallers.</p>
      <sec>
        <label>2.5.1</label>
        <title>Read accuracy</title>
        <p>The performance of basecalling for an individual example of an input current signal can be measured by calculating similarity between a basecalled sequence and the corresponding ground truth sequence. We defined the similarity according to the following criteria that can be calculated after conducting pairwise alignment between the two sequences; (i) the ratio of the number of nucleotide bases accurately basecalled calculated as <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>No</mml:mtext><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mtext>of</mml:mtext><mml:mo> </mml:mo><mml:mtext>correct</mml:mtext><mml:mo> </mml:mo><mml:mtext>matched</mml:mtext><mml:mo> </mml:mo><mml:mtext>bases</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mrow><mml:mtext>No</mml:mtext><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mtext>of</mml:mtext><mml:mo> </mml:mo><mml:mtext>all</mml:mtext><mml:mo> </mml:mo><mml:mtext>matched</mml:mtext><mml:mo> </mml:mo><mml:mtext>bases</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, (ii) the ratio of the number of inserted nucleotide bases calculated as <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>No</mml:mtext><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mtext>of</mml:mtext><mml:mo> </mml:mo><mml:mtext>inserted</mml:mtext><mml:mo> </mml:mo><mml:mtext>bases</mml:mtext><mml:mo> </mml:mo><mml:mtext>in</mml:mtext><mml:mo> </mml:mo><mml:mtext>basecalled</mml:mtext><mml:mo> </mml:mo><mml:mtext>sequence</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mrow><mml:mtext>No</mml:mtext><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mtext>of</mml:mtext><mml:mo> </mml:mo><mml:mtext>bases</mml:mtext><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mtext>in</mml:mtext><mml:mo> </mml:mo><mml:mtext>reference</mml:mtext><mml:mo> </mml:mo><mml:mtext>sequence</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> and (iii) the ratio of number of deleted bases calculated as <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>No</mml:mtext><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mtext>of</mml:mtext><mml:mo> </mml:mo><mml:mtext>deleted</mml:mtext><mml:mo> </mml:mo><mml:mtext>bases</mml:mtext><mml:mo> </mml:mo><mml:mtext>in</mml:mtext><mml:mo> </mml:mo><mml:mtext>basecalled</mml:mtext><mml:mo> </mml:mo><mml:mtext>sequence</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mrow><mml:mtext>No</mml:mtext><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mtext>of</mml:mtext><mml:mo> </mml:mo><mml:mtext>bases</mml:mtext><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mtext>in</mml:mtext><mml:mo> </mml:mo><mml:mtext>reference</mml:mtext><mml:mo> </mml:mo><mml:mtext>sequence</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. These metrics were calculated by aligning basecalled reads from each basecaller back to the reference sequence using minimap2 (<xref rid="btaa953-B17" ref-type="bibr">Li, 2018</xref>).</p>
      </sec>
      <sec>
        <label>2.5.2</label>
        <title>SNV detection</title>
        <p>SNV detection performance was measured by comparing the SNVs detected using whole nanopore basecalled reads with those detected using whole short read sequences. As short read sequences are highly accurate, we used the results as ground truths. SNVs were detected using basecalled nanopore reads obtained from our basecaller and the other baselines, each of which was compared with the ground truth SNVs.</p>
        <p>Short-read sequences were aligned to the reference sequence using BWA MEM and then processed by Strelka2 (<xref rid="btaa953-B15" ref-type="bibr">Kim <italic toggle="yes">et al.</italic>, 2018</xref>), a fast and accurate variant caller. Resultant SNVs were then filtered to extract only SNVs with high quality (QUAL &gt; 500). Nanopore basecalled reads were aligned using minimap2 (<xref rid="btaa953-B10" ref-type="bibr">Edge and Bansal, 2019</xref>) and SNVs were detected by LongShot (<xref rid="btaa953-B10" ref-type="bibr">Edge and Bansal, 2019</xref>). SNV detection recall and precision are calculated by using hap.py (v0.3.8) (<xref rid="btaa953-B16" ref-type="bibr">Krusche <italic toggle="yes">et al.</italic>, 2019</xref>). True positive rates given SNV positions for each depth (depth 6 20) are also calculated using the tool for each basecaller.</p>
      </sec>
      <sec>
        <label>2.5.3</label>
        <title>Basecalling speed</title>
        <p>Basecalling speed of five basecallers are measured (i) using only 1-threaded CPU only and (ii) using 5-threaded CPU and 1 core of GPU in Ubuntu 18.04.2 LTS x86 64 bit 257606MiB RAM with CPU: Intel Xeon Gold 6130 @ 3.700 GHz, and GPU: NVIDIA Quadro GV100.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <p>Whole genome sequencing was conducted using ONT’s MinION device against one human sample, with these reads then being used to train the neural network-based basecaller and evaluate the basecalling performance. To obtain matched raw signals and the corresponding nucleotide sequences for training, we used Taiyaki, ONT’s training model. The statistics of resulting reads by Taiyaki are shown in <xref rid="btaa953-T1" ref-type="table">Table 1</xref>. These labeled raw reads were then divided into training/test datasets according to the chromosomes.</p>
    <table-wrap position="float" id="btaa953-T1">
      <label>Table 1.</label>
      <caption>
        <p>Metrics of all 11 runs of MinION sequencing</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="center" span="1"/>
          <col valign="top" align="char" char="±" span="1"/>
          <col valign="top" align="char" char="±" span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th rowspan="1" colspan="1">MinION run</th>
            <th rowspan="1" colspan="1">Number of reads</th>
            <th rowspan="1" colspan="1">Signals length</th>
            <th rowspan="1" colspan="1">Nucleotide length</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">RUN 1</td>
            <td rowspan="1" colspan="1">198 318</td>
            <td rowspan="1" colspan="1">50 408 ± 37 154</td>
            <td rowspan="1" colspan="1">4762 ± 3577</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">RUN 2</td>
            <td rowspan="1" colspan="1">90 619</td>
            <td rowspan="1" colspan="1">53 587 ± 42 081</td>
            <td rowspan="1" colspan="1">4700 ± 4220</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">RUN 3</td>
            <td rowspan="1" colspan="1">720 885</td>
            <td rowspan="1" colspan="1">65 579 ± 41 308</td>
            <td rowspan="1" colspan="1">6724 ± 4345</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">RUN 4</td>
            <td rowspan="1" colspan="1">605 642</td>
            <td rowspan="1" colspan="1">72 292 ± 74 283</td>
            <td rowspan="1" colspan="1">7040 ± 7380</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">RUN 5</td>
            <td rowspan="1" colspan="1">541 783</td>
            <td rowspan="1" colspan="1">76 314 ± 76 775</td>
            <td rowspan="1" colspan="1">7123 ± 7347</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">RUN 6</td>
            <td rowspan="1" colspan="1">255 240</td>
            <td rowspan="1" colspan="1">75 450 ± 79 599</td>
            <td rowspan="1" colspan="1">6795 ± 7372</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">RUN 7</td>
            <td rowspan="1" colspan="1">665 879</td>
            <td rowspan="1" colspan="1">82 656 ± 82 728</td>
            <td rowspan="1" colspan="1">7503 ± 7715</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">RUN 8</td>
            <td rowspan="1" colspan="1">1 016 413</td>
            <td rowspan="1" colspan="1">72 082 ± 42 833</td>
            <td rowspan="1" colspan="1">6413 ± 3905</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">RUN 9</td>
            <td rowspan="1" colspan="1">946 914</td>
            <td rowspan="1" colspan="1">72 807 ± 44 096</td>
            <td rowspan="1" colspan="1">6299 ± 3929</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">RUN 10</td>
            <td rowspan="1" colspan="1">569 715</td>
            <td rowspan="1" colspan="1">72 186 ± 43 109</td>
            <td rowspan="1" colspan="1">6316 ± 3866</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">RUN 11</td>
            <td rowspan="1" colspan="1">220 199</td>
            <td rowspan="1" colspan="1">70 420 ± 46 432</td>
            <td rowspan="1" colspan="1">5825 ± 3905</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="tblfn1">
          <p><italic toggle="yes">Note</italic>: The number of reads obtained in each run, a mean and a standard deviation of lengths of raw signals and the lengths of nucleotides basecalled by Guppy (exploited by Taiyaki) observed in each run are also shown.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <p>The performance of Halcyon, was measured by comparing it with the performance of Guppy, Bonito, Chiron and DeepNano. The accuracy of basecallers in an individual read resolution was measured by basecalling all raw signals in a test dataset and aligning these reads to the reference sequence by minimap2 (<xref rid="btaa953-T2" ref-type="table">Table 2</xref>). <xref rid="btaa953-F1" ref-type="fig">Figure 1a–c</xref> show the distribution of read identity, insertion error rate and deletion error rate of reads from the five evaluated basecallers. These metrics collectively constitute a heuristic measurement for read precision. The results showed that Halcyon achieved competitive performance against ONT’s cutting edge basecallers and outperformed the other third-party basecallers. Among baseline basecallers, Guppy achieved maximal performance, which is in agreement with recently reported results (<xref rid="btaa953-B28" ref-type="bibr">Wick <italic toggle="yes">et al.</italic>, 2019</xref>).</p>
    <table-wrap position="float" id="btaa953-T2">
      <label>Table 2.</label>
      <caption>
        <p>Read metrics for reads basecalled by five different basecallers</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="center" span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="±" span="1"/>
          <col valign="top" align="center" span="1"/>
          <col valign="top" align="center" span="1"/>
          <col valign="top" align="center" span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th rowspan="1" colspan="1">Basecaller</th>
            <th rowspan="1" colspan="1">Total reads</th>
            <th rowspan="1" colspan="1">Total yield (Gb)</th>
            <th rowspan="1" colspan="1">Read length</th>
            <th rowspan="1" colspan="1">Read identity</th>
            <th rowspan="1" colspan="1">Insertion rate</th>
            <th rowspan="1" colspan="1">Deletion rate</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">Halcyon</td>
            <td rowspan="1" colspan="1">3 225 205</td>
            <td rowspan="1" colspan="1">20.5</td>
            <td rowspan="1" colspan="1">6359 ± 5702</td>
            <td rowspan="1" colspan="1">0.894 ± 0.084</td>
            <td rowspan="1" colspan="1">0.028 ± 0.023</td>
            <td rowspan="1" colspan="1">0.041 ± 0.043</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Guppy</td>
            <td rowspan="1" colspan="1">3 150 600</td>
            <td rowspan="1" colspan="1">20.5</td>
            <td rowspan="1" colspan="1">6519 ± 5748</td>
            <td rowspan="1" colspan="1">0.905 ± 0.080</td>
            <td rowspan="1" colspan="1">0.021 ± 0.018</td>
            <td rowspan="1" colspan="1">0.041 ± 0.044</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Bonito</td>
            <td rowspan="1" colspan="1">3 160 225</td>
            <td rowspan="1" colspan="1">20.3</td>
            <td rowspan="1" colspan="1">6410 ± 5664</td>
            <td rowspan="1" colspan="1">0.902 ± 0.080</td>
            <td rowspan="1" colspan="1">0.020 ± 0.016</td>
            <td rowspan="1" colspan="1">0.045 ± 0.050</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Chiron</td>
            <td rowspan="1" colspan="1">2 129 764</td>
            <td rowspan="1" colspan="1">17.4</td>
            <td rowspan="1" colspan="1">8161 ± 5384</td>
            <td rowspan="1" colspan="1">0.800 ± 0.061</td>
            <td rowspan="1" colspan="1">0.047 ± 0.019</td>
            <td rowspan="1" colspan="1">0.072 ± 0.033</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Deepnano</td>
            <td rowspan="1" colspan="1">2 783 926</td>
            <td rowspan="1" colspan="1">18.4</td>
            <td rowspan="1" colspan="1">6606 ± 5616</td>
            <td rowspan="1" colspan="1">0.805 ± 0.055</td>
            <td rowspan="1" colspan="1">0.042 ± 0.014</td>
            <td rowspan="1" colspan="1">0.075 ± 0.030</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="tblfn2">
          <p><italic toggle="yes">Note</italic>: Except for total reads and total yield, the mean and standard deviation of each measurement is described. Read identity, insertion rate, deletion rate are obtained by aligning basecalled reads to reference by minimap2.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <fig position="float" id="btaa953-F1">
      <label>Fig. 1.</label>
      <caption>
        <p>Overview of the network architecture of Halcyon from the input (nanopore raw signals) to the output (nucleotide sequence). Each convolution component is composed of a one-dimensional convolution layer with a rectified linear unit (ReLU) activation function followed by a batch normalization layer. A semantic relationship between the last layer among five stacked bidirectional LSTM encoding layers and the first layer among five stacked LSTM decoding layers is comprehended by monotonic attention</p>
      </caption>
      <graphic xlink:href="btaa953f1" position="float"/>
    </fig>
    <p>Although these results are obtained using the test dataset, they did not conclusively display Halcyon’s superiority in nanopore sequencing for the following reasons: (i) the basecalled reads in the test data were aligned to the reference sequences, which did not consider individual genome variation such as SNVs, and (ii) accuracy in a individual read resolution did not necessarily imply consensus accuracy, which is more valuable in practice, as actual sequencing analyses involve aggregating multiple-coverage sequences to obtain a consensus result. Therefore, we assessed SNV detection performance by utilizing short read sequence data. Whole genome sequencing against the same sample was performed using Illumina HiSeq. The obtained reads were aligned to the reference sequences using the Burrows-Wheeler aligner [BWA (<xref rid="btaa953-B18" ref-type="bibr">Li and Durbin, 2010</xref>)], and then SNVs were detected using Strelka2 (<xref rid="btaa953-B15" ref-type="bibr">Kim <italic toggle="yes">et al.</italic>, 2018</xref>). For nanopore sequences, SNVs were detected by using LongShot (<xref rid="btaa953-B10" ref-type="bibr">Edge and Bansal, 2019</xref>).</p>
    <p>The SNV detection performance was measured in recall and precision obtained by hap.py, haplotype comparison tools by Illumina (<xref rid="btaa953-B16" ref-type="bibr">Krusche <italic toggle="yes">et al.</italic>, 2019</xref>). The evaluation pipeline is shown in <xref rid="btaa953-F2" ref-type="fig">Figure 2b</xref>. The resulting SNV detection recall and precision are illustrated in <xref rid="btaa953-F3" ref-type="fig">Figure 3f</xref>. In addition to the performance in individual read resolution, Halcyon achieved competitive performance against ONT’s basecallers. These results demonstrated that the performance of Halcyon was not overfitted against the utilized reference sequences, and the model would be the most useful in practical nanopore sequencing analyses.</p>
    <fig position="float" id="btaa953-F2">
      <label>Fig. 2.</label>
      <caption>
        <p>(<bold>a</bold>) Overview of preparation of training datasets using ONT’s retraining model, Taiyaki. Labeled reads obtained by Taiyaki are then split into fixed-length raw signals and corresponding nucleotide sequences. (<bold>b</bold>) Overview of evaluation of different basecallers in terms of SNV-detection performance assuming short-read sequencing as the ground truth</p>
      </caption>
      <graphic xlink:href="btaa953f2" position="float"/>
    </fig>
    <fig position="float" id="btaa953-F3">
      <label>Fig. 3.</label>
      <caption>
        <p>Individual read statistics obtained by aligning basecalled reads to the reference sequence with minimap2. Distributions of (<bold>a</bold>) read identities, (<bold>b</bold>) insertion error rates and (<bold>c</bold>) deletion error rates calculated over all basecalled reads are illustrated using letter-value plots. The SNV detection rate measured by comparing SNVs detected by LongShot to those detected by Strelka2 using short-read sequences. (<bold>d</bold>) SNV detection rate overall each chromosome, and (<bold>e</bold>) true positive rate of SNV-detection for each read depth (6 20). Basecalling speed measured in terms of the number of nucleotide basecalled in a second. Speed of basecalling (<bold>f</bold>) measured using CPU with a single thread and (<bold>g</bold>) that measured using a single GPU and CPU with five threads</p>
      </caption>
      <graphic xlink:href="btaa953f3" position="float"/>
    </fig>
    <p>Further we investigated SNV detection performance for each read depth. Such investigation is important because (i) in the actual clinical application of nanopore reads, it might be necessary to create an important decision relying on limited coverage data, and (ii) observing the saturation of SNV detection rate along with read depth may aid in the determination of nanopore sequencing strategy. The result is shown in <xref rid="btaa953-F3" ref-type="fig">Figure 3g</xref>. Halcyon consistently performed similarly to ONT’s basecaller, with results similar to those obtained in <xref rid="btaa953-F3" ref-type="fig">Figure 3f</xref>. As basecalling speed is an important aspect, we measured the number of nucleotides basecalled in a second. <xref rid="btaa953-F3" ref-type="fig">Figure 3f and g</xref> showed the result using CPU and GPU respectively. The basecalling speed of Halcyon is slower than other basecaller except for Chiron in the CPU, and the slowest in the GPU.</p>
    <p>One advantage of incorporating the attention mechanism is that one can understand semantic correspondence between raw signals and basecalled sequences. As shown in <xref rid="btaa953-F4" ref-type="fig">Figure 4</xref>, an attention matrix obtained in an inference phase represents the information, where you can understand which part of the raw signals is referred by Halcyon to infer a certain nucleotide. Retaining this information will be helpful when investigating a single nucleotide in detail, such as for the detection of cytosine methylation.</p>
    <fig position="float" id="btaa953-F4">
      <label>Fig. 4.</label>
      <caption>
        <p>Actual row signal input (top) and an attention matrix obtained in the basecalling phase to infer nucleotides from the given signals (bottom). The number of signal values measured during single nucleotide passage through a pore changes rapidly at a certain point (indicated by a circle in the figure). In the corresponding part of the attention matrix, the gradient of attention transition speed also changes rapidly</p>
      </caption>
      <graphic xlink:href="btaa953f4" position="float"/>
    </fig>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>We developed a novel basecaller incorporating state-of-the-art neural network techniques commonly utilized for sequence-to-sequence learning. Our proposed basecaller, Halcyon, achieved high performance for individual read resolution and the detection of SNVs using multiple reads. Given the recent advances in downstream analyses using long read sequences such as the detection of cytosine methylation and structural variations, obtaining accurate reads with semantic correspondence between raw signals and the reads using Halcyon would accelerate such applications and lead to biologically significant findings. Furthermore, as models of nanopore basecallers officially developed by ONT are not public, providing the neural network specification of a well-working basecaller will facilitate the development of a more sophisticated basecaller in the future. </p>
    <p><italic toggle="yes">Financial Support</italic>: none declared.</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa953-B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Abadi</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>).  Tensorflow: A system for large-scale machine learning. In: <italic toggle="yes">12th USENIX Symposium on OSDI,</italic> pp. 265–283.</mixed-citation>
    </ref>
    <ref id="btaa953-B2">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bahdanau</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) Neural machine translation by jointly learning to align and translate. In: <italic toggle="yes">ICLR, 2015</italic>.</mixed-citation>
    </ref>
    <ref id="btaa953-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bengio</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) Scheduled sampling for sequence prediction with recurrent neural networks. In: <italic toggle="yes">NeurIPS</italic>, Vol. <volume>28</volume>, pp. <fpage>1171</fpage>–<lpage>1179</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa953-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boža</surname><given-names>V.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) 
<article-title>DeepNano: deep recurrent neural networks for base calling in MinION Nanopore reads</article-title>. <source>PLoS One</source>, <volume>12</volume>, <fpage>e0178751</fpage>.<pub-id pub-id-type="pmid">28582401</pub-id></mixed-citation>
    </ref>
    <ref id="btaa953-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>M.X.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) The best of both worlds: combining recent advances in neural machine translation. <italic toggle="yes">arXiv preprint arXiv 1804.09849.</italic></mixed-citation>
    </ref>
    <ref id="btaa953-B6">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Chiu</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) State-of-the-art speech recognition with sequence-to-sequence models. In: <italic toggle="yes">2018 ICASSP,</italic> pp. <fpage>4774</fpage>–<lpage>4778</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa953-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chorowski</surname><given-names>J.K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) Attention-based models for speech recognition. In: <italic toggle="yes">NeurIPS,</italic> Vol. <volume>28</volume>, pp. <fpage>577</fpage>–<lpage>585</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa953-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cretu Stancu</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) 
<article-title>Mapping and phasing of structural variation in patient genomes using nanopore sequencing</article-title>. <source>Nat. Commun</source>., <volume>8</volume>, <fpage>1</fpage>–<lpage>13</lpage>.<pub-id pub-id-type="pmid">28232747</pub-id></mixed-citation>
    </ref>
    <ref id="btaa953-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Coster</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>Structural variants identified by Oxford Nanopore PromethION sequencing of the human genome</article-title>. <source>Genome Res</source>., <volume>29</volume>, <fpage>1178</fpage>–<lpage>1187</lpage>.<pub-id pub-id-type="pmid">31186302</pub-id></mixed-citation>
    </ref>
    <ref id="btaa953-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Edge</surname><given-names>P.</given-names></string-name>, <string-name><surname>Bansal</surname><given-names>V.</given-names></string-name></person-group> (<year>2019</year>) 
<article-title>Longshot enables accurate variant calling in diploid genomes from single-molecule long read sequencing</article-title>. <source>Nat. Commun</source>., <volume>10</volume>, <fpage>4660</fpage>.<pub-id pub-id-type="pmid">31604920</pub-id></mixed-citation>
    </ref>
    <ref id="btaa953-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gong</surname><given-names>L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Picky comprehensively detects high-resolution structural variants in nanopore long reads</article-title>. <source>Nat. Methods</source>, <volume>15</volume>, <fpage>455</fpage>–<lpage>460</lpage>.<pub-id pub-id-type="pmid">29713081</pub-id></mixed-citation>
    </ref>
    <ref id="btaa953-B12">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Graves</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2006</year>) Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In: <italic toggle="yes">ICML ’06</italic>, pp. <fpage>369</fpage>–<lpage>376</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa953-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ioffe</surname><given-names>S.</given-names></string-name>, <string-name><surname>Szegedy</surname><given-names>C.</given-names></string-name></person-group> (<year>2015</year>) Batch normalization: accelerating deep network training by reducing internal covariate shift. In: <italic toggle="yes">PMLR,</italic> Vol. <volume>37</volume>, pp. <fpage>448</fpage>–<lpage>456</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa953-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jain</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Nanopore sequencing and assembly of a human genome with ultra-long reads</article-title>. <source>Nat. Biotechnol</source>., <volume>36</volume>, <fpage>338</fpage>–<lpage>345</lpage>.<pub-id pub-id-type="pmid">29431738</pub-id></mixed-citation>
    </ref>
    <ref id="btaa953-B15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kim</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) Strelka2: fast and accurate variant calling for clinical sequencing applications. <italic toggle="yes">Nat. Methods</italic>, <bold>15</bold>, <fpage>591</fpage>–<lpage>594</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa953-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krusche</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal>; for Genomics, t. G. A. and Team, H. B. (<year>2019</year>) 
<article-title>Best practices for benchmarking germline small-variant calls in human genomes</article-title>. <source>Nat. Biotechnol</source>., <volume>37</volume>, <fpage>555</fpage>–<lpage>560</lpage>.<pub-id pub-id-type="pmid">30858580</pub-id></mixed-citation>
    </ref>
    <ref id="btaa953-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>H.</given-names></string-name></person-group> (<year>2018</year>) 
<article-title>Minimap2: pairwise alignment for nucleotide sequences</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>3094</fpage>–<lpage>3100</lpage>.<pub-id pub-id-type="pmid">29750242</pub-id></mixed-citation>
    </ref>
    <ref id="btaa953-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>H.</given-names></string-name>, <string-name><surname>Durbin</surname><given-names>R.</given-names></string-name></person-group> (<year>2010</year>) 
<article-title>Fast and accurate long-read alignment with Burrows–Wheeler transform</article-title>. <source>Bioinformatics</source>, <volume>26</volume>, <fpage>589</fpage>–<lpage>595</lpage>.<pub-id pub-id-type="pmid">20080505</pub-id></mixed-citation>
    </ref>
    <ref id="btaa953-B19">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Luong</surname><given-names>M.-T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) Effective approaches to attention-based neural machine translation. In: <italic toggle="yes">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</italic>, pp. <fpage>1412</fpage>–<lpage>1421</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa953-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Payne</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>BulkVis: a graphical viewer for Oxford nanopore bulk FAST5 files</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>2193</fpage>–<lpage>2198</lpage>.<pub-id pub-id-type="pmid">30462145</pub-id></mixed-citation>
    </ref>
    <ref id="btaa953-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raffel</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) Online and linear-time attention by enforcing monotonic alignments. In: <italic toggle="yes">ICML ’17,</italic> Vol. <volume>70</volume>, pp. 2837–2846<italic toggle="yes">.</italic></mixed-citation>
    </ref>
    <ref id="btaa953-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schuster</surname><given-names>M.</given-names></string-name>, <string-name><surname>Paliwal</surname><given-names>K.K.</given-names></string-name></person-group> (<year>1997</year>) 
<article-title>Bidirectional recurrent neural networks</article-title>. <source>IEEE Trans. Signal Process</source>., <volume>45</volume>, <fpage>2673</fpage>–<lpage>2681</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa953-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simpson</surname><given-names>J.T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) 
<article-title>Detecting DNA cytosine methylation using nanopore sequencing</article-title>. <source>Nat. Methods</source>, <volume>14</volume>, <fpage>407</fpage>–<lpage>410</lpage>.<pub-id pub-id-type="pmid">28218898</pub-id></mixed-citation>
    </ref>
    <ref id="btaa953-B24">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Stoiber</surname><given-names>M.</given-names></string-name>, <string-name><surname>Brown</surname><given-names>J.</given-names></string-name></person-group> (<year>2017</year>) BasecRAWller: streaming nanopore basecalling directly from raw signal. <italic toggle="yes">bioRxiv, 133058.</italic></mixed-citation>
    </ref>
    <ref id="btaa953-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sutskever</surname><given-names>I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) Sequence to sequence learning with neural networks. In: <italic toggle="yes">NeurIPS</italic>, Vol. <volume>27</volume>, pp. <fpage>3104</fpage>–<lpage>3112</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa953-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teng</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Chiron: translating nanopore raw signal directly into nucleotide sequence using deep learning</article-title>. <source>GigaScience</source>, <volume>7</volume>, <fpage>giy037</fpage>.<pub-id pub-id-type="pmid">29648610</pub-id></mixed-citation>
    </ref>
    <ref id="btaa953-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Wavenano: a signal-level nanopore base-caller via simultaneous prediction of nucleotide labels and move labels through bi-directional wavenets</article-title>. <source>Quant. Biol</source>., <volume>6</volume>, <fpage>359</fpage>–<lpage>368</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa953-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wick</surname><given-names>R.R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>Performance of neural network basecalling tools for oxford nanopore sequencing</article-title>. <source>Genome Biol</source>., <volume>20</volume>, <fpage>129</fpage>.<pub-id pub-id-type="pmid">31234903</pub-id></mixed-citation>
    </ref>
    <ref id="btaa953-B29">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zeyer</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) Improved training of end-to-end attention models for speech recognition. In: <italic toggle="yes">Proceedings of Interspeech</italic>, pp. <fpage>7</fpage>–<lpage>11</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
