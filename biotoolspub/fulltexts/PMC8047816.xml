<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//ACS//DTD ACS Journal DTD v1.02 20061031//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName ACSJournal-v102.dtd?>
<?SourceDTD.Version 1.02?>
<?ConverterInfo.XSLTName acs2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Chem Theory Comput</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Chem Theory Comput</journal-id>
    <journal-id journal-id-type="publisher-id">ct</journal-id>
    <journal-id journal-id-type="coden">jctcce</journal-id>
    <journal-title-group>
      <journal-title>Journal of Chemical Theory and Computation</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1549-9618</issn>
    <issn pub-type="epub">1549-9626</issn>
    <publisher>
      <publisher-name>American
Chemical Society</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8047816</article-id>
    <article-id pub-id-type="pmid">33755446</article-id>
    <article-id pub-id-type="doi">10.1021/acs.jctc.0c01164</article-id>
    <article-categories>
      <subj-group>
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Tinker-HP: Accelerating Molecular Dynamics Simulations
of Large Complex Systems with Advanced Point Dipole Polarizable Force
Fields Using GPUs and Multi-GPU Systems</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="ath1">
        <name>
          <surname>Adjoua</surname>
          <given-names>Olivier</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes" id="ath2">
        <name>
          <surname>Lagardère</surname>
          <given-names>Louis</given-names>
        </name>
        <xref rid="cor1" ref-type="other">*</xref>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath3">
        <name>
          <surname>Jolly</surname>
          <given-names>Luc-Henri</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath4">
        <name>
          <surname>Durocher</surname>
          <given-names>Arnaud</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">§</xref>
      </contrib>
      <contrib contrib-type="author" id="ath5">
        <name>
          <surname>Very</surname>
          <given-names>Thibaut</given-names>
        </name>
        <xref rid="aff4" ref-type="aff">∥</xref>
      </contrib>
      <contrib contrib-type="author" id="ath6">
        <name>
          <surname>Dupays</surname>
          <given-names>Isabelle</given-names>
        </name>
        <xref rid="aff4" ref-type="aff">∥</xref>
      </contrib>
      <contrib contrib-type="author" id="ath7">
        <name>
          <surname>Wang</surname>
          <given-names>Zhi</given-names>
        </name>
        <xref rid="aff5" ref-type="aff">⊥</xref>
      </contrib>
      <contrib contrib-type="author" id="ath8">
        <name>
          <surname>Inizan</surname>
          <given-names>Théo Jaffrelot</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath9">
        <name>
          <surname>Célerse</surname>
          <given-names>Frédéric</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff6" ref-type="aff">∇</xref>
      </contrib>
      <contrib contrib-type="author" id="ath10">
        <name>
          <surname>Ren</surname>
          <given-names>Pengyu</given-names>
        </name>
        <xref rid="aff7" ref-type="aff">#</xref>
      </contrib>
      <contrib contrib-type="author" id="ath11">
        <name>
          <surname>Ponder</surname>
          <given-names>Jay W.</given-names>
        </name>
        <xref rid="aff5" ref-type="aff">⊥</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes" id="ath12">
        <name>
          <surname>Piquemal</surname>
          <given-names>Jean-Philip</given-names>
        </name>
        <xref rid="cor2" ref-type="other">*</xref>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff7" ref-type="aff">#</xref>
      </contrib>
      <aff id="aff1"><label>†</label><institution>Sorbonne
Université</institution>, LCT, UMR 7616
CNRS, F-75005 Paris, <country>France</country></aff>
      <aff id="aff2"><label>‡</label><institution>Sorbonne
Université</institution>, IP2CT, FR2622 CNRS, F-75005 Paris, <country>France</country></aff>
      <aff id="aff3"><label>§</label><institution>Eolen</institution>, 37-39 Rue Boissière, 75116 Paris, <country>France</country></aff>
      <aff id="aff4"><label>∥</label>IDRIS, <institution>CNRS</institution>, 91403 Orsay, <country>France</country></aff>
      <aff id="aff5"><label>⊥</label>Department
of Chemistry, <institution>Washington University in Saint
Louis</institution>, Saint Louis, Missouri 63110, <country>United
States</country></aff>
      <aff id="aff6"><label>∇</label><institution>Sorbonne
Université</institution>, CNRS, IPCM, F-75005 Paris, <country>France</country></aff>
      <aff id="aff7"><label>#</label>Department
of Biomedical Engineering, <institution>The University
of Texas at Austin</institution>, Austin, Texas 78712, <country>United States</country></aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>*</label>E-mail: <email>louis.lagardere@sorbonne-universite.fr</email>.</corresp>
      <corresp id="cor2"><label>*</label>E-mail: <email>jean-philip.piquemal@sorbonne-universite.fr</email>.</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>23</day>
      <month>03</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <day>13</day>
      <month>04</month>
      <year>2021</year>
    </pub-date>
    <volume>17</volume>
    <issue>4</issue>
    <fpage>2034</fpage>
    <lpage>2053</lpage>
    <history>
      <date date-type="received">
        <day>06</day>
        <month>11</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 The Authors. Published
by American
Chemical Society</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>The Authors</copyright-holder>
      <license>
        <license-p>Permits non-commercial access and re-use, provided that author attribution and integrity are maintained; but does not permit creation of adaptations or other derivative works (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">https://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p content-type="toc-graphic">
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0023" id="ab-tgr1"/>
      </p>
      <p>We present the extension
of the Tinker-HP package (<mixed-citation id="bodycit1" publication-type="journal"><person-group><name><surname>Lagardère</surname></name></person-group>, <etal/><source>Chem. Sci.</source><year>2018</year>, <volume>9</volume>, <fpage>956</fpage>−<lpage>972</lpage><pub-id pub-id-type="pmid">29732110</pub-id></mixed-citation>) to the
use of Graphics Processing Unit (GPU)
cards to accelerate molecular dynamics simulations using polarizable
many-body force fields. The new high-performance module allows for
an efficient use of single- and multiple-GPU architectures ranging
from research laboratories to modern supercomputer centers. After
detailing an analysis of our general scalable strategy that relies
on O<sc>pen</sc>ACC and CUDA, we discuss the various capabilities
of the package. Among them, the multiprecision possibilities of the
code are discussed. If an efficient double precision implementation
is provided to preserve the possibility of fast reference computations,
we show that a lower precision arithmetic is preferred providing a
similar accuracy for molecular dynamics while exhibiting superior
performances. As Tinker-HP is mainly dedicated to accelerate simulations
using new generation point dipole polarizable force field, we focus
our study on the implementation of the AMOEBA model. Testing various
NVIDIA platforms including 2080Ti, 3090, V100, and A100 cards, we
provide illustrative benchmarks of the code for single- and multicards
simulations on large biosystems encompassing up to millions of atoms.
The new code strongly reduces time to solution and offers the best
performances to date obtained using the AMOEBA polarizable force field.
Perspectives toward the strong-scaling performance of our multinode
massive parallelization strategy, unsupervised adaptive sampling and
large scale applicability of the Tinker-HP code in biophysics are
discussed. The present software has been released in phase advance
on GitHub in link with the High Performance Computing community COVID-19
research efforts and is free for Academics (see <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/TinkerTools/tinker-hp">https://github.com/TinkerTools/tinker-hp</uri>).</p>
    </abstract>
    <custom-meta-group>
      <custom-meta>
        <meta-name>document-id-old-9</meta-name>
        <meta-value>ct0c01164</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>document-id-new-14</meta-name>
        <meta-value>ct0c01164</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>ccc-price</meta-name>
        <meta-value/>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p>Molecular dynamics
(MD) is a very active research field that is
continuously progressing.<sup><xref ref-type="bibr" rid="ref1">1</xref>,<xref ref-type="bibr" rid="ref2">2</xref></sup> Among various evolutions,
the definition of force fields themselves grows more complex. Indeed,
beyond the popular pairwise additive models<sup><xref ref-type="bibr" rid="ref3">3</xref>−<xref ref-type="bibr" rid="ref7">7</xref></sup> that remain extensively used, polarizable force field
(PFF) approaches are becoming increasingly mainstream and start to
be more widely adopted,<sup><xref ref-type="bibr" rid="ref8">8</xref>−<xref ref-type="bibr" rid="ref11">11</xref></sup> mainly because accounting for polarizability is often crucial for
complex applications and adding new physics to the model through the
use of many-body potentials can lead to significant accuracy enhancements.<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> Numerous approaches are currently under development
but a few methodologies such as the Drude<sup><xref ref-type="bibr" rid="ref12">12</xref>−<xref ref-type="bibr" rid="ref14">14</xref></sup> or the AMOEBA<sup><xref ref-type="bibr" rid="ref15">15</xref>−<xref ref-type="bibr" rid="ref17">17</xref></sup> models emerge. These models are more and more employed because of
the alleviation of their main bottleneck: their larger computational
cost compared to classical pairwise models. Indeed, the availability
of High Performance Computing (HPC) implementations of such models
within popular packages such as NAMD<sup><xref ref-type="bibr" rid="ref18">18</xref></sup> or
GROMACS<sup><xref ref-type="bibr" rid="ref19">19</xref></sup> for Drude or Tinker-HP<sup><xref ref-type="bibr" rid="ref20">20</xref></sup> for AMOEBA fosters the diffusion of these new
generation techniques within the research community. This paper is
dedicated to the evolution of the Tinker-HP package.<sup><xref ref-type="bibr" rid="ref20">20</xref></sup> The software, which is part of the Tinker distribution,<sup><xref ref-type="bibr" rid="ref21">21</xref></sup> was initially introduced as a double precision
massively parallel message passing interface (MPI) addition to Tinker
dedicated to the acceleration of the various PFFs and nonpolarizable
force fields (n-PFFs) present within the Tinker package. The code
was shown to be really efficient, being able to scale on up to tens
of thousand cores on modern petascale supercomputers.<sup><xref ref-type="bibr" rid="ref20">20</xref>,<xref ref-type="bibr" rid="ref22">22</xref></sup> Recently, it has been optimized on various platforms taking advantage
of vectorization and of the evolution of the recent CPUs (Central
Processing Units).<sup><xref ref-type="bibr" rid="ref22">22</xref></sup> However, in the last
15 years, the field has been increasingly using GPUs (Graphic Processor
Units)<sup><xref ref-type="bibr" rid="ref23">23</xref>−<xref ref-type="bibr" rid="ref25">25</xref></sup> taking advantage of low precision arithmetic. Indeed,
such platforms offer important computing capabilities at both low
cost and high energy efficiency allowing for reaching routine microsecond
simulations on standard GPU cards with pair potentials.<sup><xref ref-type="bibr" rid="ref24">24</xref>,<xref ref-type="bibr" rid="ref26">26</xref></sup> Regarding the AMOEBA polarizable force field, the OpenMM package<sup><xref ref-type="bibr" rid="ref27">27</xref></sup> was the first to propose an AMOEBA-GPU library
that was extensively used within Tinker through the Tinker-OpenMM
GPU interface.<sup><xref ref-type="bibr" rid="ref28">28</xref></sup> The present contribution
aims to address two goals: (i) the design of an efficient native Tinker-HP
GPU implementation; (ii) the HPC optimization in a massively parallel
context to address both the use of research laboratories clusters
and modern multi-GPU pre-exascale supercomputer systems. The paper
is organized as follows. First, we will describe our O<sc>pen</sc>ACC port and its efficiency in double precision. After observing
the limitations of this implementation regarding the use of single
precision, we introduce a new CUDA approach and detail the various
parts of the code it concerns after a careful study of the precision.
In both cases, we present benchmarks of the new code on illustrative
large biosystems of increasing size on various NVIDIA platforms (including
RTX 2080Ti, 3090, Tesla V100 and A100 cards). Then, we explore how
to run on even larger systems and optimize memory management by making
use of latest tools such as NVSHMEM.<sup><xref ref-type="bibr" rid="ref29">29</xref></sup></p>
  </sec>
  <sec id="sec2">
    <title>OpenACC
Approach</title>
    <sec id="sec2.1">
      <title>Global Overview and Definitions</title>
      <p>Tinker-HP is a molecular
dynamics application with a MPI layer allowing a significant acceleration
on CPUs. The core of the application is based on the resolution of
the classical newton equations<sup><xref ref-type="bibr" rid="ref20">20</xref>,<xref ref-type="bibr" rid="ref30">30</xref></sup> given an interaction
potential (force field) between atoms. In practice, a molecular dynamic
simulation consists of the repetition of the call to an integrator
routine defining the changes of the positions and the velocities of
all the atoms of the simulated system between two consecutive time
steps. The same process is repeated as many times as needed until
the simulation duration is reached (see <xref rid="fig3" ref-type="fig">Figure <xref rid="fig3" ref-type="fig">3</xref></xref>). To distribute computations over the processes,
a traditional three-dimensional domain decomposition is performed
on the simulation box (<bold>Ω</bold>), which means that it
is divided in subdomains (<bold>ψ</bold>), each of which is
associated with a MPI process. Then, within each time step, positions
of the atoms and forces are exchanged between processes before and
after the computation of the forces. Additionally, small communications
are required after the update of the positions to deal with the fact
that an atom can change the subdomain during a time step. This workflow
is described in detail in ref (<xref ref-type="bibr" rid="ref20">20</xref>).</p>
      <p>In recent years a new paradigm has emerged to facilitate
computation and programming on GPU devices. In the rest of the text,
we will denote as <italic>kernel</italic> the smallest piece of code
made of instructions designed for a unique purpose. Thus, a succession
of kernels might constitute a <italic>routine</italic> and a <italic>program</italic> can be seen as a collection of routines designed
for a specific purpose. There are two types of kernels<list list-type="bullet"><list-item><p><italic>Serial</italic> kernels,
mostly used for variable
configuration</p></list-item><list-item><p><italic>Loops</italic> kernels,
operating on multiple
data sets</p></list-item></list></p>
      <p>This programming style, named
O<sc>pen</sc>ACC,<sup><xref ref-type="bibr" rid="ref32">32</xref>,<xref ref-type="bibr" rid="ref33">33</xref></sup> is a directive-based language
similar to the multithreading OpenMP
paradigm with an additional complexity level. Since a target kernel
is destined to be executed on GPUs, it becomes crucial to manage data
between both GPU and CPU platforms. At the most elementary level,
O<sc>pen</sc>ACC compiler interacts on a standard host (CPU) kernel
and generates a device (GPU) kernel using directives implemented to
describe its parallelism along with clauses to manage global data
behavior at both entry and exit point and/or kernel launch configuration
(<xref rid="fig1" ref-type="fig">Figure <xref rid="fig1" ref-type="fig">1</xref></xref>). This method
offers two major benefits. Unlike the low-level CUDA programming language,<sup><xref ref-type="bibr" rid="ref34">34</xref></sup> it takes only a few directives to generate a
device kernel. Second, the same kernel is compatible with both platforms,
CPUs and GPUs. The portability along with all the associated benefits
such as host debug is therefore ensured. However, there are some immediate
drawbacks mainly because CPUs and GPUs do not share the same architecture,
specifications, and features. Individual CPU cores benefit from a
significant optimization for serial tasks, a high clock frequency
and integrated vectorization instructions to increase processing speed.
GPUs on the other hand were developed and optimized from the beginning
for parallel tasks with numerous aggregations of low clock cores holding
multiple threads. This means that it may be necessary to reshape kernels
to fit device architecture in order to get appropriate acceleration.
Once we clearly exhibit a kernel parallelism and associate O<sc>pen</sc>ACC directives to offload it on a device, it should perform almost
as well as if it had been directly written in native CUDA. Still,
in addition to kernel launch instruction (performed by both O<sc>pen</sc>ACC and CUDA) before the appropriate execution, there is a global
data checking operation overhead that might slow down execution (<xref rid="fig1" ref-type="fig">Figure <xref rid="fig1" ref-type="fig">1</xref></xref>). However, it is
possible to overlap this operation using asynchronous device streams
in the kernel configuration (<xref rid="fig2" ref-type="fig">Figure <xref rid="fig2" ref-type="fig">2</xref></xref>). Under proper conditions and with directly parallel
kernels, O<sc>pen</sc>ACC can already lead to an efficient acceleration
close to the one reachable with CUDA.</p>
      <fig id="fig1" position="float">
        <label>Figure 1</label>
        <caption>
          <p>O<sc>pen</sc>ACC synchronous execution
model on test kernel <monospace>&lt;fill&gt;</monospace>.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0004" id="gr1" position="float"/>
      </fig>
      <fig id="fig2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>O<sc>pen</sc>ACC asynchronous execution on both kernels <monospace>&lt;test&gt;</monospace> and <monospace>&lt;test 1&gt;</monospace>.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0005" id="gr2" position="float"/>
      </fig>
      <p>In the following, we will say
that a kernel is <italic>semiparallel</italic> if one can find a partition
inside the instructions sequence that
does not share any dependency at all. A semiparallel kernel is consequently
defined <italic>parallel</italic> if all instructions in the partition
do not induce a race condition within its throughput.</p>
      <p>Once a
kernel is device compiled, its execution requires a configuration
defining the associated resources provided by the device. With O<sc>pen</sc>ACC, these resources are respectively the total number of
threads and the assignment stream. We can access the first one through
the <italic>gang</italic> and <italic>vector</italic> clauses attached
to a device compute region directive. A <monospace>gang</monospace> is a collection of vectors inside of which every thread can share
cache memory. All gangs run separately on device streaming multiprocessors
(SM) to process kernel instructions inside a stream where many other
kernels are sequentially queued. O<sc>pen</sc>ACC offers an intermediate
parallelism level between gang and vector called <monospace>worker</monospace>. This level can be seen as a gang subdivision.</p>
      <p>It is commonly
known that GPUs are inefficient for sequential execution
due to their latency. To cover up latency, each SM comes with a huge
register file and cache memory in order to hold and run as many vectors
as possible at the same time. Instructions from different gangs are
therefore pipe-lined and injected in the compute unit.<sup><xref ref-type="bibr" rid="ref34">34</xref>,<xref ref-type="bibr" rid="ref35">35</xref></sup> From this emerges the kernel occupancy’s concept, which is
defined as the ratio between the gang’s number concurrently
running on one SM and the maximum gang number that can actually be
held by this SM.</p>
    </sec>
    <sec id="sec2.2">
      <title>Global Scheme</title>
      <p>The <italic>parallel</italic> computing
power of GPUs is in constant evolution and the number of streaming
multiprocessors (SM) is almost doubling with every generation. Considering
their impressive compute potential in comparison to CPUs, one can
assume that the only way to entirely benefit from this power is to
offload the entire application on device. Any substantial part of
the workflow of Tinker-HP should not be performed on the CPU platform.
It will otherwise represent a bottleneck to performance in addition
to requiring several data transfers. As for all MD applications, most
of the computation lies in the evaluation of the forces. For the AMOEBA
polarizable model, it takes around 97% of a time step to evaluate
those forces when running sequentially on CPU platform. Of these 97%,
around 10% concern bonded forces and 90% the nonbonded ones, namely,
polarization, (multipolar) permanent electrostatics, and van der Waals.
The polarization, which includes the iterative resolution of induced
dipoles, largely dominates this part (see <xref rid="fig3" ref-type="fig">Figure <xref rid="fig3" ref-type="fig">3</xref></xref>). Nonbonded forces and polarization in particular will thus
be our main focus regarding the porting and optimization. We will
then benefit from the already present Tinker-HP MPI layer<sup><xref ref-type="bibr" rid="ref20">20</xref>,<xref ref-type="bibr" rid="ref22">22</xref></sup> to operate on several GPUs. The communications can then be made
directly between GPUs by using a CUDA aware MPI implementation.<sup><xref ref-type="bibr" rid="ref36">36</xref></sup> The Smooth Particle Mesh Ewald method<sup><xref ref-type="bibr" rid="ref20">20</xref>,<xref ref-type="bibr" rid="ref37">37</xref>,<xref ref-type="bibr" rid="ref38">38</xref></sup> is at the heart of both the permanent
electrostatics and polarization nonbonded forces used in Tinker-HP,
first through the iterative solution of the induced dipoles and then
through the final force evaluation. It consists in separating the
electrostatic energy in two independent pieces: real space and reciprocal
space contributions. Let us describe our O<sc>pen</sc>ACC strategy
regarding those two terms.</p>
      <fig id="fig3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>illustration of a MD time step.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0006" id="gr3" position="float"/>
      </fig>
      <sec id="sec2.2.1">
        <title>Real Space Scheme</title>
        <p>Because the real space part of the
total PME energy and forces has the same structure as the van der
Waals one, the associated O<sc>pen</sc>ACC strategy is the same. Evaluating
real space energy and forces is made through the computation of pairwise
interactions. Considering <italic>n</italic> atoms, a total of <italic>n</italic>(<italic>n</italic> – 1) pairwise interactions need
to be computed. This number is reduced by half because of the symmetry
of the interactions. Besides, because we use a cutoff distance after
which we neglect these interactions, we can reduce their number to
being proportional to <italic>n</italic> in homogeneous systems by
using neighbor lists. The up-bound constant is naturally reduced to
a maximum neighbors for every atoms noted as Neig<sub>max</sub>.</p>
        <p>The number of interactions is up-bounded by <italic>n</italic> *Neig<sub>max</sub>. In terms of implementation, we have written the compute
algorithm into a single loop kernel. As all the interactions are independent,
the kernel is semiparallel regarding each iteration. By making sure
that energy and forces are added one at a time, the kernel becomes
parallel. To do that, we can use atomic operations on GPUs, which
allow us to make this operation in parallel and solve any race condition
issue without substantially impacting parallel performance. By doing
so, real space kernels looks like <xref rid="cht1" ref-type="chart">Chart <xref rid="cht1" ref-type="chart">1</xref></xref>.</p>
        <fig id="cht1" position="float">
          <label>Chart 1</label>
          <caption>
            <title>O<sc>pen</sc>ACC Real Space Offload Scheme<xref rid="cht1-fn1" ref-type="p">a</xref></title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0001" id="gr20" position="float"/>
          <p>
            <fn id="cht1-fn1">
              <label>a</label>
              <p>The kernel is offloaded onto
a device using two of the three parallelism levels offered by O<sc>pen</sc>ACC. The first loop is broken down over gangs and gathers
all data related to atom <monospace>iglob</monospace> using gang’s
shared memory through the private clause. O<sc>pen</sc>ACC vectors
are responsible of the evaluation and the addition of forces and energy
after resolving scaling factor if necessary. Regarding data management
we make sure with the present clause that everything is available
on device before the execution of the kernel.</p>
            </fn>
          </p>
        </fig>
        <p>At first, our approach was designed to directly offload the CPU
vectorized real space compute kernels that use small arrays to compute
pairwise interactions in hopes of aligning the memory access pattern
at the vector level and therefore accelerate the code.<sup><xref ref-type="bibr" rid="ref22">22</xref></sup> This requires each gang to privatize every temporary
array and results in a significant overhead with memory reservation
associated with a superior bound on the gang’s number. Making
interactions computation scalar helps us remove those constraints
and double the kernel performance. The explanation behind this increase
arises from the use of GPU scalar registers. Still, one has to resolve
the scaling factors of every interactions. As it happens inside gang
shared memory, the performance is slightly affected. However, we would
benefit from a complete removal of this inside search. There are two
potential drawbacks to this approach:<list list-type="bullet"><list-item><p>Scaling interactions between neighboring atoms of the
same molecule can become very complex. This is particularly true with
large proteins. Storage workspace can potentially affect shared memory
and also kernel’s occupancy.</p></list-item><list-item><p>Depending
on the interactions, there is more than one
kind of scaling factor. For example, every AMOEBA polarization interaction
needs three different scaling factors.</p></list-item></list>The best
approach is then to compute scaling interactions separately
in a second kernel. Because they only involve connected atoms, their
number is small compared to the total number of nonbonded interactions.
We first compute unscaled nonbonded interactions and then apply scaling
correction in a second part. An additional issue is to make this approach
compatible with the 3d domain decomposition. Our previous kernel then
reads as in <xref rid="cht2" ref-type="chart">Chart <xref rid="cht2" ref-type="chart">2</xref></xref>.</p>
        <fig id="cht2" position="float">
          <label>Chart 2</label>
          <caption>
            <title>Final O<sc>pen</sc>ACC Real Space Offload Scheme<xref rid="cht2-fn1" ref-type="p">a</xref></title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0002" id="gr21" position="float"/>
          <p>
            <fn id="cht2-fn1">
              <label>a</label>
              <p>This kernel is more balanced
and exposes a much more computational load over vectors. A “correct_scaling”
routine applies the correction of the scaling factors. This procedure
appears to be much more suitable to device execution.</p>
            </fn>
          </p>
        </fig>
      </sec>
      <sec id="sec2.2.2">
        <title>Reciprocal Space Scheme</title>
        <p>The calculation of Reciprocal
space PME interactions essentially consists in five steps:<list id="list1" list-type="simple"><list-item><label>1.</label><p>interpolating
the (multipolar) density
of charge at stake on a 3D grid with flexible b-spline order (still,
the implementation is optimized to use an order of 5 as it is the
default and the one typically used with AMOEBA).</p></list-item><list-item><label>2.</label><p>switching to Fourier space by using
a forward fast Fourier transform (<monospace>FFt</monospace>)</p></list-item><list-item><label>3.</label><p>performing a trivial scalar
product
in reciprocal space</p></list-item><list-item><label>4.</label><p>performing a backward <monospace>FFt</monospace> to switch back to
real space</p></list-item><list-item><label>5.</label><p>performing
a final multiplication by
b-splines to interpolate the reciprocal forces</p></list-item></list>Regarding parallelism, Tinker-HP uses a two-dimensional decomposition
of the associated 3d grid based on successive 1D <monospace>FFt</monospace>s. Here, we use the <monospace>cuFFt</monospace> library.<sup><xref ref-type="bibr" rid="ref39">39</xref></sup> The O<sc>pen</sc>ACC offload scheme for reciprocal
space is described in <xref rid="fig4" ref-type="fig">Figure <xref rid="fig4" ref-type="fig">4</xref></xref>.</p>
        <fig id="fig4" position="float">
          <label>Figure 4</label>
          <caption>
            <p>Reciprocal space offload scheme. Charge interpolation and Force
interpolation are both written in a single kernel. They are naturally
parallel except for the atomic contributions to the grid in the first
one. The approach remains the same for data management between host
and device as for real space: all data are by default device resident
to prevent any sort of useless transfer. Regarding MPI communications,
exchanges take place directly between GPUs through interconnection.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0007" id="gr4" position="float"/>
        </fig>
        <p>We just reviewed our offload strategy of the nonbonded
forces kernels
with O<sc>pen</sc>ACC, but the bonded ones remain to be treated. Also,
the MPI layer has to be dealt with. The way bonded forces are computed
is very similar to the real space ones, albeit simpler, which makes
their offloading relatively straightforward. MPI layer kernels require,
on the other hand, a slight rewriting as communications are made after
a packing pretreatment. In parallel, one does not control the throughput
order of this packing operation. This is why it becomes necessary
to also communicate the atom list of each process to their neighbors.
Now that we presented the main offload strategies, we can focus on
some global optimizations regarding the implementation and execution
for single and multiple GPUs. Some of them lead to very different
results depending on the device architecture.</p>
      </sec>
    </sec>
    <sec id="sec2.3">
      <title>Optimizations
Opportunities</title>
      <p> </p>
      <p>&gt;A first
optimization is to impose an optimal bound on the vector size when
computing pair interactions. In a typical setup, for symmetry reasons,
the number of neighbors for real space interactions varies between
zero and a few hundred. Because of that second loop in <xref rid="cht2" ref-type="chart">Chart <xref rid="cht2" ref-type="chart">2</xref></xref>, the smallest vector length
(32) is appropriate to balance computation among the threads it contains.
Another optimization concerns the construction of the neighbor lists.
Let us recall that it consists of storing, for every atom, the neighbors
that are closer than a cut distance (<italic>d</italic><sub>cut</sub>) plus a buffer <italic>d</italic><sub>buff</sub>. This buffer is
related to the frequency at which the list has to be updated. To balance
computation at the vector level and at the same time reduce warp discrepancy
(as illustrated in <xref rid="fig5" ref-type="fig">Figure <xref rid="fig5" ref-type="fig">5</xref></xref>), we have implemented a reordering kernel: we reorder the
neighbor list for each atom so that the firsts are the ones under <italic>d</italic><sub>cut</sub> distance.</p>
      <fig id="fig5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Illustration of compute balance due to
the list reordering. Unbalanced
computation in the first image induces an issue called warp discrepancy:
a situation where all threads belonging to the same vector do not
follow the same instructions. Minimizing that can increase kernel
performance significantly since we ensure load balancing among each
thread inside the vector.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0008" id="gr5" position="float"/>
      </fig>
      <p>&gt;A second optimization concerns the iterative resolution of the
induced dipoles. Among the algorithms presented in refs (<xref ref-type="bibr" rid="ref38">38</xref>) and (<xref ref-type="bibr" rid="ref40">40</xref>), the first method we offloaded
is the preconditioned conjugated gradient (PCG). It involves a (polarization)
matrix-vector product at each iteration. Here, the idea is to reduce
the computation and favor coalesce memory access by precomputing and
storing the elements of (the real space part) of the matrix before
the iterations. As the matrix-vector product is being repeated, we
see a performance gain starting from the second iteration. This improves
performance but implies a memory cost that could be an issue on large
systems or on GPUs with small memory capabilities. This overhead will
be reduced at a high level of multidevice parallelism.</p>
      <p>&gt;An
additional improvement concerns the two-dimensional domain
decomposition of the reciprocal space 3D grid involved with <monospace>FFt</monospace>. The parallel scheme for <monospace>FFt</monospace> used in Tinker-HP is the following for a forward transform:<disp-formula id="ueq1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m001" position="anchor"/></disp-formula></p>
      <p>Each transposition represents an all-to-all MPI communication,
which is the major bottleneck preventing most MD applications using
PME to scale across nodes.<sup><xref ref-type="bibr" rid="ref20">20</xref>,<xref ref-type="bibr" rid="ref22">22</xref>,<xref ref-type="bibr" rid="ref41">41</xref></sup> Given the GPUs’ huge computing power, this communication
is even more problematic in that context. On the device, we use the <monospace>cuFFt</monospace>(<xref ref-type="bibr" rid="ref39">39</xref>) library. Using many <monospace>cuFFt</monospace> 1d batches is not as efficient as using fewer batches
in a higher dimension. Indeed, devices are known to underperform with
low saturation kernels. In order to reduce MPI exchanges and increase
load on device, we adopted a simple 3d dimensional cuFFt batch when
running on a single device. On multiple GPUs, we use the following
scheme based on a 1d domain decomposition along the <italic>z</italic> axis:<disp-formula id="ueq2"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m002" position="anchor"/></disp-formula>which gives a 25% improvement compared to
the initial approach.</p>
      <p>&gt;Profiling the application on a single
device, we observed that
real space computation is on average 5 times slower than reciprocal
space computation. This trend reverses using multiple GPUs because
of the communications mentioned above. This motivated the assignment
of these two parts in two different priority streams. Reciprocal space
kernels along with MPI communications are queued inside the higher
priority stream, and real space kernels, devoid of communications,
can operate at the same time on the lower priority stream to recover
communications. This is illustrated in <xref rid="fig6" ref-type="fig">Figure <xref rid="fig6" ref-type="fig">6</xref></xref>.</p>
      <fig id="fig6" position="float">
        <label>Figure 6</label>
        <caption>
          <p>Representation of <monospace>cuFFt</monospace>’s communication/computation
overlap using different streams for direct and reciprocal space. Real
space computation kernels are assigned to asynchronous stream 18.
Reciprocal ones go into high priority asynchronous stream 17. The
real space kernel therefore recovers <monospace>FFt</monospace> grid
exchanges. This profile was retrieved on 2 GPUs.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0009" id="gr6" position="float"/>
      </fig>
    </sec>
    <sec id="sec2.4">
      <title>Simulation Validation and Benchmarks</title>
      <p>Here, we use the
same bench systems as in refs (<xref ref-type="bibr" rid="ref20">20</xref>) and (<xref ref-type="bibr" rid="ref22">22</xref>): the solvated <monospace>DHFR</monospace> protein, the solvated <monospace>COX</monospace> protein, and the <monospace>STMV</monospace> virus,
all with the AMOEBA force field, respectively made of 23558, 171219,
and 1066600 atoms. The molecular dynamics simulations were run in
the <italic>NVT</italic> ensemble at 300 K for 5 ps simulation using
a (bonded/nonbonded) <monospace>RESPA</monospace> integrator with
a 2 fs outer time step (and a 1 fs inner time step)<sup><xref ref-type="bibr" rid="ref42">42</xref></sup> and the Bussi thermostat.<sup><xref ref-type="bibr" rid="ref43">43</xref></sup> The
performance was averaged over the complete runs. For validation purposes,
we compared the potential energy, temperature, and pressure of the
systems during the first 50 time steps with values obtained with Tinker-HP
v1.2. Furthermore, these results were compared to Tinker-OpenMM in
the same exact setup.<sup><xref ref-type="bibr" rid="ref28">28</xref></sup></p>
      <p>We can directly
observe the technical superiority of the Quadro architecture compared
to the Geforce one. Double precision (DP) compute units of the V100
allow us to vastly outperform the Geforce. In addition, by comparing
the performance of the Geforce RTX to the one of the Quadro GV100,
we see that Quadro devices are much less sensitive to warp discrepancy
and noncoalesced data accessing pattern. It is almost as if the architecture
of the V100 card overcomes traditional optimizations techniques related
to parallel device implementation. However, we see that our pure O<sc>pen</sc>ACC implementation manages to deliver more performance than
usual device MD application with PFF in DP. The V100 results were
obtained on the Jean-Zay HPE SGI 8600 cluster of the IDRIS supercomputer
Center (GENCI-CNRS, Orsay, France) whose converged partitions are
respectively made of 261 and 351 nodes. Each one is made of 2 Intel
Cascade Lake 6248 processors (20 cores at 2.5 GHz) accelerated with
4 NVIDIA Tesla V100 SXM2 GPUs, interconnected through NVIDIA NVLink,
and coming respectively with 32 GB of memory on the first partition
and 16 GB on the second. Here as in all the tests presented
in this paper, all the MPI communications were made with a CUDA aware
MPI implementation.<sup><xref ref-type="bibr" rid="ref36">36</xref></sup> This result is very
satisfactory as a single V100 card is at least 10 times faster than
an entire node of this supercomputer using only CPUs.</p>
      <p>Multidevice
benchmark results compared with host-platform execution
are presented in <xref rid="fig7" ref-type="fig">Figure <xref rid="fig7" ref-type="fig">7</xref></xref>. In practice, the <monospace>DHFR</monospace> protein is too small to scale out. MPI communications overcome the
computations even with optimizations. On the other hand, <monospace>COX</monospace> and <monospace>STMV</monospace> systems show good
multi-GPU performances. Adding our latest MPI optimizations (FFt reshaping
and asynchronous computing between the direct and reciprocal part
of PME) allows for a substantial gain in performances. We see that
on a Jean-Zay node we can only benefit from the maximum communication
bandwidth when running on the entire node; hence the relative inflection
point on the <monospace>STMV</monospace> performances on the 2 GPU
setup. Indeed, all devices are interconnected inside one node in such
a way that they all share the interconnection bandwidth. More precisely,
running on 2 GPUs reduces the bandwidth by three and therefore affects
the scalabity. It is almost certain that results would get better
on an interconnected node made exclusively of 2 GPUs. Those results
are more than encouraging considering the fact that we manage to achieve
them with a full O<sc>pen</sc>ACC implementation of Tinker-HP (direct
portage of the reference CPU code) in addition to some adjustments.</p>
      <fig id="fig7" position="float">
        <label>Figure 7</label>
        <caption>
          <p>Performance
ratio between single node GPU and single node CPU performance.
Reference values can be found in <xref rid="tbl1" ref-type="other">Table <xref rid="tbl1" ref-type="other">1</xref></xref>.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0010" id="gr7" position="float"/>
      </fig>
      <table-wrap id="tbl1" position="float">
        <label>Table 1</label>
        <caption>
          <title>Single
Device Benchmark: MD Production
per Day (ns/day)<xref rid="tbl1-fn1" ref-type="table-fn">a</xref></title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="char" char="."/>
            <col align="left"/>
            <col align="left"/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
          </colgroup>
          <thead>
            <tr>
              <th style="border:none;" align="center">systems/devices (ns/day)</th>
              <th style="border:none;" align="center" char=".">CPUs - one
node</th>
              <th style="border:none;" align="center">RTX 2080Ti</th>
              <th style="border:none;" align="center">RTX 2080Ti
+ optim</th>
              <th style="border:none;" align="center" char=".">V100</th>
              <th style="border:none;" align="center" char=".">V100 + optim</th>
              <th style="border:none;" align="center" char=".">Tinker-OpenMM
V100</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">
                <monospace>DHFR</monospace>
              </td>
              <td style="border:none;" align="char" char=".">0.754</td>
              <td style="border:none;" align="left">2.364</td>
              <td style="border:none;" align="left">3.903</td>
              <td style="border:none;" align="char" char=".">8.900</td>
              <td style="border:none;" align="char" char=".">9.260</td>
              <td style="border:none;" align="char" char=".">6.300</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">
                <monospace>COX</monospace>
              </td>
              <td style="border:none;" align="char" char=".">0.103</td>
              <td style="border:none;" align="left">0.341</td>
              <td style="border:none;" align="left">0.563</td>
              <td style="border:none;" align="char" char=".">1.051</td>
              <td style="border:none;" align="char" char=".">1.120</td>
              <td style="border:none;" align="char" char=".">0.957</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">
                <monospace>STMV</monospace>
              </td>
              <td style="border:none;" align="char" char=".">0.013</td>
              <td style="border:none;" align="left">n/a</td>
              <td style="border:none;" align="left">n/a</td>
              <td style="border:none;" align="char" char=".">0.111</td>
              <td style="border:none;" align="char" char=".">0.126</td>
              <td style="border:none;" align="char" char=".">0.130</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tbl1-fn1">
            <label>a</label>
            <p>All simulations were run using
a RESPA/2 fs setup.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>In summary, our DP implementation is already satisfactory compared
to other applications such as Tinker-OpenMM. Our next section concerns
the porting of Tinker-HP in a downgraded precision.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>CUDA Approach</title>
    <p>Even though we already have a robust O<sc>pen</sc>ACC implementation
of Tinker-HP in double precision, the gain in terms of computational
speed when switching directly to single precision (SP) is modest,
as shown in <xref rid="tbl2" ref-type="other">Table <xref rid="tbl2" ref-type="other">2</xref></xref>, which is inconsistent with the GPUs’ computational capabilities.</p>
    <table-wrap id="tbl2" position="float">
      <label>Table 2</label>
      <caption>
        <title>Single Precision MD Production (ns/day)
within the O<sc>pen</sc>ACC Implementation</title>
      </caption>
      <table frame="hsides" rules="groups" border="0">
        <colgroup>
          <col align="left"/>
          <col align="char" char="."/>
          <col align="char" char="."/>
          <col align="left"/>
        </colgroup>
        <thead>
          <tr>
            <th style="border:none;" align="center"> </th>
            <th style="border:none;" align="center" char=".">
              <monospace>DHFR</monospace>
            </th>
            <th style="border:none;" align="center" char=".">
              <monospace>COX</monospace>
            </th>
            <th style="border:none;" align="center">
              <monospace>STMV</monospace>
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="border:none;" align="left">V100</td>
            <td style="border:none;" align="char" char=".">11.69</td>
            <td style="border:none;" align="char" char=".">1.72</td>
            <td style="border:none;" align="left">0.15</td>
          </tr>
          <tr>
            <td style="border:none;" align="left">RTX-2080 Ti</td>
            <td style="border:none;" align="char" char=".">11.72</td>
            <td style="border:none;" align="char" char=".">1.51</td>
            <td style="border:none;" align="left">n/a</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>This is more obvious for Geforce architecture devices
since those
cards do not possess DP physical compute units and therefore emulate
DP Instructions. According to <xref rid="tbl3" ref-type="other">Table <xref rid="tbl3" ref-type="other">3</xref></xref>, theoretical ratios of 2 and 31 are respectively expected
from V100 and RTX-2080 Ti performances when switching from DP to SP,
which makes an efficient SP implementation mandatory.</p>
    <table-wrap id="tbl3" position="float">
      <label>Table 3</label>
      <caption>
        <title>Device Hardware Specifications</title>
      </caption>
      <table frame="hsides" rules="groups" border="0">
        <colgroup>
          <col align="left"/>
          <col align="char" char="."/>
          <col align="char" char="."/>
          <col align="char" char="."/>
          <col align="char" char="."/>
          <col align="char" char="."/>
        </colgroup>
        <thead>
          <tr>
            <th style="border:none;" align="center"> </th>
            <th colspan="2" align="center" char=".">performances
(Tflop/s)<hr/></th>
            <th style="border:none;" align="center" char="."> </th>
            <th colspan="2" align="center" char=".">compute/access<hr/></th>
          </tr>
          <tr>
            <th style="border:none;" align="center">GPU</th>
            <th style="border:none;" align="center" char=".">DP</th>
            <th style="border:none;" align="center" char=".">SP</th>
            <th style="border:none;" align="center" char=".">memory bandwidth (GB/s)</th>
            <th style="border:none;" align="center" char=".">DP (Ops/8B)</th>
            <th style="border:none;" align="center" char=".">SP (Ops/4B)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="border:none;" align="left">Quadro GV100</td>
            <td style="border:none;" align="char" char=".">7.40</td>
            <td style="border:none;" align="char" char=".">14.80</td>
            <td style="border:none;" align="char" char=".">870.0</td>
            <td style="border:none;" align="char" char=".">68.04</td>
            <td style="border:none;" align="char" char=".">68.04</td>
          </tr>
          <tr>
            <td style="border:none;" align="left">Tesla V100 SXM2</td>
            <td style="border:none;" align="char" char=".">7.80</td>
            <td style="border:none;" align="char" char=".">15.70</td>
            <td style="border:none;" align="char" char=".">900.0</td>
            <td style="border:none;" align="char" char=".">69.33</td>
            <td style="border:none;" align="char" char=".">69.77</td>
          </tr>
          <tr>
            <td style="border:none;" align="left">Geforce RTX-2080 Ti</td>
            <td style="border:none;" align="char" char=".">0.42</td>
            <td style="border:none;" align="char" char=".">13.45</td>
            <td style="border:none;" align="char" char=".">616.0</td>
            <td style="border:none;" align="char" char=".">5.45</td>
            <td style="border:none;" align="char" char=".">87.33</td>
          </tr>
          <tr>
            <td style="border:none;" align="left">Geforce RTX-3090</td>
            <td style="border:none;" align="char" char=".">0.556</td>
            <td style="border:none;" align="char" char=".">35.58</td>
            <td style="border:none;" align="char" char=".">936.2</td>
            <td style="border:none;" align="char" char=".">4.75</td>
            <td style="border:none;" align="char" char=".">152.01</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>In practice, instead of doubling the speed on V100
cards, we ended
up noticing a 1.25 increase factor on V100 and 3 on RTX on <monospace>DHFR</monospace> in SP compared to DP with the same setup. All tests
have been done under the assumption that our simulations are valid
in this precision mode. More results are shown in <xref rid="tbl2" ref-type="other">Table <xref rid="tbl2" ref-type="other">2</xref></xref>. Furthermore, a deep profile
conducted on the kernels representing Tinker-HP’s bottleneck
(real space nonbonded interactions) in the current state reveals an
insufficient exploitation of the GPU SP compute power. <xref rid="fig8" ref-type="fig">Figure <xref rid="fig8" ref-type="fig">8</xref></xref> suggests that there is still
room for improvements in order to take full advantage of the card’s
computing power and memory bandwidth both in SP and DP. In order to
exploit device SP computational power and get rid of the bottleneck
exposed by <xref rid="fig8" ref-type="fig">Figure <xref rid="fig8" ref-type="fig">8</xref></xref>, it becomes necessary to reshape our implementation method and consider
some technical aspects beyond O<sc>pen</sc>ACC’s scope.</p>
    <fig id="fig8" position="float">
      <label>Figure 8</label>
      <caption>
        <p>Profile of
the matrix-vector compute kernel on the <monospace>DHFR</monospace> system. The left picure is obtained with the double precision and
the right one with simple precision. In both modes, results indicate
an obvious latency issue coming from memory accessing pattern that
prevents the device from reaching its peak performance.</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0011" id="gr8" position="float"/>
    </fig>
    <sec id="sec3.1">
      <title>Global Overview and Definitions</title>
      <p>As mentioned in the
previous section, GPUs are most efficient with parallel computations
and coalesce memory access patterns. The execution model combines
and handles effectively two nested levels of parallelism. The high
level concerns multithreading and the low level the SIMD execution
model for vectorization.<sup><xref ref-type="bibr" rid="ref22">22</xref>,<xref ref-type="bibr" rid="ref44">44</xref></sup> This model stands for
single instruction multiple threads (<sc>SIMT</sc>).<sup><xref ref-type="bibr" rid="ref45">45</xref></sup> When it comes to GPU programming, <sc>SIMT</sc> also includes
control-flow instructions along with subroutine calls within the SIMD
level. This provides additional freedom of approach during implementation.
To improve the results presented in the last paragraph (<xref rid="tbl2" ref-type="other">Table <xref rid="tbl2" ref-type="other">2</xref></xref>) and increase peak performance
on computation and throughput, it is crucial to expose more computations
in real space kernels and to minimize global memory accesses in order
to benefit from cache and shared memory accesses as well as registers.
Considering O<sc>pen</sc>ACC paradigm limitations in terms of kernel
description as well as the required low-level features, we decided
to rewrite those specific kernels using the standard approach of low-level
device programming in addition to CUDA built-in intrinsics. In a following
section, we will describe our corresponding strategy after a thorough
review on precision.</p>
    </sec>
    <sec id="sec3.2">
      <title>Precision study and Validation</title>
      <p> </p>
      <p><bold>Definition</bold><bold>i</bold>. We shall call ϵ<sub><italic>p</italic></sub> the machine precision (in SP or DP), the
smallest
floating point value such that 1 + ϵ<sub><italic>p</italic></sub> &gt; 1. They are respectively 1.2 × 10<sup>–7</sup> and
2.2 × 10<sup>–16</sup> in SP and DP.</p>
      <p><bold>Definition</bold><bold>ii</bold>.Considering a positive
floating point variable <italic>a</italic>, the machine precision
ϵ<sub><italic>a</italic></sub> attached to <italic>a</italic> is<disp-formula id="ueq3"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m003" position="anchor"/></disp-formula>Therefore an error
made for a floating point
operation between <italic>a</italic> and <italic>b</italic> can be
expressed as<disp-formula id="eq1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m004" position="anchor"/><label>1</label></disp-formula>where ⊕̃ designates
the numerical operation between <italic>a</italic> and <italic>b</italic>.</p>
      <p><bold>Property</bold><bold>i</bold>. Numerical error resulting
from sequential reduction operations are linear while those resulting
from parallel reduction are logarithmic. Thus, parallel reductions
are entirely suitable to GPU implementation as they benefit from both
parallelism and accuracy.</p>
      <p>Before looking further into the matter
of downgrading precision,
we have to make sure that Tinker-HP is able to work in this mode.
Although it has been proven in the literature<sup><xref ref-type="bibr" rid="ref25">25</xref>,<xref ref-type="bibr" rid="ref27">27</xref>,<xref ref-type="bibr" rid="ref46">46</xref></sup> that many MD applications are able to provide
correct results with simple precision, extensive precision studies
with polarizable force fields are lacking.</p>
      <p>When it comes to
standard IEEE floating point arithmetic, regular
32 bit storage offers no more than 7 significant digits due to the
mantissa. In comparison, we benefit from 16 significant digits with
DP 64 storage bits. Without any consideration on the floating number’s
sign, it is safe to assume that any application working with absolute
values outside the [10<sup>–7</sup>, 10<sup>7</sup>] scope
will fail to deliver sufficient accuracy when operating in complete
SP mode. This is called the floating point overflow. To overcome this,
the common solution is to use a mixed precision mode (MP) that encompasses
both standard SP and a superior precision container to store variables
subject to SP overflowing. In practice, most MD applications adopt
SP for computation and a higher precision for accumulation. Moreover,
applications like Amber or OpenMM propose another accumulation method
which rely on a different type of variable.<sup><xref ref-type="bibr" rid="ref46">46</xref></sup></p>
      <p>The description made in the previous section shows that energy
and the virial evaluation are linear-dependent with the system’s
size. Depending on the complexity of the interaction in addition to
the number of operations it requires, we can associate a constant
error value ϵ<sub><italic>i</italic></sub> to it. Thus, we can
bound the error made on the computation of a potential energy with <italic>N</italic><sub>int</sub>ϵ<sub><italic>i</italic></sub> &lt; <italic>n</italic>Neig<sub>max</sub>ϵ<sub><italic>i</italic></sub>,
where <italic>N</italic><sub>int</sub> represents the number of interactions
contributing to this energy and Neig<sub>max</sub> the maximum number
of neighbor it involves per atom. As it is linear with respect to
the system size we have to evaluate this entity with a DP storage
container. Furthermore, to reduce even more the accumulation of error
due to large summation, we shall employ buffered parallel reduction
instead of a sequential one (<xref rid="fig9" ref-type="fig">Figure <xref rid="fig9" ref-type="fig">9</xref></xref>). On the other hand, we have to deal with the forces
that remain the principal quantities that drive a MD simulation. The
error made for each atom on the nonbonded forces is bound by Neig<sub>max</sub>·ϵ<sub><italic>i</italic></sub> depending on
the cutoff. However, each potential comes with a different ϵ<sub><italic>i</italic></sub>. In practice, the corresponding highest
values are the ones of both van der Waals and bonded potentials. The
large number of pairwise interactions induced by the larger van der
Waals cutoff in addition to the functional form that includes a power
of 14 (for AMOEBA) causes SP overflowing for distances greater than
3 Å. By reshaping the variable encompassing the pairwise distance,
we get a result much closer to DP since intermediate calculations
do not overflow. Regarding the bonded potentials, <inline-formula id="d33e1321"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m005.gif"/></inline-formula> depends more on the conformation of the
system.</p>
      <fig id="fig9" position="float">
        <label>Figure 9</label>
        <caption>
          <p>Illustration of the reduction operation on a 16 variables set.
Each arithmetic operation generates an error ϵ<sub><italic>a</italic></sub> that is accumulated during the sequential operation. On the
other hand, parallel reduction uses intermediate variables to significantly
reduce the error.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0012" id="gr9" position="float"/>
      </fig>
      <p>Parameters involved in
bond pairwise evaluation (spring stiffness,
...) cause a SP numerical error (<inline-formula id="d33e1337"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m006.gif"/></inline-formula>) standing between 1 × 10<sup>–3</sup> and 1 × 10<sup>–2</sup>, which frequently reach 1 ×
10<sup>–1</sup> (following (<xref rid="eq1" ref-type="disp-formula">eq <xref rid="eq1" ref-type="disp-formula">1</xref></xref>)) during the summation process, and this affects forces
more than total energy. In order to minimize <inline-formula id="d33e1354"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m007.gif"/></inline-formula>, we evaluate the distances in DP before
casting the result to SP. In the end, <inline-formula id="d33e1357"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m008.gif"/></inline-formula> is reduced on the scope of [1 × 10<sup>–4</sup>, 1 ×
10<sup>–3</sup>], which represents
the smallest error we can expect from SP.</p>
      <p>Furthermore, unlike
the energy, a sequential reduction using atomic
operations is applied to the forces. The resulting numerical error
is therefore linear with the total number of summation operations.
This is why we adopt a 64 bit container for those variables despite
the fact they can be held in a 32 bits container.</p>
      <p>Regarding
the type of the 64 bit container, we analyze two different
choices. First, we have the immediate choice of a floating point.
The classical mixed precision uses FP64 for accumulation and integration.
Every MD applications running on GPU integrates this mode. It presents
the advantage of being straightforward to implement. Second, we can
use an integer container for accumulation: this is the concept of
fixed point arithmetic introduced by Yates.<sup><xref ref-type="bibr" rid="ref47">47</xref></sup> To be able to hold a floating point inside an integer requires us
to define a certain number of bits to hold the decimal part. It is
called the fixed point fractional bits. The left bits are dedicated
to the integer part. Unlike the floating point, freezing the decimal
position constrains the approximation precision but offers a correct
accuracy in addition to deterministic operations. Considering a floating
point value <italic>x</italic> and an integer one <italic>a</italic> and a fractional bits value (<bold>fB</bold>), the relations establishing
the transition back and forth between them, as <italic>a</italic> = <italic>f</italic>(<italic>x</italic>) and <italic>x</italic> = <italic>f</italic><sup>–1</sup>(<italic>a</italic>), are defined as follows:<disp-formula id="eq2"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0026" id="GRAPHIC-d7e1860-autogenerated" position="float"/><label>2</label></disp-formula><disp-formula id="eq3"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0027" id="GRAPHIC-d7e1864-autogenerated" position="float"/><label>3</label></disp-formula>with <monospace>int</monospace> and <monospace>real</monospace> the converting functions
and <monospace>round</monospace> the truncation function that extracts
the integer part. When it
comes to MD, fixed point arithmetic is an excellent tool: each SP
pairwise contribution is small enough to be efficiently captured by
a 64 bit fixed point. For instance, it takes only 27 bits to capture
8 digits after the decimal point with a large place left for the integer
part. For typical values observed with different system sizes, we
are far from the limit imposed by the integer part of the container.
Inspired by the work of Walker, Götz, et al.,<sup><xref ref-type="bibr" rid="ref46">46</xref></sup> we have implemented this feature inside Tinker-HP with
the following configuration: 34 fractional bits has been selected
for forces accumulation, which leaves 30 bits for the integer part,
thus setting the absolute limit value to 2<sup>29</sup> (kcal/mol)·Å.
For the energy, we only allocated 30 fractional bits given the fact
that it grows linearly with the system size. Besides, using an integer
container for accumulation avoids dealing with DP instructions, which
significantly affects performance on Geforce cards unlike Tesla ones.
In summary, we should expect at least a performance or precision improvement
from FP.</p>
      <p>A practical verification is shown in <xref rid="fig10" ref-type="fig">Figure <xref rid="fig10" ref-type="fig">10</xref></xref>. In all cases, both MP and
FP behave similarly.
Forces being the driving components of MD, the trajectories generated
by our mixed precision implementation are accurate. However, as one
can see, if errors remain very low for forces even for large systems,
a larger error exists for energies, a phenomenon observed in all previous
MD GPU implementations. Some specific post-treatment computations,
like in a BAR free energy computation or <italic>NPT</italic> simulations
with a Monte Carlo barostat, require accurate energies. In such a
situation, one could use the DP capabilities of the code for this
postprocessing step as Tinker-HP remains exceptionally efficient in
DP even for large systems. A further validation simulation in the <italic>NVE</italic> ensemble can be found in <xref rid="fig15" ref-type="fig">Figure <xref rid="fig15" ref-type="fig">15</xref></xref>, confirming the overall excellent stability
of the code.</p>
      <fig id="fig10" position="float">
        <label>Figure 10</label>
        <caption>
          <p>Absolute error between DP implementation and both FP and
MP implementations
on total potential energy and Forces. Forces root-mean-square deviation
between DP and MP for systems from 648 up to 2592000 atoms. As expected,
both absolute errors in SP and FP are almost identical on the energy
and they grow linearly with the system size. Logarithmic regression
gives 0.99 value for the curve slope and up to 5 kcal/mol for
the largest system. However, the relative error for all systems is
located under 7 × 10<sup>–7</sup> in comparison to DP.
One can also see that the error on the forces is independent of the
system size.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0013" id="gr10" position="float"/>
      </fig>
    </sec>
    <sec id="sec3.3">
      <title>Neighbor List</title>
      <p>We want to expose the maximum of computation
inside a kernel using the device shared memory. To do so, we consider
the approach where a specific group of atoms interacts with another
one in a block-matrix pattern (see <xref rid="fig11" ref-type="fig">Figure <xref rid="fig11" ref-type="fig">11</xref></xref>). We need to load the parameters of the
group of atoms and the output structures needed for computation directly
inside cache memory and/or registers. On top of that, CUDA built-in
intrinsics can be used to read data from neighbor threads and if possible
compute cross term interactions. Ideally, we can expose <inline-formula id="d33e1465"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m009.gif"/></inline-formula> computations without a single access to
global memory, with <italic>B</italic><sub>size</sub> representing
the number of atoms within the group. With this approach, the kernel
should reach its peak in terms of computational load.</p>
      <fig id="fig11" position="float">
        <label>Figure 11</label>
        <caption>
          <p>Representation of interactions
between two groups of atoms within
Tinker-HP. <italic>B</italic><sub>size</sub> = 8 for the illustration.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0014" id="gr11" position="float"/>
      </fig>
      <p>A new approach of the neighbor list algorithm is
necessary to follow
the logic presented above. This method will be close to standard blocking
techniques used in many MD applications.<sup><xref ref-type="bibr" rid="ref25">25</xref>,<xref ref-type="bibr" rid="ref27">27</xref></sup> Let us present the structure of the algorithm in a sequential and
parallel, MPI, context.</p>
      <sec id="sec3.3.1">
        <title>Box Partitioning</title>
        <p>Lets us recall
that given a simulation
box <bold>Ω</bold>, a set of <bold>ω</bold><sub><italic>c</italic></sub> with <italic>c</italic> ∈ [0, ..., <italic>N<sub>c</sub></italic>] forms a <bold>Ω</bold> partition if
and only if<disp-formula id="ueq4"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m010" position="anchor"/></disp-formula></p>
        <p>We consider in the following that each
group deals with interactions involving atoms within a region of space.
In order to maximize <italic>B</italic><sub>comp</sub> between every
pair of groups, we must then ensure their spatial compactness. Moreover,
all these regions need to define a partition of <bold>Ω</bold> to make sure we do not end up with duplicate interactions. Following
this reasoning, we might be tempted to group them into small spheres
but it is impossible to partition a polygon with only spheres, not
to mention the difficulties arising from the implementation point
of view.</p>
        <p>The MPI layer of Tinker-HP induces a first partition
of <bold>Ω</bold> in <italic>P</italic> subdomains <bold>ψ</bold><sub><italic>p</italic></sub>, <italic>p</italic> ∈ [0, ..., <italic>P</italic>], where <italic>P</italic> is the number of MPI processes.
Tinker-HP
uses the midpoint image convention<sup><xref ref-type="bibr" rid="ref48">48</xref></sup> so
that the interactions computed by the process assigned to <bold>ψ</bold><sub><italic>p</italic></sub> are the ones whose midpoint falls into <bold>ψ</bold><sub><italic>p</italic></sub>. The approach used in
Tinker-HP for the nonbonded neighbor list uses a cubic partition <bold>ω</bold><sub><italic>c</italic></sub>, <italic>c</italic> ∈
[1, ..., <italic>N<sub>c</sub></italic>], of <bold>ψ</bold><sub><italic>p</italic></sub> and then collects the neighboring atoms
among the neighboring cells of <bold>ω</bold><sub><italic>c</italic></sub>. Here, we proceed exactly in the same way with two additional
conditions to the partitioning. First, the number of atoms inside
each cell <bold>ω</bold><sub><italic>c</italic></sub> must be
less or equal than <italic>B</italic><sub>size</sub>. Second, we must
preserve a common global numbering of the cells across all domains <bold>ψ</bold><sub><italic>p</italic></sub> to benefit from a unique
partitioning of <bold>Ω</bold>.</p>
        <p>Once the first partitioning
in cells is done, an additional sorting
operation is initiated to define groups so that each of them contains
exactly <italic>B</italic><sub>size</sub> spatially aligned atoms
following the cell numbering (note that because of the first constrain
mention earlier, one cell can contain atoms belonging to a maximum
of two groups). More precisely, the numbering of the cells follows
a one-dimensional representation of the three dimension of the simulation
box. Now, we want to find the best partitioning of <bold>ψ</bold><sub><italic>p</italic></sub> in groups that will ensure enough proximity
between atoms inside a group, minimizing the number of neighboring
groups and consequently maximizing <italic>B</italic><sub>comp</sub>.</p>
        <p>When the partitioning generates too flat domains, each group
might
end up having too many neighboring groups. The optimal cell shape
(close to a sphere) is the cube but we must not forget the first constraint
and end up with a very thin partition either. However, atom groups
are not affected by a partition along the innermost contiguous dimension
in the cell numbering. We can exploit this to get better partitioning. <xref rid="fig12" ref-type="fig">Figure <xref rid="fig12" ref-type="fig">12</xref></xref> illustrates and
explains the scheme on a two-dimensional box. Partitioning is done
in an iterative manner by cycling on every dimension. We progressively
increase the number of cells along each dimension starting on the
contiguous one until the first condition is fulfilled. During a parallel
run, we keep track of the cell with the smallest number of atoms with
a reduction operation. This allows to have a global partitioning of <bold>Ω</bold> and not just <bold>ψ</bold><sub><italic>p</italic></sub>.</p>
        <fig id="fig12" position="float">
          <label>Figure 12</label>
          <caption>
            <p>Illustration of a two-dimensional partition along with groups for
a box of water. The left figure shows a 64-cell partition of <bold>Ω</bold> while the right one refines this partitioning into
88 cells. The groups are defined by reindexing the atoms following
the cell numbering and their maximum size. Here <italic>B</italic><sub>size</sub> = 16. A unique color is associated with every atom
belonging to the same group. No cell contains more than <italic>B</italic><sub>size</sub> atoms or 2 groups. Once a group is selected (23),
searching for its neighboring groups is made through the set of cells
(with respect to the periodic boundary conditions) near the cells
it contains (43 and 44). Once this set is acquired, all the group
indexes greater than or equal to the selected group constitute the
actual list of neighbors to take the symmetry of the interactions
into account. We see that the group’s shape modulates the group
neighborhood as illustrated with the right illustration and a spatially
flat group 23.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0015" id="gr12" position="float"/>
        </fig>
        <p>Now that we do dispose of a spatial
rearrangement of the atoms
into groups, we need to construct pair-lists of all interacting groups
according to the cutoff distance plus an additional buffer to avoid
reconstructing it at each time step.</p>
        <p>Groups are built in such
a way that it is straightforward to jump
from groups indexing to cells indexing. We chose to use an adjacency
matrix which is GPU suitable and compatible with MPI parallelism.</p>
        <p>Once it is built, the adjacency matrix directly gives the pair-list.
Regarding the storage size involved with this approach, note that
we only require single bit to tag pair–group interactions.
This results in an <inline-formula id="d33e1676"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m011.gif"/></inline-formula> bits occupation that equals to <inline-formula id="d33e1679"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m012.gif"/></inline-formula> bytes. <bold><italic>n</italic></bold><sub><italic>l</italic></sub> represents the number of atoms which participates
to real space evaluation on a process domain (<bold>ψ</bold><sub><italic>p</italic></sub>). Of course, in terms of memory we
cannot afford a quadratic reservation. However, the scaling factor <inline-formula id="d33e1696"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m013.gif"/></inline-formula> is small enough even for the smallest value
of <italic>B</italic><sub>size</sub> set to 32 corresponding to device
warp size. Not to mention that, in the context of multidevice simulation,
the memory distribution is also quadratic. The pseudokernel is presented
in <xref rid="cht3" ref-type="chart">Chart <xref rid="cht3" ref-type="chart">3</xref></xref>.</p>
        <fig id="cht3" position="float">
          <label>Chart 3</label>
          <caption>
            <title>Adjacency
Matrix Construction Pseudo-kernel<xref rid="cht3-fn1" ref-type="p">a</xref></title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0003" id="gr22" position="float"/>
          <p>
            <fn id="cht3-fn1">
              <label>a</label>
              <p>We browse through
all the
cells, and for each one we loop on their neighbors. It is easy to
compute their ids since we know their length as well as their arrangement.
Given the fact that all cells form a partition of the box, we can
apply the symmetrical condition on pair-cells and retrieve the groups
inside thanks to the partitioning condition, which ensures that each
cell contains at most two groups.</p>
            </fn>
          </p>
        </fig>
        <p>Once the
adjacency matrix is built, a simple postprocessing gives
us the adjacency list with optimal memory size and we can use the
new list on real space computation kernels following the process described
in the introduction of this subsection and illustrated in <xref rid="fig11" ref-type="fig">Figure <xref rid="fig11" ref-type="fig">11</xref></xref>. In addition,
we benefit from a coalesced memory access pattern while loading blocks
data and parameters when they are spatially reordered.</p>
      </sec>
      <sec id="sec3.3.2">
        <title>List Filtering</title>
        <p>It is possible to improve the performance
of the group–group pairing with a similar approach to the list
reordering method mentioned in the O<sc>pen</sc>ACC optimizations
section above. By filtering every neighboring group, we can get a
list of atoms that really belong to a group’s neighborhood.
The process is achieved by following the rule: <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0028" id="GRAPHIC-d7e2356-autogenerated" position="float"/>α and
α<sub><italic>i</italic></sub> are atoms,
β represents a group of <italic>B</italic><sub>size</sub> atoms, <inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0029.jpg"/> is the neighborhood of a group and <monospace>dist</monospace>: <inline-formula id="d33e1754"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m014.gif"/></inline-formula> is the euclidean distance.</p>
        <p>An illustration
of the results using the filtering process is depicted in <xref rid="fig13" ref-type="fig">Figure <xref rid="fig13" ref-type="fig">13</xref></xref>.</p>
        <fig id="fig13" position="float">
          <label>Figure 13</label>
          <caption>
            <p>Starting from the situation
illustrated by <xref rid="fig12" ref-type="fig">Figure <xref rid="fig12" ref-type="fig">12</xref></xref>, we represent the geometry resulting from
the filtering process. We significantly reduce the group 23 neighborhood
with the list filtering; it decreases from 144 atoms with the first
list to 77 with the filtered one. <italic>B</italic><sub>comp</sub> increases, which corresponds to more interactions computed within
each group pair.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0016" id="gr13" position="float"/>
        </fig>
        <p>When the number of neighbor
atoms is not a multiple of <italic>B</italic><sub>size</sub>, we create
phantom atoms to complete the
actual neighbor lists. A drawback of the filtering process is a loss
of coalesced memory access pattern. As it has been entirely constructed
in parallel, we do not have control of the output order. Nonetheless,
this is compensated by an increase of <italic>B</italic><sub>comp</sub> for each interaction between groups, as represented by <xref rid="fig13" ref-type="fig">Figure <xref rid="fig13" ref-type="fig">13</xref></xref>. In practice,
we measure a 75% performance gain between the original list and the
filtered one for the van der Waals interaction kernel. Moreover, <xref rid="fig14" ref-type="fig">Figure <xref rid="fig14" ref-type="fig">14</xref></xref> (deep profile
of the previous bottleneck kernel: matrix-vector product) shows a
much better utilization of the device computational capability. We
apply the same strategy for the other real space kernels (electrostatics
and polarization).</p>
        <fig id="fig14" position="float">
          <label>Figure 14</label>
          <caption>
            <p>Real space kernel profiling results in mixed precision
using our
new group-Atoms list.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0017" id="gr14" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec id="sec3.4">
      <title>PME Separation</title>
      <p>As mentioned above, the Particle Mesh
Ewald method separates electrostatics computation in two, real and
reciprocal space. A new profiling of Tinker-HP in single-device mixed
precision mode with the latest developments shows that the reciprocal
part is the new bottleneck. More precisely, real space performs 20%
faster than reciprocal space within a standard PME setup. Moreover,
reciprocal space is even more a bottleneck in parallel because of
the additional MPI communications induced by the cuFFt Transformations.
This significantly narrows our chances of benefiting from the optimizations
mentioned in the previous optimization subsection. However, as both
parts are independent, we can distribute them on different MPI processes
in order to reduce or even suppress communications inside FFts. During
this operation, a subset of GPUs are assigned to reciprocal space
computation only. Depending on the system size and the load balancing
between real and reciprocal spaces, we can break through the scalability
limit and gain additional performance on a multidevice configuration.</p>
    </sec>
    <sec id="sec3.5">
      <title>Mixed Precision Validation</title>
      <p>To validate the precision
study made above, we compare a 1 ns long simulation in DP on CPU (Tinker-HP
1.2) in a constant energy setup (<italic>NVE</italic>) with the exact
same run using both GPU MP and FP implementations.</p>
      <p>We used the
solvated <monospace>DHFR</monospace> protein and the standard velocity
verlet integrator with a 0.5 fs time step, 12 and 7 Å cutoff
distances respectively for van der Waals and real space electrostatics
and a convergence criteria of 1 × 10<sup>–6</sup> for
the polarization solver. A grid of 64 × 64 × 64 was used
for reciprocal space with fifth-order splines. We also compare our
results with a trajectory obtained with Tinker-OpenMM in MP in the
exact same setup; see <xref rid="fig15" ref-type="fig">Figure <xref rid="fig15" ref-type="fig">15</xref></xref>.</p>
      <fig id="fig15" position="float">
        <label>Figure 15</label>
        <caption>
          <p>Variation of the total
energy during a <italic>NVE</italic> molecular
dynamics simulation of the <monospace>DHFR</monospace> protein in
DP and MP and FP. Energy fluctuations are respectively within 1.45,
1.82, 1.75, 1.69, and 3.45 kcal/mol for Tinker-HP DP SP FP
and Tinker-OpenMM DP MP.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0018" id="gr15" position="float"/>
      </fig>
      <p>The energy is remarkably
conserved along the trajectories obtained
with Tinker-HP in all cases: using DP, MP or FP with less oscillations
than with Tinker-OpenMM with MP.</p>
    </sec>
    <sec id="sec3.6">
      <title>Available Features</title>
      <p>The main features of Tinker-HP have
been offloaded to GPU such as its various integrators like the multi-time-step
integrators: RESPA1 and BAOAB-RESPA1,<sup><xref ref-type="bibr" rid="ref49">49</xref></sup> which allow up to a 10 fs time step with PFF (this required
to create new neighbor lists to perform short-range nonbonded interactions
computations for both van der Waals and electrostatics). Aside from
Langevin integrators, we ported the Bussi<sup><xref ref-type="bibr" rid="ref43">43</xref></sup> (which is the default) and the Berendsen thermostats, as well as
the Monte Carlo and the Berendsen barostats. We also ported free energy
methods such as the Steered Molecular Dynamics<sup><xref ref-type="bibr" rid="ref50">50</xref></sup> and van der Waals soft cores for alchemical transformations,
as well as the enhanced sampling method Gaussian Accelerated Molecular
Dynamics.<sup><xref ref-type="bibr" rid="ref51">51</xref></sup> Even if it is not the main
goal of our implementation as well optimized software suited to such
simulations exist, we also ported the routines necessary to use standard
nonpolarizable force fields such as CHARMM,<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> Amber,<sup><xref ref-type="bibr" rid="ref5">5</xref></sup> or OPLS.<sup><xref ref-type="bibr" rid="ref52">52</xref></sup> Still, we obtained already satisfactory performances with these
models despite a simple portage, the associated numbers can be found
in the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jctc.0c01164/suppl_file/ct0c01164_si_001.pdf">Supporting Information</ext-link> and further
optimization is ongoing. On top of all these features that concern
a molecular dynamics simulation, we ported the “analyze”
and “minimize” program of Tinker-HP, allowing to run
single point calculations as well as geometry optimizations. All these
capabilities are summed up in <xref rid="tbl4" ref-type="other">Table <xref rid="tbl4" ref-type="other">4</xref></xref>.</p>
      <table-wrap id="tbl4" position="float">
        <label>Table 4</label>
        <caption>
          <title>Available Features in the Initial
Tinker-HP GPU Release</title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="left"/>
          </colgroup>
          <tbody>
            <tr>
              <td style="border:none;" align="left">programs</td>
              <td style="border:none;" align="left">dynamic; analyze; minimize; bar</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">integrator</td>
              <td style="border:none;" align="left">VERLET (default); RESPA;
RESPA1; BAOAB-RESPA; BAOAB-RESPA1</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">force fields</td>
              <td style="border:none;" align="left">AMOEBA; CHARMM/AMBER/OPLS</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">miscellaneous</td>
              <td style="border:none;" align="left">steered MD (SMD); Gaussian
accelerated MD; restrained groups; soft cores; plumed</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">thermostat</td>
              <td style="border:none;" align="left">Bussi (default); Berendsen</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">barostat</td>
              <td style="border:none;" align="left">Berendsen (default); Monte
Carlo</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec3.7">
      <title>Performance
and Scalability Results</title>
      <p>We ran benchmarks
with various systems on a set of different GPUs in addition to Tesla
V100 nodes of the Jean-Zay supercomputer. We also ran the whole set
of tests on the Irène Joliot Curie ATOS Sequana supercomputer
V100 partition to ensure for the portability of the code. We used
two different integrators: (2 fs RESPA along with 10 fs
BAOAB-RESPA1 with heavy hydrogens). For each system, we performed
2.5 and 25 ps MD simulations with RESPA and BAOAB-RESPA1, respectively,
and averaged the performance on the complete runs. van der Waals and
real space electrostatics cutoffs were respectively set to 9 and 7
Å plus 0.7 Å neighbor list buffer for RESPA, 1 Å for
BAOAB-RESPA1. We used the Bussi thermostat with the RESPA integrator.
Induced dipoles were converged up to a 1 × 10<sup>–5</sup> convergence threshold with the conjugate gradient solver and a diagonal
preconditioner.<sup><xref ref-type="bibr" rid="ref38">38</xref></sup> The test cases are water
boxes within the range of 96000 atoms (i.e., <monospace>Puddle</monospace>) up to 2592000 atoms (i.e., <monospace>Bay</monospace>), the <monospace>DHFR</monospace>, <monospace>COX</monospace> and the Main Protease
of Sars-Cov2 proteins (<monospace>M<sup>pro</sup></monospace>)<sup><xref ref-type="bibr" rid="ref53">53</xref></sup> as well as the <monospace>STMV</monospace> virus. <xref rid="tbl5" ref-type="other">Table <xref rid="tbl5" ref-type="other">5</xref></xref> gathers all single
devices performances, and <xref rid="fig16" ref-type="fig">Figure <xref rid="fig16" ref-type="fig">16</xref></xref> illustrates the multidevice performance.</p>
      <table-wrap id="tbl5" position="float">
        <label>Table 5</label>
        <caption>
          <title>Tinker-HP Performances in (ns/day)
on Different Devices and Precision Modes</title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th style="border:none;" align="center"> </th>
              <th colspan="7" align="center" char=".">systems<hr/></th>
            </tr>
            <tr>
              <th style="border:none;" align="center"> </th>
              <th style="border:none;" align="center" char=".">
                <monospace>DHFR</monospace>
              </th>
              <th style="border:none;" align="center" char=".">
                <monospace>M<sup>pro</sup></monospace>
              </th>
              <th style="border:none;" align="center" char=".">
                <monospace>COX</monospace>
              </th>
              <th style="border:none;" align="center" char=".">
                <monospace>Pond</monospace>
              </th>
              <th style="border:none;" align="center" char=".">
                <monospace>Lake</monospace>
              </th>
              <th style="border:none;" align="center" char=".">
                <monospace>STMV</monospace>
              </th>
              <th style="border:none;" align="center">
                <monospace>Bay</monospace>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td colspan="8" style="border:none;" align="center">DP Quadro GV100</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">RESPA 2 fs</td>
              <td style="border:none;" align="char" char=".">11.24</td>
              <td style="border:none;" align="char" char=".">2.91</td>
              <td style="border:none;" align="char" char=".">1.76</td>
              <td style="border:none;" align="char" char=".">1.08</td>
              <td style="border:none;" align="char" char=".">0.36</td>
              <td style="border:none;" align="char" char=".">0.24</td>
              <td style="border:none;" align="left">0.11</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">BAOAB-RESPA1 10 fs</td>
              <td style="border:none;" align="char" char=".">22.03</td>
              <td style="border:none;" align="char" char=".">6.09</td>
              <td style="border:none;" align="char" char=".">3.61</td>
              <td style="border:none;" align="char" char=".">2.25</td>
              <td style="border:none;" align="char" char=".">0.76</td>
              <td style="border:none;" align="char" char=".">0.53</td>
              <td style="border:none;" align="left">0.24</td>
            </tr>
            <tr>
              <td colspan="8" style="border:none;" align="center">MP</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">RESPA 2 fs</td>
              <td style="border:none;" align="char" char=".">21.75</td>
              <td style="border:none;" align="char" char=".">5.98</td>
              <td style="border:none;" align="char" char=".">3.69</td>
              <td style="border:none;" align="char" char=".">2.20</td>
              <td style="border:none;" align="char" char=".">0.70</td>
              <td style="border:none;" align="char" char=".">0.44</td>
              <td style="border:none;" align="left">0.20</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">BAOAB-RESPA1 10 fs</td>
              <td style="border:none;" align="char" char=".">40.73</td>
              <td style="border:none;" align="char" char=".">12.80</td>
              <td style="border:none;" align="char" char=".">3.61</td>
              <td style="border:none;" align="char" char=".">4.58</td>
              <td style="border:none;" align="char" char=".">1.49</td>
              <td style="border:none;" align="char" char=".">1.01</td>
              <td style="border:none;" align="left">0.46</td>
            </tr>
            <tr>
              <td colspan="8" style="border:none;" align="center">FP</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">RESPA 2 fs</td>
              <td style="border:none;" align="char" char=".">21.46</td>
              <td style="border:none;" align="char" char=".">5.82</td>
              <td style="border:none;" align="char" char=".">3.57</td>
              <td style="border:none;" align="char" char=".">2.12</td>
              <td style="border:none;" align="char" char=".">0.67</td>
              <td style="border:none;" align="char" char=".">0.43</td>
              <td style="border:none;" align="left">0.20</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">BAOAB-RESPA1 10 fs</td>
              <td style="border:none;" align="char" char=".">40.65</td>
              <td style="border:none;" align="char" char=".">12.65</td>
              <td style="border:none;" align="char" char=".">7.77</td>
              <td style="border:none;" align="char" char=".">4.52</td>
              <td style="border:none;" align="char" char=".">1.47</td>
              <td style="border:none;" align="char" char=".">1.00</td>
              <td style="border:none;" align="left">0.45</td>
            </tr>
            <tr>
              <td colspan="8" style="border:none;" align="center">MP Geforce RTX-2080 Ti</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">RESPA 2 fs</td>
              <td style="border:none;" align="char" char=".">22.52</td>
              <td style="border:none;" align="char" char=".">5.35</td>
              <td style="border:none;" align="char" char=".">3.21</td>
              <td style="border:none;" align="char" char=".">1.82</td>
              <td style="border:none;" align="char" char=".">0.54</td>
              <td style="border:none;" align="char" char=".">0.33</td>
              <td style="border:none;" align="left">0.15</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">BAOAB-RESPA1 10 fs</td>
              <td style="border:none;" align="char" char=".">43.81</td>
              <td style="border:none;" align="char" char=".">11.85</td>
              <td style="border:none;" align="char" char=".">7.06</td>
              <td style="border:none;" align="char" char=".">4.06</td>
              <td style="border:none;" align="char" char=".">1.24</td>
              <td style="border:none;" align="char" char=".">0.82</td>
              <td style="border:none;" align="left">n/a</td>
            </tr>
            <tr>
              <td style="border:none;" align="left"> </td>
              <td colspan="7" align="char" char=".">FP</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">RESPA 2 fs</td>
              <td style="border:none;" align="char" char=".">24.95</td>
              <td style="border:none;" align="char" char=".">5.73</td>
              <td style="border:none;" align="char" char=".">3.45</td>
              <td style="border:none;" align="char" char=".">1.95</td>
              <td style="border:none;" align="char" char=".">0.57</td>
              <td style="border:none;" align="char" char=".">0.35</td>
              <td style="border:none;" align="left">0.16</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">BAOAB-RESPA1 10 fs</td>
              <td style="border:none;" align="char" char=".">47.31</td>
              <td style="border:none;" align="char" char=".">12.78</td>
              <td style="border:none;" align="char" char=".">7.63</td>
              <td style="border:none;" align="char" char=".">4.35</td>
              <td style="border:none;" align="char" char=".">1.32</td>
              <td style="border:none;" align="char" char=".">0.87</td>
              <td style="border:none;" align="left">n/a</td>
            </tr>
            <tr>
              <td colspan="8" style="border:none;" align="center">MP
Geforce RTX-3090</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">RESPA 2 fs</td>
              <td style="border:none;" align="char" char=".">29.14</td>
              <td style="border:none;" align="char" char=".">7.79</td>
              <td style="border:none;" align="char" char=".">4.76</td>
              <td style="border:none;" align="char" char=".">2.81</td>
              <td style="border:none;" align="char" char=".">0.91</td>
              <td style="border:none;" align="char" char=".">0.60</td>
              <td style="border:none;" align="left">0.28</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">BAOAB-RESPA1 10 fs</td>
              <td style="border:none;" align="char" char=".">52.80</td>
              <td style="border:none;" align="char" char=".">15.79</td>
              <td style="border:none;" align="char" char=".">9.61</td>
              <td style="border:none;" align="char" char=".">5.52</td>
              <td style="border:none;" align="char" char=".">1.81</td>
              <td style="border:none;" align="char" char=".">1.23</td>
              <td style="border:none;" align="left">0.59</td>
            </tr>
            <tr>
              <td colspan="8" style="border:none;" align="center">FP</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">RESPA 2 fs</td>
              <td style="border:none;" align="char" char=".">32.00</td>
              <td style="border:none;" align="char" char=".">8.37</td>
              <td style="border:none;" align="char" char=".">5.10</td>
              <td style="border:none;" align="char" char=".">3.02</td>
              <td style="border:none;" align="char" char=".">0.96</td>
              <td style="border:none;" align="char" char=".">0.64</td>
              <td style="border:none;" align="left">0.30</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">BAOAB-RESPA1 10 fs</td>
              <td style="border:none;" align="char" char=".">57.67</td>
              <td style="border:none;" align="char" char=".">17.20</td>
              <td style="border:none;" align="char" char=".">10.46</td>
              <td style="border:none;" align="char" char=".">5.96</td>
              <td style="border:none;" align="char" char=".">1.90</td>
              <td style="border:none;" align="char" char=".">1.32</td>
              <td style="border:none;" align="left">0.63</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <fig id="fig16" position="float">
        <label>Figure 16</label>
        <caption>
          <p>Single node mixed precision scalability on
the Jean-Zay Cluster
(V100) using the AMOEBA polarizable force field.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0019" id="gr16" position="float"/>
      </fig>
      <p>On a single GPU, the BAOAB-RESPA1 integrator performs almost twice
as fast as RESPA in all cases: 22.53–42.83 ns/day on <monospace>DHFR</monospace>, 0.57–1.11 ns/day for the <monospace>STMV</monospace> virus. Regarding the RESPA integrator, results compared
with those obtained in DP (<xref rid="tbl1" ref-type="other">Table <xref rid="tbl1" ref-type="other">1</xref></xref>) are now consistent with the Quadro V100 theoretical
performance. Moreover, we observe a significant improvement on single
V100 cards with DP in comparison to the O<sc>pen</sc>ACC implementation,
which shows that the algorithm is better suited to the architecture.
However, this new algorithm considerably underperforms on Geforce
architecture. For instance, for the <monospace>COX</monospace> system
the speed goes from 0.65 ns/j with the O<sc>pen</sc>ACC implementation
to 0.19 ns/j with the adapted CUDA implementation on Geforce
RTX-2080 Ti. This is obviously related to architecture constraints
(lack of DP Compute units, sensitivity to SIMD divergence branch,
instruction latency) and shows that there is still room for optimization.
Tinker-HP is tuned to select the quickest algorithm depending on the
target device. Concerning MP performance on Geforce Cards, we finally
get the expected ratio compared with DP: increasing computation per
access improves the use of the device (<xref rid="tbl3" ref-type="other">Table <xref rid="tbl3" ref-type="other">3</xref></xref>). Geforce RTX-2080 Ti and GV100 results
are close until the <monospace>COX</monospace> test case, which is
consistent with their computing power, but GV100 performs better for
larger systems. It is certainly due to the difference in memory bandwidth
which allows GV100 to perform better on memory bound kernels and to
reach peak performance more easily. For example, most of PME reciprocal
space kernels are memory bounded due to numerous accesses to the three-dimensional
grid during the building and extracting process.</p>
      <p>A further comparison
between architectures is given in the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jctc.0c01164/suppl_file/ct0c01164_si_001.pdf">Supporting Information</ext-link>.</p>
      <p>For FP simulations, as expected, we do not
see any performance
difference with MP on V100 cards unlike Geforce ones, which exhibit
an 8% acceleration in average as the DP accumulation is being replaced
by an integer one (an instruction natively handled by compute cores). <xref rid="tbl6" ref-type="other">Table <xref rid="tbl6" ref-type="other">6</xref></xref> shows the performance
of Tinker-OpenMM: with the same RESPA framework, Tinker-HP performs
12–30% better on GV100 when the system size grows. With Geforce
RTX-2080 Ti the difference is slightly more steady except for the
Lake test case: around 18% and 25% better performance with Tinker-HP
respectively with MP and FP compared to Tinker-OpenMM.</p>
      <table-wrap id="tbl6" position="float">
        <label>Table 6</label>
        <caption>
          <title>Tinker-OpenMM Mixed Precision Performances
Assessed with the RESPA Framework</title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th style="border:none;" align="center"> </th>
              <th colspan="7" align="center" char=".">systems<hr/></th>
            </tr>
            <tr>
              <th style="border:none;" align="center"> </th>
              <th style="border:none;" align="center" char=".">
                <monospace>DHFR</monospace>
              </th>
              <th style="border:none;" align="center" char=".">
                <monospace>M<sup>pro</sup></monospace>
              </th>
              <th style="border:none;" align="center" char=".">
                <monospace>COX</monospace>
              </th>
              <th style="border:none;" align="center" char=".">
                <monospace>Pond</monospace>
              </th>
              <th style="border:none;" align="center" char=".">
                <monospace>Lake</monospace>
              </th>
              <th style="border:none;" align="center" char=".">
                <monospace>STMV</monospace>
              </th>
              <th style="border:none;" align="center">
                <monospace>Bay</monospace>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">Quadro GV100</td>
              <td style="border:none;" align="char" char=".">17.53</td>
              <td style="border:none;" align="char" char=".">4.50</td>
              <td style="border:none;" align="char" char=".">2.56</td>
              <td style="border:none;" align="char" char=".">1.68</td>
              <td style="border:none;" align="char" char=".">0.56</td>
              <td style="border:none;" align="char" char=".">0.34</td>
              <td style="border:none;" align="left">n/a</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">Geforce RTX-2080 Ti</td>
              <td style="border:none;" align="char" char=".">18.97</td>
              <td style="border:none;" align="char" char=".">4.37</td>
              <td style="border:none;" align="char" char=".">2.63</td>
              <td style="border:none;" align="char" char=".">1.66</td>
              <td style="border:none;" align="char" char=".">0.55</td>
              <td style="border:none;" align="char" char=".">0.28</td>
              <td style="border:none;" align="left">n/a</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The parallel scalability
starts to be effective above 100000 atoms.
This is partly because of the mandatory host synchronizations needed
by MPI and because of the difference in performance between synchronous
and asynchronous computation under that scale (for example, <monospace>DHFR</monospace> production drops to 12 ns/day when running
synchronously with the host). Kernel launching times are almost equivalent
to their execution time and they do not overlap. Each GPU on the Jean-Zay
Supercomputer comes with a 300 GB/s interconnection NVlink
bandwidth. Four GPUs per node, all of them being interconnected, represents
then a 100 Gb/s interconnection for each GPU pairs. The third
generation PCI-Express bridge to the host memory only delivers 16 Gb/s.
With the RESPA integrator operating on a full node made of 4 T V100,
the speed ratio grows from 1.14 to 1.95, respectively, from <monospace>Puddle</monospace> to <monospace>Bay</monospace> test cases in comparison
to a single device execution. The relatively balanced load between
PME real and reciprocal space allows us to break through the scalability
limit on almost every run with 2 GPUs with PME separation enabled.
Performance is always worse on 4 GPUs with 1 GPU dedicated to the
reciprocal space and the others to the direct space for the same reason
mentioned earlier (direct/reciprocal space load balancing). We also
diminished the communication overhead by overlapping communication
and computation. Note that on a complete node of Jean-Zay with 4 GPUs,
the bandwidth is statically shared between all of them, which means
that the performance showed here on 2 GPUs is less that what can be
expected on a node that would only consist in 2 GPUs interconnected
through NVlink. With the BAOAB-RESPA1 integrator, ratios between a
full node and a single device vary from 1.07 to a maximum of 1.58.
Because of the additional short-range real space interactions, it
is unsuited for PME separation, yet the reduced amount of FFt offers
a potential for scalability higher than RESPA. Such a delay in the
strong scalability is understandable given the device computational
speed, the size of the messages size imposed by the parallel distribution,
and the configuration run. The overhead of the MPI layer for STMV
with BAOAB-RESPA1 and a 4 GPU bench is on average 41% of a time step.
It consists mostly in FFt grid exchange in addition to the communication
of dipoles in the polarization solver. This is an indication of the
theoretical gain we can obtain with an improvement of the interconnect
technology or the MPI layer. Ideally, we can expect to produce 2.63 ns/day
on a single node instead of 1.55 ns/day. It is already satisfactory
to be able to scale on such huge systems and further efforts will
be made to improve multi-GPU results in the future.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <title>Toward Larger
Systems</title>
    <p>As one of the goals of the development of Tinker-HP
is to be able
to treat (very) large biological systems such as protein complexes
or entire viruses encompassing up to several millions of atoms (as
it is already the case with the CPU implementation<sup><xref ref-type="bibr" rid="ref20">20</xref>,<xref ref-type="bibr" rid="ref22">22</xref></sup> by using thousands of CPU cores), we review in the following section
the scalability limit of the GPU implementation in terms of system
size knowing that GPUs do not have the same memory capabilities: where
classical CPU nodes routinely benefit from more than 128 GB
of memory, the most advanced Ampere GPU architecture holds up to 40 GB
of memory.</p>
    <sec id="sec4.1">
      <title>Tinker-HP Memory Management Model</title>
      <p>MD with 3D spatial
decomposition has its own pattern when it comes to memory distribution
among MPI processes. We use the midpoint rule to compute real space
interactions as it is done in the CPU implementation.</p>
      <p>In practice,
it means that each process holds information about its neighbors (to
be able to compute the proper forces). More precisely, a domain <bold>ψ</bold><sub><italic>q</italic></sub> belongs to the neighborhood
of <bold>ψ</bold><sub><italic>p</italic></sub> if the minimum
distance between them is under some cutoff distance plus a buffer.
To simplify data exchange between processes, we transfer all positions
in a single message; the same thing is done with the forces.</p>
      <p>An additional filtering is then performed to list the atoms actually
involved in the interactions computed by a domain <bold>ψ</bold><sub><italic>p</italic></sub>. An atom, α ∈ <bold>Ω</bold>, belongs to domain <bold>ψ</bold><sub><italic>p</italic></sub>’s interaction area (<bold>λ</bold><sub><italic>p</italic></sub>) if the distance between this atom and the domain is below <inline-formula id="d33e2456"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_m015.gif"/></inline-formula>.</p>
      <p>Let us call <bold><italic>n</italic></bold><sub><italic>p</italic></sub> the number of atoms
belonging to <bold>ψ</bold><sub><italic>p</italic></sub>, <bold><italic>n</italic></bold><sub><italic>b</italic></sub> the number
of atoms belonging to a process domain
and its neighbors, and <bold><italic>n</italic></bold><sub><italic>l</italic></sub> the number of atoms inside <bold>λ</bold><sub><italic>p</italic></sub>. This is illustrated in <xref rid="fig17" ref-type="fig">Figure <xref rid="fig17" ref-type="fig">17</xref></xref>.</p>
      <fig id="fig17" position="float">
        <label>Figure 17</label>
        <caption>
          <p>Two-dimensional spatial
decomposition of a simulation box with
MPI distribution across 16 processes. ω<sub>6</sub> collects
all the neighboring domains of <bold>ψ</bold><sub>6</sub>. Here <bold><italic>n</italic></bold><sub><italic>b</italic></sub> &lt; <bold><italic>n</italic></bold>.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0020" id="gr17" position="float"/>
      </fig>
      <p>One can see that all data reserved with a size proportional to <bold><italic>n</italic></bold><sub><italic>p</italic></sub> are equally
distributed among processes. Those with size proportional to <bold><italic>n</italic></bold><sub><italic>b</italic></sub> are only partially
distributed. This means that these data structures are not distributed
if all domains <bold>ψ</bold><sub><italic>p</italic></sub> are
neighbors. This is why in practice the distribution only takes place
at that level with a relatively high number of process, more than
26 at least on a large box with 3d domain decomposition. On the other
hand, data allocated with a size proportional to <bold><italic>n</italic></bold><sub><italic>l</italic></sub> (like the neighbor list) are
always more distributed when the number of processes increases.</p>
      <p>On top of that, some data remain undistributed (proportional to <bold><italic>n</italic></bold>) like the atomic parameters of each potential
energy term. Splitting those among MPI processes would severely increase
the communication cost, which we can not afford. As we cannot predict
how one atom will interact and move inside <bold>Ω</bold>, the
best strategy regarding such data is to make it available to each
process. Reference Tinker-HP reduces the associated memory footprint
by using MPI shared memory space: only one parameter data instance
is shared among all processes within the same node.</p>
      <p>No physical
shared memory exists between GPUs of a node, and the
only way to deal with undistributed data is by replicating them on
each device, which is quickly impractical for large systems.</p>
      <p>In the next section, we detail a strategy allowing to circumvent
this limitation.</p>
    </sec>
    <sec id="sec4.2">
      <title>NVSHMEM Feature Implementation</title>
      <p>As
explained above,
distribution of parameter data would necessarily results in additional
communications. Regarding data exchange optimizations between GPU
devices, NVIDIA develops a new library based on the OpenSHMEM<sup><xref ref-type="bibr" rid="ref54">54</xref></sup> programming pattern, which is called NVSHMEM.<sup><xref ref-type="bibr" rid="ref29">29</xref></sup> This library provides thread communication routines
that operate on a symmetric memory on each device meaning that it
is possible to initiate device communication inside kernels and not
outside with an API like MPI. The immediate benefit of such approach
resides in the fact that communications are automatically recovered
by kernel instructions and can thereby participate to recover device
internal latency. This library allows us to distribute <bold><italic>n</italic></bold> scale data over devices within one node.</p>
      <p>Our implementation follows this scheme: divide a data structure (an
array for instance) across devices belonging to the same node following
the global numbering of the atoms and access this data inside a kernel
with the help of NVSHMEM library. To do that, we rely on a NVHSMEM
feature that consists of storing a symmetric memory allocation address
in a pointer on which arithmetic operations can be done. Then, depending
on the address returned by the pointer, either a global memory access
(GBA) or a remote memory access (RMA) is instructed to fetch the data.
The implementation requires a Fortran interface to be operational
since NVSHMEM source code is written in the C language. Moreover,
an additional operation is required for every allocation performed
by the NVSHMEM specific allocator to make the data allocated accessible
through O<sc>pen</sc>ACC kernels. See <xref rid="fig18" ref-type="fig">Figure <xref rid="fig18" ref-type="fig">18</xref></xref>.</p>
      <fig id="fig18" position="float">
        <label>Figure 18</label>
        <caption>
          <p>NVSHMEM memory distribution pattern across
a four interconnected
devices node. A symmetric reserved space is allocated by the NVSHMEM
library at initialization. Thus, data are equally split across all
devices in order. Every time a device needs to access data allocated
with NVSHMEM, either a GMA or RMA is issued.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0021" id="gr18" position="float"/>
      </fig>
      <p>Such a singular approach affects performances since additional
communications have to be made inside kernels. Furthermore, all communications
do not follow a special pattern that would leave room for optimizations,
meaning that each device accesses data randomly from the others depending
on the atoms involved in the interactions it needs to compute. In
order to limit performance loss, we can decide which data are going
to be split across devices and which kernels are going to be involved
with this approach. In practice, we use this scheme for the parameters
of the bonded potentials.</p>
      <p>Doing so, we distribute most of the
parameter data (torsions, angles,
bonds, ...) and therefore reduce the duplicated memory footprint.</p>
    </sec>
    <sec id="sec4.3">
      <title>Perspectives and Additional Results</title>
      <p>During our NVSHMEM
implementation, we were able to detect and optimize several memory
wells. For instance, the adjacency matrix described in the <xref rid="sec3.3" ref-type="other">Neighbor List</xref> subsection has a quadratic memory
requirement following the groups of atoms. This means that this represents
a potential risk of memory saturation on a single device. To prevent
this, we implemented a buffer limit on this matrix to construct the
pair-group list piece by piece. We also implemented algorithms that
prioritize computing and searching over storing where ever needed,
essentially scaling factor reconstruction. In the end, Tinker-HP is
able to reach a performance of 0.15 ns/day for a 7776000 atoms
water box with the AMOEBA force field and the BAOAB-RESPA1 integrator
on a single V100 and scale-out to 0.25 ns/day on a complete
node of Jean-Zay on the same system.</p>
      <p>We also had the opportunity
to test our implementation on the latest generation NVIDIA GPU Ampere
architecture: the Selene supercomputer which is made of nodes consisting
in DGX-A100 servers. A DGX-A100 server contains eight A100 graphic
cards with 40 GB of memory each and with latest generation interconnection
NVIDIA Switches. The results we obtained on such a node with the same
systems as above in the same RESPA and BAOAB-RESPA1 framework are
listed in <xref rid="tbl8" ref-type="other">Table <xref rid="tbl8" ref-type="other">7</xref></xref> and <xref rid="fig19" ref-type="fig">Figure <xref rid="fig19" ref-type="fig">19</xref></xref>.</p>
      <table-wrap id="tbl8" position="float">
        <label>Table 7</label>
        <caption>
          <title>Performance Synthesis and Scalability
Results on the Jean-Zay (V100) and Selene (A100) Machines<xref rid="tbl8-fn1" ref-type="table-fn">a</xref></title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
          </colgroup>
          <thead>
            <tr>
              <th style="border:none;" align="center"> </th>
              <th style="border:none;" align="center" char="."> </th>
              <th colspan="2" align="center" char=".">Jean-Zay (V100)<hr/></th>
              <th colspan="2" align="center" char=".">Selene (A100)<hr/></th>
            </tr>
            <tr>
              <th style="border:none;" align="center">systems</th>
              <th style="border:none;" align="center" char=".">size (no.
of atoms)</th>
              <th style="border:none;" align="center" char=".">perf (ns/day); 1GPU</th>
              <th style="border:none;" align="center" char=".">best perf (ns/day); #GPU</th>
              <th style="border:none;" align="center" char=".">perf (ns/day); 1GPU</th>
              <th style="border:none;" align="center" char=".">best perf (ns/day); #GPU</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">
                <monospace>DHFR</monospace>
              </td>
              <td style="border:none;" align="char" char=".">23558</td>
              <td style="border:none;" align="char" char=".">43.83</td>
              <td style="border:none;" align="char" char=".">43.83</td>
              <td style="border:none;" align="char" char=".">44.96</td>
              <td style="border:none;" align="char" char=".">44.96</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">Puddle</td>
              <td style="border:none;" align="char" char=".">96000</td>
              <td style="border:none;" align="char" char=".">14.63</td>
              <td style="border:none;" align="char" char=".">15.76; 4</td>
              <td style="border:none;" align="char" char=".">15.57</td>
              <td style="border:none;" align="char" char=".">17.57</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">Mpro</td>
              <td style="border:none;" align="char" char=".">98694</td>
              <td style="border:none;" align="char" char=".">14.03</td>
              <td style="border:none;" align="char" char=".">14.57; 4</td>
              <td style="border:none;" align="char" char=".">16.36</td>
              <td style="border:none;" align="char" char=".">17.47; 4</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">
                <monospace>COX</monospace>
              </td>
              <td style="border:none;" align="char" char=".">174219</td>
              <td style="border:none;" align="char" char=".">8.64</td>
              <td style="border:none;" align="char" char=".">10.15; 4</td>
              <td style="border:none;" align="char" char=".">10.47</td>
              <td style="border:none;" align="char" char=".">11.75; 4</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">Pond</td>
              <td style="border:none;" align="char" char=".">288000</td>
              <td style="border:none;" align="char" char=".">4.90</td>
              <td style="border:none;" align="char" char=".">6.72; 4</td>
              <td style="border:none;" align="char" char=".">6.18</td>
              <td style="border:none;" align="char" char=".">10.60; 8</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">Lake</td>
              <td style="border:none;" align="char" char=".">864000</td>
              <td style="border:none;" align="char" char=".">1.62</td>
              <td style="border:none;" align="char" char=".">2.40; 4</td>
              <td style="border:none;" align="char" char=".">2.11</td>
              <td style="border:none;" align="char" char=".">5.50; 8</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">
                <monospace>STMV</monospace>
              </td>
              <td style="border:none;" align="char" char=".">1066624</td>
              <td style="border:none;" align="char" char=".">1.11</td>
              <td style="border:none;" align="char" char=".">1.77; 4</td>
              <td style="border:none;" align="char" char=".">1.50</td>
              <td style="border:none;" align="char" char=".">4.51; 8</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">SARS-Cov2</td>
              <td style="border:none;" align="char" char=".">1509506</td>
              <td style="border:none;" align="char" char=".">0.89</td>
              <td style="border:none;" align="char" char=".">1.55; 4</td>
              <td style="border:none;" align="char" char=".">1.32</td>
              <td style="border:none;" align="char" char=".">4.16; 8</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">Spike-ACE2</td>
              <td style="border:none;" align="char" char="."> </td>
              <td style="border:none;" align="char" char="."> </td>
              <td style="border:none;" align="char" char="."> </td>
              <td style="border:none;" align="char" char="."> </td>
              <td style="border:none;" align="char" char="."> </td>
            </tr>
            <tr>
              <td style="border:none;" align="left">Bay</td>
              <td style="border:none;" align="char" char=".">2592000</td>
              <td style="border:none;" align="char" char=".">0.50</td>
              <td style="border:none;" align="char" char=".">0.77; 4</td>
              <td style="border:none;" align="char" char=".">0.59</td>
              <td style="border:none;" align="char" char=".">2.38; 8</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">Sea</td>
              <td style="border:none;" align="char" char=".">7776000</td>
              <td style="border:none;" align="char" char=".">0.15</td>
              <td style="border:none;" align="char" char=".">0.25; 4</td>
              <td style="border:none;" align="char" char=".">0.22</td>
              <td style="border:none;" align="char" char=".">0.78; 8</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tbl8-fn1">
            <label>a</label>
            <p>MD production in ns/day with
the AMOEBA polarizable force field.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <fig id="fig19" position="float">
        <label>Figure 19</label>
        <caption>
          <p>Performance and one node scalibility results with the AMOEBA force
field.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_0022" id="gr19" position="float"/>
      </fig>
      <p>We observe an average of 50% of
performance gain for systems larger
than 100000 atoms on a single A100 compared to a single V100 card.
Also, the more efficient interconnection between cards (NV-switch
compared to NV-link) allows us to scale better on several GPUs with
the best performances ever obtained with our code on all the benchmark
systems, the larger ones making use of all the 8 cards of the node.
Although the code is designed to do so, the latency and the speed
of the internode interconnection on the present Jean-Zay and Selene
supercomputers did not allow us to scale efficiently across nodes,
even on the largest systems. Jean-Zay provides 32 GB/s of network
interconnection between nodes so that each GPU pair has access to
a 16 GB/s bandwidth. Unlike the 100 GB/s shared between
each GPU inside a node, we expect internode transit times to be 6.25–12.5
time slower without taking the latency into account. This is illustrated
by the experiment summarized in <xref rid="tbl7" ref-type="other">Table <xref rid="tbl7" ref-type="other">8</xref></xref> as we observe the sudden increase of the overhead
of the MPI layer relative to the total duration of a time step when
running on two nodes. In this case, changing the domain decomposition
dimension to limit the number of neighboring process quadruples the
production and exposes the latency issue (expressed here by the difference
between the fastest and the slowest MPI process). In a multinode context
the bottleneck clearly lies in the internode communications. The very
fast evolution of the compilers, as well as the incoming availability
of new classes of large pre-exascale supercomputers may improve this
situation in the future. Presently, the use of multiple nodes for
a single trajectory is the subject of active work within our group
and results will be shared in due course. Still, one can already make
use of several nodes with the present implementation by using methods
such as unsupervised adaptive sampling as we recently proposed.<sup><xref ref-type="bibr" rid="ref53">53</xref></sup> Such pleasingly parallel approach already offers
the possibility to use hundreds (if not thousands!) of GPU cards simultaneously.</p>
      <table-wrap id="tbl7" position="float">
        <label>Table 8</label>
        <caption>
          <title>Multinode Performance on Jean-Zay
with the Sea System and BAOAB-RESPA1<xref rid="tbl7-fn1" ref-type="table-fn">a</xref></title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th style="border:none;" align="center">no. of GPU:</th>
              <th colspan="2" align="center">4<hr/></th>
              <th colspan="2" align="center">8<hr/></th>
            </tr>
            <tr>
              <th style="border:none;" align="center">domain decomposition:</th>
              <th style="border:none;" align="center">3d</th>
              <th style="border:none;" align="center">1d</th>
              <th style="border:none;" align="center">3d</th>
              <th style="border:none;" align="center">1d</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">production speed (ns/day)</td>
              <td style="border:none;" align="left">0.252</td>
              <td style="border:none;" align="left">0.265</td>
              <td style="border:none;" align="left">0.02</td>
              <td style="border:none;" align="left">0.08</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">MPI layer (%)</td>
              <td style="border:none;" align="left">24</td>
              <td style="border:none;" align="left">22</td>
              <td style="border:none;" align="left">97</td>
              <td style="border:none;" align="left">91</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">MPI latency (%)</td>
              <td style="border:none;" align="left">1</td>
              <td style="border:none;" align="left">1</td>
              <td style="border:none;" align="left">4</td>
              <td style="border:none;" align="left">11</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tbl7-fn1">
            <label>a</label>
            <p>Here the latency designates the
time difference between the fastest and the slowest process.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
  </sec>
  <sec id="sec5">
    <title>Conclusion</title>
    <p>We
presented the native Tinker-HP multi-GPU multiprecision acceleration
platform. The new code is shown to be accurate and scalable across
multiple GPU cards offering unprecedented performances and new capabilities
to deal with long time scale simulations on large realistic systems
using polarizable force fields such as AMOEBA. The approach strongly
reduces the time to solution offering to achieve routine simulations
that would have required thousands of CPUs on a single GPU card. Overall,
the GPU-accelerated Tinker-HP reaches the best performances ever obtained
for AMOEBA simulations and extends the applicability of polarizable
force fields. The package is shown to be compatible with various computer
GPU system architectures ranging from research laboratories to modern
supercomputers.</p>
    <p>Future work will focus on adding new features
(sampling methods,
integrators, ...) and on further optimizing the performance on multinodes/multi-GPUs
to address the exascale challenge. We will improve the nonpolarizable
force field simulations capabilities as we will provide the high performance
implementations of additional new generation polarizable many-body
force fields such as AMOEBA+,<sup><xref ref-type="bibr" rid="ref55">55</xref>,<xref ref-type="bibr" rid="ref56">56</xref></sup> SIBFA,<sup><xref ref-type="bibr" rid="ref57">57</xref></sup> and others. We will continue to develop the recently introduced
adaptive sampling computing strategy enabling the simultaneous use
of hundreds (thousands) of GPU cards to further reduce time to solution
and deeper explore conformational spaces at high-resolution.<sup><xref ref-type="bibr" rid="ref53">53</xref></sup> With such exascale-ready simulation setup, computations
that would have taken years can now be achieved in days thanks to
GPUs. Beyond this native Tinker-HP GPU platform and its various capabilities,
an interface to the Plumed library<sup><xref ref-type="bibr" rid="ref58">58</xref></sup> providing
additional methodologies for enhanced-sampling, free-energy calculations,
and the analysis of molecular dynamics simulations is also available.
Finally, the present work, which extensively exploits low precision
arithmetic, highlights the key fact that high-performance computing
(HPC) grounded applications such as Tinker-HP can now efficiently
use converged GPU-accelerated supercomputers, combining HPC and artificial
intelligence (AI) such as the Jean-Zay machine to actually enhance
their performances.</p>
  </sec>
</body>
<back>
  <notes id="notes1" notes-type="si">
    <title>Supporting Information Available</title>
    <p>The Supporting Information is
available free of charge at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/10.1021/acs.jctc.0c01164?goto=supporting-info">https://pubs.acs.org/doi/10.1021/acs.jctc.0c01164</ext-link>.<list id="silist" list-type="simple"><list-item><p>Additional information
regarding the performance using
nonpolarizable force fields as well as a comparison between peak performance
reachable by Tinker-HP in terms of FLOP/s (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://pubs.acs.org/doi/suppl/10.1021/acs.jctc.0c01164/suppl_file/ct0c01164_si_001.pdf">PDF</ext-link>)</p></list-item></list></p>
  </notes>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sifile1">
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ct0c01164_si_001.pdf">
        <caption>
          <p>ct0c01164_si_001.pdf</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <notes notes-type="COI-statement" id="notes2">
    <p>The authors
declare no competing financial interest.</p>
  </notes>
  <notes notes-type="" id="notes3">
    <title>Notes</title>
    <p>Code Availability:
The present code has been
released in phase advance in link with the High Performance Computing
community COVID-19 research efforts. The software is freely accessible
to Academics via GitHub: <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/TinkerTools/tinker-hp">https://github.com/TinkerTools/tinker-hp</uri></p>
  </notes>
  <ack>
    <title>Acknowledgments</title>
    <p>This work was made possible thanks to funding from
the European Research Council (ERC) under the European Union’s
Horizon 2020 research and innovation programme (grant agreement No.
810367), project EMC2. This project was initiated in 2019 with a “Contrat
de Progrès” grant from GENCI (France) in collaboration
with HPE and NVIDIA to port Tinker-HP on the Jean-Zay HPE SGI 8600
GPUs system (IDRIS supercomputer center, GENCI-CNRS, Orsay, France)
using O<sc>pen</sc>ACC. F.C. acknowledges funding from the French
state funds managed by the CalSimLab LABEX and the ANR within the
Investissements d’Avenir program (reference ANR11-IDEX-0004-02)
and support from the Direction Génerale de l’Armement
(DGA) Maîtrise NRBC of the French Ministry of Defense. Computations
have been performed at GENCI on the Jean Zay machine (IDRIS) on grant
no. A0070707671 and on the Irène Joliot Curie ATOS Sequana
X1000 supercomputer (TGCC, Bruyères le Chatel, CEA, France)
thanks to PRACE COVID-19 special allocation (projet COVID-HP). We
thank NVIDIA (Romuald Josien and François Courteille, NVIDIA
France) for offering us access to A100 supercomputer systems (DGX-A100
and Selene DGX-A100 SuperPod machines). P.R. and J.W.P. are grateful
for support by National Institutes of Health (R01GM106137 and R01GM114237).</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="ref1">
      <mixed-citation publication-type="journal" id="cit1"><name><surname>Hollingsworth</surname><given-names>S. A.</given-names></name>; <name><surname>Dror</surname><given-names>R. O.</given-names></name><article-title>Molecular Dynamics Simulation for All</article-title>. <source>Neuron</source><year>2018</year>, <volume>99</volume>, <fpage>1129</fpage>–<lpage>1143</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2018.08.011</pub-id>.<pub-id pub-id-type="pmid">30236283</pub-id></mixed-citation>
    </ref>
    <ref id="ref2">
      <mixed-citation publication-type="journal" id="cit2"><name><surname>Dror</surname><given-names>R. O.</given-names></name>; <name><surname>Dirks</surname><given-names>R. M.</given-names></name>; <name><surname>Grossman</surname><given-names>J.</given-names></name>; <name><surname>Xu</surname><given-names>H.</given-names></name>; <name><surname>Shaw</surname><given-names>D. E.</given-names></name><article-title>Biomolecular
Simulation: A Computational Microscope for Molecular Biology</article-title>. <source>Annu. Rev. Biophys.</source><year>2012</year>, <volume>41</volume>, <fpage>429</fpage>–<lpage>452</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-biophys-042910-155245</pub-id>.<pub-id pub-id-type="pmid">22577825</pub-id></mixed-citation>
    </ref>
    <ref id="ref3">
      <mixed-citation publication-type="book" id="cit3"><person-group person-group-type="allauthors"><name><surname>Ponder</surname><given-names>J. W.</given-names></name>; <name><surname>Case</surname><given-names>D. A.</given-names></name></person-group><source>Advances in protein
chemistry</source>; <publisher-name>Elsevier</publisher-name>, <year>2003</year>; Vol. <volume>66</volume>, pp <fpage>27</fpage>–<lpage>85</lpage>.<pub-id pub-id-type="pmid">14631816</pub-id></mixed-citation>
    </ref>
    <ref id="ref4">
      <mixed-citation publication-type="journal" id="cit4"><name><surname>Huang</surname><given-names>J.</given-names></name>; <name><surname>Rauscher</surname><given-names>S.</given-names></name>; <name><surname>Nawrocki</surname><given-names>G.</given-names></name>; <name><surname>Ran</surname><given-names>T.</given-names></name>; <name><surname>Feig</surname><given-names>M.</given-names></name>; <name><surname>de Groot</surname><given-names>B. L.</given-names></name>; <name><surname>Grubmüller</surname><given-names>H.</given-names></name>; <name><surname>MacKerell</surname><given-names>A. D.</given-names></name><article-title>CHARMM36m:
an improved force field for folded and intrinsically disordered proteins</article-title>. <source>Nat. Methods</source><year>2017</year>, <volume>14</volume>, <fpage>71</fpage>–<lpage>73</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.4067</pub-id>.<pub-id pub-id-type="pmid">27819658</pub-id></mixed-citation>
    </ref>
    <ref id="ref5">
      <mixed-citation publication-type="journal" id="cit5"><name><surname>Maier</surname><given-names>J. A.</given-names></name>; <name><surname>Martinez</surname><given-names>C.</given-names></name>; <name><surname>Kasavajhala</surname><given-names>K.</given-names></name>; <name><surname>Wickstrom</surname><given-names>L.</given-names></name>; <name><surname>Hauser</surname><given-names>K. E.</given-names></name>; <name><surname>Simmerling</surname><given-names>C.</given-names></name><article-title>ff14SB: Improving the Accuracy of
Protein Side Chain and Backbone Parameters from ff99SB</article-title>. <source>J. Chem. Theory Comput.</source><year>2015</year>, <volume>11</volume>, <fpage>3696</fpage>–<lpage>3713</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.5b00255</pub-id>.<pub-id pub-id-type="pmid">26574453</pub-id></mixed-citation>
    </ref>
    <ref id="ref6">
      <mixed-citation publication-type="journal" id="cit6"><name><surname>Jorgensen</surname><given-names>W. L.</given-names></name>; <name><surname>Maxwell</surname><given-names>D. S.</given-names></name>; <name><surname>Tirado-Rives</surname><given-names>J.</given-names></name><article-title>Development
and Testing of the OPLS
All-Atom Force Field on Conformational Energetics and Properties of
Organic Liquids</article-title>. <source>J. Am. Chem. Soc.</source><year>1996</year>, <volume>118</volume>, <fpage>11225</fpage>–<lpage>11236</lpage>. <pub-id pub-id-type="doi">10.1021/ja9621760</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref7">
      <mixed-citation publication-type="journal" id="cit7"><name><surname>Oostenbrink</surname><given-names>C.</given-names></name>; <name><surname>Villa</surname><given-names>A.</given-names></name>; <name><surname>Mark</surname><given-names>A. E.</given-names></name>; <name><surname>Van Gunsteren</surname><given-names>W. F.</given-names></name><article-title>A biomolecular
force field based on the free enthalpy of hydration and solvation:
The GROMOS force-field parameter sets 53A5 and 53A6</article-title>. <source>J. Comput. Chem.</source><year>2004</year>, <volume>25</volume>, <fpage>1656</fpage>–<lpage>1676</lpage>. <pub-id pub-id-type="doi">10.1002/jcc.20090</pub-id>.<pub-id pub-id-type="pmid">15264259</pub-id></mixed-citation>
    </ref>
    <ref id="ref8">
      <mixed-citation publication-type="book" id="cit8"><person-group person-group-type="allauthors"><name><surname>Shi</surname><given-names>Y.</given-names></name>; <name><surname>Ren</surname><given-names>P.</given-names></name>; <name><surname>Schnieders</surname><given-names>M.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name></person-group><article-title>Polarizable Force Fields for
Biomolecular Modeling</article-title>. In <source>Reviews in Computational
Chemistry Vol. 28</source>; <publisher-name>John Wiley and Sons, Ltd.</publisher-name>, <year>2015</year>; Chapter 2, pp <fpage>51</fpage>–<lpage>86</lpage>.<pub-id pub-id-type="doi">10.1002/9781118889886.ch2</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref9">
      <mixed-citation publication-type="journal" id="cit9"><name><surname>Jing</surname><given-names>Z.</given-names></name>; <name><surname>Liu</surname><given-names>C.</given-names></name>; <name><surname>Cheng</surname><given-names>S. Y.</given-names></name>; <name><surname>Qi</surname><given-names>R.</given-names></name>; <name><surname>Walker</surname><given-names>B. D.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name>; <name><surname>Ren</surname><given-names>P.</given-names></name><article-title>Polarizable Force Fields for Biomolecular Simulations: Recent Advances
and Applications</article-title>. <source>Annu. Rev. Biophys.</source><year>2019</year>, <volume>48</volume>, <fpage>371</fpage>–<lpage>394</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-biophys-070317-033349</pub-id>.<pub-id pub-id-type="pmid">30916997</pub-id></mixed-citation>
    </ref>
    <ref id="ref10">
      <mixed-citation publication-type="journal" id="cit10"><name><surname>Melcr</surname><given-names>J.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name><article-title>Accurate biomolecular
simulations account for electronic
polarization</article-title>. <source>Front. Mol. Biosci.</source><year>2019</year>, <volume>6</volume>, <fpage>143</fpage><pub-id pub-id-type="doi">10.3389/fmolb.2019.00143</pub-id>.<pub-id pub-id-type="pmid">31867342</pub-id></mixed-citation>
    </ref>
    <ref id="ref11">
      <mixed-citation publication-type="journal" id="cit11"><name><surname>Bedrov</surname><given-names>D.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name>; <name><surname>Borodin</surname><given-names>O.</given-names></name>; <name><surname>MacKerell</surname><given-names>A. D.</given-names></name>; <name><surname>Roux</surname><given-names>B.</given-names></name>; <name><surname>Schröder</surname><given-names>C.</given-names></name><article-title>Molecular
Dynamics Simulations of
Ionic Liquids and Electrolytes Using Polarizable Force Fields</article-title>. <source>Chem. Rev.</source><year>2019</year>, <volume>119</volume>, <fpage>7940</fpage>–<lpage>7995</lpage>. <pub-id pub-id-type="doi">10.1021/acs.chemrev.8b00763</pub-id>.<pub-id pub-id-type="pmid">31141351</pub-id></mixed-citation>
    </ref>
    <ref id="ref12">
      <mixed-citation publication-type="journal" id="cit12"><name><surname>Lopes</surname><given-names>P. E. M.</given-names></name>; <name><surname>Huang</surname><given-names>J.</given-names></name>; <name><surname>Shim</surname><given-names>J.</given-names></name>; <name><surname>Luo</surname><given-names>Y.</given-names></name>; <name><surname>Li</surname><given-names>H.</given-names></name>; <name><surname>Roux</surname><given-names>B.</given-names></name>; <name><surname>MacKerell</surname><given-names>A. D.</given-names></name><article-title>Polarizable Force Field for Peptides
and Proteins Based on the Classical Drude Oscillator</article-title>. <source>J. Chem. Theory Comput.</source><year>2013</year>, <volume>9</volume>, <fpage>5430</fpage>–<lpage>5449</lpage>. <pub-id pub-id-type="doi">10.1021/ct400781b</pub-id>.<pub-id pub-id-type="pmid">24459460</pub-id></mixed-citation>
    </ref>
    <ref id="ref13">
      <mixed-citation publication-type="journal" id="cit13"><name><surname>Lemkul</surname><given-names>J. A.</given-names></name>; <name><surname>Huang</surname><given-names>J.</given-names></name>; <name><surname>Roux</surname><given-names>B.</given-names></name>; <name><surname>MacKerell</surname><given-names>A. D.</given-names></name><article-title>An Empirical
Polarizable Force Field Based on the Classical Drude Oscillator Model:
Development History and Recent Applications</article-title>. <source>Chem. Rev.</source><year>2016</year>, <volume>116</volume>, <fpage>4983</fpage>–<lpage>5013</lpage>. <pub-id pub-id-type="doi">10.1021/acs.chemrev.5b00505</pub-id>.<pub-id pub-id-type="pmid">26815602</pub-id></mixed-citation>
    </ref>
    <ref id="ref14">
      <mixed-citation publication-type="journal" id="cit14"><name><surname>Lin</surname><given-names>F.-Y.</given-names></name>; <name><surname>Huang</surname><given-names>J.</given-names></name>; <name><surname>Pandey</surname><given-names>P.</given-names></name>; <name><surname>Rupakheti</surname><given-names>C.</given-names></name>; <name><surname>Li</surname><given-names>J.</given-names></name>; <name><surname>Roux</surname><given-names>B.</given-names></name>; <name><surname>MacKerell</surname><given-names>A. D.</given-names></name><article-title>Further Optimization and Validation
of the Classical Drude Polarizable Protein Force Field</article-title>. <source>J. Chem. Theory Comput.</source><year>2020</year>, <volume>16</volume>, <fpage>3221</fpage>–<lpage>3239</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.0c00057</pub-id>.<pub-id pub-id-type="pmid">32282198</pub-id></mixed-citation>
    </ref>
    <ref id="ref15">
      <mixed-citation publication-type="journal" id="cit15"><name><surname>Ren</surname><given-names>P. Y.</given-names></name>; <name><surname>Ponder</surname><given-names>J. W.</given-names></name><article-title>Polarizable Atomic Multipole Water
Model for Molecular
Mechanics Simulation</article-title>. <source>J. Phys. Chem. B</source><year>2003</year>, <volume>107</volume>, <fpage>5933</fpage>–<lpage>5947</lpage>. <pub-id pub-id-type="doi">10.1021/jp027815+</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref16">
      <mixed-citation publication-type="journal" id="cit16"><name><surname>Shi</surname><given-names>Y.</given-names></name>; <name><surname>Xia</surname><given-names>Z.</given-names></name>; <name><surname>Zhang</surname><given-names>J.</given-names></name>; <name><surname>Best</surname><given-names>R.</given-names></name>; <name><surname>Wu</surname><given-names>C.</given-names></name>; <name><surname>Ponder</surname><given-names>J. W.</given-names></name>; <name><surname>Ren</surname><given-names>P.</given-names></name><article-title>Polarizable
Atomic Multipole-Based AMOEBA Force Field for Proteins</article-title>. <source>J. Chem. Theory Comput.</source><year>2013</year>, <volume>9</volume>, <fpage>4046</fpage>–<lpage>4063</lpage>. <pub-id pub-id-type="doi">10.1021/ct4003702</pub-id>.<pub-id pub-id-type="pmid">24163642</pub-id></mixed-citation>
    </ref>
    <ref id="ref17">
      <mixed-citation publication-type="journal" id="cit17"><name><surname>Zhang</surname><given-names>C.</given-names></name>; <name><surname>Lu</surname><given-names>C.</given-names></name>; <name><surname>Jing</surname><given-names>Z.</given-names></name>; <name><surname>Wu</surname><given-names>C.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name>; <name><surname>Ponder</surname><given-names>J. W.</given-names></name>; <name><surname>Ren</surname><given-names>P.</given-names></name><article-title>AMOEBA Polarizable Atomic Multipole Force Field for Nucleic Acids</article-title>. <source>J. Chem. Theory Comput.</source><year>2018</year>, <volume>14</volume>, <fpage>2084</fpage>–<lpage>2108</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.7b01169</pub-id>.<pub-id pub-id-type="pmid">29438622</pub-id></mixed-citation>
    </ref>
    <ref id="ref18">
      <mixed-citation publication-type="journal" id="cit18"><name><surname>Jiang</surname><given-names>W.</given-names></name>; <name><surname>Hardy</surname><given-names>D. J.</given-names></name>; <name><surname>Phillips</surname><given-names>J. C.</given-names></name>; <name><surname>MacKerell</surname><given-names>A. D.</given-names></name>; <name><surname>Schulten</surname><given-names>K.</given-names></name>; <name><surname>Roux</surname><given-names>B.</given-names></name><article-title>High-Performance Scalable Molecular
Dynamics Simulations of a Polarizable Force Field Based on Classical
Drude Oscillators in NAMD</article-title>. <source>J. Phys. Chem. Lett.</source><year>2011</year>, <volume>2</volume>, <fpage>87</fpage>–<lpage>92</lpage>. <pub-id pub-id-type="doi">10.1021/jz101461d</pub-id>.<pub-id pub-id-type="pmid">21572567</pub-id></mixed-citation>
    </ref>
    <ref id="ref19">
      <mixed-citation publication-type="journal" id="cit19"><name><surname>Lemkul</surname><given-names>J. A.</given-names></name>; <name><surname>Roux</surname><given-names>B.</given-names></name>; <name><surname>van der
Spoel</surname><given-names>D.</given-names></name>; <name><surname>MacKerell</surname><given-names>A. D.</given-names><suffix>Jr.</suffix></name><article-title>Implementation
of extended Lagrangian dynamics in GROMACS
for polarizable simulations using the classical Drude oscillator model</article-title>. <source>J. Comput. Chem.</source><year>2015</year>, <volume>36</volume>, <fpage>1473</fpage>–<lpage>1479</lpage>. <pub-id pub-id-type="doi">10.1002/jcc.23937</pub-id>.<pub-id pub-id-type="pmid">25962472</pub-id></mixed-citation>
    </ref>
    <ref id="ref20">
      <mixed-citation publication-type="journal" id="cit20"><name><surname>Lagardère</surname><given-names>L.</given-names></name>; <name><surname>Jolly</surname><given-names>L.-H.</given-names></name>; <name><surname>Lipparini</surname><given-names>F.</given-names></name>; <name><surname>Aviat</surname><given-names>F.</given-names></name>; <name><surname>Stamm</surname><given-names>B.</given-names></name>; <name><surname>Jing</surname><given-names>Z. F.</given-names></name>; <name><surname>Harger</surname><given-names>M.</given-names></name>; <name><surname>Torabifard</surname><given-names>H.</given-names></name>; <name><surname>Cisneros</surname><given-names>G. A.</given-names></name>; <name><surname>Schnieders</surname><given-names>M. J.</given-names></name>; <name><surname>Gresh</surname><given-names>N.</given-names></name>; <name><surname>Maday</surname><given-names>Y.</given-names></name>; <name><surname>Ren</surname><given-names>P. Y.</given-names></name>; <name><surname>Ponder</surname><given-names>J. W.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name><article-title>Tinker-HP: a massively
parallel molecular dynamics package for multiscale simulations of
large complex systems with advanced point dipole polarizable force
fields</article-title>. <source>Chem. Sci.</source><year>2018</year>, <volume>9</volume>, <fpage>956</fpage>–<lpage>972</lpage>. <pub-id pub-id-type="doi">10.1039/C7SC04531J</pub-id>.<pub-id pub-id-type="pmid">29732110</pub-id></mixed-citation>
    </ref>
    <ref id="ref21">
      <mixed-citation publication-type="journal" id="cit21"><name><surname>Rackers</surname><given-names>J. A.</given-names></name>; <name><surname>Wang</surname><given-names>Z.</given-names></name>; <name><surname>Lu</surname><given-names>C.</given-names></name>; <name><surname>Laury</surname><given-names>M. L.</given-names></name>; <name><surname>Lagardère</surname><given-names>L.</given-names></name>; <name><surname>Schnieders</surname><given-names>M. J.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name>; <name><surname>Ren</surname><given-names>P.</given-names></name>; <name><surname>Ponder</surname><given-names>J. W.</given-names></name><article-title>Tinker
8: Software Tools for Molecular Design</article-title>. <source>J. Chem.
Theory Comput.</source><year>2018</year>, <volume>14</volume>, <fpage>5273</fpage>–<lpage>5289</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.8b00529</pub-id>.<pub-id pub-id-type="pmid">30176213</pub-id></mixed-citation>
    </ref>
    <ref id="ref22">
      <mixed-citation publication-type="journal" id="cit22"><name><surname>Jolly</surname><given-names>L.-H.</given-names></name>; <name><surname>Duran</surname><given-names>A.</given-names></name>; <name><surname>Lagardère</surname><given-names>L.</given-names></name>; <name><surname>Ponder</surname><given-names>J. W.</given-names></name>; <name><surname>Ren</surname><given-names>P.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name><article-title>Raising the Performance
of the Tinker-HP Molecular
Modeling Package [Article v1.0]</article-title>. <source>LiveCoMS</source><year>2019</year>, <volume>1</volume>, <fpage>10409</fpage><pub-id pub-id-type="doi">10.33011/livecoms.1.2.10409</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref23">
      <mixed-citation publication-type="journal" id="cit23"><name><surname>Stone</surname><given-names>J. E.</given-names></name>; <name><surname>Hardy</surname><given-names>D. J.</given-names></name>; <name><surname>Ufimtsev</surname><given-names>I. S.</given-names></name>; <name><surname>Schulten</surname><given-names>K.</given-names></name><article-title>GPU-accelerated molecular
modeling coming of age</article-title>. <source>J. Mol. Graphics Modell.</source><year>2010</year>, <volume>29</volume>, <fpage>116</fpage>–<lpage>125</lpage>. <pub-id pub-id-type="doi">10.1016/j.jmgm.2010.06.010</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref24">
      <mixed-citation publication-type="journal" id="cit24"><name><surname>Götz</surname><given-names>A. W.</given-names></name>; <name><surname>Williamson</surname><given-names>M. J.</given-names></name>; <name><surname>Xu</surname><given-names>D.</given-names></name>; <name><surname>Poole</surname><given-names>D.</given-names></name>; <name><surname>Le Grand</surname><given-names>S.</given-names></name>; <name><surname>Walker</surname><given-names>R. C.</given-names></name><article-title>Routine Microsecond
Molecular Dynamics Simulations
with AMBER on GPUs. 1. Generalized Born</article-title>. <source>J.
Chem. Theory Comput.</source><year>2012</year>, <volume>8</volume>, <fpage>1542</fpage>–<lpage>1555</lpage>. <pub-id pub-id-type="doi">10.1021/ct200909j</pub-id>.<pub-id pub-id-type="pmid">22582031</pub-id></mixed-citation>
    </ref>
    <ref id="ref25">
      <mixed-citation publication-type="journal" id="cit25"><name><surname>Páll</surname><given-names>S.</given-names></name>; <name><surname>Zhmurov</surname><given-names>A.</given-names></name>; <name><surname>Bauer</surname><given-names>P.</given-names></name>; <name><surname>Abraham</surname><given-names>M.</given-names></name>; <name><surname>Lundborg</surname><given-names>M.</given-names></name>; <name><surname>Gray</surname><given-names>A.</given-names></name>; <name><surname>Hess</surname><given-names>B.</given-names></name>; <name><surname>Lindahl</surname><given-names>E.</given-names></name><article-title>Heterogeneous Parallelization
and Acceleration of Molecular Dynamics Simulations in GROMACS</article-title>. <source>J. Chem. Phys.</source><year>2020</year>, <volume>153</volume>, <fpage>134110</fpage><pub-id pub-id-type="doi">10.1063/5.0018516</pub-id>.<pub-id pub-id-type="pmid">33032406</pub-id></mixed-citation>
    </ref>
    <ref id="ref26">
      <mixed-citation publication-type="journal" id="cit26"><name><surname>Salomon-Ferrer</surname><given-names>R.</given-names></name>; <name><surname>Götz</surname><given-names>A. W.</given-names></name>; <name><surname>Poole</surname><given-names>D.</given-names></name>; <name><surname>Le Grand</surname><given-names>S.</given-names></name>; <name><surname>Walker</surname><given-names>R. C.</given-names></name><article-title>Routine
Microsecond Molecular Dynamics Simulations with AMBER on GPUs. 2.
Explicit Solvent Particle Mesh Ewald</article-title>. <source>J. Chem.
Theory Comput.</source><year>2013</year>, <volume>9</volume>, <fpage>3878</fpage>–<lpage>3888</lpage>. <pub-id pub-id-type="doi">10.1021/ct400314y</pub-id>.<pub-id pub-id-type="pmid">26592383</pub-id></mixed-citation>
    </ref>
    <ref id="ref27">
      <mixed-citation publication-type="journal" id="cit27"><name><surname>Eastman</surname><given-names>P.</given-names></name>; <name><surname>Swails</surname><given-names>J.</given-names></name>; <name><surname>Chodera</surname><given-names>J. D.</given-names></name>; <name><surname>McGibbon</surname><given-names>R. T.</given-names></name>; <name><surname>Zhao</surname><given-names>Y.</given-names></name>; <name><surname>Beauchamp</surname><given-names>K. A.</given-names></name>; <name><surname>Wang</surname><given-names>L.-P.</given-names></name>; <name><surname>Simmonett</surname><given-names>A. C.</given-names></name>; <name><surname>Harrigan</surname><given-names>M. P.</given-names></name>; <name><surname>Stern</surname><given-names>C. D.</given-names></name>; <name><surname>Wiewiora</surname><given-names>R. P.</given-names></name>; <name><surname>Brooks</surname><given-names>B. R.</given-names></name>; <name><surname>Pande</surname><given-names>V. S.</given-names></name><article-title>OpenMM 7: Rapid
development of high performance algorithms
for molecular dynamics</article-title>. <source>PLoS Comput. Biol.</source><year>2017</year>, <volume>13</volume>, <fpage>e1005659</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005659</pub-id>.<pub-id pub-id-type="pmid">28746339</pub-id></mixed-citation>
    </ref>
    <ref id="ref28">
      <mixed-citation publication-type="journal" id="cit28"><name><surname>Harger</surname><given-names>M.</given-names></name>; <name><surname>Li</surname><given-names>D.</given-names></name>; <name><surname>Wang</surname><given-names>Z.</given-names></name>; <name><surname>Dalby</surname><given-names>K.</given-names></name>; <name><surname>Lagardère</surname><given-names>L.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name>; <name><surname>Ponder</surname><given-names>J.</given-names></name>; <name><surname>Ren</surname><given-names>P.</given-names></name><article-title>Tinker-OpenMM: Absolute
and relative alchemical free energies using AMOEBA on GPUs</article-title>. <source>J. Comput. Chem.</source><year>2017</year>, <volume>38</volume>, <fpage>2047</fpage>–<lpage>2055</lpage>. <pub-id pub-id-type="doi">10.1002/jcc.24853</pub-id>.<pub-id pub-id-type="pmid">28600826</pub-id></mixed-citation>
    </ref>
    <ref id="ref29">
      <mixed-citation publication-type="conf-proc" id="cit29"><person-group person-group-type="allauthors"><name><surname>Potluri</surname><given-names>S.</given-names></name>; <name><surname>Luehr</surname><given-names>N.</given-names></name>; <name><surname>Sakharnykh</surname><given-names>N.</given-names></name></person-group><article-title>Simplifying Multi-GPU Communication
with NVSHMEM</article-title>. <source>GPU Technology Conference</source>; <publisher-name>NVIDIA</publisher-name>, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="ref30">
      <mixed-citation publication-type="book" id="cit30"><person-group person-group-type="allauthors"><name><surname>Frenkel</surname><given-names>D.</given-names></name>; <name><surname>Smit</surname><given-names>B.</given-names></name></person-group><source>Understanding molecular simulation:
from algorithms to applications</source>; <publisher-name>Elsevier</publisher-name>, <year>2001</year>; Vol. <volume>1</volume>.</mixed-citation>
    </ref>
    <ref id="ref32">
      <mixed-citation publication-type="book" id="cit32"><person-group person-group-type="allauthors"><name><surname>Wienke</surname><given-names>S.</given-names></name>; <name><surname>Springer</surname><given-names>P.</given-names></name>; <name><surname>Terboven</surname><given-names>C.</given-names></name>; <name><surname>an Mey</surname><given-names>D.</given-names></name></person-group><article-title>OpenACC—First
Experiences with Real-World Applications</article-title>. In <source>Euro-Par 2012 Parallel Processing</source>. <series>Euro-Par 2012.
Lecture Notes in Computer Science</series>; <person-group person-group-type="editor"><name><surname>Kaklamanis</surname><given-names>C.</given-names></name>, <name><surname>Papatheodorou</surname><given-names>T.</given-names></name>, <name><surname>Spirakis</surname><given-names>P. G.</given-names></name></person-group>, Eds.; <publisher-name>Springer</publisher-name>: <publisher-loc>Berlin, Heidelberg</publisher-loc>, <year>2012</year>; Vol. <volume>7484</volume>, pp <fpage>859</fpage>–<lpage>870</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-32820-6_85</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref33">
      <mixed-citation publication-type="book" id="cit33"><person-group person-group-type="allauthors"><name><surname>Chandrasekaran</surname><given-names>S.</given-names></name>; <name><surname>Juckeland</surname><given-names>G.</given-names></name></person-group><source>OpenACC for
Programmers: Concepts and Strategies</source>, <edition>1</edition>st ed.; <publisher-name>Addison-Wesley Professional</publisher-name>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref34">
      <mixed-citation publication-type="book" id="cit34"><person-group person-group-type="allauthors"><name><surname>Sanders</surname><given-names>J.</given-names></name>; <name><surname>Kandrot</surname><given-names>E.</given-names></name></person-group><source>CUDA by example: an introduction
to general-purpose GPU programming</source>; <publisher-name>Addison-Wesley
Professional</publisher-name>, <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="ref35">
      <mixed-citation publication-type="thesis" id="cit35"><person-group person-group-type="allauthors"><name><surname>Volkov</surname><given-names>V.</given-names></name></person-group><article-title>Understanding
Latency Hiding on GPUs</article-title>. <source>Ph.D. thesis</source>, <publisher-name>EECS Department, University of California</publisher-name>, <publisher-loc>Berkeley</publisher-loc>, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="ref36">
      <mixed-citation publication-type="weblink" id="cit36"><person-group person-group-type="allauthors"><name><surname>Kraus</surname><given-names>J.</given-names></name></person-group><article-title>An
introduction
to CUDA-aware MPI</article-title>. <source>NVIDIA Developer Blog</source>; <year>2013</year>; <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/">https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/</uri>.</mixed-citation>
    </ref>
    <ref id="ref37">
      <mixed-citation publication-type="journal" id="cit37"><name><surname>Essmann</surname><given-names>U.</given-names></name>; <name><surname>Perera</surname><given-names>L.</given-names></name>; <name><surname>Berkowitz</surname><given-names>M. L.</given-names></name>; <name><surname>Darden</surname><given-names>T.</given-names></name>; <name><surname>Lee</surname><given-names>H.</given-names></name>; <name><surname>Pedersen</surname><given-names>L. G.</given-names></name><article-title>A smooth particle
mesh Ewald method</article-title>. <source>J. Chem. Phys.</source><year>1995</year>, <volume>103</volume>, <fpage>8577</fpage>–<lpage>8593</lpage>. <pub-id pub-id-type="doi">10.1063/1.470117</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref38">
      <mixed-citation publication-type="journal" id="cit38"><name><surname>Lagardère</surname><given-names>L.</given-names></name>; <name><surname>Lipparini</surname><given-names>F.</given-names></name>; <name><surname>Polack</surname><given-names>E.</given-names></name>; <name><surname>Stamm</surname><given-names>B.</given-names></name>; <name><surname>Cancès</surname><given-names>E.</given-names></name>; <name><surname>Schnieders</surname><given-names>M.</given-names></name>; <name><surname>Ren</surname><given-names>P.</given-names></name>; <name><surname>Maday</surname><given-names>Y.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name><article-title>Scalable
Evaluation of Polarization Energy and Associated Forces in Polarizable
Molecular Dynamics: II. Toward Massively Parallel Computations Using
Smooth Particle Mesh Ewald</article-title>. <source>J. Chem. Theory
Comput.</source><year>2015</year>, <volume>11</volume>, <fpage>2589</fpage>–<lpage>2599</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.5b00171</pub-id>.<pub-id pub-id-type="pmid">26575557</pub-id></mixed-citation>
    </ref>
    <ref id="ref39">
      <mixed-citation publication-type="book" id="cit39"><collab>NVIDIA Corporation</collab>. <source>CUDA Toolkit 11.1 CUFFT Library
Programming Guide 2020</source>; <publisher-name>NVIDIA</publisher-name>, <year>2020</year>;<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://developer.nvidia.com/nvidia-gpu-computing-documentation">http://developer.nvidia.com/nvidia-gpu-computing-documentation</uri>.</mixed-citation>
    </ref>
    <ref id="ref40">
      <mixed-citation publication-type="journal" id="cit40"><name><surname>Lipparini</surname><given-names>F.</given-names></name>; <name><surname>Lagardère</surname><given-names>L.</given-names></name>; <name><surname>Stamm</surname><given-names>B.</given-names></name>; <name><surname>Cancès</surname><given-names>E.</given-names></name>; <name><surname>Schnieders</surname><given-names>M.</given-names></name>; <name><surname>Ren</surname><given-names>P.</given-names></name>; <name><surname>Maday</surname><given-names>Y.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name><article-title>Scalable
Evaluation of Polarization Energy and Associated Forces in Polarizable
Molecular Dynamics: I. Toward Massively Parallel Direct Space Computations</article-title>. <source>J. Chem. Theory Comput.</source><year>2014</year>, <volume>10</volume>, <fpage>1638</fpage>–<lpage>1651</lpage>. <pub-id pub-id-type="doi">10.1021/ct401096t</pub-id>.<pub-id pub-id-type="pmid">26512230</pub-id></mixed-citation>
    </ref>
    <ref id="ref41">
      <mixed-citation publication-type="conf-proc" id="cit41"><person-group person-group-type="allauthors"><name><surname>Phillips</surname><given-names>J. C.</given-names></name>; <name><surname>Zheng</surname><given-names>Gengbin</given-names></name>; <name><surname>Kumar</surname><given-names>S.</given-names></name>; <name><surname>Kale</surname><given-names>L.
V.</given-names></name></person-group><article-title>NAMD: Biomolecular
Simulation on Thousands of Processors</article-title>. <source>SC
’02: Proceedings of the 2002 ACM/IEEE Conference on Supercomputing</source>; <publisher-name>ACM</publisher-name>, <year>2002</year>; pp <fpage>36</fpage>–<lpage>36</lpage>.</mixed-citation>
    </ref>
    <ref id="ref42">
      <mixed-citation publication-type="journal" id="cit42"><name><surname>Tuckerman</surname><given-names>M.</given-names></name>; <name><surname>Berne</surname><given-names>B. J.</given-names></name>; <name><surname>Martyna</surname><given-names>G. J.</given-names></name><article-title>Reversible multiple time scale molecular
dynamics</article-title>. <source>J. Chem. Phys.</source><year>1992</year>, <volume>97</volume>, <fpage>1990</fpage>–<lpage>2001</lpage>. <pub-id pub-id-type="doi">10.1063/1.463137</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref43">
      <mixed-citation publication-type="journal" id="cit43"><name><surname>Bussi</surname><given-names>G.</given-names></name>; <name><surname>Donadio</surname><given-names>D.</given-names></name>; <name><surname>Parrinello</surname><given-names>M.</given-names></name><article-title>Canonical
sampling through velocity
rescaling</article-title>. <source>J. Chem. Phys.</source><year>2007</year>, <volume>126</volume>, <fpage>014101</fpage><pub-id pub-id-type="doi">10.1063/1.2408420</pub-id>.<pub-id pub-id-type="pmid">17212484</pub-id></mixed-citation>
    </ref>
    <ref id="ref44">
      <mixed-citation publication-type="conf-proc" id="cit44"><person-group person-group-type="allauthors"><name><surname>Zhou</surname><given-names>J.</given-names></name>; <name><surname>Ross</surname><given-names>K.
A.</given-names></name></person-group><article-title>Implementing database
operations using SIMD instructions</article-title>. <source>Proceedings
of the 2002 ACM SIGMOD international conference on Management of data</source>; <publisher-name>ACM</publisher-name>, <year>2002</year>; pp <fpage>145</fpage>–<lpage>156</lpage>.</mixed-citation>
    </ref>
    <ref id="ref45">
      <mixed-citation publication-type="journal" id="cit45"><name><surname>Nickolls</surname><given-names>J.</given-names></name>; <name><surname>Dally</surname><given-names>W. J.</given-names></name><article-title>The GPU computing era</article-title>. <source>IEEE micro</source><year>2010</year>, <volume>30</volume>, <fpage>56</fpage>–<lpage>69</lpage>. <pub-id pub-id-type="doi">10.1109/MM.2010.41</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref46">
      <mixed-citation publication-type="journal" id="cit46"><name><surname>Le
Grand</surname><given-names>S.</given-names></name>; <name><surname>Götz</surname><given-names>A. W.</given-names></name>; <name><surname>Walker</surname><given-names>R. C.</given-names></name><article-title>SPFP: Speed without
compromise—A mixed precision model for GPU accelerated molecular
dynamics simulations</article-title>. <source>Comput. Phys. Commun.</source><year>2013</year>, <volume>184</volume>, <fpage>374</fpage>–<lpage>380</lpage>. <pub-id pub-id-type="doi">10.1016/j.cpc.2012.09.022</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref47">
      <mixed-citation publication-type="journal" id="cit47"><name><surname>Yates</surname><given-names>R.</given-names></name><article-title>Fixed-point
arithmetic: An introduction</article-title>. <source>Digital Signal
Labs</source><year>2009</year>, <volume>81</volume>, <fpage>198</fpage>.</mixed-citation>
    </ref>
    <ref id="ref48">
      <mixed-citation publication-type="journal" id="cit48"><name><surname>Bowers</surname><given-names>K. J.</given-names></name>; <name><surname>Dror</surname><given-names>R. O.</given-names></name>; <name><surname>Shaw</surname><given-names>D. E.</given-names></name><article-title>The midpoint
method for parallelization
of particle simulations</article-title>. <source>J. Chem. Phys.</source><year>2006</year>, <volume>124</volume>, <fpage>184109</fpage><pub-id pub-id-type="doi">10.1063/1.2191489</pub-id>.<pub-id pub-id-type="pmid">16709099</pub-id></mixed-citation>
    </ref>
    <ref id="ref49">
      <mixed-citation publication-type="journal" id="cit49"><name><surname>Lagardère</surname><given-names>L.</given-names></name>; <name><surname>Aviat</surname><given-names>F.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name><article-title>Pushing
the Limits of Multiple-Time-Step
Strategies for Polarizable Point Dipole Molecular Dynamics</article-title>. <source>J. Phys. Chem. Lett.</source><year>2019</year>, <volume>10</volume>, <fpage>2593</fpage>–<lpage>2599</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jpclett.9b00901</pub-id>.<pub-id pub-id-type="pmid">31050904</pub-id></mixed-citation>
    </ref>
    <ref id="ref50">
      <mixed-citation publication-type="journal" id="cit50"><name><surname>Célerse</surname><given-names>F.</given-names></name>; <name><surname>Lagardère</surname><given-names>L.</given-names></name>; <name><surname>Derat</surname><given-names>E.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name><article-title>Massively
parallel
implementation of Steered Molecular Dynamics in Tinker-HP:comparisons
of polarizable and non-polarizable simulations of realistic systems</article-title>. <source>J. Chem. Theory Comput.</source><year>2019</year>, <volume>15</volume>, <fpage>3694</fpage>–<lpage>3709</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.9b00199</pub-id>.<pub-id pub-id-type="pmid">31059250</pub-id></mixed-citation>
    </ref>
    <ref id="ref51">
      <mixed-citation publication-type="journal" id="cit51"><name><surname>Miao</surname><given-names>Y.</given-names></name>; <name><surname>Feher</surname><given-names>V. A.</given-names></name>; <name><surname>McCammon</surname><given-names>J. A.</given-names></name><article-title>Gaussian accelerated
molecular dynamics:
Unconstrained enhanced sampling and free energy calculation</article-title>. <source>J. Chem. Theory Comput.</source><year>2015</year>, <volume>11</volume>, <fpage>3584</fpage>–<lpage>3595</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.5b00436</pub-id>.<pub-id pub-id-type="pmid">26300708</pub-id></mixed-citation>
    </ref>
    <ref id="ref52">
      <mixed-citation publication-type="journal" id="cit52"><name><surname>Jorgensen</surname><given-names>W. L.</given-names></name>; <name><surname>Maxwell</surname><given-names>D. S.</given-names></name>; <name><surname>Tirado-Rives</surname><given-names>J.</given-names></name><article-title>Development
and Testing of the OPLS
All-Atom Force Field on Conformational Energetics and Properties of
Organic Liquids</article-title>. <source>J. Am. Chem. Soc.</source><year>1996</year>, <volume>117</volume>, <fpage>11225</fpage>–<lpage>11236</lpage>. <pub-id pub-id-type="doi">10.1021/ja9621760</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref53">
      <mixed-citation publication-type="journal" id="cit53"><name><surname>Jaffrelot-Inizan</surname><given-names>T.</given-names></name>; <name><surname>Célerse</surname><given-names>F.</given-names></name>; <name><surname>Adjoua</surname><given-names>O.</given-names></name>; <name><surname>El Ahdab</surname><given-names>D.</given-names></name>; <name><surname>Jolly</surname><given-names>L.-H.</given-names></name>; <name><surname>Liu</surname><given-names>C.</given-names></name>; <name><surname>Ren</surname><given-names>P.</given-names></name>; <name><surname>Montes</surname><given-names>M.</given-names></name>; <name><surname>Lagarde</surname><given-names>N.</given-names></name>; <name><surname>Lagardère</surname><given-names>L.</given-names></name><article-title>High-Resolution
Mining of SARS-CoV-2 Main Protease Conformational Space: Supercomputer-Driven
Unsupervised Adaptive Sampling</article-title>. <source>Chem. Sci.</source><year>2021</year>, <pub-id pub-id-type="doi">10.1039/D1SC00145K</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref54">
      <mixed-citation publication-type="conf-proc" id="cit54"><person-group person-group-type="allauthors"><name><surname>Chapman</surname><given-names>B.</given-names></name>; <name><surname>Curtis</surname><given-names>T.</given-names></name>; <name><surname>Pophale</surname><given-names>S.</given-names></name>; <name><surname>Poole</surname><given-names>S.</given-names></name>; <name><surname>Kuehn</surname><given-names>J.</given-names></name>; <name><surname>Koelbel</surname><given-names>C.</given-names></name>; <name><surname>Smith</surname><given-names>L.</given-names></name></person-group><article-title>Introducing OpenSHMEM: SHMEM for the PGAS community</article-title>. <source>Proceedings of the Fourth Conference on Partitioned Global
Address Space Programming Model</source>; <publisher-name>ACM</publisher-name>, <year>2010</year>; pp <fpage>1</fpage>–<lpage>3</lpage>.</mixed-citation>
    </ref>
    <ref id="ref55">
      <mixed-citation publication-type="journal" id="cit55"><name><surname>Liu</surname><given-names>C.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name>; <name><surname>Ren</surname><given-names>P.</given-names></name><article-title>AMOEBA+ Classical Potential for Modeling
Molecular Interactions</article-title>. <source>J. Chem. Theory Comput.</source><year>2019</year>, <volume>15</volume>, <fpage>4122</fpage>–<lpage>4139</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.9b00261</pub-id>.<pub-id pub-id-type="pmid">31136175</pub-id></mixed-citation>
    </ref>
    <ref id="ref56">
      <mixed-citation publication-type="journal" id="cit56"><name><surname>Liu</surname><given-names>C.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name>; <name><surname>Ren</surname><given-names>P.</given-names></name><article-title>Implementation
of Geometry-Dependent
Charge Flux into the Polarizable AMOEBA+ Potential</article-title>. <source>J. Phys. Chem. Lett.</source><year>2020</year>, <volume>11</volume>, <fpage>419</fpage>–<lpage>426</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jpclett.9b03489</pub-id>.<pub-id pub-id-type="pmid">31865706</pub-id></mixed-citation>
    </ref>
    <ref id="ref57">
      <mixed-citation publication-type="journal" id="cit57"><name><surname>Gresh</surname><given-names>N.</given-names></name>; <name><surname>Cisneros</surname><given-names>G. A.</given-names></name>; <name><surname>Darden</surname><given-names>T. A.</given-names></name>; <name><surname>Piquemal</surname><given-names>J.-P.</given-names></name><article-title>Anisotropic,
polarizable
molecular mechanics studies of inter-, intra-molecular interactions,
and ligand-macromolecule complexes. A bottom-up strategy</article-title>. <source>J. Chem. Theory Comput.</source><year>2007</year>, <volume>3</volume>, <fpage>1960</fpage>–<lpage>1986</lpage>. <pub-id pub-id-type="doi">10.1021/ct700134r</pub-id>.<pub-id pub-id-type="pmid">18978934</pub-id></mixed-citation>
    </ref>
    <ref id="ref58">
      <mixed-citation publication-type="journal" id="cit58"><name><surname>Bonomi</surname><given-names>M.</given-names></name>; <name><surname>Bussi</surname><given-names>G.</given-names></name>; <name><surname>Camilloni</surname><given-names>C.</given-names></name>; <name><surname>Tribello</surname><given-names>G. A.</given-names></name>; <name><surname>Banáš</surname><given-names>P.</given-names></name>; <name><surname>Barducci</surname><given-names>A.</given-names></name>; <name><surname>Bernetti</surname><given-names>M.</given-names></name>; <name><surname>Bolhuis</surname><given-names>P. G.</given-names></name>; <name><surname>Bottaro</surname><given-names>S.</given-names></name>; <name><surname>Branduardi</surname><given-names>D.</given-names></name>; et al. <article-title>Promoting transparency and reproducibility
in enhanced molecular simulations</article-title>. <source>Nat. Methods</source><year>2019</year>, <volume>16</volume>, <fpage>670</fpage>–<lpage>673</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0506-8</pub-id>.<pub-id pub-id-type="pmid">31363226</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
