<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">NPJ Digit Med</journal-id>
    <journal-id journal-id-type="iso-abbrev">NPJ Digit Med</journal-id>
    <journal-title-group>
      <journal-title>NPJ Digital Medicine</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2398-6352</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7893172</article-id>
    <article-id pub-id-type="publisher-id">399</article-id>
    <article-id pub-id-type="doi">10.1038/s41746-021-00399-3</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CovidCTNet: an open-source deep learning approach to diagnose covid-19 using small cohort of CT images</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Javaheri</surname>
          <given-names>Tahereh</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Homayounfar</surname>
          <given-names>Morteza</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Amoozgar</surname>
          <given-names>Zohreh</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1323-8942</contrib-id>
        <name>
          <surname>Reiazi</surname>
          <given-names>Reza</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Homayounieh</surname>
          <given-names>Fatemeh</given-names>
        </name>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Abbas</surname>
          <given-names>Engy</given-names>
        </name>
        <xref ref-type="aff" rid="Aff8">8</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Laali</surname>
          <given-names>Azadeh</given-names>
        </name>
        <xref ref-type="aff" rid="Aff9">9</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7462-118X</contrib-id>
        <name>
          <surname>Radmard</surname>
          <given-names>Amir Reza</given-names>
        </name>
        <xref ref-type="aff" rid="Aff10">10</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gharib</surname>
          <given-names>Mohammad Hadi</given-names>
        </name>
        <xref ref-type="aff" rid="Aff11">11</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mousavi</surname>
          <given-names>Seyed Ali Javad</given-names>
        </name>
        <xref ref-type="aff" rid="Aff12">12</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ghaemi</surname>
          <given-names>Omid</given-names>
        </name>
        <xref ref-type="aff" rid="Aff10">10</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Babaei</surname>
          <given-names>Rosa</given-names>
        </name>
        <xref ref-type="aff" rid="Aff13">13</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mobin</surname>
          <given-names>Hadi Karimi</given-names>
        </name>
        <xref ref-type="aff" rid="Aff13">13</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hosseinzadeh</surname>
          <given-names>Mehdi</given-names>
        </name>
        <xref ref-type="aff" rid="Aff14">14</xref>
        <xref ref-type="aff" rid="Aff15">15</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jahanban-Esfahlan</surname>
          <given-names>Rana</given-names>
        </name>
        <xref ref-type="aff" rid="Aff16">16</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Seidi</surname>
          <given-names>Khaled</given-names>
        </name>
        <xref ref-type="aff" rid="Aff16">16</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9938-7476</contrib-id>
        <name>
          <surname>Kalra</surname>
          <given-names>Mannudeep K.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Guanglan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff17">17</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chitkushev</surname>
          <given-names>L. T.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff17">17</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Haibe-Kains</surname>
          <given-names>Benjamin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff18">18</xref>
        <xref ref-type="aff" rid="Aff19">19</xref>
        <xref ref-type="aff" rid="Aff20">20</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Malekzadeh</surname>
          <given-names>Reza</given-names>
        </name>
        <xref ref-type="aff" rid="Aff21">21</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2607-1777</contrib-id>
        <name>
          <surname>Rawassizadeh</surname>
          <given-names>Reza</given-names>
        </name>
        <address>
          <email>rezar@bu.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff17">17</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.189504.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 7558</institution-id><institution>Health Informatics Lab, Metropolitan College, </institution><institution>Boston University, </institution></institution-wrap>Boston, USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.411368.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 0611 6995</institution-id><institution>Department of Biomedical Engineering, </institution><institution>Amirkabir University of Technology, </institution></institution-wrap>Tehran, Iran </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Department of Radiation Oncology, Massachusetts General Hospital, </institution><institution>Harvard Medical School, </institution></institution-wrap>Boston, USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.17063.33</institution-id><institution-id institution-id-type="ISNI">0000 0001 2157 2938</institution-id><institution>Princess Margaret Cancer Centre, </institution><institution>University of Toronto, </institution></institution-wrap>Toronto, Canada </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.17063.33</institution-id><institution-id institution-id-type="ISNI">0000 0001 2157 2938</institution-id><institution>Department of Medical Biophysics, </institution><institution>University of Toronto, </institution></institution-wrap>Toronto, Canada </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.411746.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 4911 7066</institution-id><institution>Department of Medical Physics, School of Medicine, </institution><institution>Iran university of Medical Sciences, </institution></institution-wrap>Tehran, Iran </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Department of Radiology, Massachusetts General Hospital, </institution><institution>Harvard Medical School, </institution></institution-wrap>Boston, USA </aff>
      <aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="GRID">grid.17063.33</institution-id><institution-id institution-id-type="ISNI">0000 0001 2157 2938</institution-id><institution>Joint Department of Medical Imaging, </institution><institution>University of Toronto, </institution></institution-wrap>Toronto, Canada </aff>
      <aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="GRID">grid.411746.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 4911 7066</institution-id><institution>Department of Infectious Diseases, Firoozgar Hospital, </institution><institution>Iran University of Medical Sciences, </institution></institution-wrap>Tehran, Iran </aff>
      <aff id="Aff10"><label>10</label><institution-wrap><institution-id institution-id-type="GRID">grid.411705.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 0166 0922</institution-id><institution>Department of Radiology, Shariati Hospital, </institution><institution>Tehran University of Medical Sciences, </institution></institution-wrap>Tehran, Iran </aff>
      <aff id="Aff11"><label>11</label><institution-wrap><institution-id institution-id-type="GRID">grid.411747.0</institution-id><institution-id institution-id-type="ISNI">0000 0004 0418 0096</institution-id><institution>Department of Radiology and Golestan Rheumatology Research Center, </institution><institution>Golestan University of Medical Sciences, </institution></institution-wrap>Gorgan, Iran </aff>
      <aff id="Aff12"><label>12</label><institution-wrap><institution-id institution-id-type="GRID">grid.411746.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 4911 7066</institution-id><institution>Department of Internal Medicine, </institution><institution>Iran University of Medical Sciences, </institution></institution-wrap>Tehran, Iran </aff>
      <aff id="Aff13"><label>13</label><institution-wrap><institution-id institution-id-type="GRID">grid.411746.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 4911 7066</institution-id><institution>Department of Radiology, </institution><institution>Iran University of Medical Sciences, </institution></institution-wrap>Tehran, Iran </aff>
      <aff id="Aff14"><label>14</label><institution-wrap><institution-id institution-id-type="GRID">grid.444918.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 1794 7022</institution-id><institution>Institute of Research and Development, </institution><institution>Duy Tan University, </institution></institution-wrap>Da Nang, Vietnam </aff>
      <aff id="Aff15"><label>15</label><institution-wrap><institution-id institution-id-type="GRID">grid.411746.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 4911 7066</institution-id><institution>Health Management and Economics Research Center, </institution><institution>Iran University of Medical Sciences, </institution></institution-wrap>Tehran, Iran </aff>
      <aff id="Aff16"><label>16</label><institution-wrap><institution-id institution-id-type="GRID">grid.412888.f</institution-id><institution-id institution-id-type="ISNI">0000 0001 2174 8913</institution-id><institution>Department of Medical Biotechnology, School of Advanced Medical Sciences, </institution><institution>Tabriz University of Medical Sciences, </institution></institution-wrap>Tabriz, Iran </aff>
      <aff id="Aff17"><label>17</label><institution-wrap><institution-id institution-id-type="GRID">grid.189504.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 7558</institution-id><institution>Department of Computer Science, Metropolitan College, </institution><institution>Boston University, </institution></institution-wrap>Boston, USA </aff>
      <aff id="Aff18"><label>18</label><institution-wrap><institution-id institution-id-type="GRID">grid.17063.33</institution-id><institution-id institution-id-type="ISNI">0000 0001 2157 2938</institution-id><institution>Department of Computer Science, </institution><institution>University of Toronto, </institution></institution-wrap>Toronto, ON Canada </aff>
      <aff id="Aff19"><label>19</label><institution-wrap><institution-id institution-id-type="GRID">grid.419890.d</institution-id><institution-id institution-id-type="ISNI">0000 0004 0626 690X</institution-id><institution>Ontario Institute for Cancer Research, </institution></institution-wrap>Toronto, ON Canada </aff>
      <aff id="Aff20"><label>20</label><institution-wrap><institution-id institution-id-type="GRID">grid.494618.6</institution-id><institution>Vector Institute for Artificial Intelligence, </institution></institution-wrap>Toronto, ON Canada </aff>
      <aff id="Aff21"><label>21</label><institution-wrap><institution-id institution-id-type="GRID">grid.411705.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 0166 0922</institution-id><institution>Digestive Disease Research Center, </institution><institution>Tehran University of Medical Sciences, </institution></institution-wrap>Tehran, Iran </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>18</day>
      <month>2</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>18</day>
      <month>2</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>4</volume>
    <elocation-id>29</elocation-id>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>5</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>10</day>
        <month>12</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Coronavirus disease 2019 (Covid-19) is highly contagious with limited treatment options. Early and accurate diagnosis of Covid-19 is crucial in reducing the spread of the disease and its accompanied mortality. Currently, detection by reverse transcriptase-polymerase chain reaction (RT-PCR) is the gold standard of outpatient and inpatient detection of Covid-19. RT-PCR is a rapid method; however, its accuracy in detection is only ~70–75%. Another approved strategy is computed tomography (CT) imaging. CT imaging has a much higher sensitivity of ~80–98%, but similar accuracy of 70%. To enhance the accuracy of CT imaging detection, we developed an open-source framework, CovidCTNet, composed of a set of deep learning algorithms that accurately differentiates Covid-19 from community-acquired pneumonia (CAP) and other lung diseases. CovidCTNet increases the accuracy of CT imaging detection to 95% compared to radiologists (70%). CovidCTNet is designed to work with heterogeneous and small sample sizes independent of the CT imaging hardware. To facilitate the detection of Covid-19 globally and assist radiologists and physicians in the screening process, we are releasing all algorithms and model parameter details as open-source. Open-source sharing of CovidCTNet enables developers to rapidly improve and optimize services while preserving user privacy and data ownership.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Biological techniques</kwd>
      <kwd>Image processing</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par2">In the era of communication, the current epidemic of highly contagious Covid-19 (SARS-Cov-2) has negatively impacted the global health, trade, and economy. To date, the mortality rate of Covid-19 is estimated to be 35–45 times higher than the pandemic influenza, accounting for more than 1,000,000 deaths<sup><xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR4">4</xref></sup>. Covid-19 has surpassed its predecessors SARS-CoV, and MERS-CoV, in morbidity and mortality<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. Unfortunately, the long-term studies on SARS-CoV, the cause of SARS<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, did not find effective and safe treatments<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Lack of effective therapy underlines the importance of early diagnosis, rapid isolation, and strict infection control to minimize the spread of Covid-19.</p>
    <p id="Par3">Currently, diagnosis is mainly based on the patient’s medical history, RT-PCR, and CT imaging<sup><xref ref-type="bibr" rid="CR8">8</xref>–<xref ref-type="bibr" rid="CR12">12</xref></sup>. High error (30–35%) of RT-PCR<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>, lack of distinction between viral contamination versus disease-bearing individuals<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> or false-positive/negative<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> may have contributed to the high prevalence of Covid-19 and the dismal therapeutic outcomes. Here, CT imaging plays a critical role in Covid-19 diagnosis since it not only detects the presence of disease in the lung but also enables identifying the stage of the disease by scoring the CT images<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. CT imaging, however, has its own limitations that need to be addressed. The lack of specificity and the similarities between the lung lesions generated by other types of viral infection or community-acquired pneumonia (CAP) may contribute to misdiagnosis for Covid-19<sup><xref ref-type="bibr" rid="CR18">18</xref>–<xref ref-type="bibr" rid="CR20">20</xref></sup>. We hypothesized that using robust tools such as machine learning can resolve the CT imaging technical bias and corrects for human errors<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR21">21</xref>–<xref ref-type="bibr" rid="CR26">26</xref></sup>.</p>
    <p id="Par4">An appropriate machine learning framework for Covid-19 detection should (i) be able to assist radiologists and their staff to rapidly and accurately detect Covid-19, (ii) be compatible with a wide range of image scanning hardware’s, and (iii) be user friendly to the medical community without computer-science expertize. In our effort to address the clinical diagnostic needs in the Covid-19 pandemic crisis under institutional review board (IRB) approval (IR.TUMS.VCR.REC.1399.007), we designed CovidCTNet framework. CovidCTNet is composed of a pipeline of deep learning algorithms trained on identifying Covid-19 lesions in lung CT images to improve the process of Covid-19 detection.</p>
    <p id="Par5">While deep learning approaches used for Covid-19 detection require large datasets<sup><xref ref-type="bibr" rid="CR27">27</xref>–<xref ref-type="bibr" rid="CR29">29</xref></sup>, CovidCTNet by employing BCDU-Net<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> requires only a small sample size for training to achieve accurate detection of Covid-19 without potential bias. For these reasons, our model is significantly different from other models<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup> which requires a large dataset of CT images. In our framework, we first applied multiple pre-processing steps on CT images using BCDU-Net<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> which is designed based on the U-Net<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup>, a well-known convolutional network for biomedical image analysis. BCDU-Net is an optimum network due to memory (LSTM cells), allowing the model to remember the structure of the healthy lung. In particular, the CovidCTNet used BCDU-Net to (i) clean images, i.e., removing the image segments unrelated to infection, such as heart, skin, or the bed of CT image device and (ii) train a noise cancellation model, which was used by our model to extract infection. Note that both Covid-19 and CAP are associated with a lung infection, and visually they are very similar. Therefore, a robust Covid-19 identification approach should distinguish them accurately. Otherwise, classification algorithms cannot distinguish Covid-19, CAP, and control lungs in the small dataset and from original CT images. However, with the assistance of BCDU-Net, our model cleaned the CT images from other tissues, except the lung infection.</p>
    <p id="Par6">After the process of infection extraction, the result of CT images was fed into a convolutional neural network (CNN) to classify the given CT images as control, CAP, or Covid-19. All of our codes, including details of model parameters, are clearly explained and released as open-source. In this study, we developed the CovidCTNet, which consists of a pipeline of deep learning algorithms to accurately detect Covid-19 infection. A heterogeneous dataset was analyzed in this study to ensure that CovidCTNet can address the needs of hospitals across the globe, irrespective of the sample size, an imaging device (hardware), or the imaging software.</p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>Extraction of Covid-19 lesions from CT images</title>
      <p id="Par7">We assessed a dataset consisting of 16,750 slices of all CT scan images from 335 patients. Among this dataset, 111 (5550 CT slices) patients were infected with Covid-19 with a confirmed RT-PCR, patient’s medical history, and radiologist diagnosis. The second cohort was 115 (5750 CT slices) patients infected with CAP or other viral sources with CT images that can be potentially misdiagnosed for Covid-19. Our Control group consists of a cohort of patients 109 (5450 CT slices) with healthy lungs or other non-Covid-19/non-CAP diseases. Additionally, a cohort of 70 CT scans was used from SPIE-AAPM-NCI lung nodule classification challenge dataset<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, a heterogeneous dataset that contains lung cancer as well (summarized in Tables <xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab2" ref-type="table">2</xref>). 66 cases (21,888 CT slices) out of 70 were randomly selected for training and validation phases. Four cases served as a control for reader tests. CT images were acquired from multiple institutions, including five medical centers in Iran, a country that is highly affiliated with Covid-19 and from publicly available dataset from lung nodule classification (LUNGx) challenge, an archive generated by the University of Chicago<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR36">36</xref></sup>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Detail information of the samples in multiple steps of analysis (preprocessing, train, validation, and test phases).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">#Patient cohort</th><th colspan="2">Control</th><th colspan="2">Total</th><th rowspan="2">Loss (binary cross-entropy) Optimizer = Adam learning rate = 0.001</th></tr><tr><th>CT slices with Perlin noise</th><th>CT slices without Perlin noise</th><th>Patients</th><th>CT slices</th></tr></thead><tbody><tr><td>Preprocessing-train</td><td>9913</td><td>9914</td><td>60</td><td>19,827</td><td>0.3585</td></tr><tr><td>Preprocessing-validation</td><td>1031</td><td>1030</td><td>6</td><td>2061</td><td>0.3638</td></tr></tbody></table><table-wrap-foot><p>Details of individual samples and total cases that were used in preprocessing.</p></table-wrap-foot></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Detail information of the samples in multiple steps of analysis (preprocessing, train, validation, and test phases).</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">#Patient cohort</th><th colspan="2">Control</th><th colspan="2">CAP</th><th colspan="2">Covid-19</th><th colspan="2">Total</th></tr><tr><th>Patients</th><th>CT slices</th><th>Patients</th><th>CT slices</th><th>Patients</th><th>CT slices</th><th>Patients</th><th>CT slices</th></tr></thead><tbody><tr><td>Train</td><td>100</td><td>100 × 50</td><td>100</td><td>100 × 50</td><td>100</td><td>100 × 50</td><td>300</td><td>15,000</td></tr><tr><td>Validation</td><td>5</td><td>5 × 50</td><td>5</td><td>5 × 50</td><td>5</td><td>5 × 50</td><td>15</td><td>750</td></tr><tr><td>Reader test</td><td>4</td><td>4 × 50</td><td>10</td><td>10 × 50</td><td>6</td><td>6 × 50</td><td>20</td><td>1000</td></tr><tr><td>Total</td><td>109</td><td>109 × 50</td><td>115</td><td>115 × 50</td><td>111</td><td>111 × 50</td><td>335</td><td>16,750</td></tr></tbody></table><table-wrap-foot><p>A summary of individual samples and total cases that were used in the train, validation, and test phases. To maintain the balance in the dataset, a total number of 100 cases were used for each of Control, CAP, and Covid-19 groups.</p></table-wrap-foot></table-wrap></p>
      <p id="Par8">The dataset was collected from 12 different CT scanner models of five different brands. Our sample size was small and to achieve a high performing model that is operational and unbiased, we used BCDU-Net as the backbone of our model. To identify Covid-19 in the lung as well as CAP lesion, we generated pseudo-infection anomalies in the CT control images using Perlin noise<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>.</p>
    </sec>
    <sec id="Sec4">
      <title>CovidCTNet mitigates the challenge of small dataset by highlighting the infection</title>
      <p id="Par9">To test whether applying Perlin noise and using BCDU-Net is necessary for preprocessing and if they increase the accuracy of our model, we conducted a validation experiment. The 3D CNN model was performed with and without the use of BCDU-Net and Perlin noise. The implementation of BCDU-Net significantly boosted the accuracy of the model and demonstrated the importance of using Perlin noise and preprocessing steps. Figure <xref rid="Fig1" ref-type="fig">1</xref> presents binary cross entropy (loss) and accuracy of the CovidCTNet in different conditions. While the accuracy of the model without using BCDU-Net and Perlin noise at the training phase is very high, it drops significantly in the validation phase. This confirms that the features and parameters that were selected by the CNN model (without BCDU-Net) were not sufficient. In addition, applying them significantly changes the accuracy of training and validation, which demonstrates the need for preprocessing in increasing the model robustness. Note that the results shown in Fig. <xref rid="Fig1" ref-type="fig">1a, b</xref> were generated by training the model with only 50 cases for each class, which proves the necessity and usefulness of applying the BCDU-Net for a limited amount of data. Figure <xref rid="Fig2" ref-type="fig">2</xref> presents the extracted infection by BCDU-Net in 2D and Fig. <xref rid="Fig3" ref-type="fig">3</xref> presents the extracted infection by BCDU-Net in 3D. The output of BCDU-Net (Fig. <xref rid="Fig3" ref-type="fig">3</xref>) will be fed to the CNN model as an input. It can be seen from Figs. <xref rid="Fig2" ref-type="fig">2</xref> and <xref rid="Fig3" ref-type="fig">3</xref> how BCDU-Net reduces the noninfectious parts of the CT image and highlights the infections inside the lung.<fig id="Fig1"><label>Fig. 1</label><caption><title>BCDU-Net increases the robustness of the CNN model.</title><p><bold>a</bold> To show the effect of BCDU-Net on the preprocessing, the procedure was done with and without applying BCDU-Net/Perlin noise. The outcome of the model is presented with respect to loss and accuracy. <bold>b</bold> The confusion matrix and other classification related metrics in detail. The results shown in this figure are based on just 50 randomly selected cases for each class of Covid versus non-Covid.</p></caption><graphic xlink:href="41746_2021_399_Fig1_HTML" id="d32e1072"/></fig><fig id="Fig2"><label>Fig. 2</label><caption><title>Covid-19 and CAP infection extraction by BCDU-Net.</title><p>The filtered images (left) will be used for classification by CNN. An unprocessed 3D image of the whole lung infected with Covid-19 is shown in Fig. <xref rid="Fig3" ref-type="fig">3a</xref>. The same image was processed with BCDU-Net to remove non lung-related parts and to extract and highlight the Covid-19 infection (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>).</p></caption><graphic xlink:href="41746_2021_399_Fig2_HTML" id="d32e1089"/></fig><fig id="Fig3"><label>Fig. 3</label><caption><title>Schematic representation of BCDU-Net module to detect the infection in CT images.</title><p><bold>a</bold> The original CT images visualized in point cloud. <bold>b</bold> Reconstructed lung image acquired by feeding the CT slices (Fig. <xref rid="Fig8" ref-type="fig">8</xref> middle part h) into BCDU-Net. The Covid-19 infection area is highlighted in <bold>b</bold>.</p></caption><graphic xlink:href="41746_2021_399_Fig3_HTML" id="d32e1111"/></fig></p>
    </sec>
    <sec id="Sec5">
      <title>CovidCTNet accurately detects Covid-19 from other lung diseases</title>
      <p id="Par10">The output of the algorithm (a tensor such as the right side of Fig. <xref rid="Fig3" ref-type="fig">3b</xref>) was fed into the CNN classification algorithm. In CNN assessment, the dataset was split in 95% to train the algorithm, and 5% to validate the model in the hold-out. The area under receiver operating characteristics (ROC) curve (AUC) for Covid-19 at the validation phase was 94%, with an accuracy of 93.33% when CNN classified Covid-19 versus non-Covid-19 (two classes) (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). CNN achieved the accuracy of 86.66% when it classified Covid-19 versus CAP and Control (three classes). The detection sensitivity of 90.91% and specificity of 100% were recorded for Covid-19 (Fig. <xref rid="Fig4" ref-type="fig">4</xref>).<fig id="Fig4"><label>Fig. 4</label><caption><title>Performance of CovidCTNet in detecting Control, Covid-19, and CAP.</title><p>The model’s AUC for Covid-19 detection is 0.94 (<italic>n</italic> = 15 cases). The accuracy, sensitivity, and specificity of the model are shown. The model operation in three classes demonstrates the detection of all three classes including Covid-19 versus CAP and versus Control and in two classes indicates the detection of Covid-19 as one class versus non-Covid-19 (CAP and Control) as second class.</p></caption><graphic xlink:href="41746_2021_399_Fig4_HTML" id="d32e1139"/></fig></p>
    </sec>
    <sec id="Sec6">
      <title>CovidCTNet outperforms radiologists</title>
      <p id="Par11">To test the classification quality of our framework, an independent dataset consisting of 20 cases mixed of Control, Covid-19, and CAP were assessed using our framework and in parallel four certified and independent radiologists who were not involved in the process of data collection. The average reader performance of four radiologists showed a sensitivity of 79% for Covid-19 and specificity of 82.14%. The CNN classification of CovidCTNet, however outperformed the radiologists and achieved Covid-19 detection with sensitivity and specificity of 93 and 100%, respectively. Table <xref rid="Tab3" ref-type="table">3</xref>. details the comparison of radiologist performance versus CovidCTNet.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Comparison of the accuracy of CovidCTNet versus radiologists.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Precision</th><th>Recall</th><th><italic>F</italic>1-score</th></tr></thead><tbody><tr><td><italic>Radiologist 1</italic></td><td/><td/><td/></tr><tr><td rowspan="1"> Control</td><td>1</td><td>0.5</td><td>0.533</td></tr><tr><td rowspan="1"> CAP</td><td>0.8</td><td>0.4</td><td>0.533</td></tr><tr><td rowspan="1"> Covid-19</td><td>0.385</td><td>0.833</td><td>0.526</td></tr><tr><td> Accuracy</td><td colspan="3">0.55</td></tr><tr><td><italic>Radiologist 2</italic></td><td/><td/><td/></tr><tr><td> Control</td><td>0.5</td><td>0.5</td><td>0.5</td></tr><tr><td> CAP</td><td>0.889</td><td>0.8</td><td>0.842</td></tr><tr><td> Covid-19</td><td>0.714</td><td>0.833</td><td>0.769</td></tr><tr><td> Accuracy</td><td colspan="3">0.75</td></tr><tr><td><italic>Radiologist 3</italic></td><td/><td/><td/></tr><tr><td> Control</td><td>1</td><td>1</td><td>1</td></tr><tr><td> CAP</td><td>1</td><td>1</td><td>1</td></tr><tr><td> Covid-19</td><td>1</td><td>1</td><td>1</td></tr><tr><td> Accuracy</td><td colspan="3">1</td></tr><tr><td><italic>Radiologist 4</italic></td><td/><td/><td/></tr><tr><td> Control</td><td>0.25</td><td>0.5</td><td>0.33</td></tr><tr><td> CAP</td><td>0.66</td><td>0.6</td><td>0.63</td></tr><tr><td> Covid-19</td><td>1</td><td>0.5</td><td>0.66</td></tr><tr><td> Accuracy</td><td colspan="3">0.55</td></tr><tr><td><italic>All radiologist average</italic></td><td/><td/><td/></tr><tr><td> Control</td><td>0.68</td><td>0.625</td><td>0.624</td></tr><tr><td> CAP</td><td>0.837</td><td>0.7</td><td>0.751</td></tr><tr><td> Covid-19</td><td>0.774</td><td>0.791</td><td>0.738</td></tr><tr><td> Accuracy</td><td colspan="3">0.71</td></tr><tr><td><italic>CovidCTNet</italic></td><td/><td/><td/></tr><tr><td> Control</td><td>0.6</td><td>0.75</td><td>0.67</td></tr><tr><td> CAP</td><td>0.9</td><td>0.9</td><td>0.9</td></tr><tr><td> Covid-19</td><td>1</td><td>0.83</td><td>0.91</td></tr><tr><td> Accuracy</td><td colspan="3">0.85</td></tr></tbody></table><table-wrap-foot><p>Table <xref rid="Tab2" ref-type="table">2</xref> summarizes the precision (positive predictive value), recall (sensitivity), accuracy, and <italic>F</italic>-score of each radiologist in comparison to the CovidCTNet.</p></table-wrap-foot></table-wrap></p>
      <p id="Par12">Radiologists performance accuracy was 81%, while CovidCTNet classification achieved a 95% accuracy when the question was detecting between Covid-19 versus non-Covid-19 (2 classes). When we asked to detect Covid-19 versus CAP versus control (three classes), again our approach outperformed the radiologists with an accuracy of 85% compared with human accuracy of 71%. The AUC of the model in Covid-19 detection versus reader test was 90% (Fig. <xref rid="Fig5" ref-type="fig">5</xref>). The accuracy, sensitivity, and specificity of the model showed a significantly higher validity compared to the average of radiologists.<fig id="Fig5"><label>Fig. 5</label><caption><title>Comparison of the outcome of CovidCTNet versus reader study.</title><p>Performance of model and radiologists (reader) in a pool of chest CT dataset mixed of control, Covid-19 and CAP. AUC of Covid-19 is 0.90 (<italic>n</italic> = 20 cases). The accuracy, sensitivity, and specificity of readers versus model are shown. The model operation in three classes demonstrates the detection of all three classes including Covid-19, CAP, and control separately and in two classes indicates the detection of Covid-19 as one class versus CAP and control as second class. While macroaverage takes the metric of each class independently and computes their average, the microaverage computes the average metric after aggregating the contributions of all classes.</p></caption><graphic xlink:href="41746_2021_399_Fig5_HTML" id="d32e1528"/></fig></p>
    </sec>
    <sec id="Sec7">
      <title>Images of Covid-19 and CAP infection share structural similarities</title>
      <p id="Par13">Despite some clear differences in Covid-19 and CAP infection pattern, the high similarities in ground-glass opacities (GGO) and consolidation on chest CT of Covid-19 and CAP (Fig. <xref rid="Fig4" ref-type="fig">4</xref>) makes differential detection a challenge. Several suspicious and challenging images are shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>.<fig id="Fig6"><label>Fig. 6</label><caption><title>Representative examples of CT images used to test the performance of CovidCTNet versus radiologists.</title><p><bold>a</bold> A CT image of CAP. This image is misidentified as Covid-19 or control by two out of four radiologists and correctly diagnosed by CovidCTNet. <bold>b</bold> A CT image of control, that was misdiagnosed by three out of four radiologists as Covid-19 or CAP and correctly diagnosed by CovidCTNet as control. <bold>c</bold> A sample of Covid-19 that was detected as Control by CovidCTNet and as Control or CAP by three out of the entire panel of radiologists (four members). <bold>d</bold> Image of control that was misdiagnosed by the CovidCTNet as CAP and by two radiologists as Covid-19 or control. Note that, in this figure one single slide of the entire scan is shown as a representative of all CT images of a patient.</p></caption><graphic xlink:href="41746_2021_399_Fig6_HTML" id="d32e1561"/></fig></p>
      <p id="Par14">The CAP CT slide (Fig. <xref rid="Fig6" ref-type="fig">6a</xref>) was misdiagnosed by radiologists as Covid-19 or Control by radiologists but correctly by CovidCTNet. Figure <xref rid="Fig6" ref-type="fig">6b</xref> is a control image that was correctly diagnosed by CovidCTNet and misdiagnosed by three radiologists. Figure <xref rid="Fig6" ref-type="fig">6c</xref> is a Covid-19. Its diagnosis posed a challenge for three radiologists out of four and also for CovidCTNet. The Control shown in Fig. <xref rid="Fig6" ref-type="fig">6d</xref> was misdiagnosed by two radiologists and by CovidCTNet.</p>
    </sec>
  </sec>
  <sec id="Sec8" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par15">In recent studies the average sensitivity of radiologists to detect the Covid-19 infection is reported to be approximately 70%<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, indicating the need for decision support tools to assist radiologists in detecting Covid-19, especially in regions where there is limited in the number of trained clinical staff or the comprehensive expertize to detect Covid-19.</p>
    <p id="Par16">Due to the high value of patch-based classification<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> we implemented it as one of our models, which did not perform as good as CovidCTNet on the small dataset. The CovidCTNet improved the accuracy and consistency of lung screening for Covid-19 detection through a ready-to-use platform with a sensitivity of 93% and accuracy of 95% (in making a binary decision, i.e., Covid-19 and non-Covid19). While the broad similarity of patterns and image features of Covid-19 and CAP posed a challenge for algorithm training, the high accuracy of the model indicates the potential for CovidCTNet to be further refined and adapted as a clinical decision support tool. In contrast to state-of-the-art works<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup> the dataset we used in this study is significantly smaller and highly heterogeneous. Adding more CT images will increase the accuracy and performance of the model.</p>
    <p id="Par17">Beyond optimizing and improving the Covid-19 detection, CovidCTNet has the potential to significantly impact the clinical workflow and patient care by offering a rapid, inexpensive, and accurate methodology to empower healthcare workers during the pandemic. Our radiologists are highly experienced from prestigious institutions. In an underrepresented region, it is not easy to find an experienced radiologist and we believe these types of AI systems will be significantly helpful to save lives. Importantly, when an infection type is hard to be diagnosed by the human eye, and when a consensus among radiologists cannot be made, CovidCTNet can be operated as a reliable source of diagnosis. To our knowledge, despite other promising efforts<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR39">39</xref></sup>, (summarized in Table <xref rid="Tab4" ref-type="table">4</xref>) CovidCTNet is an open-source framework that allows researchers and developers to adjust and build other applications based on it in a fraction of time. Besides, our approach follows the guideline proposed by Mongan et al.<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> for developing an AI method for medical image analysis.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Comparison of state-of-the-art Covid-19 classification approaches which used CT images.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Citation</th><th>Characteristics</th><th>Algorithm</th><th>Outcome</th><th># of analyzed patients (#of images)</th></tr></thead><tbody><tr><td>Javaheri et al.</td><td>U-net-based image preprocessing</td><td>BCDU-Net and Perlin noise exposure; CNN</td><td>AUC: 90% Sensitivity: 93% Specificity: 100%</td><td>335 (16,750)</td></tr><tr><td>Bai et al.<sup><xref ref-type="bibr" rid="CR20">20</xref></sup></td><td>HU-based lung segmentation</td><td>EfficientNet B4;CNN</td><td><p>AUC: 87% Sensitivity: 89%</p><p>Specificity: 86%</p></td><td>1186 (132,583)</td></tr><tr><td>Mei et al.<sup><xref ref-type="bibr" rid="CR39">39</xref></sup></td><td>A joint model using Lung segmentation and clinical data</td><td>ResNet-18; CNN; SVM; Random Forest; MLP</td><td><p>AUC: 92%</p><p>Sensitivity: 84.3% Specificity: 82.8%</p></td><td>905 (unknown)</td></tr><tr><td>Li et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup></td><td>U-net-based lung segmentation</td><td>Resnet50; CNN</td><td>AUC: 96% Sensitivity: 90% Specificity: 96%</td><td>3322 (4356)</td></tr><tr><td>Zhang et al.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup></td><td>A joint model using Lung segmentation and clinical data. U-net, DRUNET, FCN, SegNet and DeepLabv3-based lung segmentation</td><td>3D Resnet-18; CNN</td><td>AUC: 97% Sensitivity: 92% Specificity: 85%</td><td>4154 (617,775)</td></tr></tbody></table><table-wrap-foot><p>This table provides a summary on existing models including their method, achieved AUC, sensitivity, and specificity. We report here the list of approaches that rely on chest CT scans. The X-ray images were excluded from this list as they have been studied by Maguolo et al.<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>.</p></table-wrap-foot></table-wrap></p>
    <p id="Par18">In future efforts, we intend to (i) increase other samples as the CT scans using in this study are mostly from Iranian patients, (ii) include other demographic details of patients including age, gender and medical history to develop a predictive model, (iii) testing the model with a larger number of CT scan databases to further validate and broaden the application of our strategy.</p>
  </sec>
  <sec id="Sec9">
    <title>Methods</title>
    <sec id="Sec10">
      <title>Pre-processing</title>
      <p id="Par19">In our effort to address the clinical diagnostic needs, with the written informed consent of patients, under institutional review board (IRB) approval (IR.TUMS.VCR.REC.1399.007, Tehran University of Medical Sciences), we collected CT images. The CT images covered a variety of image sizes, slice thicknesses, and different configurations of a range of CT scanning devices. Depending on the device and the radiologist decision, the number of scans (e.g., 60, 70, etc.), the image resolution (e.g., 512 × 512 pixels, 768 × 768 pixels, etc.), and pixel spaces in the CT images varied. Together these factors allowed us to generate a heterogeneous collection that accounts for differences in CT imaging that exist among the medical community. This broad heterogeneity within the image collection aimed to resolve the potential bias in image analysis towards a specific image quality or types of CT imaging device<sup><xref ref-type="bibr" rid="CR40">40</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup>.</p>
      <p id="Par20">In the first step of pre-processing, the CT slices were resampled along three axes (<italic>z, y, x</italic>) to account for the variety of voxel dimensions among the CT slices (voxel is a single pixel, but in three dimensions). We used the distances of 1 × 1 × 1 mm for all voxel dimensions. Our method unified CT scans into the same scale and created a resampled dataset from the original dataset, known as resampling to an isomorphic resolution<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> (<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/kmader/finding-lungs-in-ct-data">https://www.kaggle.com/kmader/finding-lungs-in-ct-data</ext-link>) (Fig. <xref rid="Fig8" ref-type="fig">8</xref> upper part). In the second pre-processing step, pixel value of the resampled CT images (3D) was optimized to have a proper range of Hounsfield Units (HU). In our dataset, the least dense object such as air takes a value of −1000. Lung is an organ filled with air and thus acquires a HU value of −700 to −600. Other organs that may interfere with our analysis include water (HU of 0), fat (HU of −90 to −120) soft tissue (HU of 100–300), and bone (HU of 300–1900).</p>
      <p id="Par21">Consequently, we filtered CT slices (2D) to remove non-lung tissue (e.g., skin, bone, or scanner bed) that may negatively impact our analysis and to keep only the lung related parts with an HU value ranging from −1000 to 400. Next, a min-max normalization is applied to rescale the −1000 and 400 numerical ranges of pixels to a 0.0 and 1.0 scale (Fig. <xref rid="Fig7" ref-type="fig">7b</xref>). In the third step of pre-processing, all CT slices of various pixel sizes were resized to a uniform 128 × 128 pixels on their <italic>x</italic> and <italic>y</italic> dimensions but the number of slices (<italic>z</italic>) remained intact (Fig. <xref rid="Fig7" ref-type="fig">7b</xref>).<fig id="Fig7"><label>Fig. 7</label><caption><title>Schematic representation of the pre-processing phases.</title><p><bold>a</bold> Each patient’s CT image (3D) was resampled to isomorphic resolution, while <italic>x</italic> and <italic>y</italic> are the image coordinates and <italic>z</italic> represents the number of slices. <bold>b</bold> All CT slices (2D) with different sizes were resized to have 128 × 128 pixels on the <italic>x</italic> and <italic>y</italic> axis, but the <italic>z</italic> axis that depicts the number of slices remained intact. Here, a 512 × 512 pixels CT slice is resized into a 128 × 128 pixels CT slice.</p></caption><graphic xlink:href="41746_2021_399_Fig7_HTML" id="d32e1849"/></fig></p>
    </sec>
    <sec id="Sec11">
      <title>Algorithms</title>
      <p id="Par22">The architecture of CovidCTNet is presented in detail in Fig. <xref rid="Fig8" ref-type="fig">8</xref>. To identify Covid-19 in the lung from CAP lesion, we generated pseudo-infection anomalies in the CT Control images using BCDU-Net<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. The BCDU-Net module played a critical role in allowing the detection of infections that have numerous features in a small dataset. It helped us to increase the accuracy and the rate of model convergence by using the initialization of the model that is trained on the Kaggle dataset for lung segmentation (<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/kmader/finding-lungs-in-ct-data">https://www.kaggle.com/kmader/finding-lungs-in-ct-data</ext-link>). The BCDU-Net was used in our model for two purposes, first cleaning images, by removing tissues that are not related to lung infection, such as heart, skin, or the bed of CT image device, and second canceling the noise, which is used by our model for lung infection identification. To cancel the noise, BCDU-Net focused on lung infection by generating Perlin noise<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> (pseudo-infection) and detecting infections. A subset of Control images mixed of noisy and non-noisy were given to BCDU-Net as an input. At the same time, the original Control images of noisy or non-noisy subsets were targeted in the model as output, mimicking the Covid-19 and CAP anomalies in the Control cases (Fig. <xref rid="Fig8" ref-type="fig">8</xref> upper part). The motivation of using artificial noise (to train the model for infection detection) was to simulate both the healthy and infected state of the same lung. Therefore, the BCDU-Net will learn the differences and how to extract infection from the CT images. To learn how to clean the CT images, BCDU-Net received original CT images without noise along with the images that have noise applied on them. Afterward, the model learnt to identify and tried removing unnecessary image contents such as heart tissue. By feeding the BCDU-Net with noisy CT images, the model learnt to identify infections or lesions.<fig id="Fig8"><label>Fig. 8</label><caption><title>Multistep pipeline of deep learning algorithms to detect Covid-19 from CT images.</title><p>Upper part, Training step of the model for learning the structure of Control CT slices. Middle part, Images subtracting and lung reconstructing from CT slices with highlighted Covid-19 or CAP infection (violet color). The results of step “i” are a 2D image. The slices at z axis concatenate to generate 3D CT image, the input of CNN model. Lower part, CNN model classifies the images that were constructed in the previous stage. To integrate this pipeline into an application the user needs to start from stage (middle part) and then the CNN algorithm recognizes whether the given CT images of a given patient presents Covid-19, CAP, or control. The number outside the parentheses in CNN model, present the number of channels in the CNN model.</p></caption><graphic xlink:href="41746_2021_399_Fig8_HTML" id="d32e1885"/></fig></p>
      <p id="Par23">To this end, we specified the input which is a combination of original healthy CT images and the CT images with noise. We defined the target of our model to be the same input CT images but without noise. Consequently, the output of BCDU-Net is de-noised and reconstructed CT images from the original and noisy CT images. Reconstructing the CT images helped the model to learn the pattern of the control lung and reconstruct the original Control image as output by noise reduction (de-noised) and infection removal (Fig. <xref rid="Fig8" ref-type="fig">8b, c</xref>). Thus, Covid-19 or CAP images could not be reconstructed correctly at this stage. Identifying the Control lung pattern led to recognition of non-control slices such as Covid-19 or CAP. In the first step, the training phase (Fig. <xref rid="Fig8" ref-type="fig">8</xref> upper part) starts by randomly selecting a dataset of 66 control patients (21,888 slices) and applying the pre-processing steps on their CT images (Fig. <xref rid="Fig8" ref-type="fig">8a, b</xref>). The dataset was divided into two subsets: (i) the original CT images of 10,944 slices (Fig. <xref rid="Fig8" ref-type="fig">8c</xref> right), and (ii) CT images of 10,944 slices with applied Perlin noise<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> (Fig. <xref rid="Fig8" ref-type="fig">8c</xref> left). The BCDU-Net model was trained with two noisy and non-noisy subsets of Control images and the trained model was frozen at this step (Fig. <xref rid="Fig8" ref-type="fig">8d</xref>).</p>
      <p id="Par24">Next, we applied pre-processing to the entire dataset. To ensure fair comparison, the images that were used in the pre-processing step (Fig. <xref rid="Fig8" ref-type="fig">8</xref> upper part), were excluded from the validation step.</p>
      <p id="Par25">Afterwards these CT slices, including Control, CAP and Covid-19 were resized (Fig. <xref rid="Fig8" ref-type="fig">8</xref> middle part e, f) and fed into the frozen BCDU-Net model (Fig. <xref rid="Fig8" ref-type="fig">8</xref>, middle part g). The output of the BCDU-Net is the de-noised CT slices (Fig. <xref rid="Fig8" ref-type="fig">8</xref>, middle part h). The algorithm subtracted the output of BCDU-Net, the lung slices without infection, (Fig. <xref rid="Fig8" ref-type="fig">8</xref>, middle part i) from the preprocessed CT slices, infected lung with Covid-19 or CAP (Fig. <xref rid="Fig8" ref-type="fig">8</xref>, middle part e, f) to acquire the infected areas of lung (Fig. <xref rid="Fig8" ref-type="fig">8</xref>, middle part i). Because the outcome of subtraction (Fig. <xref rid="Fig8" ref-type="fig">8</xref>, middle part i) depicted the highlighted infection area (Covid-19 and CAP) without other tissues or artifacts (Fig. <xref rid="Fig8" ref-type="fig">8</xref>, middle part image in violet color and Fig. <xref rid="Fig3" ref-type="fig">3b</xref>), it provided a reliable source for the infection classification as Covid-19 or CAP. Further examples of the result of this step are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, which shows exactly how Covid-19 and CAP infections were extracted by BCDU-Net. In other words, an example of the subtracted data (violet CT slices resulted from Fig. <xref rid="Fig8" ref-type="fig">8</xref>, middle part k) depicts the infection area in the lung (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>).</p>
      <p id="Par26">In the validation, we observed that the subtraction resulted from original non-infected CT slices versus the output of BCDU-Net was insignificant, confirming the accuracy of detecting noise as an indication for the infected area. The slices at <italic>z</italic>-axis were concatenated to generate a 3D CT image that was the input of a three-dimensional convolutional neural network (CNN) model (Fig. <xref rid="Fig8" ref-type="fig">8</xref>, middle part j). The outcome of CT slices was resized due to high variation among the number of CT slices for each patient (Fig. <xref rid="Fig8" ref-type="fig">8</xref>, middle part k). Resizing ensures that all CT images have equal sizes, which is required by CNN to have a unified size (50 × 128 × 128). Here, 50 in the <italic>z</italic>-axis indicates that all the patients’ CT slices were resized to 50 slices. These 3D images were already labeled by radiologists as Covid-19, CAP, or control. To implement the classification algorithm, we used CNN. In the final step, the result of Fig. <xref rid="Fig8" ref-type="fig">8</xref> middle part k was fed into the CNN model (Fig. <xref rid="Fig8" ref-type="fig">8</xref>, lower part l) as a training dataset. In the training phase, the model learned to distinguish Covid-19, CAP, and control. CNN model was then validated by using 15 cases that were selected randomly and were never used before in any of the training and preceding steps. The output of the CNN algorithm is a numerical value that classifies the given patient CT images as Covid-19 or CAP or control (Fig. <xref rid="Fig8" ref-type="fig">8</xref>, lower part l).</p>
      <p id="Par27">The slices at <italic>z</italic>-axis were concatenated to generate a 3D CT image that was the input of a three-dimensional convolutional neural network (CNN) model (Fig. <xref rid="Fig8" ref-type="fig">8</xref>, middle part j). The outcome of CT slices was resized due to high variation among the number of CT slices for each patient (Fig. <xref rid="Fig8" ref-type="fig">8</xref>, middle part k). Resizing ensures that all CT images have equal sizes, which is required by CNN to have a unified size (50 × 128 × 128). Here, 50 in the <italic>z</italic>-axis indicates that all the patients’ CT slices were resized to 50 slices. These 3D images were already labeled by radiologists as Covid-19, CAP, or control.</p>
    </sec>
    <sec id="Sec12">
      <title>Reporting summary</title>
      <p id="Par28">Further information on research design is available in the <xref rid="MOESM1" ref-type="media">Nature Research Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec13">
      <supplementary-material content-type="local-data" id="MOESM1">
        <media xlink:href="41746_2021_399_MOESM1_ESM.pdf">
          <caption>
            <p>Reporting Summary</p>
          </caption>
        </media>
      </supplementary-material>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>These authors contributed equally: Tahereh Javaheri, Morteza Homayounfar, Zohreh Amoozgar, Reza Reiazi.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>The online version contains supplementary material available at 10.1038/s41746-021-00399-3.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>This research received no specific grant from any funding agency and all authors contributed solely as volunteers. We acknowledge the assistance of Prof. Reza Yousefi-Nooraei from University of Rochester, U.S. and Dr. Bita Mesgarpour from National Institute for Medical Research Development, Iran for establishing the connections between scientists from different places in the world in a very short time. Besides, we warmly appreciate Dr. Ali Babaei Jandaghi from Princess Margaret Cancer Centre, Canada and Dr. Nahid Sadighi from Tehran University of Medical Sciences, Iran for sharing their expertize on CT images annotation. We dedicate our results to all nurses and physicians around the world.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>R.R., (Rawassizadeh) planned and directed the study. T.J. and R.R. (Rawassizadeh) conceived and planned the study. M.H. and R.R., (Rawassizadeh) designed the model and the computational framework and analysed the data. T.J. and Z.A. took the lead in writing the manuscript and contributed to the interpretation of the results. R.R., (Reiazi) performed the data analysis and calculations. F.H., E.A., A.L., A.R.R., M.H.G., S.A.J.M., O.G., R.B., H.K.M., M.H., R.J., K.S., M.K.K., G.Z., L.T.C., B.H.K., and R.M. contributed to image collection, provided critical feedback and helped to shape the research, analysis and manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>The raw image dataset generated or analyzed during this study is not publicly available due to the patient privacy/consent. Datasets are available to qualified researchers following completion of a Dataset License Agreement, which is available from the corresponding author’.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>To allow full reproducibility of our claims<sup><xref ref-type="bibr" rid="CR43">43</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup> all codes are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/mohofar/covidctnet">https://github.com/mohofar/covidctnet</ext-link>. The repository contains all necessary information of model instruction and code execution. For model validation, some CT images of all three groups (Covid-19, CAP, and control) are included in the dataset.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par29">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fraser</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Pandemic potential of a strain of influenza A (H1N1): early findings</article-title>
        <source>Science</source>
        <year>2009</year>
        <volume>324</volume>
        <fpage>1557</fpage>
        <lpage>1561</lpage>
        <pub-id pub-id-type="doi">10.1126/science.1176062</pub-id>
        <pub-id pub-id-type="pmid">19433588</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <mixed-citation publication-type="other">WHO Coronavirus disease (COVID-19) pandemic <ext-link ext-link-type="uri" xlink:href="https://www.who.int/emergencies/diseases/novel-coronavirus-2019">https://www.who.int/emergencies/diseases/novel-coronavirus-2019</ext-link> Accessed 15 Aug 2020.</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kobayashi</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Communicating the risk of death from novel coronavirus disease (COVID-19)</article-title>
        <source>J. Clin. Med.</source>
        <year>2020</year>
        <volume>9</volume>
        <fpage>580</fpage>
        <pub-id pub-id-type="doi">10.3390/jcm9020580</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Riou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Althaus</surname>
            <given-names>CL</given-names>
          </name>
        </person-group>
        <article-title>Pattern of early human-to-human transmission of Wuhan 2019 novel coronavirus (2019-nCoV), December 2019 to January 2020</article-title>
        <source>Eurosurveillance</source>
        <year>2020</year>
        <volume>25</volume>
        <fpage>2000058</fpage>
        <pub-id pub-id-type="doi">10.2807/1560-7917.ES.2020.25.4.2000058</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Park, M., Thwaites, R. S. &amp; Openshaw, P. J. M. COVID-19: lessons from SARS and MERS. <italic>Eur. J. Immunol.</italic>10.1002/eji.202070035 (2020).</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhong</surname>
            <given-names>NS</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Epidemiology and cause of severe acute respiratory syndrome (SARS) in Guangdong, People’s Republic of China, in February 2003</article-title>
        <source>Lancet</source>
        <year>2003</year>
        <volume>362</volume>
        <fpage>1353</fpage>
        <lpage>1358</lpage>
        <pub-id pub-id-type="doi">10.1016/S0140-6736(03)14630-2</pub-id>
        <pub-id pub-id-type="pmid">14585636</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hui</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>PK</given-names>
          </name>
        </person-group>
        <article-title>Severe acute respiratory syndrome and coronavirus. Infectious disease clinics of North America</article-title>
        <source>Infect. Dis. Clin.</source>
        <year>2010</year>
        <volume>24</volume>
        <fpage>619</fpage>
        <lpage>638</lpage>
        <pub-id pub-id-type="doi">10.1016/j.idc.2010.04.009</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Fang, Y. et al. Sensitivity of chest CT for COVID-19: comparison to RT-PCR. <italic>Radiology</italic>10.1148/radiol.2020200432 (2020).</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Ai, T. et al. Correlation of chest CT and RT-PCR testing in coronavirus disease 2019 (COVID-19) in China: a report of 1014 cases. <italic>Radiology</italic>10.1148/radiol.2020200642 (2020).</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Bernheim, A. et al. Chest CT findings in coronavirus disease-19 (COVID-19): relationship to duration of infection. <italic>Radiology</italic>10.1148/radiol.2020200463 (2020).</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yuan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Tao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Association of radiologic findings with mortality of patients infected with 2019 novel coronavirus in Wuhan, China</article-title>
        <source>PLoS ONE</source>
        <year>2020</year>
        <volume>15</volume>
        <fpage>e0230548</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0230548</pub-id>
        <pub-id pub-id-type="pmid">32191764</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Li, K. et al. CT image visual quantitative evaluation and clinical classification of coronavirus disease (COVID-19). <italic>Eur. Radiol.</italic>10.1007/s00330-020-06817-6 (2020).</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Xie, X. et al. Chest CT for typical 2019-nCoV pneumonia: relationship to negative RT-PCR testing. <italic>Radiology</italic>10.1148/radiol.2020200343 (2020).</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Spellberg, B. et al. Community prevalence of SARS-CoV-2 among patients with influenzalike illnesses presenting to a Los Angeles Medical Center in March 2020. <italic>JAMA</italic>10.1001/jama.2020.4958 (2020).</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Yan, G. et al. Covert COVID-19 and false-positive dengue serology in Singapore. <italic>Lancet Infect. Dis.</italic>10.1016/S1473-3099(20)30158-4 (2020).</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Chest CT severity score: an imaging tool for assessing severe COVID-19</article-title>
        <source>Radiology</source>
        <year>2020</year>
        <volume>2</volume>
        <fpage>e200047</fpage>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Wang, S. et al. A deep learning algorithm using CT images to screen for corona virus disease (COVID-19). medRxiv, 10.1101/2020.02.14.20023028 (2020).</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Li, Y. &amp; Xia, L. Coronavirus disease 2019 (COVID-19): role of chest CT in diagnosis and management. <italic>Am. J. Roentgenol.</italic>10.2214/AJR.20.22954 (2020).</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chung</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CT imaging features of 2019 novel coronavirus (2019-nCoV)</article-title>
        <source>Radiology</source>
        <year>2020</year>
        <volume>295</volume>
        <fpage>202</fpage>
        <lpage>207</lpage>
        <pub-id pub-id-type="doi">10.1148/radiol.2020200230</pub-id>
        <pub-id pub-id-type="pmid">32017661</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Bai, H. X. et al. Performance of radiologists in differentiating COVID-19 from viral pneumonia on chest CT. <italic>Radiology</italic>10.1148/radiol.2020200823 (2020).</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bulten</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated deep-learning system for Gleason grading of prostate cancer using biopsies: a diagnostic study</article-title>
        <source>Lancet Oncol.</source>
        <year>2020</year>
        <volume>21</volume>
        <fpage>233</fpage>
        <lpage>241</lpage>
        <pub-id pub-id-type="doi">10.1016/S1470-2045(19)30739-9</pub-id>
        <pub-id pub-id-type="pmid">31926805</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shen</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning to improve breast cancer detection on screening mammography</article-title>
        <source>Sci. Rep.</source>
        <year>2019</year>
        <volume>9</volume>
        <fpage>12495</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-019-48995-4</pub-id>
        <pub-id pub-id-type="pmid">31467326</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yoo</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gujrathi</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Haider</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Khalvati</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Prostate cancer detection using deep convolutional neural networks</article-title>
        <source>Sci. Rep.</source>
        <year>2019</year>
        <volume>9</volume>
        <fpage>19518</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-019-55972-4</pub-id>
        <pub-id pub-id-type="pmid">31863034</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ardila</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography</article-title>
        <source>Nat. Med.</source>
        <year>2019</year>
        <volume>25</volume>
        <fpage>954</fpage>
        <lpage>961</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-019-0447-x</pub-id>
        <pub-id pub-id-type="pmid">31110349</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Li, L. et al. Artificial intelligence distinguishes COVID-19 from community acquired pneumonia on chest CT. <italic>Radiology</italic>10.1148/radiol.2020200905 (2020).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Apostolopoulos, I. D. &amp; Mpesiana, T. A. Covid-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks. <italic>Phys. Eng. Sci. Med.</italic>10.1007/s13246-020-00865-4 (2020).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Sun, C., Shrivastava, A., Singh, S. &amp; Gupta, A. Revisiting unreasonable effectiveness of data in deep learning era. <italic>2007 IEEE International Conference on Computer Vision (ICCV)</italic>, 843–852, Venice, (2017). 10.1109/ICCV.2017.97.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Erickson</surname>
            <given-names>BJ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning in radiology: does one size fit all?</article-title>
        <source>J. Am. Coll. Radiol.</source>
        <year>2018</year>
        <volume>15</volume>
        <fpage>521</fpage>
        <lpage>526</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jacr.2017.12.027</pub-id>
        <pub-id pub-id-type="pmid">29396120</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Zhang, K. et al. Clinically applicable AI system for accurate diagnosis, quantitative measurements and prognosis of COVID-19 pneumonia using computed tomography. <italic>Cell</italic>10.1016/j.cell.2020.04.045 (2020).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Azad, R., Asadi-Aghbolaghi, M., Fathy, M., &amp; Escalera, S. Bi-directional ConvLSTM U-Net with densley connected convolutions. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1909.00166">https://arxiv.org/abs/1909.00166</ext-link> (2019).</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Linda, W. &amp; Alexander, W. COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/2003.0987">https://arxiv.org/2003.0987</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Chassagnon, G. et al. AI-driven CT-based quantification, staging and short-term outcome prediction of COVID-19 pneumonia. <italic>medRxiv</italic>10.1101/2020.04.17.20069187 (2020).</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Guan, S., Khan, A., Sikdar, S. &amp; Chitnis, P. Fully dense UNet for 2D sparse photoacoustic tomography artifact removal. <italic>IEEE J. Biomed. Health</italic>10.1109/JBHI.2019.2912935 (2019).</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &amp; Brox, T. <italic>Medical Image Computing and Computer-Assisted Intervention—MICCAI 2015</italic>. 234–241 (Springer, 2015).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Armato</surname>
            <given-names>SG</given-names>
            <suffix>III</suffix>
          </name>
          <etal/>
        </person-group>
        <article-title>SPIE-AAPM-NCI lung nodule classification challenge dataset</article-title>
        <source>Cancer Imaging Arch.</source>
        <year>2015</year>
        <volume>10</volume>
        <fpage>p.K9</fpage>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kirby</surname>
            <given-names>JS</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>LUNGx Challenge for computerized lung nodule classification</article-title>
        <source>J. Med. Imaging</source>
        <year>2016</year>
        <volume>3</volume>
        <fpage>044506</fpage>
        <pub-id pub-id-type="doi">10.1117/1.JMI.3.4.044506</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ken</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>An image synthesizer</article-title>
        <source>Comput. Graph.</source>
        <year>1985</year>
        <volume>19</volume>
        <fpage>287</fpage>
        <lpage>296</lpage>
        <pub-id pub-id-type="doi">10.1145/325165.325247</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Hou, L. et al. Patch-based convolutional neural network for whole slide tissue image classification. In <italic>Proc IEEE Computer Society Conference on Computer Vision and Pattern Recognition</italic>, 2424–2433 (2016) 10.1109/CVPR.2016.266.</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Mei, X. et al. Artificial intelligence-enabled rapid diagnosis of patients with COVID-19. <italic>Nat. Med</italic>. 10.1038/s41591-020-0931-3 (2020).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Mongan, J. et al. Checklist for artificial intelligence in medical imaging (CLAIM): a guide for authors and reviewers. <italic>Radiology</italic>10.1148/ryai.2020200029 (2020).</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Maguolo, G. et al. A critic evaluation of methods for COVID-19 automatic detection from X-ray images. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/2004.12823">https://arxiv.org/2004.12823</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Automated pulmonary nodule detection in CT images using deep convolutional neural networks</article-title>
        <source>Pattern Recognit.</source>
        <year>2019</year>
        <volume>85</volume>
        <fpage>109</fpage>
        <lpage>119</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2018.07.031</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sim</surname>
            <given-names>I</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Time for NIH to lead on data sharing</article-title>
        <source>Science</source>
        <year>2020</year>
        <volume>367</volume>
        <fpage>1308</fpage>
        <lpage>1309</lpage>
        <pub-id pub-id-type="doi">10.1126/science.aba4456</pub-id>
        <pub-id pub-id-type="pmid">32193313</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Benjamin, H. K. et al. The importance of transparency and reproducibility in artificial intelligence research. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/2003.00898">https://arxiv.org/2003.00898</ext-link> (2020).</mixed-citation>
    </ref>
  </ref-list>
</back>
