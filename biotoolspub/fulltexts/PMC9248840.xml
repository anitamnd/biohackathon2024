<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Acta Crystallogr D Struct Biol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Acta Crystallogr D Struct Biol</journal-id>
    <journal-id journal-id-type="publisher-id">Acta Cryst. D</journal-id>
    <journal-title-group>
      <journal-title>Acta Crystallographica. Section D, Structural Biology</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2059-7983</issn>
    <publisher>
      <publisher-name>International Union of Crystallography</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9248840</article-id>
    <article-id pub-id-type="publisher-id">ni5022</article-id>
    <article-id pub-id-type="doi">10.1107/S2059798322005654</article-id>
    <article-id pub-id-type="coden">ACSDAD</article-id>
    <article-id pub-id-type="pii">S2059798322005654</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Papers</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Development of <italic toggle="yes">CryoVR</italic>, a virtual reality training system for hands-on cryoEM operations</article-title>
      <alt-title>
        <italic toggle="yes">CryoVR</italic>
      </alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9211-3149</contrib-id>
        <name>
          <surname>Dong</surname>
          <given-names>Jiahui</given-names>
        </name>
        <xref rid="a" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2913-8049</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Daoyi</given-names>
        </name>
        <xref rid="b" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9337-9798</contrib-id>
        <name>
          <surname>Ozcan</surname>
          <given-names>Kadir</given-names>
        </name>
        <xref rid="b" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2104-0768</contrib-id>
        <name>
          <surname>Wan</surname>
          <given-names>Dayu</given-names>
        </name>
        <xref rid="a" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1292-2411</contrib-id>
        <name>
          <surname>Jiang</surname>
          <given-names>Wen</given-names>
        </name>
        <xref rid="b" ref-type="aff">b</xref>
        <xref rid="cor" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6705-3535</contrib-id>
        <name>
          <surname>Chen</surname>
          <given-names>Yingjie</given-names>
        </name>
        <xref rid="a" ref-type="aff">a</xref>
        <xref rid="cor" ref-type="corresp">*</xref>
      </contrib>
      <aff id="a"><label>a</label>Computer Graphics Technology, <institution>Purdue University</institution>, Knoy Hall Room 363, 401 North Grant Street, West Lafayette, IN 47907, <country>USA</country></aff>
      <aff id="b"><label>b</label>Department of Biological Sciences, <institution>Purdue University</institution>, 915 W. State Street, West Lafayette, IN 47907, <country>USA</country></aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor">Correspondence e-mail: <email>jiang12@purdue.edu</email>, <email>victorchen@purdue.edu</email>
</corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>27</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>27</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <!--PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>.-->
    <volume>78</volume>
    <issue>Pt 7</issue>
    <issue-id pub-id-type="publisher-id">d220700</issue-id>
    <fpage>903</fpage>
    <lpage>910</lpage>
    <history>
      <date date-type="received">
        <day>04</day>
        <month>3</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>5</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© Jiahui Dong et al. 2022</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution (CC-BY) Licence, which permits
unrestricted use, distribution, and reproduction in any medium, provided the original authors and source are cited.</license-p>
      </license>
      <ali:free_to_read xmlns:ali="http://www.niso.org/schemas/ali/1.0/"/>
    </permissions>
    <self-uri xlink:href="https://doi.org/10.1107/S2059798322005654">A full version of this article is available from Crystallography Journals Online.</self-uri>
    <abstract abstract-type="toc">
      <p>This paper describes the design and assessment of <italic toggle="yes">CryoVR</italic>, a virtual reality-based training system for cryoEM devices. <italic toggle="yes">CryoVR</italic> provides a unique learning opportunity by giving users experience of cryoEM experimental procedures through a safe and accessible virtual environment.</p>
    </abstract>
    <abstract>
      <p>Cryogenic electron microscopy (cryoEM) has emerged as a revolutionary method for solving high-resolution structures and studying the dynamics of macromolecular complexes and viruses in near-native states. However, the availability of the equipment, and the time and cost needed for training, severely limit the opportunities for training. To solve these problems, a virtual reality-based training system, <italic toggle="yes">CryoVR</italic>, has been developed to prepare trainees before operating real-world cryoEM equipment. This paper describes the design and assessment of <italic toggle="yes">CryoVR</italic> (available at <ext-link xlink:href="https://www.purdue.edu/cryoVR" ext-link-type="uri">https://www.purdue.edu/cryoVR</ext-link>), which helps users learn cryoEM experimental procedures in a virtual environment, allowing immersive training with step-by-step tutorials with vivid visual, audio and text guidance. Implemented as a training step before a novice user interacts with the expensive real-world cryoEM equipment, <italic toggle="yes">CryoVR</italic> can help users to become familiar with hands-on operational procedures through multiple training modules and earning certificates after passing the built-in Exam mode. Qualitative evaluation and feedback of <italic toggle="yes">CryoVR</italic> from users with various levels of cryoEM experience indicate the substantial value of <italic toggle="yes">CryoVR</italic> as a tool for a comprehensive cryoEM procedural training.</p>
    </abstract>
    <kwd-group>
      <kwd>
        <italic toggle="yes">CryoVR</italic>
      </kwd>
      <kwd>cryoEM</kwd>
      <kwd>virtual reality</kwd>
      <kwd>virtual reality training</kwd>
      <kwd>human–computer interaction</kwd>
      <kwd>structural biology</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>National Institutes of Health</funding-source>
        <award-id>1R25EY029127-01</award-id>
      </award-group>
      <funding-statement>Funding for this research was provided by: National Institutes of Health (grant No. 1R25EY029127-01).</funding-statement>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="introduction" id="sec1">
    <label>1.</label>
    <title>Introduction</title>
    <p>Cryogenic electron microscopy (cryoEM) has emerged as a revolutionary method for solving high-resolution structures and studying the dynamics of biological macromolecular complexes and viruses in near-native states (Jiang &amp; Tang, 2017<xref rid="bb22" ref-type="bibr"> ▸</xref>; Callaway, 2015<xref rid="bb9" ref-type="bibr"> ▸</xref>). The rapidly increasing number of near-atomic resolution structures solved using single-particle cryoEM has generated the data needed to understand a variety of fundamental biological processes (Sirohi <italic toggle="yes">et al.</italic>, 2016<xref rid="bb37" ref-type="bibr"> ▸</xref>; Wrapp <italic toggle="yes">et al.</italic>, 2020<xref rid="bb41" ref-type="bibr"> ▸</xref>; Yan <italic toggle="yes">et al.</italic>, 2020<xref rid="bb42" ref-type="bibr"> ▸</xref>). The cryoEM resolution revolution has attracted attention from researchers in various biomedical fields, and thus there is a crucial need to expand the training capabilities of cryoEM facilities around the world. However, the training capacities of these facilities are limited due to the need for repetitive practices on the instruments by the trainees, limited cryoEM instruments and staff availability, and cost (Alewijnse <italic toggle="yes">et al.</italic>, 2017<xref rid="bb4" ref-type="bibr"> ▸</xref>). The complex practical procedures required by cryoEM pose a steep learning curve to new users, and the challenges in offering fast, high-capacity training are considerable hurdles in cryoEM training. Moreover, a clear demonstration of safety procedures is crucial to reduce the potential risk of operating the equipment. Effective cryoEM training requires a combination of theory, observation and hands-on operation under close supervision from highly trained experts. These limitations in training have been well recognized and there are many efforts to address these needs. For example, the NIH Common Fund Transformative High Resolution Cryo-Electron Microscopy program (<ext-link xlink:href="https://commonfund.nih.gov/cryoem" ext-link-type="uri">https://commonfund.nih.gov/cryoem</ext-link>) has funded multiple national cryoEM centers and cryogenic electron tomography (cryoET) centers to provide both data-collection and user-training services. The NIH cryoEM program has also funded multiple curriculum-development teams to create new training materials spanning a wide range of approaches from web-based lectures on cryoEM (available at <ext-link xlink:href="https://cryoem101.org/" ext-link-type="uri">https://cryoem101.org/</ext-link>), in-depth theoretical understanding of principles (available at <ext-link xlink:href="https://cryoemprinciples.yale.edu/" ext-link-type="uri">https://cryoemprinciples.yale.edu/</ext-link>), a comprehensive collection of videos on theory and practical operations (available at <ext-link xlink:href="http://cryo-em-course.caltech.edu/" ext-link-type="uri">http://cryo-em-course.caltech.edu/</ext-link>) and, to our team, a virtual reality (VR) augmented training system named <italic toggle="yes">CryoVR</italic> for hands-on operations (Gonzalez <italic toggle="yes">et al.</italic>, 2019<xref rid="bb18" ref-type="bibr"> ▸</xref>).</p>
    <p>Since the first consumer-level head-mount display (HMD) for VR was brought to the market in 2013 by Oculus (<ext-link xlink:href="https://www.oculus.com/compare/" ext-link-type="uri">https://www.oculus.com/compare/</ext-link>), VR has become accessible to the public for gaming, education and research (Jensen &amp; Konradsen, 2018<xref rid="bb21" ref-type="bibr"> ▸</xref>). Modern VR technology has made the virtual experience much more realistic and affordable. The high-fidelity simulated sensations of visual, audio and touch by VR devices can create the feeling of presence in a virtual environment for the user. VR training can reduce training costs, especially when traditional training requires experienced expert trainers and can suffer from the breakdown of devices used during training (Carlson <italic toggle="yes">et al.</italic>, 2015<xref rid="bb10" ref-type="bibr"> ▸</xref>). VR has been widely adopted in simulation training for critical operations such as flight (Yavrucuk <italic toggle="yes">et al.</italic>, 2011<xref rid="bb44" ref-type="bibr"> ▸</xref>) and medical surgery (Lam <italic toggle="yes">et al.</italic>, 2013<xref rid="bb24" ref-type="bibr"> ▸</xref>) as it enables knowledge and skills to be acquired in an efficient, consistent and economical manner. In a systematic review of 34 studies from 2016 to 2018 on applications of VR in higher education, it was found that the true potential of VR is to provide the students with the ‘learn by doing’ experience, which is often difficult to implement in traditional lectures.</p>
    <p>In this paper, we introduce the development of <italic toggle="yes">CryoVR</italic> (<italic toggle="yes">CryoEM Virtual Reality</italic>), a VR training system that is dedicated to training users in the sample grid-preparation procedures of single-particle cryoEM. It contains four groups of modules: Grid Glow Discharging (easiGlow), Plunge Freezing (ThermoFisher Vitrobot and Gatan CP3), AutoGrid Clipping and AutoGrid Loading.</p>
  </sec>
  <sec id="sec2">
    <label>2.</label>
    <title>VR augmented training theory</title>
    <p>Recent studies have found that VR training is especially suitable for delivering procedural knowledge and declarative knowledge. Procedural knowledge refers to the knowledge of how to perform a specific task, for example procedures of operating an instrument. While current VR technology still cannot vividly simulate fine haptic feedback and is not able to train fine motor skills of the hands and body during operation, researchers have found that in terms of procedural knowledge training a VR environment can be as effective as a real environment (Smith <italic toggle="yes">et al.</italic>, 2016<xref rid="bb39" ref-type="bibr"> ▸</xref>). For example, military tank-maintenance work can be learned in a virtual environment as well as in a real-world training environment based on the success rate, time and the amount of guidance needed from an instructor (Ganier <italic toggle="yes">et al.</italic>, 2014<xref rid="bb17" ref-type="bibr"> ▸</xref>). It was found that in a VR environment using a ghost virtual trainer in a first-person viewpoint (called Just Follow Me; Yang &amp; Kim, 2002<xref rid="bb43" ref-type="bibr"> ▸</xref>) can produce a training and transfer effect at least as good as in the real world. This Just Follow Me approach can transfer proprioceptive information more directly and with high fidelity from a trainer to a trainee. The effect of VR on learning can be explained by multiple cognitive theories (Dale, 1969<xref rid="bb11" ref-type="bibr"> ▸</xref>). The experience theory demonstrates that learners can only remember 10% of what they read, but up to 90% if they can perform actions during a training period, suggesting that practical experience is vital for learning procedural knowledge (Anderson, 1982<xref rid="bb5" ref-type="bibr"> ▸</xref>). These findings suggest that VR is promising for procedural learning by providing learners with the ‘learn by doing’ experience. Based on embodied cognition theory, embodied learning has become an important part of modern learning practices (Foglia &amp; Wilson, 2013<xref rid="bb15" ref-type="bibr"> ▸</xref>; Price <italic toggle="yes">et al.</italic>, 2009<xref rid="bb33" ref-type="bibr"> ▸</xref>). An embodied learning scheme can be a great expansion of conventional training (Abrahamson, 2014<xref rid="bb3" ref-type="bibr"> ▸</xref>) because it can provide the engagement of the physical body in multi-modal learning experiences which may involve bodies, minds and social beings (Nguyen &amp; Larson, 2015<xref rid="bb30" ref-type="bibr"> ▸</xref>; Wilson, 2002<xref rid="bb40" ref-type="bibr"> ▸</xref>). Andrade and coworkers suggested that attention to gestures and movements in the design of learning environments can improve learning (Andrade <italic toggle="yes">et al.</italic>, 2017<xref rid="bb6" ref-type="bibr"> ▸</xref>). Designing instructions to account for the body can legitimize and mix new modalities and sets of intellectual resources for learning (Enyedy <italic toggle="yes">et al.</italic>, 2015<xref rid="bb14" ref-type="bibr"> ▸</xref>). As VR training typically involves the embodied learning activities by providing the trainees with virtual hands-on experiences, VR has the potential to leverage embodied cognition and can boost learning outcomes (Kosmas <italic toggle="yes">et al.</italic>, 2018<xref rid="bb23" ref-type="bibr"> ▸</xref>; Macedonia <italic toggle="yes">et al.</italic>, 2011<xref rid="bb27" ref-type="bibr"> ▸</xref>; Repetto <italic toggle="yes">et al.</italic>, 2015<xref rid="bb35" ref-type="bibr"> ▸</xref>).</p>
  </sec>
  <sec id="sec3">
    <label>3.</label>
    <title><italic toggle="yes">CryoVR</italic>: motivations and goals</title>
    <p>CryoEM projects consist of multiple steps with very different characteristics; for example, sample grid preparation and data collection require hands-on operation of various instruments, while image processing and 3D reconstruction require the running of complex computer programs. Training for these distinct tasks is best served by multiple approaches or media (Lyumkis, 2019<xref rid="bb26" ref-type="bibr"> ▸</xref>). Much of the training in theoretical knowledge and computational tasks can be implemented through in-person or remote classes, workshops or self-learning using a combination of PowerPoint slides, video recordings, live presentations and reading of manuals and protocols. However, training tasks that teach the users how to properly operate various instruments, for example plunger freezers and electron microscopes, require physical access to the instruments. A common feature of these tasks is that there are multiple steps to be performed in a particular sequence, and the learning requires hands-on operations of the instruments under the close supervision of an expert. These hands-on operations require exceptional cognitive skills and precise execution of correct procedures backed up by sufficient domain knowledge. These training tasks can be time-consuming, for example, observation of a 2–4 h demo session for cryo-grid loading and low-dose EM imaging followed by multiple supervised 2–4 h practice sessions before certification by the facility for independent work (Eng <italic toggle="yes">et al.</italic>, 2020<xref rid="bb13" ref-type="bibr"> ▸</xref>). This hands-on training is essential, but also challenging, as it is restricted by the availability of both the trainer and the instrument, often leading to long waiting times and potentially many weeks between sessions. To master the range and complexity of skills needed for cryoEM, trainees need to practice repeatedly to accumulate sufficient ‘mileage’ to achieve skill mastery. Any significant time away from the instruments requires additional time to refresh the skills. Inexperienced trainees, when practicing without the close supervision of experts, may make ‘rookie’ mistakes that can lead to instrument damage, further constraining the training and data-collection capacity of cryoEM facilities. For example, dropping an unsecured grid or clip ring into the stage or column from a side-entry holder can result in a downtime of 1–2 weeks, which not only impacts instrument availability but also incurs expensive service costs beyond warranty. The availability and cost of occupying cryoEM instruments and the trainer time and risk of damaging the instruments often render these training tasks a bottleneck in the overall training of new users. The ongoing COVID-19 pandemic has also unexpectedly further reduced access to cryoEM facilities and increased the health risks of one-on-one training.</p>
    <p>Building on the knowledge of VR in effective training of procedure knowledge and its potential for training operations of devices with limited access, such as the sample-preparation devices and electron microscopes involved in cryoEM, we have thus developed a VR-based interactive training system, <italic toggle="yes">CryoVR</italic>. The significance of <italic toggle="yes">CryoVR</italic> lies in its ability to provide a virtual, engaging, self-paced hands-on training environment for users to familiarize themselves with the instrument and operation protocols before accessing the physical instruments. It not only can serve as an effective pre-training system for new users, but also can help an infrequent user to warm up their skills before returning to real operations on cryoEM instruments. Virtual mastery of operational procedures will not only significantly reduce the per-trainee time necessary for physical instrument and trainer supervision, but also should be very effective in reducing ‘rookie’ errors that might damage the physical instruments.</p>
    <p>Due to the complexity and interdisciplinary nature of cryoEM, the learning curve for cryoEM is steep and training is time- and resource-demanding. A typical traditional training pipeline can be roughly divided into three stages (Fig. 1<xref rid="fig1" ref-type="fig"> ▸</xref>): a trainee can start self-learning by going through the literature or taking a course to learn the theory and protocols for each of the specific tasks. There are already many excellent resources for users to go through these training steps. These teaching materials are exhaustive but have an inadequate level of interaction from the learner side. In our training system, <italic toggle="yes">CryoVR</italic>, the users can benefit from the immersive VR environment and the gamification features to improve their learning efficiency.</p>
    <p>It should be pointed out that cryoEM tasks consist of two major types of tasks: hands-on operations and computer screen-focused operations (Nogales &amp; Scheres, 2015<xref rid="bb32" ref-type="bibr"> ▸</xref>). For example, the loading/unloading of sample grids to/from the autoloader of a Titan Krios CryoTEM primarily uses hands-on operations, while the instrument alignment and automated data collection using software such as <italic toggle="yes">Leginon</italic>, <italic toggle="yes">SerialEM</italic> or <italic toggle="yes">EPU</italic>
<italic toggle="yes">etc.</italic> are primarily performed through the mouse, keyboard and computer screens. Only the hands-on operations are well suited for <italic toggle="yes">CryoVR</italic>, while computer screen-focused interactions are more suited by other training approaches that are beyond the scope of <italic toggle="yes">CryoVR</italic>. As an exception, a small amount of computer-screen interactions, for example for the Vitrobot, can be simulated well by <italic toggle="yes">CryoVR</italic>. We would like to emphasize that <italic toggle="yes">CryoVR</italic> will augment but not replace traditional training approaches. In the VR training pipeline (Fig. 1<xref rid="fig1" ref-type="fig"> ▸</xref>), <italic toggle="yes">CryoVR</italic> is intended as a pre-training step to familiarize trainees with the instruments and operational procedures using the virtual instruments in <italic toggle="yes">CryoVR</italic> before starting actual hands-on training on the physical instruments in the cryoEM facility. By pre-training on the virtual instruments and achieving virtual mastery of the operation procedure, trainees are expected to need significantly fewer orientation sessions on the instrument and can instead focus on polishing the fine touches of manipulating the physical objects that current VR technology is not able to faithfully reproduce. The final stage of practice on physical instruments will still be essential, but <italic toggle="yes">CryoVR</italic> pre-training will help to significantly reduce the amount of time needed. This pre-training provides multiple benefits to the trainees, including faster completion of training and less expense to cover the instrument usage and trainer time. For trainers and cryoEM facilities, the benefits include an increased training capacity to grow the skilled workforce, more instrument time for data collection and potentially less damage and downtime of the instruments caused by ‘rookie’ errors. Of course, the final certification by the trainer for independent operation of the physical instruments will continue to be performed on the physical instruments. Considering the strengths and limitations of VR training, we set the following learning objectives for <italic toggle="yes">CryoVR</italic>.<list list-type="simple" id="l1"><list-item><p>(i) Demonstrate familiarity with the sequence of operations needed for major cryoEM instruments prior to one-on-one hands-on training on physical instruments.</p></list-item><list-item><p>(ii) Demonstrate a reduced training time on physical instruments needed to obtain approval from the trainer for the independent operation of cryoEM instruments.</p></list-item><list-item><p>(iii) Demonstrate a conceptual understanding of safety procedures to avoid key errors and know how to respond appropriately in the event of errors.</p></list-item></list>
</p>
  </sec>
  <sec id="sec4">
    <label>4.</label>
    <title><italic toggle="yes">CryoVR</italic> modules and features</title>
    <sec id="sec4.1">
      <label>4.1.</label>
      <title>Training modules</title>
      <p>Currently, <italic toggle="yes">CryoVR</italic> covers the early-stage tasks of a single-particle cryoEM project: from grid glow discharging, plunge-freezing and clipping the autogrids to loading the grids into the TEM column. Each of the tasks is implemented in a separate module that can be individually installed and played. The executables of the <italic toggle="yes">CryoVR</italic> modules are freely available at <ext-link xlink:href="https://www.purdue.edu/CryoVR" ext-link-type="uri">https://www.purdue.edu/CryoVR</ext-link> and the source code and models are freely available at <ext-link xlink:href="https://github.com/CryoVR" ext-link-type="uri">https://github.com/CryoVR</ext-link>.</p>
      <sec id="sec4.1.1">
        <label>4.1.1.</label>
        <title>Grid Glow Discharging</title>
        <p>This module simulates glow discharging of cryoEM grids, which is frequently used to remove surface contamination and render the grid surface hydrophilic before samples are applied and plunge-frozen (Fig. 2<xref rid="fig2" ref-type="fig"> ▸</xref>
<italic toggle="yes">a</italic>).</p>
      </sec>
      <sec id="sec4.1.2">
        <label>4.1.2.</label>
        <title>Plunge Freezing</title>
        <p>We have created two modules to simulate two plunger-freezing devices for sample grid preparation: the ThermoFisher Scientific (TFS) Vitrobot Mark IV (Fig. 2<xref rid="fig2" ref-type="fig"> ▸</xref>
<italic toggle="yes">b</italic>) and the Gatan CP3 (Fig. 2<xref rid="fig2" ref-type="fig"> ▸</xref>
<italic toggle="yes">c</italic>).</p>
      </sec>
      <sec id="sec4.1.3">
        <label>4.1.3.</label>
        <title>Autogrid Clipping</title>
        <p>This module simulates how TFS autogrids are assembled in a dedicated workstation by seating cryoEM grids over a clip ring and then locking the grid and clip ring together with a C-shaped spring or C-clip (Fig. 2<xref rid="fig2" ref-type="fig"> ▸</xref>
<italic toggle="yes">d</italic>).</p>
      </sec>
      <sec id="sec4.1.4">
        <label>4.1.4.</label>
        <title>Autogrid Loading</title>
        <p>This module teaches the users how to insert the frozen autogrids into the autoloader cassette and then load the nanocab with the cassette into the autoloader of the cryoTEM (Fig. 2<xref rid="fig2" ref-type="fig"> ▸</xref>
<italic toggle="yes">e</italic>).</p>
      </sec>
      <sec id="d5e489">
        <p>The virtual environment of these <italic toggle="yes">CryoVR</italic> modules simulates a real laboratory environment. In <italic toggle="yes">CryoVR</italic>, the user can walk around and look in arbitrary directions. At the start of each module, the user is positioned at the center of the virtual laboratory and faces the target cryoEM device of the module. In general, each module contains a workbench, the device related to the training and all necessary tools during the training procedure.</p>
      </sec>
    </sec>
    <sec id="sec4.2">
      <label>4.2.</label>
      <title>Training modes</title>
      <sec id="sec4.2.1">
        <label>4.2.1.</label>
        <title>Tutorial mode</title>
        <p>For those who are not familiar with experimental procedures, a Tutorial mode with fully guided text, sound and visual instructions is both useful and necessary. The instructions are designed to clearly demonstrate the detailed operation steps (Fig. 3<xref rid="fig3" ref-type="fig"> ▸</xref>), as well as showing potential mistakes that users may make during their experiments. As a result, users will gain comprehensive experience during VR training that will carry forward to training on the physical instruments. Additionally, there are video clips of expert operations embedded in <italic toggle="yes">CryoVR</italic> for cross-referencing. These modules can record a the actions of a user and play back the recordings to help users improve their learning retention and become aware of mistakes that they may have made.</p>
      </sec>
      <sec id="sec4.2.2">
        <label>4.2.2.</label>
        <title>Exam mode</title>
        <p>We aim at conferring the users of <italic toggle="yes">CryoVR</italic> modules with sufficient skills necessary for the use of the instruments and their operational procedures to reduce the time needed for their final on-site training on physical instruments. To assess learning outcomes, a final examination will be critical to evaluate user skills. In our survey, a majority of respondents (74.3%) considered the <italic toggle="yes">CryoVR</italic> Exam mode important. Currently, we score the user’s operations against a standard operation procedure, with each operation step specified by the instrument parts or ancillary devices to be operated, the starting/ending positions, allowable time <italic toggle="yes">etc</italic>. In practice, some alternative operations or sequences of operations in a procedure can also be acceptable for a real experiment. We have consulted with cryoEM operation experts to incorporate acceptable variations in <italic toggle="yes">CryoVR</italic> scoring criteria. To ensure mastery and avoid future mistakes on physical instruments, we require a perfect score (100 points) in the Exam mode to complete a <italic toggle="yes">CryoVR</italic> training module. We have found that most novice users could pass the Exam mode after just a few passes through the modules.</p>
      </sec>
    </sec>
    <sec id="sec4.3">
      <label>4.3.</label>
      <title>Standard operation procedures (SOPs)</title>
      <p>To better coordinate the training efforts among the NIH-funded national CryoEM centers and curriculum developers (with <italic toggle="yes">CryoVR</italic> being one of the curriculum developers), these teams are currently collaborating to establish SOPs in the different cryoEM tasks which <italic toggle="yes">CryoVR</italic> has implemented in the corresponding modules. At the same time, a merit badge system is designed to certify the training outcome that will be honored across the centers. In <italic toggle="yes">CryoVR</italic>, once the user passes the Exam mode of the certain module, a certificate will be issued to the user, recorded in our online database and can be verified by trainers.</p>
    </sec>
    <sec id="sec4.4">
      <label>4.4.</label>
      <title>Safety training</title>
      <p>In any laboratory training, safety is a top priority. We note the importance of including the same safety training within our <italic toggle="yes">CryoVR</italic> modules. There are multiple potential safety risks related to the use of cryoEM instruments and operations, for example, fire hazard from ethane gas, cold burns from liquid nitrogen <italic toggle="yes">etc</italic>. We consider VR an ideal environment for new users to familiarize themselves with potential dangers in using instruments and reagents without risking harm to themselves or others or damaging expensive equipment and samples. In <italic toggle="yes">CryoVR</italic>, we include both general laboratory safety and cryoEM-specific safety to ensure that the users understand risks, know how to avoid risks and know how to properly respond to these risks to protect themselves and the equipment. We have developed the relevant models and instructed users to begin all laboratory procedures by putting on gloves, a laboratory coat and safety goggles. By reinforcing this behavior in VR, we expect users to always think about safety and protection during real-world operations. As the VR game engine can return all necessary data, including time, position, rotation and status, it is possible not only to detect whether users follow the safety procedures but also to simulate the disastrous consequences of missing the safety procedures, for example, a fire will be caused if the user has forgotten to close the valve of the ethane gas tank during a plunge-freezing VR session. The dramatic virtual experience of disasters in <italic toggle="yes">CryoVR</italic> should help ensure the users to consciously follow the safety procedures and avoid safety accidents in real-world operations.</p>
    </sec>
    <sec id="sec4.5">
      <label>4.5.</label>
      <title>Supported VR platforms</title>
      <p><italic toggle="yes">CryoVR</italic> supports major VR devices such as the HTC Vive series (Borges <italic toggle="yes">et al.</italic>, 2018<xref rid="bb7" ref-type="bibr"> ▸</xref>) and Oculus Quest series (<ext-link xlink:href="https://www.oculus.com/compare/" ext-link-type="uri">https://www.oculus.com/compare/</ext-link>). The user will use two controllers of these VR devices as their virtual hands to interact with the virtual objects (Fig. 4<xref rid="fig4" ref-type="fig"> ▸</xref>). We recommend using the high-end devices from these series, for example HTC Vive Pro or equivalent, due to their superior performance at relatively affordable cost ($499–1199). Accurate tracking, high video resolution (2800 × 1600) and refresh rate (≥72 Hz) and a wide field of view (110°) are all crucial to ensure the realism and accuracy of VR training (Niehorster <italic toggle="yes">et al.</italic>, 2017<xref rid="bb31" ref-type="bibr"> ▸</xref>). With its flexibility and economy, the Oculus Quest series is another good option. We have developed specific <italic toggle="yes">CryoVR</italic> versions that perform better with an Oculus platform. These high-end consumer-level products have been widely used in serious business applications, such as hazardous situation training and medical surgery training.</p>
    </sec>
  </sec>
  <sec id="sec5">
    <label>5.</label>
    <title>Design considerations of <italic toggle="yes">CryoVR</italic>
</title>
    <p>Presence and realism are key factors that determine whether a user can have a satisfactory experience in a virtual environment and are vital to the success of VR training (Hays &amp; Singer, 1989<xref rid="bb19" ref-type="bibr"> ▸</xref>; Bowman &amp; McMahan, 2007<xref rid="bb8" ref-type="bibr"> ▸</xref>). Realistic surroundings and interactions with virtual objects are especially important elements when designing VR applications for higher education (Radianti <italic toggle="yes">et al.</italic>, 2020<xref rid="bb34" ref-type="bibr"> ▸</xref>). ‘Immersion’ and ‘presence’ objectively measure a simulation environment. There are many key factors related to hardware, for example, a wide field of view, short reaction latency and high display resolution and refresh rate (Bowman &amp; McMahan, 2007<xref rid="bb8" ref-type="bibr"> ▸</xref>; Hoffman <italic toggle="yes">et al.</italic>, 2006<xref rid="bb20" ref-type="bibr"> ▸</xref>; Meehan <italic toggle="yes">et al.</italic>, 2003<xref rid="bb29" ref-type="bibr"> ▸</xref>; McMahan <italic toggle="yes">et al.</italic>, 2012<xref rid="bb28" ref-type="bibr"> ▸</xref>). Researchers and hardware manufacturers have devoted huge efforts to improving VR devices and have made consumer-level VR headsets and controllers suitable for simulating daily physical activities (Borges <italic toggle="yes">et al.</italic>, 2018<xref rid="bb7" ref-type="bibr"> ▸</xref>; Desai <italic toggle="yes">et al.</italic>, 2014<xref rid="bb12" ref-type="bibr"> ▸</xref>). While <italic toggle="yes">CryoVR</italic> cannot improve hardware, we have made a great effort on the design and software-development side to make simulations high fidelity. More specifically, we focused on perceptual (for example visual and auditory) fidelity and interaction fidelity, as these significantly affect performance and subjective judgments of engagement and usability (McMahan <italic toggle="yes">et al.</italic>, 2012<xref rid="bb28" ref-type="bibr"> ▸</xref>; Rogers <italic toggle="yes">et al.</italic>, 2019<xref rid="bb36" ref-type="bibr"> ▸</xref>).</p>
    <sec id="sec5.1">
      <label>5.1.</label>
      <title>Accurate hand gestures for interactions with devices</title>
      <p>In cryoEM, there are some devices with complicated structures that require specific hand gestures to handle them in real life. To make the virtual operations realistic and to educate trainees in the proper way of handling an object or instrument, we carefully examined the standard modes of manipulating various objects and implemented corresponding hand gestures for each instrument (Fig. 5<xref rid="fig5" ref-type="fig"> ▸</xref>). These hand gestures largely cover various forms of grabbing, twisting, pressing and holding various objects. When a user’s virtual hand is close to the object, the hand gestures in VR will automatically change to the matching gesture at an appropriate pose to interact with the object. These gestures help to provide more intuitive interactions in <italic toggle="yes">CryoVR</italic>.</p>
      <p>We conducted a human subject study to evaluate how <italic toggle="yes">CryoVR</italic> users react to virtual hand gestures in <italic toggle="yes">CryoVR</italic> using subjects outside the bioscience domain. There could potentially be some extent of cognitive conflicts between the user’s real hand gestures (always holding controllers with both hands) and their perceived hand gestures in VR (for example twisting a handwheel to open an ethane gas tank or picking up a grid using tweezers). In our study, the users did not report such conflict or inconsistency between their real hand gestures and perceived hand gestures. They feel the virtual hand natural as they are working with specific gestures. We then asked the users to pick up real devices (pipettes and tweezers) and to use them. Although none of these nonbiology users had ever used or even seen these devices in the real world, they could subconsciously properly use these devices. Apparently, the sense of perception can override feelings from their hands, and what was seen in VR influenced the trainee in the real world.</p>
    </sec>
    <sec id="sec5.2">
      <label>5.2.</label>
      <title>Visual and auditory fidelity</title>
      <p>Visual realism is crucial for a user to feel realism and immersion in a VR environment since visual realism enhances realistic responses (Slater <italic toggle="yes">et al.</italic>, 2009<xref rid="bb38" ref-type="bibr"> ▸</xref>). Also, operation and background sound in a VR scene can enhance a user’s sense of presence and help them recognize operation sequences and risks (Lu &amp; Davis, 2016<xref rid="bb25" ref-type="bibr"> ▸</xref>). To achieve high fidelity of cryoEM operations, we first focused on the perception of visual and auditory fidelity. For auditory fidelity, we have embedded the recorded sound of physical instruments in <italic toggle="yes">CryoVR</italic> and play the recordings during the corresponding virtual operations. For visual fidelity, we should not only make the virtual devices and laboratory environment look real, but must also take special care of small objects that are often involved in cryoEM operations, for example holding a grid with tweezers. To achieve realism, we rendered high-level details for these objects in close-up views. However, when the user works on other tasks, these details of small objects will be unnecessary and could waste computing resources that may hinder fluency. We thus make <italic toggle="yes">CryoVR</italic> adaptively display the level of detail of the virtual objects to ensure both visual fidelity and high fluency.</p>
    </sec>
  </sec>
  <sec id="sec6">
    <label>6.</label>
    <title>Dissemination and usability test</title>
    <p>To determine the usability and effectiveness of <italic toggle="yes">CryoVR</italic>, we conducted several rounds of usability testing. Usability testing has proved be one of the main methods to evaluate virtual reality applications (Fussell <italic toggle="yes">et al.</italic>, 2019<xref rid="bb16" ref-type="bibr"> ▸</xref>). With the opinions from cryoEM device operators, we can measure the influence of <italic toggle="yes">CryoVR</italic> in cryoEM training procedures. We can also detect usability problems in <italic toggle="yes">CryoVR</italic> to propose improvements and solutions that optimize the user experience.</p>
    <p>In March 2022, the National Center of CryoEM Access and Training (NCCAT) held a two-day workshop on cryoEM operation. 16 biological researchers participated in the workshop. We were invited to demonstrate <italic toggle="yes">CryoVR</italic> in the workshop. We invited these 16 participants to experience the four single-particle cryoEM VR training modules. We then interviewed and collected their opinions about <italic toggle="yes">CryoVR</italic>. These questions are from the IGROUP questionnaire template. Participants gave very positive feedback (Table 1<xref rid="table1" ref-type="table"> ▸</xref>). The results show the fact that users are actively engaged in operating cryoEM devices in the virtual environment (Q3, 4.5). They feel the devices are realistic (Q1, 3.89) and feel presence in a cryoEM laboratory (Q2, 3.56). The instructions shown in the VR scenes are clear and effective. They agree that <italic toggle="yes">CryoVR</italic> is an effective training tool (Q4, 4.03; Q5, 4.56), are willing to use <italic toggle="yes">CryoVR</italic> in their future training and will recommend <italic toggle="yes">CryoVR</italic> to other people (Q7, 4.06; Q8, 4.61).</p>
    <p>Due to the limitations of the hardware (VR goggles and controllers), the appearance of cryoEM devices in VR still has room for improvement (Q1 and Q2).</p>
    <p><italic toggle="yes">CryoVR</italic> has been released to the community for anyone to freely download from <ext-link xlink:href="https://www.purdue.edu/cryoVR" ext-link-type="uri">https://www.purdue.edu/cryoVR</ext-link>. The <italic toggle="yes">CryoVR</italic> website also includes additional information such as VR hardware/software requirements, installation instruments, source-code availability on GitHub and a gallery of videos of <italic toggle="yes">CryoVR</italic> modules <italic toggle="yes">etc</italic>. Over the past few years of developing <italic toggle="yes">CryoVR</italic>, we have also had multiple chances to disseminate our work to the cryoEM community through training workshops at a variety of cryoEM events and conferences. We have received much constructive feedback from the users:<disp-quote><p>The VR training system was really well programmed and easy to navigate.</p><p>I feel like overall it is a pretty good way to explain a procedure that I had no idea what it was for. I think that the technology was consistent and easy to use, I had no troubles following the on screen directions and the indicator lights that showed where to place things. I imagine that it is also a lot more attention grabbing than a training video, simulates the ‘hands on experience’.</p></disp-quote>
</p>
    <p>To assess the value to students with limited experience of cryo-EM, we disseminated our HTC Vive version of the <italic toggle="yes">CryoVR</italic> Training mode and Exam mode to students in the cryoEM class and collected their feedback. Except for minor bugs, their comments were generally positive and they agreed that it is a good preview of the procedures, especially for students without any hands-on cryo-EM experience. Here are some of the comments from them:<disp-quote><p>Overall, it is good and can [be] used for educational purpose especially for the new users like us.</p><p>This is a helpful preview for all the procedures before [we] practice it on the real instrument.</p></disp-quote>
</p>
  </sec>
  <sec id="sec7">
    <label>7.</label>
    <title>Conclusion</title>
    <p>Our <italic toggle="yes">CryoVR</italic> training system provides a unique cryoEM training experience of learning by doing in a virtual environment to acquaint users with instrument interfaces, operation procedures and manipulation skills before starting hands-on training with physical cryoEM instruments. It enables multi-modal learning by integrating text, visual and audio cues and other training materials such as relevant video clips of instruments and SOPs to create a holistic training approach. It is a fun and engaging experience that incentivizes users to practice more. It is a self-paced training system in a VR environment that users can use to practice anywhere and anytime rather than having to be in the facility and relying on limited trainer and instrument availability. This capability is particularly useful in situations where in-person contact is severely limited, as in the current COVID-19 pandemic. Furthermore, it simulates hazard consequences of incorrect operations to allow trainees to virtually experience dangerous situations from a first-person perspective, and trains users to properly handle and recover from a hazardous situation.</p>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="bb3">
      <mixed-citation publication-type="other">Abrahamson, D. (2014). <italic toggle="yes">Int. J. Child Comput. Interact.</italic>
<bold>2</bold>, 1–16.</mixed-citation>
    </ref>
    <ref id="bb4">
      <mixed-citation publication-type="other">Alewijnse, B., Ashton, A. W., Chambers, M. G., Chen, S., Cheng, A., Ebrahim, M., Eng, E., Hagen, W. J. H., Koster, A. J., López, C. S., Lukoyanova, N., Ortega, J., Renault, L., Reyntjens, S., Rice, W. J., Scapin, G., Schrijver, R., Siebert, A., Stagg, S. M., Grum-Tokars, V., Wright, E. R., Wu, S., Yu, Z., Zhou, Z. H., Carragher, B. &amp; Potter, C. S. (2017). <italic toggle="yes">J. Struct. Biol.</italic>
<bold>199</bold>, 225–236.</mixed-citation>
    </ref>
    <ref id="bb5">
      <mixed-citation publication-type="other">Anderson, J. R. (1982). <italic toggle="yes">Psychol. Rev.</italic>
<bold>89</bold>, 369–406.</mixed-citation>
    </ref>
    <ref id="bb6">
      <mixed-citation publication-type="other">Andrade, A., Danish, J. A. &amp; Maltese, A. V. (2017). <italic toggle="yes">J. Learn. Anal.</italic>
<bold>4</bold>, 18–46.</mixed-citation>
    </ref>
    <ref id="bb7">
      <mixed-citation publication-type="other">Borges, M., Symington, A., Coltin, B., Smith, T. &amp; Ventura, R. (2018). <italic toggle="yes">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</italic>, pp. 2610–2615. Piscataway: IEEE.</mixed-citation>
    </ref>
    <ref id="bb8">
      <mixed-citation publication-type="other">Bowman, D. A. &amp; McMahan, R. P. (2007). <italic toggle="yes">Computer</italic>, <bold>40</bold>, 36–43.</mixed-citation>
    </ref>
    <ref id="bb9">
      <mixed-citation publication-type="other">Callaway, E. (2015). <italic toggle="yes">Nature</italic>, <bold>525</bold>, 172–174.</mixed-citation>
    </ref>
    <ref id="bb10">
      <mixed-citation publication-type="other">Carlson, P., Peters, A., Gilbert, S. B., Vance, J. M. &amp; Luse, A. (2015). <italic toggle="yes">IEEE Trans. Vis. Comput. Graph.</italic>
<bold>21</bold>, 770–782.</mixed-citation>
    </ref>
    <ref id="bb11">
      <mixed-citation publication-type="other">Dale, E. (1969). <italic toggle="yes">Audiovisual Methods in Teaching</italic>, 3rd ed. New York: Dryden Press.</mixed-citation>
    </ref>
    <ref id="bb12">
      <mixed-citation publication-type="other">Desai, P. R., Desai, P. N., Ajmera, K. D. &amp; Mehta, K. (2014). <italic toggle="yes">arXiv</italic>:1408.1173.</mixed-citation>
    </ref>
    <ref id="bb13">
      <mixed-citation publication-type="other">Eng, E., Cheng, A., Wei, H., Aragon, M., Castello, C., Kopylov, E., Potter, C. &amp; Carragher, B. (2020). <italic toggle="yes">Microsc. Microanal.</italic>
<bold>26</bold>, 324–325.</mixed-citation>
    </ref>
    <ref id="bb14">
      <mixed-citation publication-type="other">Enyedy, N., Danish, J. A. &amp; DeLiema, D. (2015). <italic toggle="yes">Int. J. Comput. Support. Collab. Learn.</italic>
<bold>10</bold>, 7–34.</mixed-citation>
    </ref>
    <ref id="bb15">
      <mixed-citation publication-type="other">Foglia, L. &amp; Wilson, R. A. (2013). <italic toggle="yes">Wiley Interdiscip. Rev. Cogn. Sci.</italic>
<bold>4</bold>, 319–325.</mixed-citation>
    </ref>
    <ref id="bb16">
      <mixed-citation publication-type="other">Fussell, S. G., Derby, J. L., Smith, J. K., Shelstad, W. J., Benedict, J. D., Chaparro, B. S., Thomas, R. &amp; Dattel, A. R. (2019). <italic toggle="yes">Proc. Hum. Fact. Ergon. Soc. Annu. Meet.</italic>
<bold>63</bold>, 2303–2307.</mixed-citation>
    </ref>
    <ref id="bb17">
      <mixed-citation publication-type="other">Ganier, F., Hoareau, C. &amp; Tisseau, J. (2014). <italic toggle="yes">Ergonomics</italic>, <bold>57</bold>, 828–843.</mixed-citation>
    </ref>
    <ref id="bb18">
      <mixed-citation publication-type="other">Gonzalez, B., Zhang, J., Han, G., Zhang, J., Dong, J., Watson, W., Sors, T., Chen, V. &amp; Jiang, W. (2019). <italic toggle="yes">Acta Cryst.</italic> A<bold>75</bold>, a66.</mixed-citation>
    </ref>
    <ref id="bb19">
      <mixed-citation publication-type="other">Hays, R. T. &amp; Singer, M. J. (1989). <italic toggle="yes">Simulation Fidelity in Training System Design: Bridging the Gap Between Reality and Training</italic>, edited by R. T. Hays &amp; M. J. Singer, pp. 47–75. New York: Springer.</mixed-citation>
    </ref>
    <ref id="bb20">
      <mixed-citation publication-type="other">Hoffman, H. G., Seibel, E. J., Richards, T. L., Furness, T. A., Patterson, D. R. &amp; Sharar, S. R. (2006). <italic toggle="yes">J. Pain</italic>, <bold>7</bold>, 843–850.</mixed-citation>
    </ref>
    <ref id="bb21">
      <mixed-citation publication-type="other">Jensen, L. &amp; Konradsen, F. (2018). <italic toggle="yes">Educ. Inf. Technol.</italic>
<bold>23</bold>, 1515–1529.</mixed-citation>
    </ref>
    <ref id="bb22">
      <mixed-citation publication-type="other">Jiang, W. &amp; Tang, L. (2017). <italic toggle="yes">Curr. Opin. Struct. Biol.</italic>
<bold>46</bold>, 122–129.</mixed-citation>
    </ref>
    <ref id="bb23">
      <mixed-citation publication-type="other">Kosmas, P., Ioannou, A. &amp; Zaphiris, P. (2019). <italic toggle="yes">Educ. Media Int.</italic>
<bold>56</bold>, 59–74.</mixed-citation>
    </ref>
    <ref id="bb24">
      <mixed-citation publication-type="other">Lam, C. K., Sundaraj, K. &amp; Sulaiman, M. N. (2013). <italic toggle="yes">Procedia Comput. Sci.</italic>
<bold>18</bold>, 742–748.</mixed-citation>
    </ref>
    <ref id="bb25">
      <mixed-citation publication-type="other">Lu, X. &amp; Davis, S. (2016). <italic toggle="yes">Saf. Sci.</italic>
<bold>86</bold>, 184–194.</mixed-citation>
    </ref>
    <ref id="bb26">
      <mixed-citation publication-type="other">Lyumkis, D. (2019). <italic toggle="yes">J. Biol. Chem.</italic>
<bold>294</bold>, 5181–5197.</mixed-citation>
    </ref>
    <ref id="bb27">
      <mixed-citation publication-type="other">Macedonia, M., Müller, K. &amp; Friederici, A. D. (2011). <italic toggle="yes">Hum. Brain Mapp.</italic>
<bold>32</bold>, 982–998.</mixed-citation>
    </ref>
    <ref id="bb28">
      <mixed-citation publication-type="other">McMahan, R. P., Bowman, D. A., Zielinski, D. J. &amp; Brady, R. B. (2012). <italic toggle="yes">IEEE Trans. Vis. Comput. Graph.</italic>
<bold>18</bold>, 626–633.</mixed-citation>
    </ref>
    <ref id="bb29">
      <mixed-citation publication-type="other">Meehan, M., Razzaque, S., Whitton, M. C. &amp; Brooks, F. P. (2003). <italic toggle="yes">IEEE Virtual Reality 2003. Proceedings</italic>, pp. 141–148. Piscataway: IEEE.</mixed-citation>
    </ref>
    <ref id="bb30">
      <mixed-citation publication-type="other">Nguyen, D. J. &amp; Larson, J. B. (2015). <italic toggle="yes">Innov. High. Educ.</italic>
<bold>40</bold>, 331–344.</mixed-citation>
    </ref>
    <ref id="bb31">
      <mixed-citation publication-type="other">Niehorster, D. C., Li, L. &amp; Lappe, M. (2017). <italic toggle="yes">i-Perception</italic>, <bold>8</bold>, 2041669517708205.</mixed-citation>
    </ref>
    <ref id="bb32">
      <mixed-citation publication-type="other">Nogales, E. &amp; Scheres, S. H. W. (2015). <italic toggle="yes">Mol. Cell</italic>, <bold>58</bold>, 677–689.</mixed-citation>
    </ref>
    <ref id="bb33">
      <mixed-citation publication-type="other">Price, S., Roussos, G., Falcão, T. P. &amp; Sheridan, J. G. (2009). <italic toggle="yes">Technology and Embodiment: Relationships and Implications for Knowledge, Creativity and Communication</italic>. London: Futurelab/DCSF.</mixed-citation>
    </ref>
    <ref id="bb34">
      <mixed-citation publication-type="other">Radianti, J., Majchrzak, T. A., Fromm, J. &amp; Wohlgenannt, I. (2020). <italic toggle="yes">Comput. Educ.</italic>
<bold>147</bold>, 103778.</mixed-citation>
    </ref>
    <ref id="bb35">
      <mixed-citation publication-type="other">Repetto, C., Cipresso, P. &amp; Riva, G. (2015). <italic toggle="yes">Front. Psychol.</italic>
<bold>6</bold>, 176.</mixed-citation>
    </ref>
    <ref id="bb36">
      <mixed-citation publication-type="other">Rogers, K., Funke, J., Frommel, J., Stamm, S. &amp; Weber, M. (2019). <italic toggle="yes">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI ’19)</italic>, Paper 414. New York: ACM.</mixed-citation>
    </ref>
    <ref id="bb37">
      <mixed-citation publication-type="other">Sirohi, D., Chen, Z., Sun, L., Klose, T., Pierson, T. C., Rossmann, M. G. &amp; Kuhn, R. J. (2016). <italic toggle="yes">Science</italic>, <bold>352</bold>, 467–470.</mixed-citation>
    </ref>
    <ref id="bb38">
      <mixed-citation publication-type="other">Slater, M., Khanna, P., Mortensen, J. &amp; Yu, I. (2009). <italic toggle="yes">IEEE Comput. Graph. Appl.</italic>
<bold>29</bold>, 76–84.</mixed-citation>
    </ref>
    <ref id="bb39">
      <mixed-citation publication-type="other">Smith, S. J., Farra, S., Ulrich, D. L., Hodgson, E., Nicely, S. &amp; Matcham, W. (2016). <italic toggle="yes">Nurs. Educ. Perspect.</italic>
<bold>37</bold>, 210–214.</mixed-citation>
    </ref>
    <ref id="bb40">
      <mixed-citation publication-type="other">Wilson, M. (2002). <italic toggle="yes">Psychon. Bull. Rev.</italic>
<bold>9</bold>, 625–636.</mixed-citation>
    </ref>
    <ref id="bb41">
      <mixed-citation publication-type="other">Wrapp, D., Wang, N., Corbett, K. S., Goldsmith, J. A., Hsieh, C., Abiona, O., Graham, B. S. &amp; McLellan, J. S. (2020). <italic toggle="yes">Science</italic>, <bold>367</bold>, 1260–1263.</mixed-citation>
    </ref>
    <ref id="bb42">
      <mixed-citation publication-type="other">Yan, R., Zhang, Y., Li, Y., Xia, L., Guo, Y. &amp; Zhou, Q. (2020). <italic toggle="yes">Science</italic>, <bold>367</bold>, 1444–1448.</mixed-citation>
    </ref>
    <ref id="bb43">
      <mixed-citation publication-type="other">Yang, U. &amp; Kim, G. J. (2002). <italic toggle="yes">Presence</italic>, <bold>11</bold>, 304–323.</mixed-citation>
    </ref>
    <ref id="bb44">
      <mixed-citation publication-type="other">Yavrucuk, I., Kubali, E. &amp; Tarimci, O. (2011). <italic toggle="yes">IEEE Aerosp. Electron. Syst. Mag.</italic>
<bold>26</bold>, 10–14.</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="fig1">
    <label>Figure 1</label>
    <caption>
      <p>The position of <italic toggle="yes">CryoVR</italic> in the cryoEM training pipeline.</p>
    </caption>
    <graphic xlink:href="d-78-00903-fig1" position="float"/>
  </fig>
  <fig position="float" id="fig2">
    <label>Figure 2</label>
    <caption>
      <p><italic toggle="yes">CryoVR</italic> modules. (<italic toggle="yes">a</italic>) Grid Glow Discharging (easiGlow); (<italic toggle="yes">b</italic>, <italic toggle="yes">c</italic>) Plunge Freezing with TFS Vitrobot Mark IV (<italic toggle="yes">b</italic>) and Gatan CP3 (<italic toggle="yes">c</italic>); (<italic toggle="yes">d</italic>) TFS AutoGrid Clipping; (<italic toggle="yes">e</italic>) TFS AutoGrid Loading.</p>
    </caption>
    <graphic xlink:href="d-78-00903-fig2" position="float"/>
  </fig>
  <fig position="float" id="fig3">
    <label>Figure 3</label>
    <caption>
      <p>Tutorial mode of the Vitrobot training module in <italic toggle="yes">CryoVR</italic>. Voice, text/visual user interface and video instructions guide the user to learn the experimental procedures.</p>
    </caption>
    <graphic xlink:href="d-78-00903-fig3" position="float"/>
  </fig>
  <fig position="float" id="fig4">
    <label>Figure 4</label>
    <caption>
      <p>A user performing virtual plunge-freezing in <italic toggle="yes">CryoVR</italic> with an HTC Vive device.</p>
    </caption>
    <graphic xlink:href="d-78-00903-fig4" position="float"/>
  </fig>
  <fig position="float" id="fig5">
    <label>Figure 5</label>
    <caption>
      <p>Different hand gestures tailed to operations for different virtual objects in <italic toggle="yes">CryoVR</italic>.</p>
    </caption>
    <graphic xlink:href="d-78-00903-fig5" position="float"/>
  </fig>
  <table-wrap position="float" id="table1">
    <label>Table 1</label>
    <caption>
      <title>Average and SD scores of the IGROUP questionnaire</title>
    </caption>
    <table frame="hsides" rules="groups">
      <thead valign="top">
        <tr>
          <th style="border-bottom:1px solid black;" rowspan="1" colspan="1" align="left" valign="top">Question</th>
          <th style="border-bottom:1px solid black;" rowspan="1" colspan="1" align="left" valign="top">Average</th>
          <th style="border-bottom:1px solid black;" rowspan="1" colspan="1" align="left" valign="top">SD</th>
        </tr>
      </thead>
      <tbody valign="top">
        <tr>
          <td rowspan="1" colspan="1" align="left" valign="top">Q1. Realism for cryoEM devices</td>
          <td rowspan="1" colspan="1" align="left" valign="top">3.89</td>
          <td rowspan="1" colspan="1" align="left" valign="top">0.76</td>
        </tr>
        <tr>
          <td rowspan="1" colspan="1" align="left" valign="top">Q2. Sense of presence</td>
          <td rowspan="1" colspan="1" align="left" valign="top">3.56</td>
          <td rowspan="1" colspan="1" align="left" valign="top">0.92</td>
        </tr>
        <tr>
          <td rowspan="1" colspan="1" align="left" valign="top">Q3. Engagement</td>
          <td rowspan="1" colspan="1" align="left" valign="top">4.50</td>
          <td rowspan="1" colspan="1" align="left" valign="top">0.51</td>
        </tr>
        <tr>
          <td rowspan="1" colspan="1" align="left" valign="top">Q4. If VR experience can help hands-on training</td>
          <td rowspan="1" colspan="1" align="left" valign="top">4.03</td>
          <td rowspan="1" colspan="1" align="left" valign="top">0.85</td>
        </tr>
        <tr>
          <td rowspan="1" colspan="1" align="left" valign="top">Q5. The effectiveness of <italic toggle="yes">CryoVR</italic>
</td>
          <td rowspan="1" colspan="1" align="left" valign="top">4.56</td>
          <td rowspan="1" colspan="1" align="left" valign="top">0.62</td>
        </tr>
        <tr>
          <td rowspan="1" colspan="1" align="left" valign="top">Q6. Clarity of instructions</td>
          <td rowspan="1" colspan="1" align="left" valign="top">4.33</td>
          <td rowspan="1" colspan="1" align="left" valign="top">1.03</td>
        </tr>
        <tr>
          <td rowspan="1" colspan="1" align="left" valign="top">Q7. Willingness to use <italic toggle="yes">CryoVR</italic> in future training</td>
          <td rowspan="1" colspan="1" align="left" valign="top">4.06</td>
          <td rowspan="1" colspan="1" align="left" valign="top">0.87</td>
        </tr>
        <tr>
          <td rowspan="1" colspan="1" align="left" valign="top">Q8. Willingness to recommend <italic toggle="yes">CryoVR</italic> to others</td>
          <td rowspan="1" colspan="1" align="left" valign="top">4.61</td>
          <td rowspan="1" colspan="1" align="left" valign="top">0.50</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
