<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLOS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9591068</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0276609</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-22-24699</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Physiology</subject>
          <subj-group>
            <subject>Physiological Processes</subject>
            <subj-group>
              <subject>Molting</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Biochemistry</subject>
          <subj-group>
            <subject>Proteins</subject>
            <subj-group>
              <subject>Protein Interactions</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Neural Networks</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neural Networks</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Deep Learning</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Pharmacology</subject>
          <subj-group>
            <subject>Drug Interactions</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Chemical Characterization</subject>
          <subj-group>
            <subject>Binding Analysis</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Network Analysis</subject>
          <subj-group>
            <subject>Protein Interaction Networks</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Biochemistry</subject>
          <subj-group>
            <subject>Proteomics</subject>
            <subj-group>
              <subject>Protein Interaction Networks</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ICAN: Interpretable cross-attention network for identifying drug and target protein interactions</article-title>
      <alt-title alt-title-type="running-head">Interpretable cross-attention network for DTI</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4254-2214</contrib-id>
        <name>
          <surname>Kurata</surname>
          <given-names>Hiroyuki</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <xref rid="cor001" ref-type="corresp">*</xref>
        <xref rid="aff001" ref-type="aff"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tsukiyama</surname>
          <given-names>Sho</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <xref rid="aff001" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <addr-line>Department of Bioscience and Bioinformatics, Kyushu Institute of Technology, Iizuka, Fukuoka, Japan</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Hu</surname>
          <given-names>Lun</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Xinjiang Technical Institute of Physics and Chemistry, Chinese Academy of Sciences, CHINA</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>kurata@bio.kyutech.ac.jp</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>24</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>17</volume>
    <issue>10</issue>
    <elocation-id>e0276609</elocation-id>
    <history>
      <date date-type="received">
        <day>5</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>10</day>
        <month>10</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Kurata, Tsukiyama</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Kurata, Tsukiyama</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0276609.pdf"/>
    <abstract>
      <p>Drug–target protein interaction (DTI) identification is fundamental for drug discovery and drug repositioning, because therapeutic drugs act on disease-causing proteins. However, the DTI identification process often requires expensive and time-consuming tasks, including biological experiments involving large numbers of candidate compounds. Thus, a variety of computation approaches have been developed. Of the many approaches available, chemo-genomics feature-based methods have attracted considerable attention. These methods compute the feature descriptors of drugs and proteins as the input data to train machine and deep learning models to enable accurate prediction of unknown DTIs. In addition, attention-based learning methods have been proposed to identify and interpret DTI mechanisms. However, improvements are needed for enhancing prediction performance and DTI mechanism elucidation. To address these problems, we developed an attention-based method designated the interpretable cross-attention network (ICAN), which predicts DTIs using the Simplified Molecular Input Line Entry System of drugs and amino acid sequences of target proteins. We optimized the attention mechanism architecture by exploring the cross-attention or self-attention, attention layer depth, and selection of the context matrixes from the attention mechanism. We found that a plain attention mechanism that decodes drug-related protein context features without any protein-related drug context features effectively achieved high performance. The ICAN outperformed state-of-the-art methods in several metrics on the DAVIS dataset and first revealed with statistical significance that some weighted sites in the cross-attention weight matrix represent experimental binding sites, thus demonstrating the high interpretability of the results. The program is freely available at <ext-link xlink:href="https://github.com/kuratahiroyuki/ICAN" ext-link-type="uri">https://github.com/kuratahiroyuki/ICAN</ext-link>.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001691</institution-id>
            <institution>Japan Society for the Promotion of Science</institution>
          </institution-wrap>
        </funding-source>
        <award-id>22H03688</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4254-2214</contrib-id>
          <name>
            <surname>Kurata</surname>
            <given-names>Hiroyuki</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001691</institution-id>
            <institution>Japan Society for the Promotion of Science</institution>
          </institution-wrap>
        </funding-source>
        <award-id>22J22706</award-id>
        <principal-award-recipient>Sho Tsukiyama</principal-award-recipient>
      </award-group>
      <funding-statement>This work was supported by a Grant-in-Aid for Scientific Research (B) (22H03688) and partially supported by a Grant-in-Aid for JSPS Research Fellows (22J22706) from the Japan Society for the Promotion of Science (JSPS). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="7"/>
      <table-count count="3"/>
      <page-count count="20"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>The program is freely available at <ext-link xlink:href="https://github.com/kuratahiroyuki/ICAN" ext-link-type="uri">https://github.com/kuratahiroyuki/ICAN</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>The program is freely available at <ext-link xlink:href="https://github.com/kuratahiroyuki/ICAN" ext-link-type="uri">https://github.com/kuratahiroyuki/ICAN</ext-link>.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>Drug–target protein interaction (DTI) identification is fundamental for drug discovery and drug repositioning, because therapeutic drugs act on disease-causing proteins. However, the DTI identification process often requires expensive and time-consuming tasks, including biological experiments involving large numbers of candidate compounds [<xref rid="pone.0276609.ref001" ref-type="bibr">1</xref>]. To address these problems, intensive efforts have focused on virtual screening or computational prediction of DTIs based on large biological datasets and information available in public databases. A variety of computational approaches, including structure-based, similarity-based, and feature-based chemo-genomics methods, have been proposed for DTI prediction [<xref rid="pone.0276609.ref002" ref-type="bibr">2</xref>]. Conventional structure-based approaches, that include docking simulations have been studied for decades, but they are limited to proteins for which the three-dimensional structure is known or can be precisely reproduced by molecular dynamics simulations. Similarity-based approaches such as kernel regression [<xref rid="pone.0276609.ref003" ref-type="bibr">3</xref>–<xref rid="pone.0276609.ref005" ref-type="bibr">5</xref>] and matrix factorization [<xref rid="pone.0276609.ref006" ref-type="bibr">6</xref>–<xref rid="pone.0276609.ref008" ref-type="bibr">8</xref>] utilize known drug-target similarity information to infer new DTIs. However, these methods are not applicable to target proteins of different classes for which similarity information is lacking [<xref rid="pone.0276609.ref009" ref-type="bibr">9</xref>]. Feature-based methods used in chemo-genomics approaches compute the descriptors of drugs and proteins as the input data to train machine and deep learning models so that they can accurately predict unknown DTIs as classification models and the associated binding affinity as regression models, respectively. In the feature-based approaches of the classification models, drug compounds can be represented as types of fingerprints, including extended connected fingerprints (ECFPs) [<xref rid="pone.0276609.ref010" ref-type="bibr">10</xref>] and PubChem [<xref rid="pone.0276609.ref011" ref-type="bibr">11</xref>], one-dimensional (1D) sequences, and molecular graphs (traditionally called two-dimensional (2D) structures). Protein sequences are described as 1D sequences, which are converted into feature vectors using various descriptors, including composition transition distribution [<xref rid="pone.0276609.ref012" ref-type="bibr">12</xref>] and protein sequence composition (PSC) descriptors [<xref rid="pone.0276609.ref013" ref-type="bibr">13</xref>]. In general, machine learning methods, such as random forest and gradient boosting, can be combined with such descriptors to predict DTIs [<xref rid="pone.0276609.ref014" ref-type="bibr">14</xref>–<xref rid="pone.0276609.ref018" ref-type="bibr">18</xref>], where appropriate design and selection of descriptors are critically important [<xref rid="pone.0276609.ref019" ref-type="bibr">19</xref>,<xref rid="pone.0276609.ref020" ref-type="bibr">20</xref>]. From a different point of view, several recent studies have integrated heterogeneous, protein- or drug-related knowledge, such as GO semantic information [<xref rid="pone.0276609.ref021" ref-type="bibr">21</xref>], biological networks [<xref rid="pone.0276609.ref022" ref-type="bibr">22</xref>], and drug-drug interactions [<xref rid="pone.0276609.ref023" ref-type="bibr">23</xref>], into PPI datasets to improve PPI prediction performance.</p>
    <p>Recent studies have proposed various end-to-end, deep learning frameworks that integrated sequence representation and model training in a unified architecture [<xref rid="pone.0276609.ref024" ref-type="bibr">24</xref>,<xref rid="pone.0276609.ref025" ref-type="bibr">25</xref>]. Deep learning–based methods such as deep neural network [<xref rid="pone.0276609.ref026" ref-type="bibr">26</xref>], autoencoder [<xref rid="pone.0276609.ref027" ref-type="bibr">27</xref>], long short-term memory [<xref rid="pone.0276609.ref028" ref-type="bibr">28</xref>,<xref rid="pone.0276609.ref029" ref-type="bibr">29</xref>], convolutional neural networks (CNNs) [<xref rid="pone.0276609.ref030" ref-type="bibr">30</xref>,<xref rid="pone.0276609.ref031" ref-type="bibr">31</xref>], and Transformer [<xref rid="pone.0276609.ref013" ref-type="bibr">13</xref>] have improved the prediction performance of machine learning methods. DeepDTI [<xref rid="pone.0276609.ref009" ref-type="bibr">9</xref>] proposed a deep belief network [<xref rid="pone.0276609.ref032" ref-type="bibr">32</xref>] that uses ECFP2, ECFP4, and ECFP6 [<xref rid="pone.0276609.ref010" ref-type="bibr">10</xref>] to encode drugs and PSC descriptors to encode amino acids (AAs). DeepDTA [<xref rid="pone.0276609.ref030" ref-type="bibr">30</xref>] uses two CNNs that extract the features of the Simplified Molecular Input Line Entry System (SMILES) of drugs and AA sequences from their label-encoding vectors. DeepConv-DTI [<xref rid="pone.0276609.ref033" ref-type="bibr">33</xref>] involves an architecture similar to DeepDTA, which uses CNNs with max pooling layers and fingerprint ECFP4 to encode SMILES. From another aspect in which a drug compound is regarded as a molecular graph, GNN-CPI [<xref rid="pone.0276609.ref034" ref-type="bibr">34</xref>] and GraphDTA [<xref rid="pone.0276609.ref035" ref-type="bibr">35</xref>] have implemented graph neural networks (GNNs) [<xref rid="pone.0276609.ref036" ref-type="bibr">36</xref>,<xref rid="pone.0276609.ref037" ref-type="bibr">37</xref>] and graph CNNs [<xref rid="pone.0276609.ref038" ref-type="bibr">38</xref>,<xref rid="pone.0276609.ref039" ref-type="bibr">39</xref>], respectively. These predictors separately learn the molecular representations of drugs and proteins and then concatenate their features to calculate a final output, while neglecting the interactions of sub-sequences between drugs and proteins.</p>
    <p>Such interactions are widely considered by exploiting the attention mechanisms of Transformer, an encoder-decoder model consisting of multi-headed attention layers and pairwise feed-forward to extract sequence-to-sequence features. TransformerCPI [<xref rid="pone.0276609.ref013" ref-type="bibr">13</xref>] implemented a Transformer that applies drugs and proteins via a sequence-to-sequence or cross-attention mechanism. MolTrans [<xref rid="pone.0276609.ref040" ref-type="bibr">40</xref>] employs two self-attention mechanisms to create an interaction map that integrates the separately generated context features from drugs and proteins. These attention-based methods have suggested a few examples that roughly explain how a high value of attention weight matrixes correspond to experimental binding sites, but they do not prove this in an objective and statistical manner.</p>
    <p>To overcome these problems, we developed a new attention-based method that predicts DTIs based on SMILES of drugs and AA sequences of proteins; this method was designated the interpretable cross-attention network (ICAN). We optimized the attention mechanism architectures in terms of cross-attention or self-attention, attention layer depth, and selection of the context matrixes from the attention networks. ICAN outperformed state-of-the-art methods in several respects and first revealed with statistical significance that some weighted attention sites represent experimental binding sites, thus demonstrating the distinct interpretability of the method.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec002">
    <title>Materials and methods</title>
    <sec id="sec003">
      <title>Computation framework</title>
      <p>The proposed cross-attention model consists of three networks: an embedding layer, an attention layer, and an output layer, as shown in <xref rid="pone.0276609.g001" ref-type="fig">Fig 1</xref>. In the embedding layer, the SMILES of drugs and AA sequences of target proteins are separately encoded as the embedding matrixes. These matrixes are input into the cross-attention network to generate the drug-related context matrix of a protein and the protein-related context matrix of a drug. The resulting context matrixes are applied to the output layer of the CNN, which functions as a decoder to capture local feature patterns at different levels, followed by computing of a DTI probability score. The cross-attention considers sub-sequence interactions between a drug and a protein to produce the context matrixes; the CNN uses different filters to extract local sub-sequence patters within the context matrixes. Details in encoding methods and attention-based models are described in Tables <xref rid="pone.0276609.t001" ref-type="table">1</xref> and <xref rid="pone.0276609.t002" ref-type="table">2</xref>.</p>
      <fig position="float" id="pone.0276609.g001">
        <object-id pub-id-type="doi">10.1371/journal.pone.0276609.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>Architecture of the cross-attention-based neural network.</title>
          <p>It is the CA_DP network (<xref rid="pone.0276609.t002" ref-type="table">Table 2</xref>). Q, K, and V indicate Query, Key, and Value matrixes. AW: Attention-weight matrix, A: Attention matrix, D: Drug-context matrix, P: Protein-context matrix, d: Length of drug sequence, p: Length of protein sequence, h: Hidden dimension size.</p>
        </caption>
        <graphic xlink:href="pone.0276609.g001" position="float"/>
      </fig>
      <table-wrap position="float" id="pone.0276609.t001">
        <object-id pub-id-type="doi">10.1371/journal.pone.0276609.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Drug and protein encoding methods.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0276609.t001" id="pone.0276609.t001g" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Name</th>
                <th align="left" rowspan="1" colspan="1">Drug token</th>
                <th align="left" rowspan="1" colspan="1">Protein token</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">nn.Embedding of FCS</td>
                <td align="left" rowspan="1" colspan="1">SMILES FCS</td>
                <td align="left" rowspan="1" colspan="1">AA FCS</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">nn.Embedding of SMILES</td>
                <td align="left" rowspan="1" colspan="1">SMILES character</td>
                <td align="left" rowspan="1" colspan="1">AA letter</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">nn.Embedding of SELFIES</td>
                <td align="left" rowspan="1" colspan="1">SELFIES word</td>
                <td align="left" rowspan="1" colspan="1">AA letter</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">one-hot encoding of SMILES</td>
                <td align="left" rowspan="1" colspan="1">SMILES character</td>
                <td align="left" rowspan="1" colspan="1">AA letter</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">one-hot encoding of SELFIES</td>
                <td align="left" rowspan="1" colspan="1">SELFIES word</td>
                <td align="left" rowspan="1" colspan="1">AA letter</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t001fn001">
            <p>“Character” and “word” indicate “a letter or a symbol” and “minimum chemically valid characters”, respectively.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap position="float" id="pone.0276609.t002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0276609.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Architectures of different attention-based models.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0276609.t002" id="pone.0276609.t002g" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="justify" rowspan="1" colspan="1">Method name</th>
                <th align="justify" rowspan="1" colspan="1">Attention mechanism</th>
                <th align="justify" rowspan="1" colspan="1">Attention layer depth</th>
                <th align="justify" rowspan="1" colspan="1">Context matrix</th>
                <th align="justify" rowspan="1" colspan="1">Output layer</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">CA_DP</td>
                <td align="left" rowspan="1" colspan="1">Cross-attention</td>
                <td align="left" rowspan="1" colspan="1">1</td>
                <td align="left" rowspan="1" colspan="1">Drug+Protein</td>
                <td align="left" rowspan="1" colspan="1">CNN</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">CA2_DP</td>
                <td align="left" rowspan="1" colspan="1">Cross-attention</td>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">Drug+Protein</td>
                <td align="left" rowspan="1" colspan="1">CNN</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">CA3_DP</td>
                <td align="left" rowspan="1" colspan="1">Cross-attention</td>
                <td align="left" rowspan="1" colspan="1">3</td>
                <td align="left" rowspan="1" colspan="1">Drug+Protein</td>
                <td align="left" rowspan="1" colspan="1">CNN</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">CA_DP_FCL</td>
                <td align="left" rowspan="1" colspan="1">Cross-attention</td>
                <td align="left" rowspan="1" colspan="1">1</td>
                <td align="left" rowspan="1" colspan="1">Drug+Protein</td>
                <td align="left" rowspan="1" colspan="1">FCL</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">SA_DP</td>
                <td align="left" rowspan="1" colspan="1">Self-attention</td>
                <td align="left" rowspan="1" colspan="1">1</td>
                <td align="left" rowspan="1" colspan="1">Drug+Protein</td>
                <td align="left" rowspan="1" colspan="1">CNN</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">CA_D</td>
                <td align="left" rowspan="1" colspan="1">Cross-attention</td>
                <td align="left" rowspan="1" colspan="1">1</td>
                <td align="left" rowspan="1" colspan="1">Drug</td>
                <td align="left" rowspan="1" colspan="1">CNN</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">CA_P (ICAN)</td>
                <td align="left" rowspan="1" colspan="1">Cross-attention</td>
                <td align="left" rowspan="1" colspan="1">1</td>
                <td align="left" rowspan="1" colspan="1">Protein</td>
                <td align="left" rowspan="1" colspan="1">CNN</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t002fn001">
            <p>All of the models employ the nn.Embedding of FCS (<xref rid="pone.0276609.t001" ref-type="table">Table 1</xref>).</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="sec004">
      <title>DTI dataset</title>
      <p>The three widely used datasets were prepared: DAVIS, BindingDB, and BIOSNAP, as shown in <xref rid="pone.0276609.t003" ref-type="table">Table 3</xref>. These were the same datasets employed in a previous study [<xref rid="pone.0276609.ref040" ref-type="bibr">40</xref>] and facilitated direct comparison of our proposed method with existing methods. The DAVIS dataset consists of biochemically determined experimental Kd (dissociation constant) values for 68 drugs and 379 proteins [<xref rid="pone.0276609.ref041" ref-type="bibr">41</xref>]. The BindingDB database [<xref rid="pone.0276609.ref042" ref-type="bibr">42</xref>] includes Kd values for 10,665 drugs and 1,413 proteins. DTI pairs with a Kd &lt;30 μM are regarded as positive samples. The MINER DTI dataset from the BIOSNAP collection [<xref rid="pone.0276609.ref043" ref-type="bibr">43</xref>] includes 13,741 DTI pairs for 4,510 drugs and 2,181 proteins from DrugBank [<xref rid="pone.0276609.ref044" ref-type="bibr">44</xref>]. As the BIOSNAP dataset contains only positive DTI pairs, negative DTI pairs are collected and removed from among the unseen pairs [<xref rid="pone.0276609.ref040" ref-type="bibr">40</xref>]. We removed samples for which SMILES was not applicable to RDKit [<xref rid="pone.0276609.ref045" ref-type="bibr">45</xref>].</p>
      <table-wrap position="float" id="pone.0276609.t003">
        <object-id pub-id-type="doi">10.1371/journal.pone.0276609.t003</object-id>
        <label>Table 3</label>
        <caption>
          <title>Statistics of the employed datasets: DAVIS, BindingDB and BIOSNAP.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0276609.t003" id="pone.0276609.t003g" position="float"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="1" colspan="1">Dataset</th>
                <th align="center" rowspan="1" colspan="1">Drug</th>
                <th align="center" rowspan="1" colspan="1">Protein</th>
                <th align="center" colspan="3" rowspan="1">Positive sample</th>
                <th align="center" colspan="3" rowspan="1">Negative sample</th>
              </tr>
              <tr>
                <th align="center" rowspan="1" colspan="1"/>
                <th align="center" rowspan="1" colspan="1"/>
                <th align="center" rowspan="1" colspan="1"/>
                <th align="center" rowspan="1" colspan="1">Train</th>
                <th align="center" rowspan="1" colspan="1">Valid</th>
                <th align="center" rowspan="1" colspan="1">Test</th>
                <th align="center" rowspan="1" colspan="1">Train</th>
                <th align="center" rowspan="1" colspan="1">Valid</th>
                <th align="center" rowspan="1" colspan="1">Test</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="1" colspan="1">DAVIS</td>
                <td align="center" rowspan="1" colspan="1">68</td>
                <td align="center" rowspan="1" colspan="1">379</td>
                <td align="center" rowspan="1" colspan="1">1,043</td>
                <td align="center" rowspan="1" colspan="1">1,043</td>
                <td align="center" rowspan="1" colspan="1">160</td>
                <td align="center" rowspan="1" colspan="1">2,846</td>
                <td align="center" rowspan="1" colspan="1">303</td>
                <td align="center" rowspan="1" colspan="1">5,708</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">BindingDB</td>
                <td align="center" rowspan="1" colspan="1">10,665</td>
                <td align="center" rowspan="1" colspan="1">1,413</td>
                <td align="center" rowspan="1" colspan="1">6,329</td>
                <td align="center" rowspan="1" colspan="1">6,334</td>
                <td align="center" rowspan="1" colspan="1">925</td>
                <td align="center" rowspan="1" colspan="1">5,712</td>
                <td align="center" rowspan="1" colspan="1">1,903</td>
                <td align="center" rowspan="1" colspan="1">11,377</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">BIOSNAP</td>
                <td align="center" rowspan="1" colspan="1">4,510</td>
                <td align="center" rowspan="1" colspan="1">2,181</td>
                <td align="center" rowspan="1" colspan="1">9,656</td>
                <td align="center" rowspan="1" colspan="1">9,533</td>
                <td align="center" rowspan="1" colspan="1">1,394</td>
                <td align="center" rowspan="1" colspan="1">1,344</td>
                <td align="center" rowspan="1" colspan="1">2,761</td>
                <td align="center" rowspan="1" colspan="1">2,719</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t003fn001">
            <p>The numerical values indicate the numbers.</p>
          </fn>
          <fn id="t003fn002">
            <p>Positive DTI pairs were divided into training, validation, and testing sets at a ratio of 7:1:2. For well-shaped training, the number of negative DTI samples was set to the same number as positive samples in the training datasets. The remaining negative pairs were placed in the validation and test datasets. The positive and negative pairs are balanced in the BIOSNAP test dataset; they are imbalanced in the DAVIS and BindingDB test datasets, where the number of negative samples is more than 20-fold and 7-fold greater than that of positive samples, respectively.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="sec005">
      <title>Drug and protein representations</title>
      <p>Drugs are represented by the SMILES [<xref rid="pone.0276609.ref046" ref-type="bibr">46</xref>], which is a character sequence of atoms and bonds (e.g., C, N, O, =), generated by depth-first traversal for a molecule graph. The SMILES has the disadvantage that its respective characters do not always correspond to chemically valid molecules. To overcome this problem, SELF-referencIng Embedded Strings (SELFIES) [<xref rid="pone.0276609.ref047" ref-type="bibr">47</xref>] are introduced so that all of the tokens or words of the SELFIES correspond to chemically valid molecules, increasing the reliability of interpretation and rationality of the learning algorithms or mechanisms. It should be noted that a token is the minimum meaningful character set. A target protein is represented as a letter sequence of the 20 standard AAs.</p>
    </sec>
    <sec id="sec006">
      <title>Frequent consecutive sub-sequence mining</title>
      <p>The frequent consecutive sub-sequence (FCS) mining method [<xref rid="pone.0276609.ref040" ref-type="bibr">40</xref>] is used to extract sub-sequences with variable length for both drug compounds and proteins. The FCS algorithm finds recurring sub-sequences across drug and protein sequence datasets. According to the natural language processing–based identification method of sub-word tokens [<xref rid="pone.0276609.ref048" ref-type="bibr">48</xref>,<xref rid="pone.0276609.ref049" ref-type="bibr">49</xref>], FCS produces a hierarchical set of frequently occurring sub-sequences. After collecting a sequence set of SMILES and AA sequences, the FCS method decomposes or tokenizes the sequences of drugs and proteins into sub-sequences and individual atoms and AAs that are designated a token set. FCS scans the token set to identify the most-frequent consecutive tokens (e.g., X, Y) and updates them with their combined token (XY). FCS repeats the process of scanning, identification, and updating until no frequent token is above a specific threshold or the number of tokens reaches a specific threshold. Frequent sub-sequences are bound into one token, and infrequent sub-sequences are decomposed into short tokens.</p>
    </sec>
    <sec id="sec007">
      <title>Encoding methods</title>
      <p>We paired each token of SMILES FCS, AA FCS, SMILES, SELFIES, and AAs with its corresponding label integer to generate a label-encoding vector. The resulting label-encoding vectors were embedded into token-embedding matrixes by two encoding methods, nn.Embedding (PyTorch) and one-hot encoding. <xref rid="pone.0276609.t001" ref-type="table">Table 1</xref> shows the five encoding methods employed: (1) nn.Embedding of FCS embeds the label-encoding vectors of SMILES FCSs and AA FCSs; (2) nn.Embedding of SMILES embeds the label-encoding vectors of SMILES characters and AA letters; (3) nn.Embedding of SELFIES embeds the label-encoding vectors of SELFIES words and AA letters; (4) one-hot encoding of SMILES embeds the label-encoding vectors of SMILES characters and AA letters; and (5) one-hot encoding of SELFIES embeds the label-encoding vectors of SELFIES words and AA letters. Here, we demonstrate how to make a label encoding vector with SMILES characters. SMILES [CN = C = O] is decomposed into a character list: [C, N, =, C, =, O], which is then converted into a label-encoding vector [<xref rid="pone.0276609.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0276609.ref003" ref-type="bibr">3</xref>, 60, <xref rid="pone.0276609.ref001" ref-type="bibr">1</xref>, 60, <xref rid="pone.0276609.ref004" ref-type="bibr">4</xref>, 60], where (C:1, N:3, =: 60, O:4).</p>
      <p>As drug and protein sequences have varying lengths, the maximum sizes of the label-encoding vectors for drugs and proteins were set to 50 and 545, respectively. nn.Embedding embeds the label-encoding vectors of a drug and protein into 50 × 384 and 545 × 384 token-embedding matrixes, respectively. The one-hot encoding method encodes a drug and a protein into 50× (length of the SMILES character list or SELFIES word list) and 545× (length of AA letter list) token-embedding matrixes, respectively.</p>
      <p>We also added a positional embedding method to the token-embedding, because the position of words generally plays an essential role in any language grammar, defining the semantics of a sentence. As the token-embedding matrixes do not have any information regarding position of the sub-sequences or tokens, we added position information for the sub-sequences to their features. The position label–encoding vector is embedded by nn.Embedding into the position-embedding matrix, which is added to the token-embedding matrix. The resulting matrix was designated the embedding matrix.</p>
    </sec>
    <sec id="sec008">
      <title>Cross-attention network</title>
      <p>We built multi-head cross-attention networks with an attention head number of <italic toggle="yes">h</italic> [<xref rid="pone.0276609.ref050" ref-type="bibr">50</xref>]. The query of a target protein, <inline-formula id="pone.0276609.e001"><alternatives><graphic xlink:href="pone.0276609.e001.jpg" id="pone.0276609.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow/><mml:mi>P</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mn>1</mml:mn><mml:mi>P</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mn>…</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>h</mml:mi><mml:mi>P</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, is provided with the embedding matrix of a protein <italic toggle="yes">Y</italic><sup><italic toggle="yes">P</italic></sup> by:
<disp-formula id="pone.0276609.e002"><alternatives><graphic xlink:href="pone.0276609.e002.jpg" id="pone.0276609.e002g" position="anchor"/><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>i</mml:mi><mml:mi>P</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow/><mml:mi>P</mml:mi></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>Q</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn mathvariant="italic">1</mml:mn><mml:mo>,</mml:mo><mml:mn mathvariant="italic">2</mml:mn><mml:mo>,</mml:mo><mml:mn>‥</mml:mn><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives><label>(1)</label></disp-formula>
where <inline-formula id="pone.0276609.e003"><alternatives><graphic xlink:href="pone.0276609.e003.jpg" id="pone.0276609.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>Q</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> represents the weight matrix. The Key and Value of a drug, <inline-formula id="pone.0276609.e004"><alternatives><graphic xlink:href="pone.0276609.e004.jpg" id="pone.0276609.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mrow/><mml:mi>D</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mn>…</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>K</mml:mi><mml:mi>h</mml:mi><mml:mi>D</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0276609.e005"><alternatives><graphic xlink:href="pone.0276609.e005.jpg" id="pone.0276609.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow/><mml:mi>D</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mn>…</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>h</mml:mi><mml:mi>D</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, are provided with the embedding matrix of a drug <italic toggle="yes">Y</italic><sup><italic toggle="yes">D</italic></sup> by:
<disp-formula id="pone.0276609.e006"><alternatives><graphic xlink:href="pone.0276609.e006.jpg" id="pone.0276609.e006g" position="anchor"/><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mi>i</mml:mi><mml:mi>D</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow/><mml:mi>D</mml:mi></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>K</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives><label>(2)</label></disp-formula>
and
<disp-formula id="pone.0276609.e007"><alternatives><graphic xlink:href="pone.0276609.e007.jpg" id="pone.0276609.e007g" position="anchor"/><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>D</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow/><mml:mi>D</mml:mi></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>V</mml:mi></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives><label>(3)</label></disp-formula></p>
      <p>We calculated the dot product of <inline-formula id="pone.0276609.e008"><alternatives><graphic xlink:href="pone.0276609.e008.jpg" id="pone.0276609.e008g" position="anchor"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>i</mml:mi><mml:mi>P</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0276609.e009"><alternatives><graphic xlink:href="pone.0276609.e009.jpg" id="pone.0276609.e009g" position="anchor"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mi>i</mml:mi><mml:mi>D</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> to obtain the attention weight matrix given by:
<disp-formula id="pone.0276609.e010"><alternatives><graphic xlink:href="pone.0276609.e010.jpg" id="pone.0276609.e010g" position="anchor"/><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="2pt"/><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>P</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>i</mml:mi><mml:mi>P</mml:mi></mml:msubsup><mml:msubsup><mml:mi>K</mml:mi><mml:mi>i</mml:mi><mml:mi>D</mml:mi></mml:msubsup><mml:msup><mml:mrow/><mml:mrow><mml:mspace width="2pt"/><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives><label>(4)</label></disp-formula>
where <italic toggle="yes">d</italic> represents the column dimension of <inline-formula id="pone.0276609.e011"><alternatives><graphic xlink:href="pone.0276609.e011.jpg" id="pone.0276609.e011g" position="anchor"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>Q</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0276609.e012"><alternatives><graphic xlink:href="pone.0276609.e012.jpg" id="pone.0276609.e012g" position="anchor"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mi>i</mml:mi><mml:mi>D</mml:mi></mml:msubsup><mml:msup><mml:mrow/><mml:mrow><mml:mspace width="2pt"/><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> represents the transposed matrix of <inline-formula id="pone.0276609.e013"><alternatives><graphic xlink:href="pone.0276609.e013.jpg" id="pone.0276609.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>K</mml:mi><mml:mi>i</mml:mi><mml:mi>D</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. The attention weight matrix is multiplied by <inline-formula id="pone.0276609.e014"><alternatives><graphic xlink:href="pone.0276609.e014.jpg" id="pone.0276609.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>D</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> to compute the attention, or head, which is given by:
<disp-formula id="pone.0276609.e015"><alternatives><graphic xlink:href="pone.0276609.e015.jpg" id="pone.0276609.e015g" position="anchor"/><mml:math id="M15" display="block" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msubsup><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>P</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msubsup><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>P</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="2pt"/><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>P</mml:mi></mml:msubsup><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>D</mml:mi></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives><label>(5)</label></disp-formula></p>
      <p>The multi-head attention is expressed by:
<disp-formula id="pone.0276609.e016"><alternatives><graphic xlink:href="pone.0276609.e016.jpg" id="pone.0276609.e016g" position="anchor"/><mml:math id="M16" display="block" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mi>P</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>D</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msubsup><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>P</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mn>…</mml:mn><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msubsup><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>P</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mn>0</mml:mn><mml:mi>P</mml:mi></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives><label>(6)</label></disp-formula></p>
      <p>Subsequently, context matrix of protein <italic toggle="yes">C</italic><sup><italic toggle="yes">P</italic></sup> is given by
<disp-formula id="pone.0276609.e017"><alternatives><graphic xlink:href="pone.0276609.e017.jpg" id="pone.0276609.e017g" position="anchor"/><mml:math id="M17" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi>P</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mi>P</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mi>P</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mi>D</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>D</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives><label>(7)</label></disp-formula></p>
      <p>Next to the context matrix of a protein, we computed the following context matrix of a drug, and vice versa.</p>
      <disp-formula id="pone.0276609.e018">
        <alternatives>
          <graphic xlink:href="pone.0276609.e018.jpg" id="pone.0276609.e018g" position="anchor"/>
          <mml:math id="M18" display="block" overflow="scroll">
            <mml:mrow>
              <mml:msup>
                <mml:mi>C</mml:mi>
                <mml:mi>D</mml:mi>
              </mml:msup>
              <mml:mo>=</mml:mo>
              <mml:msup>
                <mml:mi>Q</mml:mi>
                <mml:mi>D</mml:mi>
              </mml:msup>
              <mml:mo>+</mml:mo>
              <mml:mi>M</mml:mi>
              <mml:mi>u</mml:mi>
              <mml:mi>l</mml:mi>
              <mml:mi>t</mml:mi>
              <mml:mi>i</mml:mi>
              <mml:mi>h</mml:mi>
              <mml:mi>e</mml:mi>
              <mml:mi>a</mml:mi>
              <mml:mi>d</mml:mi>
              <mml:mo stretchy="false">(</mml:mo>
              <mml:msup>
                <mml:mi>Q</mml:mi>
                <mml:mi>D</mml:mi>
              </mml:msup>
              <mml:mo>,</mml:mo>
              <mml:msup>
                <mml:mi>K</mml:mi>
                <mml:mi>P</mml:mi>
              </mml:msup>
              <mml:mo>,</mml:mo>
              <mml:msup>
                <mml:mi>V</mml:mi>
                <mml:mi>P</mml:mi>
              </mml:msup>
              <mml:mo stretchy="false">)</mml:mo>
            </mml:mrow>
          </mml:math>
        </alternatives>
        <label>(8)</label>
      </disp-formula>
      <p>The final context matrixes from the cross-attention networks have three types: consecutively concatenated context matrixes of a drug and protein (<italic toggle="yes">C</italic><sup><italic toggle="yes">D</italic></sup>, <italic toggle="yes">C</italic><sup><italic toggle="yes">P</italic></sup>), context matrix of a drug alone <italic toggle="yes">C</italic><sup><italic toggle="yes">D</italic></sup>, and context matrix of a protein alone <italic toggle="yes">C</italic><sup><italic toggle="yes">P</italic></sup>.</p>
      <p>The resulting context matrix was sent to the output layer to obtain a final probability or prediction score. The output layer consists of two sets of 1D-CNN, ReLU function, max pooling layer, and dropout layer, and one linear transformation layer with the sigmoid function. As an alternative output layer, we prepared fully connected layers composed of two sets of a linear transformation layer, ReLU function, and dropout layer, and one linear transformation network with the sigmoid function.</p>
    </sec>
    <sec id="sec009">
      <title>Evaluation</title>
      <p>Six statistical metrics were used to evaluate the prediction performance of the proposed models: sensitivity (recall) (SN), specificity (SP), precision (PR), F1, area under the receiver operating characteristic curve (ROCAUC), and area under the precision and recall curve (PRAUC) [<xref rid="pone.0276609.ref051" ref-type="bibr">51</xref>]. SN, SP, PR, and F1 are given by:
<disp-formula id="pone.0276609.e019"><alternatives><graphic xlink:href="pone.0276609.e019.jpg" id="pone.0276609.e019g" position="anchor"/><mml:math id="M19" display="block" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives><label>(9)</label></disp-formula>
<disp-formula id="pone.0276609.e020"><alternatives><graphic xlink:href="pone.0276609.e020.jpg" id="pone.0276609.e020g" position="anchor"/><mml:math id="M20" display="block" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives><label>(10)</label></disp-formula>
<disp-formula id="pone.0276609.e021"><alternatives><graphic xlink:href="pone.0276609.e021.jpg" id="pone.0276609.e021g" position="anchor"/><mml:math id="M21" display="block" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives><label>(11)</label></disp-formula>
and
<disp-formula id="pone.0276609.e022"><alternatives><graphic xlink:href="pone.0276609.e022.jpg" id="pone.0276609.e022g" position="anchor"/><mml:math id="M22" display="block" overflow="scroll"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo>⋅</mml:mo><mml:mi>S</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives><label>(12)</label></disp-formula>
where <italic toggle="yes">TP</italic>, <italic toggle="yes">TN</italic>, <italic toggle="yes">FP</italic>, and <italic toggle="yes">FN</italic> denote the numbers of true positives, true negatives, false positives, and false negatives, respectively. We conducted five independent runs to train the models and evaluate them with the test datasets. The statistical metrics were averaged over 5 models. PRAUC was an effective metric on the imbalanced test datasets of DAVIS and BindingDB, where the number of negative pairs was much larger than that of positive pairs.</p>
    </sec>
    <sec id="sec010">
      <title>Attention site analysis</title>
      <p>To demonstrate the interpretability of the attention mechanisms, we investigated the relationship between a high value in the attention weight matrix and the experimental drug-binding sites of target proteins or whether the attention sites correspond to the experimental DTI binding sites. First, we added experimental data regarding drug-binding sites, as derived from sc-PDB [<xref rid="pone.0276609.ref052" ref-type="bibr">52</xref>], to the BIOSNAP test dataset. Consequently, we updated 160 DTIs with their binding sites. Notably, DTI samples with experimental drug-binding sites are limited. Second, we fed the 160 DTIs into the CA_P model (<xref rid="pone.0276609.t002" ref-type="table">Table 2</xref>) to generate attention weight matrixes, which were expanded by the protein and drug sequence axes. For each DTI sample, we sorted the attention sites according to value and then selected the top 30 attention sites. Third, we counted the attention sites that corresponded to the experimental binding sites for each target protein, and this value was designated the experimental consistency number. Fourth, to statistically validate the experimental consistency numbers, we artificially generated random binding sites for each target protein, while keeping the number of random sites the same as the number of experimental sites. We then calculated how many times the top 30 attention sites corresponded to the randomly generated binding sites, which was designated the simulated consistency number. This simulation process was iterated 10,000 times for the 160 DTIs to obtain a profile of simulated consistency numbers. Finally, based on the simulated profile, we calculated the z-score of the experimental consistency number between the top 30 attention sites and the experimental binding sites.</p>
    </sec>
    <sec id="sec011">
      <title>Implementation</title>
      <p>All the programs were written in Python. Programming regarding SMILES and SELFIES was written using RDKit [<xref rid="pone.0276609.ref045" ref-type="bibr">45</xref>]. The deep learning programs, including the attention mechanisms and CNN, were implemented in PyTorch [<xref rid="pone.0276609.ref053" ref-type="bibr">53</xref>].</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec012">
    <title>Results and discussion</title>
    <sec id="sec013">
      <title>Optimization of neural network structures</title>
      <p>There are a number of network architectures in the attention-based methods (<xref rid="pone.0276609.t002" ref-type="table">Table 2</xref>). We designed or optimized the network structures of the attention-based network and output layer using the DAVIS dataset with nn.Embedding of the FCS. This optimality of this encoding will be demonstrated later. The attention mechanism selects either of two types: self-attention or cross-attention. The depth of attention layers varies as 1, 2, and 3. The context matrixes resulting from the attention network have three types of concatenated context matrixes: drug and protein, drug context alone, and protein context alone. The output layer employs either of two neural networks: CNN or fully connected layer.</p>
      <p>First, we defined CA_DP (<xref rid="pone.0276609.t002" ref-type="table">Table 2</xref>, <xref rid="pone.0276609.g001" ref-type="fig">Fig 1</xref>) as the base model that consists of nn.Embedding of the FCS, one-layer cross attention mechanisms for the concatenated matrixes of drugs and proteins, and CNN output layer, and tuned its hyper-parameters. Consequently, we determined the number of multi-head attentions as 4, the learning rate as 0.001, maximum epochs as 50, and training batch size as 128. The Adam optimizer was used. All hyper-parameters are provided in our program. To select the neural network suitable for the output layer, we compared CA_DP with CA_DP_FCL that used the fully connected layer as the output layer, as shown in <xref rid="pone.0276609.g002" ref-type="fig">Fig 2</xref> and <xref rid="pone.0276609.s003" ref-type="supplementary-material">S1 Table</xref>. CNN presented higher SN, ROCAUC, and PRAUC values than FCL; thus, CNN was selected. We emphasized the values of PRAUC and ROCAUC to determine the best method, because they are independent of a threshold value, while SP, PR and F1 depend on the threshold. <xref rid="pone.0276609.s003" ref-type="supplementary-material">S1 Table</xref> indicates the values of all the six metrics including PR and F1. We considered that CNN captured local context patterns with different filters. To investigate the effect of attention layer depth on performance, we compared one layer (CA_DP), two layers (CA2_DP), and three layers (CA3_DP). An increase in the layer number decreased the ROCAUC and PRAUC values, suggesting that complex attention mechanisms do not improve performance. Thus, we selected a one-layer attention mechanism. We compared the cross-attention (CA_DP) and self-attention (SA_DP) mechanisms (<xref rid="pone.0276609.s001" ref-type="supplementary-material">S1 Fig</xref>). The cross-attention mechanism exhibited better performance for the four metrics than the self-attention mechanism. This suggests that interactive cross-attentions between drug and protein features are critically important for prediction. This was not consistent with the observation that the two self-attention mechanisms employed by MolTrans exhibit high prediction performance. We speculate that MolTrans complements the lack of cross-attention between drugs and proteins with an interaction map that integrates drug and protein context features from the self-attention mechanism.</p>
      <fig position="float" id="pone.0276609.g002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0276609.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Optimization of the attention-based network architecture.</title>
          <p>Attention-based models with nn.Embedding of FCS were trained using the DAVIS training dataset and evaluated using the DAVIS test dataset. Details of the methods are shown in <xref rid="pone.0276609.t002" ref-type="table">Table 2</xref>. CA_P was designated ICAN.</p>
        </caption>
        <graphic xlink:href="pone.0276609.g002" position="float"/>
      </fig>
      <p>To select the optimal context matrixes resulting from the attention mechanisms, we compared CA_DP, CA_D, and CA_P (<xref rid="pone.0276609.s002" ref-type="supplementary-material">S2 Fig</xref>). CA_P provided higher SN, ROCAUC, and PRAUC values than CA_DP and CA_D, suggesting that CA_P decodes drug-related protein context features without any protein-related drug context features. This is similar to TransformerCPI, which decodes protein features based on encoded drug features, whereas CA_P uses a plain one-attention-layer architecture without any pairwise feed-forward. We conclude that the plain attention mechanism, CA_P, is sufficiently effective to achieve high prediction performance.</p>
    </sec>
    <sec id="sec014">
      <title>Comparison of encoding methods</title>
      <p>To demonstrate the superiority of the nn.Embedding of FCS, five CA_Ps with different encoding methods (<xref rid="pone.0276609.t001" ref-type="table">Table 1</xref>) were tested on the DAVIS dataset, as shown in <xref rid="pone.0276609.g003" ref-type="fig">Fig 3</xref> and <xref rid="pone.0276609.s004" ref-type="supplementary-material">S2 Table</xref>: nn.Embeddings of FCS, SMILES, SELFIES, and one-hot encodings of SMILES and SELFIES. nn.Embedding of FCS presented the highest SN, ROCAUC, and PRAUC values. nn.Embeddings of SMILES and SELFIES outperformed the one-hot encodings of SMILES and SELFIES, suggesting that nn.Embedding facilitates attention-based learning to a greater degree than one-hot encoding. This difference is due to the perceptron architecture implemented by nn.Embedding. nn.Embedding of SELFIES provided higher SP, ROCAUC, and PRAUC values than nn.Embedding of SMILES; one-hot encoding of SELFIES provided much higher SN, ROCAUC, and PRAUC values than one-hot encoding of SMILES. This result indicates that SELFIES is more effective than SMILES at learning features, because SELFIES decomposes chemical formulas into chemically meaningful character lists, whereas SMILES does not. These optimization processes confirmed that CA_P with nn.Embedding of FCS is the optimal method, which was then designated ICAN.</p>
      <fig position="float" id="pone.0276609.g003">
        <object-id pub-id-type="doi">10.1371/journal.pone.0276609.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Optimization of encoding methods.</title>
          <p>CA_Ps (ICANs) with 5 different encoding methods were trained using the DAVIS training dataset and evaluated using the DAVIS test dataset. Details regarding the encoding methods are shown in <xref rid="pone.0276609.t001" ref-type="table">Table 1</xref>.</p>
        </caption>
        <graphic xlink:href="pone.0276609.g003" position="float"/>
      </fig>
    </sec>
    <sec id="sec015">
      <title>Comparison with state-of-the-art methods</title>
      <p>We compared the optimal architecture of ICAN with 7 state-of-the-art methods on DAVIS, as shown in <xref rid="pone.0276609.g004" ref-type="fig">Fig 4</xref> and <xref rid="pone.0276609.s005" ref-type="supplementary-material">S3 Table</xref>. <xref rid="pone.0276609.s005" ref-type="supplementary-material">S3 Table</xref> presents the values of all the six metrics including PR and F1. We employed a classical machine learning method, a linear regression (LR)-based model that used ECFP for encoding drugs and PSC for encoding proteins [<xref rid="pone.0276609.ref040" ref-type="bibr">40</xref>,<xref rid="pone.0276609.ref054" ref-type="bibr">54</xref>], and the following CNN-based models: GNN-CPI [<xref rid="pone.0276609.ref034" ref-type="bibr">34</xref>], DeepDTI [<xref rid="pone.0276609.ref009" ref-type="bibr">9</xref>], DeepDTA [<xref rid="pone.0276609.ref030" ref-type="bibr">30</xref>], DeepConv-DTI [<xref rid="pone.0276609.ref033" ref-type="bibr">33</xref>], and the latest attention mechanism–based methods of TransformerCPI [<xref rid="pone.0276609.ref013" ref-type="bibr">13</xref>] and MolTrans [<xref rid="pone.0276609.ref040" ref-type="bibr">40</xref>]. ICAN and MolTrans outperformed the CNN-based methods (GNN-CPI, DeepDTI, DeepDTA, and DeepConv-DTI) in terms of ROCAUC and PRAUC. In particular, ICAN provided the highest SN, ROCAUC, and PRAUC values for all of the methods, although MolTrans was very competitive compared with ICAN. ICAN and MolTrans exhibited higher SN, SP, ROCAUC, and PRAUC values than TransformerCPI. The high PRAUC value of the ICAN method suggests that it takes advantage in the imbalanced dataset. The CNN-based methods provided higher SN, ROCAUC, and PRAUC values than the classical machine learning model of LR. In the CNN-based methods, DeepDTA exhibited the highest SN, whereas DeepConv-DTI exhibited the highest ROCAUC and PRAUC values.</p>
      <fig position="float" id="pone.0276609.g004">
        <object-id pub-id-type="doi">10.1371/journal.pone.0276609.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Comparison of the performance of ICAN with that of state-of-the-art models on DAVIS datasets.</title>
          <p>All models were trained using the DAVIS training dataset and evaluated using the DAVIS test dataset. CA_P was designated ICAN.</p>
        </caption>
        <graphic xlink:href="pone.0276609.g004" position="float"/>
      </fig>
    </sec>
    <sec id="sec016">
      <title>Robustness analysis</title>
      <p>To characterize the robustness of the optimal network architecture of ICAN, we compared it with 7 state-of-the-art methods on the BindingDB and BIOSNAP datasets, as shown in Figs <xref rid="pone.0276609.g005" ref-type="fig">5</xref> and <xref rid="pone.0276609.g006" ref-type="fig">6</xref> and <xref rid="pone.0276609.s006" ref-type="supplementary-material">S4</xref> and <xref rid="pone.0276609.s007" ref-type="supplementary-material">S5</xref> Tables. In BindingDB (<xref rid="pone.0276609.g005" ref-type="fig">Fig 5</xref>, <xref rid="pone.0276609.s006" ref-type="supplementary-material">S4 Table</xref>), ICAN provided the highest PRAUC value, whereas the CNN-based models (GNN-CPI, DeepDTA, and DeepConv-DTI) and MolTrans were competitive compared with ICAN in terms of ROCAUC and PRAUC values. For the three attention-based models (TransformerCPI, MolTrans, and ICAN), MolTrans and ICAN provided higher ROCAUC and PRAUC values than TransformerCPI. Although GNN-CPI provided high SP, ROCAUC, and PRAUC values, it exhibited poor performance on DAVIS. On BIOSNAP (<xref rid="pone.0276609.g006" ref-type="fig">Fig 6</xref>, <xref rid="pone.0276609.s007" ref-type="supplementary-material">S5 Table</xref>), the CNN-based methods (GNN-CPI, DeepDTI, DeepDTA, and DeepConv-DTI) provided high ROCAUC and PRAUC values that were competitive with or greater than those obtained with the three attention-based methods. DeepDTA provided the highest ROCAUC and PRAUC values for all of the methods. For the three attention-based models, MolTrans provided slightly higher ROCAUC and PRAUC values than ICAN, whereas TransformerCPI was competitive compared with ICAN. TransformerCPI functioned well using the balanced dataset (BIOSNAP), but it performed poorly on the imbalanced datasets (DAVIS, BindingDB).</p>
      <fig position="float" id="pone.0276609.g005">
        <object-id pub-id-type="doi">10.1371/journal.pone.0276609.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>Comparison of the performance of ICAN with that of state-of-the-art models on BindingDB.</title>
          <p>All models were trained using the BindingDB training dataset and evaluated using the BindingDB test dataset. CA_P was designated ICAN.</p>
        </caption>
        <graphic xlink:href="pone.0276609.g005" position="float"/>
      </fig>
      <fig position="float" id="pone.0276609.g006">
        <object-id pub-id-type="doi">10.1371/journal.pone.0276609.g006</object-id>
        <label>Fig 6</label>
        <caption>
          <title>Comparison of the performance of ICAN with that of state-of-the-art models on BIOSNAP.</title>
          <p>All models were trained using the BIOSNAP training dataset and evaluated using the BIOSNAP test dataset. CA_P was designated ICAN.</p>
        </caption>
        <graphic xlink:href="pone.0276609.g006" position="float"/>
      </fig>
      <p>The performance of the deep learning models including ICAN depended on the datasets. No method is ideal or perfect for all three datasets. ICAN provided the highest PRAUC value on the DAVIS and BindingDB test datasets (imbalanced dataset) but not on the BIOSNAP test dataset (balanced dataset). This suggests that ICNA has an advantage in predicting imbalanced test datasets. It is notable that the DAVIS and BindingDB test datasets contain a much greater abundance of negative samples than positive samples. In contrast, the BIOSNAP test dataset contains an equal number of negative and positive samples (<xref rid="pone.0276609.t003" ref-type="table">Table 3</xref>). Both the attention-based methods (ICAN, MolTrans) and CNN-based methods (DeepDTA, DeepConv-DTI) exhibited competitive performance on the BindingDB test dataset. DeepDTA and DeepConv-DTI exhibited higher prediction performance on the BIOSNAP test dataset compared with the attention-based models. CNN may learn a large BIOSNAP dataset efficiently to provide high performance.</p>
      <p>It is essential that DTI predictors exhibit generalization capability and robustness for external tests and practical applications. To this end, we must understand what deep learning models learn that can affect the generalization capability and use not only publically available data for drugs and proteins but also unlabeled biochemical and biophysical data, including structural information. In this regard, structural information emerging from the AlphaFold Protein Structure Database [<xref rid="pone.0276609.ref055" ref-type="bibr">55</xref>] would be useful for enabling accurate predictions.</p>
    </sec>
    <sec id="sec017">
      <title>Interpretability of binding mechanism</title>
      <p>To demonstrate the interpretability of the attention mechanisms, we fed the 160 DTIs with their binding sites into ICAN to generate corresponding attention weight matrixes (<xref rid="pone.0276609.g007" ref-type="fig">Fig 7A</xref>) and calculated the experimental consistency number between the top 30 attention sites and the experimental drug-binding sites as 2.020 (<xref rid="pone.0276609.g007" ref-type="fig">Fig 7B</xref>). We found that approximately 2 of the 30 attention sites corresponded to the experimental binding sites for each target protein. To statistically evaluate this number, we generated 10,000 simulated consistency numbers between the top 30 attention sites and randomly generated binding sites of proteins for 160 DTIs (<xref rid="pone.0276609.g007" ref-type="fig">Fig 7B</xref>). The mean and standard deviation of the consistency number were 1.64 and 0.119, respectively. Based on this profile, the calculated z-score for the experimental consistency number (2.020) was 3.186, indicating that the attention sites were definitely related to the experimental binding sites.</p>
      <fig position="float" id="pone.0276609.g007">
        <object-id pub-id-type="doi">10.1371/journal.pone.0276609.g007</object-id>
        <label>Fig 7</label>
        <caption>
          <title>Attention-weight analysis.</title>
          <p>160 DTIs with their binding sites were tested by ICAN. The simulation was iterated 10,000 times. (A) Attention-weight matrix for the DTI between DB06896 and P08581 (hepatocyte growth factor receptor). (B) Profile of simulated consistency numbers between the top 30 attention sites and randomly generated binding sites. (C) Effect of the number of top attention sites on experimental consistency numbers and associated z-scores. Experimental consistency numbers and associated z-scores were calculated with respect to a change in the top attention site number. (D) Effect of a shift of attention sites on experimental consistency number and their z-score values.</p>
        </caption>
        <graphic xlink:href="pone.0276609.g007" position="float"/>
      </fig>
      <p>To determine why it is necessary to use the top 30 attention sites, we investigated how a change in the maximum number of attention sites affects the experimental consistency numbers and their corresponding z-scores, as shown in <xref rid="pone.0276609.g007" ref-type="fig">Fig 7C</xref>. Using the top 30 attention sites provided the greatest increase in both the experimental consistency number and z-score. Thus, we selected the top 30 attention sites. Another reason was that the average number of experimental binding sites was approximately 28.9 (~30) for each target protein. Using the top 10 attention sites provided a small experimental consistency number of 0.5, whereas the top 30 attention sites provided a number of 2. This suggests that the top-ranked attention site does not always represent the binding site. Only 2 of 30 attention sites corresponded to binding sites, whereas the remaining 28 did not. The remaining sites may interact indirectly with the binding sites or affect the biochemical and/or structural properties of DTIs. Consequently, although the binding sites are clearly focused by the attention mechanism, they are not priority factors of the mechanism. It would be very interesting to identify what the attention mechanism actually focuses on.</p>
      <p>To further validate the relationship between attention sites and experimental binding sites, we shifted the attention sites by −2, −1, 0, 1, and 2 to calculate the experimental consistency numbers and corresponding z-scores. Zero indicates no shift of the attention sites, whereas a value of 1 indicates that the attention site was up-shifted by one amino acid. A shown in <xref rid="pone.0276609.g007" ref-type="fig">Fig 7D</xref>, a shift of −1 did not reduce the experimental consistency number and its corresponding z-score, whereas shifts of −2, 1, and 2 decreased these values. These results demonstrate that the attention mechanism focuses on the exact binding site and neighboring sites.</p>
      <p>Recently developed attention-based deep learning models have suggested that attention can be interpreted by investigating the relationship between highly weighted attention sites and experimental binding sites. Data from TransformerCPI suggested that weighted attention sites are related to binding sites of transforming growth factor-beta type 1 receptor and C-X-C chemokine receptor type 4 [<xref rid="pone.0276609.ref013" ref-type="bibr">13</xref>]. MolTrans suggested binding sites for Ephrin type-A receptor and Dasatinib and binding sites for histone deacetylase 2 and hydroxamic acid [<xref rid="pone.0276609.ref040" ref-type="bibr">40</xref>]. Although these studies provided a few successful examples, they did not conduct statistical analyses of the results. To overcome this limitation, we first conducted a statistical analysis to determine the relationships between the attention sites and experimental binding sites and discovered that the attention sites were closely related to the experimental binding sites. This conclusion resulted from the plain structure of the cross-attention mechanism, which directly reads the drug and protein encoding features (<xref rid="pone.0276609.g001" ref-type="fig">Fig 1</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec018">
    <title>Conclusions</title>
    <p>We designed an attention-based DTI method designated ICAN, which is composed of an embedding layer, attention mechanism, and output layer. Analyses of the attention mechanism factors (cross attention/self-attention, attention layer depth, selection of context matrixes from the attention networks, and output layer algorithms [CNN or fully connected layer]) revealed that the plain cross-attention mechanism with CNN and CA_P (ICAN) provided the highest prediction performance. The cross-attention mechanism considers sub-sequence interactions between a drug and a protein to produce context matrixes; the subsequent CNN extracts local sub-sequence patterns within the context matrixes using different filters. ICAN successfully decodes drug-related protein context features without the need for any protein-related drug context features. This is similar to TransformerCPI, which decodes protein features based on encoded drug features, but it is not consistent with MolTrans, which uses two self-attention mechanisms. MolTrans implements an interaction map that integrates the drug and protein context features from the self-attention mechanism to complement the lack of a cross-attention mechanism between drugs and proteins. We then selected nn.Embedding of FCS as the best of the five encoding methods: nn.Embedding of FCS, nn.Embedding of SMILES, nn.Embedding of SELFIES, one-hot encoding of SMILES, and one-hot encoding of SELFIES. Furthermore, we found that SELFIES was more effective than SMILES in terms of DTI prediction, because SELFIES decomposes chemical formulas into a chemically meaningful character list, but SMILES does not.</p>
    <p>We also characterized the optimal architecture of ICAN in comparison with 7 state-of-the-art methods: a classical LR-based method, CNN-based models (GNN-CPI, DeepDTI, DeepDTA, and DeepConv-DTI), and the latest attention mechanism–based methods (TransformerCPI, MolTrans). ICAN provided the highest PRAUC value on the imbalanced datasets (DAVIS, BindingDB) but not on the balanced dataset (BIOSNAP). These results suggest that ICAN has an advantage in predicting imbalanced test datasets. The attention-based methods provided higher ROCAUC and PRAUC values than CNN-based models on DAVIS. Both the attention-based methods (ICAN and MolTrans) and CNN-based methods (DeepDTA and DeepConv-DTI) showed very competitive performance on BindingDB. DeepDTA and DeepConv-DTI exhibited better performance than the attention-based models on BIOSNAP. No best method was identified for all three datasets. These results indicate that the performance of deep learning models depends on the dataset.</p>
    <p>The important task is to first conduct a statistical analysis to clearly determine the relationships between attention sites and experimental binding sites. ICAN was shown to enable the interpretation of various DTI mechanisms. We consider that such interpretability results from the plain structure of the cross-attention mechanism between a drug and protein. On the other hand, our analyses also suggested that the attention mechanism captures critical factors other than binding sites, as it was shown to explain only a few of the 30 attention sites considered. To further increase the interpretability and robustness in examining independent datasets, it is necessary to determine exactly what the attention mechanism really captures and how this affects the generalization capability. Additional DTI predictors remain to be developed using different chemo-genomics approaches.</p>
  </sec>
  <sec id="sec019" sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material id="pone.0276609.s001" position="float" content-type="local-data">
      <label>S1 Fig</label>
      <caption>
        <title>Self-attention–based model.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0276609.s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0276609.s002" position="float" content-type="local-data">
      <label>S2 Fig</label>
      <caption>
        <title>Cross-attention–based model.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0276609.s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0276609.s003" position="float" content-type="local-data">
      <label>S1 Table</label>
      <caption>
        <title>Performance of different network architectures.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0276609.s003.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0276609.s004" position="float" content-type="local-data">
      <label>S2 Table</label>
      <caption>
        <title>Performance of different encoding methods in CA_P (ICAN).</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0276609.s004.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0276609.s005" position="float" content-type="local-data">
      <label>S3 Table</label>
      <caption>
        <title>Performance of different learning methods on the DAVIS test dataset.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0276609.s005.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0276609.s006" position="float" content-type="local-data">
      <label>S4 Table</label>
      <caption>
        <title>Performance of different learning methods on the BindingDB test dataset.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0276609.s006.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0276609.s007" position="float" content-type="local-data">
      <label>S5 Table</label>
      <caption>
        <title>Performance of different learning methods on the BIOSNAP test dataset.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0276609.s007.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>We are grateful to Mitsuki Watanabe for collecting DTI binding-site data.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="pone.0276609.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Broach</surname><given-names>JR</given-names></name>, <name><surname>Thorner</surname><given-names>J</given-names></name>. <article-title>High-throughput screening for drug discovery</article-title>. <source>Nature</source>. <year>1996</year>;<volume>384</volume>(<issue>6604 Suppl</issue>):<fpage>14</fpage>–<lpage>6</lpage>. Epub 1996/11/07. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/384014a0</pub-id> .<?supplied-pmid 8895594?><pub-id pub-id-type="pmid">8895594</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Ezzat</surname><given-names>A</given-names></name>, <name><surname>Wu</surname><given-names>M</given-names></name>, <name><surname>Li</surname><given-names>XL</given-names></name>, <name><surname>Kwoh</surname><given-names>CK</given-names></name>. <article-title>Computational prediction of drug-target interactions using chemogenomic approaches: an empirical survey</article-title>. <source>Brief Bioinform</source>. <year>2019</year>;<volume>20</volume>(<issue>4</issue>):<fpage>1337</fpage>–<lpage>57</lpage>. Epub 2018/01/30. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bib/bby002</pub-id> .<?supplied-pmid 29377981?><pub-id pub-id-type="pmid">29377981</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Yamanishi</surname><given-names>Y</given-names></name>, <name><surname>Araki</surname><given-names>M</given-names></name>, <name><surname>Gutteridge</surname><given-names>A</given-names></name>, <name><surname>Honda</surname><given-names>W</given-names></name>, <name><surname>Kanehisa</surname><given-names>M</given-names></name>. <article-title>Prediction of drug-target interaction networks from the integration of chemical and genomic spaces</article-title>. <source>Bioinformatics</source>. <year>2008</year>;<volume>24</volume>(<issue>13</issue>):<fpage>i232</fpage>–<lpage>40</lpage>. Epub 2008/07/01. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btn162</pub-id> .<?supplied-pmid 18586719?><pub-id pub-id-type="pmid">18586719</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Ding</surname><given-names>H</given-names></name>, <name><surname>Takigawa</surname><given-names>I</given-names></name>, <name><surname>Mamitsuka</surname><given-names>H</given-names></name>, <name><surname>Zhu</surname><given-names>S</given-names></name>. <article-title>Similarity-based machine learning methods for predicting drug-target interactions: a brief review</article-title>. <source>Brief Bioinform</source>. <year>2014</year>;<volume>15</volume>(<issue>5</issue>):<fpage>734</fpage>–<lpage>47</lpage>. Epub 2013/08/13. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bib/bbt056</pub-id> .<?supplied-pmid 23933754?><pub-id pub-id-type="pmid">23933754</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Pahikkala</surname><given-names>T</given-names></name>, <name><surname>Airola</surname><given-names>A</given-names></name>, <name><surname>Pietila</surname><given-names>S</given-names></name>, <name><surname>Shakyawar</surname><given-names>S</given-names></name>, <name><surname>Szwajda</surname><given-names>A</given-names></name>, <name><surname>Tang</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Toward more realistic drug-target interaction predictions</article-title>. <source>Brief Bioinform</source>. <year>2015</year>;<volume>16</volume>(<issue>2</issue>):<fpage>325</fpage>–<lpage>37</lpage>. Epub 2014/04/12. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bib/bbu010</pub-id> .<?supplied-pmid 24723570?><pub-id pub-id-type="pmid">24723570</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Gonen</surname><given-names>M.</given-names></name><article-title>Predicting drug-target interactions from chemical and genomic kernels using Bayesian matrix factorization</article-title>. <source>Bioinformatics</source>. <year>2012</year>;<volume>28</volume>(<issue>18</issue>):<fpage>2304</fpage>–<lpage>10</lpage>. Epub 2012/06/26. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/bts360</pub-id> .<?supplied-pmid 22730431?><pub-id pub-id-type="pmid">22730431</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref007">
      <label>7</label>
      <mixed-citation publication-type="book"><name><surname>Zheng</surname><given-names>X</given-names></name>, editor <part-title>Collaborative matrix factorization with multiple similarities for predicting drug-target interactions</part-title>. <source>KDD</source>; <year>2013</year>; <publisher-loc>Chicago. Chicago, USA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="pone.0276609.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Ezzat</surname><given-names>A</given-names></name>, <name><surname>Zhao</surname><given-names>P</given-names></name>, <name><surname>Wu</surname><given-names>M</given-names></name>, <name><surname>Li</surname><given-names>XL</given-names></name>, <name><surname>Kwoh</surname><given-names>CK</given-names></name>. <article-title>Drug-Target Interaction Prediction with Graph Regularized Matrix Factorization</article-title>. <source>IEEE/ACM Trans Comput Biol Bioinform</source>. <year>2017</year>;<volume>14</volume>(<issue>3</issue>):<fpage>646</fpage>–<lpage>56</lpage>. Epub 2016/02/19. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TCBB.2016.2530062</pub-id> .<?supplied-pmid 26890921?><pub-id pub-id-type="pmid">26890921</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Wen</surname><given-names>M</given-names></name>, <name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Niu</surname><given-names>S</given-names></name>, <name><surname>Sha</surname><given-names>H</given-names></name>, <name><surname>Yang</surname><given-names>R</given-names></name>, <name><surname>Yun</surname><given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Deep-Learning-Based Drug-Target Interaction Prediction</article-title>. <source>J Proteome Res</source>. <year>2017</year>;<volume>16</volume>(<issue>4</issue>):<fpage>1401</fpage>–<lpage>9</lpage>. Epub 2017/03/07. <comment>doi: </comment><pub-id pub-id-type="doi">10.1021/acs.jproteome.6b00618</pub-id> .<?supplied-pmid 28264154?><pub-id pub-id-type="pmid">28264154</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Rogers</surname><given-names>D</given-names></name>, <name><surname>Hahn</surname><given-names>M</given-names></name>. <article-title>Extended-connectivity fingerprints</article-title>. <source>J Chem Inf Model</source>. <year>2010</year>;<volume>50</volume>(<issue>5</issue>):<fpage>742</fpage>–<lpage>54</lpage>. Epub 2010/04/30. <comment>doi: </comment><pub-id pub-id-type="doi">10.1021/ci100050t</pub-id> .<?supplied-pmid 20426451?><pub-id pub-id-type="pmid">20426451</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref011">
      <label>11</label>
      <mixed-citation publication-type="book"><name><surname>Bolton</surname><given-names>EE</given-names></name>. <part-title>Pubchem: integrated platform of small molecules and biological activities</part-title>. <source>Annual Reports in Computational Chemistry</source>. <volume>4</volume>: <publisher-name>Elsevier</publisher-name>; <year>2008</year>. p. <fpage>217</fpage>–<lpage>41</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0276609.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Dubchak</surname><given-names>I</given-names></name>, <name><surname>Muchnik</surname><given-names>I</given-names></name>, <name><surname>Holbrook</surname><given-names>SR</given-names></name>, <name><surname>Kim</surname><given-names>SH</given-names></name>. <article-title>Prediction of protein folding class using global description of amino acid sequence</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>1995</year>;<volume>92</volume>(<issue>19</issue>):<fpage>8700</fpage>–<lpage>4</lpage>. Epub 1995/09/12. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.92.19.8700</pub-id> .<?supplied-pmid 7568000?><pub-id pub-id-type="pmid">7568000</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>L</given-names></name>, <name><surname>Tan</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>D</given-names></name>, <name><surname>Zhong</surname><given-names>F</given-names></name>, <name><surname>Liu</surname><given-names>X</given-names></name>, <name><surname>Yang</surname><given-names>T</given-names></name>, <etal>et al</etal>. <article-title>TransformerCPI: improving compound-protein interaction prediction by sequence-based deep learning with self-attention mechanism and label reversal experiments</article-title>. <source>Bioinformatics</source>. <year>2020</year>;<volume>36</volume>(<issue>16</issue>):<fpage>4406</fpage>–<lpage>14</lpage>. Epub 2020/05/20. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa524</pub-id> .<?supplied-pmid 32428219?><pub-id pub-id-type="pmid">32428219</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Yu</surname><given-names>H</given-names></name>, <name><surname>Chen</surname><given-names>J</given-names></name>, <name><surname>Xu</surname><given-names>X</given-names></name>, <name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Zhao</surname><given-names>H</given-names></name>, <name><surname>Fang</surname><given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>A systematic prediction of multiple drug-target interactions from chemical, genomic, and pharmacological data</article-title>. <source>PLoS One</source>. <year>2012</year>;<volume>7</volume>(<issue>5</issue>):<fpage>e37608</fpage>. Epub 2012/06/06. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0037608</pub-id> .<?supplied-pmid 22666371?><pub-id pub-id-type="pmid">22666371</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Bagherian</surname><given-names>M</given-names></name>, <name><surname>Sabeti</surname><given-names>E</given-names></name>, <name><surname>Wang</surname><given-names>K</given-names></name>, <name><surname>Sartor</surname><given-names>MA</given-names></name>, <name><surname>Nikolovska-Coleska</surname><given-names>Z</given-names></name>, <name><surname>Najarian</surname><given-names>K</given-names></name>. <article-title>Machine learning approaches and databases for prediction of drug-target interaction: a survey paper</article-title>. <source>Brief Bioinform</source>. <year>2021</year>;<volume>22</volume>(<issue>1</issue>):<fpage>247</fpage>–<lpage>69</lpage>. Epub 2020/01/18. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bib/bbz157</pub-id> .<?supplied-pmid 31950972?><pub-id pub-id-type="pmid">31950972</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>He</surname><given-names>T</given-names></name>, <name><surname>Heidemeyer</surname><given-names>M</given-names></name>, <name><surname>Ban</surname><given-names>F</given-names></name>, <name><surname>Cherkasov</surname><given-names>A</given-names></name>, <name><surname>Ester</surname><given-names>M</given-names></name>. <article-title>SimBoost: a read-across approach for predicting drug-target binding affinities using gradient boosting machines</article-title>. <source>J Cheminform</source>. <year>2017</year>;<volume>9</volume>(<issue>1</issue>):<fpage>24</fpage>. Epub 2017/11/01. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/s13321-017-0209-z</pub-id> .<?supplied-pmid 29086119?><pub-id pub-id-type="pmid">29086119</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Ezzat</surname><given-names>A</given-names></name>, <name><surname>Wu</surname><given-names>M</given-names></name>, <name><surname>Li</surname><given-names>XL</given-names></name>, <name><surname>Kwoh</surname><given-names>CK</given-names></name>. <article-title>Drug-target interaction prediction via class imbalance-aware ensemble learning</article-title>. <source>BMC Bioinformatics</source>. <year>2016</year>;<volume>17</volume>(<issue>Suppl 19</issue>):<fpage>509</fpage>. Epub 2017/02/06. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/s12859-016-1377-y</pub-id> .<?supplied-pmid 28155697?><pub-id pub-id-type="pmid">28155697</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Islam</surname><given-names>SM</given-names></name>, <name><surname>Hossain</surname><given-names>SMM</given-names></name>, <name><surname>Ray</surname><given-names>S</given-names></name>. <article-title>DTI-SNNFRA: Drug-target interaction prediction by shared nearest neighbors and fuzzy-rough approximation</article-title>. <source>PLoS One</source>. <year>2021</year>;<volume>16</volume>(<issue>2</issue>):<fpage>e0246920</fpage>. Epub 2021/02/20. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0246920</pub-id> .<?supplied-pmid 33606741?><pub-id pub-id-type="pmid">33606741</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Mahmud</surname><given-names>SMH</given-names></name>, <name><surname>Chen</surname><given-names>W</given-names></name>, <name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Awal</surname><given-names>MA</given-names></name>, <name><surname>Ahmed</surname><given-names>K</given-names></name>, <name><surname>Rahman</surname><given-names>MH</given-names></name>, <etal>et al</etal>. <article-title>PreDTIs: prediction of drug-target interactions based on multiple feature information using gradient boosting framework with data balancing and feature selection techniques</article-title>. <source>Brief Bioinform</source>. <year>2021</year>;<volume>22</volume>(<issue>5</issue>). Epub 2021/03/13. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bib/bbab046</pub-id> .<?supplied-pmid 33709119?><pub-id pub-id-type="pmid">33709119</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Chu</surname><given-names>Y</given-names></name>, <name><surname>Kaushik</surname><given-names>AC</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>W</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Shan</surname><given-names>X</given-names></name>, <etal>et al</etal>. <article-title>DTI-CDF: a cascade deep forest model towards the prediction of drug-target interactions based on hybrid features</article-title>. <source>Brief Bioinform</source>. <year>2021</year>;<volume>22</volume>(<issue>1</issue>):<fpage>451</fpage>–<lpage>62</lpage>. Epub 2019/12/31. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bib/bbz152</pub-id> .<?supplied-pmid 31885041?><pub-id pub-id-type="pmid">31885041</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Pan</surname><given-names>X</given-names></name>, <name><surname>Hu</surname><given-names>L</given-names></name>, <name><surname>Hu</surname><given-names>P</given-names></name>, <name><surname>You</surname><given-names>ZH</given-names></name>. <article-title>Identifying Protein Complexes from Protein-protein Interaction Networks Based on Fuzzy Clustering and GO Semantic Information</article-title>. <source>IEEE/ACM Trans Comput Biol Bioinform</source>. <year>2021</year>;PP. Epub 2021/07/10. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TCBB.2021.3095947</pub-id> .<?supplied-pmid 34242171?><pub-id pub-id-type="pmid">34242171</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref022">
      <label>22</label>
      <mixed-citation publication-type="journal"><name><surname>Hu</surname><given-names>L</given-names></name>, <name><surname>Zhang</surname><given-names>J</given-names></name>, <name><surname>Pan</surname><given-names>X</given-names></name>, <name><surname>Yan</surname><given-names>H</given-names></name>, <name><surname>You</surname><given-names>ZH</given-names></name>. <article-title>HiSCF: leveraging higher-order structures for clustering analysis in biological networks</article-title>. <source>Bioinformatics</source>. <year>2021</year>;<volume>37</volume>(<issue>4</issue>):<fpage>542</fpage>–<lpage>50</lpage>. Epub 2020/09/16. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa775</pub-id> .<?supplied-pmid 32931549?><pub-id pub-id-type="pmid">32931549</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Su</surname><given-names>X</given-names></name>, <name><surname>Hu</surname><given-names>L</given-names></name>, <name><surname>You</surname><given-names>Z</given-names></name>, <name><surname>Hu</surname><given-names>P</given-names></name>, <name><surname>Zhao</surname><given-names>B</given-names></name>. <article-title>Attention-based Knowledge Graph Representation Learning for Predicting Drug-drug Interactions</article-title>. <source>Brief Bioinform</source>. <year>2022</year>;<volume>23</volume>(<issue>3</issue>). Epub 2022/04/23. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bib/bbac140</pub-id> .<?supplied-pmid 35453147?><pub-id pub-id-type="pmid">35453147</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Huang</surname><given-names>K</given-names></name>, <name><surname>Fu</surname><given-names>T</given-names></name>, <name><surname>Glass</surname><given-names>LM</given-names></name>, <name><surname>Zitnik</surname><given-names>M</given-names></name>, <name><surname>Xiao</surname><given-names>C</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>. <article-title>DeepPurpose: a deep learning library for drug-target interaction prediction</article-title>. <source>Bioinformatics</source>. <year>2021</year>;<volume>36</volume>(<issue>22–23</issue>):<fpage>5545</fpage>–<lpage>7</lpage>. Epub 2020/12/05. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa1005</pub-id> .<?supplied-pmid 33275143?><pub-id pub-id-type="pmid">33275143</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>You</surname><given-names>J</given-names></name>, <name><surname>McLeod</surname><given-names>RD</given-names></name>, <name><surname>Hu</surname><given-names>P</given-names></name>. <article-title>Predicting drug-target interaction network using deep learning model</article-title>. <source>Comput Biol Chem</source>. <year>2019</year>;<volume>80</volume>:<fpage>90</fpage>–<lpage>101</lpage>. Epub 2019/04/03. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.compbiolchem.2019.03.016</pub-id> .<?supplied-pmid 30939415?><pub-id pub-id-type="pmid">30939415</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Hamanaka</surname><given-names>M</given-names></name>, <name><surname>Taneishi</surname><given-names>K</given-names></name>, <name><surname>Iwata</surname><given-names>H</given-names></name>, <name><surname>Ye</surname><given-names>J</given-names></name>, <name><surname>Pei</surname><given-names>J</given-names></name>, <name><surname>Hou</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>CGBVS-DNN: Prediction of Compound-protein Interactions Based on Deep Learning</article-title>. <source>Mol Inform</source>. <year>2017</year>;<volume>36</volume>(<issue>1–2</issue>). Epub 2016/08/16. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/minf.201600045</pub-id> .<?supplied-pmid 27515489?><pub-id pub-id-type="pmid">27515489</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Sajadi</surname><given-names>SZ</given-names></name>, <name><surname>Zare Chahooki</surname><given-names>MA</given-names></name>, <name><surname>Gharaghani</surname><given-names>S</given-names></name>, <name><surname>Abbasi</surname><given-names>K</given-names></name>. <article-title>AutoDTI++: deep unsupervised learning for DTI prediction by autoencoders</article-title>. <source>BMC Bioinformatics</source>. <year>2021</year>;<volume>22</volume>(<issue>1</issue>):<fpage>204</fpage>. Epub 2021/04/22. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/s12859-021-04127-2</pub-id> .<?supplied-pmid 33879050?><pub-id pub-id-type="pmid">33879050</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Yu</surname><given-names>L</given-names></name>, <name><surname>Qiu</surname><given-names>W</given-names></name>, <name><surname>Lin</surname><given-names>W</given-names></name>, <name><surname>Cheng</surname><given-names>X</given-names></name>, <name><surname>Xiao</surname><given-names>X</given-names></name>, <name><surname>Dai</surname><given-names>J</given-names></name>. <article-title>HGDTI: predicting drug-target interaction by using information aggregation based on heterogeneous graph neural network</article-title>. <source>BMC Bioinformatics</source>. <year>2022</year>;<volume>23</volume>(<issue>1</issue>):<fpage>126</fpage>. Epub 2022/04/14. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/s12859-022-04655-5</pub-id> .<?supplied-pmid 35413800?><pub-id pub-id-type="pmid">35413800</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref029">
      <label>29</label>
      <mixed-citation publication-type="journal"><name><surname>Abbasi</surname><given-names>K</given-names></name>, <name><surname>Razzaghi</surname><given-names>P</given-names></name>, <name><surname>Poso</surname><given-names>A</given-names></name>, <name><surname>Amanlou</surname><given-names>M</given-names></name>, <name><surname>Ghasemi</surname><given-names>JB</given-names></name>, <name><surname>Masoudi-Nejad</surname><given-names>A</given-names></name>. <article-title>DeepCDA: deep cross-domain compound-protein affinity prediction through LSTM and convolutional neural networks</article-title>. <source>Bioinformatics</source>. <year>2020</year>;<volume>36</volume>(<issue>17</issue>):<fpage>4633</fpage>–<lpage>42</lpage>. Epub 2020/05/29. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa544</pub-id> .<?supplied-pmid 32462178?><pub-id pub-id-type="pmid">32462178</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref030">
      <label>30</label>
      <mixed-citation publication-type="journal"><name><surname>Ozturk</surname><given-names>H</given-names></name>, <name><surname>Ozgur</surname><given-names>A</given-names></name>, <name><surname>Ozkirimli</surname><given-names>E</given-names></name>. <article-title>DeepDTA: deep drug-target binding affinity prediction</article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>34</volume>(<issue>17</issue>):<fpage>i821</fpage>–<lpage>i9</lpage>. Epub 2018/11/14. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/bty593</pub-id> .<?supplied-pmid 30423097?><pub-id pub-id-type="pmid">30423097</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref031">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>Rifaioglu</surname><given-names>AS</given-names></name>, <name><surname>Nalbat</surname><given-names>E</given-names></name>, <name><surname>Atalay</surname><given-names>V</given-names></name>, <name><surname>Martin</surname><given-names>MJ</given-names></name>, <name><surname>Cetin-Atalay</surname><given-names>R</given-names></name>, <name><surname>Dogan</surname><given-names>T</given-names></name>. <article-title>DEEPScreen: high performance drug-target interaction prediction with convolutional neural networks using 2-D structural compound representations</article-title>. <source>Chem Sci</source>. <year>2020</year>;<volume>11</volume>(<issue>9</issue>):<fpage>2531</fpage>–<lpage>57</lpage>. Epub 2020/11/20. <comment>doi: </comment><pub-id pub-id-type="doi">10.1039/c9sc03414e</pub-id> .<?supplied-pmid 33209251?><pub-id pub-id-type="pmid">33209251</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref032">
      <label>32</label>
      <mixed-citation publication-type="journal"><name><surname>Sutskever</surname><given-names>I</given-names></name>, <name><surname>Hinton</surname><given-names>GE</given-names></name>. <article-title>Deep, narrow sigmoid belief networks are universal approximators</article-title>. <source>Neural Comput</source>. <year>2008</year>;<volume>20</volume>(<issue>11</issue>):<fpage>2629</fpage>–<lpage>36</lpage>. Epub 2008/06/07. <comment>doi: </comment><pub-id pub-id-type="doi">10.1162/neco.2008.12-07-661</pub-id> .<?supplied-pmid 18533819?><pub-id pub-id-type="pmid">18533819</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref033">
      <label>33</label>
      <mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>I</given-names></name>, <name><surname>Keum</surname><given-names>J</given-names></name>, <name><surname>Nam</surname><given-names>H</given-names></name>. <article-title>DeepConv-DTI: Prediction of drug-target interactions via deep learning with convolution on protein sequences</article-title>. <source>PLoS Comput Biol</source>. <year>2019</year>;<volume>15</volume>(<issue>6</issue>):<fpage>e1007129</fpage>. Epub 2019/06/15. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007129</pub-id> .<?supplied-pmid 31199797?><pub-id pub-id-type="pmid">31199797</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>Tsubaki</surname><given-names>M</given-names></name>, <name><surname>Tomii</surname><given-names>K</given-names></name>, <name><surname>Sese</surname><given-names>J</given-names></name>. <article-title>Compound-protein interaction prediction with end-to-end learning of neural networks for graphs and sequences</article-title>. <source>Bioinformatics</source>. <year>2019</year>;<volume>35</volume>(<issue>2</issue>):<fpage>309</fpage>–<lpage>18</lpage>. Epub 2018/07/10. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/bty535</pub-id> .<?supplied-pmid 29982330?><pub-id pub-id-type="pmid">29982330</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref035">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Nguyen</surname><given-names>T</given-names></name>, <name><surname>Le</surname><given-names>H</given-names></name>, <name><surname>Quinn</surname><given-names>TP</given-names></name>, <name><surname>Nguyen</surname><given-names>T</given-names></name>, <name><surname>Le</surname><given-names>TD</given-names></name>, <name><surname>Venkatesh</surname><given-names>S</given-names></name>. <article-title>GraphDTA: predicting drug-target binding affinity with graph neural networks</article-title>. <source>Bioinformatics</source>. <year>2021</year>;<volume>37</volume>(<issue>8</issue>):<fpage>1140</fpage>–<lpage>7</lpage>. Epub 2020/10/30. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa921</pub-id> .<?supplied-pmid 33119053?><pub-id pub-id-type="pmid">33119053</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref036">
      <label>36</label>
      <mixed-citation publication-type="journal"><name><surname>Scarselli</surname><given-names>F</given-names></name>, <name><surname>Gori</surname><given-names>M</given-names></name>, <name><surname>Tsoi</surname><given-names>AC</given-names></name>, <name><surname>Hagenbuchner</surname><given-names>M</given-names></name>, <name><surname>Monfardini</surname><given-names>G</given-names></name>. <article-title>The graph neural network model</article-title>. <source>IEEE Trans Neural Netw</source>. <year>2009</year>;<volume>20</volume>(<issue>1</issue>):<fpage>61</fpage>–<lpage>80</lpage>. Epub 2008/12/11. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TNN.2008.2005605</pub-id> .<?supplied-pmid 19068426?><pub-id pub-id-type="pmid">19068426</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref037">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Lim</surname><given-names>J</given-names></name>, <name><surname>Ryu</surname><given-names>S</given-names></name>, <name><surname>Park</surname><given-names>K</given-names></name>, <name><surname>Choe</surname><given-names>YJ</given-names></name>, <name><surname>Ham</surname><given-names>J</given-names></name>, <name><surname>Kim</surname><given-names>WY</given-names></name>. <article-title>Predicting Drug-Target Interaction Using a Novel Graph Neural Network with 3D Structure-Embedded Graph Representation</article-title>. <source>J Chem Inf Model</source>. <year>2019</year>;<volume>59</volume>(<issue>9</issue>):<fpage>3981</fpage>–<lpage>8</lpage>. Epub 2019/08/25. <comment>doi: </comment><pub-id pub-id-type="doi">10.1021/acs.jcim.9b00387</pub-id> .<?supplied-pmid 31443612?><pub-id pub-id-type="pmid">31443612</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref038">
      <label>38</label>
      <mixed-citation publication-type="other">Kipf T, Welling M. Semi-supervised classification with graph convolutional networks. 5th International Conference on Learning Representations. Toulon, France2017. p. 1–14.</mixed-citation>
    </ref>
    <ref id="pone.0276609.ref039">
      <label>39</label>
      <mixed-citation publication-type="journal"><name><surname>Torng</surname><given-names>W</given-names></name>, <name><surname>Altman</surname><given-names>RB</given-names></name>. <article-title>Graph Convolutional Neural Networks for Predicting Drug-Target Interactions</article-title>. <source>J Chem Inf Model</source>. <year>2019</year>;<volume>59</volume>(<issue>10</issue>):<fpage>4131</fpage>–<lpage>49</lpage>. Epub 2019/10/04. <comment>doi: </comment><pub-id pub-id-type="doi">10.1021/acs.jcim.9b00628</pub-id> .<?supplied-pmid 31580672?><pub-id pub-id-type="pmid">31580672</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref040">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Huang</surname><given-names>K</given-names></name>, <name><surname>Xiao</surname><given-names>C</given-names></name>, <name><surname>Glass</surname><given-names>LM</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>. <article-title>MolTrans: Molecular Interaction Transformer for drug-target interaction prediction</article-title>. <source>Bioinformatics</source>. <year>2021</year>;<volume>37</volume>(<issue>6</issue>):<fpage>830</fpage>–<lpage>6</lpage>. Epub 2020/10/19. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa880</pub-id> .<?supplied-pmid 33070179?><pub-id pub-id-type="pmid">33070179</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref041">
      <label>41</label>
      <mixed-citation publication-type="journal"><name><surname>Davis</surname><given-names>MI</given-names></name>, <name><surname>Hunt</surname><given-names>JP</given-names></name>, <name><surname>Herrgard</surname><given-names>S</given-names></name>, <name><surname>Ciceri</surname><given-names>P</given-names></name>, <name><surname>Wodicka</surname><given-names>LM</given-names></name>, <name><surname>Pallares</surname><given-names>G</given-names></name>, <etal>et al</etal>. <article-title>Comprehensive analysis of kinase inhibitor selectivity</article-title>. <source>Nat Biotechnol</source>. <year>2011</year>;<volume>29</volume>(<issue>11</issue>):<fpage>1046</fpage>–<lpage>51</lpage>. Epub 2011/11/01. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nbt.1990</pub-id> .<?supplied-pmid 22037378?><pub-id pub-id-type="pmid">22037378</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref042">
      <label>42</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>T</given-names></name>, <name><surname>Lin</surname><given-names>Y</given-names></name>, <name><surname>Wen</surname><given-names>X</given-names></name>, <name><surname>Jorissen</surname><given-names>RN</given-names></name>, <name><surname>Gilson</surname><given-names>MK</given-names></name>. <article-title>BindingDB: a web-accessible database of experimentally determined protein-ligand binding affinities</article-title>. <source>Nucleic Acids Res</source>. <year>2007</year>;<volume>35</volume>(<issue>Database issue</issue>):<fpage>D198</fpage>–<lpage>201</lpage>. Epub 2006/12/06. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/gkl999</pub-id> .<?supplied-pmid 17145705?><pub-id pub-id-type="pmid">17145705</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref043">
      <label>43</label>
      <mixed-citation publication-type="other">Zitnik M, Sosi R, Maheshwari S, Leskovec J. BioSNAP datasets: Stanford biomedical network dataset collection. <ext-link xlink:href="https://snapstanfordedu/biodata/indexhtml" ext-link-type="uri">https://snapstanfordedu/biodata/indexhtml</ext-link>. 2018.</mixed-citation>
    </ref>
    <ref id="pone.0276609.ref044">
      <label>44</label>
      <mixed-citation publication-type="journal"><name><surname>Wishart</surname><given-names>DS</given-names></name>, <name><surname>Knox</surname><given-names>C</given-names></name>, <name><surname>Guo</surname><given-names>AC</given-names></name>, <name><surname>Cheng</surname><given-names>D</given-names></name>, <name><surname>Shrivastava</surname><given-names>S</given-names></name>, <name><surname>Tzur</surname><given-names>D</given-names></name>, <etal>et al</etal>. <article-title>DrugBank: a knowledgebase for drugs, drug actions and drug targets</article-title>. <source>Nucleic Acids Res</source>. <year>2008</year>;<volume>36</volume>(<issue>Database issue</issue>):<fpage>D901</fpage>–<lpage>6</lpage>. Epub 2007/12/01. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/gkm958</pub-id> .<?supplied-pmid 18048412?><pub-id pub-id-type="pmid">18048412</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref045">
      <label>45</label>
      <mixed-citation publication-type="other">Landrum G. RDKit: Open-source cheminformatics. <ext-link xlink:href="https://wwwrdkitorg" ext-link-type="uri">https://wwwrdkitorg</ext-link>. 2006;3.</mixed-citation>
    </ref>
    <ref id="pone.0276609.ref046">
      <label>46</label>
      <mixed-citation publication-type="journal"><name><surname>Weininger</surname><given-names>D.</given-names></name><article-title>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules</article-title>. <source>J Chem Inf Comput Sci</source>. <year>1988</year>;<volume>28</volume>:<fpage>31</fpage>–<lpage>6</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0276609.ref047">
      <label>47</label>
      <mixed-citation publication-type="journal"><name><surname>Krenn</surname><given-names>M</given-names></name>, <name><surname>Haese</surname><given-names>F</given-names></name>, <name><surname>AkshatKumar</surname><given-names>Nigam</given-names></name>, <name><surname>Friederich</surname><given-names>P</given-names></name>, <name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name>. <article-title>Self-Referencing Embedded Strings (SELFIES): A 100% robust molecular string representation</article-title>. <source>Machine Learning: Science and Technology</source>. <year>2020</year>;<volume>1</volume>:<fpage>045024</fpage>.</mixed-citation>
    </ref>
    <ref id="pone.0276609.ref048">
      <label>48</label>
      <mixed-citation publication-type="book"><name><surname>Sennrich</surname><given-names>R</given-names></name>, <name><surname>Haddow</surname><given-names>B</given-names></name>, <name><surname>Birch</surname><given-names>A</given-names></name>, editors. <source>Neural Machine Translation of Rare Words with Subword Units2016</source>. <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>; <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="pone.0276609.ref049">
      <label>49</label>
      <mixed-citation publication-type="journal"><name><surname>Gage</surname><given-names>P.</given-names></name><article-title>A new algorithm for data compression</article-title>. <source>C Users J</source>. <year>1994</year>;<volume>12</volume>:<fpage>23</fpage>–<lpage>38</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0276609.ref050">
      <label>50</label>
      <mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention Is All You Need. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. 2017:1–11.</mixed-citation>
    </ref>
    <ref id="pone.0276609.ref051">
      <label>51</label>
      <mixed-citation publication-type="book"><name><surname>Ewens</surname><given-names>WJ</given-names></name>, <name><surname>Grant</surname><given-names>GR</given-names></name>. <source>Statistical Methods in Bioinformatics: An Introduction</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2004</year>.</mixed-citation>
    </ref>
    <ref id="pone.0276609.ref052">
      <label>52</label>
      <mixed-citation publication-type="journal"><name><surname>Meslamani</surname><given-names>J</given-names></name>, <name><surname>Rognan</surname><given-names>D</given-names></name>, <name><surname>Kellenberger</surname><given-names>E</given-names></name>. <article-title>sc-PDB: a database for identifying variations and multiplicity of ’druggable’ binding sites in proteins</article-title>. <source>Bioinformatics</source>. <year>2011</year>;<volume>27</volume>(<issue>9</issue>):<fpage>1324</fpage>–<lpage>6</lpage>. Epub 2011/03/15. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btr120</pub-id> .<?supplied-pmid 21398668?><pub-id pub-id-type="pmid">21398668</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref053">
      <label>53</label>
      <mixed-citation publication-type="other">Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al. PyTorch: An Imperative Style, High-Performance Deep Learning Library. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. 2019:1–12.</mixed-citation>
    </ref>
    <ref id="pone.0276609.ref054">
      <label>54</label>
      <mixed-citation publication-type="journal"><name><surname>Cao</surname><given-names>DS</given-names></name>, <name><surname>Xu</surname><given-names>QS</given-names></name>, <name><surname>Liang</surname><given-names>YZ</given-names></name>. <article-title>propy: a tool to generate various modes of Chou’s PseAAC</article-title>. <source>Bioinformatics</source>. <year>2013</year>;<volume>29</volume>(<issue>7</issue>):<fpage>960</fpage>–<lpage>2</lpage>. Epub 2013/02/22. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btt072</pub-id> .<?supplied-pmid 23426256?><pub-id pub-id-type="pmid">23426256</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0276609.ref055">
      <label>55</label>
      <mixed-citation publication-type="journal"><name><surname>Jumper</surname><given-names>J</given-names></name>, <name><surname>Evans</surname><given-names>R</given-names></name>, <name><surname>Pritzel</surname><given-names>A</given-names></name>, <name><surname>Green</surname><given-names>T</given-names></name>, <name><surname>Figurnov</surname><given-names>M</given-names></name>, <name><surname>Ronneberger</surname><given-names>O</given-names></name>, <etal>et al</etal>. <article-title>Highly accurate protein structure prediction with AlphaFold</article-title>. <source>Nature</source>. <year>2021</year>;<volume>596</volume>(<issue>7873</issue>):<fpage>583</fpage>–<lpage>9</lpage>. Epub 2021/07/16. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id> .<?supplied-pmid 34265844?><pub-id pub-id-type="pmid">34265844</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
