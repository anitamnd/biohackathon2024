<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//ACS//DTD ACS Journal DTD v1.02 20061031//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName ACSJournal-v102.dtd?>
<?SourceDTD.Version 1.02?>
<?ConverterInfo.XSLTName acs2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Chem Inf Model</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Chem Inf Model</journal-id>
    <journal-id journal-id-type="publisher-id">ci</journal-id>
    <journal-id journal-id-type="coden">jcisd8</journal-id>
    <journal-title-group>
      <journal-title>Journal of Chemical Information and Modeling</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1549-9596</issn>
    <issn pub-type="epub">1549-960X</issn>
    <publisher>
      <publisher-name>American Chemical Society</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9049592</article-id>
    <article-id pub-id-type="pmid">35349259</article-id>
    <article-id pub-id-type="doi">10.1021/acs.jcim.1c01198</article-id>
    <article-categories>
      <subj-group>
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PDFDataExtractor: A Tool for Reading Scientific Text
and Interpreting Metadata from the Typeset Literature in the Portable
Document Format</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="ath1">
        <name>
          <surname>Zhu</surname>
          <given-names>Miao</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes" id="ath2">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1552-8743</contrib-id>
        <name>
          <surname>Cole</surname>
          <given-names>Jacqueline M.</given-names>
        </name>
        <xref rid="cor1" ref-type="other">*</xref>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff2" ref-type="aff">‡</xref>
        <xref rid="aff3" ref-type="aff">§</xref>
      </contrib>
      <aff id="aff1"><label>†</label>Cavendish
Laboratory, Department of Physics, <institution>University
of Cambridge</institution>, J. J. Thomson Avenue, Cambridge CB3 0HE, <country>U.K.</country></aff>
      <aff id="aff2"><label>‡</label><institution>ISIS
Neutron and Muon Source, STFC Rutherford Appleton Laboratory, Harwell
Science and Innovation Campus</institution>, Didcot, Oxfordshire OX11
0QX, <country>U.K.</country></aff>
      <aff id="aff3"><label>§</label>Department
of Chemical Engineering and Biotechnology, <institution>University of Cambridge</institution>, West Cambridge Site, Philippa Fawcett Drive, Cambridge CB3 0AS, <country>U.K.</country></aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>*</label>Email: <email>jmc61@cam.ac.uk</email>.</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>29</day>
      <month>03</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <day>11</day>
      <month>04</month>
      <year>2022</year>
    </pub-date>
    <volume>62</volume>
    <issue>7</issue>
    <fpage>1633</fpage>
    <lpage>1643</lpage>
    <history>
      <date date-type="received">
        <day>01</day>
        <month>10</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 American Chemical Society</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>American Chemical Society</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Permits the broadest form of re-use including for commercial purposes, provided that author attribution and integrity are maintained (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p content-type="toc-graphic">
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0017" id="ab-tgr1"/>
      </p>
      <p>The
layout of portable document format (PDF) files is constant
to any screen, and the metadata therein are latent, compared to mark-up
languages such as HTML and XML. No semantic tags are usually provided,
and a PDF file is not designed to be edited or its data interpreted
by software. However, data held in PDF files need to be extracted
in order to comply with open-source data requirements that are now
government-regulated. In the chemical domain, related chemical and
property data also need to be found, and their correlations need to
be exploited to enable data science in areas such as data-driven materials
discovery. Such relationships may be realized using text-mining software
such as the “chemistry-aware” natural-language-processing
tool, ChemDataExtractor; however, this tool has limited data-extraction
capabilities from PDF files. This study presents the PDFDataExtractor
tool, which can act as a plug-in to ChemDataExtractor. It outperforms
other PDF-extraction tools for the chemical literature by coupling
its functionalities to the chemical-named entity-recognition capabilities
of ChemDataExtractor. The intrinsic PDF-reading abilities of ChemDataExtractor
are much improved. The system features a template-based architecture.
This enables semantic information to be extracted from the PDF files
of scientific articles in order to reconstruct the logical structure
of articles. While other existing PDF-extracting tools focus on quantity
mining, this template-based system is more focused on quality mining
on different layouts. PDFDataExtractor outputs information in JSON
and plain text, including the metadata of a PDF file, such as paper
title, authors, affiliation, email, abstract, keywords, journal, year,
document object identifier (DOI), reference, and issue number. With
a self-created evaluation article set, PDFDataExtractor achieved promising
precision for all key assessed metadata areas of the document text.</p>
    </abstract>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>BASF</institution>
            <institution-id institution-id-type="doi">10.13039/100004349</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NA</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Royal Academy of Engineering</institution>
            <institution-id institution-id-type="doi">10.13039/501100000287</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>RCSRF1819710</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Science and Technology Facilities Council</institution>
            <institution-id institution-id-type="doi">10.13039/501100000271</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NA</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>document-id-old-9</meta-name>
        <meta-value>ci1c01198</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>document-id-new-14</meta-name>
        <meta-value>ci1c01198</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>ccc-price</meta-name>
        <meta-value/>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p>The
number of publications has increasingly grown since the digitalization
of publishing,<sup><xref ref-type="bibr" rid="ref1">1</xref></sup> providing a more efficient
platform for scientific communities to share research results. This
large number of publications has led to the literature becoming a
form of “Big Data.” Such data are useful to data science,
which has evolved into a research field owing to the stepwise increase
in data capacity for high-performance computing, and the increasing
availability of open-source scientific data and software code. It
is exciting to realize that the field of “Big Data”
has emerged to produce exciting opportunities for discovering new
science from patterns found in large arrays of data. Such patterns
are best found when data are mined from a structured assembly of information
(a database) that contains the most relevant information about the
problem in hand.</p>
    <p>However, related data are difficult to collate.
This is because
researchers typically share scientific results through many distinct
reports, which can take a variety of forms such as academic papers,
technical reports, books, patents, dissertations, or theses. Data
are thus strewn across scientific documents in a highly fragmented
form. A document may feature unstructured data (e.g., in-line text)
or semistructured data (e.g., a table of information), while related
data may span many documents. Related data need to be structured and
collated in a fashion that auto-builds a database in order to become
useful. Text-mining tools that employ natural-language processing
(NLP) have enabled the structuring and collation of related data.
Open-source software packages, such as CoreNLP<sup><xref ref-type="bibr" rid="ref2">2</xref></sup> and Spacy,<sup><xref ref-type="bibr" rid="ref3">3</xref></sup> can mine text that uses general
language. However, such tools perform poorly when applied to the scientific
domain, owing to its highly specialized language and writing style.
The “chemistry-aware” NLP-based text-mining tool, ChemDataExtractor,<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> was created to overcome this limitation.</p>
    <p>ChemDataExtractor<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> uses an NLP-enabled
workflow that is geared specifically to mine chemistry-related information
from publications. ChemDataExtractor<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> performs
best in the scientific literature that is imported as mark-up language,
for example, HTML or XML. This is because the literature provided
in the HTML or XML format is suitable for parsing in sections that
are semantically marked.<sup><xref ref-type="bibr" rid="ref5">5</xref></sup> For example, <italic>“PDFDataExtractor: A Tool for Reading Scientific Text and
Interpreting Metadata from the Typeset Literature in the Portable
Document Format”</italic> in this document would be tagged
as “<italic>title</italic>” in mark-up language. Other
sections like headings, paragraphs, captions, and tables are also
tagged in the literature. Therefore, once combined with auxiliary
semantic information, it is possible to perform analysis on one or
more user-defined specific sections. Thus, textual noise from document
features such as headers, page numbers, and author affiliations can
be prevented from being fed into the extraction pipeline.</p>
    <p>Unlike
HTML and XML, the layout of the literature provided in the
portable document format (PDF) stays the same across all different
viewing devices.<sup><xref ref-type="bibr" rid="ref5">5</xref></sup> No semantic tags are
usually provided in PDF files<sup><xref ref-type="bibr" rid="ref6">6</xref></sup> as the text
within this format was not originally designed to be read or interpreted
by software programs. Nevertheless, many NLP applications rely on
semantic information of text fed into the pipeline.<sup><xref ref-type="bibr" rid="ref7">7</xref></sup> For example, it is essential to correctly identify the
semantic roles of text blocks from the literature if one solely wants
to perform NLP analysis on abstracts or to find affiliations of authors
from references of a given number of scientific documents that are
provided in the PDF; that way, only text from the abstract or reference
sections of each document are fed to a software program for extraction
and analysis.<sup><xref ref-type="bibr" rid="ref8">8</xref></sup> Although most articles can
be accessed through HTML or XML, there are a large number of articles
that can only be accessed in PDF<sup>5</sup>. Services such as literature
mining and database creation rely on accurate metadata from articles.
However, metadata are sometimes missing.<sup><xref ref-type="bibr" rid="ref9">9</xref></sup></p>
    <p>ChemDataExtractor<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> only has very
limited
proficiency for extracting and interpreting data from PDF files. This
limited functionality relies on a PDF-layout-analysis tool called
PDFMiner<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> to process input files. PDFMiner<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> is a PDF-file extraction tool that outputs excellent
results in terms of PDF-layout analysis, that is, representing a PDF
file in many text blocks with correct reading sequences. It spaces
the positions and fonts of the individual characters.<sup><xref ref-type="bibr" rid="ref6">6</xref></sup> However, the extraction ability of PDFMiner<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> is primitive, where limited semantic information
about text blocks is extracted. This is because PDFMiner<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> is essentially a structural-analysis package,
and no identification of the logical role of text blocks is performed.<sup><xref ref-type="bibr" rid="ref11">11</xref></sup></p>
    <p>Extracting information from PDF files
is well studied, and notable
results have been achieved from previous studies. Available data-extraction
solutions usually tackle problems by utilizing either rule-based or
machine-learning-based approaches. PDFMiner,<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> PDFX,<sup><xref ref-type="bibr" rid="ref1">1</xref></sup> pdftotext,<sup><xref ref-type="bibr" rid="ref12">12</xref></sup> and PDFExtract<sup><xref ref-type="bibr" rid="ref13">13</xref></sup> represent the
rule-based approaches to convert PDF files. These tools generally
use a combination of visual and text/content information to reconstruct
the logical representation of a PDF file. Machine learning has increasingly
drawn more attention in almost every research field. For example,
Cermine,<sup><xref ref-type="bibr" rid="ref9">9</xref></sup> ParsCit,<sup><xref ref-type="bibr" rid="ref14">14</xref></sup> and GROBID<sup><xref ref-type="bibr" rid="ref15">15</xref></sup> are excellent tools for
information retrieval, which use techniques like support vector machines
(SVMs) and conditional random fields (CRFs) to classify text. Overall,
these solutions tend to use generalized methods to cover different
layouts. They offer fair extraction results with different emphases.
However, the driving force behind the creation of PDFDataExtractor
presented herein is to (1) create and populate databases and (2) serve
as a new PDF extraction plug-in in ChemDataExtractor,<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> which hosts metadata rather than simply perform PDF layout
analysis. To this end, we present PDFDataExtractor, a tool that extracts
metadata from scientific articles using PDFMiner<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> to build text blocks.</p>
    <p>This study shows how this is
possible via a template-based approach.
Templates can push PDF extraction limits further at the expense of
handling more layouts. When building a database, precision is considered
to be a more important factor than recall, affording a template-based
approach suitable. The modular system of PDFDataExtractor also facilitates
future customization. Overall, five large publishers account for more
than 70% of chemistry and nearly 40% of physics fields of research.<sup><xref ref-type="bibr" rid="ref16">16</xref></sup></p>
    <p>PDFDataExtractor has been created as a
plug-in to ChemDataExtractor.<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> This allows
PDF-file data extraction to be channeled
directly into a text-mining pipeline with chemical-named entity-recognition
capabilities, as realized by ChemDataExtractor.<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> PDFDataExtractor also generates metadata that are useful
for various ChemDataExtractor<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> functions
that require knowledge of the sectioning of a document. PDFDataExtractor
can generate metadata for 13 primary logical parts that can principally
represent a scientific publication: these are the title, author, abstract,
section headings, paragraph of the body text, figure, header, caption,
journal information, DOI, page number, acknowledgments, and references
(<xref rid="fig1" ref-type="fig">Figure <xref rid="fig1" ref-type="fig">1</xref></xref>).</p>
    <fig id="fig1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>High-level
schematic workflow of PDFDataExtractor.</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0002" id="gr1" position="float"/>
    </fig>
  </sec>
  <sec id="sec2">
    <title>System
Overview</title>
    <p>The general workflow consists of five principal
stages: (1) the
preprocessing of text; (2) metadata extraction; (3) documentation
section detection; (4) reference detection; and (5) output extracted
results to ChemDataExtractor.<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> The extraction
process begins with a PDF being fed to PDFMiner,<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> where it is converted into a representation of text blocks
in the correct reading order. Each text block is assigned a universal
numbering label in PDFDataExtractor, which is later used to segment
the text body. Thus, for each text block, more features are generated
by PDFDataExtractor, based on the information provided by PDFMiner,<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> as shown in <xref rid="fig2" ref-type="fig">Figures <xref rid="fig2" ref-type="fig">2</xref></xref> and <xref rid="fig3" ref-type="fig">3</xref>, which is essential
for all its later extraction stages. Then, PDFDataExtractor automatically
picks a predefined extraction template and passes it to the subsequent
extraction components. Following this preprocessing stage, the program
then loops through text blocks on the first page of the document in
order to determine if they include metadata. If this is the case,
the text block is fed to the metadata-extraction stage of the pipeline,
and the extracted information is attached sequentially to the output
result. In the case that the text block does not contain metadata,
a separate section detection is performed to extract the body section
titles, which is accomplished by simultaneously constructing the text
body and indexing it. Afterward, the extracted reference text is fed
to the reference extraction component for citation parsing. Finally,
extracted results are fed into ChemDataExtractor<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> for chemical information extraction.</p>
    <fig id="fig2" position="float">
      <label>Figure 2</label>
      <caption>
        <p>Low-level schematic workflow
of PDFDataExtractor.</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0003" id="gr2" position="float"/>
    </fig>
    <sec id="sec2.1">
      <title>Preprocessing of Text</title>
      <p>It is necessary to preprocess
text blocks and present the entire PDF in a correct data model (<xref rid="fig3" ref-type="fig">Figure <xref rid="fig3" ref-type="fig">3</xref></xref>) prior to any information
extraction. By default, PDFMiner<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> processes
the document page by page. It is like a blank sheet of paper is used
every time a page is turned. This means that everything is refreshed,
and there is no link between pages; simply, those text blocks are
not in sequence at the documentation level. To this end, each text
block is assigned a universal sequence number to present its relative
location at the document level, providing the ability to index and
select text blocks across different pages. One way to understand the
universal sequence number is to imagine that all pages are flattened
into a single page, where the sequence number of text blocks proceeds
sequentially from 0, with an interval of 1. <xref rid="fig4" ref-type="fig">Figure <xref rid="fig4" ref-type="fig">4</xref></xref> schematically shows distinct types of extracted
text blocks by PDFMiner.<sup><xref ref-type="bibr" rid="ref10">10</xref></sup></p>
      <fig id="fig3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Schematic pseudocode
data model showing features of a preprocessed
text block.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0004" id="gr3" position="float"/>
      </fig>
      <fig id="fig4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Schematic presentation of text blocks extracted
by PDFMiner.<sup><xref ref-type="bibr" rid="ref8">8</xref></sup></p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0005" id="gr4" position="float"/>
      </fig>
      <p>Each text block is stored as a key-value pair, as shown in <xref rid="fig3" ref-type="fig">Figure <xref rid="fig3" ref-type="fig">3</xref></xref>. The key is a combination
of the current page number being processed and the text block within
the page. This enables quick indexing of text blocks across the whole
document, in addition to restricting the pages that are processed.
The value is a subdictionary containing all the features of one text
block that are selectively used by later-processing components to
extract information. For example, “number_of_word” is
used by the metadata-extraction component to extract the abstract,
and “universal_sequence” is used by the section-detection
component for indexing the sections. Some text blocks, such as headers
and page numbers, contribute no information to the extraction pipeline
and are therefore removed at later stages. Notably, not all features
of the text blocks are used by the extraction pipeline; indeed, many
of these might be useful for future development of the program.</p>
    </sec>
    <sec id="sec2.2">
      <title>Metadata Extraction</title>
      <p>Extraction of semantic information
begins once all text blocks have been preprocessed and a template
is selected by PDFDataExtractor. Text blocks are initially fed to
the metadata-extraction stage of the pipeline, where the abstract,
title, keywords, and caption are extracted using a combination of
predefined rules and grammar, which are discussed in the following
subsections. Such metadata are usually displayed on the first page
of a PDF; thus, part of the extraction process is restricted to this
page by indexing the page number of each text block.</p>
      <sec id="sec2.2.1">
        <title>Abstract
Extraction</title>
        <p>The abstract-extraction component
of the program can handle three different situations, which are indicated
in <xref rid="fig5" ref-type="fig">Figure <xref rid="fig5" ref-type="fig">5</xref></xref>. This consists
of (1) one text block containing the string “abstract”
within; (2) two text blocks with string “abstract” separated;
and (3) one text block with no string “abstract.”</p>
        <fig id="fig5" position="float">
          <label>Figure 5</label>
          <caption>
            <p>Three cases
of usually seen abstracts from scientific articles.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0006" id="gr5" position="float"/>
        </fig>
        <p>These three situations represent the majority of abstract
stylings
and are shown in <xref rid="fig5" ref-type="fig">Figure <xref rid="fig5" ref-type="fig">5</xref></xref>. The preprocessed text blocks from the first page of the PDF are
the input source for abstract extraction. The program checks each
text block and automatically selects the appropriate method for the
extraction of the abstract.</p>
        <p>In the first situation, indicated
as (1) in <xref rid="fig5" ref-type="fig">Figure <xref rid="fig5" ref-type="fig">5</xref></xref>,
the program can locate the target text
if one text block contains the string “abstract” and
the length of the block satisfies the predefined threshold.</p>
        <p>In the second situation, the string “abstract” is
separated from the target text, resulting in two text blocks designated
as “abstract” and “target text”, shown
as (2) in <xref rid="fig5" ref-type="fig">Figure <xref rid="fig5" ref-type="fig">5</xref></xref>.
An “identifier” is used in this case, which contains
the coordinates of the first “abstract” text block.
The program then compares the coordinates of each text block with
the “identifier.” The target text is found once the
coordinates of one text block are within the allowed error when compared
to the “identifier.” In other words, a target text block
is identified once the text block vertically aligns with the identifier
“abstract” text block. However, errors can occur in
this situation because the two text blocks are not always perfectly
aligned.</p>
        <p>In the third situation, as shown as (3) in <xref rid="fig5" ref-type="fig">Figure <xref rid="fig5" ref-type="fig">5</xref></xref>, where no “abstract”
string
is included in text blocks, the program works based on the assumption
that the abstract will occupy the largest area on the first page with
other rules; the combination of such rules varies among different
publishers. Therefore, the program compares the area of each text
block, with the result being continuously updated until the target
text block is found. In this case, it is highly likely to select the
“introduction” on the first page. Thus, postextraction
filtration is applied before assigning the result to the final output.</p>
      </sec>
      <sec id="sec2.2.2">
        <title>Title Extraction</title>
        <p>The title-extraction workflow (<xref rid="fig6" ref-type="fig">Figure <xref rid="fig6" ref-type="fig">6</xref></xref>) assumes that the
target text block is generally expected to be located at the top center
of the first page. This pipeline first selects text blocks that are
centered on the first page and then filters out text blocks that do
not meet the length requirement. Finally, a font size filter is applied.
Nonetheless, certain publishers, such as Elsevier, slightly offset
their title text. Hence, their article titles are not perfectly centered
on the page, and the extracted text block might contain unnecessary
data or “noise”, such as the type of the document, author
names, and institute affiliations. To this end, an extra step is added.
Such an extra step analyzes each character from the extracted text
block and performs a parallel character string and size construction.
It assigns each character with its own font size, and then, only characters
that satisfy the font size requirement are selected.</p>
        <fig id="fig6" position="float">
          <label>Figure 6</label>
          <caption>
            <p>Schematic title-extraction
workflow.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0007" id="gr6" position="float"/>
        </fig>
      </sec>
      <sec id="sec2.2.3">
        <title>Keywords and Caption Extraction</title>
        <p>The extraction of keywords
and captions is more straightforward than other processes. In the
case of keywords, the text block usually contains the string “keywords,”
while for captions, each text block usually starts with “Figure
X” and naturally forms a separate block from the rest of the
text. Therefore, the text block for both cases usually contains no
noise, and the extraction can be achieved with simple rules and grammar.
An extraction flow chart for keywords and captions is shown in <xref rid="fig7" ref-type="fig">Figure <xref rid="fig7" ref-type="fig">7</xref></xref>. Each caption is
then assigned a sequence number for sorting.</p>
        <fig id="fig7" position="float">
          <label>Figure 7</label>
          <caption>
            <p>Schematic extraction
flow for captions, keywords, and so forth.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0008" id="gr7" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec id="sec2.3">
      <title>Documentation-Section Detection</title>
      <p>This stage of the extraction
pipeline has two main components that are “noise-search-pattern
creation” and “main-body-text segmentation.”</p>
      <sec id="sec2.3.1">
        <title>Noise-Search-Pattern
Creation</title>
        <p>The noise-search component
of the pipeline is used to exclude noisy information such as the header,
page number, and so forth; it then converts these into a machine-readable
search pattern. Such a pattern is then fed to the main-body-text-segmentation
component of the pipeline. The workflow for the noise-search-pattern
creation is shown in <xref rid="fig8" ref-type="fig">Figure <xref rid="fig8" ref-type="fig">8</xref></xref>. The program assumes that all headers are located at the
beginning of each page. Thus, it takes the first six text blocks from
each page into consideration.</p>
        <fig id="fig8" position="float">
          <label>Figure 8</label>
          <caption>
            <p>Schematic of the noise-search-pattern-creation
workflow.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0009" id="gr8" position="float"/>
        </fig>
        <p>A key aspect of the “noise-search-pattern-creation”
workflow is the difference between the “compare-list”
and “header-list.” The compare-list is used as a buffer
to temporarily store candidates, whereas the header-list contains
the actual extracted headers. Starting from the first cycle, all of
the first six text blocks are appended to the compare-list as a base.
Then, for the second and all subsequent cycles, the string of each
text block is measured against the compare-list and is then appended
to the header-list, provided that matching strings are found. This
compare-list is actively updated until the final page is processed.
The strings of the extracted headers are automatically converted into
machine-readable search patterns once all of the pages have been processed.
For example, all of the special characters such as space, period,
and parentheses are converted into machine-readable search patterns.
Also, captions and page-number search patterns are appended. Finally,
this search pattern is ready to be used by the main-body-text-segmentation
component of the document-section-detection stage of the extraction
pipeline.</p>
      </sec>
      <sec id="sec2.3.2">
        <title>Main-Body-Text Segmentation</title>
        <p>Main-body
segmentation
takes three items of input: location pairs, titles, and full body
text, as shown in <xref rid="fig9" ref-type="fig">Figure <xref rid="fig9" ref-type="fig">9</xref></xref>.</p>
        <fig id="fig9" position="float">
          <label>Figure 9</label>
          <caption>
            <p>Flow chart showing the main-body-text-segmentation component of
the documentation-section detection. Areas colored blue and green
represent the location pairs and main-body-text constructions, respectively.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0010" id="gr9" position="float"/>
        </fig>
        <sec id="sec2.3.2.1">
          <title>Location Pairs and Title Constructions (in
Blue)</title>
          <p>Location
pairs are essentially the location information for each section, which
represents the start and end positions of each section. The pipeline
first checks each text block and determines if it is a section title
by using predefined rules and grammar. Accordingly, the three characteristics
of the candidate are appended to three different lists known as “location,”
“string,” and “font” lists.</p>
          <p>Three
lists work accordingly to construct the location pairs. First, the
string-list stores the text of candidate text blocks, which is later
used for naming the extracted sections. Then, the location-list stores
the location information of the extracted text blocks alongside the
universal sequence number, which is a document-level feature of one
text block and essentially informs the program about the span of one
section. Finally, the font-list stores font size information for every
candidate text block.</p>
          <p>However, the lists must be cleaned before
being used by the segmentation
component that is indicated in orange in <xref rid="fig9" ref-type="fig">Figure <xref rid="fig9" ref-type="fig">9</xref></xref>. The principle behind the cleaning process
is that true positive (TP) candidates have the largest font size among
the false positives (FPs).</p>
          <p>Cleaning starts after all pages have
been processed. At this point,
the three lists are arranged in parallel and are therefore of the
same length. At each index, there are three correspondingly linked
values, as shown in <xref rid="fig10" ref-type="fig">Figure <xref rid="fig10" ref-type="fig">10</xref></xref>; these are “string,” “location,”
and ‘font size’. Cleaning begins with calculating the
maximum font size from the font list. Once the maximum font size has
been found, the program loops through the font-list to find the indexes
of TPs. It is assumed that TPs have the largest font sizes. Once those
indexes are extracted, they are then used to filter out FPs in the
location and string lists. Consider the example in <xref rid="fig10" ref-type="fig">Figure <xref rid="fig10" ref-type="fig">10</xref></xref>, which contains one intended
error marked in red. The function finds that the largest font size
is 10.003, and the corresponding indexes are 0, 1, 3, 4, and 5, where
2 is dropped because the font size of this index is smaller than that
of all others. Then, string and location are selected based on indexes
0, 1, 3, 4, and 5, where “6. Intended Error” and 50
are dropped.</p>
          <fig id="fig10" position="float">
            <label>Figure 10</label>
            <caption>
              <p>Diagram showing the cleaning process.</p>
            </caption>
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0011" id="gr10" position="float"/>
          </fig>
          <p>Finally, as shown in <xref rid="fig11" ref-type="fig">Figure <xref rid="fig11" ref-type="fig">11</xref></xref>, the updated location-list is then used to construct
location pairs that are machine-readable. This is performed by simply
inserting each element from location-list into two location pairs.
For the first pair, the element is placed at the second place, and
for the second pair, the element is placed at the first place. Each
location pair informs the program of the start and end positions for
each section. For example, “Introduction” is located
between the 10th and 25th text blocks. It should be noted that in
the first and last identities of the entire sequence of location pairs,
“10” and “:” are specifically added to
indicate the start and end points of the whole body of the document.
In summary, this function returns two lists that are the “location-pair”
and the “string” list, which are used by the main-body-segmentation
component in the next step. This pipeline conducts the following manipulations:<list list-type="simple"><list-item><label>a)</label><p>Locating section
titles using predefined
rules and grammar; storing the corresponding information into location,
string, and font lists, which are in parallel.</p></list-item><list-item><label>b)</label><p>Identifying the maximum font size from
the font list and using its index to filter out the FPs in string
and location lists.</p></list-item><list-item><label>c)</label><p>The updated location and string lists
are used to build location pairs and name-extracted sections, respectively.</p></list-item></list></p>
          <fig id="fig11" position="float">
            <label>Figure 11</label>
            <caption>
              <p>Diagram showing the construction of location pairs that
are machine-readable.</p>
            </caption>
            <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0012" id="gr11" position="float"/>
          </fig>
        </sec>
        <sec id="sec2.3.2.2">
          <title>Main-Body-Text Construction</title>
          <p>Looking at the main-body-text-construction
component in <xref rid="fig9" ref-type="fig">Figure <xref rid="fig9" ref-type="fig">9</xref></xref>, as indicated in green, the pipeline first takes the search pattern
returned from the noise-creation function and uses it to check every
text block to filter out the page number, headers, and captions. If
the search is positive, the program removes the corresponding text,
and a space occupier is inserted into the body list, as the total
number of universal sequence numbers must remain unchanged to match
the location pair list. If the search is negative, the string of the
text block is appended to the body list until all pages have been
processed. The final returned object is a list containing noise-free
text from the PDF.</p>
        </sec>
        <sec id="sec2.3.2.3">
          <title>Main-Body Segmentation</title>
          <p>The main-body-segmentation
component
of <xref rid="fig9" ref-type="fig">Figure <xref rid="fig9" ref-type="fig">9</xref></xref>, as shown
in orange, consists of three lists, two from the location-pair-construction
component, namely, string (title) and location-pair, and the full-body-text
list from the main-body-text construction. This pipeline segments
the full-body-text list using location pairs, as shown in <xref rid="fig11" ref-type="fig">Figure <xref rid="fig11" ref-type="fig">11</xref></xref>. For example,
the “Introduction” section spans from the 10th element
to the 25th element, meaning that every element within this range
is part of the introduction; the same concept applies to the other
sections. The title list is then used to name the extracted sections.
The essence of the segmentation function is the parallel processing
of each text block, whereby each one is processed through two functions
simultaneously, and the outputs of the two subfunctions are used together
in the final step.</p>
        </sec>
      </sec>
    </sec>
    <sec id="sec2.4">
      <title>Reference Detection</title>
      <p>The reference-extraction workflow
is shown in <xref rid="fig12" ref-type="fig">Figure <xref rid="fig12" ref-type="fig">12</xref></xref>. The extracted information is stored as a list that contains all
of the text from the reference section but without context between
one another, as shown in <xref rid="fig13" ref-type="fig">Figure <xref rid="fig13" ref-type="fig">13</xref></xref>. Entries that are longer than one line can be extracted
into two separate elements. Therefore, it is difficult to determine
which elements should be grouped together as a single reference entry.
To summarize, there is no pattern to follow how each reference will
be extracted by PDFMiner,<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> and one reference
entry is very likely to be extracted as more than one part. In this
way, the result list can be very complex and fragmented. Therefore,
instead of defining many decision functions and rules, the complexity
of the information in the results list is lowered by flattening all
of the elements into a single long string, as schematically shown
in <xref rid="fig14" ref-type="fig">Figure <xref rid="fig14" ref-type="fig">14</xref></xref>.</p>
      <fig id="fig12" position="float">
        <label>Figure 12</label>
        <caption>
          <p>Schematic
flow chart for reference extraction.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0013" id="gr12" position="float"/>
      </fig>
      <fig id="fig13" position="float">
        <label>Figure 13</label>
        <caption>
          <p>Extracted
information is stored as a result list. Alternating colors
represent different references, and the black line indicates a different
element in the result list.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0014" id="gr13" position="float"/>
      </fig>
      <fig id="fig14" position="float">
        <label>Figure 14</label>
        <caption>
          <p>Schematic
presentation of flattened citations with the sequence
number shown in red.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0015" id="gr14" position="float"/>
      </fig>
      <p>Each reference has already
been given a sequence number from the
article, as shown in red in <xref rid="fig15" ref-type="fig">Figure <xref rid="fig15" ref-type="fig">15</xref></xref>. Therefore, entries can be extracted by detecting
the spans of sequence numbers from the string-list shown in <xref rid="fig14" ref-type="fig">Figure <xref rid="fig14" ref-type="fig">14</xref></xref>.</p>
      <fig id="fig15" position="float">
        <label>Figure 15</label>
        <caption>
          <p>Diagram showing the
extraction of metadata from each reference.
The upper left section shows the span of the sequence number, and
the lower section shows the construction of slicing indexes. The upper
right section shows the semantics of each reference entry where authors,
journal name, year, and page number are indicated in green, orange,
purple, and blue, respectively.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_0016" id="gr15" position="float"/>
      </fig>
      <p>For example, sequence numbers of references “[1]”,
“[2]”, “[3]”, “[4]” and
“[5]” have spans of “[10, 14]”, “[104,
108]”, “[237, 241]”, “[353, 357]”
and “[420, 424]” and each reference entry can be described
with this pattern. Therefore, a single reference entry can be defined
as anything that is between two sequence numbers. For machines, this
is done by offsetting all of the starting indexes one element away
from ending indexes to produce slicing indexes that are used to slice
the string-list, as displayed in <xref rid="fig15" ref-type="fig">Figure <xref rid="fig15" ref-type="fig">15</xref></xref>.</p>
    </sec>
    <sec id="sec2.5">
      <title>Output Extracted Results to ChemDataExtractor</title>
      <p>PDFDataExtractor
extracts semantic information from journal articles. However, it can
be made “chemistry-aware” to extract chemical entities
by feeding extracted information into its parent tool, ChemDataExtractor.<sup><xref ref-type="bibr" rid="ref4">4</xref></sup></p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Evaluation</title>
    <p>The performance of PDFDataExtractor
was evaluated based on precision,
recall, and F-score calculations, as shown below:<disp-formula id="eq1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_m001" position="anchor"/><label>1</label></disp-formula><disp-formula id="eq2"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_m002" position="anchor"/><label>2</label></disp-formula><disp-formula id="eq3"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_m003" position="anchor"/><label>3</label></disp-formula></p>
    <p>TPs are defined as the correctly extracted texts, FPs are
defined
as the incorrectly extracted texts, and false negatives (FN) are texts
that should have been extracted but are not returned by the program.
The corresponding XML file is used as the ground truth. Each extracted
string is compared against its corresponding ground truth to count
as TP if the similarity exceeds a predefined similarity threshold.
Such a threshold is set to allow any formatting and textual variations.
The similarity is calculated using the Gestalt Pattern Matching algorithm,
and <xref rid="eq4" ref-type="disp-formula">eq <xref rid="eq4" ref-type="disp-formula">4</xref></xref> states the comparison
between two strings, where S1 and S2 are the two strings to be compared.<disp-formula id="eq4"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_m004" position="anchor"/><label>4</label></disp-formula></p>
    <p>We evaluated the performance of PDFDataExtractor against six
datasets
of journal articles from the American Chemical Society (ACS), Elsevier,
the Royal Society of Chemistry (RSC), as well as Angewandte Chemie,
Chemistry—A European Journal, and the Advanced Materials family
from Wiley. However, creating each dataset to the same size would
be impractical because different publishers have different text-mining
policies, and some access cannot be gained. Also, the ideal dataset
would satisfy the following requirements: (i) It should be easily
accessible, which would allow for the creation of large-scale databases;
(ii) it should be publicly available to allow every user to perform
extraction; (iii) good APIs are necessary when performing large-scale
extraction to enhance the coding production efficiency; (iv) it should
be large enough and sufficiently diverse to cover various research
fields; and (v) it should have good uniformity across different journals
that can enhance the extraction performance. Hence, we selected Elsevier
as our main evaluation dataset, which covers 10 different research
fields with a total of 5797 articles, using search keywords: “solar,”
“super alloy,” “Neel temperatures,” “catalysis,”
“nano,” “cells,” “light,”
“dssc,” “battery,” and “city.”
The downloaded Elsevier dataset contains conference papers that are
not targeted by PDFDataExtractor at the moment. Therefore, such papers
are removed from the extraction.</p>
    <p>Meanwhile, the rest of the
dataset contains two articles each,
with a total of 100 articles.</p>
    <p>Image-based articles were filtered
out when creating datasets.
Datasets store each article in two different formats, one as PDF for
extraction, another one as XML or HTML to use as ground-truth data
for evaluation against extracted data.</p>
  </sec>
  <sec id="sec4">
    <title>Results: The Elsevier Dataset</title>
    <p>The evaluation of the performance of PDFDataExtractor yields promising
results, as shown in <xref rid="tbl1" ref-type="other">Table <xref rid="tbl1" ref-type="other">1</xref></xref>.</p>
    <table-wrap id="tbl1" position="float">
      <label>Table 1</label>
      <caption>
        <title>Evaluation Results for Elsevier<xref rid="t1fn1" ref-type="table-fn">a</xref></title>
      </caption>
      <table frame="hsides" rules="groups" border="0">
        <colgroup>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
        </colgroup>
        <thead>
          <tr>
            <th style="border:none;" align="center">dataset</th>
            <th colspan="6" align="center">metadata</th>
            <th colspan="2" align="center">body</th>
            <th style="border:none;" align="center">references</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="border:none;" align="left">Elsevier</td>
            <td style="border:none;" align="left">title</td>
            <td style="border:none;" align="left">abstract</td>
            <td style="border:none;" align="left">doi</td>
            <td style="border:none;" align="left">journal</td>
            <td style="border:none;" align="left">keywords</td>
            <td style="border:none;" align="left">author</td>
            <td style="border:none;" align="left">sections</td>
            <td style="border:none;" align="left">captions</td>
            <td style="border:none;" align="left">refs</td>
          </tr>
          <tr>
            <td style="border:none;" align="left">precision</td>
            <td style="border:none;" align="left">77.9</td>
            <td style="border:none;" align="left">68.9</td>
            <td style="border:none;" align="left">99.2</td>
            <td style="border:none;" align="left">60.0</td>
            <td style="border:none;" align="left">71.3</td>
            <td style="border:none;" align="left">90.6</td>
            <td style="border:none;" align="left">57.0</td>
            <td style="border:none;" align="left">46.0</td>
            <td style="border:none;" align="left">48.7</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="t1fn1">
          <label>a</label>
          <p>Each cell is the calculated precision.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <sec id="sec4.1">
      <title>Metadata</title>
      <p>Overall, the extraction
results are promising
with each assessed extraction exceeding nearly 60% precision, apart
from journal information. The precision for the journal is lower when
compared to other types of metadata information. There are several
reasons for this deficit in journal information. For example, some
authors use slightly different styles such as moving journal information
leftwards; in some such cases, this might be visually the same to
human eyes but would be different to machines, resulting in it being
ignored by the program. Another reason is that some articles downloaded
from Elsevier show a completely different layout to the typical format
of this publisher. This is because that Elsevier publishes a small
number of articles on behalf of another publisher; this also lowers
the precision of such articles for abstract extraction. Although each
template in PDFDataExtractor is designed to have some generic extraction
abilities, handling a completely different layout using the same template
is impossible. Sometimes, authors swap the locations of year and volume,
which might confuse the program during the evaluation process as the
evaluation compares journal name, year, volume, and page fields independently.</p>
      <p>For the body and reference, the precision is significantly lower.
Such lowering is caused by the ground truth during the evaluation
process. For XML files, the labels that mark the corresponding information
occasionally target other text, resulting in noisy text being treated
as ground truth. Such lowering is eliminated during manual evaluation
and can be seen from other datasets.</p>
    </sec>
    <sec id="sec4.2">
      <title>Body</title>
      <p>PDFDataExtractor
can extract article sections
and captions with good precision, as shown in <xref rid="tbl1" ref-type="other">Table <xref rid="tbl1" ref-type="other">1</xref></xref>. The text under each section is ignored
for evaluation. This is purely because the matching for extracted
and ground-truth data can be interrupted by text from images, tables,
formulae, and so forth, or simply an encoding of characters. Also,
the extraction of metadata information is the main focus of the current
version of PDFDataExtractor. When extracting captions, there are two
main issues that lower the precision. The first is that the current
version of PDFDataExtractor is not able to efficiently separate subcaptions
from the main caption. For example, there are captions named <xref rid="fig1" ref-type="fig">Figure <xref rid="fig1" ref-type="fig">1</xref></xref>a,b, and so forth,
while PDFDataExtractor treats every subcaption as a whole. The second
issue is that PDFDataExtractor includes the string “figure
x” in the results, where “x” is the sequence
number of each caption. Such noise also contributes to a lowering
of the precision.</p>
    </sec>
    <sec id="sec4.3">
      <title>Reference</title>
      <p>For each reference entry,
extracted metadata
were discarded. Only plain substrings were used for evaluation against
the ground-truth dataset. The precision is lower than the other results.
There are several reasons for this, the main one being the noise in
ground-truth data, which is impractical to remove. Another reason
is that the reading order of each reference entry can be extracted
incorrectly, resulting in a comparison of two completely different
reference entries during the evaluation stage. Also, PDFDataExtractor
is not able to extract reference entries without a sequencing number
at the beginning, in a robust manner; the current rules/grammar have
their weights more on reference entries with sequencing numbers.</p>
    </sec>
  </sec>
  <sec id="sec5">
    <title>Results: The Other Datasets</title>
    <p>The title, abstract, and DOI
extractions for each publisher yield
good results with at least 75% precision, except for the RSC (<xref rid="tbl2" ref-type="other">Table <xref rid="tbl2" ref-type="other">2</xref></xref>). This might be caused
by the lack of actual “abstract” text; thus, the program
struggles to locate the target text block. For email and keywords,
some publishers simply do not include such information, and Chemistry—A
European Journal puts keywords at the very end of the article, which
PDFDataExtractor struggles to process. RSC journals and the Chemistry—A
European Journal each give 23 and 27% precision for author extraction,
respectively. Such a low precision is potentially caused by the narrow
structural spacing between the title, author, and abstract. Sometimes,
these text blocks can be extracted as one single text block. For the
RSC, Angewandte, and Chemistry—A European Journal, the section
title extraction precisions are lower than others. This is because
these publishers do not include a sequence number or special character
in front of each section title such that the program can be confused.
For reference extraction, the Advanced Materials family of journals
and the Chemistry—A European Journal outperform others with
a precision of 75 and 80%, respectively. Such high results are because
each reference entry is assigned a sequence number, which the program
can use as anchoring points to separate and extract each reference
entry. Overall, PDFDataExtractor performs well on all publishers,
especially on metadata information extraction.</p>
    <table-wrap id="tbl2" position="float">
      <label>Table 2</label>
      <caption>
        <title>Evaluation results for the ACS, the
RSC, the Advanced Materials family, Chemistry-A European Journal,
and Angewandte Chemie<xref rid="t2fn1" ref-type="table-fn">a</xref></title>
      </caption>
      <table frame="hsides" rules="groups" border="0">
        <colgroup>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
          <col align="left"/>
        </colgroup>
        <thead>
          <tr>
            <th style="border:none;" align="center">dataset</th>
            <th colspan="6" align="center">metadata<hr/></th>
            <th colspan="2" align="center">body<hr/></th>
            <th style="border:none;" align="center">reference</th>
          </tr>
          <tr>
            <th style="border:none;" align="center">publisher</th>
            <th style="border:none;" align="center">title</th>
            <th style="border:none;" align="center">abstract</th>
            <th style="border:none;" align="center">doi</th>
            <th style="border:none;" align="center">journal</th>
            <th style="border:none;" align="center">keywords</th>
            <th style="border:none;" align="center">author</th>
            <th style="border:none;" align="center">sections</th>
            <th style="border:none;" align="center">captions</th>
            <th style="border:none;" align="center">refs</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="border:none;" align="left">ACS</td>
            <td style="border:none;" align="left">0.95</td>
            <td style="border:none;" align="left">0.90</td>
            <td style="border:none;" align="left">0.75</td>
            <td style="border:none;" align="left">0.20</td>
            <td style="border:none;" align="left">0.50</td>
            <td style="border:none;" align="left">0.65</td>
            <td style="border:none;" align="left">0.80</td>
            <td style="border:none;" align="left">0.90</td>
            <td style="border:none;" align="left">0.25</td>
          </tr>
          <tr>
            <td style="border:none;" align="left">RSC</td>
            <td style="border:none;" align="left">0.83</td>
            <td style="border:none;" align="left">0.78</td>
            <td style="border:none;" align="left">1.00</td>
            <td style="border:none;" align="left">0.00</td>
            <td style="border:none;" align="left">0.00</td>
            <td style="border:none;" align="left">0.23</td>
            <td style="border:none;" align="left">0.25</td>
            <td style="border:none;" align="left">0.90</td>
            <td style="border:none;" align="left">0.30</td>
          </tr>
          <tr>
            <td style="border:none;" align="left">Advanced</td>
            <td style="border:none;" align="left">0.90</td>
            <td style="border:none;" align="left">0.90</td>
            <td style="border:none;" align="left">0.90</td>
            <td style="border:none;" align="left">0.76</td>
            <td style="border:none;" align="left">0.80</td>
            <td style="border:none;" align="left">0.76</td>
            <td style="border:none;" align="left">0.85</td>
            <td style="border:none;" align="left">0.75</td>
            <td style="border:none;" align="left">0.75</td>
          </tr>
          <tr>
            <td style="border:none;" align="left">Chemistry-A</td>
            <td style="border:none;" align="left">0.80</td>
            <td style="border:none;" align="left">0.75</td>
            <td style="border:none;" align="left">1.00</td>
            <td style="border:none;" align="left">0.00</td>
            <td style="border:none;" align="left">0.66</td>
            <td style="border:none;" align="left">0.27</td>
            <td style="border:none;" align="left">0.50</td>
            <td style="border:none;" align="left">0.65</td>
            <td style="border:none;" align="left">0.80</td>
          </tr>
          <tr>
            <td style="border:none;" align="left">Angewandte</td>
            <td style="border:none;" align="left">0.90</td>
            <td style="border:none;" align="left">0.80</td>
            <td style="border:none;" align="left">1.00</td>
            <td style="border:none;" align="left">0.26</td>
            <td style="border:none;" align="left">0.65</td>
            <td style="border:none;" align="left">0.50</td>
            <td style="border:none;" align="left">0.30</td>
            <td style="border:none;" align="left">0.80</td>
            <td style="border:none;" align="left">0.10</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="t2fn1">
          <label>a</label>
          <p>Each cell displays the calculated
precision.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <p>PDFDataExtractor
has several known limitations under certain scenarios,
which are as follows:<list list-type="simple"><list-item><p>PDFDataExtractor
ignores formulae within an article
body and purely treats them as plain text; also, lines within formulae
are discarded. The extracted information is essentially correct and
at the correct position within body text but without structures. This
can be solved by first locating each formula using optical character
recognition and then extracting using open-source third-party formula
conversion tools.</p></list-item><list-item><p>Tables contain plenty
of structured text. However, tables
are ignored in PDFDataExtractor, although the information contained
in tables is essentially extracted in common with the formula issue.
Extraction information from tables is a research topic within its
own right. For example, a third-party package termed Camelot<sup><xref ref-type="bibr" rid="ref17">17</xref></sup> claims to resolve the issue.</p></list-item><list-item><p>Author information is sometimes seen in extracted titles.
This is because of the misclassified text blocks from PDFMiner,<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> where the author text block and the title text
block are merged together if the two visually appear to be close.
Filters are used to separate titles from the results, but more robust
ones are needed.</p></list-item><list-item><p>The reference-extraction
component of PDFDataExtractor
relies on the sequence number of each reference entry. However, such
information is sometimes missing.</p></list-item></list></p>
  </sec>
  <sec id="sec6">
    <title>Conclusions</title>
    <p>PDFDataExtractor is a plug-in in ChemDataExtractor<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> for reconstructing PDF articles to ultimately create or
autopopulate databases. Its high precision-oriented system design
ensures that good-quality databases can be achieved. PDFDataExtractor
automatically outputs extracted metadata, article bodies, and references
in JSON or the plain text format. These results are then fed into
its parent ChemDataExtractor<sup><xref ref-type="bibr" rid="ref4">4</xref></sup> text-mining
software package, for which it is a plug-in tool; this enables data
extraction from PDF files to be connected directly to the text mining
of chemical information. The customizable modular structure of PDFDataExtractor
allows for the user to adapt prewritten templates for specific use.</p>
  </sec>
</body>
<back>
  <notes id="notes1" notes-type="si">
    <title>Supporting Information Available</title>
    <p>The Supporting
Information is
available free of charge at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/10.1021/acs.jcim.1c01198?goto=supporting-info">https://pubs.acs.org/doi/10.1021/acs.jcim.1c01198</ext-link>.<list id="silist" list-type="simple"><list-item><p>Documents used for the
evaluation of PDFDataExtractor
and the corresponding scripts (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.1c01198/suppl_file/ci1c01198_si_001.zip">ZIP</ext-link>)</p></list-item></list></p>
  </notes>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sifile1">
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci1c01198_si_001.zip">
        <caption>
          <p>ci1c01198_si_001.zip</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <notes notes-type="" id="notes3">
    <title>Author Contributions</title>
    <p>J.M.C. and
M.Z. designed the research. M.Z. developed the concepts associated
with each stage of the work, with guidance from his PhD supervisor
(J.M.C.). M.Z. wrote the code, performed the evaluation tests, implemented
the tool as a plug-in to ChemDataExtractor, and provided the evaluation
test set for PDFDataExtractor. M.Z. drafted the manuscript with help
from J.M.C. Both authors have reviewed and approved the final manuscript.</p>
  </notes>
  <notes notes-type="COI-statement" id="NOTES-d7e1100-autogenerated">
    <p>The authors declare no
competing financial interest.</p>
  </notes>
  <notes notes-type="" id="notes2">
    <title>Notes</title>
    <p>PDFDataExtractor
is released under the MIT license
and is available to download from <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/cat-lemonade/PDFDataExtractor">https://github.com/cat-lemonade/PDFDataExtractor</uri>. A user guide with examples of code and templates for common chemical
publishers is available from <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/cat-lemonade/PDFDataExtractor/demo">https://github.com/cat-lemonade/PDFDataExtractor/demo</uri>. Document source information for the documents used for the evaluation
of PDFDataExtractor is available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/cat-lemonade/PDFDataExtractor/SI">https://github.com/cat-lemonade/PDFDataExtractor/SI</uri>.</p>
  </notes>
  <ack>
    <title>Acknowledgments</title>
    <p>J.M.C. is grateful for the BASF/Royal Academy
of
Engineering Research Chair in Data-Driven Molecular Engineering of
Functional Materials, which is partly supported by the STFC via the
ISIS Neutron and Muon Source. The BASF portion of this Fellowship
includes provision of PhD funding to support MZ. The authors also
thank BASF for helpful discussions, especially Dr. Peter Geyer, who
helped to set up the project and advised on the initial stages of
work. Taketomo Isazawa from the Molecular Engineering group at the
Cavendish Laboratory is also acknowledged for checking and testing
the code. The authors acknowledge the use of research resources at
the Argonne Leadership Computing Facility (ALCF), which is a DOE Office
of Science Facility, under contract No. DEAC02-06CH11357.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="ref1">
      <mixed-citation publication-type="book" id="cit1"><person-group person-group-type="allauthors"><name><surname>Constantin</surname><given-names>A.</given-names></name>; <name><surname>Pettifer</surname><given-names>S.</given-names></name>; <name><surname>Voronkov</surname><given-names>A.</given-names></name></person-group><article-title>PDFX:
Fully-Automated PDF-to-XML Conversion of Scientific Literature</article-title>. In <source>Proceedings of the 2013 ACM symposium on Document engineering
- DocEng’13</source>; <publisher-name>ACM Press</publisher-name>: <publisher-loc>Florence, Italy</publisher-loc>, <year>2013</year>; p. <fpage>177</fpage>.</mixed-citation>
    </ref>
    <ref id="ref2">
      <mixed-citation publication-type="undeclared" id="cit2"><person-group person-group-type="allauthors"><name><surname>Manning</surname><given-names>C.</given-names></name>; <name><surname>Surdeanu</surname><given-names>M.</given-names></name>; <name><surname>Bauer</surname><given-names>J.</given-names></name>; <name><surname>Finkel</surname><given-names>J.</given-names></name>; <name><surname>Bethard</surname><given-names>S.</given-names></name>; <name><surname>McClosky</surname><given-names>D.</given-names></name></person-group><article-title>The
Stanford Corenlp Natural
Language Processing Toolkit</article-title>. In <source>Proceedings
of 52nd Annual Meeting of the Association for Computational Linguistics:
System Demonstrations</source>, <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="ref3">
      <mixed-citation publication-type="undeclared" id="cit3"><article-title>GitHub - explosion/spaCy:
Industrial-strength Natural Language Processing (NLP) in Python</article-title>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/explosion/spaCy">https://github.com/explosion/spaCy</uri> (accessed Mar 3, 2022).</mixed-citation>
    </ref>
    <ref id="ref4">
      <mixed-citation publication-type="journal" id="cit4"><name><surname>Swain</surname><given-names>M. C.</given-names></name>; <name><surname>Cole</surname><given-names>J. M.</given-names></name><article-title>ChemDataExtractor:
A Toolkit for Automated Extraction
of Chemical Information from the Scientific Literature</article-title>. <source>J. Chem. Inf. Model.</source><year>2016</year>, <volume>56</volume>, <fpage>1894</fpage>–<lpage>1904</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.6b00207</pub-id>.<pub-id pub-id-type="pmid">27669338</pub-id></mixed-citation>
    </ref>
    <ref id="ref5">
      <mixed-citation publication-type="book" id="cit5"><person-group person-group-type="allauthors"><name><surname>Déjean</surname><given-names>H.</given-names></name>; <name><surname>Meunier</surname><given-names>J.-L.</given-names></name></person-group><article-title>A System for Converting PDF Documents into Structured XML Format</article-title>. In <source>Document Analysis Systems VII</source>; <person-group person-group-type="editor"><name><surname>Bunke</surname><given-names>H.</given-names></name>, <name><surname>Spitz</surname><given-names>A.
L.</given-names></name></person-group>, Eds.; <person-group person-group-type="editor"><name><surname>Hutchison</surname><given-names>D.</given-names></name>, <name><surname>Kanade</surname><given-names>T.</given-names></name>, <name><surname>Kittler</surname><given-names>J.</given-names></name>, <name><surname>Kleinberg</surname><given-names>J. M.</given-names></name>, <name><surname>Mattern</surname><given-names>F.</given-names></name>, <name><surname>Mitchell</surname><given-names>J. C.</given-names></name>, <name><surname>Naor</surname><given-names>M.</given-names></name>, <name><surname>Nierstrasz</surname><given-names>O.</given-names></name>, <name><surname>Pandu Rangan</surname><given-names>C.</given-names></name>, <name><surname>Steffen</surname><given-names>B.</given-names></name></person-group>, <etal/> Series Eds.; <publisher-name>Springer Berlin Heidelberg</publisher-name>: <publisher-loc>Berlin,
Heidelberg</publisher-loc>, <year>2006</year>; Vol. <volume>3872</volume>, pp. <fpage>129</fpage>–<lpage>140</lpage>.</mixed-citation>
    </ref>
    <ref id="ref6">
      <mixed-citation publication-type="book" id="cit6"><person-group person-group-type="allauthors"><name><surname>Bast</surname><given-names>H.</given-names></name>; <name><surname>Korzen</surname><given-names>C.</given-names></name></person-group><article-title>A Benchmark and
Evaluation for Text Extraction from PDF</article-title>. In <source>2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL)</source>; <publisher-name>IEEE</publisher-name>: <publisher-loc>Toronto,
ON, Canada</publisher-loc>, <year>2017</year>; pp. <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="ref7">
      <mixed-citation publication-type="journal" id="cit7"><name><surname>Ramakrishnan</surname><given-names>C.</given-names></name>; <name><surname>Patnia</surname><given-names>A.</given-names></name>; <name><surname>Hovy</surname><given-names>E.</given-names></name>; <name><surname>Burns</surname><given-names>G. A.</given-names></name><article-title>Layout-Aware Text
Extraction from Full-Text PDF of Scientific Articles</article-title>. <source>Source Code Biol. Med.</source><year>2012</year>, <volume>7</volume>, <fpage>7</fpage><pub-id pub-id-type="doi">10.1186/1751-0473-7-7</pub-id>.<pub-id pub-id-type="pmid">22640904</pub-id></mixed-citation>
    </ref>
    <ref id="ref8">
      <mixed-citation publication-type="journal" id="cit8"><name><surname>Krallinger</surname><given-names>M.</given-names></name>; <name><surname>Rabal</surname><given-names>O.</given-names></name>; <name><surname>Lourenço</surname><given-names>A.</given-names></name>; <name><surname>Oyarzabal</surname><given-names>J.</given-names></name>; <name><surname>Valencia</surname><given-names>A.</given-names></name><article-title>Information Retrieval and Text Mining
Technologies
for Chemistry</article-title>. <source>Chem. Rev.</source><year>2017</year>, <volume>117</volume>, <fpage>7673</fpage>–<lpage>7761</lpage>. <pub-id pub-id-type="doi">10.1021/acs.chemrev.6b00851</pub-id>.<pub-id pub-id-type="pmid">28475312</pub-id></mixed-citation>
    </ref>
    <ref id="ref9">
      <mixed-citation publication-type="journal" id="cit9"><name><surname>Tkaczyk</surname><given-names>D.</given-names></name>; <name><surname>Szostek</surname><given-names>P.</given-names></name>; <name><surname>Fedoryszak</surname><given-names>M.</given-names></name>; <name><surname>Dendek</surname><given-names>P. J.</given-names></name>; <name><surname>Bolikowski</surname><given-names>Ł.</given-names></name><article-title>CERMINE:
Automatic Extraction of Structured Metadata from Scientific Literature</article-title>. <source>Int. J. Doc. Anal. Recognit.</source><year>2015</year>, <volume>18</volume>, <fpage>317</fpage>–<lpage>335</lpage>. <pub-id pub-id-type="doi">10.1007/s10032-015-0249-8</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref10">
      <mixed-citation publication-type="undeclared" id="cit10"><article-title>GitHub -
pdfminer/pdfminer.six:
Community maintained fork of pdfminer - we fathom PDF</article-title>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/pdfminer/pdfminer.six">https://github.com/pdfminer/pdfminer.six</uri> (accessed Mar 3, 2022).</mixed-citation>
    </ref>
    <ref id="ref11">
      <mixed-citation publication-type="book" id="cit11"><person-group person-group-type="allauthors"><name><surname>Suzuki</surname><given-names>M.</given-names></name>; <name><surname>Yamaguchi</surname><given-names>K.</given-names></name></person-group><article-title>Recognition of
E-Born PDF Including Mathematical Formulas</article-title>. In <source>Computers Helping People with Special Needs</source>; <person-group person-group-type="editor"><name><surname>Miesenberger</surname><given-names>K.</given-names></name>, <name><surname>Bühler</surname><given-names>C.</given-names></name>, <name><surname>Penaz</surname><given-names>P.</given-names></name></person-group>, Eds.; <publisher-name>Springer International
Publishing</publisher-name>: <publisher-loc>Cham</publisher-loc>, <year>2016</year>; Vol. <volume>9758</volume>, pp. <fpage>35</fpage>–<lpage>42</lpage>.</mixed-citation>
    </ref>
    <ref id="ref12">
      <mixed-citation publication-type="undeclared" id="cit12"><article-title>GitHub - jalan/pdftotext:
Simple PDF text extraction</article-title>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/jalan/pdftotext">https://github.com/jalan/pdftotext</uri> (accessed Mar 3, 2022).</mixed-citation>
    </ref>
    <ref id="ref13">
      <mixed-citation publication-type="undeclared" id="cit13"><source>A Tool and Library That
Can Extract Various Areas of Text from a PDF, Especially a Scholarly
Article PDF.: CrossRef/Pdfextract</source>. Crossref, <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="ref14">
      <mixed-citation publication-type="undeclared" id="cit14"><person-group person-group-type="allauthors"><name><surname>Councill</surname><given-names>I. G.</given-names></name>; <name><surname>Giles</surname><given-names>C. L.</given-names></name>; <name><surname>Kan</surname><given-names>M.-Y. P. C.</given-names></name></person-group><article-title>An Open-Source
CRF Reference String Parsing Package</article-title>. In <source>LREC</source>, <year>2008</year>.</mixed-citation>
    </ref>
    <ref id="ref15">
      <mixed-citation publication-type="book" id="cit15"><person-group person-group-type="allauthors"><name><surname>Lopez</surname><given-names>P.</given-names></name></person-group><article-title>GROBID: Combining
Automatic Bibliographic Data Recognition and Term Extraction for Scholarship
Publications</article-title>. In <source>Research and Advanced Technology
for Digital Libraries</source>; <person-group person-group-type="editor"><name><surname>Agosti</surname><given-names>M.</given-names></name>, <name><surname>Borbinha</surname><given-names>J.</given-names></name>, <name><surname>Kapidakis</surname><given-names>S.</given-names></name>, <name><surname>Papatheodorou</surname><given-names>C.</given-names></name>, <name><surname>Tsakonas</surname><given-names>G.</given-names></name></person-group>, Eds.; <publisher-loc>Lecture Notes in Computer Science; Springer</publisher-loc><publisher-name>Berlin, Heidelberg</publisher-name>, <year>2009</year>; pp. <fpage>473</fpage>–<lpage>474</lpage>.</mixed-citation>
    </ref>
    <ref id="ref16">
      <mixed-citation publication-type="journal" id="cit16"><name><surname>Larivière</surname><given-names>V.</given-names></name>; <name><surname>Haustein</surname><given-names>S.</given-names></name>; <name><surname>Mongeon</surname><given-names>P.</given-names></name><article-title>The Oligopoly of Academic Publishers
in the Digital Era</article-title>. <source>PLoS One</source><year>2015</year>, <volume>10</volume>, <elocation-id>e0127502</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0127502</pub-id>.<pub-id pub-id-type="pmid">26061978</pub-id></mixed-citation>
    </ref>
    <ref id="ref17">
      <mixed-citation publication-type="undeclared" id="cit17"><article-title>GitHub -
atlanhq/camelot:
Camelot: PDF Table Extraction for Humans</article-title>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/atlanhq/camelot">https://github.com/atlanhq/camelot</uri> (accessed Mar 3, 2022).</mixed-citation>
    </ref>
  </ref-list>
</back>
