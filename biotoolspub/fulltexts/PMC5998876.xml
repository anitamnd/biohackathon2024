<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5998876</article-id>
    <article-id pub-id-type="pmid">29745837</article-id>
    <article-id pub-id-type="publisher-id">2067</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-018-2067-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CNNH_PSS: protein 8-class secondary structure prediction by convolutional neural network with highway</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Jiyun</given-names>
        </name>
        <address>
          <email>zhoujiyun2010@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Hongpeng</given-names>
        </name>
        <address>
          <email>wanghp@hit.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhao</surname>
          <given-names>Zhishan</given-names>
        </name>
        <address>
          <email>zhishan777@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Xu</surname>
          <given-names>Ruifeng</given-names>
        </name>
        <address>
          <phone>(+86) 0755-26033283</phone>
          <email>xuruifeng@hit.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lu</surname>
          <given-names>Qin</given-names>
        </name>
        <address>
          <email>csluqin@comp.polyu.edu.hk</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.452527.3</institution-id><institution>School Computer Science and Technology, </institution><institution>Harbin Institute of Technology Shenzhen Graduate School, </institution></institution-wrap>HIT Campus Shenzhen University Town, Xili, Shenzhen, Guangdong 518055 China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1764 6123</institution-id><institution-id institution-id-type="GRID">grid.16890.36</institution-id><institution>Department of Computing, </institution><institution>the Hong Kong Polytechnic University, </institution></institution-wrap>Hung Hom, Hong Kong </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>8</day>
      <month>5</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>8</day>
      <month>5</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2018</year>
    </pub-date>
    <volume>19</volume>
    <issue>Suppl 4</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. The Supplement Editors declare that they have no competing interests.</issue-sponsor>
    <elocation-id>60</elocation-id>
    <permissions>
      <copyright-statement>© The Author(s). 2018</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Protein secondary structure is the three dimensional form of local segments of proteins and its prediction is an important problem in protein tertiary structure prediction. Developing computational approaches for protein secondary structure prediction is becoming increasingly urgent.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We present a novel deep learning based model, referred to as CNNH_PSS, by using multi-scale CNN with highway. In CNNH_PSS, any two neighbor convolutional layers have a highway to deliver information from current layer to the output of the next one to keep local contexts. As lower layers extract local context while higher layers extract long-range interdependencies, the highways between neighbor layers allow CNNH_PSS to have ability to extract both local contexts and long-range interdependencies. We evaluate CNNH_PSS on two commonly used datasets: CB6133 and CB513. CNNH_PSS outperforms the multi-scale CNN without highway by at least 0.010 Q8 accuracy and also performs better than CNF, DeepCNF and SSpro8, which cannot extract long-range interdependencies, by at least 0.020 Q8 accuracy, demonstrating that both local contexts and long-range interdependencies are indeed useful for prediction. Furthermore, CNNH_PSS also performs better than GSM and DCRNN which need extra complex model to extract long-range interdependencies. It demonstrates that CNNH_PSS not only cost less computer resource, but also achieves better predicting performance.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">CNNH_PSS have ability to extracts both local contexts and long-range interdependencies by combing multi-scale CNN and highway network. The evaluations on common datasets and comparisons with state-of-the-art methods indicate that CNNH_PSS is an useful and efficient tool for protein secondary structure prediction.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Protein secondary structure</kwd>
      <kwd>Convolutional neural network</kwd>
      <kwd>Highway</kwd>
      <kwd>Local context</kwd>
      <kwd>Long-range interdependency</kwd>
    </kwd-group>
    <conference xlink:href="http://apbc2018.bio.keio.ac.jp/">
      <conf-name>The Sixteenth Asia Pacific Bioinformatics Conference</conf-name>
      <conf-acronym>APBC 2018</conf-acronym>
      <conf-loc>Yokohama, Japan</conf-loc>
      <conf-date>15-17 January 2018</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2018</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par22">The concept of secondary structure was first introduced by Linderstrøm-Lang at Stanford in 1952 [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>] to represent the three dimensional form of local segments of proteins. Protein secondary structure is defined by the pattern of hydrogen bonds between the amine hydrogen and carbonyl oxygen. There are two ways used for the classification of protein secondary structures: three-category classification(Q3) and eight-category classification(Q8). Q3 classifies target amino acid residues into helix(H), strand(E) and coil(C) while Q8 classifies target amino acid residues into (1) 3-turn helix(G), (2) 4-turn helix(H), (3) 5-turn helix(I), (4) hydrogen bonded turn(T), (5) extended strand in parallel and/or anti-parallel β-sheet conformation(E), (6) residue in isolated β-bridge (B), (7) bend(S) and (8) coil(C) [<xref ref-type="bibr" rid="CR3">3</xref>–<xref ref-type="bibr" rid="CR5">5</xref>]. Most protein secondary structure prediction studies have been focused Q3 prediction. Q8 prediction is more challenging and can reveal more structural details [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>], so we focus the Q8 prediction in this study.</p>
    <p id="Par23">Protein secondary structure prediction is secondary structure inference of protein fragments based on their amino acid sequence. In bioinformatics and theoretical chemistry, protein secondary structure prediction is very important for medicine and biotechnology, for example drug design [<xref ref-type="bibr" rid="CR8">8</xref>] and the design of novel enzymes. Since secondary structure can be used to find distant relationship for proteins with unalignable primary structures, incorporating both secondary structure information and simple sequence information can improve the accuracy of their alignment [<xref ref-type="bibr" rid="CR9">9</xref>]. Finally, protein secondary structure prediction also plays an important role in protein tertiary structure prediction. Protein secondary structure can determine the structure types of protein local fragments, so the freedom degree of protein local fragments in the tertiary structure can be reduced. Therefore accurate secondary structure prediction is potential for improving the accuracy of protein tertiary structure prediction [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR10">10</xref>].</p>
    <p id="Par24">Three experimental methods were proposed to determine secondary structures for proteins: far-ultraviolet circular dichroism, infrared spectroscopy and NMR spectrum. Far-ultraviolet circular dichroism predict pronounced double minimum at 208 and 222 nm as α-helical structure and single minimum at 204 nm or 217 nm as random-coil or β-sheet structure, respectively [<xref ref-type="bibr" rid="CR11">11</xref>]. Infrared spectroscopy uses the differences in the bond oscillations of amide groups for prediction [<xref ref-type="bibr" rid="CR12">12</xref>] while NMR spectrum predict protein secondary structure by using the estimated chemical shifts [<xref ref-type="bibr" rid="CR12">12</xref>]. As experimental methods are costly and the proteins with known sequence continue to outnumber the experimentally determined secondary structures, developing computational approaches for protein secondary structure prediction becomes increasingly urgent. Existing computational approaches for protein secondary structure prediction can be divided into 3 categories. The first category is statistical model based methods, which can date back to 1970s. Early, this category uses statistical models to analyze the probability of secondary structure elements for individual amino acid residue [<xref ref-type="bibr" rid="CR13">13</xref>]. Next, the statistical models were applied for the prediction of segments of 9–21 amino acids. For example, the GOR method [<xref ref-type="bibr" rid="CR14">14</xref>] used amino acid segment to predict the structure of its central residue. However, the performances (&lt; 60% Q3 accuracy) of this category of methods are far from practical application due to inadequate features.</p>
    <p id="Par25">Due to the lacking of inadequate features for the statistical model based methods, evolutionary information based methods have been proposed. These methods usually used the evolutionary information of proteins from a same structural family [<xref ref-type="bibr" rid="CR15">15</xref>] extracted by multiple-sequence alignment or position-specific scoring matrices (PSSM) [<xref ref-type="bibr" rid="CR16">16</xref>] from PSI-BLAST for prediction. An earlier evolutionary information based method was developed based on a two-layered feed-forward neural network, for which the evolutionary information in the form of multiple sequence alignment is used as input instead of single sequences [<xref ref-type="bibr" rid="CR15">15</xref>]. As SVM [<xref ref-type="bibr" rid="CR17">17</xref>] is significantly better than neural network in a wide range of pattern recognition problems [<xref ref-type="bibr" rid="CR18">18</xref>–<xref ref-type="bibr" rid="CR21">21</xref>], Hua and Sun first proposed a SVM classifier for protein secondary structure prediction [<xref ref-type="bibr" rid="CR22">22</xref>]. The input for SVM is evolutionary information in the form of multiple sequence alignment. Unbalanced data is a challenging problem in protein secondary structure prediction and existing methods lack the ability to handle it [<xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR24">24</xref>]. So Kim and Park proposed a new protein secondary structure prediction method, SVMpsi, by an improved SVM, which reduces the influence of imbalanced data by using different penalty parameters in the improved SVM [<xref ref-type="bibr" rid="CR23">23</xref>]. By using different penalty parameters, SVMpsi resolved the situation where the recall value of the smaller class is too small. Another SVM based method is PMSVM which was proposed by using dual-layer support vector machine (SVM) and evolutionary information in form of PSSMs [<xref ref-type="bibr" rid="CR25">25</xref>].</p>
    <p id="Par26">Protein sequence usually contains two types of sequence information: local context and long-range interdependencies [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR27">27</xref>]. Local contexts denote the correlations between residues with distance less than or equal to a predefined threshold while long-range correlation are the correlations between residues with distance more the threshold. Inspired by the success of convolutional neural networks (CNN) [<xref ref-type="bibr" rid="CR28">28</xref>] for local context extraction in natural language processing tasks [<xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR30">30</xref>], multi-scale CNN has been used to capture local contexts for protein secondary structure prediction [<xref ref-type="bibr" rid="CR31">31</xref>]. For example, Wang et al. proposed conditional neural fields (CNF) [<xref ref-type="bibr" rid="CR7">7</xref>] to extract local contexts for prediction by integrating a windows-based neural network with conditional random field (CRF). In addition to local contexts, long-range interdependencies also are important for protein secondary structure prediction(BRNN) [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR32">32</xref>–<xref ref-type="bibr" rid="CR34">34</xref>]. In order to extract both local contexts and long-range interdependencies for prediction, Zhou and Troyanskaya proposed GSN [<xref ref-type="bibr" rid="CR4">4</xref>] by using convolutional architecture and supervised generative stochastic network, which is a recently proposed deep learning technique [<xref ref-type="bibr" rid="CR26">26</xref>]. In addition to GSN, a novel deep convolutional and recurrent neural network (DCRNN) also has been proposed by Li and Yu [<xref ref-type="bibr" rid="CR27">27</xref>] for protein secondary structure prediction by extracting both local contexts and long-range interdependencies.</p>
    <p id="Par27">In summary, the statistical model based methods and evolutionary information based methods cannot extract local contexts and long-range interdependencies for prediction. For the deep learning based methods, some methods cannot extract both local contexts and long-range interdependencies for prediction. Although several methods can extract both local contexts and long-range interdependencies, such as GSN and DCRNN, they need extra complex models to extract long-range interdependencies, which are complex and time-consuming. In this paper, we propose a novel method, referred to as CNNH_PSS, by combining multi-scale CNN with highway network, which has ability to extract both local contexts and long-range interdependencies without needing extra models. CNNH_PSS consists of two parts: multi-scale CNN and fully connected and softmax layer. In the multi-scale CNN, any two neighbor convolutional layer contains a highway to deliver information from current layer to the output of the next one to keep local contexts. As the convolutional kernels in higher layer can extract long-range interdependencies by using the local contexts extracted by lower layers, thus with the layer number increasing, CNNH_PSS can extract long-range interdependencies covering more remote residues while keeping local contexts extracted by lower layers by using highway. So CNNH_PSS can extract both local contexts and long-range interdependencies covering very remote residues for prediction. The source code of our proposed method CNNH_PSS is provided for free access to the biological research community at <ext-link ext-link-type="uri" xlink:href="http://hlt.hitsz.edu.cn/CNNH_PSS/">http://hlt.hitsz.edu.cn/CNNH_PSS/</ext-link> and <ext-link ext-link-type="uri" xlink:href="http://119.23.18.63:8080/CNNH_PSS/">http://119.23.18.63:8080/CNNH_PSS/</ext-link>.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <p id="Par28">As shown by many recently published works [<xref ref-type="bibr" rid="CR35">35</xref>–<xref ref-type="bibr" rid="CR37">37</xref>], a complete prediction model in bioinformatics should contain the following four components: validation benchmark dataset(s), an effective feature extraction procedure, an efficient predicting algorithm, a set of fair evaluation criteria. In the following text, we will describe the four components of our proposed CNNH_PSS in details.</p>
    <sec id="Sec3">
      <title>Datasets</title>
      <p id="Par29">Two publicly available datasets: CB6133 and CB513 were used to evaluate the performance of our proposed method CNNH_PSS and compare with state-of-the-art methods.</p>
      <sec id="Sec4">
        <title>CB6133</title>
        <p id="Par30">CB6133 was produced by PISCES CullPDB [<xref ref-type="bibr" rid="CR38">38</xref>] and is a larger non-homologous protein dataset with known secondary structure for every protein. It contains 6128 proteins, in which 5600 proteins are training samples, 256 proteins are validation samples and 272 proteins are testing samples. This dataset is publicly available from literature [<xref ref-type="bibr" rid="CR4">4</xref>].</p>
      </sec>
      <sec id="Sec5">
        <title>CB513</title>
        <p id="Par31">CB513 is a public testing dataset and can be freely obtained from [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR39">39</xref>]. For the testing on CB513, CB6133 is used as the training dataset. As there exists redundancy between CB513 and CB6133, CB6133 is filtered by removing sequences having over 25% sequence similarity with sequences in CB513. After filtering, 5534 proteins left in CB6133 are used as training samples. Since GSN [<xref ref-type="bibr" rid="CR4">4</xref>] and DCRNN [<xref ref-type="bibr" rid="CR27">27</xref>] as well as other state-of-the-art methods [<xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR40">40</xref>] performed a validation on CB513 to get their best performance, we also perform a validation on CB513 to get the best performance of our method CNNH_PSS to make fair comparisons with them.</p>
      </sec>
    </sec>
    <sec id="Sec6">
      <title>Feature representation</title>
      <p id="Par32">Given a protein with <italic>L</italic> amino acid residues as <italic>X</italic> = <italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, <italic>x</italic><sub>3</sub>, ⋯, <italic>x</italic><sub><italic>L</italic></sub>, where <italic>x</italic><sub><italic>i</italic></sub>(∈ℝ<sup><italic>m</italic></sup>) is the <italic>m</italic>-dimensional feature vector of the <italic>i</italic><sup>th</sup> residue, the secondary structure prediction for this protein is formulated as determining <italic>S</italic> = <italic>s</italic><sub>1</sub>, <italic>s</italic><sub>2</sub>, <italic>s</italic><sub>3</sub>, ⋯, <italic>s</italic><sub><italic>L</italic></sub> for <italic>X</italic> where <italic>s</italic><sub><italic>i</italic></sub> is a Q8 secondary structure label. In this study,<italic>x</italic><sub><italic>i</italic></sub> is encoded by both sequence features and evolutionary information. Sequence features are used to specify the identity of the target residue. Two methods are used to encode sequence features: one hot and residue embedding. One hot encodes sequence features of each residue by a 21-dimension one-hot vector, in which only one element equals to 1 and the remaining elements are set to 0, where 21 denotes the 20 standard types of residues and one extra residue type which represents all non-standard residue types. However, one-hot vector is a sparse representation and unsuitable for measuring relation between different residues. In order to get dense representation of sequence features, an embedding technique in natural language processing is used to transform 21-dimensional one-hot vector to a 21-dimensional denser representation [<xref ref-type="bibr" rid="CR41">41</xref>]. The embedding technique maps words or phrases from the vocabulary to vectors of real numbers. Specifically, it maps words from a space with one dimension per word to a continuous vector space with much lower dimension. So the embedding technique provides a real value for every dimension. As the dimension of amino acid representation is already low, we only calculate a real value for every dimension by embedding technique and don’t decrease the dimension. The residue embedding in this paper is implemented by a feedforward neural network layer before multi-scale CNN in CNNH_PSS [<xref ref-type="bibr" rid="CR42">42</xref>].</p>
      <p id="Par33">Evolutionary information such as position-specific scoring matrix (PSSM) is considered as informative features for predicting secondary structure by previous research [<xref ref-type="bibr" rid="CR16">16</xref>]. PSSM is a common representation for evolutionary information and has been used in many bioinformatics studies including protein functionality annotation and protein structure prediction [<xref ref-type="bibr" rid="CR43">43</xref>–<xref ref-type="bibr" rid="CR47">47</xref>]. In this study, PSSM is calculated by PSI-BLAST [<xref ref-type="bibr" rid="CR48">48</xref>] against the UniRef90 database with E-value threshold 0.001 and 3 iterations. UniRef90 database contains the known protein sequences with sequence identity less than 90 from almost all known species. So the PSSM calculated from UniRef90 database contains the common sequence information among the known protein sequences of different species. Overtime, scientists have reached a consensus that a protein’s structure primarily depends on its amino acid sequence and concluded that the local and long-range interaction are a cause of protein second and tertiary structure. Based on this hypothesis, we can deduce that proteins with similar amino acid sequence tend to have similar secondary structure sequence. Therefore, the common sequence information contained by PSSM can contribute to the secondary structure prediction. For a protein with length <italic>L</italic>, PSSM is usually represented as a matrix with <italic>L</italic> × 21 dimensions where 21 denotes the 20 standard types of residues and one extra residue type which represents all non-standard residue types. Before PSSMs are used inputs for CNNH_PSS, they need to be transformed to 0–1 range by the sigmoid function. By concatenating sequence features and evolutional information, each residue in protein sequences can be encoded by a feature vector with dimension of 42.</p>
    </sec>
    <sec id="Sec7">
      <title>Multi-scale CNN with highway between neighbor layers</title>
      <p id="Par34">In CNN model, a kernel can examine a local patch in input sequence and extract interdependence among the residues contained in the local patch. With stacking of convolutional layers, the kernels for deep layers have ability to cover correlations among more spread-out residues in the input sequence. So, CNN model with more number of convolutional layers have the ability to extracted long-range interdependencies between residues with more large distance. However, with the number of layers increasing, CNN model will lose the local contexts extracted by lower layers. In this paper, we propose a novel method, referred to as CNNH_PSS, to resolve this problem. CNNH_PSS contains a highway between any two neighbor convolutional layers in multi-scale CNN. As the number of convolutional layers increases, CNNH_PSS can not only extract long-range interdependencies by higher layers, but also obtain the local contexts extracted by lower layers through highway. The frame of CNNH_PSS is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Figure <xref rid="Fig1" ref-type="fig">1</xref> shows that CNNH_PSS contains three parts: input section, multi-scale CNN with highway and output section. In the input section, <italic>x</italic><sub><italic>i</italic></sub> ∈ R<sup><italic>m</italic></sup> denotes the feature vector of the <italic>i</italic><sup>th</sup> residue in protein, which is the concatenation of sequence features and evolutional information. Thus a protein of length <italic>L</italic> is encoded as a <italic>L</italic> × <italic>m</italic> matrix <italic>x</italic><sub>1 : <italic>L</italic></sub> = [<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, ⋯, <italic>x</italic><sub><italic>L</italic></sub>]<sup>T</sup>, where <italic>L</italic> and <italic>m</italic> denote the length of protein and the number features used to encode residues, respectively. In this study, <italic>m</italic> equals to 42. In order to keep the output of convolutional layer have the same height with the input, we need to pad ⌊<italic>h</italic>/2⌋ and ⌊(<italic>h</italic> − 1)/2⌋ <italic>m</italic>-dimensional zero vectors to the head and the tail of the input <italic>x</italic><sub>1 : <italic>L</italic></sub>, respectively, where <italic>h</italic> is the length of convolutional kernels in the convolutional layer. The second section contains two parts: multi-scale CNN and highway, where the multi-scale CNN contains <italic>n</italic> convolutional layers. In the (<italic>t</italic> − 1)<sup>th</sup> layer, the convolution operation of the <italic>k</italic><sup>th</sup> kernel <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {w}_k^{t-1}\in {\mathrm{R}}^{h\times m} $$\end{document}</tex-math><mml:math id="M2" display="inline"><mml:msubsup><mml:mi>w</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="normal">R</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2018_2067_Article_IEq1.gif"/></alternatives></inline-formula> executed on protein fragment <italic>x</italic><sub><italic>i</italic> : <italic>i</italic> + <italic>h</italic> − 1</sub> is expressed as<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {c}_{k,i}^{t-1}={f}^{t-1}\left({w}_k^{t-1}\cdot {x}_{i:i+h-1}+{b}_k^{t-1}\right) $$\end{document}</tex-math><mml:math id="M4" display="block"><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>h</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2018_2067_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>h</italic> is the length of convolution kernel,<inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {b}_k^{t-1} $$\end{document}</tex-math><mml:math id="M6" display="inline"><mml:msubsup><mml:mi>b</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2018_2067_Article_IEq2.gif"/></alternatives></inline-formula> is the bias of the <italic>k</italic><sup>th</sup> kernel, <italic>f</italic> is activation function and <italic>x</italic><sub><italic>i</italic> : <italic>i</italic> + <italic>h</italic> − 1</sub> denotes the protein fragment <italic>x</italic><sub><italic>i</italic></sub>, <italic>x</italic><sub><italic>i</italic> + 1</sub>, <italic>x</italic><sub><italic>i</italic> + 2</sub>, ⋯, <italic>x</italic><sub><italic>i</italic> + <italic>h</italic> − 1</sub>. Through executing convolution operation of the <italic>k</italic><sup>th</sup> kernel on all fragments with length <italic>h</italic> of the padded input, we get a novel feature vector<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {c}_k^{t-1}={\left[{c}_{k,1}^{t-1},{c}_{k,2}^{t-1},{c}_{k,3}^{t-1},\cdots, {c}_{k,L}^{t-1}\right]}^{\mathrm{T}} $$\end{document}</tex-math><mml:math id="M8" display="block"><mml:msubsup><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mfenced close="]" open="[" separators=",,,,"><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⋯</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mfenced><mml:mi mathvariant="normal">T</mml:mi></mml:msup></mml:math><graphic xlink:href="12859_2018_2067_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><fig id="Fig1"><label>Fig. 1</label><caption><p>The frame of CNNH_PSS</p></caption><graphic xlink:href="12859_2018_2067_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par35">Suppose we have <italic>d</italic> kernels in the convolutional layer, thus we can get <italic>d</italic> novel features vectors. By concatenating the <italic>d</italic> novel feature vectors, we can get a novel feature matrix with dimension <italic>L</italic> × <italic>d</italic><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {c}^{t-1}=\left[{c}_1^{t-1},{c}_2^{t-1},{c}_3^{t-1},\cdots, {c}_L^{t-1}\right] $$\end{document}</tex-math><mml:math id="M10" display="block"><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfenced close="]" open="[" separators=",,,,"><mml:msubsup><mml:mi>c</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>c</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>c</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⋯</mml:mo><mml:msubsup><mml:mi>c</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mfenced></mml:math><graphic xlink:href="12859_2018_2067_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par36">This novel feature matrix is used as the input of the next convolutional layer. If there are <italic>n</italic> convolutional layers and <italic>θ</italic><sub><italic>t</italic></sub> is used to denote the kernels and the bias of the <italic>t</italic><sup><italic>th</italic></sup> convolutional layer, then the output of the <italic>n</italic><sup><italic>th</italic></sup> convolutional layer is<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {c}^n={f}_{\theta_n}^n\left({f}_{\theta_{n-1}}^{n-1}\left(\cdots {f}_{\theta_1}^1\left({x}_{1:L}\right)\right)\right) $$\end{document}</tex-math><mml:math id="M12" display="block"><mml:msup><mml:mi>c</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mi>n</mml:mi></mml:msubsup><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mfenced close=")" open="("><mml:mrow><mml:mo>⋯</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mn>1</mml:mn></mml:msubsup><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2018_2067_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par37">Finally, the output of the <italic>n</italic><sup><italic>th</italic></sup> convolutional layer is used as the input of the fully connected softmax layer for prediction<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {y}_i= argmax\left(w\cdot {c_i}^n+b\right) $$\end{document}</tex-math><mml:math id="M14" display="block"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">argmax</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:mi>w</mml:mi><mml:mo>⋅</mml:mo><mml:msup><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>n</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2018_2067_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <italic>w</italic> and <italic>b</italic> is the weight and bias of the fully connected softmax layer, respectively. <italic>c</italic><sub><italic>i</italic></sub><sup><italic>n</italic></sup> is the feature vector of the <italic>i</italic><sup><italic>th</italic></sup> outputted by the <italic>n</italic><sup><italic>th</italic></sup> convolutional layer and <italic>y</italic><sub><italic>i</italic></sub> is its predicted secondary structure.</p>
      <p id="Par38">CNN has achieved huge progress in many tasks of image processing filed, one common sense is that the successes of CNN are attributed to the multiple convolutional layers in CNN, because CNN with more number of layers can extract correlations covering more residues. However, with the increasing of number of layers in CNN, the information communication between layers will become more difficult and the gradient will disappear [<xref ref-type="bibr" rid="CR49">49</xref>]. Furthermore, the local contexts extracted by lower layers also will lose. Srivastava et al. [<xref ref-type="bibr" rid="CR49">49</xref>] have proposed highway network to resolve these problems. So CNNH_PSS incorporates highway network and multi-scale CNN to extract both local contexts and long-range interdependencies for secondary structure prediction.</p>
      <p id="Par39">In CNNH_PSS, each convolutional layer except the last layer has three accesses to the next layer (shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>). Two accesses are used to deliver information from the current layer to the output and convolution kernels of the next layer, respectively. The other one is a weight used to determine the share of information in for information from highway. So the output <italic>c</italic><sup><italic>t</italic></sup> of the <italic>t</italic><sup><italic>th</italic></sup> convolutional layer is the weighted sum of the information delivered by highway from last layer and that outputted by the convolution kernels of current layer<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {z}_t=\delta \left({w}^z{c}^{t-1}\right) $$\end{document}</tex-math><mml:math id="M16" display="block"><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mi>z</mml:mi></mml:msup><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2018_2067_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {c}^t=\left(1-{z}_t\right)\times {f}_{\theta_t}^t\left({c}^{t-1}\right)+{z}_t\times {c}^{t-1} $$\end{document}</tex-math><mml:math id="M18" display="block"><mml:msup><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>t</mml:mi></mml:msubsup><mml:mfenced close=")" open="("><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math><graphic xlink:href="12859_2018_2067_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where <italic>δ</italic>(⋅) is <italic>sigmoid</italic> function,<italic>z</italic><sub><italic>t</italic></sub> is the weight of the highway and <inline-formula id="IEq3"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {f}_{\theta_t}^t\left(\cdot \right) $$\end{document}</tex-math><mml:math id="M20" display="inline"><mml:msubsup><mml:mi>f</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>t</mml:mi></mml:msubsup><mml:mfenced close=")" open="("><mml:mo>⋅</mml:mo></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2018_2067_Article_IEq3.gif"/></alternatives></inline-formula> is the convolution operation of current convolutional layer. So the output of the<italic>t</italic><sup><italic>th</italic></sup>convolutional layer contains two portion: information from the (<italic>t</italic> − 1)<sup><italic>th</italic></sup>convolutional layer delivered by highway and that outputted by the convolution kernels of current layer.</p>
    </sec>
  </sec>
  <sec id="Sec8">
    <title>Results</title>
    <p id="Par40">The purpose of the evaluation is to examine the effectiveness of our proposed CNNH_PSS over other methods. Four sets of evaluations are conducted here. The first experiment evaluates the performance of multi-scale CNN on CB6133 and CB513. The second experiment evaluates our proposed method CNNH_PSS on CB6133 and CB513. The third experiment compares CNNH_PSS with state-of-the-art methods. Finally, based on CB6133, we analyze the local contexts and long-range interdependencies learned by CNNH_PSS. As we mainly focus the Q8 prediction of protein secondary structure in this study, the performances of prediction methods are measured by Q8 accuracy [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR27">27</xref>]. The Q8 accuracy is the percentage of the amino acid residues for which the predicted secondary structure labels are correct. The source code of our proposed method CNNH_PSS is provide for free access at <ext-link ext-link-type="uri" xlink:href="http://hlt.hitsz.edu.cn/CNNH_PSS">http://hlt.hitsz.edu.cn/CNNH_PSS/</ext-link>.</p>
    <sec id="Sec9">
      <title>The performance of multi-scale CNN model</title>
      <p id="Par41">In this section, multi-scale CNN is used to predict secondary structure for proteins. The hyper-parameters of the multi-scale CNN for protein secondary structure prediction in this study are listed in Table <xref rid="Tab1" ref-type="table">1</xref>. Note that three kernel lengths are used in the multi-scale CNN model and 80 kernels are used for each kernel length. To conveniently encode and process protein sequences, the length of all protein sequences are normalized to 700. When sequences are shorter than 700, they will be padded with zero vectors. And when sequences are longer than 700, they will be truncated. In order to get best performance, we need to determine how many convolutional layers the multi-scale CNN should contains. We conduct experiments to evaluate the performances of the multi-scale CNNs with different number of convolutional layers on CB513. The performances are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, where the <italic>x</italic>-axis is the number of epochs used to train multi-scale CNN and the <italic>y</italic>-axis is Q8 accuracy. Fig. <xref rid="Fig2" ref-type="fig">2</xref> shows the performances for models with number of convolutional layers from 1 to 5. From this figure, we see that the model with 3 convolutional layers gets the best accuracy. When the number of convolutional layers is increased to 4 or 5, the accuracy is decreased obviously. The main reason for this phenomenon may be the loss of extracted local contexts with the increasing of the number of convolutional layers in CNN. With the increasing of the number of convolutional layers in CNN, the correlations extracted by higher can cover more residues so that they contains interdependencies between more remote residues. When the number of convolutional layers is increased to 3, the CNN may achieve both local contexts and long-range interdependencies, which is validated by that the CNN with 3 convolutional layers gets the best accuracy in our problem. However, when the number of convolutional layers is more than 3, most local contexts extracted by lower layers are lost in the transport processes of information cross layers, causing the relationships outputted by the last layer in CNN contains less and less local contexts. So the predicting accuracy starts to decrease when the number of convolutional layers is more than 3.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Hyper-parameters of multi-scale CNN</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Layer</th><th>Hyper-parameter</th><th>Value</th></tr></thead><tbody><tr><td rowspan="7">Multi-scale CNN</td><td>Kernel length</td><td>[<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR11">11</xref>]</td></tr><tr><td>Number of kernels</td><td>80 for each kernel length</td></tr><tr><td>Batch size</td><td>50</td></tr><tr><td>Learning rate</td><td>2e-3</td></tr><tr><td>Regularizer</td><td>5e-5</td></tr><tr><td>Decay rate</td><td>0.05</td></tr><tr><td>Activation function</td><td>ReLU</td></tr></tbody></table></table-wrap><fig id="Fig2"><label>Fig. 2</label><caption><p>The performance of multi-scale CNN with different number of convolutional layers</p></caption><graphic xlink:href="12859_2018_2067_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par42">The performances of multi-scale CNN with 3 convolutional layers on CB6133 and CB513 are shown in Table <xref rid="Tab2" ref-type="table">2</xref>, where two sequence features encoding methods for residues are evaluated: one hot and residue embedding. Table <xref rid="Tab2" ref-type="table">2</xref> shows that residue embedding outperforms one hot on both CB6133 and CB513 by at 0.004 Q8 accuracy, indicating that residue embedding is a better encoding method for sequence features of residues. In the following text, we will use residue embedding method to encode sequence features in the multi-scale CNN model and our proposed method CNNH_PSS.<table-wrap id="Tab2"><label>Table 2</label><caption><p>The Q8 accuracy of Multi-scale CNN with 3 convolutional layers</p></caption><table frame="hsides" rules="groups"><thead><tr><th>datasets</th><th>CB6133</th><th>CB513</th></tr></thead><tbody><tr><td>Multi-scale CNN(one hot)</td><td>0.721</td><td>0.689</td></tr><tr><td>Multi-scale CNN(embedding)</td><td><italic>0.729</italic></td><td><italic>0.693</italic></td></tr></tbody></table><table-wrap-foot><p>The data in italic denote the best performance</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec10">
      <title>The performance of CNNH_PSS</title>
      <p id="Par43">Local contexts are the relationships among residues at close range while long-range interdependencies are the relationships among remote residues. As there is no strict bounds between local contexts and long-range interdependencies, we specify the information extracted by the first convolutional layers as local contexts and that extracted by all other layers as long-range interdependencies in this study. In CNNH_PSS, any two neighbor convolutional layers have a highway to deliver information from current convolutional layer to the output of the next one, so it can make sure that the output of each layer contains a portion of the local contexts. Furthermore, the convolution kernels of each layer except the first one can extract long-range interdependencies by using the information from previous layer. With the increasing of the number of convolutional layers, CNNH_PSS can extract long-range interdependencies between more remote residues. Therefore, the output the last convolutional layer in CNNH_PSS contains two portion of information: local contexts extracted by the first layer and long-range interdependencies extracted by all other layers.</p>
      <p id="Par44">Similarly, we evaluate the performances of our proposed method CNNH_PSSs with different number of convolutional layers on CB513. The performances are shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. Figure <xref rid="Fig3" ref-type="fig">3</xref> shows that CNNH_PSS achieves the best performance when the number of convolutional layers is 5. When number of convolutional layers is more than 5, the performance of our method starts to decrease. So CNNH_PSS with 5 convolutional layers is used in the following text. Comparing the multi-scale CNN model with 3 convolutional layers described in above section, our proposed method CNNH_PSS not only contains a highway between any two neigbouring layers, but also have more number of convolutional layers. It makes sure that CNNH_PSS with 5 layers can not only extract local contexts, but also capture long-range interdependencies between more remote residues than the multi-scale CNN model with 3 layers. The performances of CNNH_PSS and the multi-scale CNN model on CB6133 and CB513 are shown in Table <xref rid="Tab3" ref-type="table">3</xref>. Table <xref rid="Tab3" ref-type="table">3</xref> shows that our proposed method CNNH_PSS outperforms the multi-scale CNN model by 0.011 Q8 accuracy on CB6133 and 0.010 Q8 accuracy on CB513. The outperformance of CNNH_PSS over the multi-scale CNN model on both CB6133 and CB513 validates that the highway in CNN indeed are useful for protein secondary structure prediction.<fig id="Fig3"><label>Fig. 3</label><caption><p>The performance of CNNH_PSS with different number of convolutional layers</p></caption><graphic xlink:href="12859_2018_2067_Fig3_HTML" id="MO3"/></fig><table-wrap id="Tab3"><label>Table 3</label><caption><p>The Q8 accuracy of CNNH_PSS</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>CB6133</th><th>CB513</th></tr></thead><tbody><tr><td>Multi-scale CNN</td><td>0.729</td><td>0.693</td></tr><tr><td>CNNH_PSS</td><td><italic>0.740</italic></td><td><italic>0.703</italic></td></tr></tbody></table><table-wrap-foot><p>The data in italic denote the best performance</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec11">
      <title>Comparison with state-of-the-art methods</title>
      <p id="Par45">Protein secondary structure prediction is an important problem in bioinformatics and critical for analyzing protein function and applications like drug design. So many state-of-the-art methods have been proposed for the prediction. SSpro8 is a prediction method proposed by Pollastri et al. [<xref ref-type="bibr" rid="CR6">6</xref>] by combining bidirectional recurrent neural networks (RNN) and PSI-BLAST-derived profiles. CNF is a Conditional Neural Fields based method which was proposed by Wang et al. [<xref ref-type="bibr" rid="CR40">40</xref>], which can not only extract relationships between sequence features of residues and their secondary structures, but also capture local contexts [<xref ref-type="bibr" rid="CR40">40</xref>]. Later, an extension version of CNF (DeepCNF) was proposed by Wang et al. [<xref ref-type="bibr" rid="CR31">31</xref>] using deep learning extension of conditional neural fields, which is an integration of conditional neural fields and shallow neural networks. It can extract both complex sequence-structure relationship and interdependency between adjacent SS labels. These three methods can only extract local contexts for prediction. Furthermore, GSN is a prediction method proposed by Zhou and Troyanskaya [<xref ref-type="bibr" rid="CR4">4</xref>] using supervised generative stochastic network and convolutional architectures. Supervised generative stochastic network is a recently proposed deep learning technique [<xref ref-type="bibr" rid="CR26">26</xref>], which is well suitable for extracting local contexts and also can capture some long-range interdependencies. Finally, DCRNN is the best performing method up to now, which was recently proposed by Li and Yu [<xref ref-type="bibr" rid="CR27">27</xref>] using multi-scale CNN and three staked bidirectional gate recurrent units (BGRUs) [<xref ref-type="bibr" rid="CR50">50</xref>]. GSN and DCRNN can extract both local contexts and long-range interdependencies. We first compare our proposed method CNNH_PSS with the three state-of-the-art methods which only can extract local contexts on CB513. The performances of these three methods and our method on CB513 are listed in Table <xref rid="Tab4" ref-type="table">4</xref>. Table <xref rid="Tab4" ref-type="table">4</xref> shows that CNNH_PSS outperforms the three methods by at least 0.020 Q8 accuracy. The outperformance of CNNH_PSS over the three state-of-the-art methods which only can extract local contexts indicates that the long-range interdependencies extracted by CNNH_PSS are indeed useful for protein secondary structure prediction.<table-wrap id="Tab4"><label>Table 4</label><caption><p>The Q8 accuracy of CNNH_PSS and state-of-the-art methods containing only local contexts</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>CB513</th></tr></thead><tbody><tr><td>SSpro8</td><td>0.511</td></tr><tr><td>CNF</td><td>0.633</td></tr><tr><td>DeepCNF</td><td>0.683</td></tr><tr><td>CNNH_PSS</td><td><italic>0.703</italic></td></tr></tbody></table><table-wrap-foot><p>The data in italic denote the best performance</p></table-wrap-foot></table-wrap></p>
      <p id="Par46">Then, we compare our method CNNH_PSS to GSN and DCRNN by both CB6133 and CB513, which also can extract both local contexts and long-range interdependencies. The performances of these two methods and our method on CB6133 and CB513 are listed in Table <xref rid="Tab5" ref-type="table">5</xref>. The table shows that CNNH_PSS performs better than GSN and DCRNN by at least 0.008 Q8 accuracy on CB6133 and 0.009 Q8 accuracy on CB513. GSN and DCRNN consist of CNN for local context extraction and extra models for long-range interdependencies extraction. As the extra models of the two methods are complex and time-consuming, these two methods need consume much computer resource. Trained on GTX TITANX GPU, CNNH_PSS tends to converge after only a half hour while DCRNN needs more than 24 h to converge [<xref ref-type="bibr" rid="CR27">27</xref>]. So CNNH_PSS is almost 50 times faster than DCRNN. Although we do not known the exact running time for GSN, we know that GSN needs to be trained for 300 epochs [<xref ref-type="bibr" rid="CR4">4</xref>] while CNNH_PSS tends to converge after training for less than 35 epochs shown by Fig. <xref rid="Fig3" ref-type="fig">3</xref>. It means that CNNH_PSS is almost 9 times faster then GSN. Therefore, the outperformance of our method over GSN and DCRNN further demonstrates that CNNH_PSS can not only cost less computer resource but also achieves better predicting performance.<table-wrap id="Tab5"><label>Table 5</label><caption><p>The Q8 accuracy of CNNH_PSS and state-of-the-art methods containing both local contexts and long-range interdependencies</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>CB6133</th><th>CB513</th></tr></thead><tbody><tr><td>GSN</td><td>0.721</td><td>0.664</td></tr><tr><td>DCRNN</td><td>0.732</td><td>0.694</td></tr><tr><td>CNNH_PSS</td><td><italic>0.740</italic></td><td><italic>0.703</italic></td></tr></tbody></table><table-wrap-foot><p>The data in italic denote the best performance</p></table-wrap-foot></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Discussion</title>
    <p id="Par47">The advantage of our proposed method CNNH_PSS over state-of-the-art methods is that it can extract both local contexts and long-range interdependencies by using multi-scale CNN with highway. In CNNH_PSS, any two neighbor convolutional layers have a highway to deliver information from current convolutional layer to the output of the next one and each layer except the first one have convolution kernels to extract long-range interdependencies by using the information from previous layer. In this section, we use CNNH_PSS with 5 convolutional layers and the kernel length of 11 to introduce the process for local contexts and long-range interdependencies extraction, which is shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. First, the target protein are inputted to the first layer and the convolution kernels in the first layer extract local contexts from the inputted protein. So the output of first layer contains local contexts among 11 residues. Then the information in the output of the first layer are delivered to the output of the second layer by two ways: highway between them and the convolution kernels in the second layer. Finally, the output of the second layer is the weighted sum of the information transmitted by the two way. As the convolution kernels in the secondary layer can extract relationships among 21 residues, the output of the second layer contains both local contexts among 11 residues and long-range interdependencies among 21 residues. And so on, the output of the fifth layer contains two parts. One part is the information from the output of the fourth layer by the highway between them, which contains local contexts among 11 and long-range interdependencies among 21, 31 and 41 residues. The other part is the information outputted by the convolutional kernels of current layer, which contains long-range interdependencies among 51 residues. Therefore CNNH_PSS can output local contexts among 11 and long-range interdependencies among 21, 31, 41 and 51 residues while the multi-scale CNN with the same number of convolutional layer as CNNH_PSS outputs only long-range interdependencies among 51 residues.<fig id="Fig4"><label>Fig. 4</label><caption><p>Extraction process for local contexts and long-range interdependencies</p></caption><graphic xlink:href="12859_2018_2067_Fig4_HTML" id="MO4"/></fig></p>
    <p id="Par48">In order to demonstrate the importance of learned local contexts and long-range interdependencies in protein secondary structure prediction, we show the local contexts and the long-range interdependencies learned in a representative protein PDB 154 L [<xref ref-type="bibr" rid="CR51">51</xref>], which obtained from the publicly available protein data bank [<xref ref-type="bibr" rid="CR52">52</xref>]. The learned local contexts and long-range interdependencies by CNNH_PSS in protein PDB 154 L are shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. In Fig. <xref rid="Fig5" ref-type="fig">5</xref>, the five rows correspond to the predicted results by CNNH_PSS with 5 layers, that by CNNH_PSS with 3 layers, that by the multi-scale CNN with 5 layers, real secondary structures and protein sequence, respectively. The reason for why CNNH_PSS with 5 layers, CNNH_PSS with 3 layers and the multi-scale CNN with 5 layers are selected for comparison is that CNNH_PSS with 5 layers can extracted local contexts and long-range interdependencies among up to 51 residues while CNNH_PSS with 3 layers cannot extracted long-range interdependencies among more than 41 residues and the multi-scale CNN with 5 layers cannot extract local contexts. Fig. <xref rid="Fig5" ref-type="fig">5</xref> shows three instances for long-range interdependencies: (1) interdependencies among 24th, 25th and 60th amino acid; (2) that between 60th and 100th and (3) that between 85th and 131th amino acid. As the number of residues covered by these three learned interdependencies is more than 31 and less than 51 residues, both CNNH_PSS with 5 layers and the multi-scale CNN with 5 layers can extract them for correct prediction them while CNNH_PSS with 3 layers cannot capture them. So both CNNH_PSS with 5 layers and the multi-scale CNN with 5 layers make correct prediction for the 24th, 25th, 85th, 100th and 131th residues while CNNH_PSS with 3 layers cannot make correct predictions for them. It validates that CNNH_PSS with more layers indeed can extract long-range interdependencies between more remote residues.<fig id="Fig5"><label>Fig. 5</label><caption><p>Prediction results of 154 L by CNNH_PSS</p></caption><graphic xlink:href="12859_2018_2067_Fig5_HTML" id="MO5"/></fig></p>
    <p id="Par49">Furthermore, Fig. <xref rid="Fig4" ref-type="fig">4</xref> also shows 4 instances for learned local contexts: (1) contexts from 31th to 35th residues; (2) that from 111th to 115th residues; (3) that from 146th to 149th residues and (4) that from 158th to 163th residues. Both CNNH_PSS with 3 layers and that with 5 layers can learn these four contexts so that the secondary structures of all the residues in the learned contexts can be correctly predicted. However, the multi-scale CNN with 5 layers cannot learn these four contexts. So it cannot predict the secondary structures correctly for these residues. It validates that the highways in the CNNH_PSS indeed can be used to extract local contexts for prediction.</p>
  </sec>
  <sec id="Sec13">
    <title>Conclusion</title>
    <p id="Par50">Protein secondary structure prediction is an important problem in bioinformatics and critical for analyzing protein function and applications like drug design. Several experimental methods have been proposed to determined the secondary structures for proteins, such as far-ultraviolet circular dichroism, infrared spectroscopy and NMR spectrum. However, experimental methods usually are costly and time-consuming. And the proteins with known sequence continues to outnumber the experimentally determined secondary structures. So developing computational approaches that can accurately handle large amount of data becomes increasingly urgent. However, most of these proposed methods cannot extract either local contexts or long-range interdependencies. Although GSM and DCRNN can extract both of them, they are build by combing CNN architecture and extra complex models. Yet CNNH_PSS is developed by only multi-scale CNN with highway. So comparing to GSM and DCRNN, CNNH_PSS may cost less computer resource. We evaluate CNNH_PSS on two commonly used datasets: CB6133 and CB513. CNNH_PSS outperforms the multi-scale CNN without highway on both datasets, which demonstrates that the extracted local contexts through highways are indeed useful for protein secondary structure prediction. CNNH_PSS also outperforms CNF and DeepCNF as well as SSpro8 on CB513, which cannot extract long-range interdependencies. It indicates that long-range interdependencies extracted by CNNH_PSS are useful for protein secondary structure prediction. Furthermore, CNNH_PSS performs better than GSM and DCRNN, demonstrating that CNNH_PSS can not only cost less computer resource but also achieves better predicting performance than state-of-the-art methods. We also analyze the local contexts and long-range interdependencies learned by CNNH_PSS in protein PDB 154 L and show their roles in protein secondary structure prediction. X-ray diffraction crystallography and NMR can measure the structures of proteins, so these methods can be used to calculate the distances between any two residues in a protein sequence. By analyzing the second structures of long-range residues but with short distance in space and short-range residues, we can further demonstrate the importance of long-range interdependencies and local contexts for second structure prediction. Therefore, our future works will validate the conclusions achieved in this paper by using these experimental methods.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>B</term>
        <def>
          <p id="Par4">residue in isolated β-bridge</p>
        </def>
      </def-item>
      <def-item>
        <term>C</term>
        <def>
          <p id="Par5">coil</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p id="Par6">convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>CRF</term>
        <def>
          <p id="Par7">conditional random field</p>
        </def>
      </def-item>
      <def-item>
        <term>E</term>
        <def>
          <p id="Par8">extended strand in parallel and/or anti-parallel β-sheet conformation</p>
        </def>
      </def-item>
      <def-item>
        <term>G</term>
        <def>
          <p id="Par9">3-turn helix</p>
        </def>
      </def-item>
      <def-item>
        <term>GSN</term>
        <def>
          <p id="Par10">generative stochastic network</p>
        </def>
      </def-item>
      <def-item>
        <term>H</term>
        <def>
          <p id="Par11">4-turn helix</p>
        </def>
      </def-item>
      <def-item>
        <term>I</term>
        <def>
          <p id="Par12">5-turn helix</p>
        </def>
      </def-item>
      <def-item>
        <term>NMR</term>
        <def>
          <p id="Par13">nuclear magnetic resonance</p>
        </def>
      </def-item>
      <def-item>
        <term>PDB</term>
        <def>
          <p id="Par14">Protein Data Bank</p>
        </def>
      </def-item>
      <def-item>
        <term>PSSM</term>
        <def>
          <p id="Par15">Position Specific Score Matrix</p>
        </def>
      </def-item>
      <def-item>
        <term>Q3</term>
        <def>
          <p id="Par16">three-category classification</p>
        </def>
      </def-item>
      <def-item>
        <term>Q8</term>
        <def>
          <p id="Par17">eight-category classification</p>
        </def>
      </def-item>
      <def-item>
        <term>RNN</term>
        <def>
          <p id="Par18">recurrent neural networks</p>
        </def>
      </def-item>
      <def-item>
        <term>S</term>
        <def>
          <p id="Par19">bend</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p id="Par20">Support Vector Machine</p>
        </def>
      </def-item>
      <def-item>
        <term>T</term>
        <def>
          <p id="Par21">hydrogen bonded turn</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
    <sec id="FPar1">
      <title>Funding</title>
      <p id="Par51">Publication of this article was supported by the National Natural Science Foundation of China U1636103, 61632011 and Shenzhen Foundational Research Funding 20170307150024907.</p>
    </sec>
    <sec id="FPar2">
      <title>Availability of data and materials</title>
      <p id="Par52">All data generated or analysed during this study are included in this published article. The datasets analysed during this study and the source code for CNNH_PSS can be accessed from our webserver of this paper <ext-link ext-link-type="uri" xlink:href="http://hlt.hitsz.edu.cn/CNNH_PSS/">http://hlt.hitsz.edu.cn/CNNH_PSS/</ext-link> and <ext-link ext-link-type="uri" xlink:href="http://119.23.18.63:8080/CNNH_PSS/">http://119.23.18.63:8080/CNNH_PSS/</ext-link>.</p>
    </sec>
    <sec id="FPar3">
      <title>About this supplement</title>
      <p id="Par53">This article has been published as part of <italic>BMC Bioinformatics</italic> Volume 19 Supplement 4, 2018: Selected articles from the 16th Asia Pacific Bioinformatics Conference (APBC 2018): bioinformatics. The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-18-supplement-17">https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-18-supplement-17</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>JZ initiated and designed the study. RX made substantial contributions to acquisition of data, analysis and interpretation of data. JZ drafted the manuscript. RX and QL involved in drafting the manuscript or revising it. ZZ and HW provided valuable insights on biomolecular interactions and systems biology modeling, participated in result interpretation and manuscript preparation. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="COI-statement">
    <sec id="FPar4">
      <title>Competing interest</title>
      <p id="Par54">The authors declare that they have no conflicting interests.</p>
    </sec>
    <sec id="FPar5">
      <title>Ethics approval and consent to participate</title>
      <p id="Par55">Not applicable.</p>
    </sec>
    <sec id="FPar6">
      <title>Consent for publication</title>
      <p id="Par56">Not applicable.</p>
    </sec>
    <sec id="FPar7">
      <title>Publisher’s Note</title>
      <p id="Par57">Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </sec>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Linderstrøm-Lang KU. Lane medical lectures: proteins and enzymes. California: Stanford University Press; 1952. p. 115.</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schellman</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Schellman</surname>
            <given-names>CG</given-names>
          </name>
        </person-group>
        <article-title>Kaj Ulrik Linderstrøm-Lang (1896-1959)</article-title>
        <source>Protein Sci</source>
        <year>1997</year>
        <volume>6</volume>
        <issue>5</issue>
        <fpage>1092</fpage>
        <lpage>1100</lpage>
        <pub-id pub-id-type="doi">10.1002/pro.5560060516</pub-id>
        <?supplied-pmid 9144781?>
        <pub-id pub-id-type="pmid">9144781</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kabsch</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features</article-title>
        <source>Biopolymers</source>
        <year>1983</year>
        <volume>22</volume>
        <issue>12</issue>
        <fpage>2577</fpage>
        <lpage>2637</lpage>
        <pub-id pub-id-type="doi">10.1002/bip.360221211</pub-id>
        <?supplied-pmid 6667333?>
        <pub-id pub-id-type="pmid">6667333</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Troyanskaya</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Deep supervised and convolutional generative stochastic network for protein secondary structure prediction</article-title>
        <source>Proceedings of the 31st international conference on machine learning (ICML-14)</source>
        <year>2014</year>
        <fpage>745</fpage>
        <lpage>753</lpage>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yaseen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Template-based c8-scorpion: a protein 8-state secondary structure prediction method using structural information and context-based features</article-title>
        <source>BMC Bioinformatics</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>Suppl 8</issue>
        <fpage>S3</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-15-S8-S3</pub-id>
        <?supplied-pmid 25080939?>
        <pub-id pub-id-type="pmid">25080939</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pollastri</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Przybylski</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Improving the prediction of protein secondary structure in three and eight classes using recurrent neural networks and profiles</article-title>
        <source>Proteins: Structure, Function, and Bioinformatics</source>
        <year>2002</year>
        <volume>47</volume>
        <issue>2</issue>
        <fpage>228</fpage>
        <lpage>235</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.10082</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Protein 8-class secondary structure prediction using conditional neural fields</article-title>
        <source>Proteomics</source>
        <year>2011</year>
        <volume>11</volume>
        <issue>19</issue>
        <fpage>3786</fpage>
        <lpage>3792</lpage>
        <pub-id pub-id-type="doi">10.1002/pmic.201100196</pub-id>
        <?supplied-pmid 21805636?>
        <pub-id pub-id-type="pmid">21805636</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Noble</surname>
            <given-names>ME</given-names>
          </name>
          <name>
            <surname>Endicott</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>LN</given-names>
          </name>
        </person-group>
        <article-title>Protein kinase inhibitors: insights into drug design from structure</article-title>
        <source>Science</source>
        <year>2004</year>
        <volume>303</volume>
        <issue>5665</issue>
        <fpage>1800</fpage>
        <lpage>1805</lpage>
        <pub-id pub-id-type="doi">10.1126/science.1095920</pub-id>
        <?supplied-pmid 15031492?>
        <pub-id pub-id-type="pmid">15031492</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simossis</surname>
            <given-names>VA</given-names>
          </name>
          <name>
            <surname>Heringa</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Integrating protein secondary structure prediction and multiple sequence alignment</article-title>
        <source>Curr Protein Pept Sci</source>
        <year>2004</year>
        <volume>5</volume>
        <issue>4</issue>
        <fpage>249</fpage>
        <lpage>266</lpage>
        <pub-id pub-id-type="doi">10.2174/1389203043379675</pub-id>
        <?supplied-pmid 15320732?>
        <pub-id pub-id-type="pmid">15320732</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ashraf</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yaohang</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Context-based features enhance protein secondary structure prediction accuracy. Journal of chemical information and modeling</article-title>
        <source>J Chem Inf Model</source>
        <year>2014</year>
        <volume>54</volume>
        <issue>3</issue>
        <fpage>992</fpage>
        <lpage>1002</lpage>
        <pub-id pub-id-type="doi">10.1021/ci400647u</pub-id>
        <pub-id pub-id-type="pmid">24571803</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pelton</surname>
            <given-names>JT</given-names>
          </name>
          <name>
            <surname>McLean</surname>
            <given-names>LR</given-names>
          </name>
        </person-group>
        <article-title>Spectroscopic methods for analysis of protein secondary structure</article-title>
        <source>Anal Biochem</source>
        <year>2000</year>
        <volume>277</volume>
        <issue>2</issue>
        <fpage>167</fpage>
        <lpage>176</lpage>
        <pub-id pub-id-type="doi">10.1006/abio.1999.4320</pub-id>
        <?supplied-pmid 10625503?>
        <pub-id pub-id-type="pmid">10625503</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meiler</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Baker</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Rapid protein fold determination using unassigned NMR data</article-title>
        <source>Proc Natl Acad Sci U S A</source>
        <year>2003</year>
        <volume>100</volume>
        <issue>26</issue>
        <fpage>15404</fpage>
        <lpage>15409</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.2434121100</pub-id>
        <?supplied-pmid 14668443?>
        <pub-id pub-id-type="pmid">14668443</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chou</surname>
            <given-names>PY</given-names>
          </name>
          <name>
            <surname>Fasman</surname>
            <given-names>GD</given-names>
          </name>
        </person-group>
        <article-title>Prediction of protein conformation</article-title>
        <source>Biochemistry</source>
        <year>1974</year>
        <volume>13</volume>
        <issue>2</issue>
        <fpage>222</fpage>
        <lpage>245</lpage>
        <pub-id pub-id-type="doi">10.1021/bi00699a002</pub-id>
        <?supplied-pmid 4358940?>
        <pub-id pub-id-type="pmid">4358940</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gascuel</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Golmard</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>A simple method for predicting the secondary structure of globular proteins: implications and accuracy</article-title>
        <source>Computer Appl Biosci</source>
        <year>1988</year>
        <volume>4</volume>
        <issue>3</issue>
        <fpage>357</fpage>
        <lpage>365</lpage>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Prediction of protein secondary structure at better than 70% accuracy</article-title>
        <source>J Mol Biol</source>
        <year>1993</year>
        <volume>232</volume>
        <issue>2</issue>
        <fpage>584</fpage>
        <lpage>599</lpage>
        <pub-id pub-id-type="doi">10.1006/jmbi.1993.1413</pub-id>
        <?supplied-pmid 8345525?>
        <pub-id pub-id-type="pmid">8345525</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>TD</given-names>
          </name>
        </person-group>
        <article-title>Protein secondary structure prediction based on position-specific scoring matrices</article-title>
        <source>J Mol Biol</source>
        <year>1999</year>
        <volume>292</volume>
        <issue>2</issue>
        <fpage>195</fpage>
        <lpage>202</lpage>
        <pub-id pub-id-type="doi">10.1006/jmbi.1999.3091</pub-id>
        <?supplied-pmid 10493868?>
        <pub-id pub-id-type="pmid">10493868</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cortes</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Support vector networks</article-title>
        <source>Mach Learn</source>
        <year>1995</year>
        <volume>20</volume>
        <fpage>273</fpage>
        <lpage>293</lpage>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Scholkopf</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Burges</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Extracting support data for a given task</article-title>
        <source>Proceedings, first international conference on knowledge discovery and data mining</source>
        <year>1995</year>
        <publisher-loc>Menlo Park, CA</publisher-loc>
        <publisher-name>AAAI Press</publisher-name>
        <fpage>252</fpage>
        <lpage>257</lpage>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Roobaert</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hulle</surname>
            <given-names>MM</given-names>
          </name>
        </person-group>
        <article-title>View-based 3D object recognition with support vector machines</article-title>
        <source>Proceedings of the IEEE neural networks for signal processing workshop</source>
        <year>1999</year>
        <publisher-loc>NJ</publisher-loc>
        <publisher-name>IEEE Press</publisher-name>
        <fpage>77</fpage>
        <lpage>84</lpage>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Schmidt</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Grish</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Speaker identification via support vector classifiers</article-title>
        <source>The proceedings of the international conference on acoustics, speech and signal processing, 1996</source>
        <year>1996</year>
        <publisher-loc>Long Beach, CA</publisher-loc>
        <publisher-name>IEEE Press</publisher-name>
        <fpage>105</fpage>
        <lpage>108</lpage>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Drucker</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Support vector machines for spam categorization</article-title>
        <source>IEEE Trans Neural Netw</source>
        <year>1999</year>
        <volume>10</volume>
        <fpage>1048</fpage>
        <lpage>1054</lpage>
        <pub-id pub-id-type="doi">10.1109/72.788645</pub-id>
        <?supplied-pmid 18252607?>
        <pub-id pub-id-type="pmid">18252607</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hua</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>A novel method of protein secondary structure prediction with high segment overlap measure: support vector machine approach</article-title>
        <source>J Mol Biol</source>
        <year>2001</year>
        <volume>308</volume>
        <issue>2</issue>
        <fpage>397</fpage>
        <lpage>407</lpage>
        <pub-id pub-id-type="doi">10.1006/jmbi.2001.4580</pub-id>
        <?supplied-pmid 11327775?>
        <pub-id pub-id-type="pmid">11327775</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Protein secondary structure prediction based on an improved support vector machines approach</article-title>
        <source>Protein Eng Des Sel</source>
        <year>2003</year>
        <volume>16</volume>
        <issue>8</issue>
        <fpage>553</fpage>
        <lpage>560</lpage>
        <pub-id pub-id-type="doi">10.1093/protein/gzg072</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>EL_PSSM-RT: DNA-binding residue prediction by integrating ensemble learning with PSSM relation transformation</article-title>
        <source>BMC Bioinformatics</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>379</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-017-1792-8</pub-id>
        <?supplied-pmid 28851273?>
        <pub-id pub-id-type="pmid">28851273</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>A novel method for protein secondary structure prediction using dual-layer SVM and profiles</article-title>
        <source>Proteins: Structure, Function, and Bioinformatics</source>
        <year>2004</year>
        <volume>54</volume>
        <issue>4</issue>
        <fpage>738</fpage>
        <lpage>743</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.10634</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Thibodeau-Laufer</surname>
            <given-names>É</given-names>
          </name>
          <name>
            <surname>Alain</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Yosinski</surname>
            <given-names>J</given-names>
          </name>
          <collab>preprint arXiv:.1091</collab>
        </person-group>
        <article-title>Deep generative stochastic networks trainable by backprop</article-title>
        <source>Computer Sci</source>
        <year>2013</year>
        <volume>2</volume>
        <fpage>226</fpage>
        <lpage>234</lpage>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <source>Protein secondary structure prediction using cascaded convolutional and recurrent neural networks</source>
        <year>2016</year>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lawrence</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Giles</surname>
            <given-names>CL</given-names>
          </name>
          <name>
            <surname>Tsoi</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Back</surname>
            <given-names>AD</given-names>
          </name>
        </person-group>
        <article-title>Face recognition: a convolutional neural-network approach</article-title>
        <source>IEEE Trans Neural Netw</source>
        <year>1997</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>98</fpage>
        <lpage>113</lpage>
        <pub-id pub-id-type="doi">10.1109/72.554195</pub-id>
        <?supplied-pmid 18255614?>
        <pub-id pub-id-type="pmid">18255614</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Yih</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Toutanova</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Platt</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Meek</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Learning discriminative projections for text similarity measures</article-title>
        <source>Proceedings of the fifteenth conference on computational natural language learning</source>
        <year>2011</year>
        <fpage>247</fpage>
        <lpage>256</lpage>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Character-level convolutional networks for text classification</article-title>
        <source>Advances in neural information processing systems</source>
        <year>2015</year>
        <fpage>649</fpage>
        <lpage>657</lpage>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Protein secondary structure prediction using deep convolutional neural fields</article-title>
        <source>Sci Rep</source>
        <year>2016</year>
        <volume>6</volume>
        <fpage>18962</fpage>
        <pub-id pub-id-type="doi">10.1038/srep18962</pub-id>
        <?supplied-pmid 26752681?>
        <pub-id pub-id-type="pmid">26752681</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brunak</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Frasconi</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Soda</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Pollastri</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Exploiting the past and the future in protein secondary structure prediction</article-title>
        <source>Bioinformatics</source>
        <year>1999</year>
        <volume>15</volume>
        <issue>11</issue>
        <fpage>937</fpage>
        <lpage>946</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/15.11.937</pub-id>
        <?supplied-pmid 10743560?>
        <pub-id pub-id-type="pmid">10743560</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schmidler</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Brutlag</surname>
            <given-names>DL</given-names>
          </name>
        </person-group>
        <article-title>Bayesian segmentation of protein secondary structure</article-title>
        <source>J Comput Biol</source>
        <year>2000</year>
        <volume>7</volume>
        <issue>1–2</issue>
        <fpage>233</fpage>
        <lpage>248</lpage>
        <pub-id pub-id-type="doi">10.1089/10665270050081496</pub-id>
        <?supplied-pmid 10890399?>
        <pub-id pub-id-type="pmid">10890399</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Chu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Ghahramani</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wild</surname>
            <given-names>DL</given-names>
          </name>
        </person-group>
        <article-title>A graphical model for protein secondary structure prediction</article-title>
        <source>Proceedings of the twenty-first international conference conference on machine learning (ICML)</source>
        <year>2004</year>
        <fpage>161</fpage>
        <lpage>168</lpage>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>KC</given-names>
          </name>
        </person-group>
        <article-title>Identification of DNA-binding proteins by incorporating evolutionary information into pseudo amino acid composition via the top-n-gram approach</article-title>
        <source>J Biomol Struct Dyn</source>
        <year>2015</year>
        <volume>33</volume>
        <issue>8</issue>
        <fpage>1720</fpage>
        <pub-id pub-id-type="doi">10.1080/07391102.2014.968624</pub-id>
        <?supplied-pmid 25252709?>
        <pub-id pub-id-type="pmid">25252709</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Identifying DNA-binding proteins by combining support vector machine and PSSM distance transformation</article-title>
        <source>BMC Syst Biol</source>
        <year>2015</year>
        <volume>9</volume>
        <issue>S1</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <?supplied-pmid 25582171?>
        <pub-id pub-id-type="pmid">25582171</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Kong</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>PDNAsite: identification of DNA-binding site from protein sequence by incorporating spatial and sequence context</article-title>
        <source>Sci Rep</source>
        <year>2016</year>
        <volume>6</volume>
        <fpage>27653</fpage>
        <pub-id pub-id-type="doi">10.1038/srep27653</pub-id>
        <?supplied-pmid 27282833?>
        <pub-id pub-id-type="pmid">27282833</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Jr</surname>
            <given-names>DR</given-names>
          </name>
        </person-group>
        <article-title>Pisces: a protein sequence culling server</article-title>
        <source>Bioinformatics</source>
        <year>2003</year>
        <volume>19</volume>
        <issue>12</issue>
        <fpage>1589</fpage>
        <lpage>1591</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btg224</pub-id>
        <?supplied-pmid 12912846?>
        <pub-id pub-id-type="pmid">12912846</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cuff</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Barton</surname>
            <given-names>GJ</given-names>
          </name>
        </person-group>
        <article-title>Evaluation and improvement of multiple sequence methods for protein secondary structure prediction</article-title>
        <source>Proteins: Structure, Function, and Bioinformatics</source>
        <year>1999</year>
        <volume>34</volume>
        <issue>4</issue>
        <fpage>508</fpage>
        <lpage>519</lpage>
        <pub-id pub-id-type="doi">10.1002/(SICI)1097-0134(19990301)34:4&lt;508::AID-PROT10&gt;3.0.CO;2-4</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Protein 8class secondary structure prediction using conditional neural fields</article-title>
        <source>IEEE Int Conf Bioinformatics Biomed</source>
        <year>2011</year>
        <volume>11</volume>
        <issue>19</issue>
        <fpage>3786</fpage>
        <lpage>3792</lpage>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mesnil</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Dauphin</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Hakkani-Tur</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Heck</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Tar</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Using recurrent neural networks for slot filling in spoken language understanding</article-title>
        <source>IEEE/ACM Trans Audio Speech Lang Process</source>
        <year>2015</year>
        <volume>23</volume>
        <issue>3</issue>
        <fpage>530</fpage>
        <lpage>539</lpage>
        <pub-id pub-id-type="doi">10.1109/TASLP.2014.2383614</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Corrado</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Distributed representations of words and phrases and their compositionality</article-title>
        <source>Advances in neural information processing systems</source>
        <year>2013</year>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kumar</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gromiha</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Raghava</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Identification of DNA-binding proteins using support vector machines and evolutionary profiles</article-title>
        <source>BMC Bioinformatics</source>
        <year>2007</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>563</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-8-463</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
        </person-group>
        <article-title>Improving the accuracy of transmembrane protein topology prediction using evolutionary information</article-title>
        <source>Bioinformatics</source>
        <year>2007</year>
        <volume>23</volume>
        <fpage>538</fpage>
        <lpage>544</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl677</pub-id>
        <?supplied-pmid 17237066?>
        <pub-id pub-id-type="pmid">17237066</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Biswas</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Noman</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Sikder</surname>
            <given-names>AR</given-names>
          </name>
        </person-group>
        <article-title>Machine learning approach to predict protein phosphorylation sites by incorporating evolutionary information</article-title>
        <source>BMC Bioinformatics</source>
        <year>2010</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>273</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-11-273</pub-id>
        <?supplied-pmid 20492656?>
        <pub-id pub-id-type="pmid">20492656</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ruchi</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Grish</surname>
            <given-names>CV</given-names>
          </name>
          <name>
            <surname>Raghava</surname>
            <given-names>GPS</given-names>
          </name>
        </person-group>
        <article-title>Prediction of mitochondrial proteins of malaria parasite using split amino acid composition and PSSM profile</article-title>
        <source>Amino Acids</source>
        <year>2010</year>
        <volume>39</volume>
        <fpage>101</fpage>
        <lpage>110</lpage>
        <pub-id pub-id-type="doi">10.1007/s00726-009-0381-1</pub-id>
        <pub-id pub-id-type="pmid">19908123</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>XW</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>XT</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>ZQ</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>MH</given-names>
          </name>
        </person-group>
        <article-title>Prediction of lysine ubiquitylation with ensemble classifier and feature selection</article-title>
        <source>Int J Mol Sci</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>8347</fpage>
        <lpage>8361</lpage>
        <pub-id pub-id-type="doi">10.3390/ijms12128347</pub-id>
        <?supplied-pmid 22272076?>
        <pub-id pub-id-type="pmid">22272076</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schaffer</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Aravind</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Madden</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Shavirin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Spouge</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Wolf</surname>
            <given-names>YI</given-names>
          </name>
          <name>
            <surname>Koonin</surname>
            <given-names>EV</given-names>
          </name>
          <name>
            <surname>Altschul</surname>
            <given-names>SF</given-names>
          </name>
        </person-group>
        <article-title>Improving the accuracy of PSI-BLAST protein database searches with composition-based statistics and other refinements</article-title>
        <source>Nucleic Acids Res</source>
        <year>2001</year>
        <volume>29</volume>
        <issue>14</issue>
        <fpage>2994</fpage>
        <lpage>3005</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/29.14.2994</pub-id>
        <?supplied-pmid 11452024?>
        <pub-id pub-id-type="pmid">11452024</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>RK</given-names>
          </name>
          <name>
            <surname>Greff</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Cortes</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lawrence</surname>
            <given-names>ND</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>DD</given-names>
          </name>
          <name>
            <surname>Sugiyama</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Garnett</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Training very deep networks</article-title>
        <source>Advances in neural information processing systems</source>
        <year>2015</year>
        <fpage>2377</fpage>
        <lpage>2385</lpage>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Cho K, Merrienboer BV, Gulcehre C, Bahdanau D, Bougares F, Schwenk H, Bengio Y. Learning phrase representations using RNN encoder-decoder for statistical machine translation. Computer Sci. 2014;</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simpson</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Morgan</surname>
            <given-names>FJ</given-names>
          </name>
        </person-group>
        <article-title>Complete amino acid sequence of embden goose (anser anser) egg-white lysozyme</article-title>
        <source>Biochimica et Biophysica Acta (BBA)-Protein Structure and Molecular Enzymology</source>
        <year>1983</year>
        <volume>744</volume>
        <issue>3</issue>
        <fpage>349</fpage>
        <lpage>351</lpage>
        <pub-id pub-id-type="doi">10.1016/0167-4838(83)90210-8</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Berman</surname>
            <given-names>HM</given-names>
          </name>
          <name>
            <surname>Westbrook</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Gilliland</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Bhat</surname>
            <given-names>TN</given-names>
          </name>
          <name>
            <surname>Weissig</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Shindyalov</surname>
            <given-names>IN</given-names>
          </name>
          <name>
            <surname>Bourne</surname>
            <given-names>PE</given-names>
          </name>
        </person-group>
        <article-title>The protein data bank</article-title>
        <source>Nucleic Acids Res</source>
        <year>2000</year>
        <volume>28</volume>
        <fpage>235</fpage>
        <lpage>242</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/28.1.235</pub-id>
        <?supplied-pmid 10592235?>
        <pub-id pub-id-type="pmid">10592235</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
