<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Oral Health</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Oral Health</journal-id>
    <journal-title-group>
      <journal-title>BMC Oral Health</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1472-6831</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10283267</article-id>
    <article-id pub-id-type="pmid">37340367</article-id>
    <article-id pub-id-type="publisher-id">3112</article-id>
    <article-id pub-id-type="doi">10.1186/s12903-023-03112-w</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ORIENTATE: automated machine learning classifiers for oral health prediction and research</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Gomez-Rios</surname>
          <given-names>Inmaculada</given-names>
        </name>
        <address>
          <email>macu@innovadental.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Egea-Lopez</surname>
          <given-names>Esteban</given-names>
        </name>
        <address>
          <email>esteban.egea@upct.es</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ortiz Ruiz</surname>
          <given-names>Antonio José</given-names>
        </name>
        <address>
          <email>ajortiz@um.es</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.10586.3a</institution-id><institution-id institution-id-type="ISNI">0000 0001 2287 8496</institution-id><institution>Department of Dermatology, Stomatology, Radiology and Physical Medicine, </institution><institution>Universidad de Murcia, </institution></institution-wrap>Murcia, Spain </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.218430.c</institution-id><institution-id institution-id-type="ISNI">0000 0001 2153 2602</institution-id><institution>Dept. Information Technologies and Communications, </institution><institution>Universidad Politecnica de Cartagena (UPCT), </institution></institution-wrap>Cartagena, Spain </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>20</day>
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>23</volume>
    <elocation-id>408</elocation-id>
    <history>
      <date date-type="received">
        <day>8</day>
        <month>3</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>6</day>
        <month>6</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">The application of data-driven methods is expected to play an increasingly important role in healthcare. However, a lack of personnel with the necessary skills to develop these models and interpret its output is preventing a wider adoption of these methods. To address this gap, we introduce and describe ORIENTATE, a software for automated application of machine learning classification algorithms by clinical practitioners lacking specific technical skills. ORIENTATE allows the selection of features and the target variable, then automatically generates a number of classification models and cross-validates them, finding the best model and evaluating it. It also implements a custom feature selection algorithm for systematic searches of the best combination of predictors for a given target variable. Finally, it outputs a comprehensive report with graphs that facilitates the explanation of the classification model results, using global interpretation methods, and an interface for the prediction of new input samples. Feature relevance and interaction plots provided by ORIENTATE allow to use it for statistical inference, which can replace and/or complement classical statistical studies.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">Its application to a dataset with healthy and special health care needs (SHCN) children, treated under deep sedation, was discussed as case study. On the example dataset, despite its small size, the feature selection algorithm found a set of features able to predict the need for a second sedation with a f1 score of 0.83 and a ROC (AUC) of 0.92. Eight predictive factors for both populations were found and ordered by the relevance assigned to them by the model. A discussion of how to derive inferences from the relevance and interaction plots and a comparison with a classical study is also provided.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">ORIENTATE automatically finds suitable features and generates accurate classifiers which can be used in preventive tasks. In addition, researchers without specific skills on data methods can use it for the application of machine learning classification and as a complement to classical studies for inferential analysis of features. In the case study, a high prediction accuracy for a second sedation in SHCN children was achieved. The analysis of the relevance of the features showed that the number of teeth with pulpar treatments at the first sedation is a predictive factor for a second sedation.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Machine learning</kwd>
      <kwd>Classification</kwd>
      <kwd>Special health care needs</kwd>
      <kwd>Deep sedation</kwd>
      <kwd>Predictive dentistry</kwd>
      <kwd>Second sedation risk</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© BioMed Central Ltd., part of Springer Nature 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par11">The application of data-driven methods is expected to play an increasingly important role in healthcare [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>], as they can be a particularly effective tool for diagnosis (disease presence) or prognosis (risk of future outcome), among other tasks. To mention a few representative examples: they have been used for the identification and classifications of different types of cancers [<xref ref-type="bibr" rid="CR3">3</xref>], or to predict individualized optimal drug doses for patients [<xref ref-type="bibr" rid="CR4">4</xref>]; they have also been used for natural language processing of medical records for improving the accuracy of appendicitis diagnoses [<xref ref-type="bibr" rid="CR4">4</xref>] and to prevent hypoxaemia during surgery [<xref ref-type="bibr" rid="CR5">5</xref>]. They also allow to analyze how reliable medical information is conveyed on social networks [<xref ref-type="bibr" rid="CR6">6</xref>]. Machine Learning (ML) is the branch of artificial intelligence that encompasses methods to make computers learn how to do some task from experience (data). In particular, ML uses statistical methods to predict outcomes in future data [<xref ref-type="bibr" rid="CR2">2</xref>], that is, to make classifications or predictions. ML algorithms are <italic>trained</italic> with data and fall into two broad categories: supervised learning, which uses data that have been previously labeled (with the desired or correct value), and unsupervised learning, which uses data that have not been labeled. As described in recent works [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>] the prediction accuracy of ML systems in some fields may meet or surpass that of the experts. Finally, ML systems handle well large volumes of high dimensional data [<xref ref-type="bibr" rid="CR2">2</xref>] and, therefore, can be a very effective tool for large scale preventive healthcare programs, for tasks such as large-scale screening [<xref ref-type="bibr" rid="CR7">7</xref>]. As an example, <italic>AutoPrognosis</italic>, a software that automatically builds an ensemble of predictive ML models, has been used to predict cardiovascular diseases: in a study done over 5 years with a sample of 4801 cases it was able to predict correctly 368 more cases than alternative classical methods [<xref ref-type="bibr" rid="CR8">8</xref>].</p>
    <p id="Par12">However, there are a number of challenges that prevent the wider adoption of these methods, both technical, such as the need for appropriate and interoperable data models [<xref ref-type="bibr" rid="CR2">2</xref>] and high-quality data [<xref ref-type="bibr" rid="CR7">7</xref>]; and operational, such as their integration in the clinical workflow [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR7">7</xref>]. One particular problem pointed out by Callahan [<xref ref-type="bibr" rid="CR2">2</xref>] is the <italic>lack of personnel with the necessary skills to develop these models and interpret their output</italic>. Developing an ML model for healthcare is a demanding task, which includes problem selection, data curation, development, and validation [<xref ref-type="bibr" rid="CR7">7</xref>]. Even though the application of sophisticated ML algorithms is relatively easy thanks to high-level programming libraries such as <italic>scikit-learn</italic> [<xref ref-type="bibr" rid="CR9">9</xref>], clinical practitioners usually lack the technical background required for its use. And conversely, technicians often lack the required clinical skills for appropriate model evaluation and refinement, which usually results in a time-consuming exchange of requirements and model prototypes between the two groups [<xref ref-type="bibr" rid="CR10">10</xref>]. In summary, a first step before deploying ML systems appropriate for preventive healthcare is the development of a validated model for the problem at hand, which is complicated due to a lack of complementary skills of the involved actors.</p>
    <p id="Par13">To address this problem, in this paper we introduce and describe ORIENTATE (applicatiOn of machine leaRning for classIfication of dENTal pATiEnts), a software that allows the use of sophisticated ML algorithms for the classification and prediction of oral health conditions by clinical practitioners lacking specific technical skills (<italic>users</italic>). Given a collected <italic>dataset</italic>, the application basically provides a web-based interface for the selection of its features (<italic>predictors</italic>) that may be useful in the prediction of some other variable of interest (<italic>target</italic>). It then generates a number of ML classification models and cross-validate them against subsets of a training set, selecting the best model according to some performance metric, and evaluating it against a validation set. The tool shows a detailed summary of the evaluation with graphs that facilitates the explanation of the ML model results, using global interpretation methods [<xref ref-type="bibr" rid="CR11">11</xref>], and an interface for the prediction of new input samples. The previous procedure summarizes a first stage of use of the tool, where researchers generate and evaluate a suitable prediction model from a given dataset. The integration of the tool into the clinical workflow would come in a second stage, where the validated model can be used, either with the provided interface or a custom (not developed yet) application. The practitioner would generate a prediction, just by filling in the corresponding predictors for a new patient, that may guide her in the planning of further treatments, complementing the information on caries risk and lesion management provided by clinical tools such as the Caries Management by Risk Assessment (CAMBRA) [<xref ref-type="bibr" rid="CR12">12</xref>] or the International Caries Classification and Management System (ICCMS) [<xref ref-type="bibr" rid="CR13">13</xref>].</p>
    <p id="Par14">A first goal of the tool is to facilitate testing different combinations of predictors according to the user criteria. It, therefore, allows an exploratory analysis that can be used to validate hypotheses about the influence of different features on the target variable [<xref ref-type="bibr" rid="CR7">7</xref>]. The tool guides the user in the development process of the model, offering the selection of the class of interest, alternatives for imputation of missing data and selection of the performance metric. It is also agnostic with respect to the dataset used, that is, it can be used with different datasets as long as they follow a minimal shared data model. Once the exploratory phase has finished, the obtained model has usually enough quality (prediction performance) as to be used in production. Therefore, trained models can be used in preventive tasks by clinical practitioners without technical training. A second goal is to perform systematic searches for the best combination of predictors for a target variable. A custom feature selection algorithm has been implemented, which tests thousands of combinations of predictors and returns the best model found together with an extensive explanatory report of the feature relevance and interactions. Finally, the explanatory report can be used to perform an inferential analysis of the data, complementary to classical statistical studies [<xref ref-type="bibr" rid="CR14">14</xref>]. This analysis can be done by examining the relevance and interactions of the predictors used by the model, an information that is visually provided by the generated report. Users with varying technical expertise may benefit from the use of ORIENTATE. Researchers only need to complement their dataset with an additional metadata CSV file in a simple format and then proceed with the model generation. In fact, providing metadata forces the researcher to analyze her data and collect and organized it in a systematic way, facilitating the discovery of errors and removing redundant information. The generated models can be critically analyzed by other researchers or practitioners just by examining the provided visual reports. The validation of the best model found is internally done by the tool by evaluating a separated test set from the available data. The prediction results for all the samples are provided to the user, so that she may assess directly the quality of the predictions, even individually, even with plots of the relevance of each predictor for the final result. Finally, as mentioned previously, clinical practitioners can generate new predictions just by filling out a form and then examining the provided report.</p>
    <p id="Par15">To illustrate the use of the application and the performance of the various functionalities, we use a particular case study: a dataset that describes the oral health condition in two populations, one with healthy children and another one with SHCN, was collected to compare the treatments performed under deep sedation in both populations. The dataset was analyzed with classical statistical methods in a previous work [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>]. This particular dataset is described in detail in the “<xref rid="Sec8" ref-type="sec">Material and methods of the previous study</xref>” section. We apply to this dataset the feature selection algorithm implemented by ORIENTATE and show how it finds a set of predictors that results in an accurate predictor model of the need for a second sedation. Next, we examine the relevance of the predictors and their interactions and discuss how they agree with the inferences derived in our previous work using classical statistical methods.</p>
    <p id="Par16">ORIENTATE source code is freely available in our repository, as well as the datasets used in the case study and an additional public dataset, whose format has been adapted, from [<xref ref-type="bibr" rid="CR17">17</xref>], which can be used to test the application.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <p id="Par17">ORIENTATE has been implemented as a web-based application and so it is independent of the platform and only requires a web browser to be used.</p>
    <sec id="Sec3">
      <title>Data source</title>
      <p id="Par18">The application is independent of the dataset used, as long as it is provided in the format described in the <xref rid="Sec6" ref-type="sec">Machine learning workflow</xref> section. The particular dataset used in our use case is described in detail in Material and methods of the previous study section.</p>
    </sec>
    <sec id="Sec4">
      <title>Libraries</title>
      <p id="Par19">A first prototype of the web application was implemented in Python v3.9.12 with Flask v2.0.3. Machine learning algorithms are implemented with SciKit-Learn v1.1.1 and SHAP v.0.41.0.</p>
    </sec>
    <sec id="Sec5">
      <title>Functionality</title>
      <p id="Par20">ORIENTATE allows uploading of the working dataset which is stored for future use. Once the dataset has been uploaded or selected from the available ones, the entry point of the web application shows a view of the current dataset and a menu with the available actions, as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a. The menu entries with Statistics and Distributions provide a statistical summary of the dataset and the empirical distributions of the variables respectively. The main functionality can be found in Classifier evaluation, (CE) and Feature selection (FS) as described next.<list list-type="bullet"><list-item><p id="Par21"><italic>Classifier evaluation (CE)</italic>. This link performs a search for the best classifier to predict a chosen target feature, given a set of features selected by the user. The application collects the information from the user in two separate steps as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>b and c. First, the user selects the target variable to be predicted and the set of features to be used as predictors. Next, the user selects the class of interest for binary variables. For instance, for the variable <italic>Caries</italic> the user selects whether she is interested in presence (class 1) or absence (class 0) of caries. For multiclass variables this action is skipped. The application shows the available options, and, in case of missing values (usually not registered) in the target feature, the user selects an imputation method for the missing values. Currently, the user can only select between removal of rows with missing values or replacement with a constant value. Once the input is collected, the application searches for the best classifier among a set of predefined types of classifiers, which are described in detail in the “<xref rid="Sec6" ref-type="sec">Machine learning workflow</xref>” section, and shows a summary of the results. This step is described in detail in the following section. A partial view of the evaluation results window is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>d. Our goal is to simplify the interpretation of the results to clinical practitioners, so in this step an explanation of the performance metrics is provided and additional information is presented to the user. The summary includes the metric scores and confusion matrix obtained for all the classifiers tested [<xref ref-type="bibr" rid="CR18">18</xref>]. The best classifier found according to the metric is separately highlighted. In addition, the predictions of the best classifier for each sample in the validation test are also displayed. That is, for all the samples in the validation test, the application shows a table with a row with the predicted value, the real value, the probability for each class and the values of the predictors. Finally, the user can predict, with the best classifier found, the value of a new sample by inserting the values of the predictors in a form provided. All the results can be downloaded for further use. The goal of this functionality is to let the user find a good classifier when the set of predictors (features) is previously established, for instance, by the expert knowledge of the clinical practitioner. The results allow to examine the differences between classifiers. It also allows to test how minor variations in the feature set, such as the replacement of a few predictors, influence the classifier performance.</p></list-item><list-item><p id="Par22"><italic>Feature selection (FS)</italic>. In many cases, however, the clinical practitioner is interested in finding out which features may help to predict a certain target variable. In fact, the goal of many studies is to uncover and describe the associational relationships between a set of features and the target variable. Similarly, our tool allows to search for the set of features that result in more accurate classifiers. This functionality can be viewed as an extension of CE, because the above procedure is repeated for different subsets of the feature set. Since it is usually not possible to test all the combinations of features, the application uses a feature selection algorithm that is described in detail in the <xref rid="Sec6" ref-type="sec">Machine learning workflow</xref> section. The application shows the user the same steps as in CE: first, a feature selection window, where the user selects the set of features for the search, and then the class selection and imputation window. In this case, the application informs the user of the number of combinations to be tested and asks for a name for the selection task to be created. Since this task is time-consuming usually, a background task is executed and the user is directed to another tab where the progress of the pending tasks is shown. Once the background task has finished the search for the best combination of features, the user can view the results. First, the user is presented with a summary of the feature selection search which includes: 1) a plot of the best metric achieved for each of the combinations of features tested, as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>a; 2) the best model and features found; 3) the relative relevance of each of the features for the best model found; and 4) when the best model found is a DecisionTree classifier, the application additionally shows the actual decision tree generated by the model, shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b. Our goal again is to provide the information in a way that can be interpreted by the clinical user as easily as possible. Relative feature relevance helps the user understand the importance that the model assigns to each feature, which can be critically assessed by the user. The generated decision tree is particularly helpful in this sense, since it allows the user to actually reproduce the decision process of the model and assess its soundness. Moreover, users can use the tree to derive their own protocols, adapting it to the clinical practice by refining or simplifyingthe tree. When clicking on the SHAP report button for the found model, further support for interpretation is provided. In this case, a SHAP report of the model is generated. SHAP [<xref ref-type="bibr" rid="CR11">11</xref>] is a method to explain individual predictions and generate global interpretations. The application computes the SHAP values and generates a number of global interpretation plots, which include feature importance, feature dependence, interactions and clustering of feature relevance, as shown in the snapshots in Fig. <xref rid="Fig2" ref-type="fig">2</xref>c. Finally, the user can inspect individual results. In that case, the application displays the individual predictions and corresponding SHAP explanations for all the elements in the dataset. The snapshot in Fig. <xref rid="Fig2" ref-type="fig">2</xref>d shows the particular values of the features for that individual as well as the predicted and real values of the target variable. Below, a SHAP waterfall plot displays how the values of the features contribute to the particular decision. In the <xref rid="Sec13" ref-type="sec">Case study</xref> section we use our case study to discuss and describe in detail those plots.</p></list-item><list-item><p id="Par23"><italic>Predictions for new samples</italic>. Once a satisfactory classifier has been found, either by CE or FE, it can be used to predict the result for new samples. ORIENTATE generates automatically a form, shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a, for the user to fill the values of the predictors for a new sample. After submitting, the results are displayed to the user, as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>b, including the probabilities for the different predicted classes and a plot for the SHAP values of the new samples to interpret how the feature values have influenced the newly predicted sampled.</p></list-item></list><fig id="Fig1"><label>Fig. 1</label><caption><p>Screenshots of the web application interface and steps for the evaluation of classifiers</p></caption><graphic xlink:href="12903_2023_3112_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par24">
        <fig id="Fig2">
          <label>Fig. 2</label>
          <caption>
            <p>Screenshots of the web application interface for the selection of features</p>
          </caption>
          <graphic xlink:href="12903_2023_3112_Fig2_HTML" id="MO2"/>
        </fig>
      </p>
      <p id="Par25">
        <fig id="Fig3">
          <label>Fig. 3</label>
          <caption>
            <p>Screenshots of the web application interface for the prediction of new samples</p>
          </caption>
          <graphic xlink:href="12903_2023_3112_Fig3_HTML" id="MO3"/>
        </fig>
      </p>
    </sec>
    <sec id="Sec6">
      <title>Machine learning workflow</title>
      <p id="Par26">In this section we describe the ML procedures used by the tool. Most of them are implemented as functions that are combined to yield the desired result. For instance, Feature Selection uses data selection and preprocessing and then calls repeatedly classifier evaluation with different combinations of features. Details can be checked in the source code in our repository.<list list-type="bullet"><list-item><p id="Par27"><italic>Data format</italic>. Data are compiled into a CSV file, with an additional metadata file with custom format, providing a description of the type of variable of each column and its range of values and optionally a legend to facilitate showing content in a human-friendly format. Variable types may be continuous variables, bounded and unbounded discrete variables, categorical variables encoded as integers with a given valid range and <italic>tooth-lists</italic>. The latter maps a set of teeth to their number according to the ISO 3950:2016 standard [<xref ref-type="bibr" rid="CR19">19</xref>], which designates teeth or areas of the oral cavity using two digits.</p></list-item><list-item><p id="Par28"><italic>Data selection and preprocessing</italic>. ORIENTATE allows the user to select the target and the predictor features from the set of features present in the dataset. Once they are selected, the tool analyzes the number of classes present in the target feature. For binary classification, the user is prompted to mark the relevant class, which is used later to select the best model as the one with the best selected performance metric for that class. The performance metric used can also be chosen, among precision, recall and f1-score [<xref ref-type="bibr" rid="CR18">18</xref>]. For multiclass classification, the best model is that with the best average performance metric over all the classes. The presence of missing data in the target feature is analyzed. In case there is missing data, the user must select an imputation method for it. Since the dataset used comprised only categorical data, the options are to remove or fill with some constant value the missing data. Additional imputation methods are available (use average or more frequent value). Afterward, the predictor features are prepared for the classifiers by normalizing continuous variables and one-hot encoding tooth-lists and optionally categorical variables. A transformation pipeline with the imputation and encoding is generated and the dataset is passed for the evaluation of the classifiers.</p></list-item><list-item><p id="Par29"><italic>Classifier evaluation</italic>. The dataset is split into a train set with 80% of the samples and a validation set with the remaining ones. Then, a default set of classifiers is evaluated with 3-fold cross-validation on the training set and a series of performance scores is generated for each classifier. The default set of classifiers includes single instance classifiers: logistic regression, support vector machines with linear and Gaussian RBF kernels, linear classifier with stochastic gradient descent learning, different <italic>naive</italic> Bayes classifiers, Gaussian Process classifier, multilayer perceptron; and ensemble methods: random forest, gradient boosting (XGBoost), AdaBoost and <italic>stacking</italic> with random forest, Gaussian and logistic regression. All the classifiers are initialized with a set of previously selected default parameters, not configurable at the moment by the user. Automated hyperparameter optimization might be added as an additional stage for the tool, if necessary, but we consider that the potential gains are not worthy for the current functionality and given the increased complexity of the software. Once the cross-validation is done for each of the classifiers, the tool selects as best classifier the one with the best performance metric as described before. The best classifier is trained with the complete training set and finally the validation set is predicted with the fitted model, resulting in a final set of performance scores. The results from this process are a set of metrics for each of the classifiers evaluated as well as for the fitted best model: confusion matrix, recall, precision, f1-score and ROC AUC. All of them are gathered and shown to the user by the tool. The best model is saved to be used for the prediction of new input samples. For the validation set, the prediction probabilities for all the samples in the set are also stored and shown. Since for most of the classifiers these are not precise estimates of the class probabilities, they need to be calibrated before being shown to the user.</p></list-item><list-item><p id="Par30"><italic>Feature selection</italic>. When the user completes the data selection and preprocessing stage described before, a set of potential features is ready. At this point two different algorithms are used, depending on the number of features in the set (<italic>N</italic>). The total number of combinations to test is <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c=2^N - N -1$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:msup><mml:mo>-</mml:mo><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq1.gif"/></alternatives></inline-formula>, since we include at least two features, and when this number is too high, it is not computationally feasible to test them all. So we have arbitrarily set <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=13$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>13</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq2.gif"/></alternatives></inline-formula> as the maximum number of features for fully testing all the combinations. Obviously, this parameter should be set according to the available processing power. For instance, in our server (i9-9280X CPU, 2 NVIDIA RTX 2080Ti GPUS, 32 GB RAM), testing <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=11$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq3.gif"/></alternatives></inline-formula> features, that is, <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c=2036$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>2036</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq4.gif"/></alternatives></inline-formula> combinations, takes around one hour. Therefore, when the number of features is up to 13, <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N \le 13$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≤</mml:mo><mml:mn>13</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq5.gif"/></alternatives></inline-formula>, a background processing task iteratively executes the classifier evaluation procedure described above for each of the combinations in the dataset, that is, first with the <italic>N</italic> features, then all the <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N-1$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq6.gif"/></alternatives></inline-formula> combinations and so on until testing all combinations of 2 features. The results of each evaluation are saved to the database. Otherwise, when <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N &gt; 13$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mi>N</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>13</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq7.gif"/></alternatives></inline-formula>, a custom feature selection algorithm is executed in the background task. Feature selection is, in general, a hard problem due to the combinatorial explosion, and there are multiple algorithms available and different methods [<xref ref-type="bibr" rid="CR20">20</xref>]. We employ here a so-called wrapper method [<xref ref-type="bibr" rid="CR20">20</xref>], using the predictor performance as the objective function, and implement a simple variant of a Sequential Backward Selection (SBS) algorithm. Though this kind of algorithms typically returns a local optimum, they can produce good results, are computationally feasible, and we avoid the additional complexity of more sophisticated methods, which we do not consider critical for our goals. Our algorithm proceeds as follows: at each iteration there is a set of predictors to be tested, which we call <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbb {T}$$\end{document}</tex-math><mml:math id="M16"><mml:mi mathvariant="double-struck">T</mml:mi></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq8.gif"/></alternatives></inline-formula> with size <italic>T</italic>, and a removal parameter, <italic>k</italic>. The algorithm is initialized with <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbb {T}$$\end{document}</tex-math><mml:math id="M18"><mml:mi mathvariant="double-struck">T</mml:mi></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq9.gif"/></alternatives></inline-formula> including the total number of features, <italic>N</italic>, and evaluates the performance for all the classifiers in the default set as described in the <italic>classifier evaluation</italic> procedure. The results and metrics obtained in the evaluation are saved to the database, including the best classifier found. Next it evaluates all combinations of <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T-k$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq10.gif"/></alternatives></inline-formula> elements from the total set and saves the metrics and best classifier found. Afterward, the predictors in the test set are replaced by the predictors used by the best classifier. Notice that now the number of elements in the test set, <italic>T</italic>, has been reduced by <italic>k</italic> elements. And again all the combinations of <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T-k$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq11.gif"/></alternatives></inline-formula> elements are evaluated as described above. This procedure is repeated until the test set contains fewer than 2 elements. Finally, we select the best classifier found from all the combinations evaluated. We illustrate the algorithm with an example, where we start with <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=20$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq12.gif"/></alternatives></inline-formula> features, so that <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbb {T}$$\end{document}</tex-math><mml:math id="M26"><mml:mi mathvariant="double-struck">T</mml:mi></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq13.gif"/></alternatives></inline-formula> contains the all the 20 features, and use <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k=2$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq14.gif"/></alternatives></inline-formula>. After the first evaluation with all the features, we evaluate all the combinations of 18 elements, taken from the 20 elements. Then we replace the test set with the features used by the best classifier found, that is, the test set contains now 18 elements, and evaluate all the combinations of 16 elements from the 18 features in the test set. Notice how the size of the test set is reduced at each iteration, in this example by <inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k=2$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq15.gif"/></alternatives></inline-formula> elements.</p></list-item><list-item><p id="Par31"><italic>Generation of reports</italic>. Reports are generated at the end of the classifier evaluation (CE) or feature selection (FS) procedures. For the former, the confusion matrices and metrics obtained for all the evaluated models are shown. In addition, the probabilities predicted by the best model for all the elements in the validation set are displayed. For the feature selection procedure, we show a plot of the f1-metric for the best classifier for all the evaluated combinations and the list of the predictors for the best classifier found. If the best found model is able to compute the feature importances, those are shown. Additionally, when the best found model is a DecisionTree, the application displays the generated tree. Afterwards, the SHAP values for the model are computed [<xref ref-type="bibr" rid="CR11">11</xref>]. For this, we use the specialized TreeExplainer [<xref ref-type="bibr" rid="CR21">21</xref>], otherwise, we use an ExactExplainer [<xref ref-type="bibr" rid="CR11">11</xref>] with an independent masker of 1000 samples. Once the values are computed, different plots are shown to the user as described in the <xref rid="Sec5" ref-type="sec">Functionality</xref> section, including waterfall plots for individual explanations.</p></list-item></list></p>
    </sec>
  </sec>
  <sec id="Sec7">
    <title>Results</title>
    <p id="Par32">We have applied our tool to a dataset collected for a previous study [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>]. The goal of that study was to describe the oral health condition in two populations, one with healthy children and another one with SHCN, that is, children who due to physical, medical, developmental or cognitive conditions require special consideration when receiving dental treatment [<xref ref-type="bibr" rid="CR22">22</xref>]; and to compare the treatments performed under deep sedation in both populations. This way we illustrate two of the main goals of our tool: (1) the performance of the feature selection algorithm and prediction power of the trained classifier and (2) its capabilities as an alternative to classical statistical studies. We first briefly describe our previous work, then describe the feature selection results and the interpretation plots provided by ORIENTATE for inferences.</p>
    <p id="Par33">All these results are put in context in the <xref rid="Sec11" ref-type="sec">Discussion</xref> section, where we discuss our results in comparison with the findings of our previous study [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>] and related works.</p>
    <sec id="Sec8">
      <title>Material and methods of the previous study</title>
      <p id="Par34">A cohort study was carried out with children who had been treated for their oral pathology under deep sedation in a private clinic in Cartagena (Murcia, Spain), during the years 2006 to 2018, both included. 274 medical records of patients aged 2 to 18 years were reviewed, including both children with an optimal general health condition which we will call "healthy children", and SHCN children [<xref ref-type="bibr" rid="CR22">22</xref>]. Patients whose medical records did not correctly show demographic data or health status data were excluded. Finally, a total of 230 clinical records (109 (47.4%) healthy and 121 (52.6%) SHCN) were included in the study. Within the SHCN, 19.50% were general developmental disorders, 14.70% were encephalopathies and cerebral palsy, Down syndrome 3.5%, intellectual and/or motor disability 4.7%, and other syndromes 10%. The sample consisted of 142 men (61.74%) and 88 women (38.26%), with a mean age of 7.10 ± 3.40 years. The mean age of the healthy patients was 5.04 ± 2.42 years and that of the group of SHCN children was 8.95 ± 3.09 years. Most of children were 4, 6, 7, 8 and 9 years old.</p>
      <p id="Par35">The information that was collected from the clinical records was: from the first visit, sex, age, systemic health status (healthy or SHCN child), reason for the first sedation, information on the oral health status prior to the intervention (hygiene habits, plaque, tartar, caries lesions, pulp involvement, root remains and absences). On the day of the intervention, the following data were collected: the types of treatments performed (fillings, direct pulp protections, pulpotomies, pulpectomies, endodontics, apex formation, scaling, scaling and root planing, application of fluoride and extractions) and the number of teeth treated. Follow-up data include attendance or not at the check-up where the presence of plaque is assessed, the need for medication due to oral pathology, and improvement at mealtime. In addition, it was recorded the performance of the prevention follow-up behavior in appointments, classifying the patient as "cooperative" or "non-cooperative", according to whether the patient allowed the dentist and/or hygienist to carry out their work; the motivation of parents in the oral care of their children, classifying them as “motivated” or “not motivated”, according to whether they are involved in the care of their children mouth and implement at home the dietary and oral hygiene recommendations that are given to them. Finally, treatments performed subsequently without sedation, year of last check-up and tracking time. The data of the successive sedations included the cause, failed treatments, treatments performed, number of sedations and time from the first to the last sedation.</p>
      <p id="Par36">The collected data described above were incorporated into a dataset and systematically encoded and labeled according to the type of data and data format described in the <xref rid="Sec6" ref-type="sec">Machine learning workflow</xref> section: binary variables are labeled by a descriptive name, continuous or integer variables are prefixed with <italic>Num</italic>, while tooth-lists are prefixed with <italic>List</italic>. For instance, the presence of caries is encoded with a binary variable (0 or 1) and labeled <italic>Caries</italic>, the number of teeth with caries is labeled <italic>NumCaries</italic> and the list of teeth with caries is labeled <italic>ListCaries</italic>. Notice that some information is redundant.</p>
      <p id="Par37">Our previous study is a <italic>traditional</italic> one, where the data were statistically analyzed with <italic>classical</italic> methods. In particular, a descriptive analysis of all the variables was performed. Continuous quantitative variables were compared two by two using a T-test, a T-test with Welch’s correction, or a Mann-Whitney test, depending on the assumptions of normality and homoscedasticity. For discrete variables, the Kruskal-Wallis test was used together with the Dwass-Steel-Critchlow-Fligner test to determine the two-to-two differences. To establish the relationship between discrete qualitative or quantitative variables, contingency tables were made with Pearson <inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\chi ^2$$\end{document}</tex-math><mml:math id="M32"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq16.gif"/></alternatives></inline-formula> or Fisher exact test, depending on whether or not the assumptions were fulfilled. Finally, a test of equality of proportions without continuity correction was used for testing proportions. Additional information on these methods can be found in [<xref ref-type="bibr" rid="CR15">15</xref>].</p>
    </sec>
    <sec id="Sec9">
      <title>Feature selection performance</title>
      <p id="Par38">The dataset compiled from the previously described cohort study contains 102 different features, some of them redundant. From them, 30 potential features of interest were extracted by consensus between 2 odontologists, ruling out those not determining, such as being referred or need to take oral medication, those with insufficient number of data, such as endodontic treatments and apical formation, or those for whom there were not enough variability in the data. The <italic>Feature Selection (FS)</italic> functionality was executed on them, with <italic>SecondSedation</italic> as target variable, a binary variable that indicates whether the patient was subjected to a second sedation. Or in other words, the goal of the test is to find the classifier that is able to better predict the need for a second sedation from all or a subset of the 30 selected predictors (features), using f1-score as peformance metric. Since the number of features <inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=30$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq17.gif"/></alternatives></inline-formula> is greater than 13, our SBS algorithm was executed, taking 1 hour and 40 minutes to evaluate <inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c=2360$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>2360</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq18.gif"/></alternatives></inline-formula> combinations of predictors.</p>
      <p id="Par39">Figure <xref rid="Fig4" ref-type="fig">4</xref> shows the f1-score for the best classifiers found versus the number of features in the test set and combinations evaluated. As can be seen, the best classifier is found when the feature set contains 10 features and all the combinations of 8 features were tested. Therefore, the best classifier uses 8 predictors, listed in Table <xref rid="Tab1" ref-type="table">1</xref>. The best model is a DecisionTree classifier, therefore, the generated tree can be displayed and the decision process of the model can be analyzed. Actually, the generated tree has been shown previously as example in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b. The scores achieved by the classifier on the validation set are shown in Table <xref rid="Tab2" ref-type="table">2</xref>. Additionally, the ROC (AUC) achieved is 0.92. It has to be taken into account that the dataset is <italic>imbalanced</italic>, that is, only 24% of the patients underwent a second sedation. Therefore, it is easier to predict the absence of second sedation. Anyway, the model found is able to accurately predict the need for a second sedation, with a f1-score for the class of interest of 0.83 and a balanced accuracy of 0.81. There is an additional imbalance in the dataset, that of the healthy to SHCN patients: the number of second sedations for SHCN patients is much higher than that for healthy children. In Table <xref rid="Tab3" ref-type="table">3</xref> we compare the scores when conditioning on the two groups. As seen, only 6% of the healthy patients undergo a second sedation, and the model has difficulties to correctly predict those sedations, as expected, but for the SHCN patients it is accurate in the predictions.<fig id="Fig4"><label>Fig. 4</label><caption><p>f1 score obtained by the best classifier found versus the number of features in the test set and number of combinations evaluated (Predictors-Combinations). The f1 score shown is for the class of interest, in this case, the occurrence of a second sedation</p></caption><graphic xlink:href="12903_2023_3112_Fig4_HTML" id="MO4"/></fig></p>
      <p id="Par40">
        <table-wrap id="Tab1">
          <label>Table 1</label>
          <caption>
            <p>Features used by the best classifier found by the feature selection algorithm</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left">Feature</th>
                <th align="left">Type</th>
                <th align="left">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">Healthy</td>
                <td align="left">binary</td>
                <td align="left">Systemic health status</td>
              </tr>
              <tr>
                <td align="left">Plaque</td>
                <td align="left">binary</td>
                <td align="left">Presence of plaque before first sedation</td>
              </tr>
              <tr>
                <td align="left">Fillings</td>
                <td align="left">binary</td>
                <td align="left">Fillings done at first sedation</td>
              </tr>
              <tr>
                <td align="left">PreventionTracking</td>
                <td align="left">ternary</td>
                <td align="left">Compliance with prevention follow-ups</td>
              </tr>
              <tr>
                <td align="left">NumPulparInvolvement</td>
                <td align="left">integer</td>
                <td align="left">Number of teeth with pulpar involvement before first sedation</td>
              </tr>
              <tr>
                <td align="left">NumPulparTreatmentsFirstSedation</td>
                <td align="left">integer</td>
                <td align="left">Number of pulpar treatments done at first sedation</td>
              </tr>
              <tr>
                <td align="left">NumExosPathology</td>
                <td align="left">integer</td>
                <td align="left">Number of extractions due to pathology done at first sedation</td>
              </tr>
              <tr>
                <td align="left">ListPulpotomies</td>
                <td align="left">tooth-list</td>
                <td align="left">List of teeth with pulpotomies done at first sedation</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
      <p id="Par41">
        <table-wrap id="Tab2">
          <label>Table 2</label>
          <caption>
            <p>Scores achieved by the best classifier found</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left">Second sedation</th>
                <th align="left">Precision</th>
                <th align="left">Recall</th>
                <th align="left">f1</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">No</td>
                <td align="left">0.87</td>
                <td align="left">0.96</td>
                <td align="left">0.92</td>
              </tr>
              <tr>
                <td align="left">Yes</td>
                <td align="left">0.92</td>
                <td align="left">0.75</td>
                <td align="left">0.83</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
      <p id="Par42">
        <table-wrap id="Tab3">
          <label>Table 3</label>
          <caption>
            <p>Scores achieved by the best classifier found when conditioned on the Healthy variable for both classes [0 (absence), 1 (presence)]</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left">Group</th>
                <th align="left">Size</th>
                <th align="left">Second sedation</th>
                <th align="left">sed/group</th>
                <th align="left">sed/total</th>
                <th align="left">Precision</th>
                <th align="left">Recall</th>
                <th align="left">f1</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">SHCN</td>
                <td align="left">121</td>
                <td align="left">48</td>
                <td align="left">0.39</td>
                <td align="left">0.2</td>
                <td align="left">[0.85, 0.84]</td>
                <td align="left">[0.9, 0.77]</td>
                <td align="left">[0.88, 0.8]</td>
              </tr>
              <tr>
                <td align="left">Healthy</td>
                <td align="left">109</td>
                <td align="left">7</td>
                <td align="left">0.06</td>
                <td align="left">0.03</td>
                <td align="left">[0.96, 1]</td>
                <td align="left">[1, 0.43]</td>
                <td align="left">[0.98, 0.6 ]</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
      <p id="Par43">In summary, our Feature Selection algorithm is able to find a good set of predictors and returns an accurate predictor of the target variable. Let us remark that the size of the dataset is quite small, with only 230 records. Once a potential model has been found, a larger dataset might be compiled and further optimizations and refinements of the model may also be tested before being used in production.</p>
    </sec>
    <sec id="Sec10">
      <title>Analysis of features for inference</title>
      <p id="Par44">In this subsection we analyze the relevance assigned by the model to the different features with the help of SHAP explanations. All the figures shown have been taken from the report generated and displayed by our application. The inferences that can be extracted from this information are briefly commented but a further discussion and comparison with our results in the previous traditional study [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>], as illustrated in [<xref ref-type="bibr" rid="CR14">14</xref>], is deferred to the <xref rid="Sec11" ref-type="sec">Discussion</xref> section.</p>
      <p id="Par45">As general remarks, first let us note that the fact that the algorithm has chosen these particular 8 predictors indicates that they are more relevant than the other potential features, at least for prediction. Second, we use SHAP values for interpretation [<xref ref-type="bibr" rid="CR11">11</xref>]. Briefly, a SHAP value computes the contribution of each feature to an individual prediction. That is, the SHAP value of a feature provides a numerical additive value to the final model output for an individual. However, one has to be careful regarding the quantitative aspects of the interpretation. For example, if the model predicts a probability of 0.8 for an individual to undergo a second sedation, and the <italic>Healthy</italic> feature is assigned a SHAP value of 0.16, to directly assign 0.16 of that total probability to being healthy or not is not meaningful, and may even be misleading [<xref ref-type="bibr" rid="CR23">23</xref>]. Therefore, we will mainly discuss qualitative aspects, unless a quantitative interpretation is clear: SHAP values should be interpreted as the relative influence of a predictor in the final outcome, with the sign indicating the direction of the influence in the outcome, that is, a positive value indicates higher probability of second sedation, while a negative one indicates lower probability. Let us note also that quantitative claims are not common in classical studies either, where typically only acceptance or rejection of a null hypothesis with a measure of confidence is given, reported as “significant differences were found between the two variables”.</p>
      <p id="Par46">In Fig. <xref rid="Fig5" ref-type="fig">5</xref> we plot two views of the features’ relevance, according to their mean SHAP value. On the left, in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a the mean absolute average value for each feature is plotted, grouped by healthy or SHCN patients. With this plot we can see the relative influence of each feature in the model output (the probability that a patient undergoes a second sedation) in absolute value, that is, without assessing whether the influence is positive or negative. While on the right, in Fig. <xref rid="Fig5" ref-type="fig">5</xref>b we show a scatter plot, which does show the direction of the influence as a function of the feature value. The value of the feature at each row is represented as a color scale, with red for high values and blue for low values. The assigned color depends on the type of variable, for instance, for a binary value, 1 is shown in red while 0 is shown in blue. For an integer variable, red represents the maximum, blue the minimum and purple hues values in between. This plot allows us to clearly see the influence of the different features in the final outcome. For instance, we see that being healthy clearly decreases the value of the output, and since that value is a probability, we see that it makes it less likely to undergo a second sedation.<fig id="Fig5"><label>Fig. 5</label><caption><p>Feature relevance and impact on model output</p></caption><graphic xlink:href="12903_2023_3112_Fig5_HTML" id="MO5"/></fig></p>
      <p id="Par47">As seen, the healthy status has the major influence in absolute value, as expected, since SHCN patients require more often a second sedation. Since the results are grouped by this feature in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a, its value is equal for both groups. In Fig. <xref rid="Fig5" ref-type="fig">5</xref>b we can see the direction of the influence with the sign of the feature: healthy children (red points) SHAP values are negative, that is, tend to decrease the probability of a second sedation, while SHCN children (blue points) are positive and so tend to increase the probability of a second sedation. For SHCN patients, next in importance is the compliance with the prevention tracking program (<italic>PreventionTracking</italic> in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a). However, for healthy children, the second more relevant feature is the number of teeth that required pulpar treatment at the first sedation (<italic>NumPulparTreatmentsFirstSedation</italic> in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a). The number of teeth that showed pulpar involvement is the fourth more relevant predictive factor for a second sedation for both groups (Fig. <xref rid="Fig5" ref-type="fig">5</xref>a). Figure <xref rid="Fig5" ref-type="fig">5</xref> also shows that the probability of a second sedation increases with the presence of bacterial plaque and a high number of extractions due to pathology during the first sedation.</p>
      <p id="Par48">Next, we turn to interactions between features. In Fig. <xref rid="Fig6" ref-type="fig">6</xref> we plot the strongest interactions found by the model as dependence plots. These plots show the dependence of the SHAP value (Y axis right) with the value of the feature (X axis), and the interaction is depicted by coloring the points according to the value of another feature which has the strongest interaction with the former (Y axis left). The feature that has the strongest interaction is automatically computed by the application, but the user can also select other feature interactions that she might be interested in. As expected, most of the features have the strongest interaction with the healthy condition.<fig id="Fig6"><label>Fig. 6</label><caption><p>Dependence plot with interactions between variables. Brownish shaded bands show the histogram of the data in the X-axis. Interaction with the healthy status is displayed by showing healthy patients in red</p></caption><graphic xlink:href="12903_2023_3112_Fig6_HTML" id="MO6"/></fig></p>
      <p id="Par49">Figure <xref rid="Fig6" ref-type="fig">6</xref>a shows that, for SHCN patients, a higher number of teeth with pulpar involvement is associated with less likelihood of a second sedation. From Fig. <xref rid="Fig6" ref-type="fig">6</xref>b we see that the majority of the patients have undergone fillings during the first sedation, but it is noticeable that patients, especially SHCN, that did not receive that treatment are more likely to be sedated again. Finally, the model shows in Fig. <xref rid="Fig6" ref-type="fig">6</xref>c that the absence of plaque before the first sedation is a clear indicator that the probability of a subsequent sedation is low.</p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>Discussion</title>
    <p id="Par50">We separate our discussion into two parts: first, we focus on the technical aspects of our tool and compare them with previous similar works, then we discuss the relevant findings of our case study.</p>
    <sec id="Sec12">
      <title>Technical aspects</title>
      <p id="Par51">A general overview of ML methods in healthcare can be found in [<xref ref-type="bibr" rid="CR2">2</xref>]. Their potential advantages are outlined in [<xref ref-type="bibr" rid="CR1">1</xref>], while [<xref ref-type="bibr" rid="CR7">7</xref>] discusses how to develop effective ML models for healthcare and related challenges and problems. As in other healthcare fields, the use of data-driven methods in odontology is growing quickly. Several systematic reviews focused in particular fields are available: Revilla-Leon reviews the application of ML methods in restorative dentistry [<xref ref-type="bibr" rid="CR24">24</xref>] and also for gingivitis and periodontal disease [<xref ref-type="bibr" rid="CR25">25</xref>]. More general reviews can be found in [<xref ref-type="bibr" rid="CR26">26</xref>] and [<xref ref-type="bibr" rid="CR27">27</xref>]. Most of the reviewed works are focused on diagnosis and prediction and in the majority of cases the ML methods have been applied to radiography and medical images. Particular and general systematic reviews stress the potential and accuracy of these methods but also warn that they are still in development.</p>
      <p id="Par52">We do not further discuss image-based methods, the technique most applied, but focus on the works most similar to our own in this paper, that is, those using clinical, behavioral, demographic, and laboratory data as input predictors for a ML model. A prominent example is given by Karhade et al. [<xref ref-type="bibr" rid="CR28">28</xref>] who developed an automated ML model for classification of early childhood caries (ECC). As in our work, they performed a feature selection procedure, introducing different sets of plausible predictors, from a total number of 14 features, to the Google AutoML framework and performing an iteratively search process using the classification accuracy of the model for selecting the best performing model and proceeding with internal validation. But the feature selection procedure is not described in detail and no systematic search for features has been done. On the contrary, our feature selection algorithm tests all the combinations for a comparable size of the total set of predictors, or otherwise systematically searches the best combination for larger feature sets. In addition, ORIENTATE provides different global and individual interpretation plots to assess the relevance the model assigns to each predictor and support further inferences. Qu et al. [<xref ref-type="bibr" rid="CR29">29</xref>] provide a prediction model for early childhood caries risk based on behavioral data. For feature selection, they manually remove the low-variance features and then use all of them to train only three different classifiers. They only provide the relative weights assigned by the model to the features used, without further explanatory reports. Campo et al. [<xref ref-type="bibr" rid="CR30">30</xref>] evaluate different classifiers for the need for a root canal retreatment for a dataset with 205 cases, and propose a case-based reasoning algorithm with Bayesian networks that outperforms other types of classifiers. Unlike our work, they do not carry out a feature selection search but use all the features, and they extract the more relevant variables but do not provide further interpretability analysis or plots. Similarly, Thanathornwong [<xref ref-type="bibr" rid="CR31">31</xref>] develops a Bayesian network decision support system for the prediction of the need of an orthodontic treatment. No feature selection search is done but they provide a graphical interface for the application and the results are compared to the assessments of two experts. Finally, Cui et al. [<xref ref-type="bibr" rid="CR32">32</xref>] also trained several multiclass classifiers for tooth extraction, retention or restorative treatment. The authors do perform a feature selection algorithm after performing a feature extraction phase that recovers features and values from clinical records, retaining 34 features for the model. The most relevant features for the model are shown but no further explanation, interaction or interpretation plot is provided. Results were compared to expert predictions and showed that the model outperforms the experts.</p>
      <p id="Par53">Regarding the predictive capabilities of our models, our results agree with others [<xref ref-type="bibr" rid="CR28">28</xref>], as expected, in that the number of features used as predictors is not directly correlated with the accuracy of predictions, see Fig. <xref rid="Fig4" ref-type="fig">4</xref>, and moreover, it is not obvious which ones are going yield the best-performing classifier. In Karhade et al. [<xref ref-type="bibr" rid="CR28">28</xref>], they found that a parsimonious model including only two predictors (children age and the oral health status reported by parents) achieved the highest accuracy as predictors of ECC. A notable difference with this work is that they do not systematically search for the best combination of predictors, even though the size of their predictor set makes it computationally feasible (14 predictors). We believe the reason is that even though they use a reasonably user-friendly platform (Google AutoML), it still requires a certain technical background to truly automate a feature selection process, a problem that we intend to ameliorate with ORIENTATE. The accuracy of our best model is reasonably good (0.92 AUC, 0.75 recall and 0,92 precision) and higher than theirs (0.74 AUC, 0.67 recall and 0.64 precision), even though the size of their dataset is much larger (6404 samples) than ours (230 samples), which may also be due to their selected predictors. Therefore, we consider that an effective feature selection algorithm is essential for this kind of application and intend to improve ours as a future work.</p>
      <p id="Par54">Supporting feature relevance and global interpretation methods allows us to use our tool for statistical inference that can replace and/or complement classical statistical studies, as discussed by Bzdok et al. [<xref ref-type="bibr" rid="CR14">14</xref>], where the similarities and differences between ML and classical statistical studies are analyzed. To elaborate on this point let us note that our application provides three main services: (1) to obtain a descriptive view of the dataset, (2) to find the best classifier model for a fixed set of features of interest (CE) and (3) to find the best subset of features and associated model from a broader set of features of interest (FS). The latter service is the most interesting from our point of view, because it allows to achieve two goals: first, it covers partially the goals of statistical inference, that is, it <italic>draws inferences</italic> about a population from the sample, which is the subject of what we call <italic>traditional</italic> studies. And second, it finds the combination of features and model that best <italic>predicts</italic> our target variable, which is the goal of ML algorithms typically. Let us clarify that inferences drawn by ML methods are derived by observing the subset of features used by the model <italic>and</italic> the contribution of each feature to the outcome (feature relevance) [<xref ref-type="bibr" rid="CR14">14</xref>]. As an illustrative example, in our use case study our target is to predict the value of the variable <italic>SecondSedation</italic>, that is, the need to perform a second intervention under sedation for the subject, and we see that the best classifier uses, among others, the <italic>Healthy</italic> feature, that is, the absence of pathologies in the subject, and it is given a high relevance by the model. The equivalent procedure in a traditional study is to perform a statistical test against the null hypothesis that <italic>Healthy</italic> is associated with <italic>SecondSedation</italic> [<xref ref-type="bibr" rid="CR14">14</xref>].</p>
      <p id="Par55">The ML approach to inference has some advantages: it can handle a large number of variables (features) and few samples, multiple testing and nonparametric models [<xref ref-type="bibr" rid="CR14">14</xref>]. But it has several particular drawbacks too: ML models tend to use regularization to obtain the simplest model that predicts well, which means that it selects features that may capture efficiently the effects of other variables through correlations but are not meaningful to the user. For instance, in our case test, we find that the pulpar treatments on a particular tooth (75) have relevance for the prediction of the target variable, but which the actual relationship may be is not obvious at all. Additionally, ML models typically have worked as <italic>black boxes</italic>, where the user inputs some data and gets a result, without any clue about the actual process to reach the output [<xref ref-type="bibr" rid="CR33">33</xref>]. Fortunately, a number of methods to explain ML output have been developed [<xref ref-type="bibr" rid="CR33">33</xref>] and, in particular, the SHAP reports used here supply both individual and global explanations in intuitive formats, even though they have their own issues [<xref ref-type="bibr" rid="CR23">23</xref>]. Both ML and classical statistic studies are complementary in several aspects: with FS, the ML approach can highlight relevant features quickly but is not appropriate in this form to examine the associations of particular variables that might be of interest to the practitioner but have been discarded by the model during the feature search. Instead, using CE is useful to test those cases, by including the variable of interest and examining the model results, which can and should be complemented by additional classical statistical tests.</p>
      <p id="Par56">As a summary, from the technical point of view, therefore, the distinguishing aspects of our proposal compared to previous works are: most of them use a custom application tailored for a specific target, while we provide a general purpose classifier evaluation application, and our tool supports systematic feature selection search and extensive interpretation functionality with a user interface designed for users with non-technical background. This latter aspect also allows ORIENTATE to be used to draw inferences about the features like classical statistical studies do. A limitation of our study is the small sample size of the dataset that we have used to test the application. Most of ML algorithms improve their performance with larger datasets. But, an advantage of our tool is that it is agnostic with respect to the dataset as we said, so it can be applied seamlessly to any other dataset in the proper format. As future steps we intend to collect additional datasets for evaluation. We should finally note that the interpretation of the features relevance may also be considered another limitation of our tool: the major risk of using predictive methods for inference is that incorrect interpretation of the model results in unwarranted causal inferences [<xref ref-type="bibr" rid="CR34">34</xref>]. This kind of limitation is also acknowledged for example in [<xref ref-type="bibr" rid="CR29">29</xref>], where they warn that some of the predictors were not considered causative. But this is a problem shared with classical statistical methods [<xref ref-type="bibr" rid="CR34">34</xref>]. To tackle this problem, we plan to extend the functionality of ORIENTATE to apply causal inference methods to our datasets [<xref ref-type="bibr" rid="CR34">34</xref>]: there are programming libraries [<xref ref-type="bibr" rid="CR35">35</xref>] that can be seamlessly integrated with ORIENTATE.</p>
    </sec>
    <sec id="Sec13">
      <title>Case study</title>
      <p id="Par57">According to the relevance assigned to each feature by our model, the predictive factors were, in order, for healthy children: systemic health status, number of teeth with pulpar treatment at the first sedation, prevention tracking, number of teeth with pulpar involvement before the first sedation, presence of bacterial plaque, number of teeth extracted due to pathology and the number of fillings (Fig. <xref rid="Fig5" ref-type="fig">5</xref>a). For SHCN children, they were: systemic health status, prevention tracking, number of teeth with pulpar treatment at the first sedation, number of teeth with pulpar involvement before the first sedation, presence of bacterial plaque, number of teeth extracted due to pathology and the number of fillings (Fig. <xref rid="Fig5" ref-type="fig">5</xref>a).</p>
      <p id="Par58">According to our model, thus, healthy children would not need a second sedation, unlike SHCN children (Fig. <xref rid="Fig5" ref-type="fig">5</xref>b). This difference between both groups of children has also been observed in other published studies [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR37">37</xref>]. For SHCN, complying with the prevention tracking program after the first sedation is a predictive factor for a second sedation. One of the goals of the prevention tracking program is to achieve a collaborative attitude in the patient, which would prevent future treatments under sedation. Thus, several authors believe that the lack of preventive programs or a bad design of them may be the cause of the reinterventions [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR38">38</xref>–<xref ref-type="bibr" rid="CR41">41</xref>]. However, few works have evaluated the effect of those programs in the long term [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR42">42</xref>, <xref ref-type="bibr" rid="CR43">43</xref>]. In our previous study [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>] we showed that 80% of healthy children that complied with the prevention tracking program after a first sedation were able to receive at the dental chair treatments of relative complexity. On the contrary, due to their physical or mental limitations, only 18.4% of SHCN children were able to receive this kind of treatment at the dental chair. Despite it may seem contradictory, the fact that children show up at the prevention follows-up is a predictive factor for a second sedation. The reason is, as we said, that the prevention tracking program for this children, in addition to implement preventive measures, allows to carry out an early diagnosis of the therapeutic needs that will have to be implemented under sedation for them. A limitation here is that we have patients for whom the compliance of the prevention program has not been recorded. Let us recall that compliance with the prevention tracking program is a ternary variable. That is, some patients were referred from other clinics and returned after the first sedation, so their compliance with the prevention tracking program was not recorded. For those patients, the <italic>PreventionTracking</italic> feature takes value 2 and is shown in red in Fig. <xref rid="Fig5" ref-type="fig">5</xref>b, for children that did have complied with the prevention program the value is 1 and is shown in purple, while children that did not follow the program, with value 0, are shown in blue. We see that not-recorded values (red) decrease the likelihood of a second sedation, since those patients do not come back to the clinic in many cases and we do not have records about the evolution of their oral pathology.</p>
      <p id="Par59">For healthy children, the second predictive factor in importance according to ORIENTATE was the number of teeth with pulpar treatment at the first sedation. However, in our previous study [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>] neither the initial oral pathology nor the treatments done during the first sedation were indicators that could predict the need for a second sedation. But other works [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR43">43</xref>] did report that children that received more conservative restoration treatment during the first intervention tend to need more retreatments under deep sedation.</p>
      <p id="Par60">The fourth predictive factor for both groups of children was the number of teeth with pulpar involvement before the first sedation. In fact, when the number of such teeth increases, the probability a second sedation decreases (Fig. <xref rid="Fig5" ref-type="fig">5</xref>b). In SHCN patients, a higher number of teeth with pulpar involvement is associated with less likelihood of a second sedation, and similarly for healthy children but in a less pronounced way (Fig. <xref rid="Fig6" ref-type="fig">6</xref>a). As we already found in our previous study [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>], SHCN children are usually older so most of those teeth with pulpar involvement were about to exfoliate. In those cases, the selected treatment was extraction at the sedation. This way, since those children have more extractions and, since due to having fewer teeth, the likelihood of dental pathology is lower, ORIENTATE decreases also the probability of a second sedation. This particular result allows us to remark that, to make inferences, we have to complement the model results and explanations with the rest of the data available, in this case, the age of the patients, a feature that was not selected by the FS algorithm.</p>
      <p id="Par61">The model also shows that SHCN children that have not had fillings done at the first sedation are more likely to be sedated again (Fig. <xref rid="Fig6" ref-type="fig">6</xref>b) and the absence of plaque before the first sedation is a clear indicator that the probability of a subsequent sedation is low and vice versa (Fig. <xref rid="Fig6" ref-type="fig">6</xref>c). This seeming contradiction is due to the fact that many SHCN children without caries lesions are sedated only to remove the large amount of tartar that they produce, due to their diet and poor hygiene, since they are not able to collaborate with the dentist for its removal. So, in our previous study [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>], we found that 3 out of a total of 7 healthy children (42,85%) needed a second sedation to undergo a tartar removal procedure, whereas 42 out of a total of 48 of SHCN children (87,5%) needed it.</p>
      <p id="Par62">In summary, we have shown that the predictive factors for a second sedation found by ORIENTATE are coherent and complement the ones previously found in our classical study. For SHCN children, the systemic health status was the more relevant, followed by prevention tracking, number of teeth with pulpar treatment at the first sedation, number of teeth with pulpar involvement before the first sedation, presence of bacterial plaque, number of teeth extracted due to pathology and the number of fillings. In addition, it is capable of detecting predictive factors, which would escape the usual analysis of clinical practice, and even may seem contradictory but, upon careful reflection, turn out to be consistent as we have discussed.</p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Conclusion</title>
    <p id="Par63">ORIENTATE allows the automated application of machine learning classification algorithms on general datasets by clinical practitioners lacking technical skills. It can find the best subset of predictors for a given target variable and shows several graphs that facilitate the explanation of the classification model results, using global interpretation methods, and an interface for the prediction of new input samples. For the case study, our tool was able to achieve a high prediction accuracy for a second sedation in SHCN children, despite the dataset being heavily imbalanced and its small size. The analysis of the relevance of the features showed that, for healthy children, the number of teeth with pulpar treatments at the first sedation is a predictive factor for a second sedation, whereas for SHCN children a predictive factor was the compliance with the prevention tracking program. Our analysis is complementary to others done with classical statistical methods. In summary, ORIENTATE achieves several goals: first, it automatically finds suitable features and generates accurate classifiers for predictive tasks. Second, it helps researchers without specific skills in data methods in both the application of machine learning classification and as a complement to classical studies for inferential analysis of features.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>AUC</term>
        <def>
          <p id="Par4">Area Under the Curve</p>
        </def>
      </def-item>
      <def-item>
        <term>CE</term>
        <def>
          <p id="Par5">Classifier Evaluation</p>
        </def>
      </def-item>
      <def-item>
        <term>FS</term>
        <def>
          <p id="Par6">Feature Selection</p>
        </def>
      </def-item>
      <def-item>
        <term>ML</term>
        <def>
          <p id="Par7">Machine Learning</p>
        </def>
      </def-item>
      <def-item>
        <term>ORIENTATE</term>
        <def>
          <p id="Par8">applicatiOn of machine leaRning for classIfication of dENTal pATiEnts</p>
        </def>
      </def-item>
      <def-item>
        <term>ROC</term>
        <def>
          <p id="Par9">Receiver Operating Characteristic</p>
        </def>
      </def-item>
      <def-item>
        <term>SHCN</term>
        <def>
          <p id="Par10">Special Health Care Needs</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>IGR: Conceptualization , Investigation, Data Curation, Writing - Original Draft, Writing - Review and Editing, Methodology, Resources. EEL: Conceptualization, Software, Validation, Formal Analysis, Writing - Original Draft, Writing - Review and Editing, Visualization, Funding acquisition. AJOR: Conceptualization, Writing - Review and Edit, Methodology, Supervision.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was partially funded by grant PID2020-112675RB-C41 (ONOFRE-3) funded by MCIN/AEI/10.13039/501100011033.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p> The source code of ORIENTATE as well as the datasets generated and/or analysed during the current study are available via the following link: <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/esteban.egea/orientate">https://gitlab.com/esteban.egea/orientate</ext-link>.</p>
    <p> Project name: ORIENTATE</p>
    <p> Project home page: <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/esteban.egea/orientate">https://gitlab.com/esteban.egea/orientate</ext-link></p>
    <p> Operating system(s): Platform independent</p>
    <p> Programming language: Python</p>
    <p> Other requirements: python v3.9.12, flask v2.0.3, scikit-learn v1.1.1, shap v0.41.0, flask-cache v0.13.0.1, bokeh v2.4.3, xgboost v1.6.1, graphviz v0.20.</p>
    <p> License: MIT</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par64">The study described in the case study was conducted in accordance with the Declaration of Helsinki and approved by the Research Ethics Committee of the University of Murcia (ID:2034/2018). Informed consent was obtained from all subjects and/or their legal guardian(s).</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par65">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par66">The authors declare no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Norgeot</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Glicksberg</surname>
            <given-names>BS</given-names>
          </name>
          <name>
            <surname>Butte</surname>
            <given-names>AJ</given-names>
          </name>
        </person-group>
        <article-title>A call for deep-learning healthcare</article-title>
        <source>Nat Med.</source>
        <year>2019</year>
        <volume>25</volume>
        <fpage>14</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-018-0320-3</pub-id>
        <?supplied-pmid 30617337?>
        <pub-id pub-id-type="pmid">30617337</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Alison Callahan</surname>
            <given-names>NHS</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Aziz Sheikh</surname>
            <given-names>AW</given-names>
          </name>
          <name>
            <surname>Cresswell</surname>
            <given-names>KM</given-names>
          </name>
        </person-group>
        <article-title>Machine learning in healthcare</article-title>
        <source>Key Advances in Clinical Informatics</source>
        <year>2017</year>
        <edition>1</edition>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Academic Press</publisher-name>
        <fpage>279</fpage>
        <lpage>291</lpage>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qayyum</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Qadir</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bilal</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Al-Fuqaha</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Secure and robust machine learning for healthcare: A survey</article-title>
        <source>IEEE Rev Biomed Eng.</source>
        <year>2021</year>
        <volume>14</volume>
        <fpage>156</fpage>
        <lpage>180</lpage>
        <pub-id pub-id-type="doi">10.1109/RBME.2020.3013489</pub-id>
        <?supplied-pmid 32746371?>
        <pub-id pub-id-type="pmid">32746371</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ngiam</surname>
            <given-names>KY</given-names>
          </name>
          <name>
            <surname>Khor</surname>
            <given-names>IW</given-names>
          </name>
        </person-group>
        <article-title>Big data and machine learning algorithms for health-care delivery</article-title>
        <source>Lancet Oncol.</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>5</issue>
        <fpage>262</fpage>
        <lpage>273</lpage>
        <pub-id pub-id-type="doi">10.1016/S1470-2045(19)30149-4</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Lundberg SM, Nair B, Vavilala MS, Horibe M, Eisses MJ, Adams T, Liston DE, Low DKW, Newman SF, Kim J, Lee SI. Explainable machine-learning predictions for the prevention of hypoxaemia during surgery. Nat Biomed Eng. 2018;2. 10.1038/s41551-018-0304-0.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Qazi N, Pawar M, Padhly PP, Pawar V, D’Amico C, Nicita F, Fiorillo L, Alushi A, Minervini G, Meto A. Teledentistry: Evaluation of instagram posts related to bruxism. Technol Health Care. 2023. 10.3233/thc-220910.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>P-HC</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>How to develop machine learning models for healthcare</article-title>
        <source>Nat Mater</source>
        <year>2019</year>
        <volume>18</volume>
        <fpage>410</fpage>
        <lpage>414</lpage>
        <pub-id pub-id-type="doi">10.1038/s41563-019-0345-0</pub-id>
        <?supplied-pmid 31000806?>
        <pub-id pub-id-type="pmid">31000806</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Alaa AM, Bolton T, Angelantonio ED, Rudd JHF, van der Schaar M. Cardiovascular disease risk prediction using automated machine learning: A prospective study of 423,604 uk biobank participants. PLoS ONE. 2019;14. 10.1371/journal.pone.0213653.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Michel</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Grisel</surname>
            <given-names>O</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>J Mach Learn Res.</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Waring</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lindvall</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Umeton</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Automated machine learning: Review of the state-of-the-art and opportunities for healthcare</article-title>
        <source>Artificial Intelligence in Medicine.</source>
        <year>2020</year>
        <volume>104</volume>
        <fpage>101822</fpage>
        <pub-id pub-id-type="doi">10.1016/j.artmed.2020.101822</pub-id>
        <?supplied-pmid 32499001?>
        <pub-id pub-id-type="pmid">32499001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Lundberg SM, Lee S-I. A unified approach to interpreting model predictions. In: Guyon I, Luxburg UV, Bengio S, Wallach H, Fergus R, Vishwanathan S, Garnett R, editors. Advances in Neural Information Processing Systems, vol. 30. Curran Associates, Inc; 2017. p. 4765–74.</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Featherstone</surname>
            <given-names>JDB</given-names>
          </name>
          <name>
            <surname>Chaffee</surname>
            <given-names>BW</given-names>
          </name>
        </person-group>
        <article-title>The evidence for caries management by risk assessment (cambra®)</article-title>
        <source>Adv Dent Res.</source>
        <year>2018</year>
        <volume>29</volume>
        <issue>1</issue>
        <fpage>9</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="doi">10.1177/0022034517736500</pub-id>
        <?supplied-pmid 29355423?>
        <pub-id pub-id-type="pmid">29355423</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Ismail A, Pitts N, Tellez M, Banerjee A, Deery C, Douglas G, et al. The international caries classification and management system (iccms™) an example of a caries management pathway. BMC Oral Health. 2015;15((Suppl 1):S9). 10.1186/1472-6831-15-S1-S9.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bzdok</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Altman</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Krzywinski</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Statistics versus machine learning</article-title>
        <source>Nat Methods.</source>
        <year>2018</year>
        <volume>15</volume>
        <fpage>233</fpage>
        <lpage>234</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.4642</pub-id>
        <?supplied-pmid 30100822?>
        <pub-id pub-id-type="pmid">30100822</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gómez-Ríos</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Pérez-Silva</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Serna-Muñoz</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ibáñez-López</surname>
            <given-names>FJ</given-names>
          </name>
          <name>
            <surname>Periago-Bayonas</surname>
            <given-names>PM</given-names>
          </name>
          <name>
            <surname>Ortiz-Ruiz</surname>
            <given-names>AJ</given-names>
          </name>
        </person-group>
        <article-title>Deep sedation for dental care management in healthy and special health care needs children: A retrospective study</article-title>
        <source>Int J Environ Res Public Health.</source>
        <year>2023</year>
        <volume>20</volume>
        <fpage>3435</fpage>
        <pub-id pub-id-type="doi">10.3390/ijerph20043435</pub-id>
        <?supplied-pmid 36834126?>
        <pub-id pub-id-type="pmid">36834126</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Gomez-Rios I. Tratamiento odontológico bajo sedación profunda en una población infantil: estudio de cohortes retrospectivo. PhD thesis, Universidad de Murcia. 2022. <ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/10201/119116">http://hdl.handle.net/10201/119116</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Liu C-H, Lin C-J, Hu, Y-H, You Z-H. Predicting the Failure of Dental Implants Using Supervised Learning Techniques. 10.5281/zenodo.1227714.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tharwat</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Classification assessment methods. Appl Comput</article-title>
        <source>Inform.</source>
        <year>2021</year>
        <volume>17</volume>
        <fpage>168</fpage>
        <lpage>192</lpage>
        <pub-id pub-id-type="doi">10.1016/j.aci.2018.08.003</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">ISO 3950:2016(E): Dentistry – designation system for teeth and areas of the oral cavity. Standard, International Organization for Standardization, Geneva, CH. 2016.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chandrashekar</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Sahin</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A survey on feature selection methods</article-title>
        <source>Comput Electr Eng.</source>
        <year>2014</year>
        <volume>40</volume>
        <fpage>16</fpage>
        <lpage>28</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compeleceng.2013.11.024</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lundberg</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Erion</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>DeGrave</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Prutkin</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Nair</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Katz</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Himmelfarb</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bansal</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>S-I</given-names>
          </name>
        </person-group>
        <article-title>From local explanations to global understanding with explainable ai for trees</article-title>
        <source>Nat Mach Intell.</source>
        <year>2020</year>
        <volume>2</volume>
        <fpage>56</fpage>
        <lpage>67</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-019-0138-9</pub-id>
        <?supplied-pmid 32607472?>
        <pub-id pub-id-type="pmid">32607472</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">American Academy of Pediatric Dentistry. Definition of Special Health Care Needs. The Reference Manual of Pediatric Dentistry. Chicago: American Academy of Pediatric Dentistry; 2021.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Kumar IE, Venkatasubramanian S, Scheidegger C, Friedler S. Problems with shapley-value-based explanations as feature importance measures. In: III, H.D, Singh A, editors. Proceedings of the 37th International Conference on Machine Learning. Proceedings of Machine Learning Research. vol. 119. PMLR; 2020. pp. 5491–5500. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v119/kumar20e.html">https://proceedings.mlr.press/v119/kumar20e.html</ext-link></mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Revilla-León</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gómez-Polo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Vyas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Barmak</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Özcan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Att</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Krishnamurthy</surname>
            <given-names>VR</given-names>
          </name>
        </person-group>
        <article-title>Artificial intelligence applications in restorative dentistry: A systematic review</article-title>
        <source>J Prosthet Dent.</source>
        <year>2022</year>
        <volume>128</volume>
        <fpage>867</fpage>
        <lpage>875</lpage>
        <pub-id pub-id-type="doi">10.1016/j.prosdent.2021.02.010</pub-id>
        <?supplied-pmid 33840515?>
        <pub-id pub-id-type="pmid">33840515</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Revilla-León M, Gómez-Polo M, Barmak AB, Inam W, Kan JYK, Kois JC, Akal O. Artificial intelligence models for diagnosing gingivitis and periodontal disease: A systematic review. J Prosthet Dent. 2022. 10.1016/j.prosdent.2022.01.026.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shan</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tay</surname>
            <given-names>FR</given-names>
          </name>
          <name>
            <surname>Gu</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Application of artificial intelligence in dentistry</article-title>
        <source>J Dent Res.</source>
        <year>2021</year>
        <volume>100</volume>
        <fpage>232</fpage>
        <lpage>244</lpage>
        <pub-id pub-id-type="doi">10.1177/0022034520969115</pub-id>
        <?supplied-pmid 33118431?>
        <pub-id pub-id-type="pmid">33118431</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Khanagar</surname>
            <given-names>SB</given-names>
          </name>
          <name>
            <surname>Al-ehaideb</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Maganur</surname>
            <given-names>PC</given-names>
          </name>
          <name>
            <surname>Vishwanathaiah</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Patil</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Baeshen</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Sarode</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Bhandi</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Developments, application, and performance of artificial intelligence in dentistry – a systematic review</article-title>
        <source>J Dent Sci.</source>
        <year>2021</year>
        <volume>16</volume>
        <fpage>508</fpage>
        <lpage>522</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jds.2020.06.019</pub-id>
        <?supplied-pmid 33384840?>
        <pub-id pub-id-type="pmid">33384840</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Karhade DS, Roach J, Shrestha P, Simancas-Pallares MA, Ginnis J, Burk ZJS, et al. An automated machine learning classifier for early childhood caries. Pediatr Dent. 2021;43:191–97.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Houser</surname>
            <given-names>SH</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Prediction model for early childhood caries risk based on behavioral determinants using a machine learning algorithm</article-title>
        <source>Comput Methods Prog Biomed.</source>
        <year>2022</year>
        <volume>227</volume>
        <fpage>107221</fpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2022.107221</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Campo L, Aliaga IJ, Paz JFD, García AE, Bajo J, Villarubia G, Corchado JM. Retreatment predictions in odontology by means of cbr systems. Comput Intell Neurosci. 2016;2016:1–11. 10.1155/2016/7485250.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thanathornwong</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Bayesian-based decision support system for assessing the needs for orthodontic treatment</article-title>
        <source>Healthcare Inform Res.</source>
        <year>2018</year>
        <volume>24</volume>
        <fpage>22</fpage>
        <pub-id pub-id-type="doi">10.4258/hir.2018.24.1.22</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cui</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Wen</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Clinical decision support model for tooth extraction therapy derived from electronic dental records</article-title>
        <source>J Prosthet Dent.</source>
        <year>2021</year>
        <volume>126</volume>
        <fpage>83</fpage>
        <lpage>90</lpage>
        <pub-id pub-id-type="doi">10.1016/j.prosdent.2020.04.010</pub-id>
        <?supplied-pmid 32703604?>
        <pub-id pub-id-type="pmid">32703604</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tjoa</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>A survey on explainable artificial intelligence (xai): Toward medical xai</article-title>
        <source>IEEE Trans Neural Netw Learn Syst.</source>
        <year>2021</year>
        <volume>32</volume>
        <fpage>4793</fpage>
        <lpage>4813</lpage>
        <pub-id pub-id-type="doi">10.1109/TNNLS.2020.3027314</pub-id>
        <?supplied-pmid 33079674?>
        <pub-id pub-id-type="pmid">33079674</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Pearl J, Causality. Cambridge University Press; 2009. 10.1017/CBO9780511803161.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Sharma A, Kiciman E. DoWhy: A Python Package for Causal Inference. <ext-link ext-link-type="uri" xlink:href="https://github.com/microsoft/dowhy">https://github.com/microsoft/dowhy</ext-link>. Accessed 15 June 2023.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Guidry J, Bagher S, Felemban O, Rich A, Loo C, Reasons of repeat dental treatment under general anaesthesia: A retrospective study. European Journal of Paediatric Dentistry. 2017;18. 10.23804/ejpd.2017.18.04.09.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">König T, Reicherts P, Leha A, Hrasky V, Wiegand A. Retrospective study on risk factors for repeated dental treatment of children under general anaesthesia. Eur J Paediatr Dent. 2020;21. 10.23804/ejpd.2020.21.03.04.</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Al-Ogayyel S, Ali SA-H. Comparison of dental treatment performed under general anesthesia between healthy children and children with special health care needs in a hospital setting, Saudi Arabia. J Clin Exp Dent. 2018;10:0–0. 10.4317/jced.55060.</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Savanheimo</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Vehkalahti</surname>
            <given-names>MM</given-names>
          </name>
        </person-group>
        <article-title>Preventive aspects in children’s caries treatments preceding dental care under general anaesthesia</article-title>
        <source>Int J Paediatr Dent.</source>
        <year>2008</year>
        <volume>18</volume>
        <fpage>117</fpage>
        <lpage>123</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1365-263X.2007.00858.x</pub-id>
        <?supplied-pmid 18237294?>
        <pub-id pub-id-type="pmid">18237294</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bader</surname>
            <given-names>RM</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Almuhtaseb</surname>
            <given-names>EY</given-names>
          </name>
        </person-group>
        <article-title>A retrospective study of paediatric dental patients treated under general anesthesia</article-title>
        <source>Int J Clin Med.</source>
        <year>2013</year>
        <volume>04</volume>
        <fpage>18</fpage>
        <lpage>23</lpage>
        <pub-id pub-id-type="doi">10.4236/ijcm.2013.47A2005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mallineni</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Yiu</surname>
            <given-names>CKY</given-names>
          </name>
        </person-group>
        <article-title>A retrospective review of outcomes of dental treatment performed for special needs patients under general anaesthesia: 2-year follow-up</article-title>
        <source>Sci World J.</source>
        <year>2014</year>
        <volume>2014</volume>
        <fpage>1</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1155/2014/748353</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Foster</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Perinpanayagam</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Pfaffenbach</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Certo</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Recurrence of early childhood caries after comprehensive treatment with general anesthesia and follow-up</article-title>
        <source>J Dent Child.</source>
        <year>2006</year>
        <volume>73</volume>
        <fpage>25</fpage>
        <lpage>30</lpage>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Almeida AG, Roseman MM, Sheff M, Huntington N, Hughes CV. Future caries susceptibility in children with early childhood caries following treatment under general anesthesia. Pediatr Dent. 2000;22.</mixed-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Oral Health</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Oral Health</journal-id>
    <journal-title-group>
      <journal-title>BMC Oral Health</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1472-6831</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10283267</article-id>
    <article-id pub-id-type="pmid">37340367</article-id>
    <article-id pub-id-type="publisher-id">3112</article-id>
    <article-id pub-id-type="doi">10.1186/s12903-023-03112-w</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ORIENTATE: automated machine learning classifiers for oral health prediction and research</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Gomez-Rios</surname>
          <given-names>Inmaculada</given-names>
        </name>
        <address>
          <email>macu@innovadental.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Egea-Lopez</surname>
          <given-names>Esteban</given-names>
        </name>
        <address>
          <email>esteban.egea@upct.es</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ortiz Ruiz</surname>
          <given-names>Antonio José</given-names>
        </name>
        <address>
          <email>ajortiz@um.es</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.10586.3a</institution-id><institution-id institution-id-type="ISNI">0000 0001 2287 8496</institution-id><institution>Department of Dermatology, Stomatology, Radiology and Physical Medicine, </institution><institution>Universidad de Murcia, </institution></institution-wrap>Murcia, Spain </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.218430.c</institution-id><institution-id institution-id-type="ISNI">0000 0001 2153 2602</institution-id><institution>Dept. Information Technologies and Communications, </institution><institution>Universidad Politecnica de Cartagena (UPCT), </institution></institution-wrap>Cartagena, Spain </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>20</day>
      <month>6</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>23</volume>
    <elocation-id>408</elocation-id>
    <history>
      <date date-type="received">
        <day>8</day>
        <month>3</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>6</day>
        <month>6</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">The application of data-driven methods is expected to play an increasingly important role in healthcare. However, a lack of personnel with the necessary skills to develop these models and interpret its output is preventing a wider adoption of these methods. To address this gap, we introduce and describe ORIENTATE, a software for automated application of machine learning classification algorithms by clinical practitioners lacking specific technical skills. ORIENTATE allows the selection of features and the target variable, then automatically generates a number of classification models and cross-validates them, finding the best model and evaluating it. It also implements a custom feature selection algorithm for systematic searches of the best combination of predictors for a given target variable. Finally, it outputs a comprehensive report with graphs that facilitates the explanation of the classification model results, using global interpretation methods, and an interface for the prediction of new input samples. Feature relevance and interaction plots provided by ORIENTATE allow to use it for statistical inference, which can replace and/or complement classical statistical studies.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">Its application to a dataset with healthy and special health care needs (SHCN) children, treated under deep sedation, was discussed as case study. On the example dataset, despite its small size, the feature selection algorithm found a set of features able to predict the need for a second sedation with a f1 score of 0.83 and a ROC (AUC) of 0.92. Eight predictive factors for both populations were found and ordered by the relevance assigned to them by the model. A discussion of how to derive inferences from the relevance and interaction plots and a comparison with a classical study is also provided.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">ORIENTATE automatically finds suitable features and generates accurate classifiers which can be used in preventive tasks. In addition, researchers without specific skills on data methods can use it for the application of machine learning classification and as a complement to classical studies for inferential analysis of features. In the case study, a high prediction accuracy for a second sedation in SHCN children was achieved. The analysis of the relevance of the features showed that the number of teeth with pulpar treatments at the first sedation is a predictive factor for a second sedation.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Machine learning</kwd>
      <kwd>Classification</kwd>
      <kwd>Special health care needs</kwd>
      <kwd>Deep sedation</kwd>
      <kwd>Predictive dentistry</kwd>
      <kwd>Second sedation risk</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© BioMed Central Ltd., part of Springer Nature 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par11">The application of data-driven methods is expected to play an increasingly important role in healthcare [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>], as they can be a particularly effective tool for diagnosis (disease presence) or prognosis (risk of future outcome), among other tasks. To mention a few representative examples: they have been used for the identification and classifications of different types of cancers [<xref ref-type="bibr" rid="CR3">3</xref>], or to predict individualized optimal drug doses for patients [<xref ref-type="bibr" rid="CR4">4</xref>]; they have also been used for natural language processing of medical records for improving the accuracy of appendicitis diagnoses [<xref ref-type="bibr" rid="CR4">4</xref>] and to prevent hypoxaemia during surgery [<xref ref-type="bibr" rid="CR5">5</xref>]. They also allow to analyze how reliable medical information is conveyed on social networks [<xref ref-type="bibr" rid="CR6">6</xref>]. Machine Learning (ML) is the branch of artificial intelligence that encompasses methods to make computers learn how to do some task from experience (data). In particular, ML uses statistical methods to predict outcomes in future data [<xref ref-type="bibr" rid="CR2">2</xref>], that is, to make classifications or predictions. ML algorithms are <italic>trained</italic> with data and fall into two broad categories: supervised learning, which uses data that have been previously labeled (with the desired or correct value), and unsupervised learning, which uses data that have not been labeled. As described in recent works [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>] the prediction accuracy of ML systems in some fields may meet or surpass that of the experts. Finally, ML systems handle well large volumes of high dimensional data [<xref ref-type="bibr" rid="CR2">2</xref>] and, therefore, can be a very effective tool for large scale preventive healthcare programs, for tasks such as large-scale screening [<xref ref-type="bibr" rid="CR7">7</xref>]. As an example, <italic>AutoPrognosis</italic>, a software that automatically builds an ensemble of predictive ML models, has been used to predict cardiovascular diseases: in a study done over 5 years with a sample of 4801 cases it was able to predict correctly 368 more cases than alternative classical methods [<xref ref-type="bibr" rid="CR8">8</xref>].</p>
    <p id="Par12">However, there are a number of challenges that prevent the wider adoption of these methods, both technical, such as the need for appropriate and interoperable data models [<xref ref-type="bibr" rid="CR2">2</xref>] and high-quality data [<xref ref-type="bibr" rid="CR7">7</xref>]; and operational, such as their integration in the clinical workflow [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR7">7</xref>]. One particular problem pointed out by Callahan [<xref ref-type="bibr" rid="CR2">2</xref>] is the <italic>lack of personnel with the necessary skills to develop these models and interpret their output</italic>. Developing an ML model for healthcare is a demanding task, which includes problem selection, data curation, development, and validation [<xref ref-type="bibr" rid="CR7">7</xref>]. Even though the application of sophisticated ML algorithms is relatively easy thanks to high-level programming libraries such as <italic>scikit-learn</italic> [<xref ref-type="bibr" rid="CR9">9</xref>], clinical practitioners usually lack the technical background required for its use. And conversely, technicians often lack the required clinical skills for appropriate model evaluation and refinement, which usually results in a time-consuming exchange of requirements and model prototypes between the two groups [<xref ref-type="bibr" rid="CR10">10</xref>]. In summary, a first step before deploying ML systems appropriate for preventive healthcare is the development of a validated model for the problem at hand, which is complicated due to a lack of complementary skills of the involved actors.</p>
    <p id="Par13">To address this problem, in this paper we introduce and describe ORIENTATE (applicatiOn of machine leaRning for classIfication of dENTal pATiEnts), a software that allows the use of sophisticated ML algorithms for the classification and prediction of oral health conditions by clinical practitioners lacking specific technical skills (<italic>users</italic>). Given a collected <italic>dataset</italic>, the application basically provides a web-based interface for the selection of its features (<italic>predictors</italic>) that may be useful in the prediction of some other variable of interest (<italic>target</italic>). It then generates a number of ML classification models and cross-validate them against subsets of a training set, selecting the best model according to some performance metric, and evaluating it against a validation set. The tool shows a detailed summary of the evaluation with graphs that facilitates the explanation of the ML model results, using global interpretation methods [<xref ref-type="bibr" rid="CR11">11</xref>], and an interface for the prediction of new input samples. The previous procedure summarizes a first stage of use of the tool, where researchers generate and evaluate a suitable prediction model from a given dataset. The integration of the tool into the clinical workflow would come in a second stage, where the validated model can be used, either with the provided interface or a custom (not developed yet) application. The practitioner would generate a prediction, just by filling in the corresponding predictors for a new patient, that may guide her in the planning of further treatments, complementing the information on caries risk and lesion management provided by clinical tools such as the Caries Management by Risk Assessment (CAMBRA) [<xref ref-type="bibr" rid="CR12">12</xref>] or the International Caries Classification and Management System (ICCMS) [<xref ref-type="bibr" rid="CR13">13</xref>].</p>
    <p id="Par14">A first goal of the tool is to facilitate testing different combinations of predictors according to the user criteria. It, therefore, allows an exploratory analysis that can be used to validate hypotheses about the influence of different features on the target variable [<xref ref-type="bibr" rid="CR7">7</xref>]. The tool guides the user in the development process of the model, offering the selection of the class of interest, alternatives for imputation of missing data and selection of the performance metric. It is also agnostic with respect to the dataset used, that is, it can be used with different datasets as long as they follow a minimal shared data model. Once the exploratory phase has finished, the obtained model has usually enough quality (prediction performance) as to be used in production. Therefore, trained models can be used in preventive tasks by clinical practitioners without technical training. A second goal is to perform systematic searches for the best combination of predictors for a target variable. A custom feature selection algorithm has been implemented, which tests thousands of combinations of predictors and returns the best model found together with an extensive explanatory report of the feature relevance and interactions. Finally, the explanatory report can be used to perform an inferential analysis of the data, complementary to classical statistical studies [<xref ref-type="bibr" rid="CR14">14</xref>]. This analysis can be done by examining the relevance and interactions of the predictors used by the model, an information that is visually provided by the generated report. Users with varying technical expertise may benefit from the use of ORIENTATE. Researchers only need to complement their dataset with an additional metadata CSV file in a simple format and then proceed with the model generation. In fact, providing metadata forces the researcher to analyze her data and collect and organized it in a systematic way, facilitating the discovery of errors and removing redundant information. The generated models can be critically analyzed by other researchers or practitioners just by examining the provided visual reports. The validation of the best model found is internally done by the tool by evaluating a separated test set from the available data. The prediction results for all the samples are provided to the user, so that she may assess directly the quality of the predictions, even individually, even with plots of the relevance of each predictor for the final result. Finally, as mentioned previously, clinical practitioners can generate new predictions just by filling out a form and then examining the provided report.</p>
    <p id="Par15">To illustrate the use of the application and the performance of the various functionalities, we use a particular case study: a dataset that describes the oral health condition in two populations, one with healthy children and another one with SHCN, was collected to compare the treatments performed under deep sedation in both populations. The dataset was analyzed with classical statistical methods in a previous work [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>]. This particular dataset is described in detail in the “<xref rid="Sec8" ref-type="sec">Material and methods of the previous study</xref>” section. We apply to this dataset the feature selection algorithm implemented by ORIENTATE and show how it finds a set of predictors that results in an accurate predictor model of the need for a second sedation. Next, we examine the relevance of the predictors and their interactions and discuss how they agree with the inferences derived in our previous work using classical statistical methods.</p>
    <p id="Par16">ORIENTATE source code is freely available in our repository, as well as the datasets used in the case study and an additional public dataset, whose format has been adapted, from [<xref ref-type="bibr" rid="CR17">17</xref>], which can be used to test the application.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <p id="Par17">ORIENTATE has been implemented as a web-based application and so it is independent of the platform and only requires a web browser to be used.</p>
    <sec id="Sec3">
      <title>Data source</title>
      <p id="Par18">The application is independent of the dataset used, as long as it is provided in the format described in the <xref rid="Sec6" ref-type="sec">Machine learning workflow</xref> section. The particular dataset used in our use case is described in detail in Material and methods of the previous study section.</p>
    </sec>
    <sec id="Sec4">
      <title>Libraries</title>
      <p id="Par19">A first prototype of the web application was implemented in Python v3.9.12 with Flask v2.0.3. Machine learning algorithms are implemented with SciKit-Learn v1.1.1 and SHAP v.0.41.0.</p>
    </sec>
    <sec id="Sec5">
      <title>Functionality</title>
      <p id="Par20">ORIENTATE allows uploading of the working dataset which is stored for future use. Once the dataset has been uploaded or selected from the available ones, the entry point of the web application shows a view of the current dataset and a menu with the available actions, as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a. The menu entries with Statistics and Distributions provide a statistical summary of the dataset and the empirical distributions of the variables respectively. The main functionality can be found in Classifier evaluation, (CE) and Feature selection (FS) as described next.<list list-type="bullet"><list-item><p id="Par21"><italic>Classifier evaluation (CE)</italic>. This link performs a search for the best classifier to predict a chosen target feature, given a set of features selected by the user. The application collects the information from the user in two separate steps as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>b and c. First, the user selects the target variable to be predicted and the set of features to be used as predictors. Next, the user selects the class of interest for binary variables. For instance, for the variable <italic>Caries</italic> the user selects whether she is interested in presence (class 1) or absence (class 0) of caries. For multiclass variables this action is skipped. The application shows the available options, and, in case of missing values (usually not registered) in the target feature, the user selects an imputation method for the missing values. Currently, the user can only select between removal of rows with missing values or replacement with a constant value. Once the input is collected, the application searches for the best classifier among a set of predefined types of classifiers, which are described in detail in the “<xref rid="Sec6" ref-type="sec">Machine learning workflow</xref>” section, and shows a summary of the results. This step is described in detail in the following section. A partial view of the evaluation results window is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>d. Our goal is to simplify the interpretation of the results to clinical practitioners, so in this step an explanation of the performance metrics is provided and additional information is presented to the user. The summary includes the metric scores and confusion matrix obtained for all the classifiers tested [<xref ref-type="bibr" rid="CR18">18</xref>]. The best classifier found according to the metric is separately highlighted. In addition, the predictions of the best classifier for each sample in the validation test are also displayed. That is, for all the samples in the validation test, the application shows a table with a row with the predicted value, the real value, the probability for each class and the values of the predictors. Finally, the user can predict, with the best classifier found, the value of a new sample by inserting the values of the predictors in a form provided. All the results can be downloaded for further use. The goal of this functionality is to let the user find a good classifier when the set of predictors (features) is previously established, for instance, by the expert knowledge of the clinical practitioner. The results allow to examine the differences between classifiers. It also allows to test how minor variations in the feature set, such as the replacement of a few predictors, influence the classifier performance.</p></list-item><list-item><p id="Par22"><italic>Feature selection (FS)</italic>. In many cases, however, the clinical practitioner is interested in finding out which features may help to predict a certain target variable. In fact, the goal of many studies is to uncover and describe the associational relationships between a set of features and the target variable. Similarly, our tool allows to search for the set of features that result in more accurate classifiers. This functionality can be viewed as an extension of CE, because the above procedure is repeated for different subsets of the feature set. Since it is usually not possible to test all the combinations of features, the application uses a feature selection algorithm that is described in detail in the <xref rid="Sec6" ref-type="sec">Machine learning workflow</xref> section. The application shows the user the same steps as in CE: first, a feature selection window, where the user selects the set of features for the search, and then the class selection and imputation window. In this case, the application informs the user of the number of combinations to be tested and asks for a name for the selection task to be created. Since this task is time-consuming usually, a background task is executed and the user is directed to another tab where the progress of the pending tasks is shown. Once the background task has finished the search for the best combination of features, the user can view the results. First, the user is presented with a summary of the feature selection search which includes: 1) a plot of the best metric achieved for each of the combinations of features tested, as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>a; 2) the best model and features found; 3) the relative relevance of each of the features for the best model found; and 4) when the best model found is a DecisionTree classifier, the application additionally shows the actual decision tree generated by the model, shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b. Our goal again is to provide the information in a way that can be interpreted by the clinical user as easily as possible. Relative feature relevance helps the user understand the importance that the model assigns to each feature, which can be critically assessed by the user. The generated decision tree is particularly helpful in this sense, since it allows the user to actually reproduce the decision process of the model and assess its soundness. Moreover, users can use the tree to derive their own protocols, adapting it to the clinical practice by refining or simplifyingthe tree. When clicking on the SHAP report button for the found model, further support for interpretation is provided. In this case, a SHAP report of the model is generated. SHAP [<xref ref-type="bibr" rid="CR11">11</xref>] is a method to explain individual predictions and generate global interpretations. The application computes the SHAP values and generates a number of global interpretation plots, which include feature importance, feature dependence, interactions and clustering of feature relevance, as shown in the snapshots in Fig. <xref rid="Fig2" ref-type="fig">2</xref>c. Finally, the user can inspect individual results. In that case, the application displays the individual predictions and corresponding SHAP explanations for all the elements in the dataset. The snapshot in Fig. <xref rid="Fig2" ref-type="fig">2</xref>d shows the particular values of the features for that individual as well as the predicted and real values of the target variable. Below, a SHAP waterfall plot displays how the values of the features contribute to the particular decision. In the <xref rid="Sec13" ref-type="sec">Case study</xref> section we use our case study to discuss and describe in detail those plots.</p></list-item><list-item><p id="Par23"><italic>Predictions for new samples</italic>. Once a satisfactory classifier has been found, either by CE or FE, it can be used to predict the result for new samples. ORIENTATE generates automatically a form, shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a, for the user to fill the values of the predictors for a new sample. After submitting, the results are displayed to the user, as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>b, including the probabilities for the different predicted classes and a plot for the SHAP values of the new samples to interpret how the feature values have influenced the newly predicted sampled.</p></list-item></list><fig id="Fig1"><label>Fig. 1</label><caption><p>Screenshots of the web application interface and steps for the evaluation of classifiers</p></caption><graphic xlink:href="12903_2023_3112_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par24">
        <fig id="Fig2">
          <label>Fig. 2</label>
          <caption>
            <p>Screenshots of the web application interface for the selection of features</p>
          </caption>
          <graphic xlink:href="12903_2023_3112_Fig2_HTML" id="MO2"/>
        </fig>
      </p>
      <p id="Par25">
        <fig id="Fig3">
          <label>Fig. 3</label>
          <caption>
            <p>Screenshots of the web application interface for the prediction of new samples</p>
          </caption>
          <graphic xlink:href="12903_2023_3112_Fig3_HTML" id="MO3"/>
        </fig>
      </p>
    </sec>
    <sec id="Sec6">
      <title>Machine learning workflow</title>
      <p id="Par26">In this section we describe the ML procedures used by the tool. Most of them are implemented as functions that are combined to yield the desired result. For instance, Feature Selection uses data selection and preprocessing and then calls repeatedly classifier evaluation with different combinations of features. Details can be checked in the source code in our repository.<list list-type="bullet"><list-item><p id="Par27"><italic>Data format</italic>. Data are compiled into a CSV file, with an additional metadata file with custom format, providing a description of the type of variable of each column and its range of values and optionally a legend to facilitate showing content in a human-friendly format. Variable types may be continuous variables, bounded and unbounded discrete variables, categorical variables encoded as integers with a given valid range and <italic>tooth-lists</italic>. The latter maps a set of teeth to their number according to the ISO 3950:2016 standard [<xref ref-type="bibr" rid="CR19">19</xref>], which designates teeth or areas of the oral cavity using two digits.</p></list-item><list-item><p id="Par28"><italic>Data selection and preprocessing</italic>. ORIENTATE allows the user to select the target and the predictor features from the set of features present in the dataset. Once they are selected, the tool analyzes the number of classes present in the target feature. For binary classification, the user is prompted to mark the relevant class, which is used later to select the best model as the one with the best selected performance metric for that class. The performance metric used can also be chosen, among precision, recall and f1-score [<xref ref-type="bibr" rid="CR18">18</xref>]. For multiclass classification, the best model is that with the best average performance metric over all the classes. The presence of missing data in the target feature is analyzed. In case there is missing data, the user must select an imputation method for it. Since the dataset used comprised only categorical data, the options are to remove or fill with some constant value the missing data. Additional imputation methods are available (use average or more frequent value). Afterward, the predictor features are prepared for the classifiers by normalizing continuous variables and one-hot encoding tooth-lists and optionally categorical variables. A transformation pipeline with the imputation and encoding is generated and the dataset is passed for the evaluation of the classifiers.</p></list-item><list-item><p id="Par29"><italic>Classifier evaluation</italic>. The dataset is split into a train set with 80% of the samples and a validation set with the remaining ones. Then, a default set of classifiers is evaluated with 3-fold cross-validation on the training set and a series of performance scores is generated for each classifier. The default set of classifiers includes single instance classifiers: logistic regression, support vector machines with linear and Gaussian RBF kernels, linear classifier with stochastic gradient descent learning, different <italic>naive</italic> Bayes classifiers, Gaussian Process classifier, multilayer perceptron; and ensemble methods: random forest, gradient boosting (XGBoost), AdaBoost and <italic>stacking</italic> with random forest, Gaussian and logistic regression. All the classifiers are initialized with a set of previously selected default parameters, not configurable at the moment by the user. Automated hyperparameter optimization might be added as an additional stage for the tool, if necessary, but we consider that the potential gains are not worthy for the current functionality and given the increased complexity of the software. Once the cross-validation is done for each of the classifiers, the tool selects as best classifier the one with the best performance metric as described before. The best classifier is trained with the complete training set and finally the validation set is predicted with the fitted model, resulting in a final set of performance scores. The results from this process are a set of metrics for each of the classifiers evaluated as well as for the fitted best model: confusion matrix, recall, precision, f1-score and ROC AUC. All of them are gathered and shown to the user by the tool. The best model is saved to be used for the prediction of new input samples. For the validation set, the prediction probabilities for all the samples in the set are also stored and shown. Since for most of the classifiers these are not precise estimates of the class probabilities, they need to be calibrated before being shown to the user.</p></list-item><list-item><p id="Par30"><italic>Feature selection</italic>. When the user completes the data selection and preprocessing stage described before, a set of potential features is ready. At this point two different algorithms are used, depending on the number of features in the set (<italic>N</italic>). The total number of combinations to test is <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c=2^N - N -1$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:msup><mml:mo>-</mml:mo><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq1.gif"/></alternatives></inline-formula>, since we include at least two features, and when this number is too high, it is not computationally feasible to test them all. So we have arbitrarily set <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=13$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>13</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq2.gif"/></alternatives></inline-formula> as the maximum number of features for fully testing all the combinations. Obviously, this parameter should be set according to the available processing power. For instance, in our server (i9-9280X CPU, 2 NVIDIA RTX 2080Ti GPUS, 32 GB RAM), testing <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=11$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq3.gif"/></alternatives></inline-formula> features, that is, <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c=2036$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>2036</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq4.gif"/></alternatives></inline-formula> combinations, takes around one hour. Therefore, when the number of features is up to 13, <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N \le 13$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≤</mml:mo><mml:mn>13</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq5.gif"/></alternatives></inline-formula>, a background processing task iteratively executes the classifier evaluation procedure described above for each of the combinations in the dataset, that is, first with the <italic>N</italic> features, then all the <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N-1$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq6.gif"/></alternatives></inline-formula> combinations and so on until testing all combinations of 2 features. The results of each evaluation are saved to the database. Otherwise, when <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N &gt; 13$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mi>N</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>13</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq7.gif"/></alternatives></inline-formula>, a custom feature selection algorithm is executed in the background task. Feature selection is, in general, a hard problem due to the combinatorial explosion, and there are multiple algorithms available and different methods [<xref ref-type="bibr" rid="CR20">20</xref>]. We employ here a so-called wrapper method [<xref ref-type="bibr" rid="CR20">20</xref>], using the predictor performance as the objective function, and implement a simple variant of a Sequential Backward Selection (SBS) algorithm. Though this kind of algorithms typically returns a local optimum, they can produce good results, are computationally feasible, and we avoid the additional complexity of more sophisticated methods, which we do not consider critical for our goals. Our algorithm proceeds as follows: at each iteration there is a set of predictors to be tested, which we call <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbb {T}$$\end{document}</tex-math><mml:math id="M16"><mml:mi mathvariant="double-struck">T</mml:mi></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq8.gif"/></alternatives></inline-formula> with size <italic>T</italic>, and a removal parameter, <italic>k</italic>. The algorithm is initialized with <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbb {T}$$\end{document}</tex-math><mml:math id="M18"><mml:mi mathvariant="double-struck">T</mml:mi></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq9.gif"/></alternatives></inline-formula> including the total number of features, <italic>N</italic>, and evaluates the performance for all the classifiers in the default set as described in the <italic>classifier evaluation</italic> procedure. The results and metrics obtained in the evaluation are saved to the database, including the best classifier found. Next it evaluates all combinations of <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T-k$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq10.gif"/></alternatives></inline-formula> elements from the total set and saves the metrics and best classifier found. Afterward, the predictors in the test set are replaced by the predictors used by the best classifier. Notice that now the number of elements in the test set, <italic>T</italic>, has been reduced by <italic>k</italic> elements. And again all the combinations of <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T-k$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq11.gif"/></alternatives></inline-formula> elements are evaluated as described above. This procedure is repeated until the test set contains fewer than 2 elements. Finally, we select the best classifier found from all the combinations evaluated. We illustrate the algorithm with an example, where we start with <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=20$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq12.gif"/></alternatives></inline-formula> features, so that <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbb {T}$$\end{document}</tex-math><mml:math id="M26"><mml:mi mathvariant="double-struck">T</mml:mi></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq13.gif"/></alternatives></inline-formula> contains the all the 20 features, and use <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k=2$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq14.gif"/></alternatives></inline-formula>. After the first evaluation with all the features, we evaluate all the combinations of 18 elements, taken from the 20 elements. Then we replace the test set with the features used by the best classifier found, that is, the test set contains now 18 elements, and evaluate all the combinations of 16 elements from the 18 features in the test set. Notice how the size of the test set is reduced at each iteration, in this example by <inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k=2$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq15.gif"/></alternatives></inline-formula> elements.</p></list-item><list-item><p id="Par31"><italic>Generation of reports</italic>. Reports are generated at the end of the classifier evaluation (CE) or feature selection (FS) procedures. For the former, the confusion matrices and metrics obtained for all the evaluated models are shown. In addition, the probabilities predicted by the best model for all the elements in the validation set are displayed. For the feature selection procedure, we show a plot of the f1-metric for the best classifier for all the evaluated combinations and the list of the predictors for the best classifier found. If the best found model is able to compute the feature importances, those are shown. Additionally, when the best found model is a DecisionTree, the application displays the generated tree. Afterwards, the SHAP values for the model are computed [<xref ref-type="bibr" rid="CR11">11</xref>]. For this, we use the specialized TreeExplainer [<xref ref-type="bibr" rid="CR21">21</xref>], otherwise, we use an ExactExplainer [<xref ref-type="bibr" rid="CR11">11</xref>] with an independent masker of 1000 samples. Once the values are computed, different plots are shown to the user as described in the <xref rid="Sec5" ref-type="sec">Functionality</xref> section, including waterfall plots for individual explanations.</p></list-item></list></p>
    </sec>
  </sec>
  <sec id="Sec7">
    <title>Results</title>
    <p id="Par32">We have applied our tool to a dataset collected for a previous study [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>]. The goal of that study was to describe the oral health condition in two populations, one with healthy children and another one with SHCN, that is, children who due to physical, medical, developmental or cognitive conditions require special consideration when receiving dental treatment [<xref ref-type="bibr" rid="CR22">22</xref>]; and to compare the treatments performed under deep sedation in both populations. This way we illustrate two of the main goals of our tool: (1) the performance of the feature selection algorithm and prediction power of the trained classifier and (2) its capabilities as an alternative to classical statistical studies. We first briefly describe our previous work, then describe the feature selection results and the interpretation plots provided by ORIENTATE for inferences.</p>
    <p id="Par33">All these results are put in context in the <xref rid="Sec11" ref-type="sec">Discussion</xref> section, where we discuss our results in comparison with the findings of our previous study [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>] and related works.</p>
    <sec id="Sec8">
      <title>Material and methods of the previous study</title>
      <p id="Par34">A cohort study was carried out with children who had been treated for their oral pathology under deep sedation in a private clinic in Cartagena (Murcia, Spain), during the years 2006 to 2018, both included. 274 medical records of patients aged 2 to 18 years were reviewed, including both children with an optimal general health condition which we will call "healthy children", and SHCN children [<xref ref-type="bibr" rid="CR22">22</xref>]. Patients whose medical records did not correctly show demographic data or health status data were excluded. Finally, a total of 230 clinical records (109 (47.4%) healthy and 121 (52.6%) SHCN) were included in the study. Within the SHCN, 19.50% were general developmental disorders, 14.70% were encephalopathies and cerebral palsy, Down syndrome 3.5%, intellectual and/or motor disability 4.7%, and other syndromes 10%. The sample consisted of 142 men (61.74%) and 88 women (38.26%), with a mean age of 7.10 ± 3.40 years. The mean age of the healthy patients was 5.04 ± 2.42 years and that of the group of SHCN children was 8.95 ± 3.09 years. Most of children were 4, 6, 7, 8 and 9 years old.</p>
      <p id="Par35">The information that was collected from the clinical records was: from the first visit, sex, age, systemic health status (healthy or SHCN child), reason for the first sedation, information on the oral health status prior to the intervention (hygiene habits, plaque, tartar, caries lesions, pulp involvement, root remains and absences). On the day of the intervention, the following data were collected: the types of treatments performed (fillings, direct pulp protections, pulpotomies, pulpectomies, endodontics, apex formation, scaling, scaling and root planing, application of fluoride and extractions) and the number of teeth treated. Follow-up data include attendance or not at the check-up where the presence of plaque is assessed, the need for medication due to oral pathology, and improvement at mealtime. In addition, it was recorded the performance of the prevention follow-up behavior in appointments, classifying the patient as "cooperative" or "non-cooperative", according to whether the patient allowed the dentist and/or hygienist to carry out their work; the motivation of parents in the oral care of their children, classifying them as “motivated” or “not motivated”, according to whether they are involved in the care of their children mouth and implement at home the dietary and oral hygiene recommendations that are given to them. Finally, treatments performed subsequently without sedation, year of last check-up and tracking time. The data of the successive sedations included the cause, failed treatments, treatments performed, number of sedations and time from the first to the last sedation.</p>
      <p id="Par36">The collected data described above were incorporated into a dataset and systematically encoded and labeled according to the type of data and data format described in the <xref rid="Sec6" ref-type="sec">Machine learning workflow</xref> section: binary variables are labeled by a descriptive name, continuous or integer variables are prefixed with <italic>Num</italic>, while tooth-lists are prefixed with <italic>List</italic>. For instance, the presence of caries is encoded with a binary variable (0 or 1) and labeled <italic>Caries</italic>, the number of teeth with caries is labeled <italic>NumCaries</italic> and the list of teeth with caries is labeled <italic>ListCaries</italic>. Notice that some information is redundant.</p>
      <p id="Par37">Our previous study is a <italic>traditional</italic> one, where the data were statistically analyzed with <italic>classical</italic> methods. In particular, a descriptive analysis of all the variables was performed. Continuous quantitative variables were compared two by two using a T-test, a T-test with Welch’s correction, or a Mann-Whitney test, depending on the assumptions of normality and homoscedasticity. For discrete variables, the Kruskal-Wallis test was used together with the Dwass-Steel-Critchlow-Fligner test to determine the two-to-two differences. To establish the relationship between discrete qualitative or quantitative variables, contingency tables were made with Pearson <inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\chi ^2$$\end{document}</tex-math><mml:math id="M32"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq16.gif"/></alternatives></inline-formula> or Fisher exact test, depending on whether or not the assumptions were fulfilled. Finally, a test of equality of proportions without continuity correction was used for testing proportions. Additional information on these methods can be found in [<xref ref-type="bibr" rid="CR15">15</xref>].</p>
    </sec>
    <sec id="Sec9">
      <title>Feature selection performance</title>
      <p id="Par38">The dataset compiled from the previously described cohort study contains 102 different features, some of them redundant. From them, 30 potential features of interest were extracted by consensus between 2 odontologists, ruling out those not determining, such as being referred or need to take oral medication, those with insufficient number of data, such as endodontic treatments and apical formation, or those for whom there were not enough variability in the data. The <italic>Feature Selection (FS)</italic> functionality was executed on them, with <italic>SecondSedation</italic> as target variable, a binary variable that indicates whether the patient was subjected to a second sedation. Or in other words, the goal of the test is to find the classifier that is able to better predict the need for a second sedation from all or a subset of the 30 selected predictors (features), using f1-score as peformance metric. Since the number of features <inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=30$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq17.gif"/></alternatives></inline-formula> is greater than 13, our SBS algorithm was executed, taking 1 hour and 40 minutes to evaluate <inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c=2360$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>2360</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12903_2023_3112_Article_IEq18.gif"/></alternatives></inline-formula> combinations of predictors.</p>
      <p id="Par39">Figure <xref rid="Fig4" ref-type="fig">4</xref> shows the f1-score for the best classifiers found versus the number of features in the test set and combinations evaluated. As can be seen, the best classifier is found when the feature set contains 10 features and all the combinations of 8 features were tested. Therefore, the best classifier uses 8 predictors, listed in Table <xref rid="Tab1" ref-type="table">1</xref>. The best model is a DecisionTree classifier, therefore, the generated tree can be displayed and the decision process of the model can be analyzed. Actually, the generated tree has been shown previously as example in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b. The scores achieved by the classifier on the validation set are shown in Table <xref rid="Tab2" ref-type="table">2</xref>. Additionally, the ROC (AUC) achieved is 0.92. It has to be taken into account that the dataset is <italic>imbalanced</italic>, that is, only 24% of the patients underwent a second sedation. Therefore, it is easier to predict the absence of second sedation. Anyway, the model found is able to accurately predict the need for a second sedation, with a f1-score for the class of interest of 0.83 and a balanced accuracy of 0.81. There is an additional imbalance in the dataset, that of the healthy to SHCN patients: the number of second sedations for SHCN patients is much higher than that for healthy children. In Table <xref rid="Tab3" ref-type="table">3</xref> we compare the scores when conditioning on the two groups. As seen, only 6% of the healthy patients undergo a second sedation, and the model has difficulties to correctly predict those sedations, as expected, but for the SHCN patients it is accurate in the predictions.<fig id="Fig4"><label>Fig. 4</label><caption><p>f1 score obtained by the best classifier found versus the number of features in the test set and number of combinations evaluated (Predictors-Combinations). The f1 score shown is for the class of interest, in this case, the occurrence of a second sedation</p></caption><graphic xlink:href="12903_2023_3112_Fig4_HTML" id="MO4"/></fig></p>
      <p id="Par40">
        <table-wrap id="Tab1">
          <label>Table 1</label>
          <caption>
            <p>Features used by the best classifier found by the feature selection algorithm</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left">Feature</th>
                <th align="left">Type</th>
                <th align="left">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">Healthy</td>
                <td align="left">binary</td>
                <td align="left">Systemic health status</td>
              </tr>
              <tr>
                <td align="left">Plaque</td>
                <td align="left">binary</td>
                <td align="left">Presence of plaque before first sedation</td>
              </tr>
              <tr>
                <td align="left">Fillings</td>
                <td align="left">binary</td>
                <td align="left">Fillings done at first sedation</td>
              </tr>
              <tr>
                <td align="left">PreventionTracking</td>
                <td align="left">ternary</td>
                <td align="left">Compliance with prevention follow-ups</td>
              </tr>
              <tr>
                <td align="left">NumPulparInvolvement</td>
                <td align="left">integer</td>
                <td align="left">Number of teeth with pulpar involvement before first sedation</td>
              </tr>
              <tr>
                <td align="left">NumPulparTreatmentsFirstSedation</td>
                <td align="left">integer</td>
                <td align="left">Number of pulpar treatments done at first sedation</td>
              </tr>
              <tr>
                <td align="left">NumExosPathology</td>
                <td align="left">integer</td>
                <td align="left">Number of extractions due to pathology done at first sedation</td>
              </tr>
              <tr>
                <td align="left">ListPulpotomies</td>
                <td align="left">tooth-list</td>
                <td align="left">List of teeth with pulpotomies done at first sedation</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
      <p id="Par41">
        <table-wrap id="Tab2">
          <label>Table 2</label>
          <caption>
            <p>Scores achieved by the best classifier found</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left">Second sedation</th>
                <th align="left">Precision</th>
                <th align="left">Recall</th>
                <th align="left">f1</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">No</td>
                <td align="left">0.87</td>
                <td align="left">0.96</td>
                <td align="left">0.92</td>
              </tr>
              <tr>
                <td align="left">Yes</td>
                <td align="left">0.92</td>
                <td align="left">0.75</td>
                <td align="left">0.83</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
      <p id="Par42">
        <table-wrap id="Tab3">
          <label>Table 3</label>
          <caption>
            <p>Scores achieved by the best classifier found when conditioned on the Healthy variable for both classes [0 (absence), 1 (presence)]</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left">Group</th>
                <th align="left">Size</th>
                <th align="left">Second sedation</th>
                <th align="left">sed/group</th>
                <th align="left">sed/total</th>
                <th align="left">Precision</th>
                <th align="left">Recall</th>
                <th align="left">f1</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">SHCN</td>
                <td align="left">121</td>
                <td align="left">48</td>
                <td align="left">0.39</td>
                <td align="left">0.2</td>
                <td align="left">[0.85, 0.84]</td>
                <td align="left">[0.9, 0.77]</td>
                <td align="left">[0.88, 0.8]</td>
              </tr>
              <tr>
                <td align="left">Healthy</td>
                <td align="left">109</td>
                <td align="left">7</td>
                <td align="left">0.06</td>
                <td align="left">0.03</td>
                <td align="left">[0.96, 1]</td>
                <td align="left">[1, 0.43]</td>
                <td align="left">[0.98, 0.6 ]</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
      <p id="Par43">In summary, our Feature Selection algorithm is able to find a good set of predictors and returns an accurate predictor of the target variable. Let us remark that the size of the dataset is quite small, with only 230 records. Once a potential model has been found, a larger dataset might be compiled and further optimizations and refinements of the model may also be tested before being used in production.</p>
    </sec>
    <sec id="Sec10">
      <title>Analysis of features for inference</title>
      <p id="Par44">In this subsection we analyze the relevance assigned by the model to the different features with the help of SHAP explanations. All the figures shown have been taken from the report generated and displayed by our application. The inferences that can be extracted from this information are briefly commented but a further discussion and comparison with our results in the previous traditional study [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>], as illustrated in [<xref ref-type="bibr" rid="CR14">14</xref>], is deferred to the <xref rid="Sec11" ref-type="sec">Discussion</xref> section.</p>
      <p id="Par45">As general remarks, first let us note that the fact that the algorithm has chosen these particular 8 predictors indicates that they are more relevant than the other potential features, at least for prediction. Second, we use SHAP values for interpretation [<xref ref-type="bibr" rid="CR11">11</xref>]. Briefly, a SHAP value computes the contribution of each feature to an individual prediction. That is, the SHAP value of a feature provides a numerical additive value to the final model output for an individual. However, one has to be careful regarding the quantitative aspects of the interpretation. For example, if the model predicts a probability of 0.8 for an individual to undergo a second sedation, and the <italic>Healthy</italic> feature is assigned a SHAP value of 0.16, to directly assign 0.16 of that total probability to being healthy or not is not meaningful, and may even be misleading [<xref ref-type="bibr" rid="CR23">23</xref>]. Therefore, we will mainly discuss qualitative aspects, unless a quantitative interpretation is clear: SHAP values should be interpreted as the relative influence of a predictor in the final outcome, with the sign indicating the direction of the influence in the outcome, that is, a positive value indicates higher probability of second sedation, while a negative one indicates lower probability. Let us note also that quantitative claims are not common in classical studies either, where typically only acceptance or rejection of a null hypothesis with a measure of confidence is given, reported as “significant differences were found between the two variables”.</p>
      <p id="Par46">In Fig. <xref rid="Fig5" ref-type="fig">5</xref> we plot two views of the features’ relevance, according to their mean SHAP value. On the left, in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a the mean absolute average value for each feature is plotted, grouped by healthy or SHCN patients. With this plot we can see the relative influence of each feature in the model output (the probability that a patient undergoes a second sedation) in absolute value, that is, without assessing whether the influence is positive or negative. While on the right, in Fig. <xref rid="Fig5" ref-type="fig">5</xref>b we show a scatter plot, which does show the direction of the influence as a function of the feature value. The value of the feature at each row is represented as a color scale, with red for high values and blue for low values. The assigned color depends on the type of variable, for instance, for a binary value, 1 is shown in red while 0 is shown in blue. For an integer variable, red represents the maximum, blue the minimum and purple hues values in between. This plot allows us to clearly see the influence of the different features in the final outcome. For instance, we see that being healthy clearly decreases the value of the output, and since that value is a probability, we see that it makes it less likely to undergo a second sedation.<fig id="Fig5"><label>Fig. 5</label><caption><p>Feature relevance and impact on model output</p></caption><graphic xlink:href="12903_2023_3112_Fig5_HTML" id="MO5"/></fig></p>
      <p id="Par47">As seen, the healthy status has the major influence in absolute value, as expected, since SHCN patients require more often a second sedation. Since the results are grouped by this feature in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a, its value is equal for both groups. In Fig. <xref rid="Fig5" ref-type="fig">5</xref>b we can see the direction of the influence with the sign of the feature: healthy children (red points) SHAP values are negative, that is, tend to decrease the probability of a second sedation, while SHCN children (blue points) are positive and so tend to increase the probability of a second sedation. For SHCN patients, next in importance is the compliance with the prevention tracking program (<italic>PreventionTracking</italic> in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a). However, for healthy children, the second more relevant feature is the number of teeth that required pulpar treatment at the first sedation (<italic>NumPulparTreatmentsFirstSedation</italic> in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a). The number of teeth that showed pulpar involvement is the fourth more relevant predictive factor for a second sedation for both groups (Fig. <xref rid="Fig5" ref-type="fig">5</xref>a). Figure <xref rid="Fig5" ref-type="fig">5</xref> also shows that the probability of a second sedation increases with the presence of bacterial plaque and a high number of extractions due to pathology during the first sedation.</p>
      <p id="Par48">Next, we turn to interactions between features. In Fig. <xref rid="Fig6" ref-type="fig">6</xref> we plot the strongest interactions found by the model as dependence plots. These plots show the dependence of the SHAP value (Y axis right) with the value of the feature (X axis), and the interaction is depicted by coloring the points according to the value of another feature which has the strongest interaction with the former (Y axis left). The feature that has the strongest interaction is automatically computed by the application, but the user can also select other feature interactions that she might be interested in. As expected, most of the features have the strongest interaction with the healthy condition.<fig id="Fig6"><label>Fig. 6</label><caption><p>Dependence plot with interactions between variables. Brownish shaded bands show the histogram of the data in the X-axis. Interaction with the healthy status is displayed by showing healthy patients in red</p></caption><graphic xlink:href="12903_2023_3112_Fig6_HTML" id="MO6"/></fig></p>
      <p id="Par49">Figure <xref rid="Fig6" ref-type="fig">6</xref>a shows that, for SHCN patients, a higher number of teeth with pulpar involvement is associated with less likelihood of a second sedation. From Fig. <xref rid="Fig6" ref-type="fig">6</xref>b we see that the majority of the patients have undergone fillings during the first sedation, but it is noticeable that patients, especially SHCN, that did not receive that treatment are more likely to be sedated again. Finally, the model shows in Fig. <xref rid="Fig6" ref-type="fig">6</xref>c that the absence of plaque before the first sedation is a clear indicator that the probability of a subsequent sedation is low.</p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>Discussion</title>
    <p id="Par50">We separate our discussion into two parts: first, we focus on the technical aspects of our tool and compare them with previous similar works, then we discuss the relevant findings of our case study.</p>
    <sec id="Sec12">
      <title>Technical aspects</title>
      <p id="Par51">A general overview of ML methods in healthcare can be found in [<xref ref-type="bibr" rid="CR2">2</xref>]. Their potential advantages are outlined in [<xref ref-type="bibr" rid="CR1">1</xref>], while [<xref ref-type="bibr" rid="CR7">7</xref>] discusses how to develop effective ML models for healthcare and related challenges and problems. As in other healthcare fields, the use of data-driven methods in odontology is growing quickly. Several systematic reviews focused in particular fields are available: Revilla-Leon reviews the application of ML methods in restorative dentistry [<xref ref-type="bibr" rid="CR24">24</xref>] and also for gingivitis and periodontal disease [<xref ref-type="bibr" rid="CR25">25</xref>]. More general reviews can be found in [<xref ref-type="bibr" rid="CR26">26</xref>] and [<xref ref-type="bibr" rid="CR27">27</xref>]. Most of the reviewed works are focused on diagnosis and prediction and in the majority of cases the ML methods have been applied to radiography and medical images. Particular and general systematic reviews stress the potential and accuracy of these methods but also warn that they are still in development.</p>
      <p id="Par52">We do not further discuss image-based methods, the technique most applied, but focus on the works most similar to our own in this paper, that is, those using clinical, behavioral, demographic, and laboratory data as input predictors for a ML model. A prominent example is given by Karhade et al. [<xref ref-type="bibr" rid="CR28">28</xref>] who developed an automated ML model for classification of early childhood caries (ECC). As in our work, they performed a feature selection procedure, introducing different sets of plausible predictors, from a total number of 14 features, to the Google AutoML framework and performing an iteratively search process using the classification accuracy of the model for selecting the best performing model and proceeding with internal validation. But the feature selection procedure is not described in detail and no systematic search for features has been done. On the contrary, our feature selection algorithm tests all the combinations for a comparable size of the total set of predictors, or otherwise systematically searches the best combination for larger feature sets. In addition, ORIENTATE provides different global and individual interpretation plots to assess the relevance the model assigns to each predictor and support further inferences. Qu et al. [<xref ref-type="bibr" rid="CR29">29</xref>] provide a prediction model for early childhood caries risk based on behavioral data. For feature selection, they manually remove the low-variance features and then use all of them to train only three different classifiers. They only provide the relative weights assigned by the model to the features used, without further explanatory reports. Campo et al. [<xref ref-type="bibr" rid="CR30">30</xref>] evaluate different classifiers for the need for a root canal retreatment for a dataset with 205 cases, and propose a case-based reasoning algorithm with Bayesian networks that outperforms other types of classifiers. Unlike our work, they do not carry out a feature selection search but use all the features, and they extract the more relevant variables but do not provide further interpretability analysis or plots. Similarly, Thanathornwong [<xref ref-type="bibr" rid="CR31">31</xref>] develops a Bayesian network decision support system for the prediction of the need of an orthodontic treatment. No feature selection search is done but they provide a graphical interface for the application and the results are compared to the assessments of two experts. Finally, Cui et al. [<xref ref-type="bibr" rid="CR32">32</xref>] also trained several multiclass classifiers for tooth extraction, retention or restorative treatment. The authors do perform a feature selection algorithm after performing a feature extraction phase that recovers features and values from clinical records, retaining 34 features for the model. The most relevant features for the model are shown but no further explanation, interaction or interpretation plot is provided. Results were compared to expert predictions and showed that the model outperforms the experts.</p>
      <p id="Par53">Regarding the predictive capabilities of our models, our results agree with others [<xref ref-type="bibr" rid="CR28">28</xref>], as expected, in that the number of features used as predictors is not directly correlated with the accuracy of predictions, see Fig. <xref rid="Fig4" ref-type="fig">4</xref>, and moreover, it is not obvious which ones are going yield the best-performing classifier. In Karhade et al. [<xref ref-type="bibr" rid="CR28">28</xref>], they found that a parsimonious model including only two predictors (children age and the oral health status reported by parents) achieved the highest accuracy as predictors of ECC. A notable difference with this work is that they do not systematically search for the best combination of predictors, even though the size of their predictor set makes it computationally feasible (14 predictors). We believe the reason is that even though they use a reasonably user-friendly platform (Google AutoML), it still requires a certain technical background to truly automate a feature selection process, a problem that we intend to ameliorate with ORIENTATE. The accuracy of our best model is reasonably good (0.92 AUC, 0.75 recall and 0,92 precision) and higher than theirs (0.74 AUC, 0.67 recall and 0.64 precision), even though the size of their dataset is much larger (6404 samples) than ours (230 samples), which may also be due to their selected predictors. Therefore, we consider that an effective feature selection algorithm is essential for this kind of application and intend to improve ours as a future work.</p>
      <p id="Par54">Supporting feature relevance and global interpretation methods allows us to use our tool for statistical inference that can replace and/or complement classical statistical studies, as discussed by Bzdok et al. [<xref ref-type="bibr" rid="CR14">14</xref>], where the similarities and differences between ML and classical statistical studies are analyzed. To elaborate on this point let us note that our application provides three main services: (1) to obtain a descriptive view of the dataset, (2) to find the best classifier model for a fixed set of features of interest (CE) and (3) to find the best subset of features and associated model from a broader set of features of interest (FS). The latter service is the most interesting from our point of view, because it allows to achieve two goals: first, it covers partially the goals of statistical inference, that is, it <italic>draws inferences</italic> about a population from the sample, which is the subject of what we call <italic>traditional</italic> studies. And second, it finds the combination of features and model that best <italic>predicts</italic> our target variable, which is the goal of ML algorithms typically. Let us clarify that inferences drawn by ML methods are derived by observing the subset of features used by the model <italic>and</italic> the contribution of each feature to the outcome (feature relevance) [<xref ref-type="bibr" rid="CR14">14</xref>]. As an illustrative example, in our use case study our target is to predict the value of the variable <italic>SecondSedation</italic>, that is, the need to perform a second intervention under sedation for the subject, and we see that the best classifier uses, among others, the <italic>Healthy</italic> feature, that is, the absence of pathologies in the subject, and it is given a high relevance by the model. The equivalent procedure in a traditional study is to perform a statistical test against the null hypothesis that <italic>Healthy</italic> is associated with <italic>SecondSedation</italic> [<xref ref-type="bibr" rid="CR14">14</xref>].</p>
      <p id="Par55">The ML approach to inference has some advantages: it can handle a large number of variables (features) and few samples, multiple testing and nonparametric models [<xref ref-type="bibr" rid="CR14">14</xref>]. But it has several particular drawbacks too: ML models tend to use regularization to obtain the simplest model that predicts well, which means that it selects features that may capture efficiently the effects of other variables through correlations but are not meaningful to the user. For instance, in our case test, we find that the pulpar treatments on a particular tooth (75) have relevance for the prediction of the target variable, but which the actual relationship may be is not obvious at all. Additionally, ML models typically have worked as <italic>black boxes</italic>, where the user inputs some data and gets a result, without any clue about the actual process to reach the output [<xref ref-type="bibr" rid="CR33">33</xref>]. Fortunately, a number of methods to explain ML output have been developed [<xref ref-type="bibr" rid="CR33">33</xref>] and, in particular, the SHAP reports used here supply both individual and global explanations in intuitive formats, even though they have their own issues [<xref ref-type="bibr" rid="CR23">23</xref>]. Both ML and classical statistic studies are complementary in several aspects: with FS, the ML approach can highlight relevant features quickly but is not appropriate in this form to examine the associations of particular variables that might be of interest to the practitioner but have been discarded by the model during the feature search. Instead, using CE is useful to test those cases, by including the variable of interest and examining the model results, which can and should be complemented by additional classical statistical tests.</p>
      <p id="Par56">As a summary, from the technical point of view, therefore, the distinguishing aspects of our proposal compared to previous works are: most of them use a custom application tailored for a specific target, while we provide a general purpose classifier evaluation application, and our tool supports systematic feature selection search and extensive interpretation functionality with a user interface designed for users with non-technical background. This latter aspect also allows ORIENTATE to be used to draw inferences about the features like classical statistical studies do. A limitation of our study is the small sample size of the dataset that we have used to test the application. Most of ML algorithms improve their performance with larger datasets. But, an advantage of our tool is that it is agnostic with respect to the dataset as we said, so it can be applied seamlessly to any other dataset in the proper format. As future steps we intend to collect additional datasets for evaluation. We should finally note that the interpretation of the features relevance may also be considered another limitation of our tool: the major risk of using predictive methods for inference is that incorrect interpretation of the model results in unwarranted causal inferences [<xref ref-type="bibr" rid="CR34">34</xref>]. This kind of limitation is also acknowledged for example in [<xref ref-type="bibr" rid="CR29">29</xref>], where they warn that some of the predictors were not considered causative. But this is a problem shared with classical statistical methods [<xref ref-type="bibr" rid="CR34">34</xref>]. To tackle this problem, we plan to extend the functionality of ORIENTATE to apply causal inference methods to our datasets [<xref ref-type="bibr" rid="CR34">34</xref>]: there are programming libraries [<xref ref-type="bibr" rid="CR35">35</xref>] that can be seamlessly integrated with ORIENTATE.</p>
    </sec>
    <sec id="Sec13">
      <title>Case study</title>
      <p id="Par57">According to the relevance assigned to each feature by our model, the predictive factors were, in order, for healthy children: systemic health status, number of teeth with pulpar treatment at the first sedation, prevention tracking, number of teeth with pulpar involvement before the first sedation, presence of bacterial plaque, number of teeth extracted due to pathology and the number of fillings (Fig. <xref rid="Fig5" ref-type="fig">5</xref>a). For SHCN children, they were: systemic health status, prevention tracking, number of teeth with pulpar treatment at the first sedation, number of teeth with pulpar involvement before the first sedation, presence of bacterial plaque, number of teeth extracted due to pathology and the number of fillings (Fig. <xref rid="Fig5" ref-type="fig">5</xref>a).</p>
      <p id="Par58">According to our model, thus, healthy children would not need a second sedation, unlike SHCN children (Fig. <xref rid="Fig5" ref-type="fig">5</xref>b). This difference between both groups of children has also been observed in other published studies [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR37">37</xref>]. For SHCN, complying with the prevention tracking program after the first sedation is a predictive factor for a second sedation. One of the goals of the prevention tracking program is to achieve a collaborative attitude in the patient, which would prevent future treatments under sedation. Thus, several authors believe that the lack of preventive programs or a bad design of them may be the cause of the reinterventions [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR38">38</xref>–<xref ref-type="bibr" rid="CR41">41</xref>]. However, few works have evaluated the effect of those programs in the long term [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR42">42</xref>, <xref ref-type="bibr" rid="CR43">43</xref>]. In our previous study [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>] we showed that 80% of healthy children that complied with the prevention tracking program after a first sedation were able to receive at the dental chair treatments of relative complexity. On the contrary, due to their physical or mental limitations, only 18.4% of SHCN children were able to receive this kind of treatment at the dental chair. Despite it may seem contradictory, the fact that children show up at the prevention follows-up is a predictive factor for a second sedation. The reason is, as we said, that the prevention tracking program for this children, in addition to implement preventive measures, allows to carry out an early diagnosis of the therapeutic needs that will have to be implemented under sedation for them. A limitation here is that we have patients for whom the compliance of the prevention program has not been recorded. Let us recall that compliance with the prevention tracking program is a ternary variable. That is, some patients were referred from other clinics and returned after the first sedation, so their compliance with the prevention tracking program was not recorded. For those patients, the <italic>PreventionTracking</italic> feature takes value 2 and is shown in red in Fig. <xref rid="Fig5" ref-type="fig">5</xref>b, for children that did have complied with the prevention program the value is 1 and is shown in purple, while children that did not follow the program, with value 0, are shown in blue. We see that not-recorded values (red) decrease the likelihood of a second sedation, since those patients do not come back to the clinic in many cases and we do not have records about the evolution of their oral pathology.</p>
      <p id="Par59">For healthy children, the second predictive factor in importance according to ORIENTATE was the number of teeth with pulpar treatment at the first sedation. However, in our previous study [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>] neither the initial oral pathology nor the treatments done during the first sedation were indicators that could predict the need for a second sedation. But other works [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR43">43</xref>] did report that children that received more conservative restoration treatment during the first intervention tend to need more retreatments under deep sedation.</p>
      <p id="Par60">The fourth predictive factor for both groups of children was the number of teeth with pulpar involvement before the first sedation. In fact, when the number of such teeth increases, the probability a second sedation decreases (Fig. <xref rid="Fig5" ref-type="fig">5</xref>b). In SHCN patients, a higher number of teeth with pulpar involvement is associated with less likelihood of a second sedation, and similarly for healthy children but in a less pronounced way (Fig. <xref rid="Fig6" ref-type="fig">6</xref>a). As we already found in our previous study [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>], SHCN children are usually older so most of those teeth with pulpar involvement were about to exfoliate. In those cases, the selected treatment was extraction at the sedation. This way, since those children have more extractions and, since due to having fewer teeth, the likelihood of dental pathology is lower, ORIENTATE decreases also the probability of a second sedation. This particular result allows us to remark that, to make inferences, we have to complement the model results and explanations with the rest of the data available, in this case, the age of the patients, a feature that was not selected by the FS algorithm.</p>
      <p id="Par61">The model also shows that SHCN children that have not had fillings done at the first sedation are more likely to be sedated again (Fig. <xref rid="Fig6" ref-type="fig">6</xref>b) and the absence of plaque before the first sedation is a clear indicator that the probability of a subsequent sedation is low and vice versa (Fig. <xref rid="Fig6" ref-type="fig">6</xref>c). This seeming contradiction is due to the fact that many SHCN children without caries lesions are sedated only to remove the large amount of tartar that they produce, due to their diet and poor hygiene, since they are not able to collaborate with the dentist for its removal. So, in our previous study [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>], we found that 3 out of a total of 7 healthy children (42,85%) needed a second sedation to undergo a tartar removal procedure, whereas 42 out of a total of 48 of SHCN children (87,5%) needed it.</p>
      <p id="Par62">In summary, we have shown that the predictive factors for a second sedation found by ORIENTATE are coherent and complement the ones previously found in our classical study. For SHCN children, the systemic health status was the more relevant, followed by prevention tracking, number of teeth with pulpar treatment at the first sedation, number of teeth with pulpar involvement before the first sedation, presence of bacterial plaque, number of teeth extracted due to pathology and the number of fillings. In addition, it is capable of detecting predictive factors, which would escape the usual analysis of clinical practice, and even may seem contradictory but, upon careful reflection, turn out to be consistent as we have discussed.</p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Conclusion</title>
    <p id="Par63">ORIENTATE allows the automated application of machine learning classification algorithms on general datasets by clinical practitioners lacking technical skills. It can find the best subset of predictors for a given target variable and shows several graphs that facilitate the explanation of the classification model results, using global interpretation methods, and an interface for the prediction of new input samples. For the case study, our tool was able to achieve a high prediction accuracy for a second sedation in SHCN children, despite the dataset being heavily imbalanced and its small size. The analysis of the relevance of the features showed that, for healthy children, the number of teeth with pulpar treatments at the first sedation is a predictive factor for a second sedation, whereas for SHCN children a predictive factor was the compliance with the prevention tracking program. Our analysis is complementary to others done with classical statistical methods. In summary, ORIENTATE achieves several goals: first, it automatically finds suitable features and generates accurate classifiers for predictive tasks. Second, it helps researchers without specific skills in data methods in both the application of machine learning classification and as a complement to classical studies for inferential analysis of features.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>AUC</term>
        <def>
          <p id="Par4">Area Under the Curve</p>
        </def>
      </def-item>
      <def-item>
        <term>CE</term>
        <def>
          <p id="Par5">Classifier Evaluation</p>
        </def>
      </def-item>
      <def-item>
        <term>FS</term>
        <def>
          <p id="Par6">Feature Selection</p>
        </def>
      </def-item>
      <def-item>
        <term>ML</term>
        <def>
          <p id="Par7">Machine Learning</p>
        </def>
      </def-item>
      <def-item>
        <term>ORIENTATE</term>
        <def>
          <p id="Par8">applicatiOn of machine leaRning for classIfication of dENTal pATiEnts</p>
        </def>
      </def-item>
      <def-item>
        <term>ROC</term>
        <def>
          <p id="Par9">Receiver Operating Characteristic</p>
        </def>
      </def-item>
      <def-item>
        <term>SHCN</term>
        <def>
          <p id="Par10">Special Health Care Needs</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>IGR: Conceptualization , Investigation, Data Curation, Writing - Original Draft, Writing - Review and Editing, Methodology, Resources. EEL: Conceptualization, Software, Validation, Formal Analysis, Writing - Original Draft, Writing - Review and Editing, Visualization, Funding acquisition. AJOR: Conceptualization, Writing - Review and Edit, Methodology, Supervision.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was partially funded by grant PID2020-112675RB-C41 (ONOFRE-3) funded by MCIN/AEI/10.13039/501100011033.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p> The source code of ORIENTATE as well as the datasets generated and/or analysed during the current study are available via the following link: <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/esteban.egea/orientate">https://gitlab.com/esteban.egea/orientate</ext-link>.</p>
    <p> Project name: ORIENTATE</p>
    <p> Project home page: <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/esteban.egea/orientate">https://gitlab.com/esteban.egea/orientate</ext-link></p>
    <p> Operating system(s): Platform independent</p>
    <p> Programming language: Python</p>
    <p> Other requirements: python v3.9.12, flask v2.0.3, scikit-learn v1.1.1, shap v0.41.0, flask-cache v0.13.0.1, bokeh v2.4.3, xgboost v1.6.1, graphviz v0.20.</p>
    <p> License: MIT</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par64">The study described in the case study was conducted in accordance with the Declaration of Helsinki and approved by the Research Ethics Committee of the University of Murcia (ID:2034/2018). Informed consent was obtained from all subjects and/or their legal guardian(s).</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par65">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par66">The authors declare no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Norgeot</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Glicksberg</surname>
            <given-names>BS</given-names>
          </name>
          <name>
            <surname>Butte</surname>
            <given-names>AJ</given-names>
          </name>
        </person-group>
        <article-title>A call for deep-learning healthcare</article-title>
        <source>Nat Med.</source>
        <year>2019</year>
        <volume>25</volume>
        <fpage>14</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-018-0320-3</pub-id>
        <?supplied-pmid 30617337?>
        <pub-id pub-id-type="pmid">30617337</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Alison Callahan</surname>
            <given-names>NHS</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Aziz Sheikh</surname>
            <given-names>AW</given-names>
          </name>
          <name>
            <surname>Cresswell</surname>
            <given-names>KM</given-names>
          </name>
        </person-group>
        <article-title>Machine learning in healthcare</article-title>
        <source>Key Advances in Clinical Informatics</source>
        <year>2017</year>
        <edition>1</edition>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Academic Press</publisher-name>
        <fpage>279</fpage>
        <lpage>291</lpage>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qayyum</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Qadir</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bilal</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Al-Fuqaha</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Secure and robust machine learning for healthcare: A survey</article-title>
        <source>IEEE Rev Biomed Eng.</source>
        <year>2021</year>
        <volume>14</volume>
        <fpage>156</fpage>
        <lpage>180</lpage>
        <pub-id pub-id-type="doi">10.1109/RBME.2020.3013489</pub-id>
        <?supplied-pmid 32746371?>
        <pub-id pub-id-type="pmid">32746371</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ngiam</surname>
            <given-names>KY</given-names>
          </name>
          <name>
            <surname>Khor</surname>
            <given-names>IW</given-names>
          </name>
        </person-group>
        <article-title>Big data and machine learning algorithms for health-care delivery</article-title>
        <source>Lancet Oncol.</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>5</issue>
        <fpage>262</fpage>
        <lpage>273</lpage>
        <pub-id pub-id-type="doi">10.1016/S1470-2045(19)30149-4</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Lundberg SM, Nair B, Vavilala MS, Horibe M, Eisses MJ, Adams T, Liston DE, Low DKW, Newman SF, Kim J, Lee SI. Explainable machine-learning predictions for the prevention of hypoxaemia during surgery. Nat Biomed Eng. 2018;2. 10.1038/s41551-018-0304-0.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Qazi N, Pawar M, Padhly PP, Pawar V, D’Amico C, Nicita F, Fiorillo L, Alushi A, Minervini G, Meto A. Teledentistry: Evaluation of instagram posts related to bruxism. Technol Health Care. 2023. 10.3233/thc-220910.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>P-HC</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>How to develop machine learning models for healthcare</article-title>
        <source>Nat Mater</source>
        <year>2019</year>
        <volume>18</volume>
        <fpage>410</fpage>
        <lpage>414</lpage>
        <pub-id pub-id-type="doi">10.1038/s41563-019-0345-0</pub-id>
        <?supplied-pmid 31000806?>
        <pub-id pub-id-type="pmid">31000806</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Alaa AM, Bolton T, Angelantonio ED, Rudd JHF, van der Schaar M. Cardiovascular disease risk prediction using automated machine learning: A prospective study of 423,604 uk biobank participants. PLoS ONE. 2019;14. 10.1371/journal.pone.0213653.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Michel</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Grisel</surname>
            <given-names>O</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>J Mach Learn Res.</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Waring</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lindvall</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Umeton</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Automated machine learning: Review of the state-of-the-art and opportunities for healthcare</article-title>
        <source>Artificial Intelligence in Medicine.</source>
        <year>2020</year>
        <volume>104</volume>
        <fpage>101822</fpage>
        <pub-id pub-id-type="doi">10.1016/j.artmed.2020.101822</pub-id>
        <?supplied-pmid 32499001?>
        <pub-id pub-id-type="pmid">32499001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Lundberg SM, Lee S-I. A unified approach to interpreting model predictions. In: Guyon I, Luxburg UV, Bengio S, Wallach H, Fergus R, Vishwanathan S, Garnett R, editors. Advances in Neural Information Processing Systems, vol. 30. Curran Associates, Inc; 2017. p. 4765–74.</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Featherstone</surname>
            <given-names>JDB</given-names>
          </name>
          <name>
            <surname>Chaffee</surname>
            <given-names>BW</given-names>
          </name>
        </person-group>
        <article-title>The evidence for caries management by risk assessment (cambra®)</article-title>
        <source>Adv Dent Res.</source>
        <year>2018</year>
        <volume>29</volume>
        <issue>1</issue>
        <fpage>9</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="doi">10.1177/0022034517736500</pub-id>
        <?supplied-pmid 29355423?>
        <pub-id pub-id-type="pmid">29355423</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Ismail A, Pitts N, Tellez M, Banerjee A, Deery C, Douglas G, et al. The international caries classification and management system (iccms™) an example of a caries management pathway. BMC Oral Health. 2015;15((Suppl 1):S9). 10.1186/1472-6831-15-S1-S9.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bzdok</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Altman</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Krzywinski</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Statistics versus machine learning</article-title>
        <source>Nat Methods.</source>
        <year>2018</year>
        <volume>15</volume>
        <fpage>233</fpage>
        <lpage>234</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.4642</pub-id>
        <?supplied-pmid 30100822?>
        <pub-id pub-id-type="pmid">30100822</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gómez-Ríos</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Pérez-Silva</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Serna-Muñoz</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ibáñez-López</surname>
            <given-names>FJ</given-names>
          </name>
          <name>
            <surname>Periago-Bayonas</surname>
            <given-names>PM</given-names>
          </name>
          <name>
            <surname>Ortiz-Ruiz</surname>
            <given-names>AJ</given-names>
          </name>
        </person-group>
        <article-title>Deep sedation for dental care management in healthy and special health care needs children: A retrospective study</article-title>
        <source>Int J Environ Res Public Health.</source>
        <year>2023</year>
        <volume>20</volume>
        <fpage>3435</fpage>
        <pub-id pub-id-type="doi">10.3390/ijerph20043435</pub-id>
        <?supplied-pmid 36834126?>
        <pub-id pub-id-type="pmid">36834126</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Gomez-Rios I. Tratamiento odontológico bajo sedación profunda en una población infantil: estudio de cohortes retrospectivo. PhD thesis, Universidad de Murcia. 2022. <ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/10201/119116">http://hdl.handle.net/10201/119116</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Liu C-H, Lin C-J, Hu, Y-H, You Z-H. Predicting the Failure of Dental Implants Using Supervised Learning Techniques. 10.5281/zenodo.1227714.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tharwat</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Classification assessment methods. Appl Comput</article-title>
        <source>Inform.</source>
        <year>2021</year>
        <volume>17</volume>
        <fpage>168</fpage>
        <lpage>192</lpage>
        <pub-id pub-id-type="doi">10.1016/j.aci.2018.08.003</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">ISO 3950:2016(E): Dentistry – designation system for teeth and areas of the oral cavity. Standard, International Organization for Standardization, Geneva, CH. 2016.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chandrashekar</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Sahin</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A survey on feature selection methods</article-title>
        <source>Comput Electr Eng.</source>
        <year>2014</year>
        <volume>40</volume>
        <fpage>16</fpage>
        <lpage>28</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compeleceng.2013.11.024</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lundberg</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Erion</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>DeGrave</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Prutkin</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Nair</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Katz</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Himmelfarb</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bansal</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>S-I</given-names>
          </name>
        </person-group>
        <article-title>From local explanations to global understanding with explainable ai for trees</article-title>
        <source>Nat Mach Intell.</source>
        <year>2020</year>
        <volume>2</volume>
        <fpage>56</fpage>
        <lpage>67</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-019-0138-9</pub-id>
        <?supplied-pmid 32607472?>
        <pub-id pub-id-type="pmid">32607472</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">American Academy of Pediatric Dentistry. Definition of Special Health Care Needs. The Reference Manual of Pediatric Dentistry. Chicago: American Academy of Pediatric Dentistry; 2021.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Kumar IE, Venkatasubramanian S, Scheidegger C, Friedler S. Problems with shapley-value-based explanations as feature importance measures. In: III, H.D, Singh A, editors. Proceedings of the 37th International Conference on Machine Learning. Proceedings of Machine Learning Research. vol. 119. PMLR; 2020. pp. 5491–5500. <ext-link ext-link-type="uri" xlink:href="https://proceedings.mlr.press/v119/kumar20e.html">https://proceedings.mlr.press/v119/kumar20e.html</ext-link></mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Revilla-León</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gómez-Polo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Vyas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Barmak</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Özcan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Att</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Krishnamurthy</surname>
            <given-names>VR</given-names>
          </name>
        </person-group>
        <article-title>Artificial intelligence applications in restorative dentistry: A systematic review</article-title>
        <source>J Prosthet Dent.</source>
        <year>2022</year>
        <volume>128</volume>
        <fpage>867</fpage>
        <lpage>875</lpage>
        <pub-id pub-id-type="doi">10.1016/j.prosdent.2021.02.010</pub-id>
        <?supplied-pmid 33840515?>
        <pub-id pub-id-type="pmid">33840515</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Revilla-León M, Gómez-Polo M, Barmak AB, Inam W, Kan JYK, Kois JC, Akal O. Artificial intelligence models for diagnosing gingivitis and periodontal disease: A systematic review. J Prosthet Dent. 2022. 10.1016/j.prosdent.2022.01.026.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shan</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tay</surname>
            <given-names>FR</given-names>
          </name>
          <name>
            <surname>Gu</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Application of artificial intelligence in dentistry</article-title>
        <source>J Dent Res.</source>
        <year>2021</year>
        <volume>100</volume>
        <fpage>232</fpage>
        <lpage>244</lpage>
        <pub-id pub-id-type="doi">10.1177/0022034520969115</pub-id>
        <?supplied-pmid 33118431?>
        <pub-id pub-id-type="pmid">33118431</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Khanagar</surname>
            <given-names>SB</given-names>
          </name>
          <name>
            <surname>Al-ehaideb</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Maganur</surname>
            <given-names>PC</given-names>
          </name>
          <name>
            <surname>Vishwanathaiah</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Patil</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Baeshen</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Sarode</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Bhandi</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Developments, application, and performance of artificial intelligence in dentistry – a systematic review</article-title>
        <source>J Dent Sci.</source>
        <year>2021</year>
        <volume>16</volume>
        <fpage>508</fpage>
        <lpage>522</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jds.2020.06.019</pub-id>
        <?supplied-pmid 33384840?>
        <pub-id pub-id-type="pmid">33384840</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Karhade DS, Roach J, Shrestha P, Simancas-Pallares MA, Ginnis J, Burk ZJS, et al. An automated machine learning classifier for early childhood caries. Pediatr Dent. 2021;43:191–97.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Houser</surname>
            <given-names>SH</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Prediction model for early childhood caries risk based on behavioral determinants using a machine learning algorithm</article-title>
        <source>Comput Methods Prog Biomed.</source>
        <year>2022</year>
        <volume>227</volume>
        <fpage>107221</fpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2022.107221</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Campo L, Aliaga IJ, Paz JFD, García AE, Bajo J, Villarubia G, Corchado JM. Retreatment predictions in odontology by means of cbr systems. Comput Intell Neurosci. 2016;2016:1–11. 10.1155/2016/7485250.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thanathornwong</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Bayesian-based decision support system for assessing the needs for orthodontic treatment</article-title>
        <source>Healthcare Inform Res.</source>
        <year>2018</year>
        <volume>24</volume>
        <fpage>22</fpage>
        <pub-id pub-id-type="doi">10.4258/hir.2018.24.1.22</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cui</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Wen</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Clinical decision support model for tooth extraction therapy derived from electronic dental records</article-title>
        <source>J Prosthet Dent.</source>
        <year>2021</year>
        <volume>126</volume>
        <fpage>83</fpage>
        <lpage>90</lpage>
        <pub-id pub-id-type="doi">10.1016/j.prosdent.2020.04.010</pub-id>
        <?supplied-pmid 32703604?>
        <pub-id pub-id-type="pmid">32703604</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tjoa</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>A survey on explainable artificial intelligence (xai): Toward medical xai</article-title>
        <source>IEEE Trans Neural Netw Learn Syst.</source>
        <year>2021</year>
        <volume>32</volume>
        <fpage>4793</fpage>
        <lpage>4813</lpage>
        <pub-id pub-id-type="doi">10.1109/TNNLS.2020.3027314</pub-id>
        <?supplied-pmid 33079674?>
        <pub-id pub-id-type="pmid">33079674</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Pearl J, Causality. Cambridge University Press; 2009. 10.1017/CBO9780511803161.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Sharma A, Kiciman E. DoWhy: A Python Package for Causal Inference. <ext-link ext-link-type="uri" xlink:href="https://github.com/microsoft/dowhy">https://github.com/microsoft/dowhy</ext-link>. Accessed 15 June 2023.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Guidry J, Bagher S, Felemban O, Rich A, Loo C, Reasons of repeat dental treatment under general anaesthesia: A retrospective study. European Journal of Paediatric Dentistry. 2017;18. 10.23804/ejpd.2017.18.04.09.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">König T, Reicherts P, Leha A, Hrasky V, Wiegand A. Retrospective study on risk factors for repeated dental treatment of children under general anaesthesia. Eur J Paediatr Dent. 2020;21. 10.23804/ejpd.2020.21.03.04.</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Al-Ogayyel S, Ali SA-H. Comparison of dental treatment performed under general anesthesia between healthy children and children with special health care needs in a hospital setting, Saudi Arabia. J Clin Exp Dent. 2018;10:0–0. 10.4317/jced.55060.</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Savanheimo</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Vehkalahti</surname>
            <given-names>MM</given-names>
          </name>
        </person-group>
        <article-title>Preventive aspects in children’s caries treatments preceding dental care under general anaesthesia</article-title>
        <source>Int J Paediatr Dent.</source>
        <year>2008</year>
        <volume>18</volume>
        <fpage>117</fpage>
        <lpage>123</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1365-263X.2007.00858.x</pub-id>
        <?supplied-pmid 18237294?>
        <pub-id pub-id-type="pmid">18237294</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bader</surname>
            <given-names>RM</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Almuhtaseb</surname>
            <given-names>EY</given-names>
          </name>
        </person-group>
        <article-title>A retrospective study of paediatric dental patients treated under general anesthesia</article-title>
        <source>Int J Clin Med.</source>
        <year>2013</year>
        <volume>04</volume>
        <fpage>18</fpage>
        <lpage>23</lpage>
        <pub-id pub-id-type="doi">10.4236/ijcm.2013.47A2005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mallineni</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Yiu</surname>
            <given-names>CKY</given-names>
          </name>
        </person-group>
        <article-title>A retrospective review of outcomes of dental treatment performed for special needs patients under general anaesthesia: 2-year follow-up</article-title>
        <source>Sci World J.</source>
        <year>2014</year>
        <volume>2014</volume>
        <fpage>1</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1155/2014/748353</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Foster</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Perinpanayagam</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Pfaffenbach</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Certo</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Recurrence of early childhood caries after comprehensive treatment with general anesthesia and follow-up</article-title>
        <source>J Dent Child.</source>
        <year>2006</year>
        <volume>73</volume>
        <fpage>25</fpage>
        <lpage>30</lpage>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Almeida AG, Roseman MM, Sheff M, Huntington N, Hughes CV. Future caries susceptibility in children with early childhood caries following treatment under general anesthesia. Pediatr Dent. 2000;22.</mixed-citation>
    </ref>
  </ref-list>
</back>
