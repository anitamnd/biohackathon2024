<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Artif Intell</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Artif Intell</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Artif. Intell.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Artificial Intelligence</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2624-8212</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8969434</article-id>
    <article-id pub-id-type="doi">10.3389/frai.2022.856232</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Artificial Intelligence</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepSpectrumLite: A Power-Efficient Transfer Learning Framework for Embedded Speech and Audio Processing From Decentralized Data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Amiriparian</surname>
          <given-names>Shahin</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/529130/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hübner</surname>
          <given-names>Tobias</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1670644/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Karas</surname>
          <given-names>Vincent</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gerczuk</surname>
          <given-names>Maurice</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/792996/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ottl</surname>
          <given-names>Sandra</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schuller</surname>
          <given-names>Björn W.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/419411/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg</institution>, <addr-line>Augsburg</addr-line>, <country>Germany</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Group on Language, Audio, and Music (GLAM), Imperial College London</institution>, <addr-line>London</addr-line>, <country>United Kingdom</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Odette Scharenborg, Delft University of Technology, Netherlands</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Prayag Tiwari, Aalto University, Finland; Batyrkhan Omarov, Al-Farabi Kazakh National University, Kazakhstan</p>
      </fn>
      <corresp id="c001">*Correspondence: Shahin Amiriparian <email>shahin.amiriparian@informatik.uni-augsburg.de</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Language and Computation, a section of the journal Frontiers in Artificial Intelligence</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>17</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>5</volume>
    <elocation-id>856232</elocation-id>
    <history>
      <date date-type="received">
        <day>16</day>
        <month>1</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>2</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Amiriparian, Hübner, Karas, Gerczuk, Ottl and Schuller.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Amiriparian, Hübner, Karas, Gerczuk, Ottl and Schuller</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Deep neural speech and audio processing systems have a large number of trainable parameters, a relatively complex architecture, and require a vast amount of training data and computational power. These constraints make it more challenging to integrate such systems into embedded devices and utilize them for real-time, real-world applications. We tackle these limitations by introducing <sc>DeepSpectrumLite</sc>, an open-source, lightweight transfer learning framework for on-device speech and audio recognition using pre-trained image Convolutional Neural Networks (CNNs). The framework creates and augments Mel spectrogram plots on the fly from raw audio signals which are then used to finetune specific pre-trained CNNs for the target classification task. Subsequently, the whole pipeline can be run in real-time with a mean inference lag of 242.0 ms when a <sc>DenseNet121</sc> model is used on a consumer-grade <italic>Motorola moto e7 plus</italic> smartphone. <sc>DeepSpectrumLite</sc> operates decentralized, eliminating the need for data upload for further processing. We demonstrate the suitability of the proposed transfer learning approach for embedded audio signal processing by obtaining state-of-the-art results on a set of paralinguistic and general audio tasks, including speech and music emotion recognition, social signal processing, COVID-19 cough and COVID-19 speech analysis, and snore sound classification. We provide an extensive command-line interface for users and developers which is comprehensively documented and publicly available at <ext-link xlink:href="https://github.com/DeepSpectrum/DeepSpectrumLite" ext-link-type="uri">https://github.com/DeepSpectrum/DeepSpectrumLite</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>computational paralinguistics</kwd>
      <kwd>audio processing</kwd>
      <kwd>transfer learning</kwd>
      <kwd>embedded devices</kwd>
      <kwd>deep spectrum</kwd>
    </kwd-group>
    <counts>
      <fig-count count="3"/>
      <table-count count="5"/>
      <equation-count count="0"/>
      <ref-count count="56"/>
      <page-count count="10"/>
      <word-count count="8039"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Over the past decade, the number of wearable devices such as fitness trackers, smartphones, and smartwatches has increased remarkably (van Berkel et al., <xref rid="B49" ref-type="bibr">2015</xref>). With a rising amount of sensors, these devices are capable of gathering a vast amount of users' personal information, such as state of health (Ko et al., <xref rid="B26" ref-type="bibr">2010</xref>), speech, or physiological signals including skin conductance, skin temperature, and heart rate (Schuller et al., <xref rid="B41" ref-type="bibr">2013</xref>). In order to automatically process such data and obtain robust data-driven features, deep representation learning approaches (Amiriparian et al., <xref rid="B4" ref-type="bibr">2017b</xref>; Freitag et al., <xref rid="B14" ref-type="bibr">2017</xref>) and end-to-end learning methodologies (Tzirakis et al., <xref rid="B48" ref-type="bibr">2018</xref>) can be applied. These networks, however, have a large number of trainable parameters (correlated with the large model size) and need a high amount of data to achieve a good degree of generalization (Zhao et al., <xref rid="B55" ref-type="bibr">2019</xref>). These factors increase the energy consumption of the trained models (Yang et al., <xref rid="B50" ref-type="bibr">2017</xref>) and confine their real-time capability. Furthermore, whilst personal data in unprecedented volumes is “in transit” or being synchronized with the cloud for further processing, it is susceptible to eavesdropping (Cilliers, <xref rid="B12" ref-type="bibr">2020</xref>), and this issue raises privacy and security concerns for the user (e.g., discriminatory profiling, manipulative marketing; Montgomery et al., <xref rid="B35" ref-type="bibr">2018</xref>). Such restrictions emerged the need for novel neural architectures and collaborative machine learning techniques without centralized training data (Li et al., <xref rid="B30" ref-type="bibr">2020</xref>). Recent advancements include “squeezed” neural architectures (Iandola et al., <xref rid="B23" ref-type="bibr">2016</xref>) and the federated learning paradigms (Li et al., <xref rid="B30" ref-type="bibr">2020</xref>). Iandola et al. (<xref rid="B23" ref-type="bibr">2016</xref>) have introduced <sc>SqueezeNet</sc>, a “pruned” Convolutional Neural Network (CNN) architecture with 50× fewer trainable parameters than <sc>AlexNet</sc> (Krizhevsky et al., <xref rid="B27" ref-type="bibr">2012</xref>) with an <sc>AlexNet</sc>-level accuracy. A more squeezed architecture, <sc>SqueezeNext</sc>, with 112× fewer parameters than <sc>AlexNet</sc> (with similar accuracy) was introduced by Gholami et al. (<xref rid="B16" ref-type="bibr">2018</xref>). Mehta et al. (<xref rid="B34" ref-type="bibr">2019</xref>) have proposed <sc>ESPNetv2</sc>, a lightweight general-purpose CNN with point-wise and depth-wise dilated separable convolutions for representation learning from large receptive fields with fewer parameters. Further energy-efficient CNN architectures have been proposed and applied for traffic sign classification (Zhang et al., <xref rid="B53" ref-type="bibr">2020</xref>) and optical flow estimation (Hui et al., <xref rid="B21" ref-type="bibr">2018</xref>).</p>
    <p>For effective utilization of deep CNNs and to cope with data scarcity in the field of audio signal processing, we have introduced the <sc>Deep Spectrum</sc> system<xref rid="fn0001" ref-type="fn"><sup>1</sup></xref> (Amiriparian et al., <xref rid="B5" ref-type="bibr">2017c</xref>) at INTERSPEECH 2017. In Amiriparian et al. (<xref rid="B5" ref-type="bibr">2017c</xref>), we have forwarded (Mel) spectrogram plots of audio signals with different color mappings through pre-trained CNNs and extracted the activations of the penultimate fully connected layer of these networks as a feature set. For the effect of different color maps on the representations, please refer to Amiriparian et al. (<xref rid="B5" ref-type="bibr">2017c</xref>, <xref rid="B2" ref-type="bibr">2019</xref>, <xref rid="B7" ref-type="bibr">2020</xref>). <sc>Deep Spectrum</sc> features have shown to be effective for a variety of paralinguistic and general audio recognition tasks, including Speech Emotion Recognition (SER) (Ottl et al., <xref rid="B36" ref-type="bibr">2020</xref>), sentiment analysis (Amiriparian et al., <xref rid="B3" ref-type="bibr">2017a</xref>), and acoustic surveillance (Amiriparian et al., <xref rid="B6" ref-type="bibr">2018</xref>). Furthermore, the <sc>Deep Spectrum</sc> system has been proved to be a competitive baseline system for the 2018–2021 editions of the Computational Paralinguistics Challenge (ComParE) (Schuller et al., <xref rid="B40" ref-type="bibr">2019</xref>, <xref rid="B43" ref-type="bibr">2021</xref>). In this article, we propose <sc>DeepSpectrumLite</sc>, an extension of the <sc>Deep Spectrum</sc> framework for embedded speech and audio processing. Whereas, the <sc>Deep Spectrum</sc> framework extracts features from pre-trained CNNs, <sc>DeepSpectrumLite</sc> goes one step ahead. First, it adds a lightweight Multilayer Perceptron (MLP) to the neural network pipeline which is responsible for either classification or regression. Second, the <sc>DeepSpectrumLite</sc> offers support for efficient on-device computation of the audio signal processing including the generation and on-the-fly augmentation of spectrogram image plots. Further, it allows fine-tuning of the image CNNs for each audio recognition task. The proposed system implements a model and inference structure that is focused on mobile usage, thereby computationally expensive audio signals processing can be performed efficiently on embedded devices. We make our <sc>DeepSpectrumLite</sc> framework publicly available for users and developers on GitHub<xref rid="fn0002" ref-type="fn"><sup>2</sup></xref> and PyPI.</p>
    <p>The remainder of this article is organized as follows: in Section 2, we describe the architecture of the proposed system. Subsequently, we introduce the datasets applied for our experiments, outline the experimental settings and results, and analyse the explainability challenges in Section 3. Afterwards, in Section 4, we discuss the performance of the trained models and their inference time on embedded devices. Finally, concluding remarks and our future work plans are given in Section 5.</p>
  </sec>
  <sec id="s2">
    <title>2. Proposed System</title>
    <p>Our framework is composed of two main parts: (i) task-specific, transfer learning-based model training (cf. Section 2.1), and (ii) decentralized audio processing using the trained model (cf. Section 2.2). For the first component of <sc>DeepSpectrumLite</sc>, we make use of pre-trained neural networks (instead of training a new network from scratch) to achieve a better generalization for audio recognition tasks in which the data of the target class is scarce (Shie et al., <xref rid="B45" ref-type="bibr">2015</xref>; Hutchinson et al., <xref rid="B22" ref-type="bibr">2017</xref>; Amiriparian, <xref rid="B1" ref-type="bibr">2019</xref>). In the second component of the framework, the pre-trained (and fine-tuned) model is adapted to be run on embedded devices. In the developed architecture, both components perform in synchrony facilitating low-resource signal processing for a variety of speech and audio tasks.</p>
    <sec>
      <title>2.1. Task-Specific Transfer Learning</title>
      <p>The input of our system consists of raw audio signals with a sample rate of 16 kHz. For simplicity, our system reads only one audio channel. Subsequently, we apply a sliding window function to split the audio signals into smaller fixed-width chunks. For each chunk, we apply a signal normalization and then a Short-Time Fourier Transform (STFT) with Hanning windows of 32 ms and 50.0% hop length. The spectrograms are then transformed into Mel spectrograms with 128 Mel bins. We further compute the power spectral density on the <italic>dB</italic> power scale and apply a <italic>min-max normalization</italic> which is linearly scaled between [0, 255]. Subsequently, each value in the rescaled spectrogram matrix is mapped according to the <italic>viridis</italic> color definition. Since we use image CNNs that were pre-trained on ImageNet (Deng et al., <xref rid="B13" ref-type="bibr">2009</xref>; Huang et al., <xref rid="B20" ref-type="bibr">2017</xref>), we resize the spectrogram image plot to 224 × 224 pixels with bi-linear interpolation and mean normalize the image color channel values according to the original ImageNet dataset. Afterwards, we use the deep CNN model <sc>DenseNet121</sc> (Huang et al., <xref rid="B20" ref-type="bibr">2017</xref>) as a convolutional feature extractor for the generated audio plot images and attach an MLP classifier containing a single hidden layer with Attention-based Rectified Linear Unit (AReLU) (Chen et al., <xref rid="B11" ref-type="bibr">2020</xref>) activation on top of this base. To reduce the effect of overfitting, we further apply the regularization technique dropout (Srivastava et al., <xref rid="B47" ref-type="bibr">2014</xref>).</p>
      <p>We have used <sc>DenseNet121</sc> with weights pre-trained on ImageNet data as the feature extractor for two main reasons. First, we want to directly compare our results for the COVID-19 Cough (CCS), COVID-19 Speech (CSS), Escalation at Service-desks and in Trains (ESS), and Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpora to the official evaluation setting of the ComParE 2021 Challenge (Schuller et al., <xref rid="B43" ref-type="bibr">2021</xref>). Second, Amiriparian et al. (<xref rid="B7" ref-type="bibr">2020</xref>) has shown that an image pre-trained <sc>DenseNet121</sc> is superior to other CNNs (in particular <sc>ResNet50</sc>, <sc>VGG16</sc>, <sc>VGG19</sc>) with pre-trained and random weights.</p>
      <p>The training of our transfer learning models then proceeds in two phases. In the first phase, we freeze the CNN model structure and only train the classifier head. In the second phase, we unfreeze a part of the CNN's layers and continue training with a reduced learning rate. Furthermore, we apply different data augmentation techniques to the spectrogram plots on the fly during training. Data augmentation helps to reduce the effect of overfitting, especially when only a small number of training samples is available (Perez and Wang, <xref rid="B39" ref-type="bibr">2017</xref>; Shorten and Khoshgoftaar, <xref rid="B46" ref-type="bibr">2019</xref>).</p>
      <p><sc>DeepSpectrumLite</sc> has implemented an adapted version of the SapAugment data augmentation policy (Hu et al., <xref rid="B19" ref-type="bibr">2021</xref>). The policy decides for every training sample its portion of applied data augmentation. We apply both CutMix (Yun et al., <xref rid="B51" ref-type="bibr">2019</xref>) and SpecAugment (Park et al., <xref rid="B38" ref-type="bibr">2019</xref>) data augmentations relatively to the loss value of all samples within a batch. The basic idea of SapAugment is that a training sample with a comparably low loss value is easy to understand using the current weights of a neural network, therefore, more data augmentation can be applied. Whereas, when a sample has a comparably high loss, SapAugment argues that less data augmentation should be applied until the sample reaches a low loss value. For further details on how the portion of applied data augmentation relative to the loss value is computed, the interested reader is referred to Yun et al. (<xref rid="B51" ref-type="bibr">2019</xref>).</p>
    </sec>
    <sec>
      <title>2.2. Decentralized Audio Processing</title>
      <p>After centralized training of a task-specific model, its network structure and weights are saved into a Hierarchical Data Format (HDF) version 5. The saved model is then converted to <sc>TensorFlow</sc> (TF) Lite<xref rid="fn0003" ref-type="fn"><sup>3</sup></xref> format for compatibility on embedded devices.</p>
      <p>Since our framework applies all necessary preprocessing steps within the data pipeline structure, there is no device-specific implementation required. A schematic overview of <sc>DeepSpectrumLite</sc> deployed on a target mobile device is depicted in <xref rid="F1" ref-type="fig">Figure 1</xref>. From the input raw audio signals (e.g., signals captured from a microphone) Mel spectrogram plots are created which are then forwarded through a TF Lite version of the model trained as described in Section 2.1. It consists of a (fine-tuned) image CNN, here a <sc>DenseNet121</sc>, and a lightweight MLP head that classifies the deep representations obtained from a specific layer of the CNN.</p>
      <fig position="float" id="F1">
        <label>Figure 1</label>
        <caption>
          <p>A general overview of a <sc>DeepSpectrumLite</sc> model deployed on a target device for inference. Raw audio (from the device's microphone) is first converted to a spectrogram representation and the values are mapped to the red-green-blue (RGB) color space according to a certain color mapping definition. These spectrogram plots are then forwarded through the TFLite version of a trained CNN model, and an MLP classifier head generates predictions for the task at hand.</p>
        </caption>
        <graphic xlink:href="frai-05-856232-g0001" position="float"/>
      </fig>
      <p>The whole <bold>audio processing</bold> steps during inference, including reading the raw audio signals (e.g., from the microphone of an embedded device), extraction of the features, and the classification are conducted in a decentralized way, i.e., removing the need to send the data to a server for processing and evaluation. By doing so, all users' data will remain on their smart devices. Therefore, we have refrained from utilizing methods such as federated learning for <bold>model training</bold>.</p>
    </sec>
  </sec>
  <sec id="s3">
    <title>3. Experiments</title>
    <p>We perform experiments regarding the general learning capabilities of <sc>DeepSpectrumLite</sc> by evaluating its efficacy on eight databases which are described briefly in Section 3.1. Our framework has a set of hyperparameters that are fine-tuned for each audio task (cf. Section 3.2). We further compare the performance of <sc>DeepSpectrumLite</sc> with the original <sc>Deep Spectrum</sc> system which showed state-of-the-art results for various audio recognition tasks (Zhao et al., <xref rid="B54" ref-type="bibr">2018</xref>; Amiriparian et al., <xref rid="B7" ref-type="bibr">2020</xref>). An MLP is used as the classifier in our experiments. Utilized hyperparameters for each experiment are provided in <bold>Table 2</bold>. We then investigate the suitability of the trained <sc>DeepSpectrumLite</sc> models for real-time audio classification on an embedded device (cf. Section 3.3).</p>
    <sec>
      <title>3.1. Datasets and Partitions</title>
      <p>We utilize a diverse set of datasets to cover a range of audio processing tasks from paralinguistics to digital health. For the task of SER and social signal processing, the Database of Elicited Mood in Speech (DEMoS) (Parada-Cabaleiro et al., <xref rid="B37" ref-type="bibr">2020</xref>), IEMOCAP (Busso et al., <xref rid="B10" ref-type="bibr">2008</xref>), and ESS (Schuller et al., <xref rid="B43" ref-type="bibr">2021</xref>) are used. The datasets CCS, CSS, Munich-Passau Snore Sound Corpus (MPSSC), and Düsseldorf Sleepy Language Corpus (SLEEP) are applied for the audio-based digital health tasks. We further analyse the performance of our framework for music emotion recognition by using the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) (Livingstone and Russo, <xref rid="B32" ref-type="bibr">2018</xref>) dataset. All datasets are speaker-independently split into training, validation, and test partitions. This partitioning strategy is maintained for all experiments on all datasets in this manuscript. Detailed statistics about each dataset are provided in <xref rid="T1" ref-type="table">Table 1</xref>.</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>Statistics of the databases utilized in our experiments in terms of number of samples (#), number of speakers (<italic>Sp</italic>.), number of classes (<italic>C</italic>.), total duration (<italic>Dur</italic>.) in minutes, and mean and standard deviation of the duration (<italic>Mean &amp; Std dur</italic>.) in seconds.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Name</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>#</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1"><bold>C</bold>.</th>
              <th valign="top" align="center" rowspan="1" colspan="1"><bold>Sp</bold>.</th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dur. [min.]</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Mean dur. [s]</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Std dur. [s]</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>CCS</bold>: COVID-19 cough</td>
              <td valign="top" align="center" rowspan="1" colspan="1">725</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">397</td>
              <td valign="top" align="center" rowspan="1" colspan="1">97.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6.34</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2.2679</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>CSS</bold>: COVID-19 speech</td>
              <td valign="top" align="center" rowspan="1" colspan="1">893</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">366</td>
              <td valign="top" align="center" rowspan="1" colspan="1">194.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">13.16</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.4784</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>DEMoS</bold>: Elicited mood in speech</td>
              <td valign="top" align="center" rowspan="1" colspan="1">9,365</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">65</td>
              <td valign="top" align="center" rowspan="1" colspan="1">445.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2.86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.2564</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>ESS</bold>: Escalation in speech</td>
              <td valign="top" align="center" rowspan="1" colspan="1">914</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21</td>
              <td valign="top" align="center" rowspan="1" colspan="1">58.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.6418</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>IEMOCAP</bold>: Emotional speech</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5,531</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">10</td>
              <td valign="top" align="center" rowspan="1" colspan="1">746.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.46</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.0645</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>MPSSC</bold>: Snoring sounds</td>
              <td valign="top" align="center" rowspan="1" colspan="1">828</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">219</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.51</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.3464</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>RAVDESS</bold>: Emotional song</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1,012</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">23</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.65</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.4213</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>SLEEP</bold>: Sleepiness prediction from speech</td>
              <td valign="top" align="center" rowspan="1" colspan="1">16,462</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">915</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1059.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.6399</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <sec>
        <title>3.1.1. CCS and CSS</title>
        <p>The CCS and CSS datasets both deal with voice-based COVID-19 detection and were introduced as part of ComParE 2021 (Schuller et al., <xref rid="B43" ref-type="bibr">2021</xref>). The CCS dataset consists of crowd-sourced audio samples of coughing, recorded from 397 subjects resulting in 725 audio clips. The CSS dataset contains 893 audio samples from 366 subjects. A preceding COVID-19 test of the subjects was <italic>positive</italic> for one part and <italic>negative</italic> for the rest. The result of this test should be predicted by the challenge participants based on the audio content. We use the official challenge partitions in our experiments.</p>
      </sec>
      <sec>
        <title>3.1.2. DEMoS</title>
        <p>DEMoS is a corpus of induced emotional speech in Italian (Parada-Cabaleiro et al., <xref rid="B37" ref-type="bibr">2020</xref>), including 9, 365 emotional and 332 neutral speech samples produced by 68 native speakers (23 females, 45 males) in seven emotional states (anger, disgust, fear, guilt, happiness, sadness, and surprise). The emotional speech in the dataset is elicited by combinations of Mood Induction Proceduress (MIPss) in order to obtain more realistic speech production in comparison to acted speech. A detailed description of DEMoS is given in Parada-Cabaleiro et al. (<xref rid="B37" ref-type="bibr">2020</xref>).</p>
      </sec>
      <sec>
        <title>3.1.3. ESS</title>
        <p>The ESS corpus combines the dataset of aggression in trains (Lefter et al., <xref rid="B29" ref-type="bibr">2013</xref>) and the stress at service desk dataset (Lefter et al., <xref rid="B28" ref-type="bibr">2014</xref>). In total, 21 subjects were exposed to different scenarios that were recorded in 914 audio files. The original labels are mapped onto a 3-point scale, <italic>low, medium</italic>, and <italic>high</italic> escalation. The language in the clips is Dutch.</p>
      </sec>
      <sec>
        <title>3.1.4. IEMOCAP</title>
        <p>The IEMOCAP dataset (Busso et al., <xref rid="B10" ref-type="bibr">2008</xref>) is an English emotion dataset containing audio of (scripted and improvised) dialogues between 5 female and 5 male speakers, adding up to 5, 531 utterances. The chosen emotion classes are happiness (fused with excitement), sadness, anger, and neutral. The dataset is split into sessions 1–3 for training, session 4 for validation, and session 5 for testing. The splits' selection for the partitions is motivated by the <sc>EmoNet</sc> paper (Gerczuk et al., <xref rid="B15" ref-type="bibr">2021</xref>) which thoroughly addresses the multi-corpus SER from a deep transfer learning perspective.</p>
      </sec>
      <sec>
        <title>3.1.5. MPSSC</title>
        <p>The MPSSC (Janott et al., <xref rid="B24" ref-type="bibr">2018</xref>) dataset includes audio recordings of 828 snore events from 219 subjects. These events are annotated in terms of the VOTE classification (V, velum; O, oropharyngeal lateral walls; T, tongue base; and E, epiglottis; Kezirian et al., <xref rid="B25" ref-type="bibr">2011</xref>) for the location of snoring noise by experts. We use the official partitioning provided by the authors in the INTERSPEECH ComParE 2017 challenge (Schuller et al., <xref rid="B42" ref-type="bibr">2017</xref>).</p>
      </sec>
      <sec>
        <title>3.1.6. RAVDESS</title>
        <p>RAVDESS (Livingstone and Russo, <xref rid="B32" ref-type="bibr">2018</xref>) is an audio-visual database containing emotional displays of both speech and song. For our experiments, we exclusively use the emotional song portion with 1, 012 samples recorded from 23 actors. The portrayed emotions are angry, calm, fearful, happy, and sad. The corpus has been extensively validated by 247 individuals, producing 10 ratings for every sample.</p>
      </sec>
      <sec>
        <title>3.1.7. SLEEP</title>
        <p>We use a subset of the SLEEP Corpus created at the Institute of Psychophysiology, Düsseldorf that served as a ComParE challenge task for continuous sleepiness prediction in 2019 (Schuller et al., <xref rid="B40" ref-type="bibr">2019</xref>). The set contains 16, 462 recordings, including reading passages, speaking tasks, and elicited spontaneous speech from 915 subjects (364 females, 551 males, between the ages of 12 and 84). The recordings are annotated in terms of the Karolinska Sleepiness Scale (KSS) (Shahid et al., <xref rid="B44" ref-type="bibr">2011</xref>) (range 0 − 9) by averaging self-assessments and post-hoc observer ratings. For our experiments, we perform a binary discretization of our labels into <italic>not sleepy</italic> [0 − 7.5] and <italic>sleepy</italic> (7.5 − 9].</p>
      </sec>
    </sec>
    <sec>
      <title>3.2. Hyperparameters</title>
      <p>We train our models with the AdaDelta optimizer (Zeiler, <xref rid="B52" ref-type="bibr">2012</xref>) on the cross-entropy loss function in batches of 32 samples. After training the classifier head for a certain number of initial epochs only, we reduce the learning rate 10-fold and continue training with some of the layers of the <sc>DenseNet121</sc> unfrozen. As the datasets have different sizes and numbers of classes, we slightly adapt our hyperparameter configuration to each of them. All important details on the hyperparameters for each dataset are provided in <xref rid="T2" ref-type="table">Table 2</xref>. Furthermore, we evaluate four data augmentation configurations: (1) no augmentation, (2) only CutMix, (3) only SpecAugment, and (4) a combination of CutMix and SpecAugment.</p>
      <table-wrap position="float" id="T2">
        <label>Table 2</label>
        <caption>
          <p>This table shows the configuration of the different hyperparameters for each of the datasets used in our experiments.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Hyperparameter</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>CCS</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>CSS</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>DEMoS</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>ESS</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>IEMOCAP</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>MPSSC</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>RAVDESS</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>SLEEP</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Classifier units</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">700</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">128</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Dropout rate</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.25</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.25</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.25</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.25</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.25</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.25</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Initial learning rate</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.001</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.01</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.001</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.001</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.001</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.001</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.001</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.001</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Epochs of first phase</td>
              <td valign="top" align="center" rowspan="1" colspan="1">40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20</td>
              <td valign="top" align="center" rowspan="1" colspan="1">40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">40</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Epochs of second phase</td>
              <td valign="top" align="center" rowspan="1" colspan="1">200</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">120</td>
              <td valign="top" align="center" rowspan="1" colspan="1">200</td>
              <td valign="top" align="center" rowspan="1" colspan="1">200</td>
              <td valign="top" align="center" rowspan="1" colspan="1">120</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Fine-tuned layers</td>
              <td valign="top" align="center" rowspan="1" colspan="1">298</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">128</td>
              <td valign="top" align="center" rowspan="1" colspan="1">298</td>
              <td valign="top" align="center" rowspan="1" colspan="1">128</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85</td>
              <td valign="top" align="center" rowspan="1" colspan="1">42</td>
              <td valign="top" align="center" rowspan="1" colspan="1">128</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Audio chunk length [s]</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.0</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>In our experiments, we use SapAugment with the configuration values <italic>a</italic> = 0.5, <italic>s</italic> = 10. Our CutMix algorithm hyperparameters are set to cut and paste squared patches between sizes of [0<italic>px</italic>, 56<italic>px</italic>] among the training samples. The ground truth labels are proportionally mixed according to the pasted patch size. Moreover, the SpecAugment data augmentation creates a one-time mask and one frequency mask for every training sample. The size of every mask is between [0.0<italic>px</italic>, 67<italic>px</italic>]. The actual patch sizes and mask sizes depend on the samples' loss value (cf. Section 2.1). Because the number of available training samples is limited, we expect the problem of underfitting when applying data augmentation for every single training sample. Therefore, we throttle down the usage of all data augmentations by adding an execution probability between [10.0, 25.0%] dependent on the sample's loss value.</p>
    </sec>
    <sec>
      <title>3.3. Results</title>
      <p>We evaluate the performance on the test partitions using the Unweighted Average Recall (UAR) metric which is equivalent to balanced accuracy and provides more meaningful information when a dataset has an unbalanced class ratio. In <xref rid="T3" ref-type="table">Tables 3</xref>, <xref rid="T4" ref-type="table">4</xref>, all results obtained <italic>via</italic> our framework with and without various augmentation techniques are provided. In <xref rid="T3" ref-type="table">Table 3</xref>, we show that our framework constantly outperforms the <sc>Deep Spectrum</sc> baselines of the ComParE 2021 Challenge by 16.1, 5.8, 9.4, and 6.0% relative improvement on the unseen test set for the CCS, CSS, ESS, and IEMOCAP (not part of ComParE) tasks, respectively. Furthermore, in order to be consistent with the ComParE methodology, for all datasets, we initially find the best set of hyperparameters by validating the trained model on the development set. Afterwards, we train a final model on the combined training and development partitions before evaluation on the test set. By doing so, we provide more data to our Deep Neural Networks (DNNs) and aim for better generalization capabilities.</p>
      <table-wrap position="float" id="T3">
        <label>Table 3</label>
        <caption>
          <p>Results of the transfer learning experiments with <sc>DeepSpectrumLite</sc> (<sc>DS Lite</sc>) on three of the ComParE 2021 Challenge tasks (CCS, CSS, and ESS) and IEMOCAP compared against <sc>Deep Spectrum</sc> feature extraction + Support Vector Machine (SVM).</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <sc>
                  <bold>CCS</bold>
                </sc>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <sc>
                  <bold>CSS</bold>
                </sc>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <sc>
                  <bold>ESS</bold>
                </sc>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <sc>
                  <bold>IEMOCAP</bold>
                </sc>
              </th>
            </tr>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>[UAR %]</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dev</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Test</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>CI on test</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dev</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Test</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>CI on test</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dev</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Test</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>CI on test</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dev</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Test</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>CI on test</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><sc>Deep Spectrum</sc> + SVM (Schuller et al., <xref rid="B43" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.7−72.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.9−64.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">51.5−61.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">53.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">54.2−58.2</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><sc>DS Lite</sc> (no augmentation)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">71.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.2−79.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.1−66.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">43.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">54.3−66.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.2−63.9</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><sc>DS Lite</sc> (CutMix)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">57.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">71.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.5−79.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.4−68.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">43.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">54.1−65.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>59.7</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.6−64.0</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><sc>DS Lite</sc> (SpecAugment)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">58.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.9−80.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">58.1−69.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">48.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.4−67.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">54.9−63.5</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><sc>DS Lite</sc> (CutMix+SpecAugment)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>74.4</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.3−82.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>63.9</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.1−66.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">47.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>61.7</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.8−67.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">53.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">54.9−63.5</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>For the ComParE tasks, we evaluate against the official <sc>Deep Spectrum</sc> results presented in Schuller et al. (<xref rid="B43" ref-type="bibr">2021</xref>) while for IEMOCAP, we run the <sc>Deep Spectrum</sc> challenge baseline with the same settings ourselves. CCS, COVID-19 cough; CSS, COVID-19 speech; ESS, escalation in speech; IEMOCAP, emotional speech; UAR, unweighted average recall; CI, 95.0% confidence interval. Chance level in UAR: 50.0, 50.0, 33.3, and 25.0% for CCS, CSS, ESS, and IEMOCAP, respectively. For each dataset, the best result on the test partition is highlighted in boldface</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap position="float" id="T4">
        <label>Table 4</label>
        <caption>
          <p>Results of the transfer learning experiments with <sc>DeepSpectrumLite</sc> (<sc>DS Lite</sc>) on four datasets, the ComParE 2018 Snore Sub-Challenge tasks Snore, the ComParE 2019 Continuous Sleepiness Sub-Challenge SLEEP, RAVDESS emotional song and DEMoS.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>MPSSC</bold>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>SLEEP</bold>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>RAVDESS</bold>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>DEMoS</bold>
              </th>
            </tr>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>[UAR %]</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dev</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Test</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>CI on test</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dev</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Test</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>CI on test</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dev</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Test</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>CI on test</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dev</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Test</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>CI on test</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><sc>DS Lite</sc> (no augmentation)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">39.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33.4−45.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">65.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.9−68.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.7−84.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">58.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.8−61.4</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><sc>DS Lite</sc> (CutMix)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">43.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">43.7−55.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">71.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>69.1</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.5−71.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.6−84.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">65.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59.1−65.1</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><sc>DS Lite</sc> (SpecAugment)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">44.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">52.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">46.3−57.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">65.1−68.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>81.3</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.6−87.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">69.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">69.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">67.1−72.3</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><sc>DS Lite</sc> (CutMix+SpecAugment)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">39.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>54.2</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">49.4−58.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">67.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">67.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.9−70.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">71.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.9−80.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>72.5</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">70.1−75.0</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>For the Sleep task, we discretize the label into two classes, while for the DEMoS task, we recreate the dataset partitioning used in Baird et al. (<xref rid="B8" ref-type="bibr">2019</xref>). SLEEP, sleepiness; MPSSC, snore sound; RAVDESS, emotional song; DEMoS, emotional speech; UAR, unweighted average recall; CI, 95.0% confidence interval; Chance level in UAR, 25.0, 50.0, 20.0, and 14.3% for MPSSC, SLEEP, RAVDESS, and DEMoS, respectively. For each dataset, the best result on the test partition is highlighted in boldface</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>For the other four datasets (cf. <xref rid="T4" ref-type="table">Table 4</xref>), a direct comparison with ComParE Challenges or similar baseline systems was not possible. However, for the sake of consistency, we follow the same partitioning and evaluation strategy across all experiments. For all eight datasets, it can be seen that the applied augmentation methods improve the overall performance compared to the experiments without any augmentation. This effect is more prominent when datasets are small (e.g., for CCS, CSS, ESS, and MPSSC). The highest impact can be seen for the MPSSC dataset for which the augmentation with CutMix+SpecAugment lead to a 37.6% relative improvement on the test set compared to the model without any augmentation. In five out of eight datasets, the fusion of the CutMix and SpecAugment method was demonstrated to be superior to using these augmentation methods individually. We additionally provide 95.0% Confidence Intervals (CIs) on the test partitions. They were obtained by 1, 000× bootstrapping. In each iteration, a random selection of test samples is replaced and the UAR is computed. Moreover, the unweighted chance level for each dataset is given in each result table.</p>
    </sec>
    <sec>
      <title>3.4. Computational Performance</title>
      <p>The number of Floating Point Operations (FLOPs) is used here as a measure of the efficiency of the introduced framework. The more FLOPs an algorithm needs to finish, the longer it takes to run and the more power it consumes. Embedded devices typically have a limited power capacity, as they have a battery and no continuous power supply. Therefore, we take the <sc>DeepSpectrumLite</sc> framework's power efficiency into account. This subsection examines the models' FLOPs, mean execution time, mean of requested memory, and the model size. Our analysis is split into the audio signal preprocessing step, i. e., the spectrogram plot creation, and the final model inference. Both the TensorFlow (TF) model, and the spectrogram creation were executed 50 times on a 2.3 GHz Quad-Core Intel Core i5 CPU with 4 threads. To investigate the difference to the TF Lite model, we tested both models on the same system. Furthermore, we examined an on-device test on a consumer-grade smartphone <italic>Motorola moto e7 plus</italic> which comes with a 4 × 1.8 GHz Kryo 240, a 4 × 1.6 GHz Kryo 240, and an Adreno 610 GPU. Every on-device test was repeated 50 times. <xref rid="T5" ref-type="table">Table 5</xref> shows the performance results of our spectrogram image creation and the <sc>DenseNet121</sc> model which includes the classification layers as well. The spectrogram creation has a model size of 150.0 kB, a mean execution time of 7.1 ms, and it consumes 4.5 MB memory. Because the plot generation is not a TF model, we cannot measure the FLOPs nor are there any parameters. However, the number of FLOPs is expected to be small based on the measured execution time. During the transformation from the TF HDF file format to the TF Lite model, the model size is reduced by the factor of 2.7. Although the TF Lite model consumes more memory than the regular TF model, the mean inference time is reduced by 150.3 ms measured on the same CPU setup. The TF Lite model has a mean inference time of 242.0 ms on our embedded device.</p>
      <table-wrap position="float" id="T5">
        <label>Table 5</label>
        <caption>
          <p>This table shows the mean execution time, the number of parameters, the mean requested memory, and the model size of our preprocessing (prepr.), the <sc>DenseNet121</sc>
<sc>TensorFlow</sc> (TF), and TF Lite model.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1"><bold>Prepr</bold>.</th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>TF model</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>TF lite model</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Mean time [ms]</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">240.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.7/242.0</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">FLOPs</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.1 G</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Parameters</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.6 M</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.6 M</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Mean memory [MB]</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">116.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">185.4/292.8</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Model size</td>
              <td valign="top" align="center" rowspan="1" colspan="1">150.0 kb</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.1 MB</td>
              <td valign="top" align="center" rowspan="1" colspan="1">30.0 MB</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>In the TF Lite Model column, the values before the slash are from the test on the CPU system, whereas the values after the slash are from the on-device test. Details regarding the test setup are described in the text. FLOPs, floating point operations</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.5. Explainability Challenges</title>
      <p>The explainability of deep learning models is a well-known challenge that we would like to briefly mention in this part of our manuscript. For each component (audio processing, feature extraction, classification) of the <sc>DeepSpectrumLite</sc> framework, a different degree of explainability can be achieved. It should be mentioned that due to the complexity of the applied DNNs for deep feature extraction and classification, the obtained results and the behavior of the models cannot be thoroughly explained. However, we try to approximately explain the framework's decision-making process in a threefold manner. First, we provide Mel spectrogram plots for <italic>negative</italic> and <italic>positive</italic> classes of both CSS and CCS datasets as case examples and analyse them. Second, we discuss the DNN models' outputs with the help of SHAP (Lundberg and Lee, <xref rid="B33" ref-type="bibr">2017</xref>) method. Third, we provide confusion matrices of the classification results and compare the confusion between class predictions.</p>
      <p>For the first two parts of our explainability approach, we analyse sample Mel spectrograms from the <italic>negative</italic> and <italic>positive</italic> classes of the COVID-19 cough and speech datasets w. r. t. strength of the audio signals over time at a waveform's various frequencies. Further, we provide the outputs of SHAP (Lundberg and Lee, <xref rid="B33" ref-type="bibr">2017</xref>), a game-theoretic approach to explain the output of machine learning models. SHAP assigns each feature—in our article, regions of the Mel spectrogram plots—an <italic>importance value</italic> for each particular class prediction (cf. right side of the <xref rid="F2" ref-type="fig">Figure 2</xref>). The sample spectrograms of the <italic>negative</italic> class of both datasets show a harmonic broad-band pattern with a quite similar width. Contrarily, the <italic>positive</italic> class is characterized by disruptions in the amplitude for distinct frequency ranges, indicating discontinuities in the pulmonic airstream during phonation in COVID-19 <italic>positive</italic> participants (Bartl-Pokorny et al., <xref rid="B9" ref-type="bibr">2021</xref>). The impact of each segment of the Mel spectrograms on the output of the DNN model is visualized with help of SHAP values. Areas of the audio plots that increase the probability of the class are colored in red, and blue areas decrease the probability. For example, for the given <italic>negative</italic> class sample of the CSS dataset, F0 or fundamental frequency (marked as blue squares on the bottom of the SHAP output) pushes the prediction for the <italic>negative</italic> class lower, whilst higher frequencies (marked as red squares on the top of the SHAP output) do the opposite. For the <italic>positive</italic> class sample of CSS, an almost reverse pattern can be seen. The more prominent the red squares, the higher the model's confidence in predicting the target class. For the COVID-19 cough samples, we see that the model is quite confident in predicting the <italic>positive</italic> COVID-19 cough, whilst for the <italic>negative</italic> class, mainly the F0 and top frequencies at the beginning and the end of the non-COVID-19 cough cause it to be classified as <italic>negative</italic>.</p>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p>Visualization of the SHapley Additive exPlanations (SHAP) outputs for sample Mel spectrograms from <italic>negative</italic> and <italic>positive</italic> classes of CSS and CCS datasets. The x-axis of the spectrograms represents time [0–5 s] and the y-axis represents the frequency [0–4,096 Hz]. The range of the SHAP values for each model's output is given in the color bar below each image. Areas that increase the probability of the class are colored in red, and blue areas decrease the probability. A detailed account of the spectrogram and SHAP values analysis is given in Section 3.5. <bold>(A)</bold> SHAP outputs from the COVID-19 speech (CSS) model. <bold>(B)</bold> SHAP outputs from the COVID-19 cough (CCS) model.</p>
        </caption>
        <graphic xlink:href="frai-05-856232-g0002" position="float"/>
      </fig>
      <p>For the last part of our explainability approach, we analyse the confusion matrices of the classification results on the test partitions of CSS and CCS corpora. With the help of <xref rid="F3" ref-type="fig">Figure 3</xref>, the True Positive Rate (TPR) (sensitivity), True Negative Rate (TNR) (specificity) and the class-wise performance of the DNN models for each task can be obtained. The specificity of the CSS model (72.5%) is much higher than its sensitivity (55.3%), indicating the model's ability to correctly reject healthy patients without COVID-19 and its challenges to correctly detect ill patients who do have COVID-19. On the contrary, the low confusion between the CCS classes compared to CSS implies the superiority of the ‘cough-based' model over the “speech-based” one for the recognition of COVID-19.</p>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p>Confusion matrices for the test results obtained by the best CSS <bold>(A)</bold> and CCS <bold>(B)</bold> models. In the cells, the absolute number of cases is given, and the percentage of “classified as” of the class is provided in the respective row. The percentage values are also indicated by a color scale: the darker, the higher the prediction. A detailed account of the confusion matrices analysis is given in Section 3.5.</p>
        </caption>
        <graphic xlink:href="frai-05-856232-g0003" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>4. Discussion</title>
    <p>The results achieved with <sc>DeepSpectrumLite</sc> (described in Section 3.3) on all eight tasks show the system's efficacy, in particular, compared to the traditional <sc>Deep Spectrum</sc> feature extraction. Furthermore, the applied state-of-the-art CutMix (Yun et al., <xref rid="B51" ref-type="bibr">2019</xref>) and SpecAugment (Park et al., <xref rid="B38" ref-type="bibr">2019</xref>) techniques in combination with an adapted version of the SapAugment (Hu et al., <xref rid="B19" ref-type="bibr">2021</xref>) policy proved themselves to be useful for all datasets, especially for the smaller ones. For the IEMOCAP dataset, our best performing model achieves comparable results with the recently published EmoNet paper which uses the same partitioning strategy (Gerczuk et al., <xref rid="B15" ref-type="bibr">2021</xref>).</p>
    <p>A gap can be seen between the performance of the trained models on the development and test partitions. This could partially be explained by the lack of training materials for the MLP classifier while optimizing it on the development partition. On the other hand, the better performance on the test set could be a result of training the final model on the combined train and development sets before evaluating on the test partition. Further, the effect of dataset imbalance and suboptimal distribution of audio samples in each partition should not be neglected here.</p>
    <p>Considering embedded devices, such as consumer-grade smartphones, as deployment targets, <sc>DeepSpectrumLite</sc> is further suitable for real-time speech recognition tasks. With a total inference time of only a quarter of a second for a 3-s long raw audio chunk, time-continuous analysis from raw microphone input can be performed directly on-device. The measured performance, both in terms of recognition accuracies on the datasets as well as inference times, make <sc>DeepSpectrumLite</sc> a powerful framework for many paralinguistic and general audio recognition tasks where data is often scarce.</p>
  </sec>
  <sec sec-type="conclusions" id="s5">
    <title>5. Conclusion</title>
    <p>In this article, we presented a framework for training and deploying power-efficient deep learning models for embedded speech and audio processing. By making use of transfer learning from ImageNet pre-trained deep CNNs with spectrogram inputs and state-of-the-art data-augmentation techniques, <sc>DeepSpectrumLite</sc> can produce powerful speech and audio analysis models with high accuracy that can then be easily deployed to embedded devices as an end-to-end prediction pipeline from raw microphone input. Our framework showed high performance for general speech-based paralinguistic tasks, music emotion recognition, and a range of speech and audio-based health monitoring tasks.</p>
    <p>We have publicly released the framework including a flexible command-line interface on GitHub, such that interested researchers can use it in their own research for a variety of low-resource, real-time speech and audio recognition tasks, train their own models and apply them on embedded devices. Using the provided repository and the given parameters it is possible to reproduce all experiments conducted in this manuscript.</p>
    <p>In Section 3.5, we have discussed the subject of explainability for our framework and given more insight into the decision making process of the DNN models by analysing the generated audio plots, DNN models' predictions, and confusion matrices. A quantitative study comparing <sc>Deep Spectrum</sc> against both audio pre-trained models and untrained CNNs has been undertaken in Amiriparian et al. (<xref rid="B7" ref-type="bibr">2020</xref>) demonstrating the efficacy of using image pre-trained CNNs for audio tasks. Motivated by the findings in Amiriparian et al. (<xref rid="B7" ref-type="bibr">2020</xref>), we have decided to use pre-trained image CNNs as deep feature extractors in our framework.</p>
    <p>For future work, further reductions in model size can be pursued. From an efficiency standpoint of view, networks specifically designed with smaller memory and computation footprints than <sc>DenseNet121</sc>, such as Mobilenets (Howard et al., <xref rid="B18" ref-type="bibr">2017</xref>) or SqueezeNet (Iandola et al., <xref rid="B23" ref-type="bibr">2016</xref>) can be a better choice for the targeted applications and thus should be evaluated as feature extractors in <sc>DeepSpectrumLite</sc> . Using our framework, it is possible to select between 14 base models, including the above-mentioned CNNs. Furthermore, techniques such as pruning and quantization (Han et al., <xref rid="B17" ref-type="bibr">2015</xref>; Lin et al., <xref rid="B31" ref-type="bibr">2019</xref>; Zhou et al., <xref rid="B56" ref-type="bibr">2019</xref>) can be explored together with their impacts on speed and model accuracy. Finally, future iterations of our framework could also be enhanced with explainability methods like those mentioned above to make the model predictions more interpretable.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data Availability Statement</title>
    <p>The original contributions presented in the study are included in the article/supplementary material, further inquiries can be directed to the corresponding author/s.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>SA, TH, VK, and MG conceptualized the study and ran the machine learning experiments. SO and BS gave technical advice, did literature analysis, manuscript preparation, and editing. MG and VK helped with running the experiments and testing the codes. All authors revised, developed, read, and approved the final manuscript.</p>
  </sec>
  <sec sec-type="funding-information" id="s8">
    <title>Funding</title>
    <p>This study presented has received funding from the BMW Group. Further, we acknowledge funding from the DFG's Reinhart Koselleck project No. 442218748 (AUDI0NOMOUS) and the DFG project No. 421613952 (ParaStiChaD).</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s9">
    <title>Publisher's Note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="fn0001">
      <p>
        <sup>1</sup>
        <ext-link xlink:href="https://github.com/DeepSpectrum/DeepSpectrum" ext-link-type="uri">https://github.com/DeepSpectrum/DeepSpectrum</ext-link>
      </p>
    </fn>
    <fn id="fn0002">
      <p>
        <sup>2</sup>
        <ext-link xlink:href="https://github.com/DeepSpectrum/DeepSpectrumLite" ext-link-type="uri">https://github.com/DeepSpectrum/DeepSpectrumLite</ext-link>
      </p>
    </fn>
    <fn id="fn0003">
      <p>
        <sup>3</sup>
        <ext-link xlink:href="https://www.tensorflow.org/lite" ext-link-type="uri">https://www.tensorflow.org/lite</ext-link>
      </p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Amiriparian</surname><given-names>S.</given-names></name></person-group> (<year>2019</year>). <source>Deep representation learning techniques for audio signal processing</source> (Ph.D. thesis). <publisher-name>Technische Universität München</publisher-name>, <publisher-loc>Munich, Germany</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amiriparian</surname><given-names>S.</given-names></name><name><surname>Cummins</surname><given-names>N.</given-names></name><name><surname>Gerczuk</surname><given-names>M.</given-names></name><name><surname>Pugachevskiy</surname><given-names>S.</given-names></name><name><surname>Ottl</surname><given-names>S.</given-names></name><name><surname>Schuller</surname><given-names>B.</given-names></name></person-group> (<year>2019</year>). <article-title>“Are you playing a shooter again?!!” deep representation learning for audio-based video game genre recognition</article-title>. <source>IEEE Trans. Games</source>
<volume>12</volume>, <fpage>145</fpage>–<lpage>154</lpage>. <pub-id pub-id-type="doi">10.1109/TG.2019.2894532</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Amiriparian</surname><given-names>S.</given-names></name><name><surname>Cummins</surname><given-names>N.</given-names></name><name><surname>Ottl</surname><given-names>S.</given-names></name><name><surname>Gerczuk</surname><given-names>M.</given-names></name><name><surname>Schuller</surname><given-names>B.</given-names></name></person-group> (<year>2017a</year>). <article-title>Sentiment analysis using image-based deep spectrum features</article-title>, in <source>Proceedings of the 7th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos</source> (<publisher-loc>San Antonio, TX</publisher-loc>), <fpage>26</fpage>–<lpage>29</lpage>. <pub-id pub-id-type="doi">10.1109/ACIIW.2017.8272618</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Amiriparian</surname><given-names>S.</given-names></name><name><surname>Freitag</surname><given-names>M.</given-names></name><name><surname>Cummins</surname><given-names>N.</given-names></name><name><surname>Schuller</surname><given-names>B.</given-names></name></person-group> (<year>2017b</year>). <article-title>Sequence to sequence autoencoders for unsupervised representation learning from audio</article-title>, in <source>Proceedings of the DCASE 2017 Workshop</source> (<publisher-loc>Munich</publisher-loc>), <fpage>17</fpage>–<lpage>21</lpage>. <?supplied-pmid 30441416?></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Amiriparian</surname><given-names>S.</given-names></name><name><surname>Gerczuk</surname><given-names>M.</given-names></name><name><surname>Ottl</surname><given-names>S.</given-names></name><name><surname>Cummins</surname><given-names>N.</given-names></name><name><surname>Freitag</surname><given-names>M.</given-names></name><name><surname>Pugachevskiy</surname><given-names>S.</given-names></name><etal/></person-group>. (<year>2017c</year>). <article-title>Snore sound classification using image-based deep spectrum features</article-title>, in <source>Proceedings of INTERSPEECH</source> (<publisher-loc>Stockholm</publisher-loc>), <fpage>3512</fpage>–<lpage>3516</lpage>. <pub-id pub-id-type="doi">10.21437/Interspeech.2017-434</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Amiriparian</surname><given-names>S.</given-names></name><name><surname>Gerczuk</surname><given-names>M.</given-names></name><name><surname>Ottl</surname><given-names>S.</given-names></name><name><surname>Cummins</surname><given-names>N.</given-names></name><name><surname>Pugachevskiy</surname><given-names>S.</given-names></name><name><surname>Schuller</surname><given-names>B.</given-names></name></person-group> (<year>2018</year>). <article-title>Bag-of-deep-features: Noise-robust deep feature representations for audio analysis</article-title>, in <source>Proceedings of the International Joint Conference on Neural Networks</source> (<publisher-loc>Rio de Janieiro</publisher-loc>), <fpage>1</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1109/IJCNN.2018.8489416</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amiriparian</surname><given-names>S.</given-names></name><name><surname>Gerczuk</surname><given-names>M.</given-names></name><name><surname>Ottl</surname><given-names>S.</given-names></name><name><surname>Stappen</surname><given-names>L.</given-names></name><name><surname>Baird</surname><given-names>A.</given-names></name><name><surname>Koebe</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Towards cross-modal pre-training and learning tempo-spatial characteristics for audio recognition with convolutional and recurrent neural networks</article-title>. <source>EURASIP J. Audio Speech Mus. Process</source>. <volume>2020</volume>, <fpage>1</fpage>–<lpage>11</lpage>. <pub-id pub-id-type="doi">10.1186/s13636-020-00186-0</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Baird</surname><given-names>A.</given-names></name><name><surname>Amiriparian</surname><given-names>S.</given-names></name><name><surname>Schuller</surname><given-names>B.</given-names></name></person-group> (<year>2019</year>). <article-title>Can deep generative audio be emotional? Towards an approach for personalised emotional audio generation</article-title>, in <source>2019 IEEE 21st International Workshop on Multimedia Signal Processing</source> (<publisher-loc>Kuala Lumper</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.1109/MMSP.2019.8901785</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartl-Pokorny</surname><given-names>K. D.</given-names></name><name><surname>Pokorny</surname><given-names>F. B.</given-names></name><name><surname>Batliner</surname><given-names>A.</given-names></name><name><surname>Amiriparian</surname><given-names>S.</given-names></name><name><surname>Semertzidou</surname><given-names>A.</given-names></name><name><surname>Eyben</surname><given-names>F.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>The voice of covid-19: acoustic correlates of infection in sustained vowels</article-title>. <source>J. Acoust. Soc. Am</source>. <volume>149</volume>, <fpage>4377</fpage>–<lpage>4383</lpage>. <pub-id pub-id-type="doi">10.1121/10.0005194</pub-id><?supplied-pmid 34241490?><pub-id pub-id-type="pmid">34241490</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Busso</surname><given-names>C.</given-names></name><name><surname>Bulut</surname><given-names>M.</given-names></name><name><surname>Lee</surname><given-names>C.-C.</given-names></name><name><surname>Kazemzadeh</surname><given-names>A.</given-names></name><name><surname>Mower</surname><given-names>E.</given-names></name><name><surname>Kim</surname><given-names>S.</given-names></name><etal/></person-group>. (<year>2008</year>). <article-title>IEMOCAP: interactive emotional dyadic motion capture database</article-title>. <source>Lang. Resour. Eval</source>. <volume>42</volume>:<fpage>335</fpage>. <pub-id pub-id-type="doi">10.1007/s10579-008-9076-6</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>D.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Xu</surname><given-names>K.</given-names></name></person-group> (<year>2020</year>). <publisher-loc>Arelu</publisher-loc>: <publisher-name>attention-based rectified linear unit</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cilliers</surname><given-names>L.</given-names></name></person-group> (<year>2020</year>). <article-title>Wearable devices in healthcare: privacy and information security issues</article-title>. <source>Health Inform. Manage. J</source>. <volume>49</volume>, <fpage>150</fpage>–<lpage>156</lpage>. <pub-id pub-id-type="doi">10.1177/1833358319851684</pub-id><?supplied-pmid 31146589?><pub-id pub-id-type="pmid">31146589</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J.</given-names></name><name><surname>Dong</surname><given-names>W.</given-names></name><name><surname>Socher</surname><given-names>R.</given-names></name><name><surname>Li</surname><given-names>L.-J.</given-names></name><name><surname>Li</surname><given-names>K.</given-names></name><name><surname>Fei-Fei</surname><given-names>L.</given-names></name></person-group> (<year>2009</year>). <article-title>ImageNet: a large-scale hierarchical image database</article-title>, in <source>Proceedings of the Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Miami, FL</publisher-loc>), <fpage>248</fpage>–<lpage>255</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freitag</surname><given-names>M.</given-names></name><name><surname>Amiriparian</surname><given-names>S.</given-names></name><name><surname>Pugachevskiy</surname><given-names>S.</given-names></name><name><surname>Cummins</surname><given-names>N.</given-names></name><name><surname>Schuller</surname><given-names>B.</given-names></name></person-group> (<year>2017</year>). <article-title>auDeep: Unsupervised learning of representations from audio with deep recurrent neural networks</article-title>. <source>J. Mach. Learn. Res</source>. <volume>18</volume>, <fpage>6340</fpage>–<lpage>6344</lpage>. <pub-id pub-id-type="doi">10.5555/3122009.3242030</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerczuk</surname><given-names>M.</given-names></name><name><surname>Amiriparian</surname><given-names>S.</given-names></name><name><surname>Ottl</surname><given-names>S.</given-names></name><name><surname>Schuller</surname><given-names>B. W.</given-names></name></person-group> (<year>2021</year>). <article-title>EmoNet: a transfer learning framework for multi-corpus speech emotion recognition</article-title>. <source>IEEE Trans. Affect. Comput</source>. <fpage>1</fpage>–<lpage>1</lpage>. <pub-id pub-id-type="doi">10.1109/TAFFC.2021.3135152</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gholami</surname><given-names>A.</given-names></name><name><surname>Kwon</surname><given-names>K.</given-names></name><name><surname>Wu</surname><given-names>B.</given-names></name><name><surname>Tai</surname><given-names>Z.</given-names></name><name><surname>Yue</surname><given-names>X.</given-names></name><name><surname>Jin</surname><given-names>P.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Squeezenext: hardware-aware neural network design</article-title>, in <source>Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</source> (<publisher-loc>Salt Lake City, UT</publisher-loc>), <fpage>1638</fpage>–<lpage>1647</lpage>. <pub-id pub-id-type="doi">10.1109/CVPRW.2018.00215</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>S.</given-names></name><name><surname>Mao</surname><given-names>H.</given-names></name><name><surname>Dally</surname><given-names>W. J.</given-names></name></person-group> (<year>2015</year>). <article-title>Deep compression: compressing deep neural networks with pruning, trained quantization and Huffman coding</article-title>. <source>arXiv preprint arXiv:1510.00149</source>.</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>A. G.</given-names></name><name><surname>Zhu</surname><given-names>M.</given-names></name><name><surname>Chen</surname><given-names>B.</given-names></name><name><surname>Kalenichenko</surname><given-names>D.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name><name><surname>Weyand</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>MobileNets: Efficient convolutional neural networks for mobile vision applications</article-title>. <source>arXiv:1704.04861</source>.</mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>T.-Y.</given-names></name><name><surname>Shrivastava</surname><given-names>A.</given-names></name><name><surname>Chang</surname><given-names>J.-H. R.</given-names></name><name><surname>Koppula</surname><given-names>H.</given-names></name><name><surname>Braun</surname><given-names>S.</given-names></name><name><surname>Hwang</surname><given-names>K.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>SapAugment: Learning a sample adaptive policy for data augmentation</article-title>, in <source>ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing</source> (<publisher-loc>ICASSP</publisher-loc>), <fpage>4040</fpage>–<lpage>4044</lpage>. <pub-id pub-id-type="doi">10.1109/ICASSP39728.2021.9413928</pub-id><?supplied-pmid 27295638?></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>G.</given-names></name><name><surname>Liu</surname><given-names>Z.</given-names></name><name><surname>Van Der Maaten</surname><given-names>L.</given-names></name><name><surname>Weinberger</surname><given-names>K. Q.</given-names></name></person-group> (<year>2017</year>). <article-title>Densely connected convolutional networks</article-title>, in <source>Proceedings of the Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Honolulu, HI</publisher-loc>), <fpage>4700</fpage>–<lpage>4708</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2017.243</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hui</surname><given-names>T.-W.</given-names></name><name><surname>Tang</surname><given-names>X.</given-names></name><name><surname>Loy</surname><given-names>C. C.</given-names></name></person-group> (<year>2018</year>). <article-title>Liteflownet: a lightweight convolutional neural network for optical flow estimation</article-title>, in <source>Proceedings of the Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Salt Lake City, UT</publisher-loc>), <fpage>8981</fpage>–<lpage>8989</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2018.00936</pub-id><?supplied-pmid 32142417?></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hutchinson</surname><given-names>M. L.</given-names></name><name><surname>Antono</surname><given-names>E.</given-names></name><name><surname>Gibbons</surname><given-names>B. M.</given-names></name><name><surname>Paradiso</surname><given-names>S.</given-names></name><name><surname>Ling</surname><given-names>J.</given-names></name><name><surname>Meredig</surname><given-names>B.</given-names></name></person-group> (<year>2017</year>). <article-title>Overcoming data scarcity with transfer learning</article-title>. <source>arXiv preprint arXiv:1711.05099</source>.</mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iandola</surname><given-names>F. N.</given-names></name><name><surname>Han</surname><given-names>S.</given-names></name><name><surname>Moskewicz</surname><given-names>M. W.</given-names></name><name><surname>Ashraf</surname><given-names>K.</given-names></name><name><surname>Dally</surname><given-names>W. J.</given-names></name><name><surname>Keutzer</surname><given-names>K.</given-names></name></person-group> (<year>2016</year>). <article-title>Squeezenet: alexnet-level accuracy with 50x fewer parameters and &lt;0.5 mb model size</article-title>. <source>arXiv preprint arXiv:1602.07360</source>.</mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janott</surname><given-names>C.</given-names></name><name><surname>Schmitt</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Qian</surname><given-names>K.</given-names></name><name><surname>Pandit</surname><given-names>V.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Snoring classified: the Munich-Passau snore sound corpus</article-title>. <source>Comput. Biol. Med</source>. <volume>94</volume>, <fpage>106</fpage>–<lpage>118</lpage>. <pub-id pub-id-type="doi">10.1016/j.compbiomed.2018.01.007</pub-id><?supplied-pmid 29407995?><pub-id pub-id-type="pmid">29407995</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kezirian</surname><given-names>E. J.</given-names></name><name><surname>Hohenhorst</surname><given-names>W.</given-names></name><name><surname>de Vries</surname><given-names>N.</given-names></name></person-group> (<year>2011</year>). <article-title>Drug-induced sleep endoscopy: the vote classification</article-title>. <source>Eur. Arch. Oto-Rhino-Laryngol</source>. <volume>268</volume>, <fpage>1233</fpage>–<lpage>1236</lpage>. <pub-id pub-id-type="doi">10.1007/s00405-011-1633-8</pub-id><?supplied-pmid 21614467?><pub-id pub-id-type="pmid">21614467</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ko</surname><given-names>J.</given-names></name><name><surname>Lu</surname><given-names>C.</given-names></name><name><surname>Srivastava</surname><given-names>M. B.</given-names></name><name><surname>Stankovic</surname><given-names>J. A.</given-names></name><name><surname>Terzis</surname><given-names>A.</given-names></name><name><surname>Welsh</surname><given-names>M.</given-names></name></person-group> (<year>2010</year>). <article-title>Wireless sensor networks for healthcare</article-title>. <source>Proc. IEEE</source>
<volume>98</volume>, <fpage>1947</fpage>–<lpage>1960</lpage>. <pub-id pub-id-type="doi">10.1109/JPROC.2010.2065210</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Hinton</surname><given-names>G. E.</given-names></name></person-group> (<year>2012</year>). <article-title>Imagenet classification with deep convolutional neural networks</article-title>. <source>Adv. Neural Inform. Process. Syst</source>. <volume>25</volume>, <fpage>1097</fpage>–<lpage>1105</lpage>. <pub-id pub-id-type="doi">10.1145/3065386</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lefter</surname><given-names>I.</given-names></name><name><surname>Burghouts</surname><given-names>G. J.</given-names></name><name><surname>Rothkrantz</surname><given-names>L. J.</given-names></name></person-group> (<year>2014</year>). <article-title>An audio-visual dataset of human-human interactions in stressful situations</article-title>. <source>J. Multimodal User Interfaces</source>
<volume>8</volume>, <fpage>29</fpage>–<lpage>41</lpage>. <pub-id pub-id-type="doi">10.1007/s12193-014-0150-7</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lefter</surname><given-names>I.</given-names></name><name><surname>Rothkrantz</surname><given-names>L. J.</given-names></name><name><surname>Burghouts</surname><given-names>G. J.</given-names></name></person-group> (<year>2013</year>). <article-title>A comparative study on automatic audio-visual fusion for aggression detection using meta-information</article-title>. <source>Pattern Recogn. Lett</source>. <volume>34</volume>, <fpage>1953</fpage>–<lpage>1963</lpage>. <pub-id pub-id-type="doi">10.1016/j.patrec.2013.01.002</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>T.</given-names></name><name><surname>Sahu</surname><given-names>A. K.</given-names></name><name><surname>Talwalkar</surname><given-names>A.</given-names></name><name><surname>Smith</surname><given-names>V.</given-names></name></person-group> (<year>2020</year>). <article-title>Federated learning: challenges, methods, and future directions</article-title>. <source>Sign. Process. Mag</source>. <volume>37</volume>, <fpage>50</fpage>–<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1109/MSP.2020.2975749</pub-id><?supplied-pmid 31419814?><pub-id pub-id-type="pmid">31419814</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>S.</given-names></name><name><surname>Ji</surname><given-names>R.</given-names></name><name><surname>Yan</surname><given-names>C.</given-names></name><name><surname>Zhang</surname><given-names>B.</given-names></name><name><surname>Cao</surname><given-names>L.</given-names></name><name><surname>Ye</surname><given-names>Q.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Towards optimal structured cnn pruning <italic>via</italic> generative adversarial learning</article-title>, in <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>, (<publisher-loc>Long Beach, CA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>2790</fpage>–<lpage>2799</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2019.00290</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livingstone</surname><given-names>S. R.</given-names></name><name><surname>Russo</surname><given-names>F. A.</given-names></name></person-group> (<year>2018</year>). <article-title>The Ryerson audio-visual database of emotional speech and song (RAVDESS): a dynamic, multimodal set of facial and vocal expressions in North American English</article-title>. <source>PLoS ONE</source>
<volume>13</volume>:<fpage>e0196391</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0196391</pub-id><?supplied-pmid 29768426?><pub-id pub-id-type="pmid">29768426</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lundberg</surname><given-names>S. M.</given-names></name><name><surname>Lee</surname><given-names>S.-I.</given-names></name></person-group> (<year>2017</year>). <article-title>A unified approach to interpreting model predictions</article-title>. <source>Adv. Neural Inform. Process. Syst</source>. <volume>30</volume>:<fpage>10</fpage>.</mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mehta</surname><given-names>S.</given-names></name><name><surname>Rastegari</surname><given-names>M.</given-names></name><name><surname>Shapiro</surname><given-names>L.</given-names></name><name><surname>Hajishirzi</surname><given-names>H.</given-names></name></person-group> (<year>2019</year>). <article-title>ESPNetv2: a light-weight, power efficient, and general purpose convolutional neural network</article-title>, in <source>Proceedings of the Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Long Beach, CA</publisher-loc>), <fpage>9190</fpage>–<lpage>9200</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2019.00941</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montgomery</surname><given-names>K.</given-names></name><name><surname>Chester</surname><given-names>J.</given-names></name><name><surname>Kopp</surname><given-names>K.</given-names></name></person-group> (<year>2018</year>). <article-title>Health wearables: ensuring fairness, preventing discrimination, and promoting equity in an emerging internet-of-things environment</article-title>. <source>J. Information Policy</source>
<volume>8</volume>, <fpage>34</fpage>–<lpage>77</lpage>. <pub-id pub-id-type="doi">10.5325/jinfopoli.8.1.0034</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ottl</surname><given-names>S.</given-names></name><name><surname>Amiriparian</surname><given-names>S.</given-names></name><name><surname>Gerczuk</surname><given-names>M.</given-names></name><name><surname>Karas</surname><given-names>V.</given-names></name><name><surname>Schuller</surname><given-names>B.</given-names></name></person-group> (<year>2020</year>). <article-title>Group-level speech emotion recognition utilising deep spectrum features</article-title>, in <source>Proceedings of the International Conference on Multimodal Interaction</source> (<publisher-loc>Utrecht</publisher-loc>), <fpage>821</fpage>–<lpage>826</lpage>. <pub-id pub-id-type="doi">10.1145/3382507.3417964</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parada-Cabaleiro</surname><given-names>E.</given-names></name><name><surname>Costantini</surname><given-names>G.</given-names></name><name><surname>Batliner</surname><given-names>A.</given-names></name><name><surname>Schmitt</surname><given-names>M.</given-names></name><name><surname>Schuller</surname><given-names>B. W.</given-names></name></person-group> (<year>2020</year>). <article-title>Demos: an Italian emotional speech corpus</article-title>. <source>Lang. Resour. Eval</source>. <volume>54</volume>, <fpage>341</fpage>–<lpage>383</lpage>. <pub-id pub-id-type="doi">10.1007/s10579-019-09450-y</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Park</surname><given-names>D. S.</given-names></name><name><surname>Chan</surname><given-names>W.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Chiu</surname><given-names>C.-C.</given-names></name><name><surname>Zoph</surname><given-names>B.</given-names></name><name><surname>Cubuk</surname><given-names>E. D.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>SpecAugment: a simple data augmentation method for automatic speech recognition</article-title>, in <source>Proceedings of INTERSPEECH</source> (<publisher-loc>Graz</publisher-loc>), <fpage>2613</fpage>–<lpage>2617</lpage>. <pub-id pub-id-type="doi">10.21437/Interspeech.2019-2680</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perez</surname><given-names>L.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name></person-group> (<year>2017</year>). <article-title>The effectiveness of data augmentation in image classification using deep learning</article-title>. <source>arXiv preprint arXiv:1712.04621</source>.</mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schuller</surname><given-names>B.</given-names></name><name><surname>Batliner</surname><given-names>A.</given-names></name><name><surname>Bergler</surname><given-names>C.</given-names></name><name><surname>Pokorny</surname><given-names>F. B.</given-names></name><name><surname>Krajewski</surname><given-names>J.</given-names></name><name><surname>Cychosz</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>The interspeech 2019 computational paralinguistics challenge: Styrian dialects, continuous sleepiness, baby sounds &amp; orca activity</article-title>, in <source>INTERSPEECH</source> (<publisher-loc>Graz</publisher-loc>). <pub-id pub-id-type="doi">10.21437/Interspeech.2019-1122</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schuller</surname><given-names>B.</given-names></name><name><surname>Friedmann</surname><given-names>F.</given-names></name><name><surname>Eyben</surname><given-names>F.</given-names></name></person-group> (<year>2013</year>). <article-title>Automatic recognition of physiological parameters in the human voice: heart rate and skin conductance</article-title>, in <source>Proceedings of the International Conference on Acoustics, Speech and Signal Processing</source> (<publisher-loc>Vancouver, BC</publisher-loc>), <fpage>7219</fpage>–<lpage>7223</lpage>. <pub-id pub-id-type="doi">10.1109/ICASSP.2013.6639064</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schuller</surname><given-names>B.</given-names></name><name><surname>Steidl</surname><given-names>S.</given-names></name><name><surname>Batliner</surname><given-names>A.</given-names></name><name><surname>Bergelson</surname><given-names>E.</given-names></name><name><surname>Krajewski</surname><given-names>J.</given-names></name><name><surname>Janott</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>The interspeech 2017 computational paralinguistics challenge: addressee, cold &amp; snoring</article-title>, in <source>Computational Paralinguistics Challenge (ComParE)</source>, (<publisher-loc>Stockholm</publisher-loc>: <publisher-name>Interspeech</publisher-name>), <fpage>3442</fpage>–<lpage>3446</lpage>. <pub-id pub-id-type="doi">10.21437/Interspeech.2017-43</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schuller</surname><given-names>B. W.</given-names></name><name><surname>Batliner</surname><given-names>A.</given-names></name><name><surname>Bergler</surname><given-names>C.</given-names></name><name><surname>Mascolo</surname><given-names>C.</given-names></name><name><surname>Han</surname><given-names>J.</given-names></name><name><surname>Lefter</surname><given-names>I.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>The INTERSPEECH 2021 computational paralinguistics challenge: COVID-19 cough, COVID-19 speech, escalation &amp; primates</article-title>, in <source>Proceedings of Interspeech 2021</source> (<publisher-loc>Brno</publisher-loc>), <fpage>431</fpage>–<lpage>435</lpage>. <pub-id pub-id-type="doi">10.21437/Interspeech.2021-19</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="book"><person-group person-group-type="editor"><name><surname>Shahid</surname><given-names>A.</given-names></name><name><surname>Wilkinson</surname><given-names>K.</given-names></name><name><surname>Marcu</surname><given-names>S.</given-names></name><name><surname>Shapiro</surname><given-names>C. M. (eds).</given-names></name></person-group> (<year>2011</year>). <article-title>Karolinska sleepiness scale (kss)</article-title>, in <source>STOP, THAT and One Hundred Other Sleep Scales</source>, eds <person-group person-group-type="editor"><name><surname>Shahid</surname><given-names>A.</given-names></name><name><surname>Wilkinson</surname><given-names>K.</given-names></name><name><surname>Marcu</surname><given-names>S.</given-names></name><name><surname>Shapiro</surname><given-names>C. M.</given-names></name></person-group> (<publisher-name>Springer</publisher-name>), <fpage>209</fpage>–<lpage>210</lpage>. <pub-id pub-id-type="doi">10.1007/978-1-4419-9893-4_47</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shie</surname><given-names>C.-K.</given-names></name><name><surname>Chuang</surname><given-names>C.-H.</given-names></name><name><surname>Chou</surname><given-names>C.-N.</given-names></name><name><surname>Wu</surname><given-names>M.-H.</given-names></name><name><surname>Chang</surname><given-names>E. Y.</given-names></name></person-group> (<year>2015</year>). <article-title>Transfer representation learning for medical image analysis</article-title>, in <source>2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</source> (<publisher-loc>Milano</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>711</fpage>–<lpage>714</lpage>. <pub-id pub-id-type="doi">10.1109/EMBC.2015.7318461</pub-id><?supplied-pmid 26736361?></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shorten</surname><given-names>C.</given-names></name><name><surname>Khoshgoftaar</surname><given-names>T. M.</given-names></name></person-group> (<year>2019</year>). <article-title>A survey on image data augmentation for deep learning</article-title>. <source>J. Big Data</source>
<volume>6</volume>, <fpage>1</fpage>–<lpage>48</lpage>. <pub-id pub-id-type="doi">10.1186/s40537-019-0197-0</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Salakhutdinov</surname><given-names>R.</given-names></name></person-group> (<year>2014</year>). <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>. <source>J. Mach. Learn. Res</source>. <volume>15</volume>, <fpage>1929</fpage>–<lpage>1958</lpage>. <pub-id pub-id-type="doi">10.5555/2627435.2670313</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzirakis</surname><given-names>P.</given-names></name><name><surname>Zafeiriou</surname><given-names>S.</given-names></name><name><surname>Schuller</surname><given-names>B. W.</given-names></name></person-group> (<year>2018</year>). <article-title>End2you-the imperial toolkit for multimodal profiling by end-to-end learning</article-title>. <source>arXiv preprint arXiv:1802.01115</source>. <pub-id pub-id-type="doi">10.1145/3423327.3423513</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>van Berkel</surname><given-names>N.</given-names></name><name><surname>Luo</surname><given-names>C.</given-names></name><name><surname>Ferreira</surname><given-names>D.</given-names></name><name><surname>Goncalves</surname><given-names>J.</given-names></name><name><surname>Kostakos</surname><given-names>V.</given-names></name></person-group> (<year>2015</year>). <article-title>The curse of quantified-self: an endless quest for answers</article-title>, in <source>Adjunct Proceedings of the International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the International Symposium on Wearable Computers</source> (<publisher-loc>Osaka</publisher-loc>), <fpage>973</fpage>–<lpage>978</lpage>. <pub-id pub-id-type="doi">10.1145/2800835.2800946</pub-id></mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>T.-J.</given-names></name><name><surname>Chen</surname><given-names>Y.-H.</given-names></name><name><surname>Emer</surname><given-names>J.</given-names></name><name><surname>Sze</surname><given-names>V.</given-names></name></person-group> (<year>2017</year>). <article-title>A method to estimate the energy consumption of deep neural networks</article-title>, in <source>Proceedings of the 51st Asilomar Conference on Signals, Systems, and Computers</source> (<publisher-loc>Pacific Grove, CA</publisher-loc>), <fpage>1916</fpage>–<lpage>1920</lpage>. <pub-id pub-id-type="doi">10.1109/ACSSC.2017.8335698</pub-id><?supplied-pmid 33286680?></mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yun</surname><given-names>S.</given-names></name><name><surname>Han</surname><given-names>D.</given-names></name><name><surname>Oh</surname><given-names>S. J.</given-names></name><name><surname>Chun</surname><given-names>S.</given-names></name><name><surname>Choe</surname><given-names>J.</given-names></name><name><surname>Yoo</surname><given-names>Y.</given-names></name></person-group> (<year>2019</year>). <article-title>CutMix: regularization strategy to train strong classifiers with localizable features</article-title>. <source>arXiv:1905.04899</source>. <pub-id pub-id-type="doi">10.1109/ICCV.2019.00612</pub-id></mixed-citation>
    </ref>
    <ref id="B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeiler</surname><given-names>M. D.</given-names></name></person-group> (<year>2012</year>). <article-title>ADADELTA: an adaptive learning rate method</article-title>. <source>arXiv preprint arXiv:1212.5701</source>.</mixed-citation>
    </ref>
    <ref id="B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name><name><surname>Lu</surname><given-names>C.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Sangaiah</surname><given-names>A. K.</given-names></name></person-group> (<year>2020</year>). <article-title>Lightweight deep network for traffic sign classification</article-title>. <source>Ann. Telecommun</source>. <volume>75</volume>, <fpage>369</fpage>–<lpage>379</lpage>. <pub-id pub-id-type="doi">10.1007/s12243-019-00731-9</pub-id></mixed-citation>
    </ref>
    <ref id="B54">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Z.</given-names></name><name><surname>Zhao</surname><given-names>Y.</given-names></name><name><surname>Bao</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name></person-group> (<year>2018</year>). <article-title>Deep spectrum feature representations for speech emotion recognition</article-title>, in <source>Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and first Multi-Modal Affective Computing of Large-Scale Multimedia Data</source> (<publisher-loc>Seoul</publisher-loc>), <fpage>27</fpage>–<lpage>33</lpage>. <pub-id pub-id-type="doi">10.1145/3267935.3267948</pub-id></mixed-citation>
    </ref>
    <ref id="B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Z.-Q.</given-names></name><name><surname>Zheng</surname><given-names>P.</given-names></name><name><surname>Xu</surname><given-names>S.-t.</given-names></name><name><surname>Wu</surname><given-names>X.</given-names></name></person-group> (<year>2019</year>). <article-title>Object detection with deep learning: a review</article-title>. <source>Trans. Neural Netw. Learn. Syst</source>. <volume>30</volume>, <fpage>3212</fpage>–<lpage>3232</lpage>. <pub-id pub-id-type="doi">10.1109/TNNLS.2018.2876865</pub-id><?supplied-pmid 30703038?><pub-id pub-id-type="pmid">30703038</pub-id></mixed-citation>
    </ref>
    <ref id="B56">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Tian</surname><given-names>Q.</given-names></name></person-group> (<year>2019</year>). <article-title>Accelerate CNN <italic>via</italic> recursive Bayesian pruning</article-title>, in <source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source> (<publisher-loc>Seoul</publisher-loc>), <fpage>3306</fpage>–<lpage>3315</lpage>. <pub-id pub-id-type="doi">10.1109/ICCV.2019.00340</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
