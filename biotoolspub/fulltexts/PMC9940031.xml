<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with OASIS Tables v1.0 20120330//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing-oasis-article1.dtd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName jats-oasis2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Med Imaging (Bellingham)</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Med Imaging (Bellingham)</journal-id>
    <journal-id journal-id-type="coden">JMIOBU</journal-id>
    <journal-id journal-id-type="publisher-id">JMI</journal-id>
    <journal-title-group>
      <journal-title>Journal of Medical Imaging</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2329-4302</issn>
    <issn pub-type="epub">2329-4310</issn>
    <publisher>
      <publisher-name>Society of Photo-Optical Instrumentation Engineers</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9940031</article-id>
    <article-id pub-id-type="doi">10.1117/1.JMI.10.6.061403</article-id>
    <article-id pub-id-type="publisher-manuscript">JMI-22262SSR</article-id>
    <article-id pub-id-type="publisher-id">22262SSR</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Special Section on Informatics and Imaging Data Management</subject>
      </subj-group>
      <subj-group subj-group-type="SPIE-art-type">
        <subject>Paper</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title><italic>medigan</italic>: a Python library of pretrained generative models for medical image synthesis</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1835-8564</contrib-id>
        <name>
          <surname>Osuala</surname>
          <given-names>Richard</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">*</xref>
        <email>richard.osuala@gmail.com</email>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1059-6915</contrib-id>
        <name>
          <surname>Skorupko</surname>
          <given-names>Grzegorz</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
        <email>grzegorz.skorupko@gmail.com</email>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7459-146X</contrib-id>
        <name>
          <surname>Lazrak</surname>
          <given-names>Noussair</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
        <email>lazrak.noussair@gmail.com</email>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-3105-2773</contrib-id>
        <name>
          <surname>Garrucho</surname>
          <given-names>Lidia</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
        <email>lgarrucho@ub.edu</email>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0587-1919</contrib-id>
        <name>
          <surname>García</surname>
          <given-names>Eloy</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">b</xref>
        <email>eloy.garcia@ub.edu</email>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8480-023X</contrib-id>
        <name>
          <surname>Joshi</surname>
          <given-names>Smriti</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
        <email>smriti.joshi@ub.edu</email>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6809-8145</contrib-id>
        <name>
          <surname>Jouide</surname>
          <given-names>Socayna</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
        <email>socayna_jouide@ub.edu</email>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2665-753X</contrib-id>
        <name>
          <surname>Rutherford</surname>
          <given-names>Michael</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">c</xref>
        <email>MWRutherford@uams.edu</email>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6314-5683</contrib-id>
        <name>
          <surname>Prior</surname>
          <given-names>Fred</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">c</xref>
        <email>FWPrior@uams.edu</email>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-7507-5208</contrib-id>
        <name>
          <surname>Kushibar</surname>
          <given-names>Kaisar</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
        <email>kaisar.kushibar@ub.edu</email>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6789-5177</contrib-id>
        <name>
          <surname>Díaz</surname>
          <given-names>Oliver</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
        <email>o.diaz.montesdeoca@gmail.com</email>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9456-1612</contrib-id>
        <name>
          <surname>Lekadir</surname>
          <given-names>Karim</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
        <email>karim.lekadir@ub.edu</email>
      </contrib>
      <aff id="aff1"><label>a</label><institution>Universitat de Barcelona</institution>, Barcelona Artificial Intelligence in Medicine Lab (BCN-AIM), Facultat de Matemàtiques i Informàtica, Barcelona, <country>Spain</country></aff>
      <aff id="aff2"><label>b</label><institution>Universitat de Barcelona</institution>, Facultat de Matemàtiques i Informàtica, Barcelona, <country>Spain</country></aff>
      <aff id="aff3"><label>c</label><institution>University of Arkansas for Medical Sciences</institution>, Department of Biomedical Informatics, Little Rock, Arkansas, <country>United States</country></aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>*</label>Address all correspondence to Richard Osuala, <email>Richard.Osuala@ub.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>11</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>20</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <!--PMC Release delay is 12 months and 0 days and was based on the <pub-date pub-type="epub"/>.-->
    <volume>10</volume>
    <issue>6</issue>
    <elocation-id>061403</elocation-id>
    <history>
      <date date-type="received">
        <day>5</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>1</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 The Authors</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>The Authors</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Published by SPIE under a Creative Commons Attribution 4.0 International License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.</license-p>
      </license>
    </permissions>
    <self-uri xlink:title="pdf" xlink:href="JMI_10_6_061403.pdf"/>
    <abstract>
      <title>Abstract.</title>
      <sec>
        <title>Purpose</title>
        <p>Deep learning has shown great promise as the backbone of clinical decision support systems. Synthetic data generated by generative models can enhance the performance and capabilities of data-hungry deep learning models. However, there is (1) limited availability of (synthetic) datasets and (2) generative models are complex to train, which hinders their adoption in research and clinical applications. To reduce this entry barrier, we explore generative model sharing to allow more researchers to access, generate, and benefit from synthetic data.</p>
      </sec>
      <sec>
        <title>Approach</title>
        <p>We propose <italic>medigan</italic>, a one-stop shop for pretrained generative models implemented as an open-source framework-agnostic Python library. After gathering end-user requirements, design decisions based on usability, technical feasibility, and scalability are formulated. Subsequently, we implement <italic>medigan</italic> based on modular components for generative model (i) execution, (ii) visualization, (iii) search &amp; ranking, and (iv) contribution. We integrate pretrained models with applications across modalities such as mammography, endoscopy, x-ray, and MRI.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>The scalability and design of the library are demonstrated by its growing number of integrated and readily-usable pretrained generative models, which include 21 models utilizing nine different generative adversarial network architectures trained on 11 different datasets. We further analyze three <italic>medigan</italic> applications, which include (a) enabling community-wide sharing of restricted data, (b) investigating generative model evaluation metrics, and (c) improving clinical downstream tasks. In (b), we extract Fréchet inception distances (FID) demonstrating FID variability based on image normalization and radiology-specific feature extractors.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p><italic>medigan</italic> allows researchers and developers to create, increase, and domain-adapt their training data in just a few lines of code. Capable of enriching and accelerating the development of clinical machine learning models, we show <italic>medigan</italic>’s viability as platform for generative model sharing. Our multimodel synthetic data experiments uncover standards for assessing and reporting metrics, such as FID, in image synthesis studies.</p>
      </sec>
    </abstract>
    <kwd-group>
      <title>Keywords:</title>
      <kwd>synthetic data</kwd>
      <kwd>generative adversarial networks</kwd>
      <kwd>Python</kwd>
      <kwd>image synthesis</kwd>
      <kwd>deep learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="sp1">
        <funding-source>European Union’s Horizon 2020 Research and Innovation Programme Under Grant Agreement</funding-source>
        <award-id>No 952103</award-id>
        <award-id>No 101057699</award-id>
      </award-group>
      <award-group id="sp2">
        <funding-source>Ministry of Science and Innovation of Spain with Reference Numbers</funding-source>
        <award-id>FJC2019-040039-I</award-id>
        <award-id>FJC2021-047659-I</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="9"/>
      <table-count count="11"/>
      <ref-count count="108"/>
      <page-count count="28"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>running-head</meta-name>
        <meta-value>Osuala et al.: <italic>medigan</italic>: a Python library of pretrained generative models for enriched data access…</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <sec id="sec1.1">
      <label>1.1</label>
      <title>Deep Learning and the Benefits of Synthetic Data</title>
      <p>The use of deep learning has increased extensively in the last decade, thanks in part to advances in computing technology (e.g., data storage, graphics processing units) and the digitization of data. In medical imaging, deep learning algorithms have shown promising potential for clinical use due to their capability of extracting and learning meaningful patterns from imaging data and their high performance on clinically-relevant tasks. These include image-based disease diagnosis<xref rid="r1" ref-type="bibr"><sup>1</sup></xref><sup>,</sup><xref rid="r2" ref-type="bibr"><sup>2</sup></xref> and detection,<xref rid="r3" ref-type="bibr"><sup>3</sup></xref> as well as medical image reconstruction,<xref rid="r4" ref-type="bibr"><sup>4</sup></xref><sup>,</sup><xref rid="r5" ref-type="bibr"><sup>5</sup></xref> segmentation,<xref rid="r6" ref-type="bibr"><sup>6</sup></xref> and image-based treatment planning.<xref rid="r7" ref-type="bibr"><sup>7</sup></xref><named-content content-type="online"><xref rid="r8" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r9" ref-type="bibr"><sup>9</sup></xref></p>
      <p>However, deep learning models need vast amounts of well-annotated data to reliably learn to perform clinical tasks, whereas, at the same time, the availability of public medical imaging datasets remains limited due to legal, ethical, and technical patient data sharing constraints.<xref rid="r9" ref-type="bibr"><sup>9</sup></xref><sup>,</sup><xref rid="r10" ref-type="bibr"><sup>10</sup></xref> In the common scenario of limited imaging data, synthetic images, such as the ones illustrated in <xref rid="f1" ref-type="fig">Fig. 1</xref>, are a useful tool to improve the learning of the artificial intelligence (AI) algorithm, e.g., by enlarging its training dataset.<xref rid="r7" ref-type="bibr"><sup>7</sup></xref><sup>,</sup><xref rid="r11" ref-type="bibr"><sup>11</sup></xref><sup>,</sup><xref rid="r12" ref-type="bibr"><sup>12</sup></xref> Furthermore, synthetic data can be used to minimize problems associated with domain shift, data scarcity, class imbalance, and data privacy.<xref rid="r7" ref-type="bibr"><sup>7</sup></xref> For instance, a dataset can be balanced by populating the less frequent classes with synthetic data during training (class imbalance). Further, as domain-adaptation technique, a dataset can be translated from one domain to another, e.g., from MRI to CT<xref rid="r13" ref-type="bibr"><sup>13</sup></xref> (domain shift). Regarding data privacy, synthetic data can be shared instead of real patient data to improve privacy preservation.<xref rid="r7" ref-type="bibr"><sup>7</sup></xref><sup>,</sup><xref rid="r14" ref-type="bibr"><sup>14</sup></xref><sup>,</sup><xref rid="r15" ref-type="bibr"><sup>15</sup></xref></p>
      <fig position="float" id="f1">
        <label>Fig. 1</label>
        <caption>
          <p>Randomly sampled images generated by five <italic>medigan</italic> models ranging from (a) synthetic mammograms and (b) brain MRI to (c) endoscopy imaging of polyps, (d) mammogram mass patches, and (e) chest x-ray imaging. The models (a)–(e) correspond to the model IDs in <xref rid="t004" ref-type="table">Table 3</xref>, where (a) 3, (b) 7, (c) 10, (d) 12, and (e) 19.</p>
        </caption>
        <graphic xlink:href="JMI-010-061403-g001" position="float"/>
      </fig>
    </sec>
    <sec id="sec1.2">
      <label>1.2</label>
      <title>The Need of Reusable Synthetic Data Generators</title>
      <p>Commonly, generative models are used to produce synthetic imaging data, with generative adversarial networks (GANs)<xref rid="r16" ref-type="bibr"><sup>16</sup></xref> being popular models of choice. However, the adversarial training scheme required by GANs and related networks is known to pose challenges in regard to (i) achieving training stability, (ii) avoiding mode collapse, and (iii) reaching convergence.<xref rid="r17" ref-type="bibr"><sup>17</sup></xref><named-content content-type="online"><xref rid="r18" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r19" ref-type="bibr"><sup>19</sup></xref></p>
      <p>Hence, the training process of GANs and generative models at large is nontrivial and requires a considerable time investment for each training iteration as well as specific hardware and a fair amount of knowledge and skills in the area of AI and generative modeling. Given these constraints, researchers and engineers often refrain from generating and integrating synthetic data into their AI training pipelines and experiments. This issue is further exacerbated by the prevailing need of training a new generative model for each new data distribution, which, in practice, often means that a new generative model has to be trained for each new application, use-case, and dataset.</p>
    </sec>
    <sec id="sec1.3">
      <label>1.3</label>
      <title>Community-Driven Model Sharing and Reuse</title>
      <p>We argue that a feasible solution to this problem is the community-wide sharing and reuse of pretrained generative models. Once successfully trained, such a model can be of value to multiple researchers and engineers with similar needs. For example, researchers can reuse the same model if they work on the same problem, conduct similar experiments, or evaluate their methods on the same dataset. We note that such reusing ideally is subject to previous inspection of generative model limitations with the model’s output quality having qualified as suitable for the task at hand. The quality of a model’s output data and annotations can commonly be measured via (a) expert assessment, (b) computation of image quality metrics, or (c) downstream task evaluation. In sum, the problem of synthetic data generation calls for a community-driven solution, where a generative model trained by one member of the community can be reused by other members of the community. Motivated by the absence of such a community-driven solution for synthetic medical data generation, we designed and developed <italic>medigan</italic> to bridge the gap between the need for synthetic data and complex generative model creation and training processes.</p>
    </sec>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Background and Related Work</title>
    <sec id="sec2.1">
      <label>2.1</label>
      <title>Generative Models</title>
      <p>While discriminative models are able to distinguish between data instances of different kinds (label samples), generative models are able to generate new data instances (draw samples). In contrast to modeling decision boundaries in a data space, generative models model how data is distributed within that space. Deep generative models<xref rid="r20" ref-type="bibr"><sup>20</sup></xref> are composed of multihidden layer neural networks to explicitly or implicitly estimate a probability density function (PDF) from a set of real data samples. After approximating the PDF from observed data points (i.e., learning the real data distribution), these models can then sample unobserved new data points from that distribution. In computer vision and medical imaging, synthetic images are generated by sampling such unobserved points from high-dimensional imaging data distributions. Popular deep generative models to create synthetic images in these fields include variational autoencoders,<xref rid="r21" ref-type="bibr"><sup>21</sup></xref> normalizing flows,<xref rid="r22" ref-type="bibr"><sup>22</sup></xref><named-content content-type="online"><xref rid="r23" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r24" ref-type="bibr"><sup>24</sup></xref> diffusion models,<xref rid="r25" ref-type="bibr"><sup>25</sup></xref><named-content content-type="online"><xref rid="r26" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r27" ref-type="bibr"><sup>27</sup></xref> and GANs.<xref rid="r16" ref-type="bibr"><sup>16</sup></xref> From these, the versatile GAN framework has seen the most widespread adoption in medical imaging to date.<xref rid="r7" ref-type="bibr"><sup>7</sup></xref> We, hence, center our attention on GANs in the remainder of this work but emphasize that contributions of other types of generative models are equally welcome in the <italic>medigan</italic> library.</p>
    </sec>
    <sec id="sec2.2">
      <label>2.2</label>
      <title>Generative Adversarial Networks</title>
      <p>The training of GANs comprises two neural networks, the generator network (<inline-formula><mml:math id="math1" display="inline" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula>) and the discriminator network (<inline-formula><mml:math id="math2" display="inline" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>), as illustrated by <xref rid="f2" ref-type="fig">Fig. 2</xref> for the example of mammography region-of-interest patch generation. <inline-formula><mml:math id="math3" display="inline" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math4" display="inline" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula> compete against each other in a two-player zero-sum game defined by the value function shown in Eq. (1). Subsequent studies extended the adversarial learning scheme by proposing innovations of the loss function, <inline-formula><mml:math id="math5" display="inline" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math6" display="inline" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula> network architectures, and GAN applications by introducing conditions into the image generation process <disp-formula id="e001"><mml:math id="math7" display="block" overflow="scroll"><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mtext> </mml:mtext><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math><label>(1)</label></disp-formula></p>
      <fig position="float" id="f2">
        <label>Fig. 2</label>
        <caption>
          <p>The GAN framework. In this visual example, the generator network receives random noise vectors, which it learns to map to region-of-interest patches of full-field digital mammograms. During training, the adversarial loss is not only backpropagated to the discriminator as <inline-formula><mml:math id="math8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> but also to the generator as <inline-formula><mml:math id="math9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. This particular architecture and loss function was used to train <italic>medigan</italic> models listed with IDs 1, 2, and 5 in <xref rid="t004" ref-type="table">Table 3</xref>.</p>
        </caption>
        <graphic xlink:href="JMI-010-061403-g002" position="float"/>
      </fig>
      <sec id="sec2.2.1">
        <label>2.2.1</label>
        <title>
          <italic>GAN</italic>
          <italic>loss functions</italic>
        </title>
        <p>Goodfellow et al.<xref rid="r16" ref-type="bibr"><sup>16</sup></xref> define the discriminator as a binary classifier classifying whether a sample <inline-formula><mml:math id="math10" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> is either real or generated. The discriminator is, hence, trained via binary-cross entropy with the objective of minimizing the adversarial loss function shown in Eq. (2), which the generator, on the other hand, tries to maximize. In Wasserstein GAN (WGAN),<xref rid="r28" ref-type="bibr"><sup>28</sup></xref> the adversarial loss function is replaced with a loss function based on the Wasserstein-1 distance between real and fake sample distributions estimated by D (alias “critic”). Gulrajani et al.<xref rid="r29" ref-type="bibr"><sup>29</sup></xref> resolve the need to enforce a 1-Lipschitz constraint in WGAN via gradient penalty (WGAN-GP) instead of WGAN weight clipping. Equation (3) depicts the WGAN-GP discriminator loss with penalty coefficient <inline-formula><mml:math id="math11" display="inline" overflow="scroll"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula> and distribution <inline-formula><mml:math id="math12" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">P</mml:mi><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:msub></mml:mrow></mml:math></inline-formula> based on sampled pairs from (a) the real data distribution <inline-formula><mml:math id="math13" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and (b) the generated data distribution <inline-formula><mml:math id="math14" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">P</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>
<disp-formula id="e002"><mml:math id="math15" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>GAN</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mtext> </mml:mtext><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math><label>(2)</label></disp-formula><disp-formula id="e003"><mml:math id="math16" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>WGAN</mml:mi><mml:mtext>-</mml:mtext><mml:mi>GP</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">˜</mml:mo></mml:mrow></mml:mover><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">˜</mml:mo></mml:mrow></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mrow></mml:mover><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mo>∇</mml:mo></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:msub><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mrow></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math><label>(3)</label></disp-formula>In addition to changes to the adversarial loss, further studies integrate additional loss terms into the GAN framework. For instance, FastGAN<xref rid="r30" ref-type="bibr"><sup>30</sup></xref> uses an additional reconstruction loss in the discriminator, which, for improved regularisation, is trained as self-supervised feature-encoder.</p>
      </sec>
      <sec id="sec2.2.2">
        <label>2.2.2</label>
        <title>GAN network architectures and conditions</title>
        <p>A plethora of different GAN network architectures has been proposed<xref rid="r7" ref-type="bibr"><sup>7</sup></xref><sup>,</sup><xref rid="r31" ref-type="bibr"><sup>31</sup></xref> starting with a deep convolutional GAN (DCGAN)<xref rid="r32" ref-type="bibr"><sup>32</sup></xref> neural network architecture of both D and G. Later approaches, e.g., include a ResNet-based architecture as backbone<xref rid="r29" ref-type="bibr"><sup>29</sup></xref> and progressively-grow the generator and discriminator networks during training to enable high-resolution image synthesis (PGGAN).<xref rid="r33" ref-type="bibr"><sup>33</sup></xref></p>
        <p>Another line of research has been focusing on conditioning the output of GANs based on discrete or continuous labels. For example, in cGAN this is achieved by feeding a label to both D and G,<xref rid="r34" ref-type="bibr"><sup>34</sup></xref> whereas in the auxiliary classifier GAN (AC-GAN), the discriminator additionally predicts the label that is provided to the generator.<xref rid="r35" ref-type="bibr"><sup>35</sup></xref></p>
        <p>Other models condition the generation process on input images<xref rid="r36" ref-type="bibr"><sup>36</sup></xref><named-content content-type="online"><xref rid="r37" ref-type="bibr"/><xref rid="r38" ref-type="bibr"/><xref rid="r39" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r40" ref-type="bibr"><sup>40</sup></xref> unlocking image-to-image translation and domain-adaptation GAN applications. A key difference in image-to-image translation methodology is the presence (paired translation) or absence (unpaired translation) of corresponding image pairs in the target and source domain. Using an L1 reconstruction loss between target and source domain alongside the adversarial loss from Eq. (2), pix2pix<xref rid="r36" ref-type="bibr"><sup>36</sup></xref> defines a common baseline model for paired image-to-image translation. For unpaired translation, cycleGAN<xref rid="r37" ref-type="bibr"><sup>37</sup></xref> is a popular approach, which also consists of an L1 reconstruction (cycle-consistency) loss between a source (target) image and a source (target) image translated to target (source) and back to source (target) via two consecutive generators.</p>
        <p>A further methodological innovation includes SinGAN,<xref rid="r41" ref-type="bibr"><sup>41</sup></xref> which, based on only a single training image, learns to generate multiple synthetic images. This is accomplished via a multi-scale coarse-to-fine pipeline of generators, where a sample is passed sequentially through all generators, each of which also receives a random noise vector as input.</p>
      </sec>
    </sec>
    <sec id="sec2.3">
      <label>2.3</label>
      <title>Generative Model Evaluation</title>
      <p>One approach of evaluating generative models is by human expert assessment of their generated synthetic data. In medical imaging, such observer studies often enlist board-certified clinical experts such as radiologists or pathologists to examine the quality and/or realism of the synthetic medical images.<xref rid="r42" ref-type="bibr"><sup>42</sup></xref><sup>,</sup><xref rid="r43" ref-type="bibr"><sup>43</sup></xref> However, this approach is manual, laborious and costly, and, hence, research attention has been devoted to automating generative model evaluation,<xref rid="r44" ref-type="bibr"><sup>44</sup></xref><sup>,</sup><xref rid="r45" ref-type="bibr"><sup>45</sup></xref> including:</p>
      <list list-type="simple">
        <list-item>
          <label>i.</label>
          <p>Metrics for automated analysis of the synthetic data and its distribution, such as the inception score (IS)<xref rid="r17" ref-type="bibr"><sup>17</sup></xref> and Fréchet inception distance (FID).<xref rid="r46" ref-type="bibr"><sup>46</sup></xref> Both metrics are popular in computer vision,<xref rid="r31" ref-type="bibr"><sup>31</sup></xref> whereas the latter also has seen widespread adoption in medical imaging.<xref rid="r7" ref-type="bibr"><sup>7</sup></xref></p>
        </list-item>
      </list>
      <p>FID is based on a pretrained Inception<xref rid="r47" ref-type="bibr"><sup>47</sup></xref> model (e.g., v1,<xref rid="r48" ref-type="bibr"><sup>48</sup></xref> v3<xref rid="r47" ref-type="bibr"><sup>47</sup></xref>) to extract features from synthetic and real datasets, which are then fitted to multivariate Gaussians <inline-formula><mml:math id="math17" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> (e.g., real) and <inline-formula><mml:math id="math18" display="inline" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> (e.g., synthetic) with means <inline-formula><mml:math id="math19" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math20" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and covariance matrices <inline-formula><mml:math id="math21" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math22" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>Y</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Next, <inline-formula><mml:math id="math23" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math24" display="inline" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:math></inline-formula> are compared via the Wasserstein-2 (Fréchet) distance (FD), as depicted as <disp-formula id="e004"><mml:math id="math25" display="block" overflow="scroll"><mml:mrow><mml:mi>FD</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>tr</mml:mi><mml:mo minsize="3ex">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mo minsize="3ex">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo minsize="3ex">)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo minsize="3ex">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math><label>(4)</label></disp-formula></p>
      <list list-type="simple">
        <list-item>
          <label>ii.</label>
          <p>Metrics that compare a synthetic image with a real reference image such as mean squared error (MSE), peak signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM).<xref rid="r49" ref-type="bibr"><sup>49</sup></xref> Given the absence of corresponding reference images, such metrics are not readily applicable for unconditional noise-to-image generation models.</p>
        </list-item>
        <list-item>
          <label>iii.</label>
          <p>Metrics that compare the performance of a model on a surrogate downstream task with and without generative model intervention.<xref rid="r7" ref-type="bibr"><sup>7</sup></xref><sup>,</sup><xref rid="r14" ref-type="bibr"><sup>14</sup></xref><sup>,</sup><xref rid="r50" ref-type="bibr"><sup>50</sup></xref><sup>,</sup><xref rid="r51" ref-type="bibr"><sup>51</sup></xref> For instance, training on additional synthetic data can increase a model’s downstream task performance, thus, demonstrating the usefulness of the generative model that generated such data.</p>
        </list-item>
      </list>
      <p>For the analysis of generative models in the present study, we discard (ii) due to its limitation of requiring specific reference images. We further deprioritize the IS from (i) due to its limited applicability to medical imagery stemming from it missing a comparison between real and synthetic data distributions combined with it having a strong bias on natural images via its ImageNet<xref rid="r52" ref-type="bibr"><sup>52</sup></xref>-pretrained Inception classifier as backbone feature extractor. Therefore, we focus on FID from (i) and downstream task performance (iii) as potential evaluation measures for medical image synthesis models in the remainder of this work.</p>
    </sec>
    <sec id="sec2.4">
      <label>2.4</label>
      <title>Image Synthesis Tools and Libraries</title>
      <p>Related libraries, such as pygan,<xref rid="r53" ref-type="bibr"><sup>53</sup></xref> torchGAN,<xref rid="r54" ref-type="bibr"><sup>54</sup></xref> vegans,<xref rid="r55" ref-type="bibr"><sup>55</sup></xref> imaginaire,<xref rid="r56" ref-type="bibr"><sup>56</sup></xref> TF-GAN,<xref rid="r57" ref-type="bibr"><sup>57</sup></xref> PyTorch-GAN,<xref rid="r58" ref-type="bibr"><sup>58</sup></xref> keras-GAN,<xref rid="r59" ref-type="bibr"><sup>59</sup></xref> mimicry,<xref rid="r60" ref-type="bibr"><sup>60</sup></xref> and studioGAN,<xref rid="r31" ref-type="bibr"><sup>31</sup></xref> have focused on facilitating the implementation, training, and comparative evaluation of GANs in computer vision (CV). Despite a strong focus on language models, the HuggingFace transformers library and model hub<xref rid="r61" ref-type="bibr"><sup>61</sup></xref> also contain a few pretrained computer vision GAN models. The GAN Lab<xref rid="r62" ref-type="bibr"><sup>62</sup></xref> provides an interactive visual experimentation tool to examine the training process and its data flows in GANs.</p>
      <p>Specific to AI in medical imaging, Diaz et al.<xref rid="r63" ref-type="bibr"><sup>63</sup></xref> provided a comprehensive survey of tools, libraries and platforms for privacy preservation, data curation, medical image storage, annotation, and repositories. Compared to CV, fewer GAN and AI libraries and tools exist in medical imaging. Furthermore, CV libraries are not always suited to address the unique challenges of medical imaging data.<xref rid="r63" ref-type="bibr"><sup>63</sup></xref><named-content content-type="online"><xref rid="r64" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r65" ref-type="bibr"><sup>65</sup></xref> For instance, pretrained generative models from computer vision cannot be readily adapted to produce medical imaging-specific outputs. The TorchIO library<xref rid="r64" ref-type="bibr"><sup>64</sup></xref> addresses the gap between CV and medical image data processing requirements providing functions for efficient loading, augmentation, preprocessing, and patch-based sampling of medical imagery. The medical open network for AI (MONAI)<xref rid="r66" ref-type="bibr"><sup>66</sup></xref> is a PyTorch-based<xref rid="r67" ref-type="bibr"><sup>67</sup></xref> framework that facilitates the development of diagnostic AI models with tutorials for classification, segmentation, and AI model deployment. Further efforts in this realm include NiftyNet,<xref rid="r68" ref-type="bibr"><sup>68</sup></xref> the deep learning tool kit (DLTK),<xref rid="r69" ref-type="bibr"><sup>69</sup></xref> MedicalZooPytorch,<xref rid="r70" ref-type="bibr"><sup>70</sup></xref> and nnDetection.<xref rid="r71" ref-type="bibr"><sup>71</sup></xref> The recent RadImageNet initiative<xref rid="r72" ref-type="bibr"><sup>72</sup></xref> shares baseline image classification models pretrained on a dataset designed as the radiology medical imaging equivalent to ImageNet.<xref rid="r52" ref-type="bibr"><sup>52</sup></xref></p>
      <p>To the best of our knowledge, no open-access software, tool, or library exists that targets reuse and sharing of pretrained generative models in medical imaging. To this end, we expect the contribution of our <italic>medigan</italic> library to be instrumental in enabling dissemination of generative models and increased adoption of synthetic data into AI training pipelines. As an open-access plug-and-play solution for generation of multipurpose synthetic data, <italic>medigan</italic> aims to benefit patients and clinicians by enhancing the performance and robustness of AI-based clinical decision support systems.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Method: The <italic>medigan</italic> Library</title>
    <p>We contribute <italic>medigan</italic> as an open-source open-access MIT-licensed Python3 library distributed via the Python package index (Pypi) for synthetic medical dataset generation, e.g., via pretrained generative models. The metadata of <italic>medigan</italic> is summarized in <xref rid="t001" ref-type="table">Table 1</xref>. <italic>medigan</italic> accelerates research in medical imaging by flexibly providing (a) synthetic data augmentation and (b) preprocessing functionality, both readily integrable in machine learning training pipelines. It also allows contributors to add their generative models in a thought-through process and provides simplistic functions for end-users to search for, rank, and visualize models. The overview of <italic>medigan</italic> in <xref rid="f3" ref-type="fig">Fig. 3</xref> depicts the core functions demonstrating how end-users can (a) contribute a generative model, (b) find a suitable generative model inside the library, and (c) generate synthetic data with that model.</p>
    <table-wrap position="float" id="t001">
      <label>Table 1</label>
      <caption>
        <p>Overview of <italic>medigan</italic> library information.</p>
      </caption>
      <!--OASIS TABLE HERE-->
      <table frame="hsides" rules="groups">
        <colgroup>
          <col/>
          <col align="left"/>
          <col align="left"/>
        </colgroup>
        <thead>
          <tr>
            <th valign="top"> </th>
            <th align="left" valign="top">Title</th>
            <th align="left" valign="top"><italic>medigan</italic> metadata</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td> 1</td>
            <td align="left">Code version</td>
            <td align="left">v1.0.0</td>
          </tr>
          <tr>
            <td> 2</td>
            <td align="left">Code license</td>
            <td align="left">MIT</td>
          </tr>
          <tr>
            <td> 3</td>
            <td align="left">Code version control system</td>
            <td align="left">Git</td>
          </tr>
          <tr>
            <td> 4</td>
            <td align="left">Software languages</td>
            <td align="left">Python</td>
          </tr>
          <tr>
            <td> 5</td>
            <td align="left">Code repository</td>
            <td align="left"><ext-link xlink:href="https://github.com/RichardObi/medigan" ext-link-type="uri">https://github.com/RichardObi/medigan</ext-link>.</td>
          </tr>
          <tr>
            <td> 6</td>
            <td align="left">Software package repository</td>
            <td align="left">Ref. <xref rid="r73" ref-type="bibr">73</xref></td>
          </tr>
          <tr>
            <td> 7</td>
            <td align="left">Developer documentation</td>
            <td align="left">Ref. <xref rid="r74" ref-type="bibr">74</xref></td>
          </tr>
          <tr>
            <td> 8</td>
            <td align="left">Tutorial</td>
            <td align="left">medigan quickstart (tutorial.ipynb)</td>
          </tr>
          <tr>
            <td> 9</td>
            <td align="left">Requirements for compilation</td>
            <td align="left">Python v3.6+</td>
          </tr>
          <tr>
            <td>10</td>
            <td align="left">Operating system</td>
            <td align="left">OS independent. Tested on Linux, OSX, Windows.</td>
          </tr>
          <tr>
            <td>11</td>
            <td align="left">Support email address</td>
            <td align="left">Richard.Osuala[at]gmail.com</td>
          </tr>
          <tr>
            <td>12</td>
            <td align="left">Dependencies</td>
            <td align="left">tqdm, requests, torch, numpy, PyGithub, matplotlib (setup.py)</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <fig position="float" id="f3">
      <label>Fig. 3</label>
      <caption>
        <p>Architectural overview of <italic>medigan</italic>. Users interact with the library by contributing, searching, and executing generative models, the latter shown here exemplified for mammography image generation with models with IDs 1 to 4 described in <xref rid="t004" ref-type="table">Table 3</xref>.</p>
      </caption>
      <graphic xlink:href="JMI-010-061403-g003" position="float"/>
    </fig>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>User Requirements and Design Decisions</title>
      <p>End-user requirement gathering is recommended for the development of trustworthy AI solutions in medical imaging.<xref rid="r75" ref-type="bibr"><sup>75</sup></xref> Therefore, we organized requirement gathering sessions with potential end-users, model contributors, and stakeholders from the EuCanImage Consortium, a large European H2020 project<xref rid="r76" ref-type="bibr"><sup>76</sup></xref> building a cancer imaging platform for enhanced AI in oncology. Upon exploring the needs and preferences of medical imaging researchers and AI developers, respective requirements for the design of <italic>medigan</italic> were formulated to ensure usability and usefulness. For instance, the users articulated a clear preference for a user interface in the format of an importable package as opposed to a graphical user interface (GUI), web application, database system, or API. <xref rid="t002" ref-type="table">Table 2</xref> summarizes key requirements and the corresponding design decisions.</p>
      <table-wrap position="float" id="t002">
        <label>Table 2</label>
        <caption>
          <p>Overview of the key requirements gathered together with potential end-user alongside the respective design decisions taken toward fulfilling these requirements with <italic>medigan</italic>.</p>
        </caption>
        <!--OASIS TABLE HERE-->
        <table frame="hsides" rules="groups">
          <colgroup>
            <col/>
            <col align="left"/>
            <col align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th valign="top">No</th>
              <th align="left" valign="top">End-user requirement</th>
              <th align="left" valign="top">Respective design decision</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td> 1</td>
              <td align="left">Instead of a GUI tool, <italic>medigan</italic> should be implemented as a platform-independent library importable into users’ code.</td>
              <td align="left">Implementation of <italic>medigan</italic> as publicly accessible Python package distributed via PyPI.</td>
            </tr>
            <tr>
              <td> 2</td>
              <td align="left">It should support common frameworks for building generative models, e.g., PyTorch,<xref rid="r67" ref-type="bibr"><sup>67</sup></xref> TensorFlow,<xref rid="r77" ref-type="bibr"><sup>77</sup></xref> Keras.<xref rid="r78" ref-type="bibr"><sup>78</sup></xref></td>
              <td align="left"><italic>medigan</italic> is built framework-agnostic treating each model as separate Python package with freedom of choice of framework and dependencies.</td>
            </tr>
            <tr>
              <td> 3</td>
              <td align="left">The library should allow different types of generative models and generation processes.</td>
              <td align="left"><italic>medigan</italic> supports any type of data generation model including GANs,<xref rid="r16" ref-type="bibr"><sup>16</sup></xref> VAEs,<xref rid="r21" ref-type="bibr"><sup>21</sup></xref> flow-based,<xref rid="r22" ref-type="bibr"><sup>22</sup></xref><named-content content-type="online"><xref rid="r23" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r24" ref-type="bibr"><sup>24</sup></xref> diffusion,<xref rid="r25" ref-type="bibr"><sup>25</sup></xref><named-content content-type="online"><xref rid="r26" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r27" ref-type="bibr"><sup>27</sup></xref> and nondeep learning models.</td>
            </tr>
            <tr>
              <td> 4</td>
              <td align="left">The library should support different types of synthetic data.</td>
              <td align="left"><italic>medigan</italic> supports any type of synthetic data ranging from 2D and 3D images to image pairs, masks, and tabular data.</td>
            </tr>
            <tr>
              <td> 5</td>
              <td align="left">Sample generation functions should be easily integrable into diverse user code, pipelines, and workflows.</td>
              <td align="left"><italic>medigan</italic>’s generate function can (i) return samples, (ii) generate folders with samples, or (iii) return a model’s generate function as callable.</td>
            </tr>
            <tr>
              <td> 6</td>
              <td align="left">User should be able to integrate <italic>medigan</italic> data in AI training via a dataloader.</td>
              <td align="left">For each model, <italic>medigan</italic> supports returning a torch dataloader readily integrable in AI training pipelines, combinable with other dataloaders.</td>
            </tr>
            <tr>
              <td> 7</td>
              <td align="left">Despite using large deep learning models, the library should be as lightweight as possible.</td>
              <td align="left">Only the user-requested models are downloaded and locally imported. Thus, model dependencies are not part of <italic>medigan</italic>’s dependencies.</td>
            </tr>
            <tr>
              <td> 8</td>
              <td align="left">It should be possible to locally review and adjust a generative model of the library.</td>
              <td align="left">After download, a model’s code and config are available for end-users to explore and adjust. <italic>medigan</italic> can also load models from local file systems.</td>
            </tr>
            <tr>
              <td> 9</td>
              <td align="left">The library should support both CPU and GPU usage depending on a user’s hardware.</td>
              <td align="left">Contributed <italic>medigan</italic> models are reviewed and, if need be, enhanced to run on both GPU and CPU.</td>
            </tr>
            <tr>
              <td>10</td>
              <td align="left">Version and source of the models that the library load should be transparent to the end-user.</td>
              <td align="left">Convention of storing <italic>medigan</italic> models on Zenodo, where each model’s source code and version history is available.</td>
            </tr>
            <tr>
              <td>11</td>
              <td align="left">There should be no need to update the version of the <italic>medigan</italic> package each time a new model is contributed.</td>
              <td align="left"><italic>medigan</italic> is designed independently of its model packages separately stored on Zenodo. Config updates do not require new <italic>medigan</italic> versions.</td>
            </tr>
            <tr>
              <td>12</td>
              <td align="left">Following,<xref rid="r75" ref-type="bibr"><sup>75</sup></xref> models are contributed in transparently and traceably, allowing quality and reproducibility checks.</td>
              <td align="left">Model contribution is traceable via version control. Adding models to <italic>medigan</italic> requires a config change via pull request.</td>
            </tr>
            <tr>
              <td>13</td>
              <td align="left">The risk that the library downloads models that contain malicious code should be minimized.</td>
              <td align="left">Zenodo model uploads receive static DOIs. After verification, unsolicited uploads/changes do not affect <italic>medigan</italic>, which points to specific DOI.</td>
            </tr>
            <tr>
              <td>14</td>
              <td align="left">License and authorship of generative model contributors should be clearly stated and acknowledged.</td>
              <td align="left">Separation of models and library allows freedom of choice of model license and transparent authorship reported for each model.</td>
            </tr>
            <tr>
              <td>15</td>
              <td align="left">Each generative model in the library should be documented.</td>
              <td align="left">Each available model is listed and described in <italic>medigan</italic>’s documentation, in the readme, and also separately in its Zenodo entry.</td>
            </tr>
            <tr>
              <td>16</td>
              <td align="left">The library should have minimal dependencies on the user side and should run on common end-user systems.</td>
              <td align="left"><italic>medigan</italic> has a minimal set of Python dependencies, is OS-independent, and avoids system and third-party dependencies.</td>
            </tr>
            <tr>
              <td>17</td>
              <td align="left">Contributing models should be simple and at least partially automated.</td>
              <td align="left"><italic>medigan</italic>’s contribution workflow automates local model configuration, testing, packaging, Zenodo upload, and issue creation on GitHub.</td>
            </tr>
            <tr>
              <td>18</td>
              <td align="left">If different models have the same dependency but with different versions, this should not cause a conflict.</td>
              <td align="left">Model dependency versions are specified in the config. <italic>medigan</italic>’s generate method can install unsatisfied dependencies, avoiding conflicts.</td>
            </tr>
            <tr>
              <td>19</td>
              <td align="left">Any model in the library should be automatically tested and results reported to make sure all models work as designed.</td>
              <td align="left">On each commit to main, a CI pipeline automatically builds, formats, and lints <italic>medigan</italic> before testing all models and core functions.</td>
            </tr>
            <tr>
              <td>20</td>
              <td align="left">The library should make the results of the models visible with minimal code required by end-users.</td>
              <td align="left"><italic>medigan</italic>’s simple visualization feature allows users to adjust a model’s input latent vector for intuitive exploration of output diversity and fidelity.</td>
            </tr>
            <tr>
              <td>21</td>
              <td align="left">The library should support large synthetic dataset generation on user machines with limited random-access memory.</td>
              <td align="left">For large synthetic dataset generation, <italic>medigan</italic> iteratively generates samples via small batches to avoid exceeding users’ in-memory storage limits.</td>
            </tr>
            <tr>
              <td>22</td>
              <td align="left">Users can specify model weights, model inputs, number, and storage location of the synthetic samples.</td>
              <td align="left">Diverging from defaults, users can specify (i) weights, (ii) number of samples (iii) return or store, (iv) store location, (v) optional inputs.</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>Software Design and Architecture</title>
      <p><italic>medigan</italic> is built with a focus on simplicity and usability. The integration of pretrained models is designed as internal Python package import and offers simultaneously (a) high flexibility to and (b) low code dependency on these generative models. The latter allows the reuse of the same orchestration functions in <italic>medigan</italic> for all model packages.</p>
      <p>Using object-oriented programming, the same model_executor class is used to implement, instantiate, and run all different types of generative model packages. To keep the library maintainable and lightweight, and to avoid limiting interdependencies between library code and generative model code, <italic>medigan</italic>’s models are hosted outside the library (on Zenodo) as independent Python modules. To avoid long initialization times upon library import, lazy loading is applied. A model is only loaded and its <monospace>model_executor</monospace> instance is only initialized if a user specifically requests synthetic data generation for that model. To achieve high cohesion,<xref rid="r79" ref-type="bibr"><sup>79</sup></xref> i.e., keeping the library and its functions specific, manageable, and understandable, the library is structured into several modular components. These include the loosely-coupled <monospace>model_executor</monospace>, <monospace>model_selector</monospace>, and <monospace>model_contributor</monospace> modules.</p>
      <p>The <monospace>generators</monospace> module is inspired by the facade design pattern<xref rid="r80" ref-type="bibr"><sup>80</sup></xref> and acts as a single point of access to all of <italic>medigan</italic>’s functionalities. As single interface layer between users and library, it reduces interaction complexity and provides users with a clear set of readily extendable library functions. Also, the <monospace>generators</monospace> module increases internal code reusability and allows for combination of functions from other modules. For instance, a single function call can run the generation of samples by the model with the highest FID score of all models found in a keyword search.</p>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Model Metadata</title>
      <p>The FID score and all other model information such as dependencies, modality, type, zenodo link, associated publications, and generate function parameters are stored in a single comprehensive model metadata json file. Alongside its searchability, readability, and flexibility, the choice of json as file format is motivated by its extendability to a nonrelational database. As a single source of model information, the <italic>global.json</italic> file consists of an array of model IDs, where under each model id the respective model metadata is stored. Toward ensuring model traceability as recommended by the FUTURE-AI consensus guidelines,<xref rid="r75" ref-type="bibr"><sup>75</sup></xref> each model (on Zenodo) and its global.json metadata (on GitHub) are version-controlled with the latter being structured into the following objects.</p>
      <list list-type="simple">
        <list-item>
          <label>i.</label>
          <p><italic>execution</italic>: contains the information needed to download, package, and run the model resources.</p>
        </list-item>
        <list-item>
          <label>ii.</label>
          <p><italic>selection</italic>: contains model evaluation metrics and further information used to search, compare, and rank models.</p>
        </list-item>
        <list-item>
          <label>iii.</label>
          <p><italic>description</italic>: contains general information and main details about the model such as title, training dataset, license, date, and related publications.</p>
        </list-item>
      </list>
      <p>This <italic>global.json</italic> metadata file is retrieved, provided, and handled by the <monospace>config_manager</monospace> module once a user imports the <monospace>generators</monospace> module. This facilitates rapid access to a model’s metadata given its <italic>model_id</italic> and allows one to add new models or model versions to <italic>medigan</italic> via pull request without requiring a new release of the library.</p>
    </sec>
    <sec id="sec3.4">
      <label>3.4</label>
      <title>Model Search and Ranking</title>
      <p>The number of models in <italic>medigan</italic> is expected to grow over time. Potentially this will lead to the foreseeable issue where users of <italic>medigan</italic> have a large number of models to choose from. Users likely will be uncertain which model best fits their needs depending on their data, modality, use-case, and research problem at hand and would have to go through each model’s metadata to find the most suitable model in <italic>medigan</italic>. Hence, to facilitate model selection, the <monospace>model_selector</monospace> module implements model search and ranking functionalities. This search workflow is shown in <xref rid="f4" ref-type="fig">Fig. 4</xref> and triggered by running Code Snippet 1.</p>
      <fig position="float" id="f4">
        <label>Fig. 4</label>
        <caption>
          <p>The search workflow. A user sends a search query (1) to the generators class, which triggers a search (2) via the ModelSelector class. The latter retrieves the <italic>global.json</italic> model metadata/config dict (3), in which it searches for query values finding matching models (4). Next, the matched models are optionally also ranked based on a user-defined performance indicator (5) before being returned as list to the user.</p>
        </caption>
        <graphic xlink:href="JMI-010-061403-g004" position="float"/>
      </fig>
      <p>The <monospace>model_selector</monospace> module contains a search method that takes search operator (i.e OR, AND, or XOR) and a keyword search values list as parameters and recursively searches through the models’ metadata. The latter is provided by the <monospace>config_manager</monospace> module. The <monospace>model_selector</monospace> populates a <monospace>modelMatchCandidates</monospace> object with <monospace>matchedEntry</monospace> instances each of which represents a potential model match to the search query. The <monospace>modelMatchCandidates</monospace> class evaluates which of it is associated model matches should be flagged as true match given the search values and search operator. The method <monospace>rank_models_by_performance</monospace> compares either all or specified models in medigan by a performance indicator such as FID. This indicator commonly is a metric that correlates with diversity, fidelity, or condition adherence to estimate the quality of generative models and/or the data they generate.<xref rid="r7" ref-type="bibr"><sup>7</sup></xref> The <monospace>model_selector</monospace> looks up the value for the specified performance indicator in the model metadata and returns a descendingly or ascendingly ranked list of models to the user.</p>
      <table-wrap position="float" id="t003">
        <label>Code Snippet 1:</label>
        <caption>
          <p>Searching for a model in <italic>medigan</italic>.</p>
        </caption>
        <!--OASIS TABLE HERE-->
        <table frame="hsides" rules="groups">
          <col/>
          <tbody>
            <tr>
              <td>
                <monospace>1. from medigan import Generators <italic># import</italic></monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace>2. generators = Generators() <italic># init</italic></monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace>3. values=[’patches’, ’mammography’] <italic># keywords of search query</italic></monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace>4. operator=’AND’ <italic># all keywords are needed for match</italic></monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace>5. results = generators.find_model(values, operator)</monospace>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec3.5">
      <label>3.5</label>
      <title>Synthetic Data Generation</title>
      <p>Synthetic data generation is <italic>medigan</italic>’s core functionality toward overcoming scarcity of (a) training data and (b) reusable generative model in medical imaging. Posing a low entry barrier for nonexpert users, <italic>medigan</italic>’s <monospace>generate</monospace> method is both simple and scalable. While a user can run it with only one line of code, it flexibly supports any type of generative model and synthetic data generation process, as illustrated in <xref rid="t004" ref-type="table">Table 3</xref> and <xref rid="f1" ref-type="fig">Fig. 1</xref>.</p>
      <table-wrap position="float" id="t004">
        <label>Table 3</label>
        <caption>
          <p>Models currently available in <italic>medigan</italic>. Also, computed FID scores for each model in <italic>medigan</italic> are shown. The number of real samples used for FID calculation is indicated by #imgs. The <italic>lower bound</italic>
<inline-formula><mml:math id="math26" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is computed between a pair of randomly sampled sets of real data (real-real), whereas the <italic>model</italic>
<inline-formula><mml:math id="math27" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is computed between two randomly sampled sets of real and synthetic data (real-syn). The results for model 7 (Flair, T1, T1c, T2) and 21 (T1, T2) are averaged across modalities.</p>
        </caption>
        <!--OASIS TABLE HERE-->
        <table frame="hsides" rules="groups">
          <colgroup>
            <col/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col/>
            <col/>
            <col/>
            <col/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" valign="top">ID</th>
              <th rowspan="2" align="left" valign="top">Output</th>
              <th rowspan="2" align="left" valign="top">Modality</th>
              <th rowspan="2" align="left" valign="top">Model</th>
              <th rowspan="2" align="left" valign="top">Size</th>
              <th rowspan="2" align="left" valign="top">Training dataset</th>
              <th colspan="4" valign="top">
                <inline-formula>
                  <mml:math id="math28" display="inline" overflow="scroll">
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>FID</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mtext>ImageNet</mml:mtext>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
                <xref rid="r47" ref-type="bibr">
                  <sup>47</sup>
                </xref>
                <sup>,</sup>
                <xref rid="r52" ref-type="bibr">
                  <sup>52</sup>
                </xref>
              </th>
            </tr>
            <tr>
              <th valign="top">#imgs</th>
              <th valign="top">Real-real</th>
              <th valign="top">Real-syn</th>
              <th valign="top">
                <inline-formula>
                  <mml:math id="math29" display="inline" overflow="scroll">
                    <mml:mrow>
                      <mml:msub>
                        <mml:mi>r</mml:mi>
                        <mml:mi>FID</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td> 1</td>
              <td align="left">Breast calcifications</td>
              <td align="left">Mammography</td>
              <td align="left">DCGAN</td>
              <td align="left">128 × 128</td>
              <td align="left">INbreast<xref rid="r81" ref-type="bibr"><sup>81</sup></xref></td>
              <td>1000</td>
              <td>33.61</td>
              <td>67.60</td>
              <td>0.497</td>
            </tr>
            <tr>
              <td> 2</td>
              <td align="left">Breast masses</td>
              <td align="left">Mammography</td>
              <td align="left">DCGAN<xref rid="r11" ref-type="bibr"><sup>11</sup></xref></td>
              <td align="left">128 × 128</td>
              <td align="left">OPTIMAM<xref rid="r82" ref-type="bibr"><sup>82</sup></xref></td>
              <td>1000</td>
              <td>28.85</td>
              <td>80.51</td>
              <td>0.358</td>
            </tr>
            <tr>
              <td> 3</td>
              <td align="left">High/low density breasts</td>
              <td align="left">Mammography</td>
              <td align="left">CycleGAN<xref rid="r50" ref-type="bibr"><sup>50</sup></xref></td>
              <td align="left">1332 × 800</td>
              <td align="left">BCDR<xref rid="r83" ref-type="bibr"><sup>83</sup></xref></td>
              <td>74</td>
              <td>65.94</td>
              <td>150.16</td>
              <td>0.439</td>
            </tr>
            <tr>
              <td> 4</td>
              <td align="left">Breast masses with masks</td>
              <td align="left">Mammography</td>
              <td align="left">pix2pix</td>
              <td align="left">256 × 256</td>
              <td align="left">BCDR<xref rid="r83" ref-type="bibr"><sup>83</sup></xref></td>
              <td>199</td>
              <td>68.22</td>
              <td>161.17</td>
              <td>0.423</td>
            </tr>
            <tr>
              <td> 5</td>
              <td align="left">Breast masses</td>
              <td align="left">Mammography</td>
              <td align="left">DCGAN<xref rid="r14" ref-type="bibr"><sup>14</sup></xref></td>
              <td align="left">128 × 128</td>
              <td align="left">BCDR<xref rid="r83" ref-type="bibr"><sup>83</sup></xref></td>
              <td>199</td>
              <td>68.22</td>
              <td>180.04</td>
              <td>0.379</td>
            </tr>
            <tr>
              <td> 6</td>
              <td align="left">Breast masses</td>
              <td align="left">Mammography</td>
              <td align="left">WGAN-GP<xref rid="r14" ref-type="bibr"><sup>14</sup></xref></td>
              <td align="left">128 × 128</td>
              <td align="left">BCDR<xref rid="r83" ref-type="bibr"><sup>83</sup></xref></td>
              <td>199</td>
              <td>68.22</td>
              <td>221.30</td>
              <td>0.308</td>
            </tr>
            <tr>
              <td> 7</td>
              <td align="left">Brain tumors with masks</td>
              <td align="left">Cranial MRI</td>
              <td align="left">Inpaint GAN <xref rid="r84" ref-type="bibr"><sup>84</sup></xref></td>
              <td align="left">256 × 256</td>
              <td align="left">BRATS 2018<xref rid="r85" ref-type="bibr"><sup>85</sup></xref></td>
              <td>1000</td>
              <td>30.73</td>
              <td>140.02</td>
              <td>0.219</td>
            </tr>
            <tr>
              <td> 8</td>
              <td align="left">Breast masses (mal/benign)</td>
              <td align="left">Mammography</td>
              <td align="left">C-DCGAN</td>
              <td align="left">128 × 128</td>
              <td align="left">CBIS-DDSM<xref rid="r86" ref-type="bibr"><sup>86</sup></xref></td>
              <td>379</td>
              <td>37.56</td>
              <td>137.75</td>
              <td>0.272</td>
            </tr>
            <tr>
              <td> 9</td>
              <td align="left">Polyps with masks</td>
              <td align="left">Endoscopy</td>
              <td align="left">PGGAN<xref rid="r51" ref-type="bibr"><sup>51</sup></xref></td>
              <td align="left">256 × 256</td>
              <td align="left">HyperKvasir<xref rid="r87" ref-type="bibr"><sup>87</sup></xref></td>
              <td>1000</td>
              <td>43.31</td>
              <td>225.85</td>
              <td>0.192</td>
            </tr>
            <tr>
              <td>10</td>
              <td align="left">Polyps with masks</td>
              <td align="left">Endoscopy</td>
              <td align="left">FastGAN<xref rid="r51" ref-type="bibr"><sup>51</sup></xref></td>
              <td align="left">256 × 256</td>
              <td align="left">HyperKvasir<xref rid="r87" ref-type="bibr"><sup>87</sup></xref></td>
              <td>1000</td>
              <td>43.31</td>
              <td>63.99</td>
              <td>0.677</td>
            </tr>
            <tr>
              <td>11</td>
              <td align="left">Polyps with masks</td>
              <td align="left">Endoscopy</td>
              <td align="left">SinGAN<xref rid="r51" ref-type="bibr"><sup>51</sup></xref></td>
              <td align="left">≈250 × 250</td>
              <td align="left">HyperKvasir<xref rid="r87" ref-type="bibr"><sup>87</sup></xref></td>
              <td>1000</td>
              <td>43.31</td>
              <td>171.15</td>
              <td>0.253</td>
            </tr>
            <tr>
              <td>12</td>
              <td align="left">Breast masses (mal/benign)</td>
              <td align="left">Mammography</td>
              <td align="left">C-DCGAN</td>
              <td align="left">128 × 128</td>
              <td align="left">BCDR<xref rid="r83" ref-type="bibr"><sup>83</sup></xref></td>
              <td>199</td>
              <td>68.22</td>
              <td>205.29</td>
              <td>0.332</td>
            </tr>
            <tr>
              <td>13</td>
              <td align="left">High/low density breasts MLO</td>
              <td align="left">Mammography</td>
              <td align="left">CycleGAN<xref rid="r50" ref-type="bibr"><sup>50</sup></xref></td>
              <td align="left">1332 × 800</td>
              <td align="left">OPTIMAM<xref rid="r82" ref-type="bibr"><sup>82</sup></xref></td>
              <td>358</td>
              <td>65.75</td>
              <td>101.09</td>
              <td>0.650</td>
            </tr>
            <tr>
              <td>14</td>
              <td align="left">High/low density breasts CC</td>
              <td align="left">Mammography</td>
              <td align="left">CycleGAN<xref rid="r50" ref-type="bibr"><sup>50</sup></xref></td>
              <td align="left">1332 × 800</td>
              <td align="left">OPTIMAM<xref rid="r82" ref-type="bibr"><sup>82</sup></xref></td>
              <td>350</td>
              <td>41.61</td>
              <td>73.77</td>
              <td>0.564</td>
            </tr>
            <tr>
              <td>15</td>
              <td align="left">High/low density breasts MLO</td>
              <td align="left">Mammography</td>
              <td align="left">CycleGAN<xref rid="r50" ref-type="bibr"><sup>50</sup></xref></td>
              <td align="left">1332 × 800</td>
              <td align="left">CSAW<xref rid="r88" ref-type="bibr"><sup>88</sup></xref></td>
              <td>192</td>
              <td>74.96</td>
              <td>162.67</td>
              <td>0.461</td>
            </tr>
            <tr>
              <td>16</td>
              <td align="left">High/low density breasts CC</td>
              <td align="left">Mammography</td>
              <td align="left">CycleGAN<xref rid="r50" ref-type="bibr"><sup>50</sup></xref></td>
              <td align="left">1332 × 800</td>
              <td align="left">CSAW<xref rid="r88" ref-type="bibr"><sup>88</sup></xref></td>
              <td>202</td>
              <td>42.68</td>
              <td>98.38</td>
              <td>0.434</td>
            </tr>
            <tr>
              <td>17</td>
              <td align="left">Lung nodules</td>
              <td align="left">Chest x-ray</td>
              <td align="left">DCGAN</td>
              <td align="left">128 × 128</td>
              <td align="left">NODE21<xref rid="r89" ref-type="bibr"><sup>89</sup></xref></td>
              <td>1476</td>
              <td>24.34</td>
              <td>126.78</td>
              <td>0.192</td>
            </tr>
            <tr>
              <td>18</td>
              <td align="left">Lung nodules</td>
              <td align="left">Chest x-ray</td>
              <td align="left">WGAN-GP</td>
              <td align="left">128 × 128</td>
              <td align="left">NODE21<xref rid="r89" ref-type="bibr"><sup>89</sup></xref></td>
              <td>1476</td>
              <td>24.34</td>
              <td>211.47</td>
              <td>0.115</td>
            </tr>
            <tr>
              <td>19</td>
              <td align="left">Full chest radiograph</td>
              <td align="left">Chest x-ray</td>
              <td align="left">PGGAN</td>
              <td align="left">1024 × 1024</td>
              <td align="left">ChestX-ray14<xref rid="r90" ref-type="bibr"><sup>90</sup></xref></td>
              <td>1000</td>
              <td>28.74</td>
              <td>96.74</td>
              <td>0.297</td>
            </tr>
            <tr>
              <td>20</td>
              <td align="left">Full chest radiograph</td>
              <td align="left">Chest x-ray</td>
              <td align="left">PGGAN<xref rid="r91" ref-type="bibr"><sup>91</sup></xref></td>
              <td align="left">1024 × 1024</td>
              <td align="left">ChestX-ray14<xref rid="r90" ref-type="bibr"><sup>90</sup></xref></td>
              <td>1000</td>
              <td>28.33</td>
              <td>52.17</td>
              <td>0.543</td>
            </tr>
            <tr>
              <td>21</td>
              <td align="left">Brain scans (T1/T2)</td>
              <td align="left">Cranial MRI</td>
              <td align="left">CycleGAN<xref rid="r92" ref-type="bibr"><sup>92</sup></xref></td>
              <td align="left">224 × 192</td>
              <td align="left">CrossMoDA 2021<xref rid="r93" ref-type="bibr"><sup>93</sup></xref></td>
              <td>1000</td>
              <td>24.41</td>
              <td>59.49</td>
              <td>0.410</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <sec id="sec3.5.1">
        <label>3.5.1</label>
        <title>Generate workflow</title>
        <p>An example of the usage of the <monospace>generate</monospace> method is shown in Code Snippet 2, which triggers the model execution workflow illustrated in <xref rid="f5" ref-type="fig">Fig. 5</xref>. Further parameters of the generate method allow users to specify the number of samples to be generated (<monospace>num_samples</monospace>), if samples are returned as a list or stored on a disk (<monospace>save_images</monospace>), where they are stored (<monospace>output_path</monospace>), and whether model dependencies are automatically installed (<monospace>install_dependencies</monospace>). Optional model-specific inputs can be provided via the **<monospace>kwargs</monospace> parameter. These include for example, (i) a nondefault path to the model weights, (ii) a path to an input image folder for image-to-image translation models, (iii) a conditional input for class-conditional generative models, or (iv) the <monospace>input_latent_vector</monospace> as commonly used as model input in GANs.</p>
        <fig position="float" id="f5">
          <label>Fig. 5</label>
          <caption>
            <p>The generated workflow. A user specifies a <italic>model_id</italic> in a request (1) to the generators class, which checks (2) if the model’s ModelExecutor class instance is already initialized. If not, a new one is created (3), which (4) gets the model’s config from the <italic>global.json</italic> dict, (5) loads the model (e.g., from <italic>Zenodo</italic>), (6) checks its dependencies, and (7) unzips and imports it, before running its internal generate function (8). Finally, the generated samples are returned to the user.</p>
          </caption>
          <graphic xlink:href="JMI-010-061403-g005" position="float"/>
        </fig>
        <p>Running the <monospace>generate</monospace> method triggers the <monospace>generators</monospace> module to initialize a <monospace>model_executor</monospace> instance for the user-specified generative model. The model is identified via its <italic>model_id</italic> as unique key in the <italic>global.json</italic> model metadata database, parsed and managed by the <monospace>config_manager</monospace> module. Using the latter, the <monospace>model_executor</monospace> checks if the required Python package dependencies are installed, retrieves the Zenodo URL and downloads, unzips, and imports the model package. It further retrieves the name of the internal data generation function inside the model’s __<monospace>init_ _.py</monospace> script. As final step before calling this function, its parameters and their default values are retrieved from the metadata and combined with user-provided arguments. These user-provided arguments customize the generation process, which enables handling of multiple image generation scenarios. For instance, the aforementioned provision of the input image folder allows users to point to their own images to transform them using <italic>medigan</italic> models that are, e.g., pretrained for cross-modality translation. In the case of large dataset generation, the number of samples indicated by <monospace>num_samples</monospace> are chunked into smaller-sized batches and iteratively generated to avoid overloading the random-access memory available on the user’s machine.</p>
        <table-wrap position="float" id="t005">
          <label>Code Snippet 2:</label>
          <caption>
            <p> Executing a <italic>medigan</italic> model for synthetic data generation.</p>
          </caption>
          <!--OASIS TABLE HERE-->
          <table frame="hsides" rules="groups">
            <col/>
            <tbody>
              <tr>
                <td>
                  <monospace>1. from medigan import <monospace>Generators</monospace></monospace>
                </td>
              </tr>
              <tr>
                <td>
                  <monospace>2. generators = Generators()</monospace>
                </td>
              </tr>
              <tr>
                <td>
                  <monospace>
                    <italic># create 100 polyps with masks using model 10 (FASTGAN)</italic>
                  </monospace>
                </td>
              </tr>
              <tr>
                <td>
                  <monospace>generators.generate(model_id=10, num_samples=100)</monospace>
                </td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec id="sec3.5.2">
        <label>3.5.2</label>
        <title>Generate workflow extensions</title>
        <p>Apart from storing or returning samples, a callable of the model’s internal generate function can be returned to the user by setting <monospace>is_gen_function_returned</monospace>. This function with prepared but adjustable default arguments enables integration of the generate method into other workflows within <italic>medigan</italic> (e.g., model visualization) or outside of <italic>medigan</italic> (e.g., a user’s AI model training). As a further alternative, a torch<xref rid="r67" ref-type="bibr"><sup>67</sup></xref> dataset or dataloader can be returned for any model in medigan running <monospace>get_as_torch_dataset</monospace> or <monospace>get_as_torch_dataloader</monospace>, respectively. This further increases the versatility with which users can introduce <italic>medigan</italic>’s data synthesis capabilities into their AI model training and data preprocessing pipelines.</p>
        <p>Instead of a user manually selecting a model via <italic>model_id</italic>, a model can also be automatically selected based on the recommendation from the model search and/or ranking methods. For instance, as triggered by Code Snippet 3, the models found in a search for <italic>mammography</italic> are ranked in ascending order based on FID, with the highest ranking model being selected and executed to generate the synthetic dataset.</p>
        <table-wrap position="float" id="t006">
          <label>Code Snippet 3:</label>
          <caption>
            <p>Sequential searching, ranking, and data generation with highest ranked model.</p>
          </caption>
          <!--OASIS TABLE HERE-->
          <table frame="hsides" rules="groups">
            <col/>
            <tbody>
              <tr>
                <td>
                  <monospace>1. from medigan import Generators</monospace>
                </td>
              </tr>
              <tr>
                <td>
                  <monospace>2. generators = Generators()</monospace>
                </td>
              </tr>
              <tr>
                <td>
                  <monospace>3. values = [’mammography’] <italic># keywords for searching</italic></monospace>
                </td>
              </tr>
              <tr>
                <td>
                  <monospace>4. metric = ’FID’ <italic># metric for ranking</italic></monospace>
                </td>
              </tr>
              <tr>
                <td>
                  <monospace>5. generators.find_models_rank_and_generate(values=values, metric=metric)</monospace>
                </td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
    </sec>
    <sec id="sec3.6">
      <label>3.6</label>
      <title>Model Visualization</title>
      <p>To allow users to explore the generative models in <italic>medigan</italic>, a novel model visualization module has been integrated into the library. It allows users to examine how changing inputs like the latent variable <inline-formula><mml:math id="math30" display="inline" overflow="scroll"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula> and/or the class conditional label <inline-formula><mml:math id="math31" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula> (e.g., malignant/benign) can affect the generation process. Also, the correlation between multiple model outputs, such as the image and corresponding segmentation mask, can be observed and explored. <xref rid="f6" ref-type="fig">Figure 6</xref> illustrates an example showing an image-mask sample pair from medigan’s polyp generating FastGAN model.<xref rid="r51" ref-type="bibr"><sup>51</sup></xref> This depiction of the graphical user interface (GUI) of the model visualization tool can be recreated by running Code Snippet 4.</p>
      <fig position="float" id="f6">
        <label>Fig. 6</label>
        <caption>
          <p>Graphical user interface of <italic>medigan</italic>’s model visualization tool on the example of model 10, a FastGAN that synthesizes endoscopic polyp images with respective masks.<xref rid="r51" ref-type="bibr"><sup>51</sup></xref> The latent input vector can be adjusted via the sliders, reset via the <italic>Reset</italic> button, and sampled randomly via the <italic>Seed</italic> button.</p>
        </caption>
        <graphic xlink:href="JMI-010-061403-g006" position="float"/>
      </fig>
      <p>Internally, the <monospace>model_visualizer</monospace> module retrieves a model’s internal generate method as callable from the <monospace>model_executor</monospace> and adjusts the input parameters based on user interaction input from the GUI. This interaction further provides insight into a model’s performance and capabilities. On one hand, it allows one to assess the fidelity of the generated samples. On the other hand, it also shows the model’s captured sample diversity, i.e., as observed output variation over all possible input latent vectors. We leave the automation of manual visual analysis of this output variation to future work. For instance, such future work can use the <monospace>model_visualizer</monospace> to measure the variance of a reconstruction/perceptual error computed between pairs of images sampled from fixed-distance pairs of latent space vectors <inline-formula><mml:math id="math32" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:math></inline-formula>. The slider controls on the left of the interface allow one to change the latent variable, which for this specific model affects, for instance, polyp size, position, and background. As the size of the latent vector <inline-formula><mml:math id="math33" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:math></inline-formula> commonly is relatively large, each <inline-formula><mml:math id="math34" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> (e.g., 10) variables are grouped into one indexed slider resulting in <inline-formula><mml:math id="math35" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> adjustable latent input variables. The seed button on the right allows one to initialize a new set of latent variables, which results in a new generated image. The reset buttons allows one to revert user’s modifications to previous random values.</p>
      <table-wrap position="float" id="t007">
        <label>Code Snippet 4:</label>
        <caption>
          <p>Visualization of a model in <italic>medigan</italic>.</p>
        </caption>
        <!--OASIS TABLE HERE-->
        <table frame="hsides" rules="groups">
          <col/>
          <tbody>
            <tr>
              <td>
                <monospace>1. from medigan import Generators</monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace>2. generators = Generators()</monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace>3. n = 10  <italic># grouping latent vector <inline-formula><mml:math id="math36" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:math></inline-formula> dimensions by dividing them by 10</italic></monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace>4. generators.visualize(model_id=10, slider_grouper=n) <italic># polyp with mask</italic></monospace>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec3.7">
      <label>3.7</label>
      <title>Model Contribution</title>
      <p>A core idea of <italic>medigan</italic> is to provide a platform where researchers can share and access trained models via a standardized interface. We provide in-depth instructions on how to contribute a model to medigan complemented by implementations automating parts of the model contribution process for users. In general, a pretrained model in medigan consists of a Python __<monospace>init __.py</monospace> and, in case the generation process is based on a machine learning model, a respective checkpoint or weights file. The former needs to contain a synthetic data storage method and a data generation method with a set of standardized parameters described in Sec. <xref rid="sec3.5.1" ref-type="sec">3.5.1</xref>. Ideally, a model package further contains a license file, a <italic>metadata.json</italic> and/or a <italic>requirements.txt</italic> file, and a <italic>test.sh</italic> script to quickly verify the model’s functionalities. To facilitate creation of these files, <italic>medigan</italic>’s GitHub repository provides model contributors with reusable templates for each of these files.</p>
      <p>Keeping the effort of pretrained model inclusion to a minimum, the <monospace>generators</monospace> module contains a <monospace>contribute</monospace> function that initializes a <monospace>ModelContributor</monospace> class instance dedicated to automating the remainder of the model contribution process. This includes automated (i) validation of the user-provided <italic>model_id</italic>; (ii) validation of the path to the model’s __<monospace>init__.py</monospace>; (iii) test of <monospace>importlib</monospace> import of the model as package; (iv) creation of the model’s metadata dictionary; (v) adding the model metadata to <italic>medigan</italic>’s <italic>global.json</italic> metadata; (vi) end-to-end test of model with sample generation via <monospace>generators.test_model()</monospace>; (vii) upload of zipped model package to Zenodo via API; and (viii) creation of a GitHub issue, which contains the Zenodo link and model metadata, in the <italic>medigan</italic> repository. Being assigned to this GitHub issue, the <italic>medigan</italic> development team is notified about the new model, which can then be added via pull request. Code Snippet 5 shows how a user can run the <monospace>contribute</monospace> method illustrated in <xref rid="f7" ref-type="fig">Fig. 7</xref>.</p>
      <fig position="float" id="f7">
        <label>Fig. 7</label>
        <caption>
          <p>Model contribution workflow. After model preparation (1), a user provides the model’s id and metadata (2) to the generators class to (3) initialize a ModelContributor instance, which (4) validates and (5) extends the metadata. Next, (6) the model’s sample generation capability is tested after (7) integration into <italic>medigan</italic>’s <italic>global.json</italic> model metadata. If successful, (8) the model package is prepared and (9–13) pushed to Zenodo via API. Lastly, (14 and 15) a GitHub issue containing the model metadata is created, assigned, and pushed to the <italic>medigan</italic> repository.</p>
        </caption>
        <graphic xlink:href="JMI-010-061403-g007" position="float"/>
      </fig>
      <table-wrap position="float" id="t008">
        <label/>
        <caption>
          <p><bold>Code Snippet 5:</bold> Contribution of a model to <italic>medigan</italic>.</p>
        </caption>
        <!--OASIS TABLE HERE-->
        <table frame="hsides" rules="groups">
          <col/>
          <tbody>
            <tr>
              <td>
                <monospace> 1. from medigan import Generators</monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace> 2. generators = Generators()</monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace> 3. generators.contribute(</monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace> 4.     model_id = “00100_YOUR_MODEL”, <italic># assign ID</italic></monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace> 5.     init_py_path =“path/ending/with/__init__.py”, <italic># model package root</italic></monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace> 6.     generate_method_name = “generate”, <italic># method inside __init__.py</italic></monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace> 7.     model_weights_name = “10000",</monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace> 8.     model_weights_extension = ”.pt”,</monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace> 9.     dependencies = [“numpy”, “torch”],</monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace>10.   zenodo_access_token = “TOKEN”, <italic>#zenodo.org/account/settings/applications</italic></monospace>
              </td>
            </tr>
            <tr>
              <td>
                <monospace>11.   github_access_token = “TOKEN”) <italic>#github.com/settings/tokens</italic></monospace>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec3.8">
      <label>3.8</label>
      <title>Model Testing Pipeline</title>
      <p>Each new model contribution is being systematically tested before becoming part of <italic>medigan</italic>. For instance, on each submitted pull request to medigan’s GitHub repository, a CI pipeline automatically builds, formats, lints, and tests <italic>medigan</italic>’s codebase. This includes the automatic verification of each model’s package, dependencies, compatibility with the interface, and correct functioning of its generation workflow. This allows one to ensure that all models and their metadata in the <italic>global.json</italic> file are available and working in a reproducible and standardized manner.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Applications</title>
    <sec id="sec4.1">
      <label>4.1</label>
      <title>Community-Wide Data Access: Sharing the Essence of Restricted Datasets</title>
      <p><italic>medigan</italic> facilitates sharing and reusing trained generative models with the medical research community. On one hand, this reduces the need for researchers to retrain their own similar generative models, which can reduce the extensive carbon footprint<xref rid="r94" ref-type="bibr"><sup>94</sup></xref> of deep learning in medical imaging. On the other hand, this provides a platform for researchers and data owners to share their dataset distribution without sharing the real data points of the dataset. Put differently, sharing generative models trained on (and instead of) patient datasets not only is beneficial as data curation step,<xref rid="r14" ref-type="bibr"><sup>14</sup></xref> but also minimizes the need to share images and personal data directly attributable to a patient. In particular, the latter can be quantifiably achieved when the generative model is trained using a differential privacy guarantee<xref rid="r7" ref-type="bibr"><sup>7</sup></xref><sup>,</sup><xref rid="r95" ref-type="bibr"><sup>95</sup></xref> before being added to <italic>medigan</italic>. By reducing the barriers posed by data sharing restrictions and necessary patient privacy protection regulation, <italic>medigan</italic> unlocks a new paradigm of medical data sharing via generative models. This places <italic>medigan</italic> at the center toward solving the well-known issue of data scarcity<xref rid="r7" ref-type="bibr"><sup>7</sup></xref><sup>,</sup><xref rid="r9" ref-type="bibr"><sup>9</sup></xref> in medical imaging.</p>
      <p>Apart from that, <italic>medigan</italic>’s generative model contributors benefit from an increased exposure, dissemination, and impact of their work, as their generative models become readily usable by other researchers. As <xref rid="t004" ref-type="table">Table 3</xref> illustrates, to date, <italic>medigan</italic> consists of 21 pretrained deep generative models contributed to the community. Among others, these include two conditional DCGAN models, six domain translation CycleGAN models and one mask-to-image pix2pix model. The training data comes from 10 different medical imaging datasets. Various of the models were trained on breast cancer datasets including INbreast,<xref rid="r81" ref-type="bibr"><sup>81</sup></xref> OPTIMAM,<xref rid="r82" ref-type="bibr"><sup>82</sup></xref> BCDR,<xref rid="r83" ref-type="bibr"><sup>83</sup></xref> CBIS-DDSM,<xref rid="r86" ref-type="bibr"><sup>86</sup></xref> and CSAW.<xref rid="r88" ref-type="bibr"><sup>88</sup></xref> Models allow one to generate samples of different pixel resolutions ranging from regions-of-interest patches of size <inline-formula><mml:math id="math37" display="inline" overflow="scroll"><mml:mrow><mml:mn>128</mml:mn><mml:mo>×</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math38" display="inline" overflow="scroll"><mml:mrow><mml:mn>256</mml:mn><mml:mo>×</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula> to full images of <inline-formula><mml:math id="math39" display="inline" overflow="scroll"><mml:mrow><mml:mn>1024</mml:mn><mml:mo>×</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math40" display="inline" overflow="scroll"><mml:mrow><mml:mn>1332</mml:mn><mml:mo>×</mml:mo><mml:mn>800</mml:mn><mml:mtext>  </mml:mtext><mml:mtext>pixels</mml:mtext></mml:mrow></mml:math></inline-formula>.</p>
    </sec>
    <sec id="sec4.2">
      <label>4.2</label>
      <title>Investigating Synthetic Data Evaluation Methods</title>
      <p>A further application of <italic>medigan</italic> is testing the properties of medical synthetic data. For instance, evaluation metrics for generative models can be readily tested in <italic>medigan</italic>’s multiorgan, multimodality, and multimodel synthetic data setting.</p>
      <p>Compared to generative modeling, synthetic data evaluation is a less explored research area.<xref rid="r7" ref-type="bibr"><sup>7</sup></xref> In particular, in medical imaging the existing evaluation frameworks, such as the FID<xref rid="r46" ref-type="bibr"><sup>46</sup></xref> or the IS,<xref rid="r17" ref-type="bibr"><sup>17</sup></xref> are often limited in their applicability, as mentioned in Sec. <xref rid="sec2.3" ref-type="sec">2.3</xref>. The models in <italic>medigan</italic> allow one to compare existing and new synthetic data evaluation metrics and their validation in the field of medical imaging. Multimodel synthetic data evaluation allows one to measure the correlation and statistical significance between synthetic data evaluation metrics and downstream task performance metrics. This enables the assessment of clinical usefulness of generative models on one hand and of synthetic data evaluation metrics on the other hand. In that sense, the metric itself can be evaluated including its variations when measured under different settings, datasets, or preprocessing techniques.</p>
      <sec id="sec4.2.1">
        <label>4.2.1</label>
        <title>FID of medigan Models</title>
        <p>We compute the FID to assess the models in <italic>medigan</italic> and report the results in <xref rid="t003" ref-type="table">Table 3</xref>. We further note that the FID can be computed not only between a synthetic and a real dataset (<italic>rs</italic>) but also between two sets of samples of the real dataset (<italic>rr</italic>). As the <inline-formula><mml:math id="math41" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> describes the distance within two randomly sampled sets of the real data distribution, it can be used as an estimate of the real data variation and optimal lower bound for the <inline-formula><mml:math id="math42" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as shown in <xref rid="t004" ref-type="table">Table 3</xref>. Given the above, it follows that a high <inline-formula><mml:math id="math43" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> likely also results in a higher <inline-formula><mml:math id="math44" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which highlights the importance of accounting for the <inline-formula><mml:math id="math45" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> when discussing the <inline-formula><mml:math id="math46" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. To do so, we propose the reporting of a FID ratio <inline-formula><mml:math id="math47" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>FID</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to describe the <inline-formula><mml:math id="math48" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in terms of the <inline-formula><mml:math id="math49" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. <disp-formula id="e005"><mml:math id="math50" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>FID</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>FID</mml:mi></mml:mrow><mml:mrow><mml:mi>rs</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>FID</mml:mi></mml:mrow><mml:mrow><mml:mi>rr</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>FID</mml:mi></mml:mrow><mml:mrow><mml:mi>rs</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>FID</mml:mi></mml:mrow><mml:mrow><mml:mi>rr</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>FID</mml:mi></mml:mrow><mml:mrow><mml:mi>rs</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace depth="0.0ex" height="0.0ex" width="1em"/><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>FID</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0,1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>⊂</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math><label>(5)</label></disp-formula>Assuming <inline-formula><mml:math id="math51" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> bounds <inline-formula><mml:math id="math52" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>FID</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> between 0 and 1, <inline-formula><mml:math id="math53" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>FID</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> the simplifies the comparison of FIDs computed using different models and datasets. A <inline-formula><mml:math id="math54" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>FID</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> close to 1 indicates that much of the <inline-formula><mml:math id="math55" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> can be explained by the general variation in the real dataset. The code used to compute the FID scores is available at <ext-link xlink:href="https://github.com/RichardObi/medigan/blob/main/tests/fid.py" ext-link-type="uri">https://github.com/RichardObi/medigan/blob/main/tests/fid.py</ext-link>.</p>
        <p>The models in <xref rid="t003" ref-type="table">Table 3</xref> yielding the highest ImageNet-based <inline-formula><mml:math id="math56" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>FID</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> score are the ones with ID 10 (0.677, endoscopy, <inline-formula><mml:math id="math57" display="inline" overflow="scroll"><mml:mrow><mml:mn>256</mml:mn><mml:mo>×</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></inline-formula>, FastGAN), ID 13 (0.650, mammography, <inline-formula><mml:math id="math58" display="inline" overflow="scroll"><mml:mrow><mml:mn>1332</mml:mn><mml:mo>×</mml:mo><mml:mn>800</mml:mn></mml:mrow></mml:math></inline-formula>, CycleGAN), 14 (0.564, mammography, <inline-formula><mml:math id="math59" display="inline" overflow="scroll"><mml:mrow><mml:mn>1332</mml:mn><mml:mo>×</mml:mo><mml:mn>800</mml:mn></mml:mrow></mml:math></inline-formula>, CycleGAN), 20 (0.543, chest x-ray, <inline-formula><mml:math id="math60" display="inline" overflow="scroll"><mml:mrow><mml:mn>1024</mml:mn><mml:mo>×</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:math></inline-formula>, PGGAN) and 1 (0.497, mammography, DCGAN, <inline-formula><mml:math id="math61" display="inline" overflow="scroll"><mml:mrow><mml:mn>128</mml:mn><mml:mo>×</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math></inline-formula>). This indicates that the <inline-formula><mml:math id="math62" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>FID</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> does not depend on the modality, nor on the pixel resolution of the synthetic images. Further, neither image-to-image translation (e.g. CycleGAN) nor noise-to-image models (e.g., PGGAN, DCGAN, FastGAN) seem to have a particular advantage for achieving higher <inline-formula><mml:math id="math63" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>FID</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> results.</p>
        <p>The flow chart in <xref rid="f8" ref-type="fig">Fig. 8</xref> provides further insight into the comparison between the lower bound <inline-formula><mml:math id="math64" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the model <inline-formula><mml:math id="math65" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The red trend line shows a positive correlation between the <inline-formula><mml:math id="math66" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math67" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which corroborates our previous assumption that a higher model <inline-formula><mml:math id="math68" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is to be expected given a higher lower bound <inline-formula><mml:math id="math69" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Hence, for increased transparency, we motivate further studies to routinely report the lower bound <inline-formula><mml:math id="math70" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the FID ratio <inline-formula><mml:math id="math71" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>FID</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> apart from the model <inline-formula><mml:math id="math72" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The three-channel RGB endoscopic images represented by orange dots have an <inline-formula><mml:math id="math73" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> comparable with their grayscale radiologic counterparts. However, both chest x-ray datasets ChestX-ray14<xref rid="r90" ref-type="bibr"><sup>90</sup></xref> and Node21<xref rid="r89" ref-type="bibr"><sup>89</sup></xref> represented by green dots show a slightly lower <inline-formula><mml:math id="math74" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> than other modalities. The model <inline-formula><mml:math id="math75" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> shows a high variation across models without readily observable dependence on modality, generative model, or image size.</p>
        <fig position="float" id="f8">
          <label>Fig. 8</label>
          <caption>
            <p>Scatter plot illustrating the <inline-formula><mml:math id="math76" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of <italic>medigan</italic>’s models (real-synthetic) compared to the lower bound <inline-formula><mml:math id="math77" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> between two sets of the model’s respective training dataset (real-real). The lower bound can represent an optimally achievable model and, as such, facilitates interpretation. Each model is represented by a dot below its model ID. The dots’ color encoding depicts model modality, where blue: mammography, orange: endoscopy, green: chest x-ray, and pink: brain MRI. The red regression line illustrates the trend across all data points/models.</p>
          </caption>
          <graphic xlink:href="JMI-010-061403-g008" position="float"/>
        </fig>
      </sec>
      <sec id="sec4.2.2">
        <label>4.2.2</label>
        <title>Analysing potential sources of bias in FID</title>
        <p>The popular FID metric is computed based on the features of an Inception classifier (e.g., v1,<xref rid="r48" ref-type="bibr"><sup>48</sup></xref> v3<xref rid="r47" ref-type="bibr"><sup>47</sup></xref>) trained on ImageNet<xref rid="r52" ref-type="bibr"><sup>52</sup></xref>—a database of natural images inherently different from the domain of medical images. This potentially limits the applicability of the FID to medical imaging data. Furthermore, the FID has been observed to vary based on the input image resizing methods and ImageNet backbone feature extraction model types.<xref rid="r31" ref-type="bibr"><sup>31</sup></xref> Based on this, we further hypothesize a susceptibility of the FID to variation due to (a) different backbone feature extractor weights and random seed initializations, (b) different medical and nonmedical backbone model pretraining datasets, (c) different image normalization procedures for real and synthetic dataset, (d) nuances between different frameworks and libraries used for FID calculation, and (f) the dataset sizes used to compute the FID.</p>
        <p>Such variations can obstruct a reliable comparison of synthetic images generated by different generative models. Illustrating the potential of <italic>medigan</italic> to analyze such variations, we report and experiment with the FID. In particular, we subject the FID to variations in (i) the pretraining dataset of its backbone feature extractor and by (ii) testing the effects of image normalization across a set of <italic>medigan</italic> models. We experiment with the Inception v3 model trained on the recent RadImageNet dataset<xref rid="r72" ref-type="bibr"><sup>72</sup></xref> released as radiology-specific alternative to the ImageNet database.<xref rid="r52" ref-type="bibr"><sup>52</sup></xref> The RadImageNet-pretrained Inception v3 model weights we used are available at <ext-link xlink:href="https://github.com/BMEII-AI/RadImageNet" ext-link-type="uri">https://github.com/BMEII-AI/RadImageNet</ext-link>. We further compute the <inline-formula><mml:math id="math78" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="math79" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with and without normalization to analyze the respective impact on results.</p>
        <p>In <xref rid="t009" ref-type="table">Table 4</xref>, the FID results are summarized allowing for cross-analysis between variations due to image normalization and/or due to the pretraining dataset of the FID feature extraction model. We observe generally lower FID values (1.15 to 7.32) for RadImageNet compared to ImageNet as FID model pretraining datasets (52.17 to 225.85). To increase FID comparability, we compute, as before, the FID ratio <inline-formula><mml:math id="math80" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>FID</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The RadImageNet-based model results in notably lower <inline-formula><mml:math id="math81" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>FID</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> values for both normalized and non-normalized images. Notably, an exception to this are models with ID 5 (mammography, <inline-formula><mml:math id="math82" display="inline" overflow="scroll"><mml:mrow><mml:mn>128</mml:mn><mml:mo>×</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math></inline-formula>, DCGAN) and 6 (mammography, <inline-formula><mml:math id="math83" display="inline" overflow="scroll"><mml:mrow><mml:mn>128</mml:mn><mml:mo>×</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math></inline-formula>, WGAN-GP) achieving respective RadImageNet-based <inline-formula><mml:math id="math84" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>FID</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> scores of 0.593 and 0.550. In general, the RadImageNet-based model seems more robust at detecting if two sets of data originate from the same distribution resulting in low <inline-formula><mml:math id="math85" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> values. Overall, for most models, the FID is explained only by a limited amount by the variation in the real dataset and <inline-formula><mml:math id="math86" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>FID</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math></inline-formula> for all ImageNet and RadImageNet-based FIDs. The scatter plot in <xref rid="f9" ref-type="fig">Fig. 9</xref> further compares the RadImagnet-based FID with the ImageNet-FID for the models from <xref rid="t009" ref-type="table">Table 4</xref>. Noticeably, the difference between non-normalized and normalized images is surprisingly high for several models for both ImageNet and RadImageNet FIDs (e.g., models with IDs 6 and 8) while negligible for others (e.g., models with ID 1, 10, 13-16, and 19-21). Another observation is the relatively modest correlation between RadImageNet and ImageNet FID indicated by the slope of the red regression line. Counterexamples for this correlation include model 2 (normalized), which has a low ImageNet-based FID (80.51) compared to a high RadImageNet-based FID (6.19), and model 6 (normalized), which, in contrast, has a high ImageNet-based FID (221.30) and a low RadImageNet-based FID (1.80). With a low ImageNet-based FID (63.99), but surprisingly high RadImageNet-based FID (7.32), model 10 (both normalized and non-normalized) is a further counterexample. The example of model 10 is of particular interest, as it indicates limited applicability of the Radiology-specific RadImageNet-based FID for out-of-domain data, such as three-channel endoscopic images.</p>
        <table-wrap position="float" id="t009">
          <label>Table 4</label>
          <caption>
            <p>Normalized (left) and non-normalized (right) FID scores. This table measures the normalization impact on FID scores based on a promising set of <italic>medigan</italic>’s deep generative models. Synthetic samples were randomly-drawn for each model matching the number of available real samples. The <italic>lower bound</italic>
<inline-formula><mml:math id="math87" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rr</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is computed between a pair of randomly sampled sets of real data (real-real), whereas the <italic>model</italic>
<inline-formula><mml:math id="math88" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is computed between two randomly sampled sets of real and synthetic data (real-syn). The results for model 7 (Flair, T1, T1c, T2) and 21 (T1, T2) are averaged across modalities.</p>
          </caption>
          <!--OASIS TABLE HERE-->
          <table frame="hsides" rules="groups">
            <colgroup>
              <col/>
              <col/>
              <col/>
              <col/>
              <col/>
              <col/>
              <col/>
              <col/>
              <col/>
              <col/>
              <col/>
              <col/>
              <col/>
              <col/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="2" colspan="2" valign="top"> </th>
                <th colspan="6" valign="top">Normalized images</th>
                <th colspan="6" valign="top">Non-Normalized images</th>
              </tr>
              <tr>
                <th colspan="3" valign="top">
                  <inline-formula>
                    <mml:math id="math89" display="inline" overflow="scroll">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>FID</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mtext>ImageNet</mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:math>
                  </inline-formula>
                  <xref rid="r47" ref-type="bibr">
                    <sup>47</sup>
                  </xref>
                  <sup>,</sup>
                  <xref rid="r52" ref-type="bibr">
                    <sup>52</sup>
                  </xref>
                </th>
                <th colspan="3" valign="top">
                  <inline-formula>
                    <mml:math id="math90" display="inline" overflow="scroll">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>FID</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mtext>RadImageNet</mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:math>
                  </inline-formula>
                  <xref rid="r72" ref-type="bibr">
                    <sup>72</sup>
                  </xref>
                </th>
                <th colspan="3" valign="top">
                  <inline-formula>
                    <mml:math id="math91" display="inline" overflow="scroll">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>FID</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mtext>ImageNet</mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:math>
                  </inline-formula>
                  <xref rid="r47" ref-type="bibr">
                    <sup>47</sup>
                  </xref>
                  <sup>,</sup>
                  <xref rid="r52" ref-type="bibr">
                    <sup>52</sup>
                  </xref>
                </th>
                <th colspan="3" valign="top">
                  <inline-formula>
                    <mml:math id="math92" display="inline" overflow="scroll">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>FID</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mtext>RadImageNet</mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:math>
                  </inline-formula>
                  <xref rid="r72" ref-type="bibr">
                    <sup>72</sup>
                  </xref>
                </th>
              </tr>
              <tr>
                <th valign="top">ID</th>
                <th valign="top">Dataset</th>
                <th valign="top">real-real</th>
                <th valign="top">real-syn</th>
                <th valign="top">
                  <inline-formula>
                    <mml:math id="math93" display="inline" overflow="scroll">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mi>r</mml:mi>
                          <mml:mi>FID</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:math>
                  </inline-formula>
                </th>
                <th valign="top">real-real</th>
                <th valign="top">real-syn</th>
                <th valign="top">
                  <inline-formula>
                    <mml:math id="math94" display="inline" overflow="scroll">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mi>r</mml:mi>
                          <mml:mi>FID</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:math>
                  </inline-formula>
                </th>
                <th valign="top">real-real</th>
                <th valign="top">real-syn</th>
                <th valign="top">
                  <inline-formula>
                    <mml:math id="math95" display="inline" overflow="scroll">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mi>r</mml:mi>
                          <mml:mi>FID</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:math>
                  </inline-formula>
                </th>
                <th valign="top">real-real</th>
                <th valign="top">real-syn</th>
                <th valign="top">
                  <inline-formula>
                    <mml:math id="math96" display="inline" overflow="scroll">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mi>r</mml:mi>
                          <mml:mi>FID</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:math>
                  </inline-formula>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td> 1</td>
                <td>Inbreast</td>
                <td>33.61</td>
                <td>67.60</td>
                <td>0.497</td>
                <td>0.25</td>
                <td>1.27</td>
                <td>0.197</td>
                <td>28.59</td>
                <td>66.76</td>
                <td>0.428</td>
                <td>0.29</td>
                <td>1.15</td>
                <td>0.252</td>
              </tr>
              <tr>
                <td> 2</td>
                <td>Optimam</td>
                <td>28.85</td>
                <td>80.51</td>
                <td>0.358</td>
                <td>0.22</td>
                <td>6.19</td>
                <td>0.036</td>
                <td>28.75</td>
                <td>77.95</td>
                <td>0.369</td>
                <td>0.33</td>
                <td>4.11</td>
                <td>0.080</td>
              </tr>
              <tr>
                <td> 3</td>
                <td>BCDR</td>
                <td>65.94</td>
                <td>150.16</td>
                <td>0.439</td>
                <td>0.80</td>
                <td>3.00</td>
                <td>0.265</td>
                <td>66.25</td>
                <td>149.33</td>
                <td>0.444</td>
                <td>0.80</td>
                <td>3.10</td>
                <td>0.259</td>
              </tr>
              <tr>
                <td> 5</td>
                <td>BCDR</td>
                <td>68.22</td>
                <td>180.04</td>
                <td>0.379</td>
                <td>0.99</td>
                <td>1.67</td>
                <td>0.593</td>
                <td>64.45</td>
                <td>174.38</td>
                <td>0.370</td>
                <td>0.87</td>
                <td>4.04</td>
                <td>0.215</td>
              </tr>
              <tr>
                <td> 6</td>
                <td>BCDR</td>
                <td>68.22</td>
                <td>221.30</td>
                <td>0.308</td>
                <td>0.99</td>
                <td>1.80</td>
                <td>0.550</td>
                <td>64.45</td>
                <td>206.57</td>
                <td>0.312</td>
                <td>0.87</td>
                <td>2.95</td>
                <td>0.295</td>
              </tr>
              <tr>
                <td> 7</td>
                <td>BRATS 2018</td>
                <td>30.73</td>
                <td>140.02</td>
                <td>0.219</td>
                <td>0.07</td>
                <td>5.31</td>
                <td>0.012</td>
                <td>30.73</td>
                <td>144.00</td>
                <td>0.215</td>
                <td>0.07</td>
                <td>6.53</td>
                <td>0.010</td>
              </tr>
              <tr>
                <td> 8</td>
                <td>CBIS-DDSM</td>
                <td>37.56</td>
                <td>137.75</td>
                <td>0.272</td>
                <td>0.46</td>
                <td>3.05</td>
                <td>0.151</td>
                <td>32.06</td>
                <td>91.09</td>
                <td>0.352</td>
                <td>0.36</td>
                <td>6.58</td>
                <td>0.055</td>
              </tr>
              <tr>
                <td>10</td>
                <td>HyperKvasir</td>
                <td>43.31</td>
                <td>63.99</td>
                <td>0.677</td>
                <td>0.11</td>
                <td>7.32</td>
                <td>0.015</td>
                <td>43.31</td>
                <td>64.01</td>
                <td>0.677</td>
                <td>0.11</td>
                <td>7.33</td>
                <td>0.015</td>
              </tr>
              <tr>
                <td>12</td>
                <td>BCDR</td>
                <td>68.22</td>
                <td>205.29</td>
                <td>0.332</td>
                <td>0.99</td>
                <td>5.69</td>
                <td>0.080</td>
                <td>64.45</td>
                <td>199.50</td>
                <td>0.323</td>
                <td>0.87</td>
                <td>4.25</td>
                <td>0.205</td>
              </tr>
              <tr>
                <td>13</td>
                <td>OPTIMAM</td>
                <td>65.75</td>
                <td>101.01</td>
                <td>0.650</td>
                <td>0.17</td>
                <td>1.14</td>
                <td>0.153</td>
                <td>65.83</td>
                <td>101.15</td>
                <td>0.651</td>
                <td>0.18</td>
                <td>1.10</td>
                <td>0.163</td>
              </tr>
              <tr>
                <td>14</td>
                <td>OPTIMAM</td>
                <td>41.61</td>
                <td>73.77</td>
                <td>0.564</td>
                <td>0.16</td>
                <td>0.83</td>
                <td>0.190</td>
                <td>41.71</td>
                <td>74.03</td>
                <td>0.563</td>
                <td>0.15</td>
                <td>0.81</td>
                <td>0.184</td>
              </tr>
              <tr>
                <td>15</td>
                <td>CSAW</td>
                <td>74.96</td>
                <td>162.67</td>
                <td>0.461</td>
                <td>0.31</td>
                <td>4.07</td>
                <td>0.076</td>
                <td>73.62</td>
                <td>165.53</td>
                <td>0.445</td>
                <td>0.19</td>
                <td>3.71</td>
                <td>0.051</td>
              </tr>
              <tr>
                <td>16</td>
                <td>CSAW</td>
                <td>42.68</td>
                <td>98.38</td>
                <td>0.439</td>
                <td>0.38</td>
                <td>2.71</td>
                <td>0.142</td>
                <td>42.50</td>
                <td>99.81</td>
                <td>0.426</td>
                <td>0.22</td>
                <td>2.82</td>
                <td>0.077</td>
              </tr>
              <tr>
                <td>19</td>
                <td>ChestX-ray14</td>
                <td>28.75</td>
                <td>96.74</td>
                <td>0.297</td>
                <td>0.19</td>
                <td>0.77</td>
                <td>0.243</td>
                <td>28.75</td>
                <td>96.78</td>
                <td>0.297</td>
                <td>0.19</td>
                <td>0.66</td>
                <td>0.286</td>
              </tr>
              <tr>
                <td>20</td>
                <td>ChestX-ray14</td>
                <td>28.33</td>
                <td>52.17</td>
                <td>0.543</td>
                <td>0.20</td>
                <td>2.83</td>
                <td>0.071</td>
                <td>28.33</td>
                <td>52.38</td>
                <td>0.541</td>
                <td>0.20</td>
                <td>2.59</td>
                <td>0.077</td>
              </tr>
              <tr>
                <td>21</td>
                <td>CrossMoDA</td>
                <td>24.41</td>
                <td>59.49</td>
                <td>0.410</td>
                <td>0.02</td>
                <td>1.45</td>
                <td>0.014</td>
                <td>24.41</td>
                <td>60.11</td>
                <td>0.406</td>
                <td>0.02</td>
                <td>1.40</td>
                <td>0.014</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <fig position="float" id="f9">
          <label>Fig. 9</label>
          <caption>
            <p>Scatter plot demonstrating the <inline-formula><mml:math id="math97" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (real-synthetic) of <italic>medigan</italic> models from <xref rid="t010" ref-type="table">Table 4</xref>. The <inline-formula><mml:math id="math98" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>FID</mml:mi><mml:mi>rs</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is based on the features of two different inception classifiers,<xref rid="r47" ref-type="bibr"><sup>47</sup></xref> one trained on ImageNet<xref rid="r52" ref-type="bibr"><sup>52</sup></xref> (<inline-formula><mml:math id="math99" display="inline" overflow="scroll"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula>-axis) and the other trained on RadImageNet<xref rid="r72" ref-type="bibr"><sup>72</sup></xref> (<inline-formula><mml:math id="math100" display="inline" overflow="scroll"><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula>-axis). Each model is represented by a dot below its model ID. A black dot indicates an FID calculated from normalized (<inline-formula><mml:math id="math101" display="inline" overflow="scroll"><mml:mrow><mml:mtext>Norm</mml:mtext><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>) images, e.g., with pixel values scaled between 0 and 1, as opposed to a blue dot indicating an FID calculated from images without previous normalization. The dots that correspond to the same model IDs (normalized and non-normalized) are connected via black lines. The red regression line illustrates the trend across all data points.</p>
          </caption>
          <graphic xlink:href="JMI-010-061403-g009" position="float"/>
        </fig>
        <p>Given the demonstrated high impact of backbone model training set and image normalization on FID, it is to be recommended that studies specify the exact model used for FID calculation and any applied data preprocessing and normalization steps. Further, where possible, reporting the RadImageNet-based FID allows for reporting a radiology domain-specific FID. The latter is seemingly less susceptible to variation in the real datasets than the ImageNet-based FID while also being capable of capturing other, potentially complementary, patterns in the data.</p>
      </sec>
    </sec>
    <sec id="sec4.3">
      <label>4.3</label>
      <title>Improving Clinical Medical Image Analysis</title>
      <p>A high-impact clinical application of synthetic data is the improvement of clinical downstream task performance such as classification, detection, segmentation, or treatment response estimation. This can be achieved by using image synthesis for data augmentation, domain adaptation, and data curation (e.g., artifact removal, noise reduction, super-resolution)<xref rid="r7" ref-type="bibr"><sup>7</sup></xref><sup>,</sup><xref rid="r63" ref-type="bibr"><sup>63</sup></xref> to enhance the performance of clinical decision support systems such as computer-aided diagnosis (CADx) and detection (CADe) software.</p>
      <p>In <xref rid="t010" ref-type="table">Table 5</xref>, the capability of improving the clinical downstream task performance is demonstrated for various <italic>medigan</italic> models and modalities. Downstream task models trained on a combination of real and synthetic imaging data achieve promising results surpassing the alternative results achieved from training only on real data. The results are taken from the respective publications<xref rid="r11" ref-type="bibr"><sup>11</sup></xref><sup>,</sup><xref rid="r14" ref-type="bibr"><sup>14</sup></xref><sup>,</sup><xref rid="r50" ref-type="bibr"><sup>50</sup></xref><sup>,</sup><xref rid="r84" ref-type="bibr"><sup>84</sup></xref> and indicate that image synthesis can further improve the promising performance demonstrated by deep learning-based CADx and CADe systems, e.g., in mammography<xref rid="r96" ref-type="bibr"><sup>96</sup></xref> and brain MRI.<xref rid="r85" ref-type="bibr"><sup>85</sup></xref> For downstream task evaluation, we generally note the importance of avoiding data leakage between training, validation, and test sets by training the generative model either on only the dataset partition used to train the respective downstream task model (e.g., IDs 2, 3, 7, 14, 15) or to train the generative models on an entirely different dataset (e.g., IDs 5, 6).</p>
      <table-wrap position="float" id="t010">
        <label>Table 5</label>
        <caption>
          <p>Examples of the impact of synthetic data generated by <italic>medigan</italic> models on downstream task performance. Based on real test data, we compare the performance metrics of a model trained <italic>only on real</italic> data with a model trained on real data <italic>augmented with synthetic</italic> data. The metrics are taken from the respective publications describing the models.</p>
        </caption>
        <!--OASIS TABLE HERE-->
        <table frame="hsides" rules="groups">
          <colgroup>
            <col/>
            <col/>
            <col/>
            <col/>
            <col/>
            <col/>
          </colgroup>
          <thead>
            <tr>
              <th valign="top">ID</th>
              <th valign="top">Test dataset</th>
              <th valign="top">Task</th>
              <th valign="top">Metric</th>
              <th valign="top">Trained on real</th>
              <th valign="top">Real + synthetic</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td> 2</td>
              <td>OPTIMAM</td>
              <td>Mammogram patch classification<xref rid="r11" ref-type="bibr"><sup>11</sup></xref></td>
              <td>F1</td>
              <td>0.90</td>
              <td>0.96</td>
            </tr>
            <tr>
              <td> 3</td>
              <td>BCDR</td>
              <td>Mammogram mass detection<xref rid="r50" ref-type="bibr"><sup>50</sup></xref></td>
              <td>FROC AUC</td>
              <td>0.83</td>
              <td>0.89</td>
            </tr>
            <tr>
              <td> 5</td>
              <td>BCDR</td>
              <td>Mammogram patch classification<xref rid="r14" ref-type="bibr"><sup>14</sup></xref></td>
              <td>F1</td>
              <td>0.891</td>
              <td>0.920</td>
            </tr>
            <tr>
              <td> 5</td>
              <td>BCDR</td>
              <td>Mammogram patch classification<xref rid="r14" ref-type="bibr"><sup>14</sup></xref></td>
              <td>AUROC</td>
              <td>0.928</td>
              <td>0.959</td>
            </tr>
            <tr>
              <td> 5</td>
              <td>BCDR</td>
              <td>Mammogram patch classification<xref rid="r14" ref-type="bibr"><sup>14</sup></xref></td>
              <td>AUPRC</td>
              <td>0.986</td>
              <td>0.992</td>
            </tr>
            <tr>
              <td> 6</td>
              <td>BCDR</td>
              <td>Mammogram patch classification<xref rid="r14" ref-type="bibr"><sup>14</sup></xref></td>
              <td>F1</td>
              <td>0.891</td>
              <td>0.969</td>
            </tr>
            <tr>
              <td> 6</td>
              <td>BCDR</td>
              <td>Mammogram patch classification<xref rid="r14" ref-type="bibr"><sup>14</sup></xref></td>
              <td>AUROC</td>
              <td>0.928</td>
              <td>0.978</td>
            </tr>
            <tr>
              <td> 6</td>
              <td>BCDR</td>
              <td>Mammogram patch classification<xref rid="r14" ref-type="bibr"><sup>14</sup></xref></td>
              <td>AUPRC</td>
              <td>0.986</td>
              <td>0.996</td>
            </tr>
            <tr>
              <td> 7</td>
              <td>BRATS 2018</td>
              <td>Brain tumor segmentation<xref rid="r84" ref-type="bibr"><sup>84</sup></xref></td>
              <td>Dice</td>
              <td>0.796</td>
              <td>0.814</td>
            </tr>
            <tr>
              <td>14</td>
              <td>OPTIMAM</td>
              <td>Mammogram mass detection<xref rid="r50" ref-type="bibr"><sup>50</sup></xref></td>
              <td>FROC AUC</td>
              <td>0.83</td>
              <td>0.85</td>
            </tr>
            <tr>
              <td>15</td>
              <td>OPTIMAM</td>
              <td>Mammogram mass detection<xref rid="r50" ref-type="bibr"><sup>50</sup></xref></td>
              <td>FROC AUC</td>
              <td>0.83</td>
              <td>0.85</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The approaches displayed in <xref rid="t011" ref-type="table">Table 6</xref> represent the application, where synthetic data is used instead of real data to train downstream task models. Despite an observable performance decrease when training on synthetic data only, the results<xref rid="r51" ref-type="bibr"><sup>51</sup></xref><sup>,</sup><xref rid="r91" ref-type="bibr"><sup>91</sup></xref><sup>,</sup><xref rid="r92" ref-type="bibr"><sup>92</sup></xref> demonstrate the usefulness of synthetic data if none or only limited real training data is available or shareable. For example, if labels or annotations in a target domain are scarce but present in a source domain, a generative model can translate annotated data from the source domain to the target domain to enable supervised training of downstream task models.<xref rid="r92" ref-type="bibr"><sup>92</sup></xref><sup>,</sup><xref rid="r93" ref-type="bibr"><sup>93</sup></xref></p>
      <table-wrap position="float" id="t011">
        <label>Table 6</label>
        <caption>
          <p>Examples of the impact of synthetic data generated by <italic>medigan</italic> models on downstream task performance. Based on real test data, we compare the performance metrics of a model trained <italic>only on real</italic> data with a model trained <italic>only on synthetic</italic> data. The metrics are taken from the respective publications describing the models. <italic>n.a.</italic> refers to the case where only synthetic data can be used, as no annotated real training data is available.</p>
        </caption>
        <!--OASIS TABLE HERE-->
        <table frame="hsides" rules="groups">
          <colgroup>
            <col/>
            <col/>
            <col/>
            <col/>
            <col/>
            <col/>
          </colgroup>
          <thead>
            <tr>
              <th valign="top">ID</th>
              <th valign="top">Test dataset</th>
              <th valign="top">Task</th>
              <th valign="top">Metric</th>
              <th valign="top">Trained on real</th>
              <th valign="top">Trained on synthetic</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td> 4</td>
              <td>BCDR</td>
              <td>Mammogram mass segmentation</td>
              <td>Dice</td>
              <td>0.865</td>
              <td>0.737</td>
            </tr>
            <tr>
              <td>11</td>
              <td>HyperKvasir</td>
              <td>Polyp segmentation<xref rid="r51" ref-type="bibr"><sup>51</sup></xref></td>
              <td>Dice loss</td>
              <td>0.112</td>
              <td>0.137</td>
            </tr>
            <tr>
              <td>11</td>
              <td>HyperKvasir</td>
              <td>Polyp segmentation<xref rid="r51" ref-type="bibr"><sup>51</sup></xref></td>
              <td>IoU</td>
              <td>0.827</td>
              <td>0.798</td>
            </tr>
            <tr>
              <td>11</td>
              <td>HyperKvasir</td>
              <td>Polyp segmentation<xref rid="r51" ref-type="bibr"><sup>51</sup></xref></td>
              <td>F-Score</td>
              <td>0.888</td>
              <td>0.863</td>
            </tr>
            <tr>
              <td>20</td>
              <td>ChestX-ray14</td>
              <td>Lung disease classification<xref rid="r91" ref-type="bibr"><sup>91</sup></xref></td>
              <td>AUROC</td>
              <td>0.947</td>
              <td>0.878</td>
            </tr>
            <tr>
              <td>21</td>
              <td>CrossMoDA</td>
              <td>Brain tumor segmentation<xref rid="r92" ref-type="bibr"><sup>92</sup></xref></td>
              <td>Dice</td>
              <td>n.a.</td>
              <td>0.712</td>
            </tr>
            <tr>
              <td>21</td>
              <td>CrossMoDA</td>
              <td>Cochlea segmentation<xref rid="r92" ref-type="bibr"><sup>92</sup></xref></td>
              <td>Dice</td>
              <td>n.a.</td>
              <td>0.478</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
  </sec>
  <sec id="sec5">
    <label>5</label>
    <title>Discussion and Future Work</title>
    <p>In this work, we introduced <italic>medigan</italic>, an open-source Python library, which allows one to share pretrained generative models for synthetic medical image generation. The package is easily integrable into other packages and tools, including commercial ones. Synthetic data can enhance the performance, capabilities, and robustness of data-hungry deep learning models as well as to mitigate common issues such as domain shift, data scarcity, class imbalance, and data privacy restrictions. Training one’s own generative network can be complex and expensive since it requires a considerable amount of time, effort, specific dedicated hardware, carbon emissions, as well as knowledge and applied skills in generative AI. An alternative and complementary solution is the distribution of pretrained generative models to allow their reuse by AI researchers and engineers worldwide.</p>
    <p><italic>medigan</italic> can help to reduce the time to run synthetic data experiments and can readily be added as a component, e.g., as a dataloader as discussed in Sec. <xref rid="sec3.5.2" ref-type="sec">3.5.2</xref>, in AI training pipelines. As such, the generated data can be used to improve supervised learning models as described in Sec. <xref rid="sec4.3" ref-type="sec">4.3</xref> via training or fine-tuning but can also serve as plug-and-play data source for self/semisupervised learning, e.g., to pretrain clinical downstream task models.</p>
    <p>Furthermore, studies that use additional synthetic training data for training deep learning models often do not report all the specifics about their underlying generative model.<xref rid="r7" ref-type="bibr"><sup>7</sup></xref><sup>,</sup><xref rid="r75" ref-type="bibr"><sup>75</sup></xref> Within <italic>medigan</italic>, each generative model is documented, openly accessible, and reusable. This increases the reproducibility of studies that use synthetic data and makes it more transparent where the data or parts thereof originated from. This can help to achieve the traceability objectives outlined in the FUTURE-AI consensus guiding principles toward AI trustworthiness in medical imaging.<xref rid="r75" ref-type="bibr"><sup>75</sup></xref>
<italic>medigan</italic>’s currently 21 generative models are illustrated in <xref rid="t005" ref-type="table">Table 3</xref> and developed and validated by AI researchers and/or specialized medical doctors. Furthermore, each model contains traceable<xref rid="r75" ref-type="bibr"><sup>75</sup></xref> and version-controlled metadata in medigan’s <italic>global.json</italic> file, as outlined in Sec. <xref rid="sec3.3" ref-type="sec">3.3</xref>. The searchable (see Sec. <xref rid="sec3.4" ref-type="sec">3.4</xref>) metadata allows one to choose a suitable model for a user’s task at hand and includes, among others, the dataset used during the training process, the trained date, publication, modality, input arguments, model types, and comparable evaluation metrics.</p>
    <p>To assess model suitability, users are recommended to first (i) ensure the compatibility between their planned downstream task (e.g., mammogram region-of-interest classification) and a candidate medigan model (e.g., mammogram region-of-interest generator). Second, (ii) a user’s real (test) data and the model’s synthetic data should be compatible corresponding, for instance, in domain, organ, or disease manifestation. If the awareness of the domain shifts between real and synthetic data remains limited after this qualitative analysis, (iii) a quantitative assessment (e.g., via FID) is recommended. Finally, (iv) it is to be assessed if a downstream task improvement is plausible. This depends, among others, on the tested scenario and the task at hand, but also on the amount, domain, task specificity and quality of the available real data, and the generative model’s capabilities as indicated by its reported evaluation metrics from previous studies. If a positive impact of synthetic data on downstream task performance is plausible, users are recommended to proceed toward empirical verification.</p>
    <p>The exploration and multimodel evaluation of the properties of generative models and synthetic data is a further application of <italic>medigan</italic>. <italic>medigan</italic>’s visualization tool (see Sec. <xref rid="sec3.6" ref-type="sec">3.6</xref>) intuitively allows the user to explore and adjust the input latent vector of generative models to visually evaluate, e.g., its inherent diversity and condition adherence<xref rid="r7" ref-type="bibr"><sup>7</sup></xref> (i.e., how well does a given mask or label fit the generated image). The evaluation of synthetic data by human experts, such as radiologists, is a costly and time-consuming task, which motivates the usage of automated metric-based evaluation such as the FID. Our multimodel analysis reveals sources of bias in FID reporting. We show the susceptibility of FID to vary substantially based on changes in input image normalization or in the choice of the pretraining dataset of the FID feature extractor. This finding highlights the need to report the specific models, preprocessing, and implementations used to compute the FID alongside the FID ratio <inline-formula><mml:math id="math102" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>FID</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> proposed in Sec. <xref rid="sec4.2.1" ref-type="sec">4.2.1</xref> to account for the variation immanent in the real dataset. With <italic>medigan</italic> model experiments demonstrably leading to insights in synthetic data evaluation, future research can use <italic>medigan</italic> as a tool to accelerate, test, analyze, and compare new synthetic data and generative model evaluation and exploration techniques.</p>
    <sec id="sec5.1">
      <label>5.1</label>
      <title>Legal Frameworks for Sharing of Synthetic and Real Patient Data</title>
      <p>Many countries have enacted regulations that govern the use and sharing of data related to individuals. The two most recognized legal frameworks are the Health Insurance Portability and Accountability Act (HIPAA)<xref rid="r97" ref-type="bibr"><sup>97</sup></xref> from the United States (U.S.) and the General Data Protection Regulation (GDPR)<xref rid="r98" ref-type="bibr"><sup>98</sup></xref> from the European Union (E.U.). These regulations govern the use and disclosure of individuals’ protected health information (PHI) and assures individuals’ data is protected while allowing use for providing quality patient care.<xref rid="r99" ref-type="bibr"><sup>99</sup></xref><named-content content-type="online"><xref rid="r100" ref-type="bibr"/><xref rid="r101" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r102" ref-type="bibr"><sup>102</sup></xref></p>
      <p>Conceptually, synthetic data is not real data about any particular individual and conversely to real data, synthetic data can be generated at high volumes and potentially shared without restriction. In this sense, under both GDPR and HIPAA regulation, the rules govern the use of real data for the generation and evaluation of synthetic datasets, as well as the sharing of the original dataset. However, once fully synthetic data is generated, this new dataset falls outside the scope of the current regulations based on the argument that there is no direct correlation between the original subjects and the synthetic subjects. A common interpretation is that as long as the real data remains in a secure environment during the generation of synthetic data, there is little to no risk to the original subjects.<xref rid="r103" ref-type="bibr"><sup>103</sup></xref></p>
      <p>As a consequence, the use of synthetic data can help prevent researchers from inadvertently using and possibly exposing patients identifiable data. Synthetic data can also lessen the controls imposed by Institutional Review Boards (IRBs) and based on international regulations by ensuring data is never mapped to real individuals.<xref rid="r104" ref-type="bibr"><sup>104</sup></xref> There are multiple methods of generating synthetic data, some of which include building models from real data, which can create a set statistically similar to real data. How similar the synthetic data is to real-world data often defines its “utility,” which will vary depending on the synthesis methods used and the needs of the study at hand. If the utility of the synthetic data is high enough then evaluation results are expected to be similar to those that use real data.<xref rid="r103" ref-type="bibr"><sup>103</sup></xref> Being built based on real data, a common concern is patient reidentification and leaking of patient-specific features in generative models.<xref rid="r7" ref-type="bibr"><sup>7</sup></xref><sup>,</sup><xref rid="r15" ref-type="bibr"><sup>15</sup></xref> Despite the arguably permissive aforementioned regulations, deidentification<xref rid="r63" ref-type="bibr"><sup>63</sup></xref> of the training data prior to generative model training is to be recommended. This can minimize the possibility of generative models leaking sensitive patient data during inference and after sharing. A further recommended and mathematically-proven tool for privacy preservation is differential privacy (DP).<xref rid="r95" ref-type="bibr"><sup>95</sup></xref> DP can be included in the training of deep generative model, among other setups, by adding DP noise to the gradients.</p>
    </sec>
    <sec id="sec5.2">
      <label>5.2</label>
      <title>Expansion of Available Models</title>
      <p>In the future, further generative models across medical imaging disciplines, modalities, and organs can be integrated into medigan. The capabilities of additional models can range from privatising or translating the user’s data from one domain to another, balancing or debiasing imbalanced datasets, reconstructing, denoising or removing artifacts in medical images, or resizing images, e.g., using image super-resolution techniques. Despite <italic>medigan</italic>’s current focus on models based on GANs,<xref rid="r16" ref-type="bibr"><sup>16</sup></xref> the inclusion of different additional types of generative models is desirable and will enable insightful comparisons. In particular, this is to be further emphasized considering the recent successes of diffusion models,<xref rid="r25" ref-type="bibr"><sup>25</sup></xref><named-content content-type="online"><xref rid="r26" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r27" ref-type="bibr"><sup>27</sup></xref> variational autoencoders,<xref rid="r21" ref-type="bibr"><sup>21</sup></xref> and normalizing flows<xref rid="r22" ref-type="bibr"><sup>22</sup></xref><named-content content-type="online"><xref rid="r23" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r24" ref-type="bibr"><sup>24</sup></xref> in the computer vision and medical imaging<xref rid="r105" ref-type="bibr"><sup>105</sup></xref><named-content content-type="online"><xref rid="r106" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r107" ref-type="bibr"><sup>107</sup></xref> domains. Before integrating and testing a new model via the pipeline described in Sec. 3.8, we assess whether a model is to become a candidate for inclusion into medigan. This threefold assessment is based on the SynTRUST framework<xref rid="r7" ref-type="bibr"><sup>7</sup></xref> and reviews whether (1) the model is well-documented (e.g., in a respective publication), (2) the model or its synthetic data is applicable to a task of clinical relevance, and (3) whether the model has been methodically validated.</p>
    </sec>
    <sec id="sec5.3">
      <label>5.3</label>
      <title>Synthetic DICOM Generation</title>
      <p>Since the dominant data format used for medical imaging is Digital Imaging and Communications in Medicine (DICOM), we plan to enhance medigan by integrating the generation of DICOM compliant files. DICOM consists of two main components, pixel data and the DICOM header. The latter can be described as an embedded dataset rich with information related to the pixel data such as the image sequence, patient, physicians, institutions, treatments, observations, and equipment.<xref rid="r63" ref-type="bibr"><sup>63</sup></xref> Future work will explore combining our GAN generated images with synthetic DICOM headers. The latter will be created from the same training images from which the <italic>medigan</italic> models are trained to create synthetic DICOM data with high statistical similarity to real-world data. In this regard, a key research focus will be the creation of an appropriate and DICOM-compliant description of the image acquisition protocol for a synthetic image. The design and development of an open-source software package for generating DICOM files based on synthesized DICOM headers associated to (synthetic) images will extend prior work<xref rid="r108" ref-type="bibr"><sup>108</sup></xref> that demonstrated the generation of synthetic headers for the purpose of evaluating deidentification methods.</p>
    </sec>
  </sec>
  <sec id="sec6">
    <label>6</label>
    <title>Conclusion</title>
    <p>We presented the open-source <italic>medigan</italic> package, which helps research in medical imaging to rapidly create synthetic datasets for a multitude of purposes such as AI model training and benchmarking, data augmentation, domain adaptation, and intercentre data sharing. <italic>medigan</italic> provides simple functions and interfaces for users, allowing one to automate generative model search, ranking, synthetic data generation, and model contribution. By reuse and dissemination of existing generative models in the medical imaging community, <italic>medigan</italic> allows researchers to speed up their experiments with synthetic data in a reproducible and transparent manner.</p>
    <p>We discuss three key applications of <italic>medigan</italic>, which include (i) sharing of restricted datasets, (ii) improving clinical downstream task performance, and (iii) analyzing the properties of generative models, synthetic data, and associated evaluation metrics. Ultimately, the aim of <italic>medigan</italic> is to contribute to benefiting patients and clinicians, e.g., by increasing the performance and robustness of AI models in clinical decision support systems.</p>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>We would like to thank all model contributors, such as Alyafi et al.,<xref rid="r11" ref-type="bibr"><sup>11</sup></xref> Szafranowska et al.,<xref rid="r14" ref-type="bibr"><sup>14</sup></xref> Thambawita et al.,<xref rid="r51" ref-type="bibr"><sup>51</sup></xref> Kim et al.,<xref rid="r84" ref-type="bibr"><sup>84</sup></xref> Segal et al.,<xref rid="r91" ref-type="bibr"><sup>91</sup></xref> Joshi et al.,<xref rid="r92" ref-type="bibr"><sup>92</sup></xref> and Garrucho et al.<xref rid="r50" ref-type="bibr"><sup>50</sup></xref> This project has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreements No. 952103 and No. 101057699. Eloy García and Kaisar Kushibar hold the Juan de la Cierva fellowship from the Ministry of Science and Innovation of Spain with reference numbers FJC2019-040039-I and FJC2021-047659-I, respectively.</p>
  </ack>
  <bio id="d28398e7033" content-type="general">
    <p>Biographies of the authors are not available.</p>
  </bio>
  <notes notes-type="conflict-of-interest">
    <title>Disclosures</title>
    <p>The authors have no conflicts of interest to declare that are relevant to the content of this article.</p>
  </notes>
  <sec>
    <title>Data, Materials, and Code Availability</title>
    <p><italic>medigan</italic> is a free Python (v3.6+) package published under the MIT license and distributed via the Python Package Index (<ext-link xlink:href="https://pypi.org/project/medigan/" ext-link-type="uri">https://pypi.org/project/medigan/</ext-link>). The package is open-source and invites the community to contribute on GitHub (<ext-link xlink:href="https://github.com/RichardObi/medigan" ext-link-type="uri">https://github.com/RichardObi/medigan</ext-link>). A detailed documentation of <italic>medigan</italic> is available (<ext-link xlink:href="https://medigan.readthedocs.io/en/latest/" ext-link-type="uri">https://medigan.readthedocs.io/en/latest/</ext-link>) that contains installation instructions, the API reference, a general description, code examples, a testing guide, a model contribution user guide, and documentation of the generative models available in medigan.</p>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="r1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin-Isla</surname><given-names>C.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Image-based cardiac diagnosis with machine learning: a review</article-title>,” <source>Front. Cardiovasc. Med.</source>
<volume>7</volume>, <fpage>1</fpage> (<year>2020</year>).<pub-id pub-id-type="doi">10.3389/fcvm.2020.00001</pub-id><pub-id pub-id-type="pmid">32039241</pub-id></mixed-citation>
    </ref>
    <ref id="r2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aggarwal</surname><given-names>R.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis</article-title>,” <source>NPJ Digital Med.</source>
<volume>4</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>23</lpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1038/s41746-021-00438-z</pub-id></mixed-citation>
    </ref>
    <ref id="r3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>X.</given-names></name><etal>et al.</etal></person-group>, “<article-title>A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis</article-title>,” <source>Lancet Digital Health</source>
<volume>1</volume>(<issue>6</issue>), <fpage>e271</fpage>–<lpage>e297</lpage> (<year>2019</year>).<pub-id pub-id-type="doi">10.1016/S2589-7500(19)30123-2</pub-id><pub-id pub-id-type="pmid">33323251</pub-id></mixed-citation>
    </ref>
    <ref id="r4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlemper</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, “<article-title>A deep cascade of convolutional neural networks for MR image reconstruction</article-title>,” <source>Lect. Notes Comput. Sci.</source>
<volume>10265</volume>, <fpage>647</fpage>–<lpage>658</lpage> (<year>2017</year>).<pub-id pub-id-type="coden">LNCSD9</pub-id><issn>0302-9743</issn><pub-id pub-id-type="doi">10.1007/978-3-319-59050-9_51</pub-id></mixed-citation>
    </ref>
    <ref id="r5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahishakiye</surname><given-names>E.</given-names></name><etal>et al.</etal></person-group>, “<article-title>A survey on deep learning in medical image reconstruction</article-title>,” <source>Intell. Med.</source>
<volume>1</volume>(<issue>03</issue>), <fpage>118</fpage>–<lpage>127</lpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1016/j.imed.2021.03.003</pub-id></mixed-citation>
    </ref>
    <ref id="r6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tajbakhsh</surname><given-names>N.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Embracing imperfect datasets: a review of deep learning solutions for medical image segmentation</article-title>,” <source>Med. Image Anal.</source>
<volume>63</volume>, <fpage>101693</fpage> (<year>2020</year>).<pub-id pub-id-type="doi">10.1016/j.media.2020.101693</pub-id><pub-id pub-id-type="pmid">32289663</pub-id></mixed-citation>
    </ref>
    <ref id="r7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osuala</surname><given-names>R.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Data synthesis and adversarial networks: a review and meta-analysis in cancer imaging</article-title>,” <source>Med. Image Anal.</source>
<volume>84</volume>, <fpage>102704</fpage> (<year>2023</year>).<pub-id pub-id-type="doi">10.1016/j.media.2022.102704</pub-id><pub-id pub-id-type="pmid">36473414</pub-id></mixed-citation>
    </ref>
    <ref id="r8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>C.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Predicting treatment response from longitudinal images using multi-task deep learning</article-title>,” <source>Nat. Commun.</source>
<volume>12</volume>, <fpage>1851</fpage> (<year>2021</year>).<pub-id pub-id-type="coden">NCAOBW</pub-id><issn>2041-1723</issn><pub-id pub-id-type="doi">10.1038/s41467-021-22188-y</pub-id><pub-id pub-id-type="pmid">33767170</pub-id></mixed-citation>
    </ref>
    <ref id="r9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname><given-names>W. L.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Artificial intelligence in cancer imaging: clinical challenges and applications</article-title>,” <source>CA: Cancer J. Clin.</source>
<volume>69</volume>(<issue>2</issue>), <fpage>127</fpage>–<lpage>157</lpage> (<year>2019</year>).<pub-id pub-id-type="doi">10.3322/caac.21552</pub-id><pub-id pub-id-type="pmid">30720861</pub-id></mixed-citation>
    </ref>
    <ref id="r10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prior</surname><given-names>F.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Open access image repositories: high-quality data to enable machine learning research</article-title>,” <source>Clin. Radiol.</source>
<volume>75</volume>(<issue>1</issue>), <fpage>7</fpage>–<lpage>12</lpage> (<year>2020</year>).<pub-id pub-id-type="coden">CLRAAG</pub-id><issn>0009-9260</issn><pub-id pub-id-type="doi">10.1016/j.crad.2019.04.002</pub-id><pub-id pub-id-type="pmid">31040006</pub-id></mixed-citation>
    </ref>
    <ref id="r11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alyafi</surname><given-names>B.</given-names></name><name><surname>Diaz</surname><given-names>O.</given-names></name><name><surname>Marti</surname><given-names>R.</given-names></name></person-group>, “<article-title>DCGANs for realistic breast mass augmentation in x-ray mammography</article-title>,” <source>Proc. SPIE</source>
<volume>11314</volume>, <fpage>1131420</fpage> (<year>2020</year>).<pub-id pub-id-type="coden">PSISDG</pub-id><issn>0277-786X</issn><pub-id pub-id-type="doi">10.1117/12.2543506</pub-id></mixed-citation>
    </ref>
    <ref id="r12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yi</surname><given-names>X.</given-names></name><name><surname>Walia</surname><given-names>E.</given-names></name><name><surname>Babyn</surname><given-names>P.</given-names></name></person-group>, “<article-title>Generative adversarial network in medical imaging: a review</article-title>,” <source>Med. Image Anal.</source>
<volume>58</volume>, <fpage>101552</fpage> (<year>2019</year>).<pub-id pub-id-type="doi">10.1016/j.media.2019.101552</pub-id><pub-id pub-id-type="pmid">31521965</pub-id></mixed-citation>
    </ref>
    <ref id="r13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolterink</surname><given-names>J. M.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Deep MR to CT synthesis using unpaired data</article-title>,” <source>Lect. Notes Comput. Sci.</source>
<volume>10557</volume>, <fpage>14</fpage>–<lpage>23</lpage> (<year>2017</year>).<pub-id pub-id-type="coden">LNCSD9</pub-id><issn>0302-9743</issn><pub-id pub-id-type="doi">10.1007/978-3-319-68127-6_2</pub-id></mixed-citation>
    </ref>
    <ref id="r14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szafranowska</surname><given-names>Z.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Sharing generative models instead of private data: a simulation study on mammography patch classification</article-title>,” <source>Proc. SPIE</source>
<volume>12286</volume>, <fpage>122860Q</fpage> (<year>2022</year>).<pub-id pub-id-type="coden">PSISDG</pub-id><issn>0277-786X</issn><pub-id pub-id-type="doi">10.1117/12.2625781</pub-id></mixed-citation>
    </ref>
    <ref id="r15">
      <label>15.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Stadler</surname><given-names>T.</given-names></name><name><surname>Oprisanu</surname><given-names>B.</given-names></name><name><surname>Troncoso</surname><given-names>C.</given-names></name></person-group>, “<article-title>Synthetic data–anonymisation groundhog day</article-title>,” in <conf-name>31st USENIX Secur. Symp. (USENIX Security 22)</conf-name>, pp. <fpage>1451</fpage>–<lpage>1468</lpage> (<year>2022</year>).</mixed-citation>
    </ref>
    <ref id="r16">
      <label>16.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I.</given-names></name><etal>et al.</etal></person-group>, <article-title>“Generative adversarial nets</article-title>,” in <conf-name>Adv. Neural Inf. Process. Syst.</conf-name>, pp. <fpage>2672</fpage>–<lpage>2680</lpage> (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="r17">
      <label>17.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Salimans</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Improved techniques for training GANs</article-title>,” in <conf-name>Adv. Neural Inf. Process. Syst. 29</conf-name>, pp. <fpage>2234</fpage>–<lpage>2242</lpage> (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="r18">
      <label>18.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mescheder</surname><given-names>L.</given-names></name><name><surname>Geiger</surname><given-names>A.</given-names></name><name><surname>Nowozin</surname><given-names>S.</given-names></name></person-group>, “<article-title>Which training methods for GANs do actually converge?</article-title>,” in <conf-name>Int. Conf. Mach. Learn.</conf-name>, <publisher-name>PMLR</publisher-name>, pp. <fpage>3481</fpage>–<lpage>3490</lpage> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="r19">
      <label>19.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Arora</surname><given-names>S.</given-names></name><name><surname>Risteski</surname><given-names>A.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name></person-group>, “<article-title>Do GANs learn the distribution? Some theory and empirics</article-title>,” in <conf-name>Int. Conf. Learn. Represent.</conf-name> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="r20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruthotto</surname><given-names>L.</given-names></name><name><surname>Haber</surname><given-names>E.</given-names></name></person-group>, “<article-title>An introduction to deep generative modeling</article-title>,” <source>GAMM-Mitteilungen</source>
<volume>44</volume>(<issue>2</issue>), <fpage>e202100008</fpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1002/gamm.202100008</pub-id></mixed-citation>
    </ref>
    <ref id="r21">
      <label>21.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D. P.</given-names></name><name><surname>Welling</surname><given-names>M.</given-names></name></person-group>, “<article-title>Auto-encoding variational Bayes</article-title>,” arXiv:1312.6114 (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="r22">
      <label>22.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rezende</surname><given-names>D.</given-names></name><name><surname>Mohamed</surname><given-names>S.</given-names></name></person-group>, “<article-title>Variational inference with normalizing flows</article-title>,” in <conf-name>Int. Conf. Mach. Learn.</conf-name>, <publisher-name>PMLR</publisher-name>, pp. <fpage>1530</fpage>–<lpage>1538</lpage> (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="r23">
      <label>23.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Dinh</surname><given-names>L.</given-names></name><name><surname>Krueger</surname><given-names>D.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group>, “<article-title>Nice: non-linear independent components estimation</article-title>,” arXiv:1410.8516 (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="r24">
      <label>24.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Dinh</surname><given-names>L.</given-names></name><name><surname>Sohl-Dickstein</surname><given-names>J.</given-names></name><name><surname>Bengio</surname><given-names>S.</given-names></name></person-group>, “<article-title>Density estimation using real NVP</article-title>,” arXiv:1605.08803 (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="r25">
      <label>25.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sohl-Dickstein</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Deep unsupervised learning using nonequilibrium thermodynamics</article-title>,” in <conf-name>Int. Conf. Mach. Learn.</conf-name>, <publisher-name>PMLR</publisher-name>, pp. <fpage>2256</fpage>–<lpage>2265</lpage> (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="r26">
      <label>26.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Y.</given-names></name><name><surname>Ermon</surname><given-names>S.</given-names></name></person-group>, “<article-title>Generative modeling by estimating gradients of the data distribution</article-title>,” in <conf-name>Adv. Neural Inf. Process. Syst. 32</conf-name> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="r27">
      <label>27.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>J.</given-names></name><name><surname>Jain</surname><given-names>A.</given-names></name><name><surname>Abbeel</surname><given-names>P.</given-names></name></person-group>, <article-title>“Denoising diffusion probabilistic models</article-title>,” in <conf-name>Adv. Neural Inf. Process. Syst. 33</conf-name>, pp. <fpage>6840</fpage>–<lpage>6851</lpage> (<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="r28">
      <label>28.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Arjovsky</surname><given-names>M.</given-names></name><name><surname>Chintala</surname><given-names>S.</given-names></name><name><surname>Bottou</surname><given-names>L.</given-names></name></person-group>, “<article-title>Wasserstein generative adversarial networks</article-title>,” in <conf-name>Int. Conf. Mach. Learn.</conf-name>, <publisher-name>PMLR</publisher-name>, pp. <fpage>214</fpage>–<lpage>223</lpage> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="r29">
      <label>29.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Gulrajani</surname><given-names>I.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Improved training of wasserstein gans</article-title>,” arXiv:1704.00028 (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="r30">
      <label>30.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Towards faster and stabilized gan training for high-fidelity few-shot image synthesis</article-title>,” in <conf-name>Int. Conf. Learn. Represent.</conf-name> (<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="r31">
      <label>31.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Kang</surname><given-names>M.</given-names></name><name><surname>Shin</surname><given-names>J.</given-names></name><name><surname>Park</surname><given-names>J.</given-names></name></person-group>, “<article-title>StudioGAN: a taxonomy and benchmark of GANs for image synthesis</article-title>,” arXiv:2206.09479 (<year>2022</year>).</mixed-citation>
    </ref>
    <ref id="r32">
      <label>32.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A.</given-names></name><name><surname>Metz</surname><given-names>L.</given-names></name><name><surname>Chintala</surname><given-names>S.</given-names></name></person-group>, “<article-title>Unsupervised representation learning with deep convolutional generative adversarial networks</article-title>,” arXiv:1511.06434 (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="r33">
      <label>33.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Karras</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Progressive growing of gans for improved quality, stability, and variation</article-title>,” arXiv:1710.10196 (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="r34">
      <label>34.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Mirza</surname><given-names>M.</given-names></name><name><surname>Osindero</surname><given-names>S.</given-names></name></person-group>, “<article-title>Conditional generative adversarial nets</article-title>,” arXiv:1411.1784 (<year>2014</year>).</mixed-citation>
    </ref>
    <ref id="r35">
      <label>35.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Odena</surname><given-names>A.</given-names></name><name><surname>Olah</surname><given-names>C.</given-names></name><name><surname>Shlens</surname><given-names>J.</given-names></name></person-group>, “<article-title>Conditional image synthesis with auxiliary classifier GANs</article-title>,” in <conf-name>Int. Conf. Mach. Learn.</conf-name>, <publisher-name>PMLR</publisher-name>, pp. <fpage>2642</fpage>–<lpage>2651</lpage> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="r36">
      <label>36.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Isola</surname><given-names>P.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Image-to-image translation with conditional adversarial networks</article-title>,” in <conf-name>Proc. IEEE Conf. Comput. Vision and Pattern Recognit.</conf-name>, pp. <fpage>1125</fpage>–<lpage>1134</lpage> (<year>2017</year>).<pub-id pub-id-type="doi">10.1109/CVPR.2017.632</pub-id></mixed-citation>
    </ref>
    <ref id="r37">
      <label>37.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>J.-Y.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Unpaired image-to-image translation using cycle-consistent adversarial networks</article-title>,” in <conf-name>Proc. IEEE Int. Conf. Comput. Vision</conf-name>, pp. <fpage>2223</fpage>–<lpage>2232</lpage> (<year>2017</year>).<pub-id pub-id-type="doi">10.1109/ICCV.2017.244</pub-id></mixed-citation>
    </ref>
    <ref id="r38">
      <label>38.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Choi</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Stargan: unified generative adversarial networks for multi-domain image-to-image translation</article-title>,” in <conf-name>Proc. IEEE Conf. Comput. Vision and Pattern Recognit.</conf-name>, pp. <fpage>8789</fpage>–<lpage>8797</lpage> (<year>2018</year>).<pub-id pub-id-type="doi">10.1109/CVPR.2018.00916</pub-id></mixed-citation>
    </ref>
    <ref id="r39">
      <label>39.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Park</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Semantic image synthesis with spatially-adaptive normalization</article-title>,” in <conf-name>Proc. IEEE/CVF Conf. Comput. Vision and Pattern Recognit.</conf-name>, pp. <fpage>2337</fpage>–<lpage>2346</lpage> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="r40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sushko</surname><given-names>V.</given-names></name><etal>et al.</etal></person-group>, “<article-title>OASIS: only adversarial supervision for semantic image synthesis</article-title>,” <source>Int. J. Comput. Vision</source>
<volume>130</volume>(<issue>12</issue>), <fpage>2903</fpage>–<lpage>2923</lpage> (<year>2022</year>).<pub-id pub-id-type="doi">10.1007/s11263-022-01673-x</pub-id></mixed-citation>
    </ref>
    <ref id="r41">
      <label>41.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shaham</surname><given-names>T. R.</given-names></name><name><surname>Dekel</surname><given-names>T.</given-names></name><name><surname>Michaeli</surname><given-names>T.</given-names></name></person-group>, “<article-title>Singan: learning a generative model from a single natural image</article-title>,” in <conf-name>Proc. IEEE/CVF Int. Conf. Comput. Vision</conf-name>, pp. <fpage>4570</fpage>–<lpage>4580</lpage> (<year>2019</year>).<pub-id pub-id-type="doi">10.1109/ICCV.2019.00467</pub-id></mixed-citation>
    </ref>
    <ref id="r42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Korkinof</surname><given-names>D.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Perceived realism of high resolution generative adversarial network derived synthetic mammograms</article-title>,” <source>Radiol.: Artif. Intell.</source>
<volume>3</volume>, <fpage>e190181</fpage> (<year>2020</year>).<pub-id pub-id-type="coden">AINTBB</pub-id><issn>0004-3702</issn><pub-id pub-id-type="doi">10.1148/ryai.2020190181</pub-id><pub-id pub-id-type="pmid">33937856</pub-id></mixed-citation>
    </ref>
    <ref id="r43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alyafi</surname><given-names>B.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Quality analysis of DCGAN-generated mammography lesions</article-title>,” <source>Proc. SPIE</source>
<volume>11513</volume>, <fpage>115130B</fpage> (<year>2020</year>).<pub-id pub-id-type="coden">PSISDG</pub-id><issn>0277-786X</issn><pub-id pub-id-type="doi">10.1117/12.2560473</pub-id></mixed-citation>
    </ref>
    <ref id="r44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borji</surname><given-names>A.</given-names></name></person-group>, “<article-title>Pros and cons of GAN evaluation measures</article-title>,” <source>Comput. Vision Image Understanding</source>
<volume>179</volume>, <fpage>41</fpage>–<lpage>65</lpage> (<year>2019</year>).<pub-id pub-id-type="coden">CVIUF4</pub-id><issn>1077-3142</issn><pub-id pub-id-type="doi">10.1016/j.cviu.2018.10.009</pub-id></mixed-citation>
    </ref>
    <ref id="r45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borji</surname><given-names>A.</given-names></name></person-group>, “<article-title>Pros and cons of GAN evaluation measures: new developments</article-title>,” <source>Comput. Vision Image Understanding</source>
<volume>215</volume>, <fpage>103329</fpage> (<year>2022</year>).<pub-id pub-id-type="coden">CVIUF4</pub-id><issn>1077-3142</issn><pub-id pub-id-type="doi">10.1016/j.cviu.2021.103329</pub-id></mixed-citation>
    </ref>
    <ref id="r46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heusel</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, “<article-title>GANs trained by a two time-scale update rule converge to a local nash equilibrium</article-title>,” <source>Adv. Neural Inf. Process. Syst.</source>
<volume>30</volume> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="r47">
      <label>47.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Rethinking the inception architecture for computer vision</article-title>,” in <conf-name>Proc. IEEE Conf. Comput. Vision and Pattern Recognit.</conf-name>, pp. <fpage>2818</fpage>–<lpage>2826</lpage> (<year>2016</year>).<pub-id pub-id-type="doi">10.1109/CVPR.2016.308</pub-id></mixed-citation>
    </ref>
    <ref id="r48">
      <label>48.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Going deeper with convolutions</article-title>,” in <conf-name>Proc. IEEE Conf. Comput. Vision and Pattern Recognit.</conf-name>, pp. <fpage>1</fpage>–<lpage>9</lpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1109/CVPR.2015.7298594</pub-id></mixed-citation>
    </ref>
    <ref id="r49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Image quality assessment: from error visibility to structural similarity</article-title>,” <source>IEEE Trans. Image Process.</source>
<volume>13</volume>(<issue>4</issue>), <fpage>600</fpage>–<lpage>612</lpage> (<year>2004</year>).<pub-id pub-id-type="coden">IIPRE4</pub-id><issn>1057-7149</issn><pub-id pub-id-type="doi">10.1109/TIP.2003.819861</pub-id><pub-id pub-id-type="pmid">15376593</pub-id></mixed-citation>
    </ref>
    <ref id="r50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrucho</surname><given-names>L.</given-names></name><etal>et al.</etal></person-group>, “<article-title>High-resolution synthesis of high-density breast mammograms: application to improved fairness in deep learning based mass detection</article-title>,” <source>Front. Oncol.</source>
<volume>12</volume>, <fpage>1044496</fpage> (<year>2022</year>).<pub-id pub-id-type="doi">10.3389/fonc.2022.1044496</pub-id><pub-id pub-id-type="pmid">36755853</pub-id></mixed-citation>
    </ref>
    <ref id="r51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thambawita</surname><given-names>V.</given-names></name><etal>et al.</etal></person-group>, “<article-title>SinGAN-Seg: synthetic training data generation for medical image segmentation</article-title>,” <source>PLoS One</source>
<volume>17</volume>(<issue>5</issue>), <fpage>e0267976</fpage> (<year>2022</year>).<pub-id pub-id-type="coden">POLNCL</pub-id><issn>1932-6203</issn><pub-id pub-id-type="doi">10.1371/journal.pone.0267976</pub-id><pub-id pub-id-type="pmid">35500005</pub-id></mixed-citation>
    </ref>
    <ref id="r52">
      <label>52.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Imagenet: a large-scale hierarchical image database</article-title>,” in <conf-name>IEEE Conf. Comput. Vision and Pattern Recognit.</conf-name>, <publisher-name>IEEE</publisher-name>, pp. <fpage>248</fpage>–<lpage>255</lpage> (<year>2009</year>).<pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></mixed-citation>
    </ref>
    <ref id="r53">
      <label>53.</label>
      <mixed-citation publication-type="other"><collab>accel brain</collab>, “<article-title>Generative adversarial networks library: Pygan</article-title>,” <year>2021</year>, <ext-link xlink:href="https://github.com/accel-brain/accel-brain-code/tree/master/Generative-Adversarial-Networks/" ext-link-type="uri">https://github.com/accel-brain/accel-brain-code/tree/master/Generative-Adversarial-Networks/</ext-link>.</mixed-citation>
    </ref>
    <ref id="r54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pal</surname><given-names>A.</given-names></name><name><surname>Das</surname><given-names>A.</given-names></name></person-group>, “<article-title>TorchGAN: a flexible framework for gan training and evaluation</article-title>,” <source>J. Open Source Software</source>
<volume>6</volume>(<issue>66</issue>), <fpage>2606</fpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.21105/joss.02606</pub-id></mixed-citation>
    </ref>
    <ref id="r55">
      <label>55.</label>
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Herzer</surname><given-names>J.</given-names></name><name><surname>Neuer</surname><given-names>T.</given-names></name><collab>Radu</collab></person-group>, “<article-title>vegans</article-title>,” <year>2021</year>, <ext-link xlink:href="https://github.com/unit8co/vegans/" ext-link-type="uri">https://github.com/unit8co/vegans/</ext-link>.</mixed-citation>
    </ref>
    <ref id="r56">
      <label>56.</label>
      <mixed-citation publication-type="webpage"><collab>NVIDIA</collab>, “<article-title>Imaginaire</article-title>,” <year>2021</year>, <ext-link xlink:href="https://github.com/NVlabs/imaginaire" ext-link-type="uri">https://github.com/NVlabs/imaginaire</ext-link>.</mixed-citation>
    </ref>
    <ref id="r57">
      <label>57.</label>
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Shor</surname><given-names>J.</given-names></name></person-group>, “<article-title>Tensorflow-GAN (TF-GAN): tooling for gans in tensorflow</article-title>,” <year>2022</year>, <ext-link xlink:href="https://github.com/tensorflow/gan" ext-link-type="uri">https://github.com/tensorflow/gan</ext-link>.</mixed-citation>
    </ref>
    <ref id="r58">
      <label>58.</label>
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Linder-Norén</surname><given-names>E.</given-names></name></person-group>, “<article-title>Keras-GAN: Pytorch implementations of generative adversarial networks</article-title>,” <year>2021</year>, <ext-link xlink:href="https://github.com/eriklindernoren/PyTorch-GAN" ext-link-type="uri">https://github.com/eriklindernoren/PyTorch-GAN</ext-link>.</mixed-citation>
    </ref>
    <ref id="r59">
      <label>59.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Linder-Norén</surname><given-names>E.</given-names></name></person-group>, “<article-title>Keras-GAN: Keras implementations of generative adversarial networks</article-title>,” <year>2022</year>, <ext-link xlink:href="https://github.com/eriklindernoren/Keras-GAN" ext-link-type="uri">https://github.com/eriklindernoren/Keras-GAN</ext-link>.</mixed-citation>
    </ref>
    <ref id="r60">
      <label>60.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>K. S.</given-names></name><name><surname>Town</surname><given-names>C.</given-names></name></person-group>, “<article-title>Mimicry: towards the reproducibility of GAN research</article-title>,” arXiv:2005.02494 (<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="r61">
      <label>61.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>T.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Transformers: state-of-the-art natural language processing</article-title>,” in <conf-name>Proc. 2020 Conf. Empirical Methods in Nat. Language Process.: Syst. Demonstrations</conf-name>, pp. <fpage>38</fpage>–<lpage>45</lpage> (<year>2020</year>, October).</mixed-citation>
    </ref>
    <ref id="r62">
      <label>62.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahng</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, “<article-title>GAN lab: understanding complex deep generative models using interactive visual experimentation</article-title>,” <source>IEEE Trans. Vision Comput. Graphics</source>
<volume>25</volume>(<issue>1</issue>), <fpage>310</fpage>–<lpage>320</lpage> (<year>2018</year>).<issn>1077-2626</issn><pub-id pub-id-type="doi">10.1109/TVCG.2018.2864500</pub-id></mixed-citation>
    </ref>
    <ref id="r63">
      <label>63.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diaz</surname><given-names>O.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Data preparation for artificial intelligence in medical imaging: a comprehensive guide to open-access platforms and tools</article-title>,” <source>Phys. Med.</source>
<volume>83</volume>, <fpage>25</fpage>–<lpage>37</lpage> (<year>2021</year>).<pub-id pub-id-type="coden">PHYME2</pub-id><issn>1120-1797</issn><pub-id pub-id-type="doi">10.1016/j.ejmp.2021.02.007</pub-id><pub-id pub-id-type="pmid">33684723</pub-id></mixed-citation>
    </ref>
    <ref id="r64">
      <label>64.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pérez-Garca</surname><given-names>F.</given-names></name><name><surname>Sparks</surname><given-names>R.</given-names></name><name><surname>Ourselin</surname><given-names>S.</given-names></name></person-group>, “<article-title>TorchIO: a Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning</article-title>,” <source>Comput. Methods Prog. Biomed.</source>
<volume>208</volume>, <fpage>106236</fpage> (<year>2021</year>).<pub-id pub-id-type="coden">CMPBEK</pub-id><issn>0169-2607</issn><pub-id pub-id-type="doi">10.1016/j.cmpb.2021.106236</pub-id></mixed-citation>
    </ref>
    <ref id="r65">
      <label>65.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>C. M.</given-names></name><etal>et al.</etal></person-group>, “<article-title>CleanX: a Python library for data cleaning of large sets of radiology images</article-title>,” <source>J. Open Source Software</source>
<volume>7</volume>(<issue>76</issue>), <fpage>3632</fpage> (<year>2022</year>).<pub-id pub-id-type="doi">10.21105/joss.03632</pub-id></mixed-citation>
    </ref>
    <ref id="r66">
      <label>66.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Cardoso</surname><given-names>M. J.</given-names></name><etal>et al.</etal></person-group>, “<article-title>MONAI: an open-source framework for deep learning in healthcare</article-title>,” arXiv:2211.02701 (<year>2022</year>).</mixed-citation>
    </ref>
    <ref id="r67">
      <label>67.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Pytorch: an imperative style, high-performance deep learning library</article-title>,” in <conf-name>Adv. Neural Inf. Process. Syst. 32</conf-name>, <person-group person-group-type="editor"><name><surname>Wallach</surname><given-names>H.</given-names></name><etal>et al.</etal></person-group>, Eds., pp. <fpage>8024</fpage>–<lpage>8035</lpage>, <publisher-name>Curran Associates, Inc</publisher-name>. (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="r68">
      <label>68.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibson</surname><given-names>E.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Niftynet: a deep-learning platform for medical imaging</article-title>,” <source>Comput. Methods Prog. Biomed.</source>
<volume>158</volume>, <fpage>113</fpage>–<lpage>122</lpage> (<year>2018</year>).<pub-id pub-id-type="coden">CMPBEK</pub-id><issn>0169-2607</issn><pub-id pub-id-type="doi">10.1016/j.cmpb.2018.01.025</pub-id></mixed-citation>
    </ref>
    <ref id="r69">
      <label>69.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Pawlowski</surname><given-names>N.</given-names></name><etal>et al.</etal></person-group>, “<article-title>DLTK: state of the art reference implementations for deep learning on medical images</article-title>,” arXiv:1711.06853 (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="r70">
      <label>70.</label>
      <mixed-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Nikolaos</surname><given-names>A.</given-names></name></person-group>, “<article-title>Deep learning in medical image analysis: a comparative analysis of multi-modal brain-MRI segmentation with 3D deep neural networks</article-title>,” Master’s thesis, <institution>University of Patras</institution> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="r71">
      <label>71.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baumgartner</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, “<article-title>nndetection: a self-configuring method for medical object detection</article-title>,” <source>Lect. Notes Comput. Sci.</source>
<volume>12905</volume>, <fpage>530</fpage>–<lpage>539</lpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1007/978-3-030-87240-3_51</pub-id></mixed-citation>
    </ref>
    <ref id="r72">
      <label>72.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mei</surname><given-names>X.</given-names></name><etal>et al.</etal></person-group>, “<article-title>RadImageNet: an open radiologic deep learning research dataset for effective transfer learning</article-title>,” <source>Radiol.: Artif. Intell.</source>
<volume>4</volume>, <fpage>e210315</fpage> (<year>2022</year>).<pub-id pub-id-type="coden">AINTBB</pub-id><issn>0004-3702</issn><pub-id pub-id-type="doi">10.1148/ryai.210315</pub-id><pub-id pub-id-type="pmid">36204533</pub-id></mixed-citation>
    </ref>
    <ref id="r73">
      <label>73.</label>
      <mixed-citation publication-type="webpage">The Python Package Index, “<article-title>medigan 1.0.0</article-title>,” <year>2022</year>,<ext-link xlink:href="https://pypi.org/project/medigan/" ext-link-type="uri">https://pypi.org/project/medigan/</ext-link> (accessed 5 February 2023).</mixed-citation>
    </ref>
    <ref id="r74">
      <label>74.</label>
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Osuala</surname><given-names>R.</given-names></name><name><surname>Skorupko</surname><given-names>G.</given-names></name><name><surname>Lazrak</surname><given-names>N.</given-names></name></person-group>, “<article-title>medigan getting started</article-title>,” <year>2022</year>, <ext-link xlink:href="https://medigan.readthedocs.io/en/latest" ext-link-type="uri">https://medigan.readthedocs.io/en/latest</ext-link> (accessed 5 February 2023).</mixed-citation>
    </ref>
    <ref id="r75">
      <label>75.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Lekadir</surname><given-names>K.</given-names></name><etal>et al.</etal></person-group>, “<article-title>FUTURE-AI: guiding principles and consensus recommendations for trustworthy artificial intelligence in medical imaging</article-title>,” arXiv:2109.09658 (<year>2021</year>).</mixed-citation>
    </ref>
    <ref id="r76">
      <label>76.</label>
      <mixed-citation publication-type="webpage">EuCanImage Consortium, “<article-title>EuCanImage towards a European cancer imaging platform for enhanced artificial intelligence in oncology</article-title>,” <year>2020</year>, <ext-link xlink:href="https://eucanimage.eu/" ext-link-type="uri">https://eucanimage.eu/</ext-link> (accessed 5 February 2023).</mixed-citation>
    </ref>
    <ref id="r77">
      <label>77.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, “<article-title>TensorFlow: large-scale machine learning on heterogeneous systems</article-title>,” <year>2015</year>, <ext-link xlink:href="tensorflow.org" ext-link-type="uri">tensorflow.org</ext-link>.</mixed-citation>
    </ref>
    <ref id="r78">
      <label>78.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Chollet</surname><given-names>F.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Keras</article-title>,” <year>2015</year>, <ext-link xlink:href="https://github.com/fchollet/keras" ext-link-type="uri">https://github.com/fchollet/keras</ext-link>.</mixed-citation>
    </ref>
    <ref id="r79">
      <label>79.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Larman</surname><given-names>C.</given-names></name></person-group>, <source>Applying UML and Pattern: An Introduction to Object Oriented Analysis and Design and the Unified Process</source>, <publisher-name>Prentice Hall PTR</publisher-name> (<year>2001</year>).</mixed-citation>
    </ref>
    <ref id="r80">
      <label>80.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gamma</surname><given-names>E.</given-names></name><etal>et al.</etal></person-group>, <source>Design Patterns: Elements of Reusable Object-Oriented Software</source>, <publisher-name>Pearson Deutschland GmbH</publisher-name> (<year>1995</year>).</mixed-citation>
    </ref>
    <ref id="r81">
      <label>81.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreira</surname><given-names>I. C.</given-names></name><etal>et al.</etal></person-group>, “<article-title>INbreast: toward a full-field digital mammographic database</article-title>,” <source>Acad. Radiol.</source>
<volume>19</volume>(<issue>2</issue>), <fpage>236</fpage>–<lpage>248</lpage> (<year>2012</year>).<pub-id pub-id-type="doi">10.1016/j.acra.2011.09.014</pub-id><pub-id pub-id-type="pmid">22078258</pub-id></mixed-citation>
    </ref>
    <ref id="r82">
      <label>82.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halling-Brown</surname><given-names>M. D.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Optimam mammography image database: a large-scale resource of mammography images and clinical data</article-title>,” <source>Radiol.: Artif. Intell.</source>
<volume>3</volume>, <fpage>e200103</fpage> (<year>2020</year>).<pub-id pub-id-type="coden">AINTBB</pub-id><issn>0004-3702</issn><pub-id pub-id-type="doi">10.1148/ryai.2020200103</pub-id><pub-id pub-id-type="pmid">33937853</pub-id></mixed-citation>
    </ref>
    <ref id="r83">
      <label>83.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lopez</surname><given-names>M. G.</given-names></name><etal>et al.</etal></person-group>, “<article-title>BCDR: a breast cancer digital repository</article-title>,” in <conf-name>15th Int. Conf. Exp. Mech.</conf-name>, Vol. 1215 (<year>2012</year>).</mixed-citation>
    </ref>
    <ref id="r84">
      <label>84.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>S.</given-names></name><name><surname>Kim</surname><given-names>B.</given-names></name><name><surname>Park</surname><given-names>H.</given-names></name></person-group>, “<article-title>Synthesis of brain tumor multicontrast MR images for improved data augmentation</article-title>,” <source>Med. Phys.</source>
<volume>48</volume>(<issue>5</issue>), <fpage>2185</fpage>–<lpage>2198</lpage> (<year>2021</year>).<pub-id pub-id-type="coden">MPHYA6</pub-id><issn>0094-2405</issn><pub-id pub-id-type="doi">10.1002/mp.14701</pub-id><pub-id pub-id-type="pmid">33405244</pub-id></mixed-citation>
    </ref>
    <ref id="r85">
      <label>85.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menze</surname><given-names>B. H.</given-names></name><etal>et al.</etal></person-group>, “<article-title>The multimodal brain tumor image segmentation benchmark (BRATS)</article-title>,” <source>IEEE Trans. Med. Imaging</source>
<volume>34</volume>(<issue>10</issue>), <fpage>1993</fpage>–<lpage>2024</lpage> (<year>2014</year>).<pub-id pub-id-type="coden">ITMID4</pub-id><issn>0278-0062</issn><pub-id pub-id-type="doi">10.1109/TMI.2014.2377694</pub-id><pub-id pub-id-type="pmid">25494501</pub-id></mixed-citation>
    </ref>
    <ref id="r86">
      <label>86.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>R. S.</given-names></name><etal>et al.</etal></person-group>, “<article-title>A curated mammography data set for use in computer-aided detection and diagnosis research</article-title>,” <source>Sci. Data</source>
<volume>4</volume>(<issue>1</issue>), <fpage>170177</fpage> (<year>2017</year>).<pub-id pub-id-type="doi">10.1038/sdata.2017.177</pub-id><pub-id pub-id-type="pmid">29257132</pub-id></mixed-citation>
    </ref>
    <ref id="r87">
      <label>87.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borgli</surname><given-names>H.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Hyperkvasir, a comprehensive multi-class image and video dataset for gastrointestinal endoscopy</article-title>,” <source>Sci. Data</source>
<volume>7</volume>(<issue>1</issue>), <fpage>283</fpage> (<year>2020</year>).<pub-id pub-id-type="doi">10.1038/s41597-020-00622-y</pub-id><pub-id pub-id-type="pmid">32859981</pub-id></mixed-citation>
    </ref>
    <ref id="r88">
      <label>88.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dembrower</surname><given-names>K.</given-names></name><name><surname>Lindholm</surname><given-names>P.</given-names></name><name><surname>Strand</surname><given-names>F.</given-names></name></person-group>, “<article-title>A multi-million mammography image dataset and population-based screening cohort for the training and evaluation of deep neural networks-the cohort of screen-aged women (CSAW)</article-title>,” <source>J. Digital Imaging</source>
<volume>33</volume>(<issue>2</issue>), <fpage>408</fpage>–<lpage>413</lpage> (<year>2020</year>).<pub-id pub-id-type="coden">JDIMEW</pub-id><pub-id pub-id-type="doi">10.1007/s10278-019-00278-0</pub-id></mixed-citation>
    </ref>
    <ref id="r89">
      <label>89.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sogancioglu</surname><given-names>E.</given-names></name><name><surname>Murphy</surname><given-names>K.</given-names></name><name><surname>van Ginneken</surname><given-names>B.</given-names></name></person-group>, “<article-title>NODE21 (v-5) [data set]</article-title>,” <source>Zenodo</source> (<year>2021</year>).<pub-id pub-id-type="doi">10.5281/zenodo.5548363</pub-id></mixed-citation>
    </ref>
    <ref id="r90">
      <label>90.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Chestx-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</article-title>,” in <conf-name>Proc. IEEE Conf. Comput. Vision and Pattern Recognit.</conf-name>, pp. <fpage>2097</fpage>–<lpage>2106</lpage> (<year>2017</year>).<pub-id pub-id-type="doi">10.1109/CVPR.2017.369</pub-id></mixed-citation>
    </ref>
    <ref id="r91">
      <label>91.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Segal</surname><given-names>B.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Evaluating the clinical realism of synthetic chest x-rays generated using progressively growing GANs</article-title>,” <source>SN Comput. Sci.</source>
<volume>2</volume>(<issue>4</issue>), <fpage>1</fpage>–<lpage>17</lpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1007/s42979-021-00720-7</pub-id></mixed-citation>
    </ref>
    <ref id="r92">
      <label>92.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>S.</given-names></name><etal>et al.</etal></person-group>, “<article-title>nn-UNet training on CycleGAN-translated images for cross-modal domain adaptation in biomedical imaging</article-title>,” <source>Lect. Notes Comput. Sci.</source>
<volume>12963</volume>, <fpage>540</fpage>–<lpage>551</lpage> (<year>2022</year>).</mixed-citation>
    </ref>
    <ref id="r93">
      <label>93.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dorent</surname><given-names>R.</given-names></name><etal>et al.</etal></person-group>, “<article-title>CrossMoDA 2021 challenge: benchmark of cross-modality domain adaptation techniques for vestibular schwannoma and cochlea segmentation</article-title>,” <source>Med. Image Anal.</source>
<volume>83</volume>, <fpage>102628</fpage> (<year>2022</year>).<pub-id pub-id-type="doi">10.1016/j.media.2022.102628</pub-id><pub-id pub-id-type="pmid">36283200</pub-id></mixed-citation>
    </ref>
    <ref id="r94">
      <label>94.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Selvan</surname><given-names>R.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Carbon footprint of selecting and training deep learning models for medical image analysis</article-title>,” in <conf-name>Med. Image Comput. and Comput. Assist. Intervention–MICCAI 2022: 25th Int. Conf.</conf-name>, <conf-date>18–22 September 2022</conf-date>, <conf-loc>Singapore</conf-loc>, Proceedings, Part V, pp. <fpage>506</fpage>–<lpage>516</lpage>, <publisher-name>Springer Nature</publisher-name>, <publisher-loc>Cham, Switzerland</publisher-loc> (<year>2022</year>).</mixed-citation>
    </ref>
    <ref id="r95">
      <label>95.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dwork</surname><given-names>C.</given-names></name><etal>et al.</etal></person-group>, “<article-title>The algorithmic foundations of differential privacy</article-title>,” <source>Found. Trends® Theor. Comput. Sci.</source>
<volume>9</volume>(<issue>3–4</issue>), <fpage>211</fpage>–<lpage>407</lpage> (<year>2014</year>).<pub-id pub-id-type="doi">10.1561/0400000042</pub-id></mixed-citation>
    </ref>
    <ref id="r96">
      <label>96.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdelrahman</surname><given-names>L.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Convolutional neural networks for breast cancer detection in mammography: a survey</article-title>,” <source>Comput. Biol. Med.</source>
<volume>131</volume>(<issue>Jan.</issue>), <fpage>104248</fpage> (<year>2021</year>).<pub-id pub-id-type="coden">CBMDAW</pub-id><issn>0010-4825</issn><pub-id pub-id-type="doi">10.1016/j.compbiomed.2021.104248</pub-id><pub-id pub-id-type="pmid">33631497</pub-id></mixed-citation>
    </ref>
    <ref id="r97">
      <label>97.</label>
      <mixed-citation publication-type="other"><collab>Centers for Medicare &amp; Medicaid Services</collab>, “<article-title>The Health Insurance Portability and Accountability Act of 1996 (HIPAA)</article-title>,” <year>1996</year>, <ext-link xlink:href="http://www.cms.hhs.gov/hipaa/" ext-link-type="uri">http://www.cms.hhs.gov/hipaa/</ext-link>.</mixed-citation>
    </ref>
    <ref id="r98">
      <label>98.</label>
      <mixed-citation publication-type="other"><collab>European Parliament and Council of European Union</collab>, “<article-title>Council regulation (EU) no 2016/679</article-title>,” <year>2018</year>, <ext-link xlink:href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32016R0679/" ext-link-type="uri">https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32016R0679/</ext-link>.</mixed-citation>
    </ref>
    <ref id="r99">
      <label>99.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><collab>Committee on Health Research and the Privacy of Health Information: The HIPAA Privacy Rule, Board on Health Sciences Policy, Board on Health Care Services</collab><etal>et al.</etal></person-group>, <source>Beyond the HIPAA Privacy Rule: Enhancing Privacy, Improving Health Through Research</source>, p. <fpage>12458</fpage>, <publisher-name>National Academies Press</publisher-name>, <publisher-loc>Washington, D.C</publisher-loc>. (<year>2009</year>).</mixed-citation>
    </ref>
    <ref id="r100">
      <label>100.</label>
      <mixed-citation publication-type="other"><collab>U.S. Dept. of Health and Human Services</collab>, “<article-title>Summary of the HIPAA privacy rule: HIPAA compliance assistance</article-title>,” <year>2003</year>, <ext-link xlink:href="http://purl.fdlp.gov/GPO/gpo9756" ext-link-type="uri">http://purl.fdlp.gov/GPO/gpo9756</ext-link>.</mixed-citation>
    </ref>
    <ref id="r101">
      <label>101.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shah</surname><given-names>S. M.</given-names></name><name><surname>Khan</surname><given-names>R. A.</given-names></name></person-group>, “<article-title>Secondary use of electronic health record: opportunities and challenges</article-title>,” <source>IEEE Access</source>
<volume>8</volume>, <fpage>136947</fpage>–<lpage>136965</lpage> (<year>2020</year>).<pub-id pub-id-type="doi">10.1109/ACCESS.2020.3011099</pub-id></mixed-citation>
    </ref>
    <ref id="r102">
      <label>102.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mondschein</surname><given-names>C. F.</given-names></name><name><surname>Monda</surname><given-names>C.</given-names></name></person-group>, “<article-title>The EU’s general data protection regulation (GDPR) in a research context</article-title>,” in <source>Fundamentals of Clinical Data Science</source>, <person-group person-group-type="editor"><name><surname>Kubben</surname><given-names>P.</given-names></name><name><surname>Dumontier</surname><given-names>M.</given-names></name><name><surname>Dekker</surname><given-names>A.</given-names></name></person-group>, Eds., pp. <fpage>55</fpage>–<lpage>71</lpage>, <publisher-name>Springer International Publishing</publisher-name>, <publisher-loc>Cham</publisher-loc> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="r103">
      <label>103.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>El Emam</surname><given-names>K.</given-names></name><name><surname>Mosquera</surname><given-names>L.</given-names></name><name><surname>Hoptroff</surname><given-names>R.</given-names></name></person-group>, <source>Practical Synthetic Data Generation: Balancing Privacy and the Broad Availability of Data</source>, <edition>1st ed.</edition>, <publisher-name>O’Reilly Media, Inc</publisher-name>, <publisher-loc>Sebastopol, California</publisher-loc> (<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="r104">
      <label>104.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dankar</surname><given-names>F. K.</given-names></name><name><surname>Ibrahim</surname><given-names>M.</given-names></name></person-group>, “<article-title>Fake it till you make it: guidelines for effective synthetic data generation</article-title>,” <source>Applied Sciences</source>
<volume>11</volume>, <fpage>2158</fpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.3390/app11052158</pub-id></mixed-citation>
    </ref>
    <ref id="r105">
      <label>105.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pinaya</surname><given-names>W. H. L.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Brain imaging generation with latent diffusion models</article-title>,” in <conf-name>Deep Generative Models: Second MICCAI Workshop, DGM4MICCAI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings</conf-name>, pp. <fpage>117</fpage>–<lpage>126</lpage>, <publisher-name>Springer Nature</publisher-name>, <publisher-loc>Cham, Switzerland</publisher-loc> (<year>2022</year>, October).</mixed-citation>
    </ref>
    <ref id="r106">
      <label>106.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinaya</surname><given-names>W. H. L.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Unsupervised brain imaging 3D anomaly detection and segmentation with transformers</article-title>,” <source>Med. Image Anal.</source>
<volume>79</volume>, <fpage>102475</fpage> (<year>2022</year>).<pub-id pub-id-type="doi">10.1016/j.media.2022.102475</pub-id><pub-id pub-id-type="pmid">35598520</pub-id></mixed-citation>
    </ref>
    <ref id="r107">
      <label>107.</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pawlowski</surname><given-names>N.</given-names></name><name><surname>Coelho de Castro</surname><given-names>D.</given-names></name><name><surname>Glocker</surname><given-names>B.</given-names></name></person-group>, <article-title>“Deep structural causal models for tractable counterfactual inference</article-title>,” in <conf-name>Adv. Neural Inf. Process. Syst. 33</conf-name>, pp. <fpage>857</fpage>–<lpage>869</lpage> (<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="r108">
      <label>108.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutherford</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, “<article-title>A DICOM dataset for evaluation of medical image de-identification</article-title>,” <source>Sci. Data</source>
<volume>8</volume>, <fpage>183</fpage> (<year>2021</year>).<pub-id pub-id-type="doi">10.1038/s41597-021-00967-y</pub-id><pub-id pub-id-type="pmid">34272388</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
