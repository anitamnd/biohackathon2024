<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_CSBJ2447 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEga1 jpg ?>
<?FILEmmc1 xlsx ?>
<?FILEmmc2 xlsx ?>
<?FILEmmc3 xlsx ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Comput Struct Biotechnol J</journal-id>
    <journal-id journal-id-type="iso-abbrev">Comput Struct Biotechnol J</journal-id>
    <journal-title-group>
      <journal-title>Computational and Structural Biotechnology Journal</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2001-0370</issn>
    <publisher>
      <publisher-name>Research Network of Computational and Structural Biotechnology</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10772389</article-id>
    <article-id pub-id-type="pii">S2001-0370(23)00484-1</article-id>
    <article-id pub-id-type="doi">10.1016/j.csbj.2023.12.010</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software/Web Server Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deep-HPI-pred: An R-Shiny applet for network-based classification and prediction of Host-Pathogen protein-protein interactions</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au0005">
        <name>
          <surname>Tahir ul Qamar</surname>
          <given-names>Muhammad</given-names>
        </name>
        <email>m.tahirulqamar@hotmail.com</email>
        <xref rid="aff0005" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
        <xref rid="fn1" ref-type="fn">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au0010">
        <name>
          <surname>Noor</surname>
          <given-names>Fatima</given-names>
        </name>
        <xref rid="aff0010" ref-type="aff">b</xref>
        <xref rid="fn1" ref-type="fn">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au0015">
        <name>
          <surname>Guo</surname>
          <given-names>Yi-Xiong</given-names>
        </name>
        <xref rid="aff0015" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author" id="au0020">
        <name>
          <surname>Zhu</surname>
          <given-names>Xi-Tong</given-names>
        </name>
        <xref rid="aff0005" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au0025">
        <name>
          <surname>Chen</surname>
          <given-names>Ling-Ling</given-names>
        </name>
        <email>llchen@gxu.edu.cn</email>
        <xref rid="aff0005" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <aff id="aff0005"><label>a</label>State Key Laboratory for Conservation and Utilization of Subtropical Agro-bioresources, College of Life Science and Technology, Guangxi University, Nanning 530004, China</aff>
      <aff id="aff0010"><label>b</label>Integrative Omics and Molecular Modeling Laboratory, Department of Bioinformatics and Biotechnology, Government College University Faisalabad (GCUF), Faisalabad 38000, Pakistan</aff>
      <aff id="aff0015"><label>c</label>National Key Laboratory of Crop Genetic Improvement, College of Informatics, Huazhong Agricultural University, Wuhan 430070, China</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding authors. <email>m.tahirulqamar@hotmail.com</email><email>llchen@gxu.edu.cn</email></corresp>
      <fn id="fn1">
        <label>1</label>
        <p id="ntp0005">These authors contributed equally to this study.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>15</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <volume>23</volume>
    <fpage>316</fpage>
    <lpage>329</lpage>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>10</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>11</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>12</day>
        <month>12</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 The Authors. Published by Elsevier B.V. on behalf of Research Network of Computational and Structural Biotechnology.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder/>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="ab0010">
      <p>Host-pathogen interactions (HPIs) are vital in numerous biological activities and are intrinsically linked to the onset and progression of infectious diseases. HPIs are pivotal in the entire lifecycle of diseases: from the onset of pathogen introduction, navigating through the mechanisms that bypass host cellular defenses, to its subsequent proliferation inside the host. At the heart of these stages lies the synergy of proteins from both the host and the pathogen. By understanding these interlinking protein dynamics, we can gain crucial insights into how diseases progress and pave the way for stronger plant defenses and the swift formulation of countermeasures. In the framework of current study, we developed a web-based R/Shiny app, Deep-HPI-pred, that uses network-driven feature learning method to predict the yet unmapped interactions between pathogen and host proteins. Leveraging citrus and <italic>C</italic>Las bacteria training datasets as case study, we spotlight the effectiveness of Deep-HPI-pred in discerning Protein-protein interaction (PPIs) between them. Deep-HPI-pred use Multilayer Perceptron (MLP) models for HPI prediction, which is based on a comprehensive evaluation of topological features and neural network architectures. When subjected to independent validation datasets, the predicted models consistently surpassed a Matthews correlation coefficient (MCC) of 0.80 in host-pathogen interactions. Remarkably, the use of Eigenvector Centrality as the leading topological feature further enhanced this performance. Further, Deep-HPI-pred also offers relevant gene ontology (GO) term information for each pathogen and host protein within the system. This protein annotation data contributes an additional layer to our understanding of the intricate dynamics within host-pathogen interactions. In the additional benchmarking studies, the Deep-HPI-pred model has proven its robustness by consistently delivering reliable results across different host-pathogen systems, including plant-pathogens (accuracy of 98.4% and 97.9%), human-virus (accuracy of 94.3%), and animal-bacteria (accuracy of 96.6%) interactomes. These results not only demonstrate the model's versatility but also pave the way for gaining comprehensive insights into the molecular underpinnings of complex host-pathogen interactions. Taken together, the Deep-HPI-pred applet offers a unified web service for both identifying and illustrating interaction networks. Deep-HPI-pred applet is freely accessible at its homepage: <ext-link ext-link-type="uri" xlink:href="https://cbi.gxu.edu.cn/shiny-apps/Deep-HPI-pred/" id="ir0005">https://cbi.gxu.edu.cn/shiny-apps/Deep-HPI-pred/</ext-link> and at github: <ext-link ext-link-type="uri" xlink:href="https://github.com/tahirulqamar/Deep-HPI-pred" id="ir0010">https://github.com/tahirulqamar/Deep-HPI-pred</ext-link>.</p>
    </abstract>
    <abstract abstract-type="graphical" id="ab0015">
      <title>Graphical abstract</title>
      <p>
        <fig id="fig0030" position="anchor">
          <alt-text id="at0045">ga1</alt-text>
          <graphic xlink:href="ga1" id="lk0030"/>
        </fig>
      </p>
    </abstract>
    <kwd-group id="keys0005">
      <title>Keywords</title>
      <kwd>Host–pathogen interactions</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Multilayer perceptron</kwd>
      <kwd>Neural networks</kwd>
      <kwd>Topological features</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec0005">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0005">Host-pathogen interactions are crucial determinants in the dynamics of infectious diseases <xref rid="bib1" ref-type="bibr">[1]</xref>. These interactions are primarily facilitated by protein-protein interactions (PPIs), which orchestrate every phase of disease progression, from the initial pathogen invasion to its eventual establishment within the host <xref rid="bib2" ref-type="bibr">[2]</xref>. Citrus Huanglongbing (HLB), also known as citrus greening, represents a paradigmatic case of devastating plant diseases on a global scale <xref rid="bib3" ref-type="bibr">[3]</xref>. Predominantly caused by the bacterium <italic>Candidatus</italic> Liberibacter asiaticus (<italic>C</italic>Las), along with <italic>Ca.</italic> L. americanus and <italic>Ca</italic>. L. africanus, HLB has emerged as a primary threat to citrus production, particularly in regions like Florida, USA <xref rid="bib4" ref-type="bibr">[4]</xref>. CLas, the most prevalent among these pathogens, is vectored by the Asian citrus <italic>psyllid Diaphorina</italic> citri, leading to its colonization in the phloem tissues of citrus plants <xref rid="bib5" ref-type="bibr">[5]</xref>. This invasion results in severe phytopathological symptoms and extensive agricultural damage. The economic ramifications of HLB are staggering, with the disease inflicting multibillion-dollar losses globally, debilitating the citrus industry's financial stability <xref rid="bib6" ref-type="bibr">[6]</xref>. The pervasiveness of <italic>C</italic>Las and its profound impact on citrus crops underscore the necessity of an in-depth exploration of the PPIs between the host plants and the pathogen. Recently Yuan et al. <xref rid="bib7" ref-type="bibr">[7]</xref> mapped a network of <italic>Arabidopsis thaliana</italic> interactions with the bacterial pathogen <italic>Pseudomonas syringae</italic>, identifying new components involved in the plant immune response and paving the way for future plant disease control strategies. In a similar vein, Dyer et al. <xref rid="bib8" ref-type="bibr">[8]</xref> conducted a large-scale study on PPIs between human cells and <italic>Bacillus anthracis</italic>, the bacterium responsible for anthrax, revealing numerous potential therapeutic intervention targets. These studies underscore the vital role of host-pathogen PPIs in understanding the complex interplay between the host's defenses and the pathogen's invasion strategies. Therefore, these interactions form the cornerstone of research aiming to unravel the intricacies of infectious disease dynamics and serve as targets for potential therapeutic interventions.</p>
    <p id="p0010">Recent advancements in experimental methodologies, particularly in high-throughput screening and molecular imaging techniques, have significantly deepened our comprehension of host-pathogen PPIs <xref rid="bib9" ref-type="bibr">[9]</xref>. These innovations have enabled more precise identification and analysis of PPIs, shedding light on the intricate mechanisms of infection and host response, and paving the way for novel therapeutic interventions. Despite this progress, such techniques often come with high costs in terms of resources, time, and labor. Given the multitude of potential protein-interacting partners, the reliance on these methods becomes even more challenging, underlining the necessity for more cost-effective and efficient alternatives <xref rid="bib10" ref-type="bibr">[10]</xref>, <xref rid="bib11" ref-type="bibr">[11]</xref>. In contrast, computational methods for predicting host-pathogen interactions present themselves as a fitting solution to this pressing demand <xref rid="bib12" ref-type="bibr">[12]</xref>. These methods not only facilitate the detection of interactions but also have the potential to mine incomplete interaction maps for new discoveries, exponentially increasing the knowledge base regarding host-pathogen dynamics <xref rid="bib12" ref-type="bibr">[12]</xref>. Recently, Kaundal et al. <xref rid="bib13" ref-type="bibr">[13]</xref> developed deepHPI, an all-encompassing deep learning platform for the accurate prediction and visualization of HPPIs. Leveraging convolutional neural networks (CNN), their platform achieved a prediction accuracy of 96% on a curated dataset, significantly surpassing the performance of other methods on the same dataset. Complementing this work, Loaiza et al. introduced Pred-HPI <xref rid="bib14" ref-type="bibr">[14]</xref>, an integrated web server platform that employs sequence-based methods for the detection and visualization of host-pathogen interactions. PredHPI offered a sequence similarity-based approach and reached an accuracy of approximately 90% on a large-scale human-virus PPI dataset.</p>
    <p id="p0015">Despite the progress, these approaches primarily employed feature extraction strategies that represent host-pathogen protein pairs as fixed-length feature vectors, extracted from protein sequences. These sequence-based methods, while valuable, have limitations in terms of achieving high prediction accuracy, as they do not fully exploit the wealth of available structural and functional information. Recognizing these limitations, our study introduces a novel approach that leverages network-based integration methods for predicting host-pathogen interactions. Preliminary results have shown that this approach outperforms traditional sequence-based methods in terms of prediction accuracy scores.</p>
    <p id="p0020">To address these gaps, we introduced Deep-HPI-pred, an innovative R-shiny application that leverages advanced deep learning models. This application, for the first time, provides researchers with the autonomy to manually or automatically upload their training datasets for host-pathogen protein interaction prediction. The distinguishing characteristic of Deep-HPI-pred lies in its pioneering use of topological features for PPI prediction, as opposed to the conventional sequence-based features. This novel approach significantly enhances the sophistication and accuracy of predictions, marking a considerable advance in the bioinformatics field. Further, in order to substantiate the robustness and reliability of the Deep-HPI-pred model, our framework encompasses a rigorous validation protocol encompassing diverse biological systems, including plant-pathogen, human-virus, and animal-bacteria interactomes. This empirical evaluation across distinct host-pathogen pairs fortifies the scientific validity of the model's predictive prowess. In summary, as the premier R shiny application dedicated to predicting host-pathogen interactions, Deep-HPI-pred not only presents a groundbreaking tool for researchers, but also catalyzes a paradigm shift in our approach to understanding and predicting the intricate dynamics of infectious diseases.</p>
  </sec>
  <sec id="sec0010">
    <label>2</label>
    <title>Materials and methods</title>
    <sec id="sec0015">
      <label>2.1</label>
      <title>Gathering of true protein-protein interactions (PPIs) and data pre-processing</title>
      <p id="p0025">In this study, our initial host-pathogen PPI data was sourced from established host-pathogen interaction databases, including Pred-HPI <xref rid="bib14" ref-type="bibr">[14]</xref> and GreeningDB <xref rid="bib15" ref-type="bibr">[15]</xref>. These databases provided a broad spectrum of interactions, serving as a foundational reference for our research. However, to ensure a more robust and comprehensive dataset, we did not solely rely on these pre-compiled interactions. Recognizing the importance of data authenticity, we supplemented our database-derived PPIs with experimentally validated interactions. These additional PPIs were meticulously extracted from a wide range of peer-reviewed scientific literature. This step was vital for expanding the dataset and reinforcing its validity with real-world, experimentally confirmed interactions, thus substantially increasing the count of true positive PPIs in our study. The compiled data, encompassing both database-sourced and manually extracted PPIs, then underwent rigorous pre-processing. This crucial phase was aimed at maintaining the highest level of data integrity. We meticulously cleaned the combined dataset, carefully removing any duplicate entries to ensure a unique and non-redundant set of PPIs. This thorough cleaning process was instrumental in ensuring the reliability and accuracy of our dataset, thereby enhancing the overall quality of our analysis. By integrating these two diverse sources of data – established databases and manually verified literature – we aimed to strike a balance between the breadth and depth of our PPI dataset. This approach not only provided a comprehensive view of the host-pathogen interactions but also added a layer of validation to the interactions derived from databases, thereby bolstering the credibility and applicability of our study's findings.</p>
    </sec>
    <sec id="sec0020">
      <label>2.2</label>
      <title>Generation of negative protein-protein interactions (PPIs)</title>
      <p id="p0030">To augment our existing true positive PPIs dataset, the generation of negative PPIs was conducted. Negative PPIs, representing protein pairs that do not interact, serve as an important counterpart to positive interactions in our analysis <xref rid="bib16" ref-type="bibr">[16]</xref>. The generation of negative PPIs was a two-step process. The first step involved creating a random pairing between the protein sets of the selected pathogen and host. Given that there are <inline-formula><mml:math id="M1" altimg="si0001.svg"><mml:mi>m</mml:mi></mml:math></inline-formula> pathogen proteins and <inline-formula><mml:math id="M2" altimg="si0002.svg"><mml:mi>n</mml:mi></mml:math></inline-formula> host proteins, the total possible pairings, assuming no interaction between any two proteins, are <inline-formula><mml:math id="M3" altimg="si0003.svg"><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="M4" altimg="si0004.svg"><mml:mrow><mml:mi>P</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> be the set of pathogen proteins and <inline-formula><mml:math id="M5" altimg="si0005.svg"><mml:mrow><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> be the set of host proteins. The total possible random pairings <inline-formula><mml:math id="M6" altimg="si0006.svg"><mml:mi>R</mml:mi></mml:math></inline-formula> between pathogen proteins and host proteins is represented by the Cartesian product of the two sets:<disp-formula id="eqn0005"><label>(3.1)</label><mml:math id="M7" altimg="si0007.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>R</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>P</mml:mi><mml:mi mathvariant="normal">and</mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">H</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="p0035">In the second step, a subset was randomly selected from these random pairings to construct the negative dataset. This subset selection ensured that the number of negative PPIs matched the number of true positive PPIs in our dataset. This balancing act serves as a cornerstone for the impending deep learning model training, underpinning an unbiased and balanced approach to the development of a robust predictive model. In current study, the generation of negative PPIs was integral to creating a balanced dataset for effective host-pathogen PPI prediction. This approach aligns with methodologies adopted in similar studies, ensuring robustness and validity in our predictive model. Specifically, Chen et al. <xref rid="bib17" ref-type="bibr">[17]</xref> employed a comparable strategy in their research, where negative PPIs were systematically generated through random pairing, thereby maintaining an equivalent size of negative and positive datasets. This method of balancing is crucial, as underscored by Scott et al. <xref rid="bib18" ref-type="bibr">[18]</xref>, who highlighted the potential risks associated with imbalanced datasets in PPI predictions, such as overfitting and poor model generalization. Our methodology addresses these concerns by matching the number of negative PPIs with positive PPIs, thereby fostering a more accurate and unbiased predictive model.</p>
    </sec>
    <sec id="sec0025">
      <label>2.3</label>
      <title>Assembly of dataset</title>
      <p id="p0040">The assembly of the final dataset was carried out by integrating both positive <inline-formula><mml:math id="M8" altimg="si0008.svg"><mml:mi mathvariant="italic">P_set</mml:mi></mml:math></inline-formula> and negative <inline-formula><mml:math id="M9" altimg="si0009.svg"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">N_set</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> PPIs. The negative PPI dataset, representing non-interactions, was carefully merged with the positive PPI dataset. This integrated approach not only maintains the balance between interaction and non-interaction data but also encapsulates the full range of possible protein interactions. Mathematically, the final dataset <inline-formula><mml:math id="M10" altimg="si0010.svg"><mml:mi mathvariant="italic">F_set</mml:mi></mml:math></inline-formula> is represented as the union of the positive PPI dataset <inline-formula><mml:math id="M11" altimg="si0011.svg"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">set</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and the negative PPI dataset <inline-formula><mml:math id="M12" altimg="si0012.svg"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">N_set</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>:<disp-formula id="eqn0010"><label>(3.2)</label><mml:math id="M13" altimg="si0013.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>F</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="italic">set</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>P</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="italic">set</mml:mi><mml:mo>∪</mml:mo><mml:mi>N</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="italic">set</mml:mi></mml:mrow></mml:math></disp-formula></p>
      <p id="p0045">This final dataset, <inline-formula><mml:math id="M14" altimg="si0014.svg"><mml:mi mathvariant="italic">F_set</mml:mi></mml:math></inline-formula>, encapsulates a broad spectrum of potential PPIs. It is primed for rigorous training within the deep learning model, offering a well-rounded, realistic, and balanced resource for the analysis. This strategic assembly of positive and negative interactions underpins the analytical framework, fostering a robust and comprehensive model tailored for effective host-pathogen interaction prediction.</p>
    </sec>
    <sec id="sec0030">
      <label>2.4</label>
      <title>Feature extraction: quantification of network topology parameters</title>
      <p id="p0050">The creation of the final PPI dataset marked the transition to a critical phase - feature extraction, which is the quantification of the topological parameters of the constructed network <xref rid="bib19" ref-type="bibr">[19]</xref>. The intent behind this step is to decipher and numerically articulate the intricate structural patterns within the protein network. Our primary objective was to leverage network topology for the accurate prediction of interactions between host and viral proteins. To achieve this, current study strategically employed a range of centrality measures, namely Degree Centrality, Betweenness Centrality, Closeness Centrality, Eigenvector Centrality, PageRank Centrality, Hub Score, and Eccentricity. This was achieved with the assistance of the topological algorithms incorporated within the Igraph package of R <xref rid="bib20" ref-type="bibr">[20]</xref>. These features were not selected on the basis of their individual merits alone but were chosen for their collective ability to provide a comprehensive view of the network dynamics, crucial for understanding the intricacies of host-pathogen PPI networks. This methodological approach is supported by the work of Przulj et al. <xref rid="bib21" ref-type="bibr">[21]</xref> and Ashtiani et al. <xref rid="bib22" ref-type="bibr">[22]</xref>, who have demonstrated the importance of network topology in understanding PPI networks and the utility of centrality measures in identifying key proteins within these networks. These topological features assist in identifying the potential interaction points and key nodes within the host-pathogen network.</p>
      <p id="p0055">These features, ranging from quantifying the number of direct connections a protein node has (Degree Centrality) to calculating the maximum distance from a node to all other nodes (Eccentricity), offer a comprehensive view of each protein's role and importance within the overall network structure. Degree Centrality and Hub Score, for instance, shed light on the interaction richness and the core component status of a protein respectively <xref rid="bib22" ref-type="bibr">[22]</xref>, <xref rid="bib23" ref-type="bibr">[23]</xref>. For a graph <inline-formula><mml:math id="M15" altimg="si0015.svg"><mml:mi>G</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="M16" altimg="si0016.svg"><mml:mi>N</mml:mi></mml:math></inline-formula> vertices, the Degree Centrality (DC) of a vertex <inline-formula><mml:math id="M17" altimg="si0017.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> is given by <inline-formula><mml:math id="M18" altimg="si0018.svg"><mml:mrow><mml:mi mathvariant="italic">DC</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Degree</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M19" altimg="si0019.svg"><mml:mrow><mml:mi mathvariant="italic">Degree</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represents the number of edges incident on vertex <inline-formula><mml:math id="M20" altimg="si0020.svg"><mml:mi>v</mml:mi></mml:math></inline-formula>. Similarly, Betweenness Centrality, Closeness Centrality, and PageRank Centrality provide insights into a protein's role as a network connector, its centrality, and its importance based on neighbouring proteins. Betweenness Centrality is defined as <inline-formula><mml:math id="M21" altimg="si0021.svg"><mml:mrow><mml:mi mathvariant="italic">BC</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>σ</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="italic">t</mml:mi></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mspace width="1em"/></mml:mrow></mml:math></inline-formula>for all <inline-formula><mml:math id="M22" altimg="si0022.svg"><mml:mrow><mml:mi mathvariant="italic">s</mml:mi><mml:mo>≠</mml:mo><mml:mi>v</mml:mi><mml:mo>≠</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M23" altimg="si0023.svg"><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>) is the total number of shortest paths from node <inline-formula><mml:math id="M24" altimg="si0024.svg"><mml:mi>s</mml:mi></mml:math></inline-formula> to node <inline-formula><mml:math id="M25" altimg="si0025.svg"><mml:mi>t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M26" altimg="si0026.svg"><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>) is the number of those paths that pass through <inline-formula><mml:math id="M27" altimg="si0027.svg"><mml:mi>v</mml:mi></mml:math></inline-formula>. For a connected graph G with N vertices, the Closeness Centrality (CC) of a vertex v is given by <inline-formula><mml:math id="M28" altimg="si0028.svg"><mml:mrow><mml:mi mathvariant="italic">CC</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="M29" altimg="si0029.svg"><mml:mi>t</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="M30" altimg="si0030.svg"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represents the shortest-path distance between <inline-formula><mml:math id="M31" altimg="si0031.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M32" altimg="si0032.svg"><mml:mi>t</mml:mi></mml:math></inline-formula>.</p>
      <p id="p0060">On the other hand, Eigenvector Centrality allots relative scores to all nodes in the network, acknowledging that connections to high-scoring nodes contribute more to the overall node score <xref rid="bib24" ref-type="bibr">[24]</xref>. The Eigenvector Centrality (EC) of a node <inline-formula><mml:math id="M33" altimg="si0033.svg"><mml:mi>i</mml:mi></mml:math></inline-formula> is defined as <inline-formula><mml:math id="M34" altimg="si0034.svg"><mml:mrow><mml:mi mathvariant="italic">EC</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi mathvariant="italic">EC</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="M35" altimg="si0035.svg"><mml:mi>j</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="M36" altimg="si0036.svg"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the elements of the adjacency matrix and <inline-formula><mml:math id="M37" altimg="si0037.svg"><mml:mi>λ</mml:mi></mml:math></inline-formula> is a constant. PageRank Centrality is computed using the formula <inline-formula><mml:math id="M38" altimg="si0038.svg"><mml:mrow><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="badbreak">+</mml:mo><mml:mi mathvariant="italic">d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="badbreak">+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo linebreak="badbreak">+</mml:mo><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="M39" altimg="si0039.svg"><mml:mrow><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>is the PageRank of page A, <inline-formula><mml:math id="M40" altimg="si0040.svg"><mml:mrow><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the PageRank of pages <inline-formula><mml:math id="M41" altimg="si0041.svg"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> which link to page <inline-formula><mml:math id="M42" altimg="si0042.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="italic">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>is the number of outbound links on page <inline-formula><mml:math id="M43" altimg="si0043.svg"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M44" altimg="si0044.svg"><mml:mi>d</mml:mi></mml:math></inline-formula> is a damping factor which set between 0 and 1. Finally, the Eccentricity (ECC) of a vertex <inline-formula><mml:math id="M45" altimg="si0045.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> in a graph is the maximum distance from <inline-formula><mml:math id="M46" altimg="si0046.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> to all other vertices, calculated by <inline-formula><mml:math id="M47" altimg="si0047.svg"><mml:mrow><mml:mi mathvariant="italic">ECC</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>max</mml:mi><mml:mi mathvariant="italic">d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="italic">t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="italic">for all t</mml:mi><mml:mo>≠</mml:mo><mml:mi mathvariant="italic">v</mml:mi></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="M48" altimg="si0048.svg"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="italic">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the shortest-path distance between the vertices <inline-formula><mml:math id="M49" altimg="si0049.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M50" altimg="si0050.svg"><mml:mrow><mml:mi>t</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
    </sec>
    <sec id="sec0035">
      <label>2.5</label>
      <title>Deep learning models</title>
      <p id="p0065">Current study leveraged the potential of deep learning through the implementation of three distinguished models, namely Identity Convolutional Neural Networks (ID-CNN), Recurrent Neural Networks (RNN), and Multi-Layer Perceptrons (MLP). In R, the 'keras' package was employed, providing a high-level neural networks API on the TensorFlow platform. Additionally, 'caret' in R, facilitating access to numerous machine learning algorithms, was used in conjunction with 'tensorflow' and 'reticulate'. 'Tensorflow' offers an R interface for TensorFlow, while 'reticulate' enables Python integration within the R environment. Each model's unique architecture processes input data differently, offering diversified data interpretations, thereby improving the accuracy and reliability of the predicted host-pathogen PPIs.</p>
      <sec id="sec0040">
        <label>2.5.1</label>
        <title>ID-Convolutional Neural Networks (CNN)</title>
        <p id="p0070">The ID-Convolutional Neural Networks (CNN) model forms an integral part of the deep learning approach. The ID-CNN are primarily known for their application in image processing, but their utility extends to PPI prediction as well <xref rid="bib25" ref-type="bibr">[25]</xref>. The core architecture of the ID-CNN model implemented is designed with multiple layers, each layer equipped with a specific role and function to perform <xref rid="bib26" ref-type="bibr">[26]</xref>. The architecture commenced with a 2D convolution layer equipped with 16 filters of kernel size <inline-formula><mml:math id="M51" altimg="si0051.svg"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, employing the Rectified Linear Unit (ReLU) activation function <inline-formula><mml:math id="M52" altimg="si0052.svg"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. This layer was instrumental in the detection of initial patterns or features within the data, with its output, <inline-formula><mml:math id="M53" altimg="si0053.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> given by <inline-formula><mml:math id="M54" altimg="si0054.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>*</mml:mo><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Following this, a second 2D convolution layer was added, this one featuring 32 filters and a kernel size of <inline-formula><mml:math id="M55" altimg="si0055.svg"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The same ReLU activation function was used, aiding in the learning of more complex patterns based on the initial features. The output of this layer, <inline-formula><mml:math id="M56" altimg="si0056.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> was calculated by <inline-formula><mml:math id="M57" altimg="si0057.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>O</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
        <p id="p0075">Post the convolutional operations, a global max pooling layer was implemented, denoted by <inline-formula><mml:math id="M58" altimg="si0058.svg"><mml:mrow><mml:mi>M</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula>This step reduced the dimensionality of the model, mitigating computational complexity and overfitting. Subsequently, two dense layers were added to the architecture. The first of these layers consisted of 64 neurons, its output, <inline-formula><mml:math id="M59" altimg="si0059.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, defined by <inline-formula><mml:math id="M60" altimg="si0060.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo>*</mml:mo><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The final layer was a single neuron employing a sigmoid activation function<inline-formula><mml:math id="M61" altimg="si0061.svg"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, producing the model output:<disp-formula id="eqn0015"><label>(3.3)</label><mml:math id="M62" altimg="si0062.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>P</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>O</mml:mi><mml:mn>3</mml:mn><mml:mo>*</mml:mo><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0080">The entire model was trained using the Adam optimizer and the binary cross-entropy loss function, a suitable choice for the binary nature of the task. This function is given by:<disp-formula id="eqn0020"><label>(3.4)</label><mml:math id="M63" altimg="si0063.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi mathvariant="italic">Loss</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo linebreak="badbreak">−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">ylog</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0085">It's noteworthy that the input data to the model included degree centrality features for both pathogen and host proteins, which was normalized before being fed into the model.</p>
      </sec>
      <sec id="sec0045">
        <label>2.5.2</label>
        <title>Recurrent neural network</title>
        <p id="p0090">In the pursuit of host-pathogen protein interaction analysis, a Recurrent Neural Network (RNN) was incorporated, ideal due to the inherent sequential and interconnected nature of the protein data <xref rid="bib27" ref-type="bibr">[27]</xref>. An RNN model was devised via the Keras library in R <xref rid="bib28" ref-type="bibr">[28]</xref>, comprising of an input layer, a hidden layer, and an output layer. The input layer is designed to accept feature vectors signifying both the pathogen and host protein's network centrality measures, reshaped suitably for an RNN. The hidden layer utilizes simple RNN units governed by the ReLU activation function. On a mathematical note, suppose the input sequence is <inline-formula><mml:math id="M64" altimg="si0064.svg"><mml:mrow><mml:mi>X</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The new hidden state at time <inline-formula><mml:math id="M65" altimg="si0065.svg"><mml:mi mathvariant="italic">t</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="M66" altimg="si0066.svg"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is calculated by the equation<disp-formula id="eqn0025"><label>(3.5)</label><mml:math id="M67" altimg="si0067.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mspace width="1em"/></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M68" altimg="si0068.svg"><mml:mi>f</mml:mi></mml:math></inline-formula> is the ReLU function, <inline-formula><mml:math id="M69" altimg="si0069.svg"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the input at time <inline-formula><mml:math id="M70" altimg="si0070.svg"><mml:mi>t</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="M71" altimg="si0071.svg"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="M72" altimg="si0072.svg"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="M73" altimg="si0073.svg"><mml:mi>b</mml:mi></mml:math></inline-formula> stand for the weights and bias parameters. The output layer is a dense layer featuring a single unit using a sigmoid activation function, fitting for the binary classification task as it computes probabilities. Mathematically, the output <inline-formula><mml:math id="M74" altimg="si0074.svg"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> at time <inline-formula><mml:math id="M75" altimg="si0075.svg"><mml:mi>t</mml:mi></mml:math></inline-formula> is given by the equation<disp-formula id="eqn0030"><label>(3.6)</label><mml:math id="M76" altimg="si0076.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi mathvariant="italic">sigmoid</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math id="M77" altimg="si0077.svg"><mml:mi mathvariant="italic">sigmoid</mml:mi></mml:math></inline-formula> as the sigmoid functions and <inline-formula><mml:math id="M78" altimg="si0078.svg"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M79" altimg="si0079.svg"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>as the weight and bias parameters for the output layer respectively. The training of this model was accomplished using the Adam optimization algorithm, an efficient extension to stochastic gradient descent. The loss function applied was binary cross-entropy, appropriate for binary classification problems. On a mathematical level, assuming the target (true value) as <inline-formula><mml:math id="M80" altimg="si0080.svg"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and the predicted output as <inline-formula><mml:math id="M81" altimg="si0081.svg"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the binary cross-entropy loss for the output at time <inline-formula><mml:math id="M82" altimg="si0082.svg"><mml:mi>t</mml:mi></mml:math></inline-formula> is calculated as<disp-formula id="eqn0035"><label>(3.7)</label><mml:math id="M83" altimg="si0083.svg"><mml:mrow><mml:mspace width="1em"/><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0095">This framework of equations directs the forward propagation, learning, and prediction process of the RNN model within the study.</p>
      </sec>
      <sec id="sec0050">
        <label>2.5.3</label>
        <title>Multilayer Perceptron (MLP)</title>
        <p id="p0100">The Multi-layer Perceptron (MLP), an artificial neural network type, was selected for this analysis owing to the networked architecture of the protein data. The MLP's ability to decipher complex, non-linear relationships through multiple layers of nodes made it a perfect fit for this study. Each node, symbolizing a neuron, is equipped with an activation function that aids in transforming the input data <xref rid="bib29" ref-type="bibr">[29]</xref>. The MLP model employed for this study comprises two hidden layers, containing 10 and 20 neurons respectively. These hidden layers form the heart of the MLP, endowing the network with its capacity to abstract and transform the input data. As the input data flows through these hidden layers, the model unveils intricate patterns and structures, allowing for more accurate prediction outcomes.</p>
        <p id="p0105">A critical aspect of this model is the logistic activation function. Chosen for its ability to map any real-valued number into a range between 0 and 1, the logistic function is a prime choice for binary classification tasks. It proves ideal for differentiating between interaction and non-interaction within the network-based protein data. Mathematically, in the hidden layers, each neuron computes a weighted sum of the inputs, adds a bias term and applies the logistic activation function. If we denote the input to a neuron as <inline-formula><mml:math id="M84" altimg="si0084.svg"><mml:mrow><mml:mi>x</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, weights as <inline-formula><mml:math id="M85" altimg="si0085.svg"><mml:mrow><mml:mi>w</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and bias as <inline-formula><mml:math id="M86" altimg="si0086.svg"><mml:mi>b</mml:mi></mml:math></inline-formula>, the output of the neuron <inline-formula><mml:math id="M87" altimg="si0087.svg"><mml:mi>y</mml:mi></mml:math></inline-formula> is computed as follows:<disp-formula id="eqn0040"><label>(3.8)</label><mml:math id="M88" altimg="si0088.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>y</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi mathvariant="italic">logistic</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p id="p0110">The MLP model training was regulated by a learning rate of 0.01 and a convergence threshold of 0.1. These values were fine-tuned to optimize the learning process and to control the pace at which the model learns from the protein data. Balancing the learning rate was essential to avoid erratic learning behaviors and to prevent the model from learning too slowly. A convergence threshold of 0.1 was set to ensure the learning process perseveres until the error on the training data reaches an acceptably low point.</p>
        <p id="p0115">Additionally, the model utilized the backpropagation algorithm to adjust its weights and bias values. This adjustment minimizes the discrepancy between the actual and predicted outputs over numerous iterations, or epochs, thereby progressively enhancing the model's accuracy. During each epoch, the backpropagation algorithm implements two primary steps for each training sample: Forward Propagation: The predicted output (<inline-formula><mml:math id="M89" altimg="si0089.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is calculated using the current weights and bias values. On the other hand, the weights and bias terms are updated based on the gradient of the loss function with respect to the weights and bias in back propagation. If we denote the loss function as <inline-formula><mml:math id="M90" altimg="si0090.svg"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M91" altimg="si0091.svg"><mml:mi>y</mml:mi></mml:math></inline-formula> is the actual output and (<inline-formula><mml:math id="M92" altimg="si0092.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the predicted output, the weights and bias are updated as follows:<disp-formula id="eqn0045"><label>(3.9)</label><mml:math id="M93" altimg="si0093.svg"><mml:mrow><mml:mspace width="1em"/><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>η</mml:mi><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="eqn0050"><label>(3.10)</label><mml:math id="M94" altimg="si0094.svg"><mml:mrow><mml:mi>b</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>b</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>η</mml:mi><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p id="p0120">Here <inline-formula><mml:math id="M95" altimg="si0095.svg"><mml:mi>η</mml:mi></mml:math></inline-formula> is the learning rate and <inline-formula><mml:math id="M96" altimg="si0096.svg"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></inline-formula> and <inline-formula><mml:math id="M97" altimg="si0097.svg"><mml:mrow><mml:mspace width="1em"/><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> are the gradients of the loss function with respect to the weights and bias, computed using the chain rule of differentiation.</p>
      </sec>
    </sec>
    <sec id="sec0055">
      <label>2.6</label>
      <title>Evaluation of models</title>
      <p id="p0125">To thoroughly assess the predictive competence of the models, six widely recognized evaluation metrics were employed: Precision, Accuracy, Sensitivity, Specificity, F1-score, and Matthew's Correlation Coefficient (MCC). These metrics provide a comprehensive understanding of the model's performance, considering both the positive and negative classes of the dataset, as well as the balance between them.</p>
      <p id="p0130">Precision (Pre) is a metric that reflects the exactness of the positive predictions made by the model <xref rid="bib30" ref-type="bibr">[30]</xref>. It is calculated as the ratio of true positives to the sum of true positives and false positives. Precision is essential as it focuses on the proportion of true positive predictions among all positive predictions, a crucial measure when false positives can significantly skew results, as discussed in the work of Powers et al. <xref rid="bib31" ref-type="bibr">[31]</xref>. Mathematically, it's represented as:<disp-formula id="eqn0055"><label>(3.11)</label><mml:math id="M98" altimg="si0098.svg"><mml:mrow><mml:mi mathvariant="italic">Precision</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FalsePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0135">Accuracy (Acc) measures the proportion of total predictions that are correct, irrespective of whether they are positive or negative <xref rid="bib32" ref-type="bibr">[32]</xref>. However, when used alongside the other metrics, it contributes to a holistic understanding of the model’s effectiveness, as suggested by Johnson and Khoshgoftaar <xref rid="bib33" ref-type="bibr">[33]</xref>. It is computed as the sum of true positives and true negatives divided by the total number of predictions, as shown:<disp-formula id="eqn0060"><label>(3.12)</label><mml:math id="M99" altimg="si0099.svg"><mml:mrow><mml:mi mathvariant="italic">Accuracy</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="italic">TrueNegatives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TotalPredictions</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0140">Sensitivity (Sn), also known as recall or true positive rate, quantifies the ability of the model to correctly identify positive instances <xref rid="bib34" ref-type="bibr">[34]</xref>. It is calculated as the ratio of true positives to the sum of true positives and false negatives, which are expressed as:<disp-formula id="eqn0065"><label>(3.13)</label><mml:math id="M100" altimg="si0100.svg"><mml:mrow><mml:mi mathvariant="italic">Sensitivity</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FalseNegatives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">FN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0145">Specificity (Sp) is the metric that reflects the model's ability to correctly identify negative instances <xref rid="bib35" ref-type="bibr">[35]</xref>. It is the ratio of true negatives to the sum of true negatives and false positives, represented as:<disp-formula id="eqn0070"><label>(3.14)</label><mml:math id="M101" altimg="si0101.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi mathvariant="italic">Specificity</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TrueNegatives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TrueNegatives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FalsePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0150">F1-score is the harmonic mean of Precision and Sensitivity. It is particularly useful when dealing with imbalanced datasets as it takes both false positives and false negatives into account <xref rid="bib36" ref-type="bibr">[36]</xref>. The F1-score provides a balanced view of the model’s performance on both classes, ensuring that neither false positives nor false negatives are disproportionately affecting the model's evaluation, as indicated in studies by Kakkar et al. <xref rid="bib37" ref-type="bibr">[37]</xref>. The formula for the F1-score is:<disp-formula id="eqn0075"><label>(3.15)</label><mml:math id="M102" altimg="si0102.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi mathvariant="italic">score</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Precision</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="italic">Sensitivity</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Precision</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">Sensitivity</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p id="p0155">Matthew's Correlation Coefficient (MCC) is a more robust metric as it considers all values in the confusion matrix (true positives, true negatives, false positives, and false negatives) <xref rid="bib36" ref-type="bibr">[36]</xref>. The range of MCC is between − 1 and + 1, where + 1 represents a perfect prediction, 0 indicates a random prediction, and − 1 denotes an inverse prediction. The MCC is calculated as follows:<disp-formula id="eqn0080"><label>(3.16)</label><mml:math id="M103" altimg="si0103.svg"><mml:mrow><mml:mi mathvariant="italic">MCC</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="italic">FN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0160">These performance evaluation metrics were critical in systematically comparing and assessing the predictive capabilities of the 1D-CNN, RNN, and MLP models in the context of host-pathogen interaction prediction. By utilizing these six metrics, it became possible to analyze each model's proficiency from various dimensions such as precision, accuracy, sensitivity, specificity, and overall correlation. It is important to underscore that this comprehensive evaluation framework facilitated the discernment of the best-suited deep learning model for predicting potential interactions. This strategic methodology considerably enhanced the selection process, ensuring the identification of the most adept model to navigate the complex landscape of host-pathogen interactions.</p>
    </sec>
  </sec>
  <sec id="sec0060">
    <label>3</label>
    <title>Results</title>
    <sec id="sec0065">
      <label>3.1</label>
      <title>Dataset assembly and preliminary analysis</title>
      <p id="p0165">In order to predict host-pathogen interactions, the Citrus-HLB (Huanglongbing) interactome was selected as the primary case study. The primary sources of data for this study included the Pred-HPI and GreeningDB databases. The Pred-HPI database, known for its predictive modeling features, was combined with GreeningDB, a specialized database for citrus greening disease interactions, to provide a comprehensive range of interaction data. The true PPI data between Citrus and the Candidatus Liberibacter asiaticus (CLas) bacteria was sourced from the Greening and Pred-HPI database. This resulted in the retrieval of 1011 host-pathogen PPIs (45 unique pathogen proteins and 359 unique host proteins) from these databases. Additionally, 5 experimentally validated interactions were incorporated into the dataset, leading to a total of 1016 true PPIs. The dataset comprised of 45 unique pathogen proteins and 359 unique host proteins, accumulating a network of 404 nodes with a potential for 1016 possible interactions.</p>
    </sec>
    <sec id="sec0070">
      <label>3.2</label>
      <title>Construction of negative interaction sets with varied ratios</title>
      <p id="p0170">In this study, a common strategy was adopted to generate negative Protein-Protein Interactions (PPIs) based on traditional approaches found in the literature. This strategy involved creating random pairings between distinct protein sets - specifically, the selected pathogen proteins and host proteins. Subsequently, from these random pairings, subsets were strategically chosen to represent negative interactions. The importance of this lies in creating a comparative base against which positive interactions can be evaluated, thereby enhancing the predictive power of the developed models.</p>
      <p id="p0175">To further enrich the analysis and examine the robustness of the models, negative interactions were selected with varying ratios: 1:1 and 1:10. This led to the formation of two distinct datasets. The first, balanced, set comprised 1016 positive and 1016 negative PPIs, while the second, imbalanced, set encompassed 1016 positive and 10160 negative PPIs. The implementation of varying negative to positive ratios was essential in evaluating the robustness and resilience of the predictive models under different levels of data imbalance, a common scenario in biological studies. In the development of predictive models for host-pathogen PPIs, the selection of an appropriate class ratio is pivotal to balance between model performance and realistic representation of the dataset. For this study, a 1:10 class ratio was strategically chosen, aiming to effectively address the challenges presented by highly imbalanced datasets and to ensure adequate representation of the minority class. This decision is rooted in evidence from the literature, where extremely imbalanced ratios like 1:25 or 1:100, commonly used in PPI studies, have been shown to introduce a significant bias towards the majority class, leading to a high incidence of false negatives, as elucidated by Chen et al. <xref rid="bib17" ref-type="bibr">[17]</xref>. Conversely, ratios that closely approximate equal distribution, such as 1:1, often do not accurately reflect the actual distribution encountered in biological datasets, where non-interacting pairs are generally more prevalent. The adoption of a 1:10 ratio represents this balance, reducing the risk of bias towards the majority class while still mirroring the typical distribution in biological datasets. Supporting this selection, preliminary experiments within the study demonstrated that a 1:10 ratio optimally balances sensitivity and specificity. This finding is in alignment with the results presented by Lei et al. <xref rid="bib38" ref-type="bibr">[38]</xref>, where similar class ratios have been shown to enhance model performance in accurately identifying true positives without disproportionately increasing false negatives. Hence, this ratio has been found to effectively represent the real-world distribution of PPI datasets, ensuring sensitivity towards the minority class and enhancing the overall reliability of the predictive models.</p>
    </sec>
    <sec id="sec0075">
      <label>3.3</label>
      <title>Performance of the features in modeling host-pathogen interactions</title>
      <p id="p0180">The feature extraction process was integral to the deployment of the 1D-CNN, RNN, and MLP models. Seven key features, namely, Degree Centrality, Betweenness Centrality, Closeness Centrality, Eigenvector Centrality, PageRank Centrality, Hub Score, and Eccentricity were selected to comprehensively describe each host-pathogen protein pair. For each host-pathogen protein pair, these attributes were extracted and consolidated into a unified vector. This process resulted in comprehensive Protein-Protein Interaction (PPI) feature vectors, embodying the synergistic characteristics of both host and pathogen proteins. Each feature was individually assessed across the three models to deduce their unique contribution to the predictive accuracy of the models, fostering a comprehensive understanding of the interplay between individual features and model performance.</p>
      <p id="p0185">An effective evaluation of the models necessitated the careful construction of the independent datasets. Contrary to common practice where cross-validation is predominantly used due to its superior performance with unseen datasets, a different strategy was implemented for this study. Specifically, a fifth of the PPIs, encompassing both positive and negative interactions, were randomly allocated to form the independent dataset. The remainder of the PPIs, involving both types of interactions, were amalgamated into the training set. This approach, though unconventional, ensured a robust and comprehensive evaluation of the models' performance.</p>
      <sec id="sec0080">
        <label>3.3.1</label>
        <title>Assessment of 1D-CNN model at different class ratios</title>
        <p id="p0190">Analyzing the performance of the 1D-CNN model at different class ratios resulted in significant findings. The model's efficacy was assessed using multiple protein features, and statistical parameters such as Sensitivity, Specificity, Precision, Accuracy, F1-score, and MCC were evaluated (<xref rid="fig0005" ref-type="fig">Fig. 1</xref>). <xref rid="tbl0005" ref-type="table">Table 1</xref> provides an exhaustive assessment of the 1D-CNN model's performance at a class ratio of 1:1. During independent testing, the Degree Centrality feature exhibited strong performance with a Sensitivity, Specificity, and Precision of 0.8374, 0.8423, and 0.8415 respectively. The model's Accuracy and F1-score stood at 0.8399 and 0.8395, showcasing balanced categorization capabilities. Additionally, the MCC value of 0.6798 denotes reliable prediction of both classes. However, the standout feature at the 1:1 class ratio was Eigenvector Centrality. During independent testing, this feature achieved the highest Accuracy, F1-score, and MCC values, which were 0.8522, 0.8648, and 0.7171 respectively, indicating the model's proficiency in discerning true positives and negatives. Similar trends were observed during 5-fold cross-validation. Degree Centrality showed an Accuracy of 0.8529, an F1-score of 0.8568, and an MCC of 0.7074. Meanwhile, Eigenvector Centrality continued to excel with an Accuracy of 0.8388, an F1-score of 0.8493, and an MCC of 0.6844.<fig id="fig0005"><label>Fig. 1</label><caption><p>Performance evaluation results of CNN model at class ratios 1:1: (<bold>A</bold>) Independent testing (<bold>B</bold>) 5-Fold cross validation. Performance evaluation results at different class ratios 1:10: (<bold>C</bold>) Independent testing (<bold>D</bold>) 5-Fold cross validation.</p></caption><alt-text id="at0005">Fig. 1</alt-text><graphic xlink:href="gr1" id="lk0005"/></fig><table-wrap position="float" id="tbl0005"><label>Table 1</label><caption><p>1D-CNN model performance for each protein feature at class ratio of 1:1 and 1:10. Performance values of independent testing and 5-fold cross validation are shown within a same cell as follow: independent testing <italic>/</italic> 5-fold cross validation.</p></caption><alt-text id="at0030">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th><bold>Network features</bold></th><th><bold>Sensitivity</bold></th><th><bold>Specificity</bold></th><th><bold>Precision</bold></th><th><bold>Accuracy</bold></th><th><bold>F1-score</bold></th><th><bold>MCC</bold></th></tr><tr><th colspan="7"><bold>Class ratio 1:1</bold></th></tr></thead><tbody><tr><td><bold>Degree_centrality</bold></td><td>0.8374 / 0.8787</td><td>0.8423 / 0.8272</td><td>0.8415 / 0.8366</td><td>0.8399 / 0.8529</td><td>0.8395 / 0.8568</td><td>0.6798 / 0.7074</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.8325 / 0.7928</td><td>0.7733 / 0.8222</td><td>0.7860 / 0.8181</td><td>0.8029 / 0.8075</td><td>0.8086 / 0.8046</td><td>0.6069 / 0.6163</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.1576 / 0.1678</td><td>0.9901 / 0.9889</td><td>0.9411 / 0.9479</td><td>0.5738 / 0.5784</td><td>0.2700 / 0.2841</td><td>0.2667 / 0.2767</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9458 / 0.9056</td><td>0.7586 / 0.7721</td><td>0.7966 / 0.8001</td><td>0.8522 / 0.8388</td><td>0.8648 / 0.8493</td><td>0.7171 / 0.6844</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.0314 / 0.5184</td><td>0.4546 / 0.9265</td><td>0.5571 / 0.8768</td><td>0.6723 / 0.7224</td><td>0.53 / 0.6510</td><td>0.005 /0.4878</td></tr><tr><td><bold>Hub_score</bold></td><td>0.9261 / 0.9130</td><td>0.7389 / 0.7696</td><td>0.7800 / 0.7994</td><td>0.8325 / 0.8413</td><td>0.8468 / 0.8520</td><td>0.6769 / 0.6905</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.8817 / 0.8468</td><td>0.8128 / 0.8750</td><td>0.8248 / 0.8716</td><td>0.8472 / 0.8609</td><td>0.8523 / 0.8581</td><td>0.6962 / 0.7237</td></tr><tr><td colspan="7"><bold>Class ratio 1:10</bold></td></tr><tr><td><bold>Degree_centrality</bold></td><td>0.9856 / 0.9885</td><td>0.4227 / 0.4112</td><td>0.9400 / 0.9445</td><td>0.9303 / 0.9367</td><td>0.9623 / 0.9660</td><td>0.5355 / 0.5398</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.9774 / 0.9899</td><td>0.3939 / 0.3192</td><td>0.9433 / 0.9359</td><td>0.9259 / 0.9291</td><td>0.9600 / 0.9621</td><td>0.4610 / 0.4644</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.9977 / 1</td><td>0.0215 / 0.0234</td><td>0.9184 / 0.9107</td><td>0.9161 / 0.9109</td><td>0.9561 / 0.9532</td><td>0.0769 / 0.1423</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9925 / 0.9856</td><td>0.5475 / 0.5794</td><td>0.9524 / 0.9588</td><td>0.9486 / 0.9484</td><td>0.9721 / 0.9720</td><td>0.6744 / 0.6561</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.9950 / 0.9960</td><td>0.0945 / 0.0854</td><td>0.9177 / 0.9155</td><td>0.914 / 0.9129</td><td>0.9548 / 0.9540</td><td>0.2265 / 0.2178</td></tr><tr><td><bold>Hub_score</bold></td><td>0.9808 /0.9931</td><td>0.6323 / 0.53178</td><td>0.9638 / 0.9548</td><td>0.9491 / 0.9510</td><td>0.9722 / 0.9735</td><td>0.6698 / 0.6652</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.9931 / 0.9823</td><td>0.35 / 0.3871</td><td>0.9397 / 0.9410</td><td>0.9357 / 0.9279</td><td>0.9656 / 0.9612</td><td>0.5150 /0.4814</td></tr></tbody></table></table-wrap></p>
        <p id="p0195">On the other hand, the model's performance with a more imbalanced class ratio of 1:10 is elucidated in <xref rid="tbl0005" ref-type="table">Table 1</xref>. With this ratio, the Eigenvector Centrality and Hub Score features emerged as significant, recording high accuracy levels in both independent testing and 5-fold cross-validation. These results suggest that the model can uphold its performance even with a heavily skewed class distribution.</p>
      </sec>
      <sec id="sec0085">
        <label>3.3.2</label>
        <title>Comprehensive analysis of RNN model performance across features and class ratios</title>
        <p id="p0200">The RNN model's performance was thoroughly assessed across two disparate class ratios of 1:1 and 1:10, with each protein feature being evaluated individually (<xref rid="fig0010" ref-type="fig">Fig. 2</xref>). This extensive appraisal facilitated a clear distinction of the model's strengths and areas requiring improvement. At the 1:1 class ratio, several noteworthy findings were observed. The feature that stood out the most was Eigenvector Centrality, which achieved the highest metrics in the majority of categories. These included an Accuracy of 0.8694 during independent testing, a similarly impressive F1-score of 0.8716, and an MCC of 0.7393, the highest among all features. The Degree Centrality, although not leading, displayed considerable efficacy, particularly with its Sensitivity of 0.8088 and an MCC of 0.6887. However, Closeness Centrality demonstrated difficulties in this scenario, especially indicated by a negative MCC during independent testing, signifying a possible shortcoming of the RNN model when handling this feature.<fig id="fig0010"><label>Fig. 2</label><caption><p>Performance evaluation results of RNN model at class ratios 1:1: (<bold>A</bold>) Independent testing (<bold>B</bold>) 5-Fold cross validation. Performance evaluation results at different class ratios 1:10: (<bold>C</bold>) Independent testing (<bold>D</bold>) 5-Fold cross validation.</p></caption><alt-text id="at0010">Fig. 2</alt-text><graphic xlink:href="gr2" id="lk0010"/></fig></p>
        <p id="p0205">The analysis at the class ratio of 1:10 provided further intriguing insights. Eigenvector Centrality maintained its robust performance, securing an Accuracy of 0.9482 and an F1-score of 0.9720, marking its consistent influence on the RNN model's effectiveness. Degree Centrality not only exhibited exceptional Sensitivity and Precision but also contributed to an overall Accuracy of 0.9268. The Hub Score also performed consistently, exhibiting an Accuracy of 0.9477, reinforcing the model's overall effectiveness at the 1:10 ratio. On the other hand, the Closeness Centrality again presented unique challenges. It demonstrated a high Sensitivity of 0.9029 during independent testing but a strikingly low MCC of 0 in the 5-fold cross-validation. This anomaly exposes potential weaknesses in the model when operating under this class ratio and with this feature.<xref rid="tbl0010" ref-type="table">Table 2</xref>.<table-wrap position="float" id="tbl0010"><label>Table 2</label><caption><p>RNN model performance for each protein feature at class ratio of 1:1 and 1:10. Performance values of independent testing and 5-fold cross validation are shown within a same cell as follow: independent testing <italic>/</italic> 5-fold cross validation.</p></caption><alt-text id="at0035">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th><bold>Network features</bold></th><th><bold>Sensitivity</bold></th><th><bold>Specificity</bold></th><th><bold>Precision</bold></th><th><bold>Accuracy</bold></th><th><bold>F1-score</bold></th><th><bold>MCC</bold></th></tr><tr><th colspan="7"><bold>Class ratio 1:1</bold></th></tr></thead><tbody><tr><td><bold>Degree_centrality</bold></td><td>0.8088/ 0.8572</td><td>0.8839 / 0.7519</td><td>0.8965 / 0.7761</td><td>0.8423/ 0.8045</td><td>0.8504/ 0.8143</td><td>0.6887/ 0.6130</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.7969/ 0.8640</td><td>0.7799/ 0.7220</td><td>0.7733 / 0.7568</td><td>0.7881/ 0.7930</td><td>0.785 / 0.8067</td><td>0.5766/ 0.5924</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.4242 / 0.4808</td><td>0.4755 / 0.8125</td><td>0.2068 / 0.7189</td><td>0.4630 / 0.6467</td><td>0.2781 / 0.5758</td><td>-0.0860 / 0.3109</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.8571 / 0.8503</td><td>0.8826 / 0.7789</td><td>0.8866 / 0.7938</td><td>0.8694 / 0.8146</td><td>0.8716 / 0.8208</td><td>0.7393 / 0.6313</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.9 / 0.4808</td><td>0.6678 / 0.8125</td><td>0.5320 / 0.7189</td><td>0.7364 / 0.6467</td><td>0.6687 / 0.5758</td><td>0.5182 / 0.3109</td></tr><tr><td><bold>Hub_score</bold></td><td>0.7939 / 0.8505</td><td>0.8959 / 0.7656</td><td>0.9113 / 0.7841</td><td>0.8374 / 0.8081</td><td>0.8486 / 0.8159</td><td>0.6823 / 0.6186</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.8622 / 0.8608</td><td>0.8380 / 0.7610</td><td>0.8325 / 0.7832</td><td>0.8497 / 0.8109</td><td>0.8471 / 0.8200</td><td>0.6999 / 0.6254</td></tr><tr><td colspan="7"><bold>Class ratio 1:10</bold></td></tr><tr><td><bold>Degree_centrality</bold></td><td>0.9369 /<break/>0.9850</td><td>0.7413 /<break/>0.4175</td><td>0.9851 /<break/>0.9442</td><td>0.9268 /<break/>0.9334</td><td>0.9604 /<break/>0.9641</td><td>0.5050 /<break/>0.5268</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.9425 /<break/>0.9950</td><td>0.6302 /<break/>0.8571</td><td>0.9784 / 0.9950</td><td>0.9259 / 0.9308</td><td>0.9601 / 0.9631</td><td>0.4535 / 0.4769</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.9029 /<break/>1</td><td>0.2666 /<break/>0</td><td>0.9945 /<break/>0.9090</td><td>0.8988 /<break/>0.9090</td><td>0.9465 /<break/>0.9523</td><td>0.0464 /<break/>0</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9568 / 0.9900</td><td>0.8106 /<break/>0.5095</td><td>0.9877 / 0.9528</td><td>0.9482 /<break/>0.9464</td><td>0.9720 /<break/>0.9710</td><td>0.6366 /<break/>0.6288</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.9205 /<break/>0.9996</td><td>0.34 /<break/>0.0032</td><td>1 /<break/>0.9093</td><td>0.9205 / 0.9090</td><td>0.0013 /<break/>0.0151</td><td>0.9205 / 0.9523</td></tr><tr><td><bold>Hub_score</bold></td><td>0.9607 /<break/>0.9895</td><td>0.7697 /<break/>0.4945</td><td>0.9828 /<break/>0.9514</td><td>0.9477 /<break/>0.9445</td><td>0.9716 /<break/>0.9701</td><td>0.6457 /<break/>0.6136</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.9429 /<break/>0.9801</td><td>0.675 /<break/>0.3782</td><td>0.9615 /<break/>0.9403</td><td>0.9808 /<break/>0.9253</td><td>0.9286 /<break/>0.9597</td><td>0.4857 /<break/>0.4614</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="sec0090">
        <label>3.3.3</label>
        <title>Performance evaluation of MLP model for protein classification using different network features and class ratios</title>
        <p id="p0210">For the class ratio of 1:1, the model demonstrated varying degrees of effectiveness depending on the protein network feature used (<xref rid="fig0015" ref-type="fig">Fig. 3</xref>). The Eigenvector Centrality feature outshone the others, achieving an impressive accuracy rate of 0.9334 in independent testing and 0.9487 in 5-fold cross-validation. Its sensitivity and specificity scores were also quite high (0.9605 / 0.9578 and 0.9064 / 0.9126 respectively), indicating its ability to correctly identify positive classes and correctly reject the negative ones. Moreover, its high F1-Score (0.9352 / 0.9209) suggests a balanced precision and recall, and the robust MCC (0.8682 / 0.8894) demonstrates the model's quality in terms of binary classification. The Hub Score feature also yielded substantial results, with an accuracy of 0.9261 / 0.8745 and a sensitivity of 0.9310 / 0.9234, reinforcing the reliability of these two features. Conversely, the Closeness Centrality feature performed less favorably, displaying an accuracy of 0.5689 / 0.5246 and sensitivity of 0.9605 / 0.9213, hinting at its weaker ability to accurately predict protein classes.<fig id="fig0015"><label>Fig. 3</label><caption><p>Performance evaluation results of MLP model at class ratios 1:1: (<bold>A</bold>) Independent testing (<bold>B</bold>) 5-Fold cross validation. Performance evaluation results at different class ratios 1:10: (<bold>C</bold>) Independent testing (<bold>D</bold>) 5-Fold cross validation.</p></caption><alt-text id="at0015">Fig. 3</alt-text><graphic xlink:href="gr3" id="lk0015"/></fig></p>
        <p id="p0215">When the class ratio was adjusted to 1:10, the Eigenvector Centrality continued to lead in performance, particularly in sensitivity (0.9794 / 0.9651) and precision (0.9946 / 0.950739). This high sensitivity means the model could correctly identify a high proportion of actual positives, while the precision indicates that out of the classes predicted positive, most were accurate. The Degree Centrality also showed steady performance across multiple metrics like sensitivity (0.8522/0.8468), specificity (0.8472/0.8508), and accuracy (0.8497/0.8488). However, for this class ratio, Pagerank Centrality emerged as the underperforming feature with lower sensitivity (0.6453 / 0.6427) and accuracy scores (0.7266 / 0.7105), implying a lower correct prediction rate and a higher false-negative rate. In essence, these results elucidate the considerable impact of different protein network features on the MLP model's performance, as well as the role of class ratios in the efficacy of these predictions. The superior performance of Eigenvector Centrality feature indicates its potential usefulness in practical applications, although the differing results among other features underscore the importance of careful feature selection in building effective protein classification models.<xref rid="tbl0015" ref-type="table">Table 3</xref>.<table-wrap position="float" id="tbl0015"><label>Table 3</label><caption><p>MLP model performance for each protein feature at class ratio of 1:1 and 1:10. Performance values of independent testing and 5-fold cross validation are shown within a same cell as follow: independent testing <italic>/</italic> 5-fold cross validation.</p></caption><alt-text id="at0040">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th><bold>Network Features</bold></th><th><bold>Accuracy</bold></th><th><bold>Sensitivity</bold></th><th><bold>Specificity</bold></th><th><bold>F1_Score</bold></th><th><bold>Precision</bold></th><th><bold>MCC</bold></th></tr><tr><th colspan="7"><bold>Class ratio 1:1</bold></th></tr></thead><tbody><tr><td><bold>Degree_centrality</bold></td><td>0.8694 / 0.8448</td><td>0.8620 / 0.7881</td><td>0.8768 / 0.9014</td><td>0.8684 / 0.8355</td><td>0.875 / 0.8889</td><td>0.7389 / 0.6941</td></tr><tr><td><bold>Betweenness_centrality</bold></td><td>0.7881 / 0.8325</td><td>0.8423 / 0.8275</td><td>0.7339 / 0.8374</td><td>0.7990 / 0.8316</td><td>0.76 / 0.8358</td><td>0.5797 / 0.6650</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9334/ 0.9487</td><td>0.9605 / 0.9578</td><td>0.9064 / 0.9126</td><td>0.9352 / 0.9209</td><td>0.9112 / 0.9334</td><td>0.8682 / 0.8894</td></tr><tr><td><bold>Hub_score</bold></td><td>0.9261 / 0.8745</td><td>0.9310 / 0.9234</td><td>0.9211 / 0.9382</td><td>0.9264 / 0.8978</td><td>0.9219 / 0.9051</td><td>0.8522 / 0.8089</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.7044 / 0.6427</td><td>0.9359 / 0.7782</td><td>0.4729 / 0.7441</td><td>0.76 / 0.7105</td><td>0.6397 / 0.6894</td><td>0.4613 / 0.4253</td></tr><tr><td><bold>Pagerank-centrality</bold></td><td>0.8399 / 0.8570</td><td>0.8522 / 0.8721</td><td>0.8275 / 0.8091</td><td>0.84184 / 0.8101</td><td>0.8317 / 0.8566</td><td>0.6800 / 0.6421</td></tr><tr><td><bold>Closeness-centrality</bold></td><td>0.5689 / 0.5246</td><td>0.9605 / 0.9213</td><td>0.1773 / 0.1345</td><td>0.6902 / 0.6189</td><td>0.5386 / 0.4621</td><td>0.2218 / 0.2</td></tr><tr><td colspan="7"><bold>Class ratio 1:10</bold></td></tr><tr><td><bold>Degree_centrality</bold></td><td>0.8522/<break/>0.8468</td><td>0.8472/<break/>0.8508</td><td>0.8480 / 0.8502</td><td>0.8497/ 0.8488</td><td>0.8501/ 0.8483</td><td>0.8195/ 0.7980</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.8592 / 0.8832</td><td>0.8226 / 0.7909</td><td>0.8277 / 0.8093</td><td>0.8374 / 0.8370</td><td>0.8398 / 0.8443</td><td>0.8851 / 0.8345</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.8522 /<break/>0.8660</td><td>0.8029 / 0.7944</td><td>0.8122 / 0.8325</td><td>0.8275 /<break/>0.8015</td><td>0.8317 /<break/>0.80</td><td>0.7559 /<break/>0.711</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9794 /<break/>0.9651</td><td>0.8195 / 0.8411</td><td>0.9946 / 0.950739</td><td>0.8736 / 0.905</td><td>0.9352 / 0.8642</td><td>0.8647 / 0.8020</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.9113 / 0.9018</td><td>0.4926 / 0.5122</td><td>0.6423 /0.6494</td><td>0.7019 / 0.7070</td><td>0.7535 / 0.7549</td><td>0.4448 / 0.4498</td></tr><tr><td><bold>Hub_score</bold></td><td>0.8472 /<break/>0.8351</td><td>0.8916 /<break/>0.8734</td><td>0.8865 / 0.8698</td><td>0.8694 / 0.8542</td><td>0.8664 / 0.8516</td><td>0.7396 / 0.7099</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.6453 /<break/>0.6427</td><td>0.8078 /<break/>0.7782</td><td>0.7705 / 0.7441</td><td>0.7266 / 0.7105</td><td>0.7024 / 0.6894</td><td>0.4593 / 0.4253</td></tr></tbody></table></table-wrap></p>
      </sec>
    </sec>
    <sec id="sec0095">
      <label>3.4</label>
      <title>Evaluation of models</title>
      <p id="p0220">The evaluation aimed to assess the performance of the MLP model across several network-based features. The features tested included Degree_centrality, Betweenness_centrality, Closeness_centrality, Eigenvector_centrality, Pagerank_centrality, Hub_score, and Eccentricity. These features are commonly used in network analysis to measure the importance of individual nodes within a network. In the context of protein-protein interaction networks, these features assists in the identification of key proteins that play critical roles in biological processes.</p>
      <p id="p0225">On independent testing, the Eigenvector_centrality feature stood out for its performance, achieving the highest accuracy (0.8736) and Matthews Correlation Coefficient (MCC, 0.8647) among all the features tested. The Eigenvector Centrality of a protein in a network measures the extent to which it is connected to other highly connected proteins. This feature is often used in biological network analysis to identify proteins that, although they may not have many connections themselves, are connected to key proteins in the network <xref rid="bib39" ref-type="bibr">[39]</xref>. In this context, the application of Eigenvector Centrality features has garnered attention, particularly in the study of host-pathogen protein interactions. Studies such as Khorsand et al. <xref rid="bib40" ref-type="bibr">[40]</xref> and Cui et al. <xref rid="bib41" ref-type="bibr">[41]</xref> have demonstrated the efficacy of this approach in unraveling complex interaction dynamics, suggesting new pathways for therapeutic intervention. The model's performance remained high during cross-validation, achieving an accuracy of 0.905 and an MCC of 0.8020. The high cross-validation performance indicates that the model generalizes well to unseen data and is not overfitted to the training data, which adds confidence to the robustness of our model.</p>
      <p id="p0230">In order to apply these findings in a predictive manner, each protein pair was given a score based on their Eigenvector Centrality features. The pair's features were then normalized and input into the trained MLP model. The overall architecture of MLP model was also presented in <xref rid="fig0020" ref-type="fig">Fig. 4</xref>. This produced a probability of interaction for each protein pair, which can be interpreted as a likelihood score for each pair's interaction. A threshold of 0.9 was set for these probability values to focus on the most significant interactions (<bold>Supplementary File 1:</bold>
<xref rid="sec0175" ref-type="sec">Table S1</xref>). Protein pairs with a score above this threshold were predicted to interact with high probability. This approach is common in binary classification tasks where it is more important to accurately predict one class (interacting pairs, in this case). It allows for better control over the trade-off between precision and recall, ensuring that predictions made are reliable and biologically significant.<fig id="fig0020"><label>Fig. 4</label><caption><p>MLP architecture used to train the host-pathogen PPIs models.</p></caption><alt-text id="at0020">Fig. 4</alt-text><graphic xlink:href="gr4" id="lk0020"/></fig></p>
      <p id="p0235">Of significant importance, our study goes beyond the realm of predicting the likelihood of positive interactions. Instead, our model's implementation serves as an expansive exploration, unveiling hidden interactions that might not have been initially evident. This dynamic capability underscores the inherent value of our approach, which extends beyond the validation of established interactions to the prospect of uncovering novel, previously unexplored interactions. These newfound interactions, accompanied by calculated likelihoods, not only enrich our comprehension but also stimulate a more profound exploration of potential molecular dialogues within the intricate domain of host-pathogen interactions. In conclusion, through rigorous evaluation and thoughtful application, the MLP model trained on Eigenvector Centrality features proved to be a powerful tool for predicting host-pathogen interactions. This methodological approach could be useful for future research in this field, particularly for studies seeking to understand complex host-pathogen dynamics.</p>
    </sec>
    <sec id="sec0100">
      <label>3.5</label>
      <title>Developing an interactive R shiny application</title>
      <p id="p0240">Building upon the foundations of our approach, we have successfully translated our methodology into an interactive and user-friendly tool: R Shiny application named Deep-HPI-pred. This innovative platform empowers users to seamlessly engage with our predictive models and gain insights into host-pathogen interactions. By providing an intuitive interface and leveraging advanced deep learning models, Deep-HPI-pred offers an accessible means for researchers to explore, predict, and understand the intricate dynamics of PPIs between hosts and pathogens. This application acts as a bridge, converting intricate computational methodologies into a user-friendly tool that catalyzes the advancement of host-pathogen interaction studies. This application is distinctive in the sense that it allows users to either manually or automatically upload the training dataset, facilitating a more streamlined operation (<xref rid="fig0025" ref-type="fig">Fig. 5</xref>). Furthermore, it is engineered to predict PPIs leveraging the topological features of the proteins, which is a significant step forward given that traditional prediction methods often lack this level of sophistication.<fig id="fig0025"><label>Fig. 5</label><caption><p>(<bold>A</bold>) User interface of Deep-HPI-pred (<bold>B</bold>) Deep-HPI-pred Interface showcasing data upload functionality (<bold>C</bold>) Result’s tab of Deep-HPI-pred. The blue color nodes within network indicate the Host-proteins while red color nodes represented the pathogen proteins (<bold>D</bold>) The probability of interactions among host and proteins are presented in form of table (<bold>E</bold>) The 'GO Analysis' Tab in Deep-HPI-pred demonstrating the visualization of a network graph and (<bold>F)</bold> the corresponding Gene Ontology (GO) Enrichment Analysis table.</p></caption><alt-text id="at0025">Fig. 5</alt-text><graphic xlink:href="gr5" id="lk0025"/></fig></p>
      <p id="p0245">What makes Deep-HPI-pred stand out is its ability to present intricate host-pathogen protein interactions and the corresponding probability scores in a simplified, user-friendly interface. This aspect bridges the gap between complex data analysis and its interpretability, thereby promoting a broader comprehension and accessibility of the information. Notably, Deep-HPI-pred is the first-ever application of its kind dedicated to the network-based classification and prediction of host-pathogen protein interactions. This development marks a significant milestone in bioinformatics and opens new opportunities for researchers to delve deeper into the dynamics of biological interactions. The application of deep learning models combined with topological analysis presents a novel approach to study and predict PPIs. This not only broadens the scope of current research methodologies but also paves the way for future advancements in this rapidly evolving field of study. By fostering a deeper understanding of pathogenesis and disease progression, Deep-HPI-pred contributes to the advancement of the scientific community's collective knowledge. It demonstrates the potential of integrating machine learning with biological data analysis, thereby setting a precedent for future research in this direction.</p>
      <sec id="sec0105">
        <label>3.5.1</label>
        <title>Interface and data input in the deep-HPI-pred application</title>
        <p id="p0250">Upon initiation of the Deep-HPI-pred application, the user is presented with a straightforward interface, deliberately designed for easy navigation and usability (<xref rid="fig0025" ref-type="fig">Fig. 5</xref>
<bold>(A)</bold>). A feature of the application allows users to upload their specific data files in CSV format through a conveniently located sidebar. In contrast, there is also an option to use pre-existing demo data. This flexibility caters to the distinct requirements of individual research, providing a tailored experience for interaction predictions. Moreover, the incorporation of demo data serves as an effective tool for users to familiarize themselves with the application's functionality swiftly.</p>
      </sec>
      <sec id="sec0110">
        <label>3.5.2</label>
        <title>Implementing training and prediction with deep learning models</title>
        <p id="p0255">After completion of the data upload process, the application prompts the "Train and Predict" feature (<xref rid="fig0025" ref-type="fig">Fig. 5</xref><bold>(B, C)</bold>). This launches the deep learning model previously trained on an extensive dataset of host-pathogen protein interactions. The model performs a comprehensive analysis of the uploaded data to accurately predict potential interactions. This application feature transforms the data input into valuable insights, opening avenues for the elucidation of complex host-pathogen interaction dynamics.</p>
      </sec>
      <sec id="sec0115">
        <label>3.5.3</label>
        <title>Visualizing and interpreting predicted interactions</title>
        <p id="p0260">The next phase of interaction with the application is through the "Results" tab, which is designed for data interpretation and visualization (<xref rid="fig0025" ref-type="fig">Fig. 5</xref><bold>(D)</bold>). The tab displays predicted interaction data and a network visualization of these interactions. The visual depiction facilitates better comprehension of the interaction networks and is particularly valuable in understanding complex interaction dynamics.</p>
      </sec>
      <sec id="sec0120">
        <label>3.5.4</label>
        <title>Conducting GO enrichment analysis for enhanced biological comprehension</title>
        <p id="p0265">The final component of the Deep-HPI-pred application, the "GO Analysis" tab, provides an effective tool for conducting GO enrichment analysis (<xref rid="fig0025" ref-type="fig">Fig. 5</xref><bold>(E, F)</bold>). Unlike other tools, which often lack the provision of GO terms and related analyses, Deep-HPI-pred fills this critical gap. The resulting Network Graph visualizes the outcomes of the GO analysis, offering an intuitive representation of the enriched terms. By collating and categorizing gene products based on shared GO annotations, the application presents an understanding of the biological implications of the predicted interactions. Moreover, to facilitate further in-depth investigations, researchers can conveniently download both the GO table and the corresponding plot, enhancing the tool's utility for advanced research endeavors.</p>
        <p id="p0270">Thus, the development and implementation of the Deep-HPI-pred R Shiny application constitutes a significant milestone in host-pathogen interaction studies. Its ability to integrate deep learning models, employ diverse protein interaction data, and provide interactive and comprehensible output in the form of network visualizations and GO enrichment analysis, creates a uniquely accessible platform for researchers across multiple disciplines. The application, by transforming raw data into insightful knowledge, effectively supports the intricate process of deciphering the complexities inherent in host-pathogen interactions.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec0125">
    <label>4</label>
    <title>Benchmarking</title>
    <p id="p0275">In this study, the versatility and effectiveness of the Deep-HPI-pred applet were demonstrated in accurately predicting PPIs across a range of biological contexts. PPIs were analyzed across three distinct host-pathogen pairs, each representing a unique interplay between different types of organisms - bacteria, plants, and animals. The dataset of experimentally verified HPI interactions was curated from existing literature, and then classified into distinct categories: plant-bacteria, plant-fungi, animal-bacteria <xref rid="bib14" ref-type="bibr">[14]</xref>, and human-virus.</p>
    <sec id="sec0130">
      <label>4.1</label>
      <title><italic>Mus musculus</italic> and <italic>Burkholderia mallei</italic></title>
      <p id="p0280"><italic>Burkholderia mallei</italic> is known to cause glanders, a rare infectious disease that affects horses, mules, and donkeys. It can also infect humans and is considered a potential bioterrorism agent. In this study, our Deep-HPI-pred model achieved an accuracy of 96.6%, sensitivity of 94.9%, specificity of 87.3%, F1 score of 88.7%, precision of 81.4%, MCC of 90.9%, and AUC of 95.1% in predicting PPIs between <italic>M. musculus</italic> (mouse) and <italic>B. mallei</italic>. This highlights the model's potential in assisting researchers in understanding the intricate interplay between these two organisms and could provide valuable insights into the mechanism of glanders infection in various species.</p>
    </sec>
    <sec id="sec0135">
      <label>4.2</label>
      <title><italic>Arabidopsis thaliana</italic> and <italic>Golovinomyces orontii</italic></title>
      <p id="p0285"><italic>Golovinomyces orontii</italic> is known to cause powdery mildew in <italic>A. thaliana</italic>, a model organism in plant biology. Accurate prediction of PPIs is crucial for deciphering the molecular basis of plant-pathogen interactions and developing strategies for disease resistance. Our Deep-HPI-pred model demonstrated high accuracy in predicting these interactions, with an accuracy of 98.4%, sensitivity of 89.3%, specificity of 93.2%, F1 score of 95%, precision of 92.4%, MCC of 93.3%, and AUC of 92.8%.</p>
    </sec>
    <sec id="sec0140">
      <label>4.3</label>
      <title><italic>Arabidopsis thaliana</italic> and <italic>Pseudomonas syringae</italic></title>
      <p id="p0290"><italic>P. syringae</italic> is a well-known pathogenic bacterium that infects a wide range of plant species, causing bacterial speck disease in <italic>A. thaliana</italic>. Understanding PPIs between these two species is vital for developing new methods of disease control. Our Deep-HPI-pred model achieved an accuracy of 97.9%, sensitivity of 95%, specificity of 93.7%, F1 score of 91.8%, precision of 92.4%, MCC of 94.8%, and AUC of 98.2% in predicting the PPIs between <italic>A. thaliana</italic> and <italic>P. syringae</italic>.</p>
    </sec>
    <sec id="sec0145">
      <label>4.4</label>
      <title><italic>Homo sapiens</italic> and SARS-Cov-2</title>
      <p id="p0295">Additionally, we also evaluated our model on the host-pathogen interactions between <italic>H. sapiens</italic> (human) and SARS-CoV-2 (virus), a critically important relationship in light of the recent global pandemic. Given the recent global impact of the COVID-19 pandemic, understanding the interactions between human proteins and SARS-CoV-2 viral proteins is of paramount importance. In our analysis of the interactions between <italic>H. sapiens</italic> and SARS-CoV-2, the Deep-HPI-pred model demonstrated its proficiency in capturing the complex relationships characteristic of host-pathogen interactions. Given the recent global impact of the COVID-19 pandemic, understanding the interactions between human proteins and SARS-CoV-2 viral proteins is of paramount importance. Our Deep-HPI-pred model showed excellent performance in predicting the interactions between human proteins and SARS-CoV-2 viral proteins, with an accuracy of 94.3%, sensitivity of 97.5%, specificity of 98.7%, F1 score of 96.5%, precision of 96.2%, MCC of 95.3%, and AUC of 97.8%. These results demonstrate the strong predictive power of our Deep-HPI-pred model in deciphering the intricate network of interactions that take place between human proteins and SARS-CoV-2 viral proteins.</p>
      <p id="p0300">The finding revealed potential interactions between SARS-CoV-2 proteins and human host factors, which may play a pivotal role in the viral life cycle, including entry, replication, and evasion of host immune defenses. Notably, the model predicts that the SARS-CoV-2 Envelope protein interact with human proteins involved in the endocytic pathway, a route well-documented for coronavirus entry into host cells <xref rid="bib42" ref-type="bibr">[42]</xref>. Such interactions could potentially facilitate viral entry by altering endosomal trafficking. Additionally, interactions have been predicted between the SARS-CoV-2 Membrane glycoprotein and proteins located in the ER-to-Golgi intermediate compartment (ERGIC), which is instrumental in viral assembly and trafficking <xref rid="bib43" ref-type="bibr">[43]</xref>. These predictions suggest a mechanism by which SARS-CoV-2 could hijack host cellular machinery to bolster its replication process. Further results also suggest potential interactions between the ORF1a polyprotein of SARS-CoV-2 and components of the human innate immune system, particularly proteins involved in interferon signaling. This aligns with findings such as those from McClainet al. <xref rid="bib44" ref-type="bibr">[44]</xref>, which indicate the virus's ability to modulate interferon-driven responses, a strategy that likely facilitates viral replication and contributes to pathogenesis. An extensive literature review was undertaken to provide context for each predicted interaction. While direct evidence of these specific interactions has not been previously documented, the literature corroborates the biological plausibility of the proposed mechanisms. For example, Hekman et al. <xref rid="bib45" ref-type="bibr">[45]</xref> demonstrated that the SARS-CoV-2 Nucleoprotein's interaction with host RNA processing bodies suggests a potential for viral modulation of host mRNA processing, an avenue supported by recent findings. Thus, our Deep-HPI-pred model has demonstrated a high level of accuracy and reliability in predicting host-pathogen interactions across diverse biological contexts, underscoring its potential as a valuable tool for researchers studying infectious diseases and host-pathogen interactions.</p>
      <p id="p0305">Additionally, in the comparative analysis of Deep-HPI-pred with existing servers such as Pred-HPI and DeepHPI, several key differences emerge that highlight the enhanced capabilities of Deep-HPI-pred. Specifically, Deep-HPI-pred has predicted a total of 9673 interactions between SARS-CoV-2 virus and human proteins (<bold>Supplementary File 3:</bold><xref rid="sec0175" ref-type="sec">Table S1</xref><bold>)</bold>. In contrast, PredHPI, focuses primarily on sequence-based methods for detecting host-pathogen interactions, has predicted 6654 interactions (<bold>Supplementary File 3:</bold>
<xref rid="sec0175" ref-type="sec">Table S2</xref><bold>)</bold>, and notably, these predictions do not include probability information for each interaction. The higher number of interactions predicted by Deep-HPI-pred suggests its increased sensitivity in detecting potential host-pathogen interactions, which is crucial for comprehensive understanding and exploration of SARS-CoV-2 pathogenesis. Moreover, Deep-HPI-pred provides probability scores for each predicted interaction, offering a quantifiable measure of confidence in the predictions. This feature is particularly valuable for researchers, as it allows prioritization of interactions based on their likelihood, facilitating targeted experimental validation. Such probability information is absent in the predictions made by Pred-HPI and DeepHPI, which limits the ability to assess the confidence level in each predicted interaction. Additionally, Deep-HPI-pred enhances the utility of its predictions by providing GO information. This inclusion allows for immediate biological interpretation of the interactions, offering insights into potential biological processes and molecular functions involved, thereby enriching the understanding of the interaction landscape. In comparison, Deep-HPI, another existing tool in this domain, does not provide specific interaction results in a format that is directly comparable to Deep-HPI-pred and Pred-HPI. Therefore, the comparison focuses on the methodological approach and the theoretical framework of these models.</p>
      <p id="p0310">In the similar vein, GreeningDB, another host-pathogen interaction database, offers a more specialized scope, focusing specifically on citrus greening disease (Huanglongbing or HLB). It compiles data primarily relevant to HLB, including genomic, transcriptomic, and proteomic information. While invaluable for HLB research, GreeningDB's utility is confined to this particular disease. Deep-HPI-pred, however, extends its applicability beyond a singular disease context, enabling broader investigations of HPIs across multiple biological systems. This key distinction underscores Deep-HPI-pred's potential as a versatile platform for a more generalized understanding of host-pathogen dynamics, applicable to a diverse range of infectious diseases.</p>
      <p id="p0315">Another recent study by Yang et al. <xref rid="bib46" ref-type="bibr">[46]</xref> employed a transfer learning approach using multi-scale convolutional neural layers to predict human-virus PPIs. While this approach effectively captures the complex features of protein sequences, Deep-HPI-pred extends beyond sequence-based predictions. It integrates topological features and GO information, offering a more holistic view of the interaction landscape. Additionally, Deep-HPI-pred's application is not limited to human-virus interactions but encompasses a broader range of host-pathogen systems, demonstrating its versatile and comprehensive predictive capabilities. Similarly, in comparison with DeepViral <xref rid="bib47" ref-type="bibr">[47]</xref>, a deep learning-based method for predicting novel virus-host interactions from protein sequences and infectious disease phenotypes, Deep-HPI-pred showcases distinct advantages. DeepViral focuses on novel virus-host interaction predictions using protein sequences and disease phenotypes, leveraging a deep learning approach. While DeepViral's integration of infectious disease phenotypes offers a unique perspective, Deep-HPI-pred's methodology is distinguished by its utilization of MLP models based on a comprehensive evaluation of topological features and neural network architectures. This not only enhances the prediction accuracy but also provides a more detailed understanding of the underlying protein interaction mechanisms. Furthermore, Deep-HPI-pred's consistent performance across diverse host-pathogen systems, including plant-pathogens, human-viruses, and animal-bacteria, as evidenced by its high accuracy rates, illustrates its robustness and adaptability in various biological contexts.</p>
      <p id="p0320">To sum up, Deep-HPI-pred stands out in its ability to not only predict a larger number of host-pathogen interactions but also to provide critical additional information like probability scores and GO annotations. These features significantly contribute to its utility as a research tool, offering a more nuanced and informed approach to exploring host-pathogen interactions compared to tools like Pred-HPI, Deep-HPI, and DeepViral. The advanced algorithms utilized by Deep-HPI-pred enable the analysis of extensive interaction datasets, highlighting proteins that are central and often critical in these processes. This analytical capability is instrumental in deepening our understanding of the molecular mechanisms of viral infections, as supported by studies like those conducted by Barman et al. <xref rid="bib48" ref-type="bibr">[48]</xref>, which employed state-of-art techniques to identify key viral interaction proteins. Furthermore, the fusion of biological network analysis with deep learning heralds a transformative era in clinical and personalized medicine, particularly in managing viral diseases. This integration enables the unraveling of complex host-pathogen interactions at a molecular level, paving the way for targeted therapeutic strategies and more individualized treatment approaches in combating viral infections.</p>
    </sec>
  </sec>
  <sec id="sec0150">
    <label>5</label>
    <title>Conclusion</title>
    <p id="p0325">Traditional experimental techniques for host-pathogen interaction prediction, though effective, have proven to be labor-intensive, expensive, and time-consuming. To address this challenge, we have introduced Deep-HPI-pred, an R/Shiny application that provides a computational approach for predicting hitherto unmapped interactions between host and pathogen proteins. By harnessing the power of network-driven feature learning, Deep-HPI-pred, as demonstrated through our case study using citrus and <italic>C</italic>Las bacteria training sets, offers a promising alternative for accelerating the discovery of PPIs. In our research, we employed a comprehensive evaluation of various neural network architectures and topological features, the results of which led us to adopt the MLP models for HPI prediction. Notably, the MLP model using the Eigenvector Centrality topological feature exhibited exemplary performance, achieving an overall MCC value exceeding 0.80 when tested on independent validation datasets. Beyond its capacity for interaction prediction, Deep-HPI-pred further enriches our understanding of the dynamics within host-pathogen interactions by providing GO term information for each protein. This added layer of information presents an insightful view of the system and enhances our comprehension of the overall biological processes at play. Furthermore, in the benchmarking studies conducted, the Deep-HPI-pred model demonstrated its robustness and reliability across various host-pathogen systems. The model exhibited a remarkable performance in predicting interactions between different host-pathogen pairs, including plant-virus, human-virus, plant-bacteria, and animal-bacteria. Specifically, the model achieved an accuracy of 98.4% and 97.9% for plant-pathogen interactions, 94.3% for human-virus interactions, and 96.6% for animal-bacteria interactions. These results not only validate the efficacy of our model but also highlight its potential as a versatile and comprehensive tool for understanding the complex dynamics of host-pathogen interactions across different biological systems. While MLPs have demonstrated robust performance in this study, their structure is not inherently optimized for contrastive learning, which is increasingly recognized for its efficacy in unsupervised and semi-supervised scenarios. This limitation suggests a potential area for future improvement of the model, where integrating contrastive learning techniques could expand its capabilities to handle and learn from the vast amounts of unlabeled data in biological research. Such enhancements would not only address a key limitation but also significantly enrich the model’s utility in understanding and predicting complex host-pathogen interactions. In conclusion, the introduction of Deep-HPI-pred represents a significant stride in the field of bioinformatics. By integrating detection and visualization of interaction networks into a single user-friendly platform, it equips researchers with a powerful tool for understanding both model and non-model host-pathogen systems. This advancement is expected to aid in the generation of hypotheses, the design of appropriate experiments, and ultimately, in the development of disease control and prevention strategies.</p>
  </sec>
  <sec sec-type="data-availability" id="sec0155">
    <title>Availability of data and materials</title>
    <p id="p0330">Project name: Deep-HPI-pred. Project home page: <ext-link ext-link-type="uri" xlink:href="https://cbi.gxu.edu.cn/shiny-apps/Deep-HPI-pred/" id="ir0015">https://cbi.gxu.edu.cn/shiny-apps/Deep-HPI-pred/</ext-link>. Programming language: R and HTML. The resources and data used during the current study are available in the GitHub repository, <ext-link ext-link-type="uri" xlink:href="https://github.com/tahirulqamar/Deep-HPI-pred" id="ir0020">https://github.com/tahirulqamar/Deep-HPI-pred</ext-link>.</p>
  </sec>
  <sec id="sec0160">
    <title>Funding</title>
    <p id="p0335">This work was supported by the Starting Research Grant for <funding-source id="gs1">High-level Talents from Guangxi University</funding-source> and <funding-source id="gs2">Postdoctoral research platform grant of Guangxi University</funding-source>.</p>
  </sec>
  <sec id="sec0165">
    <title>CRediT authorship contribution statement</title>
    <p id="p0340"><bold>Muhammad Tahir ul Qamar and Fatima Noor</bold>: Data Curation, Methodology, Software, Formal Analysis, Investigation, Visualization, Writing - Original Draft. <bold>Yi-Xiong Guo and Xi-Tong Zhu</bold>: Software, Validation, Writing - Review &amp; Editing. <bold>Ling-Ling Chen</bold>: Conceptualization, Resources, Supervision, Project administration, Funding acquisition, Writing - Review &amp; Editing.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="p0345">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="bibliog0005">
    <title>References</title>
    <ref id="bib1">
      <label>1</label>
      <element-citation publication-type="journal" id="sbref1">
        <person-group person-group-type="author">
          <name>
            <surname>Kuo</surname>
            <given-names>Z.-Y.</given-names>
          </name>
          <name>
            <surname>Chuang</surname>
            <given-names>Y.-J.</given-names>
          </name>
          <name>
            <surname>Chao</surname>
            <given-names>C.-C.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>F.-C.</given-names>
          </name>
          <name>
            <surname>Lan</surname>
            <given-names>C.-Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>B.-S.</given-names>
          </name>
        </person-group>
        <article-title>Identification of infection-and defense-related genes via a dynamic host-pathogen interaction network using a Candida albicans-zebrafish infection model</article-title>
        <source>J innate Immun</source>
        <volume>5</volume>
        <issue>2</issue>
        <year>2013</year>
        <fpage>137</fpage>
        <lpage>152</lpage>
        <pub-id pub-id-type="pmid">23406717</pub-id>
      </element-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="journal" id="sbref2">
        <person-group person-group-type="author">
          <name>
            <surname>Garbutt</surname>
            <given-names>C.C.</given-names>
          </name>
          <name>
            <surname>Bangalore</surname>
            <given-names>P.V.</given-names>
          </name>
          <name>
            <surname>Kannar</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Mukhtar</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Getting to the edge: protein dynamical networks as a new frontier in plant–microbe interactions</article-title>
        <source>Front Plant Sci</source>
        <volume>5</volume>
        <year>2014</year>
        <fpage>312</fpage>
        <pub-id pub-id-type="pmid">25071795</pub-id>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <mixed-citation publication-type="other" id="othref0005">J.J. Da Graça, L.Korsten. Citrus huanglongbing: Review, present status and future strategies. In: Naqvi SAMH, editor. Diseases of Fruits and Vegetables Volume I: Diagnosis and Management. The Netherlands: Kluwer Academic; (2004). pp. 229–45.</mixed-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="journal" id="sbref3">
        <person-group person-group-type="author">
          <name>
            <surname>Andrade</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>Candidatus Liberibacter asiaticus: virulence traits and control strategies</article-title>
        <source>Trop Plant Pathol</source>
        <volume>45</volume>
        <year>2020</year>
        <fpage>285</fpage>
        <lpage>297</lpage>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="journal" id="sbref4">
        <person-group person-group-type="author">
          <name>
            <surname>Pandey</surname>
            <given-names>S.S.</given-names>
          </name>
          <name>
            <surname>Hendrich</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Andrade</surname>
            <given-names>M.O.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>Candidatus Liberibacter: From movement, host responses, to symptom development of citrus Huanglongbing</article-title>
        <source>Phytopathology®</source>
        <volume>112</volume>
        <issue>1</issue>
        <year>2022</year>
        <fpage>55</fpage>
        <lpage>68</lpage>
        <pub-id pub-id-type="pmid">34609203</pub-id>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="book" id="sbref5">
        <person-group person-group-type="author">
          <name>
            <surname>Hoddle</surname>
            <given-names>M.S.</given-names>
          </name>
          <name>
            <surname>Hoddle</surname>
            <given-names>C.D.</given-names>
          </name>
          <name>
            <surname>Morgan</surname>
            <given-names>D.J.</given-names>
          </name>
          <name>
            <surname>Milosavljević</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <part-title>Successful Biological Control of Asian Citrus Psyllid, Diaphorina citri, in California</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Van Driesche</surname>
            <given-names>R.G.</given-names>
          </name>
          <name>
            <surname>Winston</surname>
            <given-names>R.L.</given-names>
          </name>
          <name>
            <surname>Perring</surname>
            <given-names>T.M.</given-names>
          </name>
          <name>
            <surname>Lopez</surname>
            <given-names>V.M.</given-names>
          </name>
        </person-group>
        <source>Contributions of Classical Biological Control to the US Food Security</source>
        <year>2022</year>
        <publisher-name>Forestry, and Biodiversity. Washington</publisher-name>
        <publisher-loc>USDA FHAAST</publisher-loc>
        <fpage>127</fpage>
        <lpage>145</lpage>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="journal" id="sbref6">
        <person-group person-group-type="author">
          <name>
            <surname>Yuan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Xin</surname>
            <given-names>X.F.</given-names>
          </name>
        </person-group>
        <article-title>Bacterial Infection and Hypersensitive Response Assays in Arabidopsis-Pseudomonas syringae Pathosystem</article-title>
        <source>Bio Protoc</source>
        <volume>11</volume>
        <issue>24</issue>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">e4268</object-id>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="journal" id="sbref7">
        <person-group person-group-type="author">
          <name>
            <surname>Dyer</surname>
            <given-names>M.D.</given-names>
          </name>
          <name>
            <surname>Neff</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Dufford</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Rivera</surname>
            <given-names>C.G.</given-names>
          </name>
          <name>
            <surname>Shattuck</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Bassaganya-Riera</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The human-bacterial pathogen protein interaction networks of Bacillus anthracis, Francisella tularensis, and Yersinia pestis</article-title>
        <source>PloS One</source>
        <volume>5</volume>
        <issue>8</issue>
        <year>2010</year>
        <object-id pub-id-type="publisher-id">e12089</object-id>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="journal" id="sbref8">
        <person-group person-group-type="author">
          <name>
            <surname>Lian</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Current status and future perspectives of computational studies on human–virus protein–protein interactions</article-title>
        <source>Brief Bioinforma</source>
        <volume>22</volume>
        <issue>5</issue>
        <year>2021</year>
        <fpage>bbab029</fpage>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="journal" id="sbref9">
        <person-group person-group-type="author">
          <name>
            <surname>Westermann</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Cross-species</surname>
            <given-names>J.Vogel</given-names>
          </name>
        </person-group>
        <article-title>RNA-seq for deciphering host–microbe interactions</article-title>
        <source>Nat Rev Genet</source>
        <volume>22</volume>
        <issue>6</issue>
        <year>2021</year>
        <fpage>361</fpage>
        <lpage>378</lpage>
        <pub-id pub-id-type="pmid">33597744</pub-id>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="journal" id="sbref10">
        <person-group person-group-type="author">
          <name>
            <surname>Balotf</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Tegg</surname>
            <given-names>R.S.</given-names>
          </name>
          <name>
            <surname>Nichols</surname>
            <given-names>D.S.</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>C.R.</given-names>
          </name>
        </person-group>
        <article-title>Shotgun proteomics as a powerful tool for the study of the proteomes of plants, their pathogens, and plant–pathogen interactions</article-title>
        <source>Proteomes</source>
        <volume>10</volume>
        <issue>1</issue>
        <year>2022</year>
        <fpage>5</fpage>
        <pub-id pub-id-type="pmid">35225985</pub-id>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="journal" id="sbref11">
        <person-group person-group-type="author">
          <name>
            <surname>Jain</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mittal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Tripathi</surname>
            <given-names>L.P.</given-names>
          </name>
          <name>
            <surname>Nussinov</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Ahmad</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Host-pathogen protein-nucleic acid interactions: A comprehensive review</article-title>
        <source>Comput Struct Biotechnol J</source>
        <year>2022</year>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="journal" id="sbref12">
        <person-group person-group-type="author">
          <name>
            <surname>Kaundal</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Loaiza</surname>
            <given-names>C.D.</given-names>
          </name>
          <name>
            <surname>Duhan</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Flann</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>deepHPI: a comprehensive deep learning platform for accurate prediction and visualization of host–pathogen protein–protein interactions</article-title>
        <source>Brief Bioinforma</source>
        <volume>23</volume>
        <issue>3</issue>
        <year>2022</year>
        <fpage>bbac125</fpage>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>14</label>
      <element-citation publication-type="journal" id="sbref13">
        <person-group person-group-type="author">
          <name>
            <surname>Loaiza</surname>
            <given-names>C.D.</given-names>
          </name>
          <name>
            <surname>Kaundal</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>PredHPI: an integrated web server platform for the detection and visualization of host–pathogen interactions using sequence-based methods</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <issue>5</issue>
        <year>2021</year>
        <fpage>622</fpage>
        <lpage>624</lpage>
        <pub-id pub-id-type="pmid">33027504</pub-id>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>15</label>
      <element-citation publication-type="journal" id="sbref14">
        <person-group person-group-type="author">
          <name>
            <surname>Loaiza</surname>
            <given-names>C.D.</given-names>
          </name>
          <name>
            <surname>Duhan</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Kaundal</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>GreeningDB: A Database of Host–Pathogen Protein–Protein Interactions and Annotation Features of the Bacteria Causing Huanglongbing HLB Disease</article-title>
        <source>Int J Mol Sci</source>
        <volume>22</volume>
        <issue>19</issue>
        <year>2021</year>
        <fpage>10897</fpage>
        <pub-id pub-id-type="pmid">34639237</pub-id>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>16</label>
      <element-citation publication-type="journal" id="sbref15">
        <person-group person-group-type="author">
          <name>
            <surname>Singhal</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Resat</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>A domain-based approach to predict protein-protein interactions</article-title>
        <source>BMC Bioinform</source>
        <volume>8</volume>
        <year>2007</year>
        <fpage>1</fpage>
        <lpage>19</lpage>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>17</label>
      <element-citation publication-type="journal" id="sbref16">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Chi</surname>
            <given-names>C.-H.</given-names>
          </name>
          <name>
            <surname>Kurgan</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Systematic evaluation of machine learning methods for identifying human–pathogen protein–protein interactions</article-title>
        <source>Brief Bioinforma</source>
        <volume>22</volume>
        <issue>3</issue>
        <year>2021</year>
        <fpage>bbaa068</fpage>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>18</label>
      <element-citation publication-type="journal" id="sbref17">
        <person-group person-group-type="author">
          <name>
            <surname>Scott</surname>
            <given-names>M.S.</given-names>
          </name>
          <name>
            <surname>Probabilistic</surname>
            <given-names>G.J. Barton</given-names>
          </name>
        </person-group>
        <article-title>prediction and ranking of human protein-protein interactions</article-title>
        <source>BMC Bioinform</source>
        <volume>8</volume>
        <issue>1</issue>
        <year>2007</year>
        <fpage>1</fpage>
        <lpage>21</lpage>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>19</label>
      <element-citation publication-type="journal" id="sbref18">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>Y.D.</given-names>
          </name>
        </person-group>
        <article-title>Protein‐protein interaction networks as miners of biological discovery</article-title>
        <source>Proteomics</source>
        <volume>22</volume>
        <issue>15-16</issue>
        <year>2022</year>
        <fpage>2100190</fpage>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>20</label>
      <element-citation publication-type="journal" id="sbref19">
        <person-group person-group-type="author">
          <name>
            <surname>Csardi</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Nepusz</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>The igraph software package for complex network research</article-title>
        <source>Inter, Complex Syst</source>
        <volume>1695</volume>
        <issue>5</issue>
        <year>2006</year>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>21</label>
      <element-citation publication-type="journal" id="sbref20">
        <person-group person-group-type="author">
          <name>
            <surname>Pržulj</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Wigle</surname>
            <given-names>D.A.</given-names>
          </name>
          <name>
            <surname>Jurisica</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <article-title>Functional topology in a network of protein interactions</article-title>
        <source>Bioinformatics</source>
        <volume>20</volume>
        <issue>3</issue>
        <year>2004</year>
        <fpage>340</fpage>
        <lpage>348</lpage>
        <pub-id pub-id-type="pmid">14960460</pub-id>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>22</label>
      <element-citation publication-type="journal" id="sbref21">
        <person-group person-group-type="author">
          <name>
            <surname>Ashtiani</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Salehzadeh-Yazdi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Razaghi-Moghadam</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Hennig</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Wolkenhauer</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Mirzaie</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Jafari</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>A systematic survey of centrality measures for protein-protein interaction networks. BMC Sys</article-title>
        <source>Biol</source>
        <volume>12</volume>
        <issue>1</issue>
        <year>2018</year>
        <fpage>1</fpage>
        <lpage>17</lpage>
      </element-citation>
    </ref>
    <ref id="bib23">
      <label>23</label>
      <element-citation publication-type="journal" id="sbref22">
        <person-group person-group-type="author">
          <name>
            <surname>Eryilmaz</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Pax</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>O’Neill</surname>
            <given-names>A.G.</given-names>
          </name>
          <name>
            <surname>Vangel</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Diez</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Holt</surname>
            <given-names>D.J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Network hub centrality and working memory performance in schizophrenia</article-title>
        <source>Schizophrenia</source>
        <volume>8</volume>
        <issue>1</issue>
        <year>2022</year>
        <object-id pub-id-type="publisher-id">76</object-id>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>24</label>
      <mixed-citation publication-type="other" id="othref0010">A. Ali, V.R. Hulipalled, S. Patil, editors. Centrality measure analysis on protein interaction networks. 2020 IEEE International Conference on Technology, Engineering, Management for Societal impact using Marketing, Entrepreneurship and Talent (TEMSMET); 2020: IEEE.</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25</label>
      <mixed-citation publication-type="other" id="othref0015">V. Chaubey, M.S. Nair, G.N. Pillai, editors. Gene expression prediction using a deep 1D convolution neural network. 2019 IEEE Symposium Series on Computational Intelligence (SSCI); 2019: IEEE.</mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26</label>
      <element-citation publication-type="journal" id="sbref23">
        <person-group person-group-type="author">
          <name>
            <surname>Patiyal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Dhall</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Raghava</surname>
            <given-names>G.P.</given-names>
          </name>
        </person-group>
        <article-title>A deep learning-based method for the prediction of DNA interacting residues in a protein</article-title>
        <source>Brief Bioinforma</source>
        <volume>23</volume>
        <issue>5</issue>
        <year>2022</year>
        <fpage>bbac322</fpage>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>27</label>
      <element-citation publication-type="journal" id="sbref24">
        <person-group person-group-type="author">
          <name>
            <surname>Fang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Moriwaki</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Shimizu</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of antifungal peptides by deep learning with character embedding</article-title>
        <source>IPSJ Trans Bioinforma</source>
        <volume>12</volume>
        <year>2019</year>
        <fpage>21</fpage>
        <lpage>29</lpage>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>28</label>
      <element-citation publication-type="journal" id="sbref25">
        <person-group person-group-type="author">
          <name>
            <surname>Arnold</surname>
            <given-names>T.B.</given-names>
          </name>
        </person-group>
        <article-title>kerasR: R Interface to the Keras Deep Learning Library</article-title>
        <source>J Open Source Softw,2</source>
        <issue>14</issue>
        <year>2017</year>
        <fpage>296</fpage>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>29</label>
      <element-citation publication-type="journal" id="sbref26">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lian</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wuchty</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Transfer learning via multi-scale convolutional neural layers for human–virus protein–protein interaction prediction</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <issue>24</issue>
        <year>2021</year>
        <fpage>4771</fpage>
        <lpage>4778</lpage>
        <pub-id pub-id-type="pmid">34273146</pub-id>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>30</label>
      <element-citation publication-type="journal" id="sbref27">
        <person-group person-group-type="author">
          <name>
            <surname>Grandini</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bagli</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Visani</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Metrics for multi-class classification: an overview</article-title>
        <comment>200805756</comment>
        <source>arXiv Prepr arXiv</source>
        <year>2020</year>
        <comment>200805756</comment>
      </element-citation>
    </ref>
    <ref id="bib31">
      <label>31</label>
      <element-citation publication-type="journal" id="sbref28">
        <person-group person-group-type="author">
          <name>
            <surname>Powers</surname>
            <given-names>D.M.</given-names>
          </name>
        </person-group>
        <article-title>Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation</article-title>
        <comment>201016061</comment>
        <source>arXiv Prepr arXiv</source>
        <year>2020</year>
        <comment>201016061</comment>
      </element-citation>
    </ref>
    <ref id="bib32">
      <label>32</label>
      <element-citation publication-type="journal" id="sbref29">
        <person-group person-group-type="author">
          <name>
            <surname>Baldi</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Brunak</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Chauvin</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Andersen</surname>
            <given-names>C.A.</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Assessing the accuracy of prediction algorithms for classification: an overview</article-title>
        <source>Bioinformatics</source>
        <volume>16</volume>
        <issue>5</issue>
        <year>2000</year>
        <fpage>412</fpage>
        <lpage>424</lpage>
        <pub-id pub-id-type="pmid">10871264</pub-id>
      </element-citation>
    </ref>
    <ref id="bib33">
      <label>33</label>
      <element-citation publication-type="journal" id="sbref30">
        <person-group person-group-type="author">
          <name>
            <surname>Johnson</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Khoshgoftaar</surname>
            <given-names>T.M.</given-names>
          </name>
        </person-group>
        <article-title>Survey on deep learning with class imbalance</article-title>
        <source>J Big Data</source>
        <volume>6</volume>
        <issue>1</issue>
        <year>2019</year>
        <fpage>1</fpage>
        <lpage>54</lpage>
      </element-citation>
    </ref>
    <ref id="bib34">
      <label>34</label>
      <element-citation publication-type="journal" id="sbref31">
        <person-group person-group-type="author">
          <name>
            <surname>Broadley</surname>
            <given-names>R.W.</given-names>
          </name>
          <name>
            <surname>Klenk</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Thies</surname>
            <given-names>S.B.</given-names>
          </name>
          <name>
            <surname>Kenney</surname>
            <given-names>L.P.</given-names>
          </name>
          <name>
            <surname>Granat</surname>
            <given-names>M.H.</given-names>
          </name>
        </person-group>
        <article-title>Methods for the real-world evaluation of fall detection technology: A scoping review</article-title>
        <source>Sensors</source>
        <volume>18</volume>
        <issue>7</issue>
        <year>2018</year>
        <fpage>2060</fpage>
        <pub-id pub-id-type="pmid">29954155</pub-id>
      </element-citation>
    </ref>
    <ref id="bib35">
      <label>35</label>
      <element-citation publication-type="journal" id="sbref32">
        <person-group person-group-type="author">
          <name>
            <surname>Alves</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Castro</surname>
            <given-names>G.Z.</given-names>
          </name>
          <name>
            <surname>Oliveira</surname>
            <given-names>B.A.S.</given-names>
          </name>
          <name>
            <surname>Ferreira</surname>
            <given-names>L.A.</given-names>
          </name>
          <name>
            <surname>Ramírez</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Silva</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Guimarães</surname>
            <given-names>F.G.</given-names>
          </name>
        </person-group>
        <article-title>Explaining machine learning based diagnosis of COVID-19 from routine blood tests with decision trees and criteria graphs</article-title>
        <source>Comput Biol Med</source>
        <volume>132</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">104335</object-id>
      </element-citation>
    </ref>
    <ref id="bib36">
      <label>36</label>
      <element-citation publication-type="journal" id="sbref33">
        <person-group person-group-type="author">
          <name>
            <surname>Chicco</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Jurman</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation</article-title>
        <source>BMC Genom</source>
        <volume>21</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>13</lpage>
      </element-citation>
    </ref>
    <ref id="bib37">
      <label>37</label>
      <element-citation publication-type="journal" id="sbref34">
        <person-group person-group-type="author">
          <name>
            <surname>Kakkar</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>goyal</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Johri</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Artificial Intelligence-Based Approaches for Detection and Classification of Different Classes of Malaria Parasites Using Microscopic Images: A Systematic Review</article-title>
        <source>Arch Comput Methods Eng</source>
        <year>2023</year>
        <fpage>1</fpage>
        <lpage>20</lpage>
      </element-citation>
    </ref>
    <ref id="bib38">
      <label>38</label>
      <element-citation publication-type="journal" id="sbref35">
        <person-group person-group-type="author">
          <name>
            <surname>Lei</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wan</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A deep-learning framework for multi-level peptide–protein interaction prediction</article-title>
        <source>Nat Commun</source>
        <volume>12</volume>
        <issue>1</issue>
        <year>2021</year>
        <fpage>5465</fpage>
        <pub-id pub-id-type="pmid">34526500</pub-id>
      </element-citation>
    </ref>
    <ref id="bib39">
      <label>39</label>
      <element-citation publication-type="journal" id="sbref36">
        <person-group person-group-type="author">
          <name>
            <surname>Negre</surname>
            <given-names>C.F.</given-names>
          </name>
          <name>
            <surname>Morzan</surname>
            <given-names>U.N.</given-names>
          </name>
          <name>
            <surname>Hendrickson</surname>
            <given-names>H.P.</given-names>
          </name>
          <name>
            <surname>Pal</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lisi</surname>
            <given-names>G.P.</given-names>
          </name>
          <name>
            <surname>Loria</surname>
            <given-names>J.P.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Eigenvector centrality for characterization of protein allosteric pathways</article-title>
        <source>Proc Natl Acad Sci USA</source>
        <volume>115</volume>
        <issue>52</issue>
        <year>2018</year>
        <comment>E12201-E8</comment>
      </element-citation>
    </ref>
    <ref id="bib40">
      <label>40</label>
      <element-citation publication-type="journal" id="sbref37">
        <person-group person-group-type="author">
          <name>
            <surname>Khorsand</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Savadi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Naghibzadeh</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Comprehensive host-pathogen protein-protein interaction network analysis</article-title>
        <source>BMC Bioinform</source>
        <volume>21</volume>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>22</lpage>
      </element-citation>
    </ref>
    <ref id="bib41">
      <label>41</label>
      <element-citation publication-type="journal" id="sbref38">
        <person-group person-group-type="author">
          <name>
            <surname>Cui</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Stanley</surname>
            <given-names>H.E.</given-names>
          </name>
        </person-group>
        <article-title>Discovering disease-associated genes in weighted protein–protein interaction networks</article-title>
        <source>Phys A: Stat Mech its Appl</source>
        <volume>496</volume>
        <year>2018</year>
        <fpage>53</fpage>
        <lpage>61</lpage>
      </element-citation>
    </ref>
    <ref id="bib42">
      <label>42</label>
      <element-citation publication-type="journal" id="sbref39">
        <person-group person-group-type="author">
          <name>
            <surname>Hoffmann</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kleine-Weber</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Schroeder</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Krüger</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Herrler</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Erichsen</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SARS-CoV-2 cell entry depends on ACE2 and TMPRSS2 and is blocked by a clinically proven protease inhibitor</article-title>
        <source>cell</source>
        <volume>181</volume>
        <issue>2</issue>
        <year>2020</year>
        <fpage>271</fpage>
        <lpage>280</lpage>
        <comment>e8</comment>
        <pub-id pub-id-type="pmid">32142651</pub-id>
      </element-citation>
    </ref>
    <ref id="bib43">
      <label>43</label>
      <element-citation publication-type="journal" id="sbref40">
        <person-group person-group-type="author">
          <name>
            <surname>Cortese</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Laketa</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>Advanced microscopy technologies enable rapid response to SARS‐CoV‐2 pandemic</article-title>
        <source>Cell Microbiol</source>
        <volume>23</volume>
        <issue>7</issue>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">e13319</object-id>
      </element-citation>
    </ref>
    <ref id="bib44">
      <label>44</label>
      <element-citation publication-type="journal" id="sbref41">
        <person-group person-group-type="author">
          <name>
            <surname>McClain</surname>
            <given-names>M.T.</given-names>
          </name>
          <name>
            <surname>Constantine</surname>
            <given-names>F.J.</given-names>
          </name>
          <name>
            <surname>Henao</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Tsalik</surname>
            <given-names>E.L.</given-names>
          </name>
          <name>
            <surname>Burke</surname>
            <given-names>T.W.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Dysregulated transcriptional responses to SARS-CoV-2 in the periphery</article-title>
        <source>Nat Commun</source>
        <volume>12</volume>
        <issue>1</issue>
        <year>2021</year>
        <fpage>1079</fpage>
        <pub-id pub-id-type="pmid">33597532</pub-id>
      </element-citation>
    </ref>
    <ref id="bib45">
      <label>45</label>
      <element-citation publication-type="journal" id="sbref42">
        <person-group person-group-type="author">
          <name>
            <surname>Hekman</surname>
            <given-names>R.M.</given-names>
          </name>
          <name>
            <surname>Hume</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Goel</surname>
            <given-names>R.K.</given-names>
          </name>
          <name>
            <surname>Abo</surname>
            <given-names>K.M.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Blum</surname>
            <given-names>B.C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Actionable cytopathogenic host responses of human alveolar type 2 cells to SARS-CoV-2</article-title>
        <source>Mol Cell</source>
        <volume>80</volume>
        <issue>6</issue>
        <year>2020</year>
        <fpage>1104</fpage>
        <lpage>1122</lpage>
        <comment>e9</comment>
        <pub-id pub-id-type="pmid">33259812</pub-id>
      </element-citation>
    </ref>
    <ref id="bib46">
      <label>46</label>
      <element-citation publication-type="journal" id="sbref43">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lian</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wuchty</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Transfer learning via multi-scale convolutional neural layers for human-virus protein-protein interaction prediction</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <issue>24</issue>
        <year>2021</year>
        <fpage>4771</fpage>
        <lpage>4778</lpage>
        <pub-id pub-id-type="pmid">34273146</pub-id>
      </element-citation>
    </ref>
    <ref id="bib47">
      <label>47</label>
      <element-citation publication-type="journal" id="sbref44">
        <person-group person-group-type="author">
          <name>
            <surname>Liu-Wei</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Kafkas</surname>
            <given-names>Ş.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Dimonaco</surname>
            <given-names>N.J.</given-names>
          </name>
          <name>
            <surname>Tegnér</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hoehndorf</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>DeepViral: prediction of novel virus-host interactions from protein sequences and infectious disease phenotypes</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <issue>17</issue>
        <year>2021</year>
        <fpage>2722</fpage>
        <lpage>2729</lpage>
        <pub-id pub-id-type="pmid">33682875</pub-id>
      </element-citation>
    </ref>
    <ref id="bib48">
      <label>48</label>
      <element-citation publication-type="journal" id="sbref45">
        <person-group person-group-type="author">
          <name>
            <surname>Barman</surname>
            <given-names>R.K.</given-names>
          </name>
          <name>
            <surname>Saha</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of interactions between viral and host proteins using supervised machine learning methods</article-title>
        <source>PLoS One</source>
        <volume>9</volume>
        <issue>11</issue>
        <year>2014</year>
        <object-id pub-id-type="publisher-id">e112034</object-id>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="sec0175" sec-type="supplementary-material">
    <label>Appendix A</label>
    <title>Supplementary material</title>
    <p id="p0355"><supplementary-material content-type="local-data" id="ec0005"><caption><p>Supplementary material</p></caption><media xlink:href="mmc1.xlsx"/></supplementary-material>.</p>
    <p id="p0360"><supplementary-material content-type="local-data" id="ec0010"><caption><p>Supplementary material</p></caption><media xlink:href="mmc2.xlsx"/></supplementary-material>.</p>
    <p id="p0365"><supplementary-material content-type="local-data" id="ec0015"><caption><p>Supplementary material</p></caption><media xlink:href="mmc3.xlsx"/></supplementary-material>.</p>
  </sec>
  <fn-group>
    <fn id="sec0170" fn-type="supplementary-material">
      <label>Appendix A</label>
      <p id="p0350">Supplementary data associated with this article can be found in the online version at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.csbj.2023.12.010" id="ir0025">doi:10.1016/j.csbj.2023.12.010</ext-link>.</p>
    </fn>
  </fn-group>
</back>
<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_CSBJ2447 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEga1 jpg ?>
<?FILEmmc1 xlsx ?>
<?FILEmmc2 xlsx ?>
<?FILEmmc3 xlsx ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Comput Struct Biotechnol J</journal-id>
    <journal-id journal-id-type="iso-abbrev">Comput Struct Biotechnol J</journal-id>
    <journal-title-group>
      <journal-title>Computational and Structural Biotechnology Journal</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2001-0370</issn>
    <publisher>
      <publisher-name>Research Network of Computational and Structural Biotechnology</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10772389</article-id>
    <article-id pub-id-type="pii">S2001-0370(23)00484-1</article-id>
    <article-id pub-id-type="doi">10.1016/j.csbj.2023.12.010</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software/Web Server Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deep-HPI-pred: An R-Shiny applet for network-based classification and prediction of Host-Pathogen protein-protein interactions</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au0005">
        <name>
          <surname>Tahir ul Qamar</surname>
          <given-names>Muhammad</given-names>
        </name>
        <email>m.tahirulqamar@hotmail.com</email>
        <xref rid="aff0005" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
        <xref rid="fn1" ref-type="fn">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au0010">
        <name>
          <surname>Noor</surname>
          <given-names>Fatima</given-names>
        </name>
        <xref rid="aff0010" ref-type="aff">b</xref>
        <xref rid="fn1" ref-type="fn">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au0015">
        <name>
          <surname>Guo</surname>
          <given-names>Yi-Xiong</given-names>
        </name>
        <xref rid="aff0015" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author" id="au0020">
        <name>
          <surname>Zhu</surname>
          <given-names>Xi-Tong</given-names>
        </name>
        <xref rid="aff0005" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au0025">
        <name>
          <surname>Chen</surname>
          <given-names>Ling-Ling</given-names>
        </name>
        <email>llchen@gxu.edu.cn</email>
        <xref rid="aff0005" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <aff id="aff0005"><label>a</label>State Key Laboratory for Conservation and Utilization of Subtropical Agro-bioresources, College of Life Science and Technology, Guangxi University, Nanning 530004, China</aff>
      <aff id="aff0010"><label>b</label>Integrative Omics and Molecular Modeling Laboratory, Department of Bioinformatics and Biotechnology, Government College University Faisalabad (GCUF), Faisalabad 38000, Pakistan</aff>
      <aff id="aff0015"><label>c</label>National Key Laboratory of Crop Genetic Improvement, College of Informatics, Huazhong Agricultural University, Wuhan 430070, China</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding authors. <email>m.tahirulqamar@hotmail.com</email><email>llchen@gxu.edu.cn</email></corresp>
      <fn id="fn1">
        <label>1</label>
        <p id="ntp0005">These authors contributed equally to this study.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>15</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <volume>23</volume>
    <fpage>316</fpage>
    <lpage>329</lpage>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>10</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>11</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>12</day>
        <month>12</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 The Authors. Published by Elsevier B.V. on behalf of Research Network of Computational and Structural Biotechnology.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder/>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="ab0010">
      <p>Host-pathogen interactions (HPIs) are vital in numerous biological activities and are intrinsically linked to the onset and progression of infectious diseases. HPIs are pivotal in the entire lifecycle of diseases: from the onset of pathogen introduction, navigating through the mechanisms that bypass host cellular defenses, to its subsequent proliferation inside the host. At the heart of these stages lies the synergy of proteins from both the host and the pathogen. By understanding these interlinking protein dynamics, we can gain crucial insights into how diseases progress and pave the way for stronger plant defenses and the swift formulation of countermeasures. In the framework of current study, we developed a web-based R/Shiny app, Deep-HPI-pred, that uses network-driven feature learning method to predict the yet unmapped interactions between pathogen and host proteins. Leveraging citrus and <italic>C</italic>Las bacteria training datasets as case study, we spotlight the effectiveness of Deep-HPI-pred in discerning Protein-protein interaction (PPIs) between them. Deep-HPI-pred use Multilayer Perceptron (MLP) models for HPI prediction, which is based on a comprehensive evaluation of topological features and neural network architectures. When subjected to independent validation datasets, the predicted models consistently surpassed a Matthews correlation coefficient (MCC) of 0.80 in host-pathogen interactions. Remarkably, the use of Eigenvector Centrality as the leading topological feature further enhanced this performance. Further, Deep-HPI-pred also offers relevant gene ontology (GO) term information for each pathogen and host protein within the system. This protein annotation data contributes an additional layer to our understanding of the intricate dynamics within host-pathogen interactions. In the additional benchmarking studies, the Deep-HPI-pred model has proven its robustness by consistently delivering reliable results across different host-pathogen systems, including plant-pathogens (accuracy of 98.4% and 97.9%), human-virus (accuracy of 94.3%), and animal-bacteria (accuracy of 96.6%) interactomes. These results not only demonstrate the model's versatility but also pave the way for gaining comprehensive insights into the molecular underpinnings of complex host-pathogen interactions. Taken together, the Deep-HPI-pred applet offers a unified web service for both identifying and illustrating interaction networks. Deep-HPI-pred applet is freely accessible at its homepage: <ext-link ext-link-type="uri" xlink:href="https://cbi.gxu.edu.cn/shiny-apps/Deep-HPI-pred/" id="ir0005">https://cbi.gxu.edu.cn/shiny-apps/Deep-HPI-pred/</ext-link> and at github: <ext-link ext-link-type="uri" xlink:href="https://github.com/tahirulqamar/Deep-HPI-pred" id="ir0010">https://github.com/tahirulqamar/Deep-HPI-pred</ext-link>.</p>
    </abstract>
    <abstract abstract-type="graphical" id="ab0015">
      <title>Graphical abstract</title>
      <p>
        <fig id="fig0030" position="anchor">
          <alt-text id="at0045">ga1</alt-text>
          <graphic xlink:href="ga1" id="lk0030"/>
        </fig>
      </p>
    </abstract>
    <kwd-group id="keys0005">
      <title>Keywords</title>
      <kwd>Host–pathogen interactions</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Multilayer perceptron</kwd>
      <kwd>Neural networks</kwd>
      <kwd>Topological features</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec0005">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0005">Host-pathogen interactions are crucial determinants in the dynamics of infectious diseases <xref rid="bib1" ref-type="bibr">[1]</xref>. These interactions are primarily facilitated by protein-protein interactions (PPIs), which orchestrate every phase of disease progression, from the initial pathogen invasion to its eventual establishment within the host <xref rid="bib2" ref-type="bibr">[2]</xref>. Citrus Huanglongbing (HLB), also known as citrus greening, represents a paradigmatic case of devastating plant diseases on a global scale <xref rid="bib3" ref-type="bibr">[3]</xref>. Predominantly caused by the bacterium <italic>Candidatus</italic> Liberibacter asiaticus (<italic>C</italic>Las), along with <italic>Ca.</italic> L. americanus and <italic>Ca</italic>. L. africanus, HLB has emerged as a primary threat to citrus production, particularly in regions like Florida, USA <xref rid="bib4" ref-type="bibr">[4]</xref>. CLas, the most prevalent among these pathogens, is vectored by the Asian citrus <italic>psyllid Diaphorina</italic> citri, leading to its colonization in the phloem tissues of citrus plants <xref rid="bib5" ref-type="bibr">[5]</xref>. This invasion results in severe phytopathological symptoms and extensive agricultural damage. The economic ramifications of HLB are staggering, with the disease inflicting multibillion-dollar losses globally, debilitating the citrus industry's financial stability <xref rid="bib6" ref-type="bibr">[6]</xref>. The pervasiveness of <italic>C</italic>Las and its profound impact on citrus crops underscore the necessity of an in-depth exploration of the PPIs between the host plants and the pathogen. Recently Yuan et al. <xref rid="bib7" ref-type="bibr">[7]</xref> mapped a network of <italic>Arabidopsis thaliana</italic> interactions with the bacterial pathogen <italic>Pseudomonas syringae</italic>, identifying new components involved in the plant immune response and paving the way for future plant disease control strategies. In a similar vein, Dyer et al. <xref rid="bib8" ref-type="bibr">[8]</xref> conducted a large-scale study on PPIs between human cells and <italic>Bacillus anthracis</italic>, the bacterium responsible for anthrax, revealing numerous potential therapeutic intervention targets. These studies underscore the vital role of host-pathogen PPIs in understanding the complex interplay between the host's defenses and the pathogen's invasion strategies. Therefore, these interactions form the cornerstone of research aiming to unravel the intricacies of infectious disease dynamics and serve as targets for potential therapeutic interventions.</p>
    <p id="p0010">Recent advancements in experimental methodologies, particularly in high-throughput screening and molecular imaging techniques, have significantly deepened our comprehension of host-pathogen PPIs <xref rid="bib9" ref-type="bibr">[9]</xref>. These innovations have enabled more precise identification and analysis of PPIs, shedding light on the intricate mechanisms of infection and host response, and paving the way for novel therapeutic interventions. Despite this progress, such techniques often come with high costs in terms of resources, time, and labor. Given the multitude of potential protein-interacting partners, the reliance on these methods becomes even more challenging, underlining the necessity for more cost-effective and efficient alternatives <xref rid="bib10" ref-type="bibr">[10]</xref>, <xref rid="bib11" ref-type="bibr">[11]</xref>. In contrast, computational methods for predicting host-pathogen interactions present themselves as a fitting solution to this pressing demand <xref rid="bib12" ref-type="bibr">[12]</xref>. These methods not only facilitate the detection of interactions but also have the potential to mine incomplete interaction maps for new discoveries, exponentially increasing the knowledge base regarding host-pathogen dynamics <xref rid="bib12" ref-type="bibr">[12]</xref>. Recently, Kaundal et al. <xref rid="bib13" ref-type="bibr">[13]</xref> developed deepHPI, an all-encompassing deep learning platform for the accurate prediction and visualization of HPPIs. Leveraging convolutional neural networks (CNN), their platform achieved a prediction accuracy of 96% on a curated dataset, significantly surpassing the performance of other methods on the same dataset. Complementing this work, Loaiza et al. introduced Pred-HPI <xref rid="bib14" ref-type="bibr">[14]</xref>, an integrated web server platform that employs sequence-based methods for the detection and visualization of host-pathogen interactions. PredHPI offered a sequence similarity-based approach and reached an accuracy of approximately 90% on a large-scale human-virus PPI dataset.</p>
    <p id="p0015">Despite the progress, these approaches primarily employed feature extraction strategies that represent host-pathogen protein pairs as fixed-length feature vectors, extracted from protein sequences. These sequence-based methods, while valuable, have limitations in terms of achieving high prediction accuracy, as they do not fully exploit the wealth of available structural and functional information. Recognizing these limitations, our study introduces a novel approach that leverages network-based integration methods for predicting host-pathogen interactions. Preliminary results have shown that this approach outperforms traditional sequence-based methods in terms of prediction accuracy scores.</p>
    <p id="p0020">To address these gaps, we introduced Deep-HPI-pred, an innovative R-shiny application that leverages advanced deep learning models. This application, for the first time, provides researchers with the autonomy to manually or automatically upload their training datasets for host-pathogen protein interaction prediction. The distinguishing characteristic of Deep-HPI-pred lies in its pioneering use of topological features for PPI prediction, as opposed to the conventional sequence-based features. This novel approach significantly enhances the sophistication and accuracy of predictions, marking a considerable advance in the bioinformatics field. Further, in order to substantiate the robustness and reliability of the Deep-HPI-pred model, our framework encompasses a rigorous validation protocol encompassing diverse biological systems, including plant-pathogen, human-virus, and animal-bacteria interactomes. This empirical evaluation across distinct host-pathogen pairs fortifies the scientific validity of the model's predictive prowess. In summary, as the premier R shiny application dedicated to predicting host-pathogen interactions, Deep-HPI-pred not only presents a groundbreaking tool for researchers, but also catalyzes a paradigm shift in our approach to understanding and predicting the intricate dynamics of infectious diseases.</p>
  </sec>
  <sec id="sec0010">
    <label>2</label>
    <title>Materials and methods</title>
    <sec id="sec0015">
      <label>2.1</label>
      <title>Gathering of true protein-protein interactions (PPIs) and data pre-processing</title>
      <p id="p0025">In this study, our initial host-pathogen PPI data was sourced from established host-pathogen interaction databases, including Pred-HPI <xref rid="bib14" ref-type="bibr">[14]</xref> and GreeningDB <xref rid="bib15" ref-type="bibr">[15]</xref>. These databases provided a broad spectrum of interactions, serving as a foundational reference for our research. However, to ensure a more robust and comprehensive dataset, we did not solely rely on these pre-compiled interactions. Recognizing the importance of data authenticity, we supplemented our database-derived PPIs with experimentally validated interactions. These additional PPIs were meticulously extracted from a wide range of peer-reviewed scientific literature. This step was vital for expanding the dataset and reinforcing its validity with real-world, experimentally confirmed interactions, thus substantially increasing the count of true positive PPIs in our study. The compiled data, encompassing both database-sourced and manually extracted PPIs, then underwent rigorous pre-processing. This crucial phase was aimed at maintaining the highest level of data integrity. We meticulously cleaned the combined dataset, carefully removing any duplicate entries to ensure a unique and non-redundant set of PPIs. This thorough cleaning process was instrumental in ensuring the reliability and accuracy of our dataset, thereby enhancing the overall quality of our analysis. By integrating these two diverse sources of data – established databases and manually verified literature – we aimed to strike a balance between the breadth and depth of our PPI dataset. This approach not only provided a comprehensive view of the host-pathogen interactions but also added a layer of validation to the interactions derived from databases, thereby bolstering the credibility and applicability of our study's findings.</p>
    </sec>
    <sec id="sec0020">
      <label>2.2</label>
      <title>Generation of negative protein-protein interactions (PPIs)</title>
      <p id="p0030">To augment our existing true positive PPIs dataset, the generation of negative PPIs was conducted. Negative PPIs, representing protein pairs that do not interact, serve as an important counterpart to positive interactions in our analysis <xref rid="bib16" ref-type="bibr">[16]</xref>. The generation of negative PPIs was a two-step process. The first step involved creating a random pairing between the protein sets of the selected pathogen and host. Given that there are <inline-formula><mml:math id="M1" altimg="si0001.svg"><mml:mi>m</mml:mi></mml:math></inline-formula> pathogen proteins and <inline-formula><mml:math id="M2" altimg="si0002.svg"><mml:mi>n</mml:mi></mml:math></inline-formula> host proteins, the total possible pairings, assuming no interaction between any two proteins, are <inline-formula><mml:math id="M3" altimg="si0003.svg"><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="M4" altimg="si0004.svg"><mml:mrow><mml:mi>P</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> be the set of pathogen proteins and <inline-formula><mml:math id="M5" altimg="si0005.svg"><mml:mrow><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> be the set of host proteins. The total possible random pairings <inline-formula><mml:math id="M6" altimg="si0006.svg"><mml:mi>R</mml:mi></mml:math></inline-formula> between pathogen proteins and host proteins is represented by the Cartesian product of the two sets:<disp-formula id="eqn0005"><label>(3.1)</label><mml:math id="M7" altimg="si0007.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>R</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>P</mml:mi><mml:mi mathvariant="normal">and</mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">H</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="p0035">In the second step, a subset was randomly selected from these random pairings to construct the negative dataset. This subset selection ensured that the number of negative PPIs matched the number of true positive PPIs in our dataset. This balancing act serves as a cornerstone for the impending deep learning model training, underpinning an unbiased and balanced approach to the development of a robust predictive model. In current study, the generation of negative PPIs was integral to creating a balanced dataset for effective host-pathogen PPI prediction. This approach aligns with methodologies adopted in similar studies, ensuring robustness and validity in our predictive model. Specifically, Chen et al. <xref rid="bib17" ref-type="bibr">[17]</xref> employed a comparable strategy in their research, where negative PPIs were systematically generated through random pairing, thereby maintaining an equivalent size of negative and positive datasets. This method of balancing is crucial, as underscored by Scott et al. <xref rid="bib18" ref-type="bibr">[18]</xref>, who highlighted the potential risks associated with imbalanced datasets in PPI predictions, such as overfitting and poor model generalization. Our methodology addresses these concerns by matching the number of negative PPIs with positive PPIs, thereby fostering a more accurate and unbiased predictive model.</p>
    </sec>
    <sec id="sec0025">
      <label>2.3</label>
      <title>Assembly of dataset</title>
      <p id="p0040">The assembly of the final dataset was carried out by integrating both positive <inline-formula><mml:math id="M8" altimg="si0008.svg"><mml:mi mathvariant="italic">P_set</mml:mi></mml:math></inline-formula> and negative <inline-formula><mml:math id="M9" altimg="si0009.svg"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">N_set</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> PPIs. The negative PPI dataset, representing non-interactions, was carefully merged with the positive PPI dataset. This integrated approach not only maintains the balance between interaction and non-interaction data but also encapsulates the full range of possible protein interactions. Mathematically, the final dataset <inline-formula><mml:math id="M10" altimg="si0010.svg"><mml:mi mathvariant="italic">F_set</mml:mi></mml:math></inline-formula> is represented as the union of the positive PPI dataset <inline-formula><mml:math id="M11" altimg="si0011.svg"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">set</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and the negative PPI dataset <inline-formula><mml:math id="M12" altimg="si0012.svg"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">N_set</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>:<disp-formula id="eqn0010"><label>(3.2)</label><mml:math id="M13" altimg="si0013.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>F</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="italic">set</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>P</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="italic">set</mml:mi><mml:mo>∪</mml:mo><mml:mi>N</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="italic">set</mml:mi></mml:mrow></mml:math></disp-formula></p>
      <p id="p0045">This final dataset, <inline-formula><mml:math id="M14" altimg="si0014.svg"><mml:mi mathvariant="italic">F_set</mml:mi></mml:math></inline-formula>, encapsulates a broad spectrum of potential PPIs. It is primed for rigorous training within the deep learning model, offering a well-rounded, realistic, and balanced resource for the analysis. This strategic assembly of positive and negative interactions underpins the analytical framework, fostering a robust and comprehensive model tailored for effective host-pathogen interaction prediction.</p>
    </sec>
    <sec id="sec0030">
      <label>2.4</label>
      <title>Feature extraction: quantification of network topology parameters</title>
      <p id="p0050">The creation of the final PPI dataset marked the transition to a critical phase - feature extraction, which is the quantification of the topological parameters of the constructed network <xref rid="bib19" ref-type="bibr">[19]</xref>. The intent behind this step is to decipher and numerically articulate the intricate structural patterns within the protein network. Our primary objective was to leverage network topology for the accurate prediction of interactions between host and viral proteins. To achieve this, current study strategically employed a range of centrality measures, namely Degree Centrality, Betweenness Centrality, Closeness Centrality, Eigenvector Centrality, PageRank Centrality, Hub Score, and Eccentricity. This was achieved with the assistance of the topological algorithms incorporated within the Igraph package of R <xref rid="bib20" ref-type="bibr">[20]</xref>. These features were not selected on the basis of their individual merits alone but were chosen for their collective ability to provide a comprehensive view of the network dynamics, crucial for understanding the intricacies of host-pathogen PPI networks. This methodological approach is supported by the work of Przulj et al. <xref rid="bib21" ref-type="bibr">[21]</xref> and Ashtiani et al. <xref rid="bib22" ref-type="bibr">[22]</xref>, who have demonstrated the importance of network topology in understanding PPI networks and the utility of centrality measures in identifying key proteins within these networks. These topological features assist in identifying the potential interaction points and key nodes within the host-pathogen network.</p>
      <p id="p0055">These features, ranging from quantifying the number of direct connections a protein node has (Degree Centrality) to calculating the maximum distance from a node to all other nodes (Eccentricity), offer a comprehensive view of each protein's role and importance within the overall network structure. Degree Centrality and Hub Score, for instance, shed light on the interaction richness and the core component status of a protein respectively <xref rid="bib22" ref-type="bibr">[22]</xref>, <xref rid="bib23" ref-type="bibr">[23]</xref>. For a graph <inline-formula><mml:math id="M15" altimg="si0015.svg"><mml:mi>G</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="M16" altimg="si0016.svg"><mml:mi>N</mml:mi></mml:math></inline-formula> vertices, the Degree Centrality (DC) of a vertex <inline-formula><mml:math id="M17" altimg="si0017.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> is given by <inline-formula><mml:math id="M18" altimg="si0018.svg"><mml:mrow><mml:mi mathvariant="italic">DC</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Degree</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M19" altimg="si0019.svg"><mml:mrow><mml:mi mathvariant="italic">Degree</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represents the number of edges incident on vertex <inline-formula><mml:math id="M20" altimg="si0020.svg"><mml:mi>v</mml:mi></mml:math></inline-formula>. Similarly, Betweenness Centrality, Closeness Centrality, and PageRank Centrality provide insights into a protein's role as a network connector, its centrality, and its importance based on neighbouring proteins. Betweenness Centrality is defined as <inline-formula><mml:math id="M21" altimg="si0021.svg"><mml:mrow><mml:mi mathvariant="italic">BC</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>σ</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="italic">t</mml:mi></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mspace width="1em"/></mml:mrow></mml:math></inline-formula>for all <inline-formula><mml:math id="M22" altimg="si0022.svg"><mml:mrow><mml:mi mathvariant="italic">s</mml:mi><mml:mo>≠</mml:mo><mml:mi>v</mml:mi><mml:mo>≠</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M23" altimg="si0023.svg"><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>) is the total number of shortest paths from node <inline-formula><mml:math id="M24" altimg="si0024.svg"><mml:mi>s</mml:mi></mml:math></inline-formula> to node <inline-formula><mml:math id="M25" altimg="si0025.svg"><mml:mi>t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M26" altimg="si0026.svg"><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>) is the number of those paths that pass through <inline-formula><mml:math id="M27" altimg="si0027.svg"><mml:mi>v</mml:mi></mml:math></inline-formula>. For a connected graph G with N vertices, the Closeness Centrality (CC) of a vertex v is given by <inline-formula><mml:math id="M28" altimg="si0028.svg"><mml:mrow><mml:mi mathvariant="italic">CC</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="M29" altimg="si0029.svg"><mml:mi>t</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="M30" altimg="si0030.svg"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represents the shortest-path distance between <inline-formula><mml:math id="M31" altimg="si0031.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M32" altimg="si0032.svg"><mml:mi>t</mml:mi></mml:math></inline-formula>.</p>
      <p id="p0060">On the other hand, Eigenvector Centrality allots relative scores to all nodes in the network, acknowledging that connections to high-scoring nodes contribute more to the overall node score <xref rid="bib24" ref-type="bibr">[24]</xref>. The Eigenvector Centrality (EC) of a node <inline-formula><mml:math id="M33" altimg="si0033.svg"><mml:mi>i</mml:mi></mml:math></inline-formula> is defined as <inline-formula><mml:math id="M34" altimg="si0034.svg"><mml:mrow><mml:mi mathvariant="italic">EC</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi mathvariant="italic">EC</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="M35" altimg="si0035.svg"><mml:mi>j</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="M36" altimg="si0036.svg"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the elements of the adjacency matrix and <inline-formula><mml:math id="M37" altimg="si0037.svg"><mml:mi>λ</mml:mi></mml:math></inline-formula> is a constant. PageRank Centrality is computed using the formula <inline-formula><mml:math id="M38" altimg="si0038.svg"><mml:mrow><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="badbreak">+</mml:mo><mml:mi mathvariant="italic">d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="badbreak">+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo linebreak="badbreak">+</mml:mo><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="M39" altimg="si0039.svg"><mml:mrow><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>is the PageRank of page A, <inline-formula><mml:math id="M40" altimg="si0040.svg"><mml:mrow><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the PageRank of pages <inline-formula><mml:math id="M41" altimg="si0041.svg"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> which link to page <inline-formula><mml:math id="M42" altimg="si0042.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="italic">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>is the number of outbound links on page <inline-formula><mml:math id="M43" altimg="si0043.svg"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M44" altimg="si0044.svg"><mml:mi>d</mml:mi></mml:math></inline-formula> is a damping factor which set between 0 and 1. Finally, the Eccentricity (ECC) of a vertex <inline-formula><mml:math id="M45" altimg="si0045.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> in a graph is the maximum distance from <inline-formula><mml:math id="M46" altimg="si0046.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> to all other vertices, calculated by <inline-formula><mml:math id="M47" altimg="si0047.svg"><mml:mrow><mml:mi mathvariant="italic">ECC</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>max</mml:mi><mml:mi mathvariant="italic">d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="italic">t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="italic">for all t</mml:mi><mml:mo>≠</mml:mo><mml:mi mathvariant="italic">v</mml:mi></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="M48" altimg="si0048.svg"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="italic">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the shortest-path distance between the vertices <inline-formula><mml:math id="M49" altimg="si0049.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M50" altimg="si0050.svg"><mml:mrow><mml:mi>t</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
    </sec>
    <sec id="sec0035">
      <label>2.5</label>
      <title>Deep learning models</title>
      <p id="p0065">Current study leveraged the potential of deep learning through the implementation of three distinguished models, namely Identity Convolutional Neural Networks (ID-CNN), Recurrent Neural Networks (RNN), and Multi-Layer Perceptrons (MLP). In R, the 'keras' package was employed, providing a high-level neural networks API on the TensorFlow platform. Additionally, 'caret' in R, facilitating access to numerous machine learning algorithms, was used in conjunction with 'tensorflow' and 'reticulate'. 'Tensorflow' offers an R interface for TensorFlow, while 'reticulate' enables Python integration within the R environment. Each model's unique architecture processes input data differently, offering diversified data interpretations, thereby improving the accuracy and reliability of the predicted host-pathogen PPIs.</p>
      <sec id="sec0040">
        <label>2.5.1</label>
        <title>ID-Convolutional Neural Networks (CNN)</title>
        <p id="p0070">The ID-Convolutional Neural Networks (CNN) model forms an integral part of the deep learning approach. The ID-CNN are primarily known for their application in image processing, but their utility extends to PPI prediction as well <xref rid="bib25" ref-type="bibr">[25]</xref>. The core architecture of the ID-CNN model implemented is designed with multiple layers, each layer equipped with a specific role and function to perform <xref rid="bib26" ref-type="bibr">[26]</xref>. The architecture commenced with a 2D convolution layer equipped with 16 filters of kernel size <inline-formula><mml:math id="M51" altimg="si0051.svg"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, employing the Rectified Linear Unit (ReLU) activation function <inline-formula><mml:math id="M52" altimg="si0052.svg"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. This layer was instrumental in the detection of initial patterns or features within the data, with its output, <inline-formula><mml:math id="M53" altimg="si0053.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> given by <inline-formula><mml:math id="M54" altimg="si0054.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>*</mml:mo><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Following this, a second 2D convolution layer was added, this one featuring 32 filters and a kernel size of <inline-formula><mml:math id="M55" altimg="si0055.svg"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The same ReLU activation function was used, aiding in the learning of more complex patterns based on the initial features. The output of this layer, <inline-formula><mml:math id="M56" altimg="si0056.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> was calculated by <inline-formula><mml:math id="M57" altimg="si0057.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>O</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
        <p id="p0075">Post the convolutional operations, a global max pooling layer was implemented, denoted by <inline-formula><mml:math id="M58" altimg="si0058.svg"><mml:mrow><mml:mi>M</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula>This step reduced the dimensionality of the model, mitigating computational complexity and overfitting. Subsequently, two dense layers were added to the architecture. The first of these layers consisted of 64 neurons, its output, <inline-formula><mml:math id="M59" altimg="si0059.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, defined by <inline-formula><mml:math id="M60" altimg="si0060.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo>*</mml:mo><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The final layer was a single neuron employing a sigmoid activation function<inline-formula><mml:math id="M61" altimg="si0061.svg"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, producing the model output:<disp-formula id="eqn0015"><label>(3.3)</label><mml:math id="M62" altimg="si0062.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>P</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>O</mml:mi><mml:mn>3</mml:mn><mml:mo>*</mml:mo><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0080">The entire model was trained using the Adam optimizer and the binary cross-entropy loss function, a suitable choice for the binary nature of the task. This function is given by:<disp-formula id="eqn0020"><label>(3.4)</label><mml:math id="M63" altimg="si0063.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi mathvariant="italic">Loss</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo linebreak="badbreak">−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">ylog</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0085">It's noteworthy that the input data to the model included degree centrality features for both pathogen and host proteins, which was normalized before being fed into the model.</p>
      </sec>
      <sec id="sec0045">
        <label>2.5.2</label>
        <title>Recurrent neural network</title>
        <p id="p0090">In the pursuit of host-pathogen protein interaction analysis, a Recurrent Neural Network (RNN) was incorporated, ideal due to the inherent sequential and interconnected nature of the protein data <xref rid="bib27" ref-type="bibr">[27]</xref>. An RNN model was devised via the Keras library in R <xref rid="bib28" ref-type="bibr">[28]</xref>, comprising of an input layer, a hidden layer, and an output layer. The input layer is designed to accept feature vectors signifying both the pathogen and host protein's network centrality measures, reshaped suitably for an RNN. The hidden layer utilizes simple RNN units governed by the ReLU activation function. On a mathematical note, suppose the input sequence is <inline-formula><mml:math id="M64" altimg="si0064.svg"><mml:mrow><mml:mi>X</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The new hidden state at time <inline-formula><mml:math id="M65" altimg="si0065.svg"><mml:mi mathvariant="italic">t</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="M66" altimg="si0066.svg"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is calculated by the equation<disp-formula id="eqn0025"><label>(3.5)</label><mml:math id="M67" altimg="si0067.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mspace width="1em"/></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M68" altimg="si0068.svg"><mml:mi>f</mml:mi></mml:math></inline-formula> is the ReLU function, <inline-formula><mml:math id="M69" altimg="si0069.svg"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the input at time <inline-formula><mml:math id="M70" altimg="si0070.svg"><mml:mi>t</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="M71" altimg="si0071.svg"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="M72" altimg="si0072.svg"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="M73" altimg="si0073.svg"><mml:mi>b</mml:mi></mml:math></inline-formula> stand for the weights and bias parameters. The output layer is a dense layer featuring a single unit using a sigmoid activation function, fitting for the binary classification task as it computes probabilities. Mathematically, the output <inline-formula><mml:math id="M74" altimg="si0074.svg"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> at time <inline-formula><mml:math id="M75" altimg="si0075.svg"><mml:mi>t</mml:mi></mml:math></inline-formula> is given by the equation<disp-formula id="eqn0030"><label>(3.6)</label><mml:math id="M76" altimg="si0076.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi mathvariant="italic">sigmoid</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math id="M77" altimg="si0077.svg"><mml:mi mathvariant="italic">sigmoid</mml:mi></mml:math></inline-formula> as the sigmoid functions and <inline-formula><mml:math id="M78" altimg="si0078.svg"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M79" altimg="si0079.svg"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>as the weight and bias parameters for the output layer respectively. The training of this model was accomplished using the Adam optimization algorithm, an efficient extension to stochastic gradient descent. The loss function applied was binary cross-entropy, appropriate for binary classification problems. On a mathematical level, assuming the target (true value) as <inline-formula><mml:math id="M80" altimg="si0080.svg"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and the predicted output as <inline-formula><mml:math id="M81" altimg="si0081.svg"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the binary cross-entropy loss for the output at time <inline-formula><mml:math id="M82" altimg="si0082.svg"><mml:mi>t</mml:mi></mml:math></inline-formula> is calculated as<disp-formula id="eqn0035"><label>(3.7)</label><mml:math id="M83" altimg="si0083.svg"><mml:mrow><mml:mspace width="1em"/><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0095">This framework of equations directs the forward propagation, learning, and prediction process of the RNN model within the study.</p>
      </sec>
      <sec id="sec0050">
        <label>2.5.3</label>
        <title>Multilayer Perceptron (MLP)</title>
        <p id="p0100">The Multi-layer Perceptron (MLP), an artificial neural network type, was selected for this analysis owing to the networked architecture of the protein data. The MLP's ability to decipher complex, non-linear relationships through multiple layers of nodes made it a perfect fit for this study. Each node, symbolizing a neuron, is equipped with an activation function that aids in transforming the input data <xref rid="bib29" ref-type="bibr">[29]</xref>. The MLP model employed for this study comprises two hidden layers, containing 10 and 20 neurons respectively. These hidden layers form the heart of the MLP, endowing the network with its capacity to abstract and transform the input data. As the input data flows through these hidden layers, the model unveils intricate patterns and structures, allowing for more accurate prediction outcomes.</p>
        <p id="p0105">A critical aspect of this model is the logistic activation function. Chosen for its ability to map any real-valued number into a range between 0 and 1, the logistic function is a prime choice for binary classification tasks. It proves ideal for differentiating between interaction and non-interaction within the network-based protein data. Mathematically, in the hidden layers, each neuron computes a weighted sum of the inputs, adds a bias term and applies the logistic activation function. If we denote the input to a neuron as <inline-formula><mml:math id="M84" altimg="si0084.svg"><mml:mrow><mml:mi>x</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, weights as <inline-formula><mml:math id="M85" altimg="si0085.svg"><mml:mrow><mml:mi>w</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and bias as <inline-formula><mml:math id="M86" altimg="si0086.svg"><mml:mi>b</mml:mi></mml:math></inline-formula>, the output of the neuron <inline-formula><mml:math id="M87" altimg="si0087.svg"><mml:mi>y</mml:mi></mml:math></inline-formula> is computed as follows:<disp-formula id="eqn0040"><label>(3.8)</label><mml:math id="M88" altimg="si0088.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>y</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi mathvariant="italic">logistic</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p id="p0110">The MLP model training was regulated by a learning rate of 0.01 and a convergence threshold of 0.1. These values were fine-tuned to optimize the learning process and to control the pace at which the model learns from the protein data. Balancing the learning rate was essential to avoid erratic learning behaviors and to prevent the model from learning too slowly. A convergence threshold of 0.1 was set to ensure the learning process perseveres until the error on the training data reaches an acceptably low point.</p>
        <p id="p0115">Additionally, the model utilized the backpropagation algorithm to adjust its weights and bias values. This adjustment minimizes the discrepancy between the actual and predicted outputs over numerous iterations, or epochs, thereby progressively enhancing the model's accuracy. During each epoch, the backpropagation algorithm implements two primary steps for each training sample: Forward Propagation: The predicted output (<inline-formula><mml:math id="M89" altimg="si0089.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is calculated using the current weights and bias values. On the other hand, the weights and bias terms are updated based on the gradient of the loss function with respect to the weights and bias in back propagation. If we denote the loss function as <inline-formula><mml:math id="M90" altimg="si0090.svg"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M91" altimg="si0091.svg"><mml:mi>y</mml:mi></mml:math></inline-formula> is the actual output and (<inline-formula><mml:math id="M92" altimg="si0092.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the predicted output, the weights and bias are updated as follows:<disp-formula id="eqn0045"><label>(3.9)</label><mml:math id="M93" altimg="si0093.svg"><mml:mrow><mml:mspace width="1em"/><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>η</mml:mi><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="eqn0050"><label>(3.10)</label><mml:math id="M94" altimg="si0094.svg"><mml:mrow><mml:mi>b</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>b</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>η</mml:mi><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p id="p0120">Here <inline-formula><mml:math id="M95" altimg="si0095.svg"><mml:mi>η</mml:mi></mml:math></inline-formula> is the learning rate and <inline-formula><mml:math id="M96" altimg="si0096.svg"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></inline-formula> and <inline-formula><mml:math id="M97" altimg="si0097.svg"><mml:mrow><mml:mspace width="1em"/><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> are the gradients of the loss function with respect to the weights and bias, computed using the chain rule of differentiation.</p>
      </sec>
    </sec>
    <sec id="sec0055">
      <label>2.6</label>
      <title>Evaluation of models</title>
      <p id="p0125">To thoroughly assess the predictive competence of the models, six widely recognized evaluation metrics were employed: Precision, Accuracy, Sensitivity, Specificity, F1-score, and Matthew's Correlation Coefficient (MCC). These metrics provide a comprehensive understanding of the model's performance, considering both the positive and negative classes of the dataset, as well as the balance between them.</p>
      <p id="p0130">Precision (Pre) is a metric that reflects the exactness of the positive predictions made by the model <xref rid="bib30" ref-type="bibr">[30]</xref>. It is calculated as the ratio of true positives to the sum of true positives and false positives. Precision is essential as it focuses on the proportion of true positive predictions among all positive predictions, a crucial measure when false positives can significantly skew results, as discussed in the work of Powers et al. <xref rid="bib31" ref-type="bibr">[31]</xref>. Mathematically, it's represented as:<disp-formula id="eqn0055"><label>(3.11)</label><mml:math id="M98" altimg="si0098.svg"><mml:mrow><mml:mi mathvariant="italic">Precision</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FalsePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0135">Accuracy (Acc) measures the proportion of total predictions that are correct, irrespective of whether they are positive or negative <xref rid="bib32" ref-type="bibr">[32]</xref>. However, when used alongside the other metrics, it contributes to a holistic understanding of the model’s effectiveness, as suggested by Johnson and Khoshgoftaar <xref rid="bib33" ref-type="bibr">[33]</xref>. It is computed as the sum of true positives and true negatives divided by the total number of predictions, as shown:<disp-formula id="eqn0060"><label>(3.12)</label><mml:math id="M99" altimg="si0099.svg"><mml:mrow><mml:mi mathvariant="italic">Accuracy</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="italic">TrueNegatives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TotalPredictions</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0140">Sensitivity (Sn), also known as recall or true positive rate, quantifies the ability of the model to correctly identify positive instances <xref rid="bib34" ref-type="bibr">[34]</xref>. It is calculated as the ratio of true positives to the sum of true positives and false negatives, which are expressed as:<disp-formula id="eqn0065"><label>(3.13)</label><mml:math id="M100" altimg="si0100.svg"><mml:mrow><mml:mi mathvariant="italic">Sensitivity</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FalseNegatives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">FN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0145">Specificity (Sp) is the metric that reflects the model's ability to correctly identify negative instances <xref rid="bib35" ref-type="bibr">[35]</xref>. It is the ratio of true negatives to the sum of true negatives and false positives, represented as:<disp-formula id="eqn0070"><label>(3.14)</label><mml:math id="M101" altimg="si0101.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi mathvariant="italic">Specificity</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TrueNegatives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TrueNegatives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FalsePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0150">F1-score is the harmonic mean of Precision and Sensitivity. It is particularly useful when dealing with imbalanced datasets as it takes both false positives and false negatives into account <xref rid="bib36" ref-type="bibr">[36]</xref>. The F1-score provides a balanced view of the model’s performance on both classes, ensuring that neither false positives nor false negatives are disproportionately affecting the model's evaluation, as indicated in studies by Kakkar et al. <xref rid="bib37" ref-type="bibr">[37]</xref>. The formula for the F1-score is:<disp-formula id="eqn0075"><label>(3.15)</label><mml:math id="M102" altimg="si0102.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi mathvariant="italic">score</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Precision</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="italic">Sensitivity</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Precision</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">Sensitivity</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p id="p0155">Matthew's Correlation Coefficient (MCC) is a more robust metric as it considers all values in the confusion matrix (true positives, true negatives, false positives, and false negatives) <xref rid="bib36" ref-type="bibr">[36]</xref>. The range of MCC is between − 1 and + 1, where + 1 represents a perfect prediction, 0 indicates a random prediction, and − 1 denotes an inverse prediction. The MCC is calculated as follows:<disp-formula id="eqn0080"><label>(3.16)</label><mml:math id="M103" altimg="si0103.svg"><mml:mrow><mml:mi mathvariant="italic">MCC</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="italic">FN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0160">These performance evaluation metrics were critical in systematically comparing and assessing the predictive capabilities of the 1D-CNN, RNN, and MLP models in the context of host-pathogen interaction prediction. By utilizing these six metrics, it became possible to analyze each model's proficiency from various dimensions such as precision, accuracy, sensitivity, specificity, and overall correlation. It is important to underscore that this comprehensive evaluation framework facilitated the discernment of the best-suited deep learning model for predicting potential interactions. This strategic methodology considerably enhanced the selection process, ensuring the identification of the most adept model to navigate the complex landscape of host-pathogen interactions.</p>
    </sec>
  </sec>
  <sec id="sec0060">
    <label>3</label>
    <title>Results</title>
    <sec id="sec0065">
      <label>3.1</label>
      <title>Dataset assembly and preliminary analysis</title>
      <p id="p0165">In order to predict host-pathogen interactions, the Citrus-HLB (Huanglongbing) interactome was selected as the primary case study. The primary sources of data for this study included the Pred-HPI and GreeningDB databases. The Pred-HPI database, known for its predictive modeling features, was combined with GreeningDB, a specialized database for citrus greening disease interactions, to provide a comprehensive range of interaction data. The true PPI data between Citrus and the Candidatus Liberibacter asiaticus (CLas) bacteria was sourced from the Greening and Pred-HPI database. This resulted in the retrieval of 1011 host-pathogen PPIs (45 unique pathogen proteins and 359 unique host proteins) from these databases. Additionally, 5 experimentally validated interactions were incorporated into the dataset, leading to a total of 1016 true PPIs. The dataset comprised of 45 unique pathogen proteins and 359 unique host proteins, accumulating a network of 404 nodes with a potential for 1016 possible interactions.</p>
    </sec>
    <sec id="sec0070">
      <label>3.2</label>
      <title>Construction of negative interaction sets with varied ratios</title>
      <p id="p0170">In this study, a common strategy was adopted to generate negative Protein-Protein Interactions (PPIs) based on traditional approaches found in the literature. This strategy involved creating random pairings between distinct protein sets - specifically, the selected pathogen proteins and host proteins. Subsequently, from these random pairings, subsets were strategically chosen to represent negative interactions. The importance of this lies in creating a comparative base against which positive interactions can be evaluated, thereby enhancing the predictive power of the developed models.</p>
      <p id="p0175">To further enrich the analysis and examine the robustness of the models, negative interactions were selected with varying ratios: 1:1 and 1:10. This led to the formation of two distinct datasets. The first, balanced, set comprised 1016 positive and 1016 negative PPIs, while the second, imbalanced, set encompassed 1016 positive and 10160 negative PPIs. The implementation of varying negative to positive ratios was essential in evaluating the robustness and resilience of the predictive models under different levels of data imbalance, a common scenario in biological studies. In the development of predictive models for host-pathogen PPIs, the selection of an appropriate class ratio is pivotal to balance between model performance and realistic representation of the dataset. For this study, a 1:10 class ratio was strategically chosen, aiming to effectively address the challenges presented by highly imbalanced datasets and to ensure adequate representation of the minority class. This decision is rooted in evidence from the literature, where extremely imbalanced ratios like 1:25 or 1:100, commonly used in PPI studies, have been shown to introduce a significant bias towards the majority class, leading to a high incidence of false negatives, as elucidated by Chen et al. <xref rid="bib17" ref-type="bibr">[17]</xref>. Conversely, ratios that closely approximate equal distribution, such as 1:1, often do not accurately reflect the actual distribution encountered in biological datasets, where non-interacting pairs are generally more prevalent. The adoption of a 1:10 ratio represents this balance, reducing the risk of bias towards the majority class while still mirroring the typical distribution in biological datasets. Supporting this selection, preliminary experiments within the study demonstrated that a 1:10 ratio optimally balances sensitivity and specificity. This finding is in alignment with the results presented by Lei et al. <xref rid="bib38" ref-type="bibr">[38]</xref>, where similar class ratios have been shown to enhance model performance in accurately identifying true positives without disproportionately increasing false negatives. Hence, this ratio has been found to effectively represent the real-world distribution of PPI datasets, ensuring sensitivity towards the minority class and enhancing the overall reliability of the predictive models.</p>
    </sec>
    <sec id="sec0075">
      <label>3.3</label>
      <title>Performance of the features in modeling host-pathogen interactions</title>
      <p id="p0180">The feature extraction process was integral to the deployment of the 1D-CNN, RNN, and MLP models. Seven key features, namely, Degree Centrality, Betweenness Centrality, Closeness Centrality, Eigenvector Centrality, PageRank Centrality, Hub Score, and Eccentricity were selected to comprehensively describe each host-pathogen protein pair. For each host-pathogen protein pair, these attributes were extracted and consolidated into a unified vector. This process resulted in comprehensive Protein-Protein Interaction (PPI) feature vectors, embodying the synergistic characteristics of both host and pathogen proteins. Each feature was individually assessed across the three models to deduce their unique contribution to the predictive accuracy of the models, fostering a comprehensive understanding of the interplay between individual features and model performance.</p>
      <p id="p0185">An effective evaluation of the models necessitated the careful construction of the independent datasets. Contrary to common practice where cross-validation is predominantly used due to its superior performance with unseen datasets, a different strategy was implemented for this study. Specifically, a fifth of the PPIs, encompassing both positive and negative interactions, were randomly allocated to form the independent dataset. The remainder of the PPIs, involving both types of interactions, were amalgamated into the training set. This approach, though unconventional, ensured a robust and comprehensive evaluation of the models' performance.</p>
      <sec id="sec0080">
        <label>3.3.1</label>
        <title>Assessment of 1D-CNN model at different class ratios</title>
        <p id="p0190">Analyzing the performance of the 1D-CNN model at different class ratios resulted in significant findings. The model's efficacy was assessed using multiple protein features, and statistical parameters such as Sensitivity, Specificity, Precision, Accuracy, F1-score, and MCC were evaluated (<xref rid="fig0005" ref-type="fig">Fig. 1</xref>). <xref rid="tbl0005" ref-type="table">Table 1</xref> provides an exhaustive assessment of the 1D-CNN model's performance at a class ratio of 1:1. During independent testing, the Degree Centrality feature exhibited strong performance with a Sensitivity, Specificity, and Precision of 0.8374, 0.8423, and 0.8415 respectively. The model's Accuracy and F1-score stood at 0.8399 and 0.8395, showcasing balanced categorization capabilities. Additionally, the MCC value of 0.6798 denotes reliable prediction of both classes. However, the standout feature at the 1:1 class ratio was Eigenvector Centrality. During independent testing, this feature achieved the highest Accuracy, F1-score, and MCC values, which were 0.8522, 0.8648, and 0.7171 respectively, indicating the model's proficiency in discerning true positives and negatives. Similar trends were observed during 5-fold cross-validation. Degree Centrality showed an Accuracy of 0.8529, an F1-score of 0.8568, and an MCC of 0.7074. Meanwhile, Eigenvector Centrality continued to excel with an Accuracy of 0.8388, an F1-score of 0.8493, and an MCC of 0.6844.<fig id="fig0005"><label>Fig. 1</label><caption><p>Performance evaluation results of CNN model at class ratios 1:1: (<bold>A</bold>) Independent testing (<bold>B</bold>) 5-Fold cross validation. Performance evaluation results at different class ratios 1:10: (<bold>C</bold>) Independent testing (<bold>D</bold>) 5-Fold cross validation.</p></caption><alt-text id="at0005">Fig. 1</alt-text><graphic xlink:href="gr1" id="lk0005"/></fig><table-wrap position="float" id="tbl0005"><label>Table 1</label><caption><p>1D-CNN model performance for each protein feature at class ratio of 1:1 and 1:10. Performance values of independent testing and 5-fold cross validation are shown within a same cell as follow: independent testing <italic>/</italic> 5-fold cross validation.</p></caption><alt-text id="at0030">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th><bold>Network features</bold></th><th><bold>Sensitivity</bold></th><th><bold>Specificity</bold></th><th><bold>Precision</bold></th><th><bold>Accuracy</bold></th><th><bold>F1-score</bold></th><th><bold>MCC</bold></th></tr><tr><th colspan="7"><bold>Class ratio 1:1</bold></th></tr></thead><tbody><tr><td><bold>Degree_centrality</bold></td><td>0.8374 / 0.8787</td><td>0.8423 / 0.8272</td><td>0.8415 / 0.8366</td><td>0.8399 / 0.8529</td><td>0.8395 / 0.8568</td><td>0.6798 / 0.7074</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.8325 / 0.7928</td><td>0.7733 / 0.8222</td><td>0.7860 / 0.8181</td><td>0.8029 / 0.8075</td><td>0.8086 / 0.8046</td><td>0.6069 / 0.6163</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.1576 / 0.1678</td><td>0.9901 / 0.9889</td><td>0.9411 / 0.9479</td><td>0.5738 / 0.5784</td><td>0.2700 / 0.2841</td><td>0.2667 / 0.2767</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9458 / 0.9056</td><td>0.7586 / 0.7721</td><td>0.7966 / 0.8001</td><td>0.8522 / 0.8388</td><td>0.8648 / 0.8493</td><td>0.7171 / 0.6844</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.0314 / 0.5184</td><td>0.4546 / 0.9265</td><td>0.5571 / 0.8768</td><td>0.6723 / 0.7224</td><td>0.53 / 0.6510</td><td>0.005 /0.4878</td></tr><tr><td><bold>Hub_score</bold></td><td>0.9261 / 0.9130</td><td>0.7389 / 0.7696</td><td>0.7800 / 0.7994</td><td>0.8325 / 0.8413</td><td>0.8468 / 0.8520</td><td>0.6769 / 0.6905</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.8817 / 0.8468</td><td>0.8128 / 0.8750</td><td>0.8248 / 0.8716</td><td>0.8472 / 0.8609</td><td>0.8523 / 0.8581</td><td>0.6962 / 0.7237</td></tr><tr><td colspan="7"><bold>Class ratio 1:10</bold></td></tr><tr><td><bold>Degree_centrality</bold></td><td>0.9856 / 0.9885</td><td>0.4227 / 0.4112</td><td>0.9400 / 0.9445</td><td>0.9303 / 0.9367</td><td>0.9623 / 0.9660</td><td>0.5355 / 0.5398</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.9774 / 0.9899</td><td>0.3939 / 0.3192</td><td>0.9433 / 0.9359</td><td>0.9259 / 0.9291</td><td>0.9600 / 0.9621</td><td>0.4610 / 0.4644</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.9977 / 1</td><td>0.0215 / 0.0234</td><td>0.9184 / 0.9107</td><td>0.9161 / 0.9109</td><td>0.9561 / 0.9532</td><td>0.0769 / 0.1423</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9925 / 0.9856</td><td>0.5475 / 0.5794</td><td>0.9524 / 0.9588</td><td>0.9486 / 0.9484</td><td>0.9721 / 0.9720</td><td>0.6744 / 0.6561</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.9950 / 0.9960</td><td>0.0945 / 0.0854</td><td>0.9177 / 0.9155</td><td>0.914 / 0.9129</td><td>0.9548 / 0.9540</td><td>0.2265 / 0.2178</td></tr><tr><td><bold>Hub_score</bold></td><td>0.9808 /0.9931</td><td>0.6323 / 0.53178</td><td>0.9638 / 0.9548</td><td>0.9491 / 0.9510</td><td>0.9722 / 0.9735</td><td>0.6698 / 0.6652</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.9931 / 0.9823</td><td>0.35 / 0.3871</td><td>0.9397 / 0.9410</td><td>0.9357 / 0.9279</td><td>0.9656 / 0.9612</td><td>0.5150 /0.4814</td></tr></tbody></table></table-wrap></p>
        <p id="p0195">On the other hand, the model's performance with a more imbalanced class ratio of 1:10 is elucidated in <xref rid="tbl0005" ref-type="table">Table 1</xref>. With this ratio, the Eigenvector Centrality and Hub Score features emerged as significant, recording high accuracy levels in both independent testing and 5-fold cross-validation. These results suggest that the model can uphold its performance even with a heavily skewed class distribution.</p>
      </sec>
      <sec id="sec0085">
        <label>3.3.2</label>
        <title>Comprehensive analysis of RNN model performance across features and class ratios</title>
        <p id="p0200">The RNN model's performance was thoroughly assessed across two disparate class ratios of 1:1 and 1:10, with each protein feature being evaluated individually (<xref rid="fig0010" ref-type="fig">Fig. 2</xref>). This extensive appraisal facilitated a clear distinction of the model's strengths and areas requiring improvement. At the 1:1 class ratio, several noteworthy findings were observed. The feature that stood out the most was Eigenvector Centrality, which achieved the highest metrics in the majority of categories. These included an Accuracy of 0.8694 during independent testing, a similarly impressive F1-score of 0.8716, and an MCC of 0.7393, the highest among all features. The Degree Centrality, although not leading, displayed considerable efficacy, particularly with its Sensitivity of 0.8088 and an MCC of 0.6887. However, Closeness Centrality demonstrated difficulties in this scenario, especially indicated by a negative MCC during independent testing, signifying a possible shortcoming of the RNN model when handling this feature.<fig id="fig0010"><label>Fig. 2</label><caption><p>Performance evaluation results of RNN model at class ratios 1:1: (<bold>A</bold>) Independent testing (<bold>B</bold>) 5-Fold cross validation. Performance evaluation results at different class ratios 1:10: (<bold>C</bold>) Independent testing (<bold>D</bold>) 5-Fold cross validation.</p></caption><alt-text id="at0010">Fig. 2</alt-text><graphic xlink:href="gr2" id="lk0010"/></fig></p>
        <p id="p0205">The analysis at the class ratio of 1:10 provided further intriguing insights. Eigenvector Centrality maintained its robust performance, securing an Accuracy of 0.9482 and an F1-score of 0.9720, marking its consistent influence on the RNN model's effectiveness. Degree Centrality not only exhibited exceptional Sensitivity and Precision but also contributed to an overall Accuracy of 0.9268. The Hub Score also performed consistently, exhibiting an Accuracy of 0.9477, reinforcing the model's overall effectiveness at the 1:10 ratio. On the other hand, the Closeness Centrality again presented unique challenges. It demonstrated a high Sensitivity of 0.9029 during independent testing but a strikingly low MCC of 0 in the 5-fold cross-validation. This anomaly exposes potential weaknesses in the model when operating under this class ratio and with this feature.<xref rid="tbl0010" ref-type="table">Table 2</xref>.<table-wrap position="float" id="tbl0010"><label>Table 2</label><caption><p>RNN model performance for each protein feature at class ratio of 1:1 and 1:10. Performance values of independent testing and 5-fold cross validation are shown within a same cell as follow: independent testing <italic>/</italic> 5-fold cross validation.</p></caption><alt-text id="at0035">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th><bold>Network features</bold></th><th><bold>Sensitivity</bold></th><th><bold>Specificity</bold></th><th><bold>Precision</bold></th><th><bold>Accuracy</bold></th><th><bold>F1-score</bold></th><th><bold>MCC</bold></th></tr><tr><th colspan="7"><bold>Class ratio 1:1</bold></th></tr></thead><tbody><tr><td><bold>Degree_centrality</bold></td><td>0.8088/ 0.8572</td><td>0.8839 / 0.7519</td><td>0.8965 / 0.7761</td><td>0.8423/ 0.8045</td><td>0.8504/ 0.8143</td><td>0.6887/ 0.6130</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.7969/ 0.8640</td><td>0.7799/ 0.7220</td><td>0.7733 / 0.7568</td><td>0.7881/ 0.7930</td><td>0.785 / 0.8067</td><td>0.5766/ 0.5924</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.4242 / 0.4808</td><td>0.4755 / 0.8125</td><td>0.2068 / 0.7189</td><td>0.4630 / 0.6467</td><td>0.2781 / 0.5758</td><td>-0.0860 / 0.3109</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.8571 / 0.8503</td><td>0.8826 / 0.7789</td><td>0.8866 / 0.7938</td><td>0.8694 / 0.8146</td><td>0.8716 / 0.8208</td><td>0.7393 / 0.6313</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.9 / 0.4808</td><td>0.6678 / 0.8125</td><td>0.5320 / 0.7189</td><td>0.7364 / 0.6467</td><td>0.6687 / 0.5758</td><td>0.5182 / 0.3109</td></tr><tr><td><bold>Hub_score</bold></td><td>0.7939 / 0.8505</td><td>0.8959 / 0.7656</td><td>0.9113 / 0.7841</td><td>0.8374 / 0.8081</td><td>0.8486 / 0.8159</td><td>0.6823 / 0.6186</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.8622 / 0.8608</td><td>0.8380 / 0.7610</td><td>0.8325 / 0.7832</td><td>0.8497 / 0.8109</td><td>0.8471 / 0.8200</td><td>0.6999 / 0.6254</td></tr><tr><td colspan="7"><bold>Class ratio 1:10</bold></td></tr><tr><td><bold>Degree_centrality</bold></td><td>0.9369 /<break/>0.9850</td><td>0.7413 /<break/>0.4175</td><td>0.9851 /<break/>0.9442</td><td>0.9268 /<break/>0.9334</td><td>0.9604 /<break/>0.9641</td><td>0.5050 /<break/>0.5268</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.9425 /<break/>0.9950</td><td>0.6302 /<break/>0.8571</td><td>0.9784 / 0.9950</td><td>0.9259 / 0.9308</td><td>0.9601 / 0.9631</td><td>0.4535 / 0.4769</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.9029 /<break/>1</td><td>0.2666 /<break/>0</td><td>0.9945 /<break/>0.9090</td><td>0.8988 /<break/>0.9090</td><td>0.9465 /<break/>0.9523</td><td>0.0464 /<break/>0</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9568 / 0.9900</td><td>0.8106 /<break/>0.5095</td><td>0.9877 / 0.9528</td><td>0.9482 /<break/>0.9464</td><td>0.9720 /<break/>0.9710</td><td>0.6366 /<break/>0.6288</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.9205 /<break/>0.9996</td><td>0.34 /<break/>0.0032</td><td>1 /<break/>0.9093</td><td>0.9205 / 0.9090</td><td>0.0013 /<break/>0.0151</td><td>0.9205 / 0.9523</td></tr><tr><td><bold>Hub_score</bold></td><td>0.9607 /<break/>0.9895</td><td>0.7697 /<break/>0.4945</td><td>0.9828 /<break/>0.9514</td><td>0.9477 /<break/>0.9445</td><td>0.9716 /<break/>0.9701</td><td>0.6457 /<break/>0.6136</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.9429 /<break/>0.9801</td><td>0.675 /<break/>0.3782</td><td>0.9615 /<break/>0.9403</td><td>0.9808 /<break/>0.9253</td><td>0.9286 /<break/>0.9597</td><td>0.4857 /<break/>0.4614</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="sec0090">
        <label>3.3.3</label>
        <title>Performance evaluation of MLP model for protein classification using different network features and class ratios</title>
        <p id="p0210">For the class ratio of 1:1, the model demonstrated varying degrees of effectiveness depending on the protein network feature used (<xref rid="fig0015" ref-type="fig">Fig. 3</xref>). The Eigenvector Centrality feature outshone the others, achieving an impressive accuracy rate of 0.9334 in independent testing and 0.9487 in 5-fold cross-validation. Its sensitivity and specificity scores were also quite high (0.9605 / 0.9578 and 0.9064 / 0.9126 respectively), indicating its ability to correctly identify positive classes and correctly reject the negative ones. Moreover, its high F1-Score (0.9352 / 0.9209) suggests a balanced precision and recall, and the robust MCC (0.8682 / 0.8894) demonstrates the model's quality in terms of binary classification. The Hub Score feature also yielded substantial results, with an accuracy of 0.9261 / 0.8745 and a sensitivity of 0.9310 / 0.9234, reinforcing the reliability of these two features. Conversely, the Closeness Centrality feature performed less favorably, displaying an accuracy of 0.5689 / 0.5246 and sensitivity of 0.9605 / 0.9213, hinting at its weaker ability to accurately predict protein classes.<fig id="fig0015"><label>Fig. 3</label><caption><p>Performance evaluation results of MLP model at class ratios 1:1: (<bold>A</bold>) Independent testing (<bold>B</bold>) 5-Fold cross validation. Performance evaluation results at different class ratios 1:10: (<bold>C</bold>) Independent testing (<bold>D</bold>) 5-Fold cross validation.</p></caption><alt-text id="at0015">Fig. 3</alt-text><graphic xlink:href="gr3" id="lk0015"/></fig></p>
        <p id="p0215">When the class ratio was adjusted to 1:10, the Eigenvector Centrality continued to lead in performance, particularly in sensitivity (0.9794 / 0.9651) and precision (0.9946 / 0.950739). This high sensitivity means the model could correctly identify a high proportion of actual positives, while the precision indicates that out of the classes predicted positive, most were accurate. The Degree Centrality also showed steady performance across multiple metrics like sensitivity (0.8522/0.8468), specificity (0.8472/0.8508), and accuracy (0.8497/0.8488). However, for this class ratio, Pagerank Centrality emerged as the underperforming feature with lower sensitivity (0.6453 / 0.6427) and accuracy scores (0.7266 / 0.7105), implying a lower correct prediction rate and a higher false-negative rate. In essence, these results elucidate the considerable impact of different protein network features on the MLP model's performance, as well as the role of class ratios in the efficacy of these predictions. The superior performance of Eigenvector Centrality feature indicates its potential usefulness in practical applications, although the differing results among other features underscore the importance of careful feature selection in building effective protein classification models.<xref rid="tbl0015" ref-type="table">Table 3</xref>.<table-wrap position="float" id="tbl0015"><label>Table 3</label><caption><p>MLP model performance for each protein feature at class ratio of 1:1 and 1:10. Performance values of independent testing and 5-fold cross validation are shown within a same cell as follow: independent testing <italic>/</italic> 5-fold cross validation.</p></caption><alt-text id="at0040">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th><bold>Network Features</bold></th><th><bold>Accuracy</bold></th><th><bold>Sensitivity</bold></th><th><bold>Specificity</bold></th><th><bold>F1_Score</bold></th><th><bold>Precision</bold></th><th><bold>MCC</bold></th></tr><tr><th colspan="7"><bold>Class ratio 1:1</bold></th></tr></thead><tbody><tr><td><bold>Degree_centrality</bold></td><td>0.8694 / 0.8448</td><td>0.8620 / 0.7881</td><td>0.8768 / 0.9014</td><td>0.8684 / 0.8355</td><td>0.875 / 0.8889</td><td>0.7389 / 0.6941</td></tr><tr><td><bold>Betweenness_centrality</bold></td><td>0.7881 / 0.8325</td><td>0.8423 / 0.8275</td><td>0.7339 / 0.8374</td><td>0.7990 / 0.8316</td><td>0.76 / 0.8358</td><td>0.5797 / 0.6650</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9334/ 0.9487</td><td>0.9605 / 0.9578</td><td>0.9064 / 0.9126</td><td>0.9352 / 0.9209</td><td>0.9112 / 0.9334</td><td>0.8682 / 0.8894</td></tr><tr><td><bold>Hub_score</bold></td><td>0.9261 / 0.8745</td><td>0.9310 / 0.9234</td><td>0.9211 / 0.9382</td><td>0.9264 / 0.8978</td><td>0.9219 / 0.9051</td><td>0.8522 / 0.8089</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.7044 / 0.6427</td><td>0.9359 / 0.7782</td><td>0.4729 / 0.7441</td><td>0.76 / 0.7105</td><td>0.6397 / 0.6894</td><td>0.4613 / 0.4253</td></tr><tr><td><bold>Pagerank-centrality</bold></td><td>0.8399 / 0.8570</td><td>0.8522 / 0.8721</td><td>0.8275 / 0.8091</td><td>0.84184 / 0.8101</td><td>0.8317 / 0.8566</td><td>0.6800 / 0.6421</td></tr><tr><td><bold>Closeness-centrality</bold></td><td>0.5689 / 0.5246</td><td>0.9605 / 0.9213</td><td>0.1773 / 0.1345</td><td>0.6902 / 0.6189</td><td>0.5386 / 0.4621</td><td>0.2218 / 0.2</td></tr><tr><td colspan="7"><bold>Class ratio 1:10</bold></td></tr><tr><td><bold>Degree_centrality</bold></td><td>0.8522/<break/>0.8468</td><td>0.8472/<break/>0.8508</td><td>0.8480 / 0.8502</td><td>0.8497/ 0.8488</td><td>0.8501/ 0.8483</td><td>0.8195/ 0.7980</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.8592 / 0.8832</td><td>0.8226 / 0.7909</td><td>0.8277 / 0.8093</td><td>0.8374 / 0.8370</td><td>0.8398 / 0.8443</td><td>0.8851 / 0.8345</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.8522 /<break/>0.8660</td><td>0.8029 / 0.7944</td><td>0.8122 / 0.8325</td><td>0.8275 /<break/>0.8015</td><td>0.8317 /<break/>0.80</td><td>0.7559 /<break/>0.711</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9794 /<break/>0.9651</td><td>0.8195 / 0.8411</td><td>0.9946 / 0.950739</td><td>0.8736 / 0.905</td><td>0.9352 / 0.8642</td><td>0.8647 / 0.8020</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.9113 / 0.9018</td><td>0.4926 / 0.5122</td><td>0.6423 /0.6494</td><td>0.7019 / 0.7070</td><td>0.7535 / 0.7549</td><td>0.4448 / 0.4498</td></tr><tr><td><bold>Hub_score</bold></td><td>0.8472 /<break/>0.8351</td><td>0.8916 /<break/>0.8734</td><td>0.8865 / 0.8698</td><td>0.8694 / 0.8542</td><td>0.8664 / 0.8516</td><td>0.7396 / 0.7099</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.6453 /<break/>0.6427</td><td>0.8078 /<break/>0.7782</td><td>0.7705 / 0.7441</td><td>0.7266 / 0.7105</td><td>0.7024 / 0.6894</td><td>0.4593 / 0.4253</td></tr></tbody></table></table-wrap></p>
      </sec>
    </sec>
    <sec id="sec0095">
      <label>3.4</label>
      <title>Evaluation of models</title>
      <p id="p0220">The evaluation aimed to assess the performance of the MLP model across several network-based features. The features tested included Degree_centrality, Betweenness_centrality, Closeness_centrality, Eigenvector_centrality, Pagerank_centrality, Hub_score, and Eccentricity. These features are commonly used in network analysis to measure the importance of individual nodes within a network. In the context of protein-protein interaction networks, these features assists in the identification of key proteins that play critical roles in biological processes.</p>
      <p id="p0225">On independent testing, the Eigenvector_centrality feature stood out for its performance, achieving the highest accuracy (0.8736) and Matthews Correlation Coefficient (MCC, 0.8647) among all the features tested. The Eigenvector Centrality of a protein in a network measures the extent to which it is connected to other highly connected proteins. This feature is often used in biological network analysis to identify proteins that, although they may not have many connections themselves, are connected to key proteins in the network <xref rid="bib39" ref-type="bibr">[39]</xref>. In this context, the application of Eigenvector Centrality features has garnered attention, particularly in the study of host-pathogen protein interactions. Studies such as Khorsand et al. <xref rid="bib40" ref-type="bibr">[40]</xref> and Cui et al. <xref rid="bib41" ref-type="bibr">[41]</xref> have demonstrated the efficacy of this approach in unraveling complex interaction dynamics, suggesting new pathways for therapeutic intervention. The model's performance remained high during cross-validation, achieving an accuracy of 0.905 and an MCC of 0.8020. The high cross-validation performance indicates that the model generalizes well to unseen data and is not overfitted to the training data, which adds confidence to the robustness of our model.</p>
      <p id="p0230">In order to apply these findings in a predictive manner, each protein pair was given a score based on their Eigenvector Centrality features. The pair's features were then normalized and input into the trained MLP model. The overall architecture of MLP model was also presented in <xref rid="fig0020" ref-type="fig">Fig. 4</xref>. This produced a probability of interaction for each protein pair, which can be interpreted as a likelihood score for each pair's interaction. A threshold of 0.9 was set for these probability values to focus on the most significant interactions (<bold>Supplementary File 1:</bold>
<xref rid="sec0175" ref-type="sec">Table S1</xref>). Protein pairs with a score above this threshold were predicted to interact with high probability. This approach is common in binary classification tasks where it is more important to accurately predict one class (interacting pairs, in this case). It allows for better control over the trade-off between precision and recall, ensuring that predictions made are reliable and biologically significant.<fig id="fig0020"><label>Fig. 4</label><caption><p>MLP architecture used to train the host-pathogen PPIs models.</p></caption><alt-text id="at0020">Fig. 4</alt-text><graphic xlink:href="gr4" id="lk0020"/></fig></p>
      <p id="p0235">Of significant importance, our study goes beyond the realm of predicting the likelihood of positive interactions. Instead, our model's implementation serves as an expansive exploration, unveiling hidden interactions that might not have been initially evident. This dynamic capability underscores the inherent value of our approach, which extends beyond the validation of established interactions to the prospect of uncovering novel, previously unexplored interactions. These newfound interactions, accompanied by calculated likelihoods, not only enrich our comprehension but also stimulate a more profound exploration of potential molecular dialogues within the intricate domain of host-pathogen interactions. In conclusion, through rigorous evaluation and thoughtful application, the MLP model trained on Eigenvector Centrality features proved to be a powerful tool for predicting host-pathogen interactions. This methodological approach could be useful for future research in this field, particularly for studies seeking to understand complex host-pathogen dynamics.</p>
    </sec>
    <sec id="sec0100">
      <label>3.5</label>
      <title>Developing an interactive R shiny application</title>
      <p id="p0240">Building upon the foundations of our approach, we have successfully translated our methodology into an interactive and user-friendly tool: R Shiny application named Deep-HPI-pred. This innovative platform empowers users to seamlessly engage with our predictive models and gain insights into host-pathogen interactions. By providing an intuitive interface and leveraging advanced deep learning models, Deep-HPI-pred offers an accessible means for researchers to explore, predict, and understand the intricate dynamics of PPIs between hosts and pathogens. This application acts as a bridge, converting intricate computational methodologies into a user-friendly tool that catalyzes the advancement of host-pathogen interaction studies. This application is distinctive in the sense that it allows users to either manually or automatically upload the training dataset, facilitating a more streamlined operation (<xref rid="fig0025" ref-type="fig">Fig. 5</xref>). Furthermore, it is engineered to predict PPIs leveraging the topological features of the proteins, which is a significant step forward given that traditional prediction methods often lack this level of sophistication.<fig id="fig0025"><label>Fig. 5</label><caption><p>(<bold>A</bold>) User interface of Deep-HPI-pred (<bold>B</bold>) Deep-HPI-pred Interface showcasing data upload functionality (<bold>C</bold>) Result’s tab of Deep-HPI-pred. The blue color nodes within network indicate the Host-proteins while red color nodes represented the pathogen proteins (<bold>D</bold>) The probability of interactions among host and proteins are presented in form of table (<bold>E</bold>) The 'GO Analysis' Tab in Deep-HPI-pred demonstrating the visualization of a network graph and (<bold>F)</bold> the corresponding Gene Ontology (GO) Enrichment Analysis table.</p></caption><alt-text id="at0025">Fig. 5</alt-text><graphic xlink:href="gr5" id="lk0025"/></fig></p>
      <p id="p0245">What makes Deep-HPI-pred stand out is its ability to present intricate host-pathogen protein interactions and the corresponding probability scores in a simplified, user-friendly interface. This aspect bridges the gap between complex data analysis and its interpretability, thereby promoting a broader comprehension and accessibility of the information. Notably, Deep-HPI-pred is the first-ever application of its kind dedicated to the network-based classification and prediction of host-pathogen protein interactions. This development marks a significant milestone in bioinformatics and opens new opportunities for researchers to delve deeper into the dynamics of biological interactions. The application of deep learning models combined with topological analysis presents a novel approach to study and predict PPIs. This not only broadens the scope of current research methodologies but also paves the way for future advancements in this rapidly evolving field of study. By fostering a deeper understanding of pathogenesis and disease progression, Deep-HPI-pred contributes to the advancement of the scientific community's collective knowledge. It demonstrates the potential of integrating machine learning with biological data analysis, thereby setting a precedent for future research in this direction.</p>
      <sec id="sec0105">
        <label>3.5.1</label>
        <title>Interface and data input in the deep-HPI-pred application</title>
        <p id="p0250">Upon initiation of the Deep-HPI-pred application, the user is presented with a straightforward interface, deliberately designed for easy navigation and usability (<xref rid="fig0025" ref-type="fig">Fig. 5</xref>
<bold>(A)</bold>). A feature of the application allows users to upload their specific data files in CSV format through a conveniently located sidebar. In contrast, there is also an option to use pre-existing demo data. This flexibility caters to the distinct requirements of individual research, providing a tailored experience for interaction predictions. Moreover, the incorporation of demo data serves as an effective tool for users to familiarize themselves with the application's functionality swiftly.</p>
      </sec>
      <sec id="sec0110">
        <label>3.5.2</label>
        <title>Implementing training and prediction with deep learning models</title>
        <p id="p0255">After completion of the data upload process, the application prompts the "Train and Predict" feature (<xref rid="fig0025" ref-type="fig">Fig. 5</xref><bold>(B, C)</bold>). This launches the deep learning model previously trained on an extensive dataset of host-pathogen protein interactions. The model performs a comprehensive analysis of the uploaded data to accurately predict potential interactions. This application feature transforms the data input into valuable insights, opening avenues for the elucidation of complex host-pathogen interaction dynamics.</p>
      </sec>
      <sec id="sec0115">
        <label>3.5.3</label>
        <title>Visualizing and interpreting predicted interactions</title>
        <p id="p0260">The next phase of interaction with the application is through the "Results" tab, which is designed for data interpretation and visualization (<xref rid="fig0025" ref-type="fig">Fig. 5</xref><bold>(D)</bold>). The tab displays predicted interaction data and a network visualization of these interactions. The visual depiction facilitates better comprehension of the interaction networks and is particularly valuable in understanding complex interaction dynamics.</p>
      </sec>
      <sec id="sec0120">
        <label>3.5.4</label>
        <title>Conducting GO enrichment analysis for enhanced biological comprehension</title>
        <p id="p0265">The final component of the Deep-HPI-pred application, the "GO Analysis" tab, provides an effective tool for conducting GO enrichment analysis (<xref rid="fig0025" ref-type="fig">Fig. 5</xref><bold>(E, F)</bold>). Unlike other tools, which often lack the provision of GO terms and related analyses, Deep-HPI-pred fills this critical gap. The resulting Network Graph visualizes the outcomes of the GO analysis, offering an intuitive representation of the enriched terms. By collating and categorizing gene products based on shared GO annotations, the application presents an understanding of the biological implications of the predicted interactions. Moreover, to facilitate further in-depth investigations, researchers can conveniently download both the GO table and the corresponding plot, enhancing the tool's utility for advanced research endeavors.</p>
        <p id="p0270">Thus, the development and implementation of the Deep-HPI-pred R Shiny application constitutes a significant milestone in host-pathogen interaction studies. Its ability to integrate deep learning models, employ diverse protein interaction data, and provide interactive and comprehensible output in the form of network visualizations and GO enrichment analysis, creates a uniquely accessible platform for researchers across multiple disciplines. The application, by transforming raw data into insightful knowledge, effectively supports the intricate process of deciphering the complexities inherent in host-pathogen interactions.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec0125">
    <label>4</label>
    <title>Benchmarking</title>
    <p id="p0275">In this study, the versatility and effectiveness of the Deep-HPI-pred applet were demonstrated in accurately predicting PPIs across a range of biological contexts. PPIs were analyzed across three distinct host-pathogen pairs, each representing a unique interplay between different types of organisms - bacteria, plants, and animals. The dataset of experimentally verified HPI interactions was curated from existing literature, and then classified into distinct categories: plant-bacteria, plant-fungi, animal-bacteria <xref rid="bib14" ref-type="bibr">[14]</xref>, and human-virus.</p>
    <sec id="sec0130">
      <label>4.1</label>
      <title><italic>Mus musculus</italic> and <italic>Burkholderia mallei</italic></title>
      <p id="p0280"><italic>Burkholderia mallei</italic> is known to cause glanders, a rare infectious disease that affects horses, mules, and donkeys. It can also infect humans and is considered a potential bioterrorism agent. In this study, our Deep-HPI-pred model achieved an accuracy of 96.6%, sensitivity of 94.9%, specificity of 87.3%, F1 score of 88.7%, precision of 81.4%, MCC of 90.9%, and AUC of 95.1% in predicting PPIs between <italic>M. musculus</italic> (mouse) and <italic>B. mallei</italic>. This highlights the model's potential in assisting researchers in understanding the intricate interplay between these two organisms and could provide valuable insights into the mechanism of glanders infection in various species.</p>
    </sec>
    <sec id="sec0135">
      <label>4.2</label>
      <title><italic>Arabidopsis thaliana</italic> and <italic>Golovinomyces orontii</italic></title>
      <p id="p0285"><italic>Golovinomyces orontii</italic> is known to cause powdery mildew in <italic>A. thaliana</italic>, a model organism in plant biology. Accurate prediction of PPIs is crucial for deciphering the molecular basis of plant-pathogen interactions and developing strategies for disease resistance. Our Deep-HPI-pred model demonstrated high accuracy in predicting these interactions, with an accuracy of 98.4%, sensitivity of 89.3%, specificity of 93.2%, F1 score of 95%, precision of 92.4%, MCC of 93.3%, and AUC of 92.8%.</p>
    </sec>
    <sec id="sec0140">
      <label>4.3</label>
      <title><italic>Arabidopsis thaliana</italic> and <italic>Pseudomonas syringae</italic></title>
      <p id="p0290"><italic>P. syringae</italic> is a well-known pathogenic bacterium that infects a wide range of plant species, causing bacterial speck disease in <italic>A. thaliana</italic>. Understanding PPIs between these two species is vital for developing new methods of disease control. Our Deep-HPI-pred model achieved an accuracy of 97.9%, sensitivity of 95%, specificity of 93.7%, F1 score of 91.8%, precision of 92.4%, MCC of 94.8%, and AUC of 98.2% in predicting the PPIs between <italic>A. thaliana</italic> and <italic>P. syringae</italic>.</p>
    </sec>
    <sec id="sec0145">
      <label>4.4</label>
      <title><italic>Homo sapiens</italic> and SARS-Cov-2</title>
      <p id="p0295">Additionally, we also evaluated our model on the host-pathogen interactions between <italic>H. sapiens</italic> (human) and SARS-CoV-2 (virus), a critically important relationship in light of the recent global pandemic. Given the recent global impact of the COVID-19 pandemic, understanding the interactions between human proteins and SARS-CoV-2 viral proteins is of paramount importance. In our analysis of the interactions between <italic>H. sapiens</italic> and SARS-CoV-2, the Deep-HPI-pred model demonstrated its proficiency in capturing the complex relationships characteristic of host-pathogen interactions. Given the recent global impact of the COVID-19 pandemic, understanding the interactions between human proteins and SARS-CoV-2 viral proteins is of paramount importance. Our Deep-HPI-pred model showed excellent performance in predicting the interactions between human proteins and SARS-CoV-2 viral proteins, with an accuracy of 94.3%, sensitivity of 97.5%, specificity of 98.7%, F1 score of 96.5%, precision of 96.2%, MCC of 95.3%, and AUC of 97.8%. These results demonstrate the strong predictive power of our Deep-HPI-pred model in deciphering the intricate network of interactions that take place between human proteins and SARS-CoV-2 viral proteins.</p>
      <p id="p0300">The finding revealed potential interactions between SARS-CoV-2 proteins and human host factors, which may play a pivotal role in the viral life cycle, including entry, replication, and evasion of host immune defenses. Notably, the model predicts that the SARS-CoV-2 Envelope protein interact with human proteins involved in the endocytic pathway, a route well-documented for coronavirus entry into host cells <xref rid="bib42" ref-type="bibr">[42]</xref>. Such interactions could potentially facilitate viral entry by altering endosomal trafficking. Additionally, interactions have been predicted between the SARS-CoV-2 Membrane glycoprotein and proteins located in the ER-to-Golgi intermediate compartment (ERGIC), which is instrumental in viral assembly and trafficking <xref rid="bib43" ref-type="bibr">[43]</xref>. These predictions suggest a mechanism by which SARS-CoV-2 could hijack host cellular machinery to bolster its replication process. Further results also suggest potential interactions between the ORF1a polyprotein of SARS-CoV-2 and components of the human innate immune system, particularly proteins involved in interferon signaling. This aligns with findings such as those from McClainet al. <xref rid="bib44" ref-type="bibr">[44]</xref>, which indicate the virus's ability to modulate interferon-driven responses, a strategy that likely facilitates viral replication and contributes to pathogenesis. An extensive literature review was undertaken to provide context for each predicted interaction. While direct evidence of these specific interactions has not been previously documented, the literature corroborates the biological plausibility of the proposed mechanisms. For example, Hekman et al. <xref rid="bib45" ref-type="bibr">[45]</xref> demonstrated that the SARS-CoV-2 Nucleoprotein's interaction with host RNA processing bodies suggests a potential for viral modulation of host mRNA processing, an avenue supported by recent findings. Thus, our Deep-HPI-pred model has demonstrated a high level of accuracy and reliability in predicting host-pathogen interactions across diverse biological contexts, underscoring its potential as a valuable tool for researchers studying infectious diseases and host-pathogen interactions.</p>
      <p id="p0305">Additionally, in the comparative analysis of Deep-HPI-pred with existing servers such as Pred-HPI and DeepHPI, several key differences emerge that highlight the enhanced capabilities of Deep-HPI-pred. Specifically, Deep-HPI-pred has predicted a total of 9673 interactions between SARS-CoV-2 virus and human proteins (<bold>Supplementary File 3:</bold><xref rid="sec0175" ref-type="sec">Table S1</xref><bold>)</bold>. In contrast, PredHPI, focuses primarily on sequence-based methods for detecting host-pathogen interactions, has predicted 6654 interactions (<bold>Supplementary File 3:</bold>
<xref rid="sec0175" ref-type="sec">Table S2</xref><bold>)</bold>, and notably, these predictions do not include probability information for each interaction. The higher number of interactions predicted by Deep-HPI-pred suggests its increased sensitivity in detecting potential host-pathogen interactions, which is crucial for comprehensive understanding and exploration of SARS-CoV-2 pathogenesis. Moreover, Deep-HPI-pred provides probability scores for each predicted interaction, offering a quantifiable measure of confidence in the predictions. This feature is particularly valuable for researchers, as it allows prioritization of interactions based on their likelihood, facilitating targeted experimental validation. Such probability information is absent in the predictions made by Pred-HPI and DeepHPI, which limits the ability to assess the confidence level in each predicted interaction. Additionally, Deep-HPI-pred enhances the utility of its predictions by providing GO information. This inclusion allows for immediate biological interpretation of the interactions, offering insights into potential biological processes and molecular functions involved, thereby enriching the understanding of the interaction landscape. In comparison, Deep-HPI, another existing tool in this domain, does not provide specific interaction results in a format that is directly comparable to Deep-HPI-pred and Pred-HPI. Therefore, the comparison focuses on the methodological approach and the theoretical framework of these models.</p>
      <p id="p0310">In the similar vein, GreeningDB, another host-pathogen interaction database, offers a more specialized scope, focusing specifically on citrus greening disease (Huanglongbing or HLB). It compiles data primarily relevant to HLB, including genomic, transcriptomic, and proteomic information. While invaluable for HLB research, GreeningDB's utility is confined to this particular disease. Deep-HPI-pred, however, extends its applicability beyond a singular disease context, enabling broader investigations of HPIs across multiple biological systems. This key distinction underscores Deep-HPI-pred's potential as a versatile platform for a more generalized understanding of host-pathogen dynamics, applicable to a diverse range of infectious diseases.</p>
      <p id="p0315">Another recent study by Yang et al. <xref rid="bib46" ref-type="bibr">[46]</xref> employed a transfer learning approach using multi-scale convolutional neural layers to predict human-virus PPIs. While this approach effectively captures the complex features of protein sequences, Deep-HPI-pred extends beyond sequence-based predictions. It integrates topological features and GO information, offering a more holistic view of the interaction landscape. Additionally, Deep-HPI-pred's application is not limited to human-virus interactions but encompasses a broader range of host-pathogen systems, demonstrating its versatile and comprehensive predictive capabilities. Similarly, in comparison with DeepViral <xref rid="bib47" ref-type="bibr">[47]</xref>, a deep learning-based method for predicting novel virus-host interactions from protein sequences and infectious disease phenotypes, Deep-HPI-pred showcases distinct advantages. DeepViral focuses on novel virus-host interaction predictions using protein sequences and disease phenotypes, leveraging a deep learning approach. While DeepViral's integration of infectious disease phenotypes offers a unique perspective, Deep-HPI-pred's methodology is distinguished by its utilization of MLP models based on a comprehensive evaluation of topological features and neural network architectures. This not only enhances the prediction accuracy but also provides a more detailed understanding of the underlying protein interaction mechanisms. Furthermore, Deep-HPI-pred's consistent performance across diverse host-pathogen systems, including plant-pathogens, human-viruses, and animal-bacteria, as evidenced by its high accuracy rates, illustrates its robustness and adaptability in various biological contexts.</p>
      <p id="p0320">To sum up, Deep-HPI-pred stands out in its ability to not only predict a larger number of host-pathogen interactions but also to provide critical additional information like probability scores and GO annotations. These features significantly contribute to its utility as a research tool, offering a more nuanced and informed approach to exploring host-pathogen interactions compared to tools like Pred-HPI, Deep-HPI, and DeepViral. The advanced algorithms utilized by Deep-HPI-pred enable the analysis of extensive interaction datasets, highlighting proteins that are central and often critical in these processes. This analytical capability is instrumental in deepening our understanding of the molecular mechanisms of viral infections, as supported by studies like those conducted by Barman et al. <xref rid="bib48" ref-type="bibr">[48]</xref>, which employed state-of-art techniques to identify key viral interaction proteins. Furthermore, the fusion of biological network analysis with deep learning heralds a transformative era in clinical and personalized medicine, particularly in managing viral diseases. This integration enables the unraveling of complex host-pathogen interactions at a molecular level, paving the way for targeted therapeutic strategies and more individualized treatment approaches in combating viral infections.</p>
    </sec>
  </sec>
  <sec id="sec0150">
    <label>5</label>
    <title>Conclusion</title>
    <p id="p0325">Traditional experimental techniques for host-pathogen interaction prediction, though effective, have proven to be labor-intensive, expensive, and time-consuming. To address this challenge, we have introduced Deep-HPI-pred, an R/Shiny application that provides a computational approach for predicting hitherto unmapped interactions between host and pathogen proteins. By harnessing the power of network-driven feature learning, Deep-HPI-pred, as demonstrated through our case study using citrus and <italic>C</italic>Las bacteria training sets, offers a promising alternative for accelerating the discovery of PPIs. In our research, we employed a comprehensive evaluation of various neural network architectures and topological features, the results of which led us to adopt the MLP models for HPI prediction. Notably, the MLP model using the Eigenvector Centrality topological feature exhibited exemplary performance, achieving an overall MCC value exceeding 0.80 when tested on independent validation datasets. Beyond its capacity for interaction prediction, Deep-HPI-pred further enriches our understanding of the dynamics within host-pathogen interactions by providing GO term information for each protein. This added layer of information presents an insightful view of the system and enhances our comprehension of the overall biological processes at play. Furthermore, in the benchmarking studies conducted, the Deep-HPI-pred model demonstrated its robustness and reliability across various host-pathogen systems. The model exhibited a remarkable performance in predicting interactions between different host-pathogen pairs, including plant-virus, human-virus, plant-bacteria, and animal-bacteria. Specifically, the model achieved an accuracy of 98.4% and 97.9% for plant-pathogen interactions, 94.3% for human-virus interactions, and 96.6% for animal-bacteria interactions. These results not only validate the efficacy of our model but also highlight its potential as a versatile and comprehensive tool for understanding the complex dynamics of host-pathogen interactions across different biological systems. While MLPs have demonstrated robust performance in this study, their structure is not inherently optimized for contrastive learning, which is increasingly recognized for its efficacy in unsupervised and semi-supervised scenarios. This limitation suggests a potential area for future improvement of the model, where integrating contrastive learning techniques could expand its capabilities to handle and learn from the vast amounts of unlabeled data in biological research. Such enhancements would not only address a key limitation but also significantly enrich the model’s utility in understanding and predicting complex host-pathogen interactions. In conclusion, the introduction of Deep-HPI-pred represents a significant stride in the field of bioinformatics. By integrating detection and visualization of interaction networks into a single user-friendly platform, it equips researchers with a powerful tool for understanding both model and non-model host-pathogen systems. This advancement is expected to aid in the generation of hypotheses, the design of appropriate experiments, and ultimately, in the development of disease control and prevention strategies.</p>
  </sec>
  <sec sec-type="data-availability" id="sec0155">
    <title>Availability of data and materials</title>
    <p id="p0330">Project name: Deep-HPI-pred. Project home page: <ext-link ext-link-type="uri" xlink:href="https://cbi.gxu.edu.cn/shiny-apps/Deep-HPI-pred/" id="ir0015">https://cbi.gxu.edu.cn/shiny-apps/Deep-HPI-pred/</ext-link>. Programming language: R and HTML. The resources and data used during the current study are available in the GitHub repository, <ext-link ext-link-type="uri" xlink:href="https://github.com/tahirulqamar/Deep-HPI-pred" id="ir0020">https://github.com/tahirulqamar/Deep-HPI-pred</ext-link>.</p>
  </sec>
  <sec id="sec0160">
    <title>Funding</title>
    <p id="p0335">This work was supported by the Starting Research Grant for <funding-source id="gs1">High-level Talents from Guangxi University</funding-source> and <funding-source id="gs2">Postdoctoral research platform grant of Guangxi University</funding-source>.</p>
  </sec>
  <sec id="sec0165">
    <title>CRediT authorship contribution statement</title>
    <p id="p0340"><bold>Muhammad Tahir ul Qamar and Fatima Noor</bold>: Data Curation, Methodology, Software, Formal Analysis, Investigation, Visualization, Writing - Original Draft. <bold>Yi-Xiong Guo and Xi-Tong Zhu</bold>: Software, Validation, Writing - Review &amp; Editing. <bold>Ling-Ling Chen</bold>: Conceptualization, Resources, Supervision, Project administration, Funding acquisition, Writing - Review &amp; Editing.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="p0345">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="bibliog0005">
    <title>References</title>
    <ref id="bib1">
      <label>1</label>
      <element-citation publication-type="journal" id="sbref1">
        <person-group person-group-type="author">
          <name>
            <surname>Kuo</surname>
            <given-names>Z.-Y.</given-names>
          </name>
          <name>
            <surname>Chuang</surname>
            <given-names>Y.-J.</given-names>
          </name>
          <name>
            <surname>Chao</surname>
            <given-names>C.-C.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>F.-C.</given-names>
          </name>
          <name>
            <surname>Lan</surname>
            <given-names>C.-Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>B.-S.</given-names>
          </name>
        </person-group>
        <article-title>Identification of infection-and defense-related genes via a dynamic host-pathogen interaction network using a Candida albicans-zebrafish infection model</article-title>
        <source>J innate Immun</source>
        <volume>5</volume>
        <issue>2</issue>
        <year>2013</year>
        <fpage>137</fpage>
        <lpage>152</lpage>
        <pub-id pub-id-type="pmid">23406717</pub-id>
      </element-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="journal" id="sbref2">
        <person-group person-group-type="author">
          <name>
            <surname>Garbutt</surname>
            <given-names>C.C.</given-names>
          </name>
          <name>
            <surname>Bangalore</surname>
            <given-names>P.V.</given-names>
          </name>
          <name>
            <surname>Kannar</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Mukhtar</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Getting to the edge: protein dynamical networks as a new frontier in plant–microbe interactions</article-title>
        <source>Front Plant Sci</source>
        <volume>5</volume>
        <year>2014</year>
        <fpage>312</fpage>
        <pub-id pub-id-type="pmid">25071795</pub-id>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <mixed-citation publication-type="other" id="othref0005">J.J. Da Graça, L.Korsten. Citrus huanglongbing: Review, present status and future strategies. In: Naqvi SAMH, editor. Diseases of Fruits and Vegetables Volume I: Diagnosis and Management. The Netherlands: Kluwer Academic; (2004). pp. 229–45.</mixed-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="journal" id="sbref3">
        <person-group person-group-type="author">
          <name>
            <surname>Andrade</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>Candidatus Liberibacter asiaticus: virulence traits and control strategies</article-title>
        <source>Trop Plant Pathol</source>
        <volume>45</volume>
        <year>2020</year>
        <fpage>285</fpage>
        <lpage>297</lpage>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="journal" id="sbref4">
        <person-group person-group-type="author">
          <name>
            <surname>Pandey</surname>
            <given-names>S.S.</given-names>
          </name>
          <name>
            <surname>Hendrich</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Andrade</surname>
            <given-names>M.O.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>Candidatus Liberibacter: From movement, host responses, to symptom development of citrus Huanglongbing</article-title>
        <source>Phytopathology®</source>
        <volume>112</volume>
        <issue>1</issue>
        <year>2022</year>
        <fpage>55</fpage>
        <lpage>68</lpage>
        <pub-id pub-id-type="pmid">34609203</pub-id>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="book" id="sbref5">
        <person-group person-group-type="author">
          <name>
            <surname>Hoddle</surname>
            <given-names>M.S.</given-names>
          </name>
          <name>
            <surname>Hoddle</surname>
            <given-names>C.D.</given-names>
          </name>
          <name>
            <surname>Morgan</surname>
            <given-names>D.J.</given-names>
          </name>
          <name>
            <surname>Milosavljević</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <part-title>Successful Biological Control of Asian Citrus Psyllid, Diaphorina citri, in California</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Van Driesche</surname>
            <given-names>R.G.</given-names>
          </name>
          <name>
            <surname>Winston</surname>
            <given-names>R.L.</given-names>
          </name>
          <name>
            <surname>Perring</surname>
            <given-names>T.M.</given-names>
          </name>
          <name>
            <surname>Lopez</surname>
            <given-names>V.M.</given-names>
          </name>
        </person-group>
        <source>Contributions of Classical Biological Control to the US Food Security</source>
        <year>2022</year>
        <publisher-name>Forestry, and Biodiversity. Washington</publisher-name>
        <publisher-loc>USDA FHAAST</publisher-loc>
        <fpage>127</fpage>
        <lpage>145</lpage>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="journal" id="sbref6">
        <person-group person-group-type="author">
          <name>
            <surname>Yuan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Xin</surname>
            <given-names>X.F.</given-names>
          </name>
        </person-group>
        <article-title>Bacterial Infection and Hypersensitive Response Assays in Arabidopsis-Pseudomonas syringae Pathosystem</article-title>
        <source>Bio Protoc</source>
        <volume>11</volume>
        <issue>24</issue>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">e4268</object-id>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="journal" id="sbref7">
        <person-group person-group-type="author">
          <name>
            <surname>Dyer</surname>
            <given-names>M.D.</given-names>
          </name>
          <name>
            <surname>Neff</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Dufford</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Rivera</surname>
            <given-names>C.G.</given-names>
          </name>
          <name>
            <surname>Shattuck</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Bassaganya-Riera</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The human-bacterial pathogen protein interaction networks of Bacillus anthracis, Francisella tularensis, and Yersinia pestis</article-title>
        <source>PloS One</source>
        <volume>5</volume>
        <issue>8</issue>
        <year>2010</year>
        <object-id pub-id-type="publisher-id">e12089</object-id>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="journal" id="sbref8">
        <person-group person-group-type="author">
          <name>
            <surname>Lian</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Current status and future perspectives of computational studies on human–virus protein–protein interactions</article-title>
        <source>Brief Bioinforma</source>
        <volume>22</volume>
        <issue>5</issue>
        <year>2021</year>
        <fpage>bbab029</fpage>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="journal" id="sbref9">
        <person-group person-group-type="author">
          <name>
            <surname>Westermann</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Cross-species</surname>
            <given-names>J.Vogel</given-names>
          </name>
        </person-group>
        <article-title>RNA-seq for deciphering host–microbe interactions</article-title>
        <source>Nat Rev Genet</source>
        <volume>22</volume>
        <issue>6</issue>
        <year>2021</year>
        <fpage>361</fpage>
        <lpage>378</lpage>
        <pub-id pub-id-type="pmid">33597744</pub-id>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="journal" id="sbref10">
        <person-group person-group-type="author">
          <name>
            <surname>Balotf</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Tegg</surname>
            <given-names>R.S.</given-names>
          </name>
          <name>
            <surname>Nichols</surname>
            <given-names>D.S.</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>C.R.</given-names>
          </name>
        </person-group>
        <article-title>Shotgun proteomics as a powerful tool for the study of the proteomes of plants, their pathogens, and plant–pathogen interactions</article-title>
        <source>Proteomes</source>
        <volume>10</volume>
        <issue>1</issue>
        <year>2022</year>
        <fpage>5</fpage>
        <pub-id pub-id-type="pmid">35225985</pub-id>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="journal" id="sbref11">
        <person-group person-group-type="author">
          <name>
            <surname>Jain</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mittal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Tripathi</surname>
            <given-names>L.P.</given-names>
          </name>
          <name>
            <surname>Nussinov</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Ahmad</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Host-pathogen protein-nucleic acid interactions: A comprehensive review</article-title>
        <source>Comput Struct Biotechnol J</source>
        <year>2022</year>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="journal" id="sbref12">
        <person-group person-group-type="author">
          <name>
            <surname>Kaundal</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Loaiza</surname>
            <given-names>C.D.</given-names>
          </name>
          <name>
            <surname>Duhan</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Flann</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>deepHPI: a comprehensive deep learning platform for accurate prediction and visualization of host–pathogen protein–protein interactions</article-title>
        <source>Brief Bioinforma</source>
        <volume>23</volume>
        <issue>3</issue>
        <year>2022</year>
        <fpage>bbac125</fpage>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>14</label>
      <element-citation publication-type="journal" id="sbref13">
        <person-group person-group-type="author">
          <name>
            <surname>Loaiza</surname>
            <given-names>C.D.</given-names>
          </name>
          <name>
            <surname>Kaundal</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>PredHPI: an integrated web server platform for the detection and visualization of host–pathogen interactions using sequence-based methods</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <issue>5</issue>
        <year>2021</year>
        <fpage>622</fpage>
        <lpage>624</lpage>
        <pub-id pub-id-type="pmid">33027504</pub-id>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>15</label>
      <element-citation publication-type="journal" id="sbref14">
        <person-group person-group-type="author">
          <name>
            <surname>Loaiza</surname>
            <given-names>C.D.</given-names>
          </name>
          <name>
            <surname>Duhan</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Kaundal</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>GreeningDB: A Database of Host–Pathogen Protein–Protein Interactions and Annotation Features of the Bacteria Causing Huanglongbing HLB Disease</article-title>
        <source>Int J Mol Sci</source>
        <volume>22</volume>
        <issue>19</issue>
        <year>2021</year>
        <fpage>10897</fpage>
        <pub-id pub-id-type="pmid">34639237</pub-id>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>16</label>
      <element-citation publication-type="journal" id="sbref15">
        <person-group person-group-type="author">
          <name>
            <surname>Singhal</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Resat</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>A domain-based approach to predict protein-protein interactions</article-title>
        <source>BMC Bioinform</source>
        <volume>8</volume>
        <year>2007</year>
        <fpage>1</fpage>
        <lpage>19</lpage>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>17</label>
      <element-citation publication-type="journal" id="sbref16">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Chi</surname>
            <given-names>C.-H.</given-names>
          </name>
          <name>
            <surname>Kurgan</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Systematic evaluation of machine learning methods for identifying human–pathogen protein–protein interactions</article-title>
        <source>Brief Bioinforma</source>
        <volume>22</volume>
        <issue>3</issue>
        <year>2021</year>
        <fpage>bbaa068</fpage>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>18</label>
      <element-citation publication-type="journal" id="sbref17">
        <person-group person-group-type="author">
          <name>
            <surname>Scott</surname>
            <given-names>M.S.</given-names>
          </name>
          <name>
            <surname>Probabilistic</surname>
            <given-names>G.J. Barton</given-names>
          </name>
        </person-group>
        <article-title>prediction and ranking of human protein-protein interactions</article-title>
        <source>BMC Bioinform</source>
        <volume>8</volume>
        <issue>1</issue>
        <year>2007</year>
        <fpage>1</fpage>
        <lpage>21</lpage>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>19</label>
      <element-citation publication-type="journal" id="sbref18">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>Y.D.</given-names>
          </name>
        </person-group>
        <article-title>Protein‐protein interaction networks as miners of biological discovery</article-title>
        <source>Proteomics</source>
        <volume>22</volume>
        <issue>15-16</issue>
        <year>2022</year>
        <fpage>2100190</fpage>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>20</label>
      <element-citation publication-type="journal" id="sbref19">
        <person-group person-group-type="author">
          <name>
            <surname>Csardi</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Nepusz</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>The igraph software package for complex network research</article-title>
        <source>Inter, Complex Syst</source>
        <volume>1695</volume>
        <issue>5</issue>
        <year>2006</year>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>21</label>
      <element-citation publication-type="journal" id="sbref20">
        <person-group person-group-type="author">
          <name>
            <surname>Pržulj</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Wigle</surname>
            <given-names>D.A.</given-names>
          </name>
          <name>
            <surname>Jurisica</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <article-title>Functional topology in a network of protein interactions</article-title>
        <source>Bioinformatics</source>
        <volume>20</volume>
        <issue>3</issue>
        <year>2004</year>
        <fpage>340</fpage>
        <lpage>348</lpage>
        <pub-id pub-id-type="pmid">14960460</pub-id>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>22</label>
      <element-citation publication-type="journal" id="sbref21">
        <person-group person-group-type="author">
          <name>
            <surname>Ashtiani</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Salehzadeh-Yazdi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Razaghi-Moghadam</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Hennig</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Wolkenhauer</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Mirzaie</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Jafari</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>A systematic survey of centrality measures for protein-protein interaction networks. BMC Sys</article-title>
        <source>Biol</source>
        <volume>12</volume>
        <issue>1</issue>
        <year>2018</year>
        <fpage>1</fpage>
        <lpage>17</lpage>
      </element-citation>
    </ref>
    <ref id="bib23">
      <label>23</label>
      <element-citation publication-type="journal" id="sbref22">
        <person-group person-group-type="author">
          <name>
            <surname>Eryilmaz</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Pax</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>O’Neill</surname>
            <given-names>A.G.</given-names>
          </name>
          <name>
            <surname>Vangel</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Diez</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Holt</surname>
            <given-names>D.J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Network hub centrality and working memory performance in schizophrenia</article-title>
        <source>Schizophrenia</source>
        <volume>8</volume>
        <issue>1</issue>
        <year>2022</year>
        <object-id pub-id-type="publisher-id">76</object-id>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>24</label>
      <mixed-citation publication-type="other" id="othref0010">A. Ali, V.R. Hulipalled, S. Patil, editors. Centrality measure analysis on protein interaction networks. 2020 IEEE International Conference on Technology, Engineering, Management for Societal impact using Marketing, Entrepreneurship and Talent (TEMSMET); 2020: IEEE.</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25</label>
      <mixed-citation publication-type="other" id="othref0015">V. Chaubey, M.S. Nair, G.N. Pillai, editors. Gene expression prediction using a deep 1D convolution neural network. 2019 IEEE Symposium Series on Computational Intelligence (SSCI); 2019: IEEE.</mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26</label>
      <element-citation publication-type="journal" id="sbref23">
        <person-group person-group-type="author">
          <name>
            <surname>Patiyal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Dhall</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Raghava</surname>
            <given-names>G.P.</given-names>
          </name>
        </person-group>
        <article-title>A deep learning-based method for the prediction of DNA interacting residues in a protein</article-title>
        <source>Brief Bioinforma</source>
        <volume>23</volume>
        <issue>5</issue>
        <year>2022</year>
        <fpage>bbac322</fpage>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>27</label>
      <element-citation publication-type="journal" id="sbref24">
        <person-group person-group-type="author">
          <name>
            <surname>Fang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Moriwaki</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Shimizu</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of antifungal peptides by deep learning with character embedding</article-title>
        <source>IPSJ Trans Bioinforma</source>
        <volume>12</volume>
        <year>2019</year>
        <fpage>21</fpage>
        <lpage>29</lpage>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>28</label>
      <element-citation publication-type="journal" id="sbref25">
        <person-group person-group-type="author">
          <name>
            <surname>Arnold</surname>
            <given-names>T.B.</given-names>
          </name>
        </person-group>
        <article-title>kerasR: R Interface to the Keras Deep Learning Library</article-title>
        <source>J Open Source Softw,2</source>
        <issue>14</issue>
        <year>2017</year>
        <fpage>296</fpage>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>29</label>
      <element-citation publication-type="journal" id="sbref26">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lian</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wuchty</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Transfer learning via multi-scale convolutional neural layers for human–virus protein–protein interaction prediction</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <issue>24</issue>
        <year>2021</year>
        <fpage>4771</fpage>
        <lpage>4778</lpage>
        <pub-id pub-id-type="pmid">34273146</pub-id>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>30</label>
      <element-citation publication-type="journal" id="sbref27">
        <person-group person-group-type="author">
          <name>
            <surname>Grandini</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bagli</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Visani</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Metrics for multi-class classification: an overview</article-title>
        <comment>200805756</comment>
        <source>arXiv Prepr arXiv</source>
        <year>2020</year>
        <comment>200805756</comment>
      </element-citation>
    </ref>
    <ref id="bib31">
      <label>31</label>
      <element-citation publication-type="journal" id="sbref28">
        <person-group person-group-type="author">
          <name>
            <surname>Powers</surname>
            <given-names>D.M.</given-names>
          </name>
        </person-group>
        <article-title>Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation</article-title>
        <comment>201016061</comment>
        <source>arXiv Prepr arXiv</source>
        <year>2020</year>
        <comment>201016061</comment>
      </element-citation>
    </ref>
    <ref id="bib32">
      <label>32</label>
      <element-citation publication-type="journal" id="sbref29">
        <person-group person-group-type="author">
          <name>
            <surname>Baldi</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Brunak</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Chauvin</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Andersen</surname>
            <given-names>C.A.</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Assessing the accuracy of prediction algorithms for classification: an overview</article-title>
        <source>Bioinformatics</source>
        <volume>16</volume>
        <issue>5</issue>
        <year>2000</year>
        <fpage>412</fpage>
        <lpage>424</lpage>
        <pub-id pub-id-type="pmid">10871264</pub-id>
      </element-citation>
    </ref>
    <ref id="bib33">
      <label>33</label>
      <element-citation publication-type="journal" id="sbref30">
        <person-group person-group-type="author">
          <name>
            <surname>Johnson</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Khoshgoftaar</surname>
            <given-names>T.M.</given-names>
          </name>
        </person-group>
        <article-title>Survey on deep learning with class imbalance</article-title>
        <source>J Big Data</source>
        <volume>6</volume>
        <issue>1</issue>
        <year>2019</year>
        <fpage>1</fpage>
        <lpage>54</lpage>
      </element-citation>
    </ref>
    <ref id="bib34">
      <label>34</label>
      <element-citation publication-type="journal" id="sbref31">
        <person-group person-group-type="author">
          <name>
            <surname>Broadley</surname>
            <given-names>R.W.</given-names>
          </name>
          <name>
            <surname>Klenk</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Thies</surname>
            <given-names>S.B.</given-names>
          </name>
          <name>
            <surname>Kenney</surname>
            <given-names>L.P.</given-names>
          </name>
          <name>
            <surname>Granat</surname>
            <given-names>M.H.</given-names>
          </name>
        </person-group>
        <article-title>Methods for the real-world evaluation of fall detection technology: A scoping review</article-title>
        <source>Sensors</source>
        <volume>18</volume>
        <issue>7</issue>
        <year>2018</year>
        <fpage>2060</fpage>
        <pub-id pub-id-type="pmid">29954155</pub-id>
      </element-citation>
    </ref>
    <ref id="bib35">
      <label>35</label>
      <element-citation publication-type="journal" id="sbref32">
        <person-group person-group-type="author">
          <name>
            <surname>Alves</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Castro</surname>
            <given-names>G.Z.</given-names>
          </name>
          <name>
            <surname>Oliveira</surname>
            <given-names>B.A.S.</given-names>
          </name>
          <name>
            <surname>Ferreira</surname>
            <given-names>L.A.</given-names>
          </name>
          <name>
            <surname>Ramírez</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Silva</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Guimarães</surname>
            <given-names>F.G.</given-names>
          </name>
        </person-group>
        <article-title>Explaining machine learning based diagnosis of COVID-19 from routine blood tests with decision trees and criteria graphs</article-title>
        <source>Comput Biol Med</source>
        <volume>132</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">104335</object-id>
      </element-citation>
    </ref>
    <ref id="bib36">
      <label>36</label>
      <element-citation publication-type="journal" id="sbref33">
        <person-group person-group-type="author">
          <name>
            <surname>Chicco</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Jurman</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation</article-title>
        <source>BMC Genom</source>
        <volume>21</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>13</lpage>
      </element-citation>
    </ref>
    <ref id="bib37">
      <label>37</label>
      <element-citation publication-type="journal" id="sbref34">
        <person-group person-group-type="author">
          <name>
            <surname>Kakkar</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>goyal</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Johri</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Artificial Intelligence-Based Approaches for Detection and Classification of Different Classes of Malaria Parasites Using Microscopic Images: A Systematic Review</article-title>
        <source>Arch Comput Methods Eng</source>
        <year>2023</year>
        <fpage>1</fpage>
        <lpage>20</lpage>
      </element-citation>
    </ref>
    <ref id="bib38">
      <label>38</label>
      <element-citation publication-type="journal" id="sbref35">
        <person-group person-group-type="author">
          <name>
            <surname>Lei</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wan</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A deep-learning framework for multi-level peptide–protein interaction prediction</article-title>
        <source>Nat Commun</source>
        <volume>12</volume>
        <issue>1</issue>
        <year>2021</year>
        <fpage>5465</fpage>
        <pub-id pub-id-type="pmid">34526500</pub-id>
      </element-citation>
    </ref>
    <ref id="bib39">
      <label>39</label>
      <element-citation publication-type="journal" id="sbref36">
        <person-group person-group-type="author">
          <name>
            <surname>Negre</surname>
            <given-names>C.F.</given-names>
          </name>
          <name>
            <surname>Morzan</surname>
            <given-names>U.N.</given-names>
          </name>
          <name>
            <surname>Hendrickson</surname>
            <given-names>H.P.</given-names>
          </name>
          <name>
            <surname>Pal</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lisi</surname>
            <given-names>G.P.</given-names>
          </name>
          <name>
            <surname>Loria</surname>
            <given-names>J.P.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Eigenvector centrality for characterization of protein allosteric pathways</article-title>
        <source>Proc Natl Acad Sci USA</source>
        <volume>115</volume>
        <issue>52</issue>
        <year>2018</year>
        <comment>E12201-E8</comment>
      </element-citation>
    </ref>
    <ref id="bib40">
      <label>40</label>
      <element-citation publication-type="journal" id="sbref37">
        <person-group person-group-type="author">
          <name>
            <surname>Khorsand</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Savadi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Naghibzadeh</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Comprehensive host-pathogen protein-protein interaction network analysis</article-title>
        <source>BMC Bioinform</source>
        <volume>21</volume>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>22</lpage>
      </element-citation>
    </ref>
    <ref id="bib41">
      <label>41</label>
      <element-citation publication-type="journal" id="sbref38">
        <person-group person-group-type="author">
          <name>
            <surname>Cui</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Stanley</surname>
            <given-names>H.E.</given-names>
          </name>
        </person-group>
        <article-title>Discovering disease-associated genes in weighted protein–protein interaction networks</article-title>
        <source>Phys A: Stat Mech its Appl</source>
        <volume>496</volume>
        <year>2018</year>
        <fpage>53</fpage>
        <lpage>61</lpage>
      </element-citation>
    </ref>
    <ref id="bib42">
      <label>42</label>
      <element-citation publication-type="journal" id="sbref39">
        <person-group person-group-type="author">
          <name>
            <surname>Hoffmann</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kleine-Weber</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Schroeder</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Krüger</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Herrler</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Erichsen</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SARS-CoV-2 cell entry depends on ACE2 and TMPRSS2 and is blocked by a clinically proven protease inhibitor</article-title>
        <source>cell</source>
        <volume>181</volume>
        <issue>2</issue>
        <year>2020</year>
        <fpage>271</fpage>
        <lpage>280</lpage>
        <comment>e8</comment>
        <pub-id pub-id-type="pmid">32142651</pub-id>
      </element-citation>
    </ref>
    <ref id="bib43">
      <label>43</label>
      <element-citation publication-type="journal" id="sbref40">
        <person-group person-group-type="author">
          <name>
            <surname>Cortese</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Laketa</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>Advanced microscopy technologies enable rapid response to SARS‐CoV‐2 pandemic</article-title>
        <source>Cell Microbiol</source>
        <volume>23</volume>
        <issue>7</issue>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">e13319</object-id>
      </element-citation>
    </ref>
    <ref id="bib44">
      <label>44</label>
      <element-citation publication-type="journal" id="sbref41">
        <person-group person-group-type="author">
          <name>
            <surname>McClain</surname>
            <given-names>M.T.</given-names>
          </name>
          <name>
            <surname>Constantine</surname>
            <given-names>F.J.</given-names>
          </name>
          <name>
            <surname>Henao</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Tsalik</surname>
            <given-names>E.L.</given-names>
          </name>
          <name>
            <surname>Burke</surname>
            <given-names>T.W.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Dysregulated transcriptional responses to SARS-CoV-2 in the periphery</article-title>
        <source>Nat Commun</source>
        <volume>12</volume>
        <issue>1</issue>
        <year>2021</year>
        <fpage>1079</fpage>
        <pub-id pub-id-type="pmid">33597532</pub-id>
      </element-citation>
    </ref>
    <ref id="bib45">
      <label>45</label>
      <element-citation publication-type="journal" id="sbref42">
        <person-group person-group-type="author">
          <name>
            <surname>Hekman</surname>
            <given-names>R.M.</given-names>
          </name>
          <name>
            <surname>Hume</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Goel</surname>
            <given-names>R.K.</given-names>
          </name>
          <name>
            <surname>Abo</surname>
            <given-names>K.M.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Blum</surname>
            <given-names>B.C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Actionable cytopathogenic host responses of human alveolar type 2 cells to SARS-CoV-2</article-title>
        <source>Mol Cell</source>
        <volume>80</volume>
        <issue>6</issue>
        <year>2020</year>
        <fpage>1104</fpage>
        <lpage>1122</lpage>
        <comment>e9</comment>
        <pub-id pub-id-type="pmid">33259812</pub-id>
      </element-citation>
    </ref>
    <ref id="bib46">
      <label>46</label>
      <element-citation publication-type="journal" id="sbref43">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lian</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wuchty</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Transfer learning via multi-scale convolutional neural layers for human-virus protein-protein interaction prediction</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <issue>24</issue>
        <year>2021</year>
        <fpage>4771</fpage>
        <lpage>4778</lpage>
        <pub-id pub-id-type="pmid">34273146</pub-id>
      </element-citation>
    </ref>
    <ref id="bib47">
      <label>47</label>
      <element-citation publication-type="journal" id="sbref44">
        <person-group person-group-type="author">
          <name>
            <surname>Liu-Wei</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Kafkas</surname>
            <given-names>Ş.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Dimonaco</surname>
            <given-names>N.J.</given-names>
          </name>
          <name>
            <surname>Tegnér</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hoehndorf</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>DeepViral: prediction of novel virus-host interactions from protein sequences and infectious disease phenotypes</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <issue>17</issue>
        <year>2021</year>
        <fpage>2722</fpage>
        <lpage>2729</lpage>
        <pub-id pub-id-type="pmid">33682875</pub-id>
      </element-citation>
    </ref>
    <ref id="bib48">
      <label>48</label>
      <element-citation publication-type="journal" id="sbref45">
        <person-group person-group-type="author">
          <name>
            <surname>Barman</surname>
            <given-names>R.K.</given-names>
          </name>
          <name>
            <surname>Saha</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of interactions between viral and host proteins using supervised machine learning methods</article-title>
        <source>PLoS One</source>
        <volume>9</volume>
        <issue>11</issue>
        <year>2014</year>
        <object-id pub-id-type="publisher-id">e112034</object-id>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="sec0175" sec-type="supplementary-material">
    <label>Appendix A</label>
    <title>Supplementary material</title>
    <p id="p0355"><supplementary-material content-type="local-data" id="ec0005"><caption><p>Supplementary material</p></caption><media xlink:href="mmc1.xlsx"/></supplementary-material>.</p>
    <p id="p0360"><supplementary-material content-type="local-data" id="ec0010"><caption><p>Supplementary material</p></caption><media xlink:href="mmc2.xlsx"/></supplementary-material>.</p>
    <p id="p0365"><supplementary-material content-type="local-data" id="ec0015"><caption><p>Supplementary material</p></caption><media xlink:href="mmc3.xlsx"/></supplementary-material>.</p>
  </sec>
  <fn-group>
    <fn id="sec0170" fn-type="supplementary-material">
      <label>Appendix A</label>
      <p id="p0350">Supplementary data associated with this article can be found in the online version at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.csbj.2023.12.010" id="ir0025">doi:10.1016/j.csbj.2023.12.010</ext-link>.</p>
    </fn>
  </fn-group>
</back>
<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_CSBJ2447 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEga1 jpg ?>
<?FILEmmc1 xlsx ?>
<?FILEmmc2 xlsx ?>
<?FILEmmc3 xlsx ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Comput Struct Biotechnol J</journal-id>
    <journal-id journal-id-type="iso-abbrev">Comput Struct Biotechnol J</journal-id>
    <journal-title-group>
      <journal-title>Computational and Structural Biotechnology Journal</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2001-0370</issn>
    <publisher>
      <publisher-name>Research Network of Computational and Structural Biotechnology</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10772389</article-id>
    <article-id pub-id-type="pii">S2001-0370(23)00484-1</article-id>
    <article-id pub-id-type="doi">10.1016/j.csbj.2023.12.010</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software/Web Server Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deep-HPI-pred: An R-Shiny applet for network-based classification and prediction of Host-Pathogen protein-protein interactions</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au0005">
        <name>
          <surname>Tahir ul Qamar</surname>
          <given-names>Muhammad</given-names>
        </name>
        <email>m.tahirulqamar@hotmail.com</email>
        <xref rid="aff0005" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
        <xref rid="fn1" ref-type="fn">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au0010">
        <name>
          <surname>Noor</surname>
          <given-names>Fatima</given-names>
        </name>
        <xref rid="aff0010" ref-type="aff">b</xref>
        <xref rid="fn1" ref-type="fn">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au0015">
        <name>
          <surname>Guo</surname>
          <given-names>Yi-Xiong</given-names>
        </name>
        <xref rid="aff0015" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author" id="au0020">
        <name>
          <surname>Zhu</surname>
          <given-names>Xi-Tong</given-names>
        </name>
        <xref rid="aff0005" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au0025">
        <name>
          <surname>Chen</surname>
          <given-names>Ling-Ling</given-names>
        </name>
        <email>llchen@gxu.edu.cn</email>
        <xref rid="aff0005" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <aff id="aff0005"><label>a</label>State Key Laboratory for Conservation and Utilization of Subtropical Agro-bioresources, College of Life Science and Technology, Guangxi University, Nanning 530004, China</aff>
      <aff id="aff0010"><label>b</label>Integrative Omics and Molecular Modeling Laboratory, Department of Bioinformatics and Biotechnology, Government College University Faisalabad (GCUF), Faisalabad 38000, Pakistan</aff>
      <aff id="aff0015"><label>c</label>National Key Laboratory of Crop Genetic Improvement, College of Informatics, Huazhong Agricultural University, Wuhan 430070, China</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding authors. <email>m.tahirulqamar@hotmail.com</email><email>llchen@gxu.edu.cn</email></corresp>
      <fn id="fn1">
        <label>1</label>
        <p id="ntp0005">These authors contributed equally to this study.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>15</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <volume>23</volume>
    <fpage>316</fpage>
    <lpage>329</lpage>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>10</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>11</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>12</day>
        <month>12</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 The Authors. Published by Elsevier B.V. on behalf of Research Network of Computational and Structural Biotechnology.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder/>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="ab0010">
      <p>Host-pathogen interactions (HPIs) are vital in numerous biological activities and are intrinsically linked to the onset and progression of infectious diseases. HPIs are pivotal in the entire lifecycle of diseases: from the onset of pathogen introduction, navigating through the mechanisms that bypass host cellular defenses, to its subsequent proliferation inside the host. At the heart of these stages lies the synergy of proteins from both the host and the pathogen. By understanding these interlinking protein dynamics, we can gain crucial insights into how diseases progress and pave the way for stronger plant defenses and the swift formulation of countermeasures. In the framework of current study, we developed a web-based R/Shiny app, Deep-HPI-pred, that uses network-driven feature learning method to predict the yet unmapped interactions between pathogen and host proteins. Leveraging citrus and <italic>C</italic>Las bacteria training datasets as case study, we spotlight the effectiveness of Deep-HPI-pred in discerning Protein-protein interaction (PPIs) between them. Deep-HPI-pred use Multilayer Perceptron (MLP) models for HPI prediction, which is based on a comprehensive evaluation of topological features and neural network architectures. When subjected to independent validation datasets, the predicted models consistently surpassed a Matthews correlation coefficient (MCC) of 0.80 in host-pathogen interactions. Remarkably, the use of Eigenvector Centrality as the leading topological feature further enhanced this performance. Further, Deep-HPI-pred also offers relevant gene ontology (GO) term information for each pathogen and host protein within the system. This protein annotation data contributes an additional layer to our understanding of the intricate dynamics within host-pathogen interactions. In the additional benchmarking studies, the Deep-HPI-pred model has proven its robustness by consistently delivering reliable results across different host-pathogen systems, including plant-pathogens (accuracy of 98.4% and 97.9%), human-virus (accuracy of 94.3%), and animal-bacteria (accuracy of 96.6%) interactomes. These results not only demonstrate the model's versatility but also pave the way for gaining comprehensive insights into the molecular underpinnings of complex host-pathogen interactions. Taken together, the Deep-HPI-pred applet offers a unified web service for both identifying and illustrating interaction networks. Deep-HPI-pred applet is freely accessible at its homepage: <ext-link ext-link-type="uri" xlink:href="https://cbi.gxu.edu.cn/shiny-apps/Deep-HPI-pred/" id="ir0005">https://cbi.gxu.edu.cn/shiny-apps/Deep-HPI-pred/</ext-link> and at github: <ext-link ext-link-type="uri" xlink:href="https://github.com/tahirulqamar/Deep-HPI-pred" id="ir0010">https://github.com/tahirulqamar/Deep-HPI-pred</ext-link>.</p>
    </abstract>
    <abstract abstract-type="graphical" id="ab0015">
      <title>Graphical abstract</title>
      <p>
        <fig id="fig0030" position="anchor">
          <alt-text id="at0045">ga1</alt-text>
          <graphic xlink:href="ga1" id="lk0030"/>
        </fig>
      </p>
    </abstract>
    <kwd-group id="keys0005">
      <title>Keywords</title>
      <kwd>Host–pathogen interactions</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Multilayer perceptron</kwd>
      <kwd>Neural networks</kwd>
      <kwd>Topological features</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec0005">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0005">Host-pathogen interactions are crucial determinants in the dynamics of infectious diseases <xref rid="bib1" ref-type="bibr">[1]</xref>. These interactions are primarily facilitated by protein-protein interactions (PPIs), which orchestrate every phase of disease progression, from the initial pathogen invasion to its eventual establishment within the host <xref rid="bib2" ref-type="bibr">[2]</xref>. Citrus Huanglongbing (HLB), also known as citrus greening, represents a paradigmatic case of devastating plant diseases on a global scale <xref rid="bib3" ref-type="bibr">[3]</xref>. Predominantly caused by the bacterium <italic>Candidatus</italic> Liberibacter asiaticus (<italic>C</italic>Las), along with <italic>Ca.</italic> L. americanus and <italic>Ca</italic>. L. africanus, HLB has emerged as a primary threat to citrus production, particularly in regions like Florida, USA <xref rid="bib4" ref-type="bibr">[4]</xref>. CLas, the most prevalent among these pathogens, is vectored by the Asian citrus <italic>psyllid Diaphorina</italic> citri, leading to its colonization in the phloem tissues of citrus plants <xref rid="bib5" ref-type="bibr">[5]</xref>. This invasion results in severe phytopathological symptoms and extensive agricultural damage. The economic ramifications of HLB are staggering, with the disease inflicting multibillion-dollar losses globally, debilitating the citrus industry's financial stability <xref rid="bib6" ref-type="bibr">[6]</xref>. The pervasiveness of <italic>C</italic>Las and its profound impact on citrus crops underscore the necessity of an in-depth exploration of the PPIs between the host plants and the pathogen. Recently Yuan et al. <xref rid="bib7" ref-type="bibr">[7]</xref> mapped a network of <italic>Arabidopsis thaliana</italic> interactions with the bacterial pathogen <italic>Pseudomonas syringae</italic>, identifying new components involved in the plant immune response and paving the way for future plant disease control strategies. In a similar vein, Dyer et al. <xref rid="bib8" ref-type="bibr">[8]</xref> conducted a large-scale study on PPIs between human cells and <italic>Bacillus anthracis</italic>, the bacterium responsible for anthrax, revealing numerous potential therapeutic intervention targets. These studies underscore the vital role of host-pathogen PPIs in understanding the complex interplay between the host's defenses and the pathogen's invasion strategies. Therefore, these interactions form the cornerstone of research aiming to unravel the intricacies of infectious disease dynamics and serve as targets for potential therapeutic interventions.</p>
    <p id="p0010">Recent advancements in experimental methodologies, particularly in high-throughput screening and molecular imaging techniques, have significantly deepened our comprehension of host-pathogen PPIs <xref rid="bib9" ref-type="bibr">[9]</xref>. These innovations have enabled more precise identification and analysis of PPIs, shedding light on the intricate mechanisms of infection and host response, and paving the way for novel therapeutic interventions. Despite this progress, such techniques often come with high costs in terms of resources, time, and labor. Given the multitude of potential protein-interacting partners, the reliance on these methods becomes even more challenging, underlining the necessity for more cost-effective and efficient alternatives <xref rid="bib10" ref-type="bibr">[10]</xref>, <xref rid="bib11" ref-type="bibr">[11]</xref>. In contrast, computational methods for predicting host-pathogen interactions present themselves as a fitting solution to this pressing demand <xref rid="bib12" ref-type="bibr">[12]</xref>. These methods not only facilitate the detection of interactions but also have the potential to mine incomplete interaction maps for new discoveries, exponentially increasing the knowledge base regarding host-pathogen dynamics <xref rid="bib12" ref-type="bibr">[12]</xref>. Recently, Kaundal et al. <xref rid="bib13" ref-type="bibr">[13]</xref> developed deepHPI, an all-encompassing deep learning platform for the accurate prediction and visualization of HPPIs. Leveraging convolutional neural networks (CNN), their platform achieved a prediction accuracy of 96% on a curated dataset, significantly surpassing the performance of other methods on the same dataset. Complementing this work, Loaiza et al. introduced Pred-HPI <xref rid="bib14" ref-type="bibr">[14]</xref>, an integrated web server platform that employs sequence-based methods for the detection and visualization of host-pathogen interactions. PredHPI offered a sequence similarity-based approach and reached an accuracy of approximately 90% on a large-scale human-virus PPI dataset.</p>
    <p id="p0015">Despite the progress, these approaches primarily employed feature extraction strategies that represent host-pathogen protein pairs as fixed-length feature vectors, extracted from protein sequences. These sequence-based methods, while valuable, have limitations in terms of achieving high prediction accuracy, as they do not fully exploit the wealth of available structural and functional information. Recognizing these limitations, our study introduces a novel approach that leverages network-based integration methods for predicting host-pathogen interactions. Preliminary results have shown that this approach outperforms traditional sequence-based methods in terms of prediction accuracy scores.</p>
    <p id="p0020">To address these gaps, we introduced Deep-HPI-pred, an innovative R-shiny application that leverages advanced deep learning models. This application, for the first time, provides researchers with the autonomy to manually or automatically upload their training datasets for host-pathogen protein interaction prediction. The distinguishing characteristic of Deep-HPI-pred lies in its pioneering use of topological features for PPI prediction, as opposed to the conventional sequence-based features. This novel approach significantly enhances the sophistication and accuracy of predictions, marking a considerable advance in the bioinformatics field. Further, in order to substantiate the robustness and reliability of the Deep-HPI-pred model, our framework encompasses a rigorous validation protocol encompassing diverse biological systems, including plant-pathogen, human-virus, and animal-bacteria interactomes. This empirical evaluation across distinct host-pathogen pairs fortifies the scientific validity of the model's predictive prowess. In summary, as the premier R shiny application dedicated to predicting host-pathogen interactions, Deep-HPI-pred not only presents a groundbreaking tool for researchers, but also catalyzes a paradigm shift in our approach to understanding and predicting the intricate dynamics of infectious diseases.</p>
  </sec>
  <sec id="sec0010">
    <label>2</label>
    <title>Materials and methods</title>
    <sec id="sec0015">
      <label>2.1</label>
      <title>Gathering of true protein-protein interactions (PPIs) and data pre-processing</title>
      <p id="p0025">In this study, our initial host-pathogen PPI data was sourced from established host-pathogen interaction databases, including Pred-HPI <xref rid="bib14" ref-type="bibr">[14]</xref> and GreeningDB <xref rid="bib15" ref-type="bibr">[15]</xref>. These databases provided a broad spectrum of interactions, serving as a foundational reference for our research. However, to ensure a more robust and comprehensive dataset, we did not solely rely on these pre-compiled interactions. Recognizing the importance of data authenticity, we supplemented our database-derived PPIs with experimentally validated interactions. These additional PPIs were meticulously extracted from a wide range of peer-reviewed scientific literature. This step was vital for expanding the dataset and reinforcing its validity with real-world, experimentally confirmed interactions, thus substantially increasing the count of true positive PPIs in our study. The compiled data, encompassing both database-sourced and manually extracted PPIs, then underwent rigorous pre-processing. This crucial phase was aimed at maintaining the highest level of data integrity. We meticulously cleaned the combined dataset, carefully removing any duplicate entries to ensure a unique and non-redundant set of PPIs. This thorough cleaning process was instrumental in ensuring the reliability and accuracy of our dataset, thereby enhancing the overall quality of our analysis. By integrating these two diverse sources of data – established databases and manually verified literature – we aimed to strike a balance between the breadth and depth of our PPI dataset. This approach not only provided a comprehensive view of the host-pathogen interactions but also added a layer of validation to the interactions derived from databases, thereby bolstering the credibility and applicability of our study's findings.</p>
    </sec>
    <sec id="sec0020">
      <label>2.2</label>
      <title>Generation of negative protein-protein interactions (PPIs)</title>
      <p id="p0030">To augment our existing true positive PPIs dataset, the generation of negative PPIs was conducted. Negative PPIs, representing protein pairs that do not interact, serve as an important counterpart to positive interactions in our analysis <xref rid="bib16" ref-type="bibr">[16]</xref>. The generation of negative PPIs was a two-step process. The first step involved creating a random pairing between the protein sets of the selected pathogen and host. Given that there are <inline-formula><mml:math id="M1" altimg="si0001.svg"><mml:mi>m</mml:mi></mml:math></inline-formula> pathogen proteins and <inline-formula><mml:math id="M2" altimg="si0002.svg"><mml:mi>n</mml:mi></mml:math></inline-formula> host proteins, the total possible pairings, assuming no interaction between any two proteins, are <inline-formula><mml:math id="M3" altimg="si0003.svg"><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="M4" altimg="si0004.svg"><mml:mrow><mml:mi>P</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> be the set of pathogen proteins and <inline-formula><mml:math id="M5" altimg="si0005.svg"><mml:mrow><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> be the set of host proteins. The total possible random pairings <inline-formula><mml:math id="M6" altimg="si0006.svg"><mml:mi>R</mml:mi></mml:math></inline-formula> between pathogen proteins and host proteins is represented by the Cartesian product of the two sets:<disp-formula id="eqn0005"><label>(3.1)</label><mml:math id="M7" altimg="si0007.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>R</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>P</mml:mi><mml:mi mathvariant="normal">and</mml:mi><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">H</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="p0035">In the second step, a subset was randomly selected from these random pairings to construct the negative dataset. This subset selection ensured that the number of negative PPIs matched the number of true positive PPIs in our dataset. This balancing act serves as a cornerstone for the impending deep learning model training, underpinning an unbiased and balanced approach to the development of a robust predictive model. In current study, the generation of negative PPIs was integral to creating a balanced dataset for effective host-pathogen PPI prediction. This approach aligns with methodologies adopted in similar studies, ensuring robustness and validity in our predictive model. Specifically, Chen et al. <xref rid="bib17" ref-type="bibr">[17]</xref> employed a comparable strategy in their research, where negative PPIs were systematically generated through random pairing, thereby maintaining an equivalent size of negative and positive datasets. This method of balancing is crucial, as underscored by Scott et al. <xref rid="bib18" ref-type="bibr">[18]</xref>, who highlighted the potential risks associated with imbalanced datasets in PPI predictions, such as overfitting and poor model generalization. Our methodology addresses these concerns by matching the number of negative PPIs with positive PPIs, thereby fostering a more accurate and unbiased predictive model.</p>
    </sec>
    <sec id="sec0025">
      <label>2.3</label>
      <title>Assembly of dataset</title>
      <p id="p0040">The assembly of the final dataset was carried out by integrating both positive <inline-formula><mml:math id="M8" altimg="si0008.svg"><mml:mi mathvariant="italic">P_set</mml:mi></mml:math></inline-formula> and negative <inline-formula><mml:math id="M9" altimg="si0009.svg"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">N_set</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> PPIs. The negative PPI dataset, representing non-interactions, was carefully merged with the positive PPI dataset. This integrated approach not only maintains the balance between interaction and non-interaction data but also encapsulates the full range of possible protein interactions. Mathematically, the final dataset <inline-formula><mml:math id="M10" altimg="si0010.svg"><mml:mi mathvariant="italic">F_set</mml:mi></mml:math></inline-formula> is represented as the union of the positive PPI dataset <inline-formula><mml:math id="M11" altimg="si0011.svg"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">set</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and the negative PPI dataset <inline-formula><mml:math id="M12" altimg="si0012.svg"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">N_set</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>:<disp-formula id="eqn0010"><label>(3.2)</label><mml:math id="M13" altimg="si0013.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>F</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="italic">set</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>P</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="italic">set</mml:mi><mml:mo>∪</mml:mo><mml:mi>N</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="italic">set</mml:mi></mml:mrow></mml:math></disp-formula></p>
      <p id="p0045">This final dataset, <inline-formula><mml:math id="M14" altimg="si0014.svg"><mml:mi mathvariant="italic">F_set</mml:mi></mml:math></inline-formula>, encapsulates a broad spectrum of potential PPIs. It is primed for rigorous training within the deep learning model, offering a well-rounded, realistic, and balanced resource for the analysis. This strategic assembly of positive and negative interactions underpins the analytical framework, fostering a robust and comprehensive model tailored for effective host-pathogen interaction prediction.</p>
    </sec>
    <sec id="sec0030">
      <label>2.4</label>
      <title>Feature extraction: quantification of network topology parameters</title>
      <p id="p0050">The creation of the final PPI dataset marked the transition to a critical phase - feature extraction, which is the quantification of the topological parameters of the constructed network <xref rid="bib19" ref-type="bibr">[19]</xref>. The intent behind this step is to decipher and numerically articulate the intricate structural patterns within the protein network. Our primary objective was to leverage network topology for the accurate prediction of interactions between host and viral proteins. To achieve this, current study strategically employed a range of centrality measures, namely Degree Centrality, Betweenness Centrality, Closeness Centrality, Eigenvector Centrality, PageRank Centrality, Hub Score, and Eccentricity. This was achieved with the assistance of the topological algorithms incorporated within the Igraph package of R <xref rid="bib20" ref-type="bibr">[20]</xref>. These features were not selected on the basis of their individual merits alone but were chosen for their collective ability to provide a comprehensive view of the network dynamics, crucial for understanding the intricacies of host-pathogen PPI networks. This methodological approach is supported by the work of Przulj et al. <xref rid="bib21" ref-type="bibr">[21]</xref> and Ashtiani et al. <xref rid="bib22" ref-type="bibr">[22]</xref>, who have demonstrated the importance of network topology in understanding PPI networks and the utility of centrality measures in identifying key proteins within these networks. These topological features assist in identifying the potential interaction points and key nodes within the host-pathogen network.</p>
      <p id="p0055">These features, ranging from quantifying the number of direct connections a protein node has (Degree Centrality) to calculating the maximum distance from a node to all other nodes (Eccentricity), offer a comprehensive view of each protein's role and importance within the overall network structure. Degree Centrality and Hub Score, for instance, shed light on the interaction richness and the core component status of a protein respectively <xref rid="bib22" ref-type="bibr">[22]</xref>, <xref rid="bib23" ref-type="bibr">[23]</xref>. For a graph <inline-formula><mml:math id="M15" altimg="si0015.svg"><mml:mi>G</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="M16" altimg="si0016.svg"><mml:mi>N</mml:mi></mml:math></inline-formula> vertices, the Degree Centrality (DC) of a vertex <inline-formula><mml:math id="M17" altimg="si0017.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> is given by <inline-formula><mml:math id="M18" altimg="si0018.svg"><mml:mrow><mml:mi mathvariant="italic">DC</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Degree</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M19" altimg="si0019.svg"><mml:mrow><mml:mi mathvariant="italic">Degree</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represents the number of edges incident on vertex <inline-formula><mml:math id="M20" altimg="si0020.svg"><mml:mi>v</mml:mi></mml:math></inline-formula>. Similarly, Betweenness Centrality, Closeness Centrality, and PageRank Centrality provide insights into a protein's role as a network connector, its centrality, and its importance based on neighbouring proteins. Betweenness Centrality is defined as <inline-formula><mml:math id="M21" altimg="si0021.svg"><mml:mrow><mml:mi mathvariant="italic">BC</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>σ</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="italic">t</mml:mi></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mspace width="1em"/></mml:mrow></mml:math></inline-formula>for all <inline-formula><mml:math id="M22" altimg="si0022.svg"><mml:mrow><mml:mi mathvariant="italic">s</mml:mi><mml:mo>≠</mml:mo><mml:mi>v</mml:mi><mml:mo>≠</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M23" altimg="si0023.svg"><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>) is the total number of shortest paths from node <inline-formula><mml:math id="M24" altimg="si0024.svg"><mml:mi>s</mml:mi></mml:math></inline-formula> to node <inline-formula><mml:math id="M25" altimg="si0025.svg"><mml:mi>t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M26" altimg="si0026.svg"><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>) is the number of those paths that pass through <inline-formula><mml:math id="M27" altimg="si0027.svg"><mml:mi>v</mml:mi></mml:math></inline-formula>. For a connected graph G with N vertices, the Closeness Centrality (CC) of a vertex v is given by <inline-formula><mml:math id="M28" altimg="si0028.svg"><mml:mrow><mml:mi mathvariant="italic">CC</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="M29" altimg="si0029.svg"><mml:mi>t</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="M30" altimg="si0030.svg"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represents the shortest-path distance between <inline-formula><mml:math id="M31" altimg="si0031.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M32" altimg="si0032.svg"><mml:mi>t</mml:mi></mml:math></inline-formula>.</p>
      <p id="p0060">On the other hand, Eigenvector Centrality allots relative scores to all nodes in the network, acknowledging that connections to high-scoring nodes contribute more to the overall node score <xref rid="bib24" ref-type="bibr">[24]</xref>. The Eigenvector Centrality (EC) of a node <inline-formula><mml:math id="M33" altimg="si0033.svg"><mml:mi>i</mml:mi></mml:math></inline-formula> is defined as <inline-formula><mml:math id="M34" altimg="si0034.svg"><mml:mrow><mml:mi mathvariant="italic">EC</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi mathvariant="italic">EC</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="M35" altimg="si0035.svg"><mml:mi>j</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="M36" altimg="si0036.svg"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the elements of the adjacency matrix and <inline-formula><mml:math id="M37" altimg="si0037.svg"><mml:mi>λ</mml:mi></mml:math></inline-formula> is a constant. PageRank Centrality is computed using the formula <inline-formula><mml:math id="M38" altimg="si0038.svg"><mml:mrow><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="badbreak">+</mml:mo><mml:mi mathvariant="italic">d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="badbreak">+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo linebreak="badbreak">+</mml:mo><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="M39" altimg="si0039.svg"><mml:mrow><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>is the PageRank of page A, <inline-formula><mml:math id="M40" altimg="si0040.svg"><mml:mrow><mml:mi mathvariant="italic">PR</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the PageRank of pages <inline-formula><mml:math id="M41" altimg="si0041.svg"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> which link to page <inline-formula><mml:math id="M42" altimg="si0042.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="italic">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>is the number of outbound links on page <inline-formula><mml:math id="M43" altimg="si0043.svg"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M44" altimg="si0044.svg"><mml:mi>d</mml:mi></mml:math></inline-formula> is a damping factor which set between 0 and 1. Finally, the Eccentricity (ECC) of a vertex <inline-formula><mml:math id="M45" altimg="si0045.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> in a graph is the maximum distance from <inline-formula><mml:math id="M46" altimg="si0046.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> to all other vertices, calculated by <inline-formula><mml:math id="M47" altimg="si0047.svg"><mml:mrow><mml:mi mathvariant="italic">ECC</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>max</mml:mi><mml:mi mathvariant="italic">d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="italic">t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="italic">for all t</mml:mi><mml:mo>≠</mml:mo><mml:mi mathvariant="italic">v</mml:mi></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="M48" altimg="si0048.svg"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="italic">t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the shortest-path distance between the vertices <inline-formula><mml:math id="M49" altimg="si0049.svg"><mml:mi>v</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="M50" altimg="si0050.svg"><mml:mrow><mml:mi>t</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
    </sec>
    <sec id="sec0035">
      <label>2.5</label>
      <title>Deep learning models</title>
      <p id="p0065">Current study leveraged the potential of deep learning through the implementation of three distinguished models, namely Identity Convolutional Neural Networks (ID-CNN), Recurrent Neural Networks (RNN), and Multi-Layer Perceptrons (MLP). In R, the 'keras' package was employed, providing a high-level neural networks API on the TensorFlow platform. Additionally, 'caret' in R, facilitating access to numerous machine learning algorithms, was used in conjunction with 'tensorflow' and 'reticulate'. 'Tensorflow' offers an R interface for TensorFlow, while 'reticulate' enables Python integration within the R environment. Each model's unique architecture processes input data differently, offering diversified data interpretations, thereby improving the accuracy and reliability of the predicted host-pathogen PPIs.</p>
      <sec id="sec0040">
        <label>2.5.1</label>
        <title>ID-Convolutional Neural Networks (CNN)</title>
        <p id="p0070">The ID-Convolutional Neural Networks (CNN) model forms an integral part of the deep learning approach. The ID-CNN are primarily known for their application in image processing, but their utility extends to PPI prediction as well <xref rid="bib25" ref-type="bibr">[25]</xref>. The core architecture of the ID-CNN model implemented is designed with multiple layers, each layer equipped with a specific role and function to perform <xref rid="bib26" ref-type="bibr">[26]</xref>. The architecture commenced with a 2D convolution layer equipped with 16 filters of kernel size <inline-formula><mml:math id="M51" altimg="si0051.svg"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, employing the Rectified Linear Unit (ReLU) activation function <inline-formula><mml:math id="M52" altimg="si0052.svg"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. This layer was instrumental in the detection of initial patterns or features within the data, with its output, <inline-formula><mml:math id="M53" altimg="si0053.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> given by <inline-formula><mml:math id="M54" altimg="si0054.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>*</mml:mo><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Following this, a second 2D convolution layer was added, this one featuring 32 filters and a kernel size of <inline-formula><mml:math id="M55" altimg="si0055.svg"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The same ReLU activation function was used, aiding in the learning of more complex patterns based on the initial features. The output of this layer, <inline-formula><mml:math id="M56" altimg="si0056.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> was calculated by <inline-formula><mml:math id="M57" altimg="si0057.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>O</mml:mi><mml:mn>1</mml:mn><mml:mo>*</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
        <p id="p0075">Post the convolutional operations, a global max pooling layer was implemented, denoted by <inline-formula><mml:math id="M58" altimg="si0058.svg"><mml:mrow><mml:mi>M</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula>This step reduced the dimensionality of the model, mitigating computational complexity and overfitting. Subsequently, two dense layers were added to the architecture. The first of these layers consisted of 64 neurons, its output, <inline-formula><mml:math id="M59" altimg="si0059.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula>, defined by <inline-formula><mml:math id="M60" altimg="si0060.svg"><mml:mrow><mml:mi>O</mml:mi><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo>*</mml:mo><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The final layer was a single neuron employing a sigmoid activation function<inline-formula><mml:math id="M61" altimg="si0061.svg"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, producing the model output:<disp-formula id="eqn0015"><label>(3.3)</label><mml:math id="M62" altimg="si0062.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>P</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>O</mml:mi><mml:mn>3</mml:mn><mml:mo>*</mml:mo><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0080">The entire model was trained using the Adam optimizer and the binary cross-entropy loss function, a suitable choice for the binary nature of the task. This function is given by:<disp-formula id="eqn0020"><label>(3.4)</label><mml:math id="M63" altimg="si0063.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi mathvariant="italic">Loss</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo linebreak="badbreak">−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">ylog</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0085">It's noteworthy that the input data to the model included degree centrality features for both pathogen and host proteins, which was normalized before being fed into the model.</p>
      </sec>
      <sec id="sec0045">
        <label>2.5.2</label>
        <title>Recurrent neural network</title>
        <p id="p0090">In the pursuit of host-pathogen protein interaction analysis, a Recurrent Neural Network (RNN) was incorporated, ideal due to the inherent sequential and interconnected nature of the protein data <xref rid="bib27" ref-type="bibr">[27]</xref>. An RNN model was devised via the Keras library in R <xref rid="bib28" ref-type="bibr">[28]</xref>, comprising of an input layer, a hidden layer, and an output layer. The input layer is designed to accept feature vectors signifying both the pathogen and host protein's network centrality measures, reshaped suitably for an RNN. The hidden layer utilizes simple RNN units governed by the ReLU activation function. On a mathematical note, suppose the input sequence is <inline-formula><mml:math id="M64" altimg="si0064.svg"><mml:mrow><mml:mi>X</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The new hidden state at time <inline-formula><mml:math id="M65" altimg="si0065.svg"><mml:mi mathvariant="italic">t</mml:mi></mml:math></inline-formula> or <inline-formula><mml:math id="M66" altimg="si0066.svg"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is calculated by the equation<disp-formula id="eqn0025"><label>(3.5)</label><mml:math id="M67" altimg="si0067.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mspace width="1em"/></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M68" altimg="si0068.svg"><mml:mi>f</mml:mi></mml:math></inline-formula> is the ReLU function, <inline-formula><mml:math id="M69" altimg="si0069.svg"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the input at time <inline-formula><mml:math id="M70" altimg="si0070.svg"><mml:mi>t</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="M71" altimg="si0071.svg"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="M72" altimg="si0072.svg"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="M73" altimg="si0073.svg"><mml:mi>b</mml:mi></mml:math></inline-formula> stand for the weights and bias parameters. The output layer is a dense layer featuring a single unit using a sigmoid activation function, fitting for the binary classification task as it computes probabilities. Mathematically, the output <inline-formula><mml:math id="M74" altimg="si0074.svg"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> at time <inline-formula><mml:math id="M75" altimg="si0075.svg"><mml:mi>t</mml:mi></mml:math></inline-formula> is given by the equation<disp-formula id="eqn0030"><label>(3.6)</label><mml:math id="M76" altimg="si0076.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi mathvariant="italic">sigmoid</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math id="M77" altimg="si0077.svg"><mml:mi mathvariant="italic">sigmoid</mml:mi></mml:math></inline-formula> as the sigmoid functions and <inline-formula><mml:math id="M78" altimg="si0078.svg"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M79" altimg="si0079.svg"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>as the weight and bias parameters for the output layer respectively. The training of this model was accomplished using the Adam optimization algorithm, an efficient extension to stochastic gradient descent. The loss function applied was binary cross-entropy, appropriate for binary classification problems. On a mathematical level, assuming the target (true value) as <inline-formula><mml:math id="M80" altimg="si0080.svg"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and the predicted output as <inline-formula><mml:math id="M81" altimg="si0081.svg"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the binary cross-entropy loss for the output at time <inline-formula><mml:math id="M82" altimg="si0082.svg"><mml:mi>t</mml:mi></mml:math></inline-formula> is calculated as<disp-formula id="eqn0035"><label>(3.7)</label><mml:math id="M83" altimg="si0083.svg"><mml:mrow><mml:mspace width="1em"/><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mo>−</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0095">This framework of equations directs the forward propagation, learning, and prediction process of the RNN model within the study.</p>
      </sec>
      <sec id="sec0050">
        <label>2.5.3</label>
        <title>Multilayer Perceptron (MLP)</title>
        <p id="p0100">The Multi-layer Perceptron (MLP), an artificial neural network type, was selected for this analysis owing to the networked architecture of the protein data. The MLP's ability to decipher complex, non-linear relationships through multiple layers of nodes made it a perfect fit for this study. Each node, symbolizing a neuron, is equipped with an activation function that aids in transforming the input data <xref rid="bib29" ref-type="bibr">[29]</xref>. The MLP model employed for this study comprises two hidden layers, containing 10 and 20 neurons respectively. These hidden layers form the heart of the MLP, endowing the network with its capacity to abstract and transform the input data. As the input data flows through these hidden layers, the model unveils intricate patterns and structures, allowing for more accurate prediction outcomes.</p>
        <p id="p0105">A critical aspect of this model is the logistic activation function. Chosen for its ability to map any real-valued number into a range between 0 and 1, the logistic function is a prime choice for binary classification tasks. It proves ideal for differentiating between interaction and non-interaction within the network-based protein data. Mathematically, in the hidden layers, each neuron computes a weighted sum of the inputs, adds a bias term and applies the logistic activation function. If we denote the input to a neuron as <inline-formula><mml:math id="M84" altimg="si0084.svg"><mml:mrow><mml:mi>x</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, weights as <inline-formula><mml:math id="M85" altimg="si0085.svg"><mml:mrow><mml:mi>w</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and bias as <inline-formula><mml:math id="M86" altimg="si0086.svg"><mml:mi>b</mml:mi></mml:math></inline-formula>, the output of the neuron <inline-formula><mml:math id="M87" altimg="si0087.svg"><mml:mi>y</mml:mi></mml:math></inline-formula> is computed as follows:<disp-formula id="eqn0040"><label>(3.8)</label><mml:math id="M88" altimg="si0088.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>y</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi mathvariant="italic">logistic</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p id="p0110">The MLP model training was regulated by a learning rate of 0.01 and a convergence threshold of 0.1. These values were fine-tuned to optimize the learning process and to control the pace at which the model learns from the protein data. Balancing the learning rate was essential to avoid erratic learning behaviors and to prevent the model from learning too slowly. A convergence threshold of 0.1 was set to ensure the learning process perseveres until the error on the training data reaches an acceptably low point.</p>
        <p id="p0115">Additionally, the model utilized the backpropagation algorithm to adjust its weights and bias values. This adjustment minimizes the discrepancy between the actual and predicted outputs over numerous iterations, or epochs, thereby progressively enhancing the model's accuracy. During each epoch, the backpropagation algorithm implements two primary steps for each training sample: Forward Propagation: The predicted output (<inline-formula><mml:math id="M89" altimg="si0089.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is calculated using the current weights and bias values. On the other hand, the weights and bias terms are updated based on the gradient of the loss function with respect to the weights and bias in back propagation. If we denote the loss function as <inline-formula><mml:math id="M90" altimg="si0090.svg"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M91" altimg="si0091.svg"><mml:mi>y</mml:mi></mml:math></inline-formula> is the actual output and (<inline-formula><mml:math id="M92" altimg="si0092.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the predicted output, the weights and bias are updated as follows:<disp-formula id="eqn0045"><label>(3.9)</label><mml:math id="M93" altimg="si0093.svg"><mml:mrow><mml:mspace width="1em"/><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>η</mml:mi><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="eqn0050"><label>(3.10)</label><mml:math id="M94" altimg="si0094.svg"><mml:mrow><mml:mi>b</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>b</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>η</mml:mi><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
        <p id="p0120">Here <inline-formula><mml:math id="M95" altimg="si0095.svg"><mml:mi>η</mml:mi></mml:math></inline-formula> is the learning rate and <inline-formula><mml:math id="M96" altimg="si0096.svg"><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></inline-formula> and <inline-formula><mml:math id="M97" altimg="si0097.svg"><mml:mrow><mml:mspace width="1em"/><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pred</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> are the gradients of the loss function with respect to the weights and bias, computed using the chain rule of differentiation.</p>
      </sec>
    </sec>
    <sec id="sec0055">
      <label>2.6</label>
      <title>Evaluation of models</title>
      <p id="p0125">To thoroughly assess the predictive competence of the models, six widely recognized evaluation metrics were employed: Precision, Accuracy, Sensitivity, Specificity, F1-score, and Matthew's Correlation Coefficient (MCC). These metrics provide a comprehensive understanding of the model's performance, considering both the positive and negative classes of the dataset, as well as the balance between them.</p>
      <p id="p0130">Precision (Pre) is a metric that reflects the exactness of the positive predictions made by the model <xref rid="bib30" ref-type="bibr">[30]</xref>. It is calculated as the ratio of true positives to the sum of true positives and false positives. Precision is essential as it focuses on the proportion of true positive predictions among all positive predictions, a crucial measure when false positives can significantly skew results, as discussed in the work of Powers et al. <xref rid="bib31" ref-type="bibr">[31]</xref>. Mathematically, it's represented as:<disp-formula id="eqn0055"><label>(3.11)</label><mml:math id="M98" altimg="si0098.svg"><mml:mrow><mml:mi mathvariant="italic">Precision</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FalsePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0135">Accuracy (Acc) measures the proportion of total predictions that are correct, irrespective of whether they are positive or negative <xref rid="bib32" ref-type="bibr">[32]</xref>. However, when used alongside the other metrics, it contributes to a holistic understanding of the model’s effectiveness, as suggested by Johnson and Khoshgoftaar <xref rid="bib33" ref-type="bibr">[33]</xref>. It is computed as the sum of true positives and true negatives divided by the total number of predictions, as shown:<disp-formula id="eqn0060"><label>(3.12)</label><mml:math id="M99" altimg="si0099.svg"><mml:mrow><mml:mi mathvariant="italic">Accuracy</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="italic">TrueNegatives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TotalPredictions</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0140">Sensitivity (Sn), also known as recall or true positive rate, quantifies the ability of the model to correctly identify positive instances <xref rid="bib34" ref-type="bibr">[34]</xref>. It is calculated as the ratio of true positives to the sum of true positives and false negatives, which are expressed as:<disp-formula id="eqn0065"><label>(3.13)</label><mml:math id="M100" altimg="si0100.svg"><mml:mrow><mml:mi mathvariant="italic">Sensitivity</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TruePositives</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FalseNegatives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">FN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0145">Specificity (Sp) is the metric that reflects the model's ability to correctly identify negative instances <xref rid="bib35" ref-type="bibr">[35]</xref>. It is the ratio of true negatives to the sum of true negatives and false positives, represented as:<disp-formula id="eqn0070"><label>(3.14)</label><mml:math id="M101" altimg="si0101.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi mathvariant="italic">Specificity</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TrueNegatives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TrueNegatives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FalsePositives</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0150">F1-score is the harmonic mean of Precision and Sensitivity. It is particularly useful when dealing with imbalanced datasets as it takes both false positives and false negatives into account <xref rid="bib36" ref-type="bibr">[36]</xref>. The F1-score provides a balanced view of the model’s performance on both classes, ensuring that neither false positives nor false negatives are disproportionately affecting the model's evaluation, as indicated in studies by Kakkar et al. <xref rid="bib37" ref-type="bibr">[37]</xref>. The formula for the F1-score is:<disp-formula id="eqn0075"><label>(3.15)</label><mml:math id="M102" altimg="si0102.svg"><mml:mrow><mml:mspace width="1em"/><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi mathvariant="italic">score</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Precision</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="italic">Sensitivity</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Precision</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">Sensitivity</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p id="p0155">Matthew's Correlation Coefficient (MCC) is a more robust metric as it considers all values in the confusion matrix (true positives, true negatives, false positives, and false negatives) <xref rid="bib36" ref-type="bibr">[36]</xref>. The range of MCC is between − 1 and + 1, where + 1 represents a perfect prediction, 0 indicates a random prediction, and − 1 denotes an inverse prediction. The MCC is calculated as follows:<disp-formula id="eqn0080"><label>(3.16)</label><mml:math id="M103" altimg="si0103.svg"><mml:mrow><mml:mi mathvariant="italic">MCC</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="italic">FN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="1em"/></mml:mrow></mml:math></disp-formula></p>
      <p id="p0160">These performance evaluation metrics were critical in systematically comparing and assessing the predictive capabilities of the 1D-CNN, RNN, and MLP models in the context of host-pathogen interaction prediction. By utilizing these six metrics, it became possible to analyze each model's proficiency from various dimensions such as precision, accuracy, sensitivity, specificity, and overall correlation. It is important to underscore that this comprehensive evaluation framework facilitated the discernment of the best-suited deep learning model for predicting potential interactions. This strategic methodology considerably enhanced the selection process, ensuring the identification of the most adept model to navigate the complex landscape of host-pathogen interactions.</p>
    </sec>
  </sec>
  <sec id="sec0060">
    <label>3</label>
    <title>Results</title>
    <sec id="sec0065">
      <label>3.1</label>
      <title>Dataset assembly and preliminary analysis</title>
      <p id="p0165">In order to predict host-pathogen interactions, the Citrus-HLB (Huanglongbing) interactome was selected as the primary case study. The primary sources of data for this study included the Pred-HPI and GreeningDB databases. The Pred-HPI database, known for its predictive modeling features, was combined with GreeningDB, a specialized database for citrus greening disease interactions, to provide a comprehensive range of interaction data. The true PPI data between Citrus and the Candidatus Liberibacter asiaticus (CLas) bacteria was sourced from the Greening and Pred-HPI database. This resulted in the retrieval of 1011 host-pathogen PPIs (45 unique pathogen proteins and 359 unique host proteins) from these databases. Additionally, 5 experimentally validated interactions were incorporated into the dataset, leading to a total of 1016 true PPIs. The dataset comprised of 45 unique pathogen proteins and 359 unique host proteins, accumulating a network of 404 nodes with a potential for 1016 possible interactions.</p>
    </sec>
    <sec id="sec0070">
      <label>3.2</label>
      <title>Construction of negative interaction sets with varied ratios</title>
      <p id="p0170">In this study, a common strategy was adopted to generate negative Protein-Protein Interactions (PPIs) based on traditional approaches found in the literature. This strategy involved creating random pairings between distinct protein sets - specifically, the selected pathogen proteins and host proteins. Subsequently, from these random pairings, subsets were strategically chosen to represent negative interactions. The importance of this lies in creating a comparative base against which positive interactions can be evaluated, thereby enhancing the predictive power of the developed models.</p>
      <p id="p0175">To further enrich the analysis and examine the robustness of the models, negative interactions were selected with varying ratios: 1:1 and 1:10. This led to the formation of two distinct datasets. The first, balanced, set comprised 1016 positive and 1016 negative PPIs, while the second, imbalanced, set encompassed 1016 positive and 10160 negative PPIs. The implementation of varying negative to positive ratios was essential in evaluating the robustness and resilience of the predictive models under different levels of data imbalance, a common scenario in biological studies. In the development of predictive models for host-pathogen PPIs, the selection of an appropriate class ratio is pivotal to balance between model performance and realistic representation of the dataset. For this study, a 1:10 class ratio was strategically chosen, aiming to effectively address the challenges presented by highly imbalanced datasets and to ensure adequate representation of the minority class. This decision is rooted in evidence from the literature, where extremely imbalanced ratios like 1:25 or 1:100, commonly used in PPI studies, have been shown to introduce a significant bias towards the majority class, leading to a high incidence of false negatives, as elucidated by Chen et al. <xref rid="bib17" ref-type="bibr">[17]</xref>. Conversely, ratios that closely approximate equal distribution, such as 1:1, often do not accurately reflect the actual distribution encountered in biological datasets, where non-interacting pairs are generally more prevalent. The adoption of a 1:10 ratio represents this balance, reducing the risk of bias towards the majority class while still mirroring the typical distribution in biological datasets. Supporting this selection, preliminary experiments within the study demonstrated that a 1:10 ratio optimally balances sensitivity and specificity. This finding is in alignment with the results presented by Lei et al. <xref rid="bib38" ref-type="bibr">[38]</xref>, where similar class ratios have been shown to enhance model performance in accurately identifying true positives without disproportionately increasing false negatives. Hence, this ratio has been found to effectively represent the real-world distribution of PPI datasets, ensuring sensitivity towards the minority class and enhancing the overall reliability of the predictive models.</p>
    </sec>
    <sec id="sec0075">
      <label>3.3</label>
      <title>Performance of the features in modeling host-pathogen interactions</title>
      <p id="p0180">The feature extraction process was integral to the deployment of the 1D-CNN, RNN, and MLP models. Seven key features, namely, Degree Centrality, Betweenness Centrality, Closeness Centrality, Eigenvector Centrality, PageRank Centrality, Hub Score, and Eccentricity were selected to comprehensively describe each host-pathogen protein pair. For each host-pathogen protein pair, these attributes were extracted and consolidated into a unified vector. This process resulted in comprehensive Protein-Protein Interaction (PPI) feature vectors, embodying the synergistic characteristics of both host and pathogen proteins. Each feature was individually assessed across the three models to deduce their unique contribution to the predictive accuracy of the models, fostering a comprehensive understanding of the interplay between individual features and model performance.</p>
      <p id="p0185">An effective evaluation of the models necessitated the careful construction of the independent datasets. Contrary to common practice where cross-validation is predominantly used due to its superior performance with unseen datasets, a different strategy was implemented for this study. Specifically, a fifth of the PPIs, encompassing both positive and negative interactions, were randomly allocated to form the independent dataset. The remainder of the PPIs, involving both types of interactions, were amalgamated into the training set. This approach, though unconventional, ensured a robust and comprehensive evaluation of the models' performance.</p>
      <sec id="sec0080">
        <label>3.3.1</label>
        <title>Assessment of 1D-CNN model at different class ratios</title>
        <p id="p0190">Analyzing the performance of the 1D-CNN model at different class ratios resulted in significant findings. The model's efficacy was assessed using multiple protein features, and statistical parameters such as Sensitivity, Specificity, Precision, Accuracy, F1-score, and MCC were evaluated (<xref rid="fig0005" ref-type="fig">Fig. 1</xref>). <xref rid="tbl0005" ref-type="table">Table 1</xref> provides an exhaustive assessment of the 1D-CNN model's performance at a class ratio of 1:1. During independent testing, the Degree Centrality feature exhibited strong performance with a Sensitivity, Specificity, and Precision of 0.8374, 0.8423, and 0.8415 respectively. The model's Accuracy and F1-score stood at 0.8399 and 0.8395, showcasing balanced categorization capabilities. Additionally, the MCC value of 0.6798 denotes reliable prediction of both classes. However, the standout feature at the 1:1 class ratio was Eigenvector Centrality. During independent testing, this feature achieved the highest Accuracy, F1-score, and MCC values, which were 0.8522, 0.8648, and 0.7171 respectively, indicating the model's proficiency in discerning true positives and negatives. Similar trends were observed during 5-fold cross-validation. Degree Centrality showed an Accuracy of 0.8529, an F1-score of 0.8568, and an MCC of 0.7074. Meanwhile, Eigenvector Centrality continued to excel with an Accuracy of 0.8388, an F1-score of 0.8493, and an MCC of 0.6844.<fig id="fig0005"><label>Fig. 1</label><caption><p>Performance evaluation results of CNN model at class ratios 1:1: (<bold>A</bold>) Independent testing (<bold>B</bold>) 5-Fold cross validation. Performance evaluation results at different class ratios 1:10: (<bold>C</bold>) Independent testing (<bold>D</bold>) 5-Fold cross validation.</p></caption><alt-text id="at0005">Fig. 1</alt-text><graphic xlink:href="gr1" id="lk0005"/></fig><table-wrap position="float" id="tbl0005"><label>Table 1</label><caption><p>1D-CNN model performance for each protein feature at class ratio of 1:1 and 1:10. Performance values of independent testing and 5-fold cross validation are shown within a same cell as follow: independent testing <italic>/</italic> 5-fold cross validation.</p></caption><alt-text id="at0030">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th><bold>Network features</bold></th><th><bold>Sensitivity</bold></th><th><bold>Specificity</bold></th><th><bold>Precision</bold></th><th><bold>Accuracy</bold></th><th><bold>F1-score</bold></th><th><bold>MCC</bold></th></tr><tr><th colspan="7"><bold>Class ratio 1:1</bold></th></tr></thead><tbody><tr><td><bold>Degree_centrality</bold></td><td>0.8374 / 0.8787</td><td>0.8423 / 0.8272</td><td>0.8415 / 0.8366</td><td>0.8399 / 0.8529</td><td>0.8395 / 0.8568</td><td>0.6798 / 0.7074</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.8325 / 0.7928</td><td>0.7733 / 0.8222</td><td>0.7860 / 0.8181</td><td>0.8029 / 0.8075</td><td>0.8086 / 0.8046</td><td>0.6069 / 0.6163</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.1576 / 0.1678</td><td>0.9901 / 0.9889</td><td>0.9411 / 0.9479</td><td>0.5738 / 0.5784</td><td>0.2700 / 0.2841</td><td>0.2667 / 0.2767</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9458 / 0.9056</td><td>0.7586 / 0.7721</td><td>0.7966 / 0.8001</td><td>0.8522 / 0.8388</td><td>0.8648 / 0.8493</td><td>0.7171 / 0.6844</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.0314 / 0.5184</td><td>0.4546 / 0.9265</td><td>0.5571 / 0.8768</td><td>0.6723 / 0.7224</td><td>0.53 / 0.6510</td><td>0.005 /0.4878</td></tr><tr><td><bold>Hub_score</bold></td><td>0.9261 / 0.9130</td><td>0.7389 / 0.7696</td><td>0.7800 / 0.7994</td><td>0.8325 / 0.8413</td><td>0.8468 / 0.8520</td><td>0.6769 / 0.6905</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.8817 / 0.8468</td><td>0.8128 / 0.8750</td><td>0.8248 / 0.8716</td><td>0.8472 / 0.8609</td><td>0.8523 / 0.8581</td><td>0.6962 / 0.7237</td></tr><tr><td colspan="7"><bold>Class ratio 1:10</bold></td></tr><tr><td><bold>Degree_centrality</bold></td><td>0.9856 / 0.9885</td><td>0.4227 / 0.4112</td><td>0.9400 / 0.9445</td><td>0.9303 / 0.9367</td><td>0.9623 / 0.9660</td><td>0.5355 / 0.5398</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.9774 / 0.9899</td><td>0.3939 / 0.3192</td><td>0.9433 / 0.9359</td><td>0.9259 / 0.9291</td><td>0.9600 / 0.9621</td><td>0.4610 / 0.4644</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.9977 / 1</td><td>0.0215 / 0.0234</td><td>0.9184 / 0.9107</td><td>0.9161 / 0.9109</td><td>0.9561 / 0.9532</td><td>0.0769 / 0.1423</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9925 / 0.9856</td><td>0.5475 / 0.5794</td><td>0.9524 / 0.9588</td><td>0.9486 / 0.9484</td><td>0.9721 / 0.9720</td><td>0.6744 / 0.6561</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.9950 / 0.9960</td><td>0.0945 / 0.0854</td><td>0.9177 / 0.9155</td><td>0.914 / 0.9129</td><td>0.9548 / 0.9540</td><td>0.2265 / 0.2178</td></tr><tr><td><bold>Hub_score</bold></td><td>0.9808 /0.9931</td><td>0.6323 / 0.53178</td><td>0.9638 / 0.9548</td><td>0.9491 / 0.9510</td><td>0.9722 / 0.9735</td><td>0.6698 / 0.6652</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.9931 / 0.9823</td><td>0.35 / 0.3871</td><td>0.9397 / 0.9410</td><td>0.9357 / 0.9279</td><td>0.9656 / 0.9612</td><td>0.5150 /0.4814</td></tr></tbody></table></table-wrap></p>
        <p id="p0195">On the other hand, the model's performance with a more imbalanced class ratio of 1:10 is elucidated in <xref rid="tbl0005" ref-type="table">Table 1</xref>. With this ratio, the Eigenvector Centrality and Hub Score features emerged as significant, recording high accuracy levels in both independent testing and 5-fold cross-validation. These results suggest that the model can uphold its performance even with a heavily skewed class distribution.</p>
      </sec>
      <sec id="sec0085">
        <label>3.3.2</label>
        <title>Comprehensive analysis of RNN model performance across features and class ratios</title>
        <p id="p0200">The RNN model's performance was thoroughly assessed across two disparate class ratios of 1:1 and 1:10, with each protein feature being evaluated individually (<xref rid="fig0010" ref-type="fig">Fig. 2</xref>). This extensive appraisal facilitated a clear distinction of the model's strengths and areas requiring improvement. At the 1:1 class ratio, several noteworthy findings were observed. The feature that stood out the most was Eigenvector Centrality, which achieved the highest metrics in the majority of categories. These included an Accuracy of 0.8694 during independent testing, a similarly impressive F1-score of 0.8716, and an MCC of 0.7393, the highest among all features. The Degree Centrality, although not leading, displayed considerable efficacy, particularly with its Sensitivity of 0.8088 and an MCC of 0.6887. However, Closeness Centrality demonstrated difficulties in this scenario, especially indicated by a negative MCC during independent testing, signifying a possible shortcoming of the RNN model when handling this feature.<fig id="fig0010"><label>Fig. 2</label><caption><p>Performance evaluation results of RNN model at class ratios 1:1: (<bold>A</bold>) Independent testing (<bold>B</bold>) 5-Fold cross validation. Performance evaluation results at different class ratios 1:10: (<bold>C</bold>) Independent testing (<bold>D</bold>) 5-Fold cross validation.</p></caption><alt-text id="at0010">Fig. 2</alt-text><graphic xlink:href="gr2" id="lk0010"/></fig></p>
        <p id="p0205">The analysis at the class ratio of 1:10 provided further intriguing insights. Eigenvector Centrality maintained its robust performance, securing an Accuracy of 0.9482 and an F1-score of 0.9720, marking its consistent influence on the RNN model's effectiveness. Degree Centrality not only exhibited exceptional Sensitivity and Precision but also contributed to an overall Accuracy of 0.9268. The Hub Score also performed consistently, exhibiting an Accuracy of 0.9477, reinforcing the model's overall effectiveness at the 1:10 ratio. On the other hand, the Closeness Centrality again presented unique challenges. It demonstrated a high Sensitivity of 0.9029 during independent testing but a strikingly low MCC of 0 in the 5-fold cross-validation. This anomaly exposes potential weaknesses in the model when operating under this class ratio and with this feature.<xref rid="tbl0010" ref-type="table">Table 2</xref>.<table-wrap position="float" id="tbl0010"><label>Table 2</label><caption><p>RNN model performance for each protein feature at class ratio of 1:1 and 1:10. Performance values of independent testing and 5-fold cross validation are shown within a same cell as follow: independent testing <italic>/</italic> 5-fold cross validation.</p></caption><alt-text id="at0035">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th><bold>Network features</bold></th><th><bold>Sensitivity</bold></th><th><bold>Specificity</bold></th><th><bold>Precision</bold></th><th><bold>Accuracy</bold></th><th><bold>F1-score</bold></th><th><bold>MCC</bold></th></tr><tr><th colspan="7"><bold>Class ratio 1:1</bold></th></tr></thead><tbody><tr><td><bold>Degree_centrality</bold></td><td>0.8088/ 0.8572</td><td>0.8839 / 0.7519</td><td>0.8965 / 0.7761</td><td>0.8423/ 0.8045</td><td>0.8504/ 0.8143</td><td>0.6887/ 0.6130</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.7969/ 0.8640</td><td>0.7799/ 0.7220</td><td>0.7733 / 0.7568</td><td>0.7881/ 0.7930</td><td>0.785 / 0.8067</td><td>0.5766/ 0.5924</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.4242 / 0.4808</td><td>0.4755 / 0.8125</td><td>0.2068 / 0.7189</td><td>0.4630 / 0.6467</td><td>0.2781 / 0.5758</td><td>-0.0860 / 0.3109</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.8571 / 0.8503</td><td>0.8826 / 0.7789</td><td>0.8866 / 0.7938</td><td>0.8694 / 0.8146</td><td>0.8716 / 0.8208</td><td>0.7393 / 0.6313</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.9 / 0.4808</td><td>0.6678 / 0.8125</td><td>0.5320 / 0.7189</td><td>0.7364 / 0.6467</td><td>0.6687 / 0.5758</td><td>0.5182 / 0.3109</td></tr><tr><td><bold>Hub_score</bold></td><td>0.7939 / 0.8505</td><td>0.8959 / 0.7656</td><td>0.9113 / 0.7841</td><td>0.8374 / 0.8081</td><td>0.8486 / 0.8159</td><td>0.6823 / 0.6186</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.8622 / 0.8608</td><td>0.8380 / 0.7610</td><td>0.8325 / 0.7832</td><td>0.8497 / 0.8109</td><td>0.8471 / 0.8200</td><td>0.6999 / 0.6254</td></tr><tr><td colspan="7"><bold>Class ratio 1:10</bold></td></tr><tr><td><bold>Degree_centrality</bold></td><td>0.9369 /<break/>0.9850</td><td>0.7413 /<break/>0.4175</td><td>0.9851 /<break/>0.9442</td><td>0.9268 /<break/>0.9334</td><td>0.9604 /<break/>0.9641</td><td>0.5050 /<break/>0.5268</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.9425 /<break/>0.9950</td><td>0.6302 /<break/>0.8571</td><td>0.9784 / 0.9950</td><td>0.9259 / 0.9308</td><td>0.9601 / 0.9631</td><td>0.4535 / 0.4769</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.9029 /<break/>1</td><td>0.2666 /<break/>0</td><td>0.9945 /<break/>0.9090</td><td>0.8988 /<break/>0.9090</td><td>0.9465 /<break/>0.9523</td><td>0.0464 /<break/>0</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9568 / 0.9900</td><td>0.8106 /<break/>0.5095</td><td>0.9877 / 0.9528</td><td>0.9482 /<break/>0.9464</td><td>0.9720 /<break/>0.9710</td><td>0.6366 /<break/>0.6288</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.9205 /<break/>0.9996</td><td>0.34 /<break/>0.0032</td><td>1 /<break/>0.9093</td><td>0.9205 / 0.9090</td><td>0.0013 /<break/>0.0151</td><td>0.9205 / 0.9523</td></tr><tr><td><bold>Hub_score</bold></td><td>0.9607 /<break/>0.9895</td><td>0.7697 /<break/>0.4945</td><td>0.9828 /<break/>0.9514</td><td>0.9477 /<break/>0.9445</td><td>0.9716 /<break/>0.9701</td><td>0.6457 /<break/>0.6136</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.9429 /<break/>0.9801</td><td>0.675 /<break/>0.3782</td><td>0.9615 /<break/>0.9403</td><td>0.9808 /<break/>0.9253</td><td>0.9286 /<break/>0.9597</td><td>0.4857 /<break/>0.4614</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="sec0090">
        <label>3.3.3</label>
        <title>Performance evaluation of MLP model for protein classification using different network features and class ratios</title>
        <p id="p0210">For the class ratio of 1:1, the model demonstrated varying degrees of effectiveness depending on the protein network feature used (<xref rid="fig0015" ref-type="fig">Fig. 3</xref>). The Eigenvector Centrality feature outshone the others, achieving an impressive accuracy rate of 0.9334 in independent testing and 0.9487 in 5-fold cross-validation. Its sensitivity and specificity scores were also quite high (0.9605 / 0.9578 and 0.9064 / 0.9126 respectively), indicating its ability to correctly identify positive classes and correctly reject the negative ones. Moreover, its high F1-Score (0.9352 / 0.9209) suggests a balanced precision and recall, and the robust MCC (0.8682 / 0.8894) demonstrates the model's quality in terms of binary classification. The Hub Score feature also yielded substantial results, with an accuracy of 0.9261 / 0.8745 and a sensitivity of 0.9310 / 0.9234, reinforcing the reliability of these two features. Conversely, the Closeness Centrality feature performed less favorably, displaying an accuracy of 0.5689 / 0.5246 and sensitivity of 0.9605 / 0.9213, hinting at its weaker ability to accurately predict protein classes.<fig id="fig0015"><label>Fig. 3</label><caption><p>Performance evaluation results of MLP model at class ratios 1:1: (<bold>A</bold>) Independent testing (<bold>B</bold>) 5-Fold cross validation. Performance evaluation results at different class ratios 1:10: (<bold>C</bold>) Independent testing (<bold>D</bold>) 5-Fold cross validation.</p></caption><alt-text id="at0015">Fig. 3</alt-text><graphic xlink:href="gr3" id="lk0015"/></fig></p>
        <p id="p0215">When the class ratio was adjusted to 1:10, the Eigenvector Centrality continued to lead in performance, particularly in sensitivity (0.9794 / 0.9651) and precision (0.9946 / 0.950739). This high sensitivity means the model could correctly identify a high proportion of actual positives, while the precision indicates that out of the classes predicted positive, most were accurate. The Degree Centrality also showed steady performance across multiple metrics like sensitivity (0.8522/0.8468), specificity (0.8472/0.8508), and accuracy (0.8497/0.8488). However, for this class ratio, Pagerank Centrality emerged as the underperforming feature with lower sensitivity (0.6453 / 0.6427) and accuracy scores (0.7266 / 0.7105), implying a lower correct prediction rate and a higher false-negative rate. In essence, these results elucidate the considerable impact of different protein network features on the MLP model's performance, as well as the role of class ratios in the efficacy of these predictions. The superior performance of Eigenvector Centrality feature indicates its potential usefulness in practical applications, although the differing results among other features underscore the importance of careful feature selection in building effective protein classification models.<xref rid="tbl0015" ref-type="table">Table 3</xref>.<table-wrap position="float" id="tbl0015"><label>Table 3</label><caption><p>MLP model performance for each protein feature at class ratio of 1:1 and 1:10. Performance values of independent testing and 5-fold cross validation are shown within a same cell as follow: independent testing <italic>/</italic> 5-fold cross validation.</p></caption><alt-text id="at0040">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th><bold>Network Features</bold></th><th><bold>Accuracy</bold></th><th><bold>Sensitivity</bold></th><th><bold>Specificity</bold></th><th><bold>F1_Score</bold></th><th><bold>Precision</bold></th><th><bold>MCC</bold></th></tr><tr><th colspan="7"><bold>Class ratio 1:1</bold></th></tr></thead><tbody><tr><td><bold>Degree_centrality</bold></td><td>0.8694 / 0.8448</td><td>0.8620 / 0.7881</td><td>0.8768 / 0.9014</td><td>0.8684 / 0.8355</td><td>0.875 / 0.8889</td><td>0.7389 / 0.6941</td></tr><tr><td><bold>Betweenness_centrality</bold></td><td>0.7881 / 0.8325</td><td>0.8423 / 0.8275</td><td>0.7339 / 0.8374</td><td>0.7990 / 0.8316</td><td>0.76 / 0.8358</td><td>0.5797 / 0.6650</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9334/ 0.9487</td><td>0.9605 / 0.9578</td><td>0.9064 / 0.9126</td><td>0.9352 / 0.9209</td><td>0.9112 / 0.9334</td><td>0.8682 / 0.8894</td></tr><tr><td><bold>Hub_score</bold></td><td>0.9261 / 0.8745</td><td>0.9310 / 0.9234</td><td>0.9211 / 0.9382</td><td>0.9264 / 0.8978</td><td>0.9219 / 0.9051</td><td>0.8522 / 0.8089</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.7044 / 0.6427</td><td>0.9359 / 0.7782</td><td>0.4729 / 0.7441</td><td>0.76 / 0.7105</td><td>0.6397 / 0.6894</td><td>0.4613 / 0.4253</td></tr><tr><td><bold>Pagerank-centrality</bold></td><td>0.8399 / 0.8570</td><td>0.8522 / 0.8721</td><td>0.8275 / 0.8091</td><td>0.84184 / 0.8101</td><td>0.8317 / 0.8566</td><td>0.6800 / 0.6421</td></tr><tr><td><bold>Closeness-centrality</bold></td><td>0.5689 / 0.5246</td><td>0.9605 / 0.9213</td><td>0.1773 / 0.1345</td><td>0.6902 / 0.6189</td><td>0.5386 / 0.4621</td><td>0.2218 / 0.2</td></tr><tr><td colspan="7"><bold>Class ratio 1:10</bold></td></tr><tr><td><bold>Degree_centrality</bold></td><td>0.8522/<break/>0.8468</td><td>0.8472/<break/>0.8508</td><td>0.8480 / 0.8502</td><td>0.8497/ 0.8488</td><td>0.8501/ 0.8483</td><td>0.8195/ 0.7980</td></tr><tr><td><bold>Betweenness_ centrality</bold></td><td>0.8592 / 0.8832</td><td>0.8226 / 0.7909</td><td>0.8277 / 0.8093</td><td>0.8374 / 0.8370</td><td>0.8398 / 0.8443</td><td>0.8851 / 0.8345</td></tr><tr><td><bold>Closeness_centrality</bold></td><td>0.8522 /<break/>0.8660</td><td>0.8029 / 0.7944</td><td>0.8122 / 0.8325</td><td>0.8275 /<break/>0.8015</td><td>0.8317 /<break/>0.80</td><td>0.7559 /<break/>0.711</td></tr><tr><td><bold>Eigenvector_centrality</bold></td><td>0.9794 /<break/>0.9651</td><td>0.8195 / 0.8411</td><td>0.9946 / 0.950739</td><td>0.8736 / 0.905</td><td>0.9352 / 0.8642</td><td>0.8647 / 0.8020</td></tr><tr><td><bold>Eccentricity</bold></td><td>0.9113 / 0.9018</td><td>0.4926 / 0.5122</td><td>0.6423 /0.6494</td><td>0.7019 / 0.7070</td><td>0.7535 / 0.7549</td><td>0.4448 / 0.4498</td></tr><tr><td><bold>Hub_score</bold></td><td>0.8472 /<break/>0.8351</td><td>0.8916 /<break/>0.8734</td><td>0.8865 / 0.8698</td><td>0.8694 / 0.8542</td><td>0.8664 / 0.8516</td><td>0.7396 / 0.7099</td></tr><tr><td><bold>Pagerank_centrality</bold></td><td>0.6453 /<break/>0.6427</td><td>0.8078 /<break/>0.7782</td><td>0.7705 / 0.7441</td><td>0.7266 / 0.7105</td><td>0.7024 / 0.6894</td><td>0.4593 / 0.4253</td></tr></tbody></table></table-wrap></p>
      </sec>
    </sec>
    <sec id="sec0095">
      <label>3.4</label>
      <title>Evaluation of models</title>
      <p id="p0220">The evaluation aimed to assess the performance of the MLP model across several network-based features. The features tested included Degree_centrality, Betweenness_centrality, Closeness_centrality, Eigenvector_centrality, Pagerank_centrality, Hub_score, and Eccentricity. These features are commonly used in network analysis to measure the importance of individual nodes within a network. In the context of protein-protein interaction networks, these features assists in the identification of key proteins that play critical roles in biological processes.</p>
      <p id="p0225">On independent testing, the Eigenvector_centrality feature stood out for its performance, achieving the highest accuracy (0.8736) and Matthews Correlation Coefficient (MCC, 0.8647) among all the features tested. The Eigenvector Centrality of a protein in a network measures the extent to which it is connected to other highly connected proteins. This feature is often used in biological network analysis to identify proteins that, although they may not have many connections themselves, are connected to key proteins in the network <xref rid="bib39" ref-type="bibr">[39]</xref>. In this context, the application of Eigenvector Centrality features has garnered attention, particularly in the study of host-pathogen protein interactions. Studies such as Khorsand et al. <xref rid="bib40" ref-type="bibr">[40]</xref> and Cui et al. <xref rid="bib41" ref-type="bibr">[41]</xref> have demonstrated the efficacy of this approach in unraveling complex interaction dynamics, suggesting new pathways for therapeutic intervention. The model's performance remained high during cross-validation, achieving an accuracy of 0.905 and an MCC of 0.8020. The high cross-validation performance indicates that the model generalizes well to unseen data and is not overfitted to the training data, which adds confidence to the robustness of our model.</p>
      <p id="p0230">In order to apply these findings in a predictive manner, each protein pair was given a score based on their Eigenvector Centrality features. The pair's features were then normalized and input into the trained MLP model. The overall architecture of MLP model was also presented in <xref rid="fig0020" ref-type="fig">Fig. 4</xref>. This produced a probability of interaction for each protein pair, which can be interpreted as a likelihood score for each pair's interaction. A threshold of 0.9 was set for these probability values to focus on the most significant interactions (<bold>Supplementary File 1:</bold>
<xref rid="sec0175" ref-type="sec">Table S1</xref>). Protein pairs with a score above this threshold were predicted to interact with high probability. This approach is common in binary classification tasks where it is more important to accurately predict one class (interacting pairs, in this case). It allows for better control over the trade-off between precision and recall, ensuring that predictions made are reliable and biologically significant.<fig id="fig0020"><label>Fig. 4</label><caption><p>MLP architecture used to train the host-pathogen PPIs models.</p></caption><alt-text id="at0020">Fig. 4</alt-text><graphic xlink:href="gr4" id="lk0020"/></fig></p>
      <p id="p0235">Of significant importance, our study goes beyond the realm of predicting the likelihood of positive interactions. Instead, our model's implementation serves as an expansive exploration, unveiling hidden interactions that might not have been initially evident. This dynamic capability underscores the inherent value of our approach, which extends beyond the validation of established interactions to the prospect of uncovering novel, previously unexplored interactions. These newfound interactions, accompanied by calculated likelihoods, not only enrich our comprehension but also stimulate a more profound exploration of potential molecular dialogues within the intricate domain of host-pathogen interactions. In conclusion, through rigorous evaluation and thoughtful application, the MLP model trained on Eigenvector Centrality features proved to be a powerful tool for predicting host-pathogen interactions. This methodological approach could be useful for future research in this field, particularly for studies seeking to understand complex host-pathogen dynamics.</p>
    </sec>
    <sec id="sec0100">
      <label>3.5</label>
      <title>Developing an interactive R shiny application</title>
      <p id="p0240">Building upon the foundations of our approach, we have successfully translated our methodology into an interactive and user-friendly tool: R Shiny application named Deep-HPI-pred. This innovative platform empowers users to seamlessly engage with our predictive models and gain insights into host-pathogen interactions. By providing an intuitive interface and leveraging advanced deep learning models, Deep-HPI-pred offers an accessible means for researchers to explore, predict, and understand the intricate dynamics of PPIs between hosts and pathogens. This application acts as a bridge, converting intricate computational methodologies into a user-friendly tool that catalyzes the advancement of host-pathogen interaction studies. This application is distinctive in the sense that it allows users to either manually or automatically upload the training dataset, facilitating a more streamlined operation (<xref rid="fig0025" ref-type="fig">Fig. 5</xref>). Furthermore, it is engineered to predict PPIs leveraging the topological features of the proteins, which is a significant step forward given that traditional prediction methods often lack this level of sophistication.<fig id="fig0025"><label>Fig. 5</label><caption><p>(<bold>A</bold>) User interface of Deep-HPI-pred (<bold>B</bold>) Deep-HPI-pred Interface showcasing data upload functionality (<bold>C</bold>) Result’s tab of Deep-HPI-pred. The blue color nodes within network indicate the Host-proteins while red color nodes represented the pathogen proteins (<bold>D</bold>) The probability of interactions among host and proteins are presented in form of table (<bold>E</bold>) The 'GO Analysis' Tab in Deep-HPI-pred demonstrating the visualization of a network graph and (<bold>F)</bold> the corresponding Gene Ontology (GO) Enrichment Analysis table.</p></caption><alt-text id="at0025">Fig. 5</alt-text><graphic xlink:href="gr5" id="lk0025"/></fig></p>
      <p id="p0245">What makes Deep-HPI-pred stand out is its ability to present intricate host-pathogen protein interactions and the corresponding probability scores in a simplified, user-friendly interface. This aspect bridges the gap between complex data analysis and its interpretability, thereby promoting a broader comprehension and accessibility of the information. Notably, Deep-HPI-pred is the first-ever application of its kind dedicated to the network-based classification and prediction of host-pathogen protein interactions. This development marks a significant milestone in bioinformatics and opens new opportunities for researchers to delve deeper into the dynamics of biological interactions. The application of deep learning models combined with topological analysis presents a novel approach to study and predict PPIs. This not only broadens the scope of current research methodologies but also paves the way for future advancements in this rapidly evolving field of study. By fostering a deeper understanding of pathogenesis and disease progression, Deep-HPI-pred contributes to the advancement of the scientific community's collective knowledge. It demonstrates the potential of integrating machine learning with biological data analysis, thereby setting a precedent for future research in this direction.</p>
      <sec id="sec0105">
        <label>3.5.1</label>
        <title>Interface and data input in the deep-HPI-pred application</title>
        <p id="p0250">Upon initiation of the Deep-HPI-pred application, the user is presented with a straightforward interface, deliberately designed for easy navigation and usability (<xref rid="fig0025" ref-type="fig">Fig. 5</xref>
<bold>(A)</bold>). A feature of the application allows users to upload their specific data files in CSV format through a conveniently located sidebar. In contrast, there is also an option to use pre-existing demo data. This flexibility caters to the distinct requirements of individual research, providing a tailored experience for interaction predictions. Moreover, the incorporation of demo data serves as an effective tool for users to familiarize themselves with the application's functionality swiftly.</p>
      </sec>
      <sec id="sec0110">
        <label>3.5.2</label>
        <title>Implementing training and prediction with deep learning models</title>
        <p id="p0255">After completion of the data upload process, the application prompts the "Train and Predict" feature (<xref rid="fig0025" ref-type="fig">Fig. 5</xref><bold>(B, C)</bold>). This launches the deep learning model previously trained on an extensive dataset of host-pathogen protein interactions. The model performs a comprehensive analysis of the uploaded data to accurately predict potential interactions. This application feature transforms the data input into valuable insights, opening avenues for the elucidation of complex host-pathogen interaction dynamics.</p>
      </sec>
      <sec id="sec0115">
        <label>3.5.3</label>
        <title>Visualizing and interpreting predicted interactions</title>
        <p id="p0260">The next phase of interaction with the application is through the "Results" tab, which is designed for data interpretation and visualization (<xref rid="fig0025" ref-type="fig">Fig. 5</xref><bold>(D)</bold>). The tab displays predicted interaction data and a network visualization of these interactions. The visual depiction facilitates better comprehension of the interaction networks and is particularly valuable in understanding complex interaction dynamics.</p>
      </sec>
      <sec id="sec0120">
        <label>3.5.4</label>
        <title>Conducting GO enrichment analysis for enhanced biological comprehension</title>
        <p id="p0265">The final component of the Deep-HPI-pred application, the "GO Analysis" tab, provides an effective tool for conducting GO enrichment analysis (<xref rid="fig0025" ref-type="fig">Fig. 5</xref><bold>(E, F)</bold>). Unlike other tools, which often lack the provision of GO terms and related analyses, Deep-HPI-pred fills this critical gap. The resulting Network Graph visualizes the outcomes of the GO analysis, offering an intuitive representation of the enriched terms. By collating and categorizing gene products based on shared GO annotations, the application presents an understanding of the biological implications of the predicted interactions. Moreover, to facilitate further in-depth investigations, researchers can conveniently download both the GO table and the corresponding plot, enhancing the tool's utility for advanced research endeavors.</p>
        <p id="p0270">Thus, the development and implementation of the Deep-HPI-pred R Shiny application constitutes a significant milestone in host-pathogen interaction studies. Its ability to integrate deep learning models, employ diverse protein interaction data, and provide interactive and comprehensible output in the form of network visualizations and GO enrichment analysis, creates a uniquely accessible platform for researchers across multiple disciplines. The application, by transforming raw data into insightful knowledge, effectively supports the intricate process of deciphering the complexities inherent in host-pathogen interactions.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec0125">
    <label>4</label>
    <title>Benchmarking</title>
    <p id="p0275">In this study, the versatility and effectiveness of the Deep-HPI-pred applet were demonstrated in accurately predicting PPIs across a range of biological contexts. PPIs were analyzed across three distinct host-pathogen pairs, each representing a unique interplay between different types of organisms - bacteria, plants, and animals. The dataset of experimentally verified HPI interactions was curated from existing literature, and then classified into distinct categories: plant-bacteria, plant-fungi, animal-bacteria <xref rid="bib14" ref-type="bibr">[14]</xref>, and human-virus.</p>
    <sec id="sec0130">
      <label>4.1</label>
      <title><italic>Mus musculus</italic> and <italic>Burkholderia mallei</italic></title>
      <p id="p0280"><italic>Burkholderia mallei</italic> is known to cause glanders, a rare infectious disease that affects horses, mules, and donkeys. It can also infect humans and is considered a potential bioterrorism agent. In this study, our Deep-HPI-pred model achieved an accuracy of 96.6%, sensitivity of 94.9%, specificity of 87.3%, F1 score of 88.7%, precision of 81.4%, MCC of 90.9%, and AUC of 95.1% in predicting PPIs between <italic>M. musculus</italic> (mouse) and <italic>B. mallei</italic>. This highlights the model's potential in assisting researchers in understanding the intricate interplay between these two organisms and could provide valuable insights into the mechanism of glanders infection in various species.</p>
    </sec>
    <sec id="sec0135">
      <label>4.2</label>
      <title><italic>Arabidopsis thaliana</italic> and <italic>Golovinomyces orontii</italic></title>
      <p id="p0285"><italic>Golovinomyces orontii</italic> is known to cause powdery mildew in <italic>A. thaliana</italic>, a model organism in plant biology. Accurate prediction of PPIs is crucial for deciphering the molecular basis of plant-pathogen interactions and developing strategies for disease resistance. Our Deep-HPI-pred model demonstrated high accuracy in predicting these interactions, with an accuracy of 98.4%, sensitivity of 89.3%, specificity of 93.2%, F1 score of 95%, precision of 92.4%, MCC of 93.3%, and AUC of 92.8%.</p>
    </sec>
    <sec id="sec0140">
      <label>4.3</label>
      <title><italic>Arabidopsis thaliana</italic> and <italic>Pseudomonas syringae</italic></title>
      <p id="p0290"><italic>P. syringae</italic> is a well-known pathogenic bacterium that infects a wide range of plant species, causing bacterial speck disease in <italic>A. thaliana</italic>. Understanding PPIs between these two species is vital for developing new methods of disease control. Our Deep-HPI-pred model achieved an accuracy of 97.9%, sensitivity of 95%, specificity of 93.7%, F1 score of 91.8%, precision of 92.4%, MCC of 94.8%, and AUC of 98.2% in predicting the PPIs between <italic>A. thaliana</italic> and <italic>P. syringae</italic>.</p>
    </sec>
    <sec id="sec0145">
      <label>4.4</label>
      <title><italic>Homo sapiens</italic> and SARS-Cov-2</title>
      <p id="p0295">Additionally, we also evaluated our model on the host-pathogen interactions between <italic>H. sapiens</italic> (human) and SARS-CoV-2 (virus), a critically important relationship in light of the recent global pandemic. Given the recent global impact of the COVID-19 pandemic, understanding the interactions between human proteins and SARS-CoV-2 viral proteins is of paramount importance. In our analysis of the interactions between <italic>H. sapiens</italic> and SARS-CoV-2, the Deep-HPI-pred model demonstrated its proficiency in capturing the complex relationships characteristic of host-pathogen interactions. Given the recent global impact of the COVID-19 pandemic, understanding the interactions between human proteins and SARS-CoV-2 viral proteins is of paramount importance. Our Deep-HPI-pred model showed excellent performance in predicting the interactions between human proteins and SARS-CoV-2 viral proteins, with an accuracy of 94.3%, sensitivity of 97.5%, specificity of 98.7%, F1 score of 96.5%, precision of 96.2%, MCC of 95.3%, and AUC of 97.8%. These results demonstrate the strong predictive power of our Deep-HPI-pred model in deciphering the intricate network of interactions that take place between human proteins and SARS-CoV-2 viral proteins.</p>
      <p id="p0300">The finding revealed potential interactions between SARS-CoV-2 proteins and human host factors, which may play a pivotal role in the viral life cycle, including entry, replication, and evasion of host immune defenses. Notably, the model predicts that the SARS-CoV-2 Envelope protein interact with human proteins involved in the endocytic pathway, a route well-documented for coronavirus entry into host cells <xref rid="bib42" ref-type="bibr">[42]</xref>. Such interactions could potentially facilitate viral entry by altering endosomal trafficking. Additionally, interactions have been predicted between the SARS-CoV-2 Membrane glycoprotein and proteins located in the ER-to-Golgi intermediate compartment (ERGIC), which is instrumental in viral assembly and trafficking <xref rid="bib43" ref-type="bibr">[43]</xref>. These predictions suggest a mechanism by which SARS-CoV-2 could hijack host cellular machinery to bolster its replication process. Further results also suggest potential interactions between the ORF1a polyprotein of SARS-CoV-2 and components of the human innate immune system, particularly proteins involved in interferon signaling. This aligns with findings such as those from McClainet al. <xref rid="bib44" ref-type="bibr">[44]</xref>, which indicate the virus's ability to modulate interferon-driven responses, a strategy that likely facilitates viral replication and contributes to pathogenesis. An extensive literature review was undertaken to provide context for each predicted interaction. While direct evidence of these specific interactions has not been previously documented, the literature corroborates the biological plausibility of the proposed mechanisms. For example, Hekman et al. <xref rid="bib45" ref-type="bibr">[45]</xref> demonstrated that the SARS-CoV-2 Nucleoprotein's interaction with host RNA processing bodies suggests a potential for viral modulation of host mRNA processing, an avenue supported by recent findings. Thus, our Deep-HPI-pred model has demonstrated a high level of accuracy and reliability in predicting host-pathogen interactions across diverse biological contexts, underscoring its potential as a valuable tool for researchers studying infectious diseases and host-pathogen interactions.</p>
      <p id="p0305">Additionally, in the comparative analysis of Deep-HPI-pred with existing servers such as Pred-HPI and DeepHPI, several key differences emerge that highlight the enhanced capabilities of Deep-HPI-pred. Specifically, Deep-HPI-pred has predicted a total of 9673 interactions between SARS-CoV-2 virus and human proteins (<bold>Supplementary File 3:</bold><xref rid="sec0175" ref-type="sec">Table S1</xref><bold>)</bold>. In contrast, PredHPI, focuses primarily on sequence-based methods for detecting host-pathogen interactions, has predicted 6654 interactions (<bold>Supplementary File 3:</bold>
<xref rid="sec0175" ref-type="sec">Table S2</xref><bold>)</bold>, and notably, these predictions do not include probability information for each interaction. The higher number of interactions predicted by Deep-HPI-pred suggests its increased sensitivity in detecting potential host-pathogen interactions, which is crucial for comprehensive understanding and exploration of SARS-CoV-2 pathogenesis. Moreover, Deep-HPI-pred provides probability scores for each predicted interaction, offering a quantifiable measure of confidence in the predictions. This feature is particularly valuable for researchers, as it allows prioritization of interactions based on their likelihood, facilitating targeted experimental validation. Such probability information is absent in the predictions made by Pred-HPI and DeepHPI, which limits the ability to assess the confidence level in each predicted interaction. Additionally, Deep-HPI-pred enhances the utility of its predictions by providing GO information. This inclusion allows for immediate biological interpretation of the interactions, offering insights into potential biological processes and molecular functions involved, thereby enriching the understanding of the interaction landscape. In comparison, Deep-HPI, another existing tool in this domain, does not provide specific interaction results in a format that is directly comparable to Deep-HPI-pred and Pred-HPI. Therefore, the comparison focuses on the methodological approach and the theoretical framework of these models.</p>
      <p id="p0310">In the similar vein, GreeningDB, another host-pathogen interaction database, offers a more specialized scope, focusing specifically on citrus greening disease (Huanglongbing or HLB). It compiles data primarily relevant to HLB, including genomic, transcriptomic, and proteomic information. While invaluable for HLB research, GreeningDB's utility is confined to this particular disease. Deep-HPI-pred, however, extends its applicability beyond a singular disease context, enabling broader investigations of HPIs across multiple biological systems. This key distinction underscores Deep-HPI-pred's potential as a versatile platform for a more generalized understanding of host-pathogen dynamics, applicable to a diverse range of infectious diseases.</p>
      <p id="p0315">Another recent study by Yang et al. <xref rid="bib46" ref-type="bibr">[46]</xref> employed a transfer learning approach using multi-scale convolutional neural layers to predict human-virus PPIs. While this approach effectively captures the complex features of protein sequences, Deep-HPI-pred extends beyond sequence-based predictions. It integrates topological features and GO information, offering a more holistic view of the interaction landscape. Additionally, Deep-HPI-pred's application is not limited to human-virus interactions but encompasses a broader range of host-pathogen systems, demonstrating its versatile and comprehensive predictive capabilities. Similarly, in comparison with DeepViral <xref rid="bib47" ref-type="bibr">[47]</xref>, a deep learning-based method for predicting novel virus-host interactions from protein sequences and infectious disease phenotypes, Deep-HPI-pred showcases distinct advantages. DeepViral focuses on novel virus-host interaction predictions using protein sequences and disease phenotypes, leveraging a deep learning approach. While DeepViral's integration of infectious disease phenotypes offers a unique perspective, Deep-HPI-pred's methodology is distinguished by its utilization of MLP models based on a comprehensive evaluation of topological features and neural network architectures. This not only enhances the prediction accuracy but also provides a more detailed understanding of the underlying protein interaction mechanisms. Furthermore, Deep-HPI-pred's consistent performance across diverse host-pathogen systems, including plant-pathogens, human-viruses, and animal-bacteria, as evidenced by its high accuracy rates, illustrates its robustness and adaptability in various biological contexts.</p>
      <p id="p0320">To sum up, Deep-HPI-pred stands out in its ability to not only predict a larger number of host-pathogen interactions but also to provide critical additional information like probability scores and GO annotations. These features significantly contribute to its utility as a research tool, offering a more nuanced and informed approach to exploring host-pathogen interactions compared to tools like Pred-HPI, Deep-HPI, and DeepViral. The advanced algorithms utilized by Deep-HPI-pred enable the analysis of extensive interaction datasets, highlighting proteins that are central and often critical in these processes. This analytical capability is instrumental in deepening our understanding of the molecular mechanisms of viral infections, as supported by studies like those conducted by Barman et al. <xref rid="bib48" ref-type="bibr">[48]</xref>, which employed state-of-art techniques to identify key viral interaction proteins. Furthermore, the fusion of biological network analysis with deep learning heralds a transformative era in clinical and personalized medicine, particularly in managing viral diseases. This integration enables the unraveling of complex host-pathogen interactions at a molecular level, paving the way for targeted therapeutic strategies and more individualized treatment approaches in combating viral infections.</p>
    </sec>
  </sec>
  <sec id="sec0150">
    <label>5</label>
    <title>Conclusion</title>
    <p id="p0325">Traditional experimental techniques for host-pathogen interaction prediction, though effective, have proven to be labor-intensive, expensive, and time-consuming. To address this challenge, we have introduced Deep-HPI-pred, an R/Shiny application that provides a computational approach for predicting hitherto unmapped interactions between host and pathogen proteins. By harnessing the power of network-driven feature learning, Deep-HPI-pred, as demonstrated through our case study using citrus and <italic>C</italic>Las bacteria training sets, offers a promising alternative for accelerating the discovery of PPIs. In our research, we employed a comprehensive evaluation of various neural network architectures and topological features, the results of which led us to adopt the MLP models for HPI prediction. Notably, the MLP model using the Eigenvector Centrality topological feature exhibited exemplary performance, achieving an overall MCC value exceeding 0.80 when tested on independent validation datasets. Beyond its capacity for interaction prediction, Deep-HPI-pred further enriches our understanding of the dynamics within host-pathogen interactions by providing GO term information for each protein. This added layer of information presents an insightful view of the system and enhances our comprehension of the overall biological processes at play. Furthermore, in the benchmarking studies conducted, the Deep-HPI-pred model demonstrated its robustness and reliability across various host-pathogen systems. The model exhibited a remarkable performance in predicting interactions between different host-pathogen pairs, including plant-virus, human-virus, plant-bacteria, and animal-bacteria. Specifically, the model achieved an accuracy of 98.4% and 97.9% for plant-pathogen interactions, 94.3% for human-virus interactions, and 96.6% for animal-bacteria interactions. These results not only validate the efficacy of our model but also highlight its potential as a versatile and comprehensive tool for understanding the complex dynamics of host-pathogen interactions across different biological systems. While MLPs have demonstrated robust performance in this study, their structure is not inherently optimized for contrastive learning, which is increasingly recognized for its efficacy in unsupervised and semi-supervised scenarios. This limitation suggests a potential area for future improvement of the model, where integrating contrastive learning techniques could expand its capabilities to handle and learn from the vast amounts of unlabeled data in biological research. Such enhancements would not only address a key limitation but also significantly enrich the model’s utility in understanding and predicting complex host-pathogen interactions. In conclusion, the introduction of Deep-HPI-pred represents a significant stride in the field of bioinformatics. By integrating detection and visualization of interaction networks into a single user-friendly platform, it equips researchers with a powerful tool for understanding both model and non-model host-pathogen systems. This advancement is expected to aid in the generation of hypotheses, the design of appropriate experiments, and ultimately, in the development of disease control and prevention strategies.</p>
  </sec>
  <sec sec-type="data-availability" id="sec0155">
    <title>Availability of data and materials</title>
    <p id="p0330">Project name: Deep-HPI-pred. Project home page: <ext-link ext-link-type="uri" xlink:href="https://cbi.gxu.edu.cn/shiny-apps/Deep-HPI-pred/" id="ir0015">https://cbi.gxu.edu.cn/shiny-apps/Deep-HPI-pred/</ext-link>. Programming language: R and HTML. The resources and data used during the current study are available in the GitHub repository, <ext-link ext-link-type="uri" xlink:href="https://github.com/tahirulqamar/Deep-HPI-pred" id="ir0020">https://github.com/tahirulqamar/Deep-HPI-pred</ext-link>.</p>
  </sec>
  <sec id="sec0160">
    <title>Funding</title>
    <p id="p0335">This work was supported by the Starting Research Grant for <funding-source id="gs1">High-level Talents from Guangxi University</funding-source> and <funding-source id="gs2">Postdoctoral research platform grant of Guangxi University</funding-source>.</p>
  </sec>
  <sec id="sec0165">
    <title>CRediT authorship contribution statement</title>
    <p id="p0340"><bold>Muhammad Tahir ul Qamar and Fatima Noor</bold>: Data Curation, Methodology, Software, Formal Analysis, Investigation, Visualization, Writing - Original Draft. <bold>Yi-Xiong Guo and Xi-Tong Zhu</bold>: Software, Validation, Writing - Review &amp; Editing. <bold>Ling-Ling Chen</bold>: Conceptualization, Resources, Supervision, Project administration, Funding acquisition, Writing - Review &amp; Editing.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="p0345">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="bibliog0005">
    <title>References</title>
    <ref id="bib1">
      <label>1</label>
      <element-citation publication-type="journal" id="sbref1">
        <person-group person-group-type="author">
          <name>
            <surname>Kuo</surname>
            <given-names>Z.-Y.</given-names>
          </name>
          <name>
            <surname>Chuang</surname>
            <given-names>Y.-J.</given-names>
          </name>
          <name>
            <surname>Chao</surname>
            <given-names>C.-C.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>F.-C.</given-names>
          </name>
          <name>
            <surname>Lan</surname>
            <given-names>C.-Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>B.-S.</given-names>
          </name>
        </person-group>
        <article-title>Identification of infection-and defense-related genes via a dynamic host-pathogen interaction network using a Candida albicans-zebrafish infection model</article-title>
        <source>J innate Immun</source>
        <volume>5</volume>
        <issue>2</issue>
        <year>2013</year>
        <fpage>137</fpage>
        <lpage>152</lpage>
        <pub-id pub-id-type="pmid">23406717</pub-id>
      </element-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="journal" id="sbref2">
        <person-group person-group-type="author">
          <name>
            <surname>Garbutt</surname>
            <given-names>C.C.</given-names>
          </name>
          <name>
            <surname>Bangalore</surname>
            <given-names>P.V.</given-names>
          </name>
          <name>
            <surname>Kannar</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Mukhtar</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Getting to the edge: protein dynamical networks as a new frontier in plant–microbe interactions</article-title>
        <source>Front Plant Sci</source>
        <volume>5</volume>
        <year>2014</year>
        <fpage>312</fpage>
        <pub-id pub-id-type="pmid">25071795</pub-id>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <mixed-citation publication-type="other" id="othref0005">J.J. Da Graça, L.Korsten. Citrus huanglongbing: Review, present status and future strategies. In: Naqvi SAMH, editor. Diseases of Fruits and Vegetables Volume I: Diagnosis and Management. The Netherlands: Kluwer Academic; (2004). pp. 229–45.</mixed-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="journal" id="sbref3">
        <person-group person-group-type="author">
          <name>
            <surname>Andrade</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>Candidatus Liberibacter asiaticus: virulence traits and control strategies</article-title>
        <source>Trop Plant Pathol</source>
        <volume>45</volume>
        <year>2020</year>
        <fpage>285</fpage>
        <lpage>297</lpage>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="journal" id="sbref4">
        <person-group person-group-type="author">
          <name>
            <surname>Pandey</surname>
            <given-names>S.S.</given-names>
          </name>
          <name>
            <surname>Hendrich</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Andrade</surname>
            <given-names>M.O.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>Candidatus Liberibacter: From movement, host responses, to symptom development of citrus Huanglongbing</article-title>
        <source>Phytopathology®</source>
        <volume>112</volume>
        <issue>1</issue>
        <year>2022</year>
        <fpage>55</fpage>
        <lpage>68</lpage>
        <pub-id pub-id-type="pmid">34609203</pub-id>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="book" id="sbref5">
        <person-group person-group-type="author">
          <name>
            <surname>Hoddle</surname>
            <given-names>M.S.</given-names>
          </name>
          <name>
            <surname>Hoddle</surname>
            <given-names>C.D.</given-names>
          </name>
          <name>
            <surname>Morgan</surname>
            <given-names>D.J.</given-names>
          </name>
          <name>
            <surname>Milosavljević</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <part-title>Successful Biological Control of Asian Citrus Psyllid, Diaphorina citri, in California</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Van Driesche</surname>
            <given-names>R.G.</given-names>
          </name>
          <name>
            <surname>Winston</surname>
            <given-names>R.L.</given-names>
          </name>
          <name>
            <surname>Perring</surname>
            <given-names>T.M.</given-names>
          </name>
          <name>
            <surname>Lopez</surname>
            <given-names>V.M.</given-names>
          </name>
        </person-group>
        <source>Contributions of Classical Biological Control to the US Food Security</source>
        <year>2022</year>
        <publisher-name>Forestry, and Biodiversity. Washington</publisher-name>
        <publisher-loc>USDA FHAAST</publisher-loc>
        <fpage>127</fpage>
        <lpage>145</lpage>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="journal" id="sbref6">
        <person-group person-group-type="author">
          <name>
            <surname>Yuan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Xin</surname>
            <given-names>X.F.</given-names>
          </name>
        </person-group>
        <article-title>Bacterial Infection and Hypersensitive Response Assays in Arabidopsis-Pseudomonas syringae Pathosystem</article-title>
        <source>Bio Protoc</source>
        <volume>11</volume>
        <issue>24</issue>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">e4268</object-id>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="journal" id="sbref7">
        <person-group person-group-type="author">
          <name>
            <surname>Dyer</surname>
            <given-names>M.D.</given-names>
          </name>
          <name>
            <surname>Neff</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Dufford</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Rivera</surname>
            <given-names>C.G.</given-names>
          </name>
          <name>
            <surname>Shattuck</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Bassaganya-Riera</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The human-bacterial pathogen protein interaction networks of Bacillus anthracis, Francisella tularensis, and Yersinia pestis</article-title>
        <source>PloS One</source>
        <volume>5</volume>
        <issue>8</issue>
        <year>2010</year>
        <object-id pub-id-type="publisher-id">e12089</object-id>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="journal" id="sbref8">
        <person-group person-group-type="author">
          <name>
            <surname>Lian</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Current status and future perspectives of computational studies on human–virus protein–protein interactions</article-title>
        <source>Brief Bioinforma</source>
        <volume>22</volume>
        <issue>5</issue>
        <year>2021</year>
        <fpage>bbab029</fpage>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="journal" id="sbref9">
        <person-group person-group-type="author">
          <name>
            <surname>Westermann</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Cross-species</surname>
            <given-names>J.Vogel</given-names>
          </name>
        </person-group>
        <article-title>RNA-seq for deciphering host–microbe interactions</article-title>
        <source>Nat Rev Genet</source>
        <volume>22</volume>
        <issue>6</issue>
        <year>2021</year>
        <fpage>361</fpage>
        <lpage>378</lpage>
        <pub-id pub-id-type="pmid">33597744</pub-id>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="journal" id="sbref10">
        <person-group person-group-type="author">
          <name>
            <surname>Balotf</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Tegg</surname>
            <given-names>R.S.</given-names>
          </name>
          <name>
            <surname>Nichols</surname>
            <given-names>D.S.</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>C.R.</given-names>
          </name>
        </person-group>
        <article-title>Shotgun proteomics as a powerful tool for the study of the proteomes of plants, their pathogens, and plant–pathogen interactions</article-title>
        <source>Proteomes</source>
        <volume>10</volume>
        <issue>1</issue>
        <year>2022</year>
        <fpage>5</fpage>
        <pub-id pub-id-type="pmid">35225985</pub-id>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="journal" id="sbref11">
        <person-group person-group-type="author">
          <name>
            <surname>Jain</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mittal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Tripathi</surname>
            <given-names>L.P.</given-names>
          </name>
          <name>
            <surname>Nussinov</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Ahmad</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Host-pathogen protein-nucleic acid interactions: A comprehensive review</article-title>
        <source>Comput Struct Biotechnol J</source>
        <year>2022</year>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="journal" id="sbref12">
        <person-group person-group-type="author">
          <name>
            <surname>Kaundal</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Loaiza</surname>
            <given-names>C.D.</given-names>
          </name>
          <name>
            <surname>Duhan</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Flann</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>deepHPI: a comprehensive deep learning platform for accurate prediction and visualization of host–pathogen protein–protein interactions</article-title>
        <source>Brief Bioinforma</source>
        <volume>23</volume>
        <issue>3</issue>
        <year>2022</year>
        <fpage>bbac125</fpage>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>14</label>
      <element-citation publication-type="journal" id="sbref13">
        <person-group person-group-type="author">
          <name>
            <surname>Loaiza</surname>
            <given-names>C.D.</given-names>
          </name>
          <name>
            <surname>Kaundal</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>PredHPI: an integrated web server platform for the detection and visualization of host–pathogen interactions using sequence-based methods</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <issue>5</issue>
        <year>2021</year>
        <fpage>622</fpage>
        <lpage>624</lpage>
        <pub-id pub-id-type="pmid">33027504</pub-id>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>15</label>
      <element-citation publication-type="journal" id="sbref14">
        <person-group person-group-type="author">
          <name>
            <surname>Loaiza</surname>
            <given-names>C.D.</given-names>
          </name>
          <name>
            <surname>Duhan</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Kaundal</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>GreeningDB: A Database of Host–Pathogen Protein–Protein Interactions and Annotation Features of the Bacteria Causing Huanglongbing HLB Disease</article-title>
        <source>Int J Mol Sci</source>
        <volume>22</volume>
        <issue>19</issue>
        <year>2021</year>
        <fpage>10897</fpage>
        <pub-id pub-id-type="pmid">34639237</pub-id>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>16</label>
      <element-citation publication-type="journal" id="sbref15">
        <person-group person-group-type="author">
          <name>
            <surname>Singhal</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Resat</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>A domain-based approach to predict protein-protein interactions</article-title>
        <source>BMC Bioinform</source>
        <volume>8</volume>
        <year>2007</year>
        <fpage>1</fpage>
        <lpage>19</lpage>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>17</label>
      <element-citation publication-type="journal" id="sbref16">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Chi</surname>
            <given-names>C.-H.</given-names>
          </name>
          <name>
            <surname>Kurgan</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Systematic evaluation of machine learning methods for identifying human–pathogen protein–protein interactions</article-title>
        <source>Brief Bioinforma</source>
        <volume>22</volume>
        <issue>3</issue>
        <year>2021</year>
        <fpage>bbaa068</fpage>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>18</label>
      <element-citation publication-type="journal" id="sbref17">
        <person-group person-group-type="author">
          <name>
            <surname>Scott</surname>
            <given-names>M.S.</given-names>
          </name>
          <name>
            <surname>Probabilistic</surname>
            <given-names>G.J. Barton</given-names>
          </name>
        </person-group>
        <article-title>prediction and ranking of human protein-protein interactions</article-title>
        <source>BMC Bioinform</source>
        <volume>8</volume>
        <issue>1</issue>
        <year>2007</year>
        <fpage>1</fpage>
        <lpage>21</lpage>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>19</label>
      <element-citation publication-type="journal" id="sbref18">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>Y.D.</given-names>
          </name>
        </person-group>
        <article-title>Protein‐protein interaction networks as miners of biological discovery</article-title>
        <source>Proteomics</source>
        <volume>22</volume>
        <issue>15-16</issue>
        <year>2022</year>
        <fpage>2100190</fpage>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>20</label>
      <element-citation publication-type="journal" id="sbref19">
        <person-group person-group-type="author">
          <name>
            <surname>Csardi</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Nepusz</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>The igraph software package for complex network research</article-title>
        <source>Inter, Complex Syst</source>
        <volume>1695</volume>
        <issue>5</issue>
        <year>2006</year>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>21</label>
      <element-citation publication-type="journal" id="sbref20">
        <person-group person-group-type="author">
          <name>
            <surname>Pržulj</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Wigle</surname>
            <given-names>D.A.</given-names>
          </name>
          <name>
            <surname>Jurisica</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <article-title>Functional topology in a network of protein interactions</article-title>
        <source>Bioinformatics</source>
        <volume>20</volume>
        <issue>3</issue>
        <year>2004</year>
        <fpage>340</fpage>
        <lpage>348</lpage>
        <pub-id pub-id-type="pmid">14960460</pub-id>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>22</label>
      <element-citation publication-type="journal" id="sbref21">
        <person-group person-group-type="author">
          <name>
            <surname>Ashtiani</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Salehzadeh-Yazdi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Razaghi-Moghadam</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Hennig</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Wolkenhauer</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Mirzaie</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Jafari</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>A systematic survey of centrality measures for protein-protein interaction networks. BMC Sys</article-title>
        <source>Biol</source>
        <volume>12</volume>
        <issue>1</issue>
        <year>2018</year>
        <fpage>1</fpage>
        <lpage>17</lpage>
      </element-citation>
    </ref>
    <ref id="bib23">
      <label>23</label>
      <element-citation publication-type="journal" id="sbref22">
        <person-group person-group-type="author">
          <name>
            <surname>Eryilmaz</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Pax</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>O’Neill</surname>
            <given-names>A.G.</given-names>
          </name>
          <name>
            <surname>Vangel</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Diez</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Holt</surname>
            <given-names>D.J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Network hub centrality and working memory performance in schizophrenia</article-title>
        <source>Schizophrenia</source>
        <volume>8</volume>
        <issue>1</issue>
        <year>2022</year>
        <object-id pub-id-type="publisher-id">76</object-id>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>24</label>
      <mixed-citation publication-type="other" id="othref0010">A. Ali, V.R. Hulipalled, S. Patil, editors. Centrality measure analysis on protein interaction networks. 2020 IEEE International Conference on Technology, Engineering, Management for Societal impact using Marketing, Entrepreneurship and Talent (TEMSMET); 2020: IEEE.</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25</label>
      <mixed-citation publication-type="other" id="othref0015">V. Chaubey, M.S. Nair, G.N. Pillai, editors. Gene expression prediction using a deep 1D convolution neural network. 2019 IEEE Symposium Series on Computational Intelligence (SSCI); 2019: IEEE.</mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26</label>
      <element-citation publication-type="journal" id="sbref23">
        <person-group person-group-type="author">
          <name>
            <surname>Patiyal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Dhall</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Raghava</surname>
            <given-names>G.P.</given-names>
          </name>
        </person-group>
        <article-title>A deep learning-based method for the prediction of DNA interacting residues in a protein</article-title>
        <source>Brief Bioinforma</source>
        <volume>23</volume>
        <issue>5</issue>
        <year>2022</year>
        <fpage>bbac322</fpage>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>27</label>
      <element-citation publication-type="journal" id="sbref24">
        <person-group person-group-type="author">
          <name>
            <surname>Fang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Moriwaki</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Shimizu</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of antifungal peptides by deep learning with character embedding</article-title>
        <source>IPSJ Trans Bioinforma</source>
        <volume>12</volume>
        <year>2019</year>
        <fpage>21</fpage>
        <lpage>29</lpage>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>28</label>
      <element-citation publication-type="journal" id="sbref25">
        <person-group person-group-type="author">
          <name>
            <surname>Arnold</surname>
            <given-names>T.B.</given-names>
          </name>
        </person-group>
        <article-title>kerasR: R Interface to the Keras Deep Learning Library</article-title>
        <source>J Open Source Softw,2</source>
        <issue>14</issue>
        <year>2017</year>
        <fpage>296</fpage>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>29</label>
      <element-citation publication-type="journal" id="sbref26">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lian</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wuchty</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Transfer learning via multi-scale convolutional neural layers for human–virus protein–protein interaction prediction</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <issue>24</issue>
        <year>2021</year>
        <fpage>4771</fpage>
        <lpage>4778</lpage>
        <pub-id pub-id-type="pmid">34273146</pub-id>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>30</label>
      <element-citation publication-type="journal" id="sbref27">
        <person-group person-group-type="author">
          <name>
            <surname>Grandini</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bagli</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Visani</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Metrics for multi-class classification: an overview</article-title>
        <comment>200805756</comment>
        <source>arXiv Prepr arXiv</source>
        <year>2020</year>
        <comment>200805756</comment>
      </element-citation>
    </ref>
    <ref id="bib31">
      <label>31</label>
      <element-citation publication-type="journal" id="sbref28">
        <person-group person-group-type="author">
          <name>
            <surname>Powers</surname>
            <given-names>D.M.</given-names>
          </name>
        </person-group>
        <article-title>Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation</article-title>
        <comment>201016061</comment>
        <source>arXiv Prepr arXiv</source>
        <year>2020</year>
        <comment>201016061</comment>
      </element-citation>
    </ref>
    <ref id="bib32">
      <label>32</label>
      <element-citation publication-type="journal" id="sbref29">
        <person-group person-group-type="author">
          <name>
            <surname>Baldi</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Brunak</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Chauvin</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Andersen</surname>
            <given-names>C.A.</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Assessing the accuracy of prediction algorithms for classification: an overview</article-title>
        <source>Bioinformatics</source>
        <volume>16</volume>
        <issue>5</issue>
        <year>2000</year>
        <fpage>412</fpage>
        <lpage>424</lpage>
        <pub-id pub-id-type="pmid">10871264</pub-id>
      </element-citation>
    </ref>
    <ref id="bib33">
      <label>33</label>
      <element-citation publication-type="journal" id="sbref30">
        <person-group person-group-type="author">
          <name>
            <surname>Johnson</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Khoshgoftaar</surname>
            <given-names>T.M.</given-names>
          </name>
        </person-group>
        <article-title>Survey on deep learning with class imbalance</article-title>
        <source>J Big Data</source>
        <volume>6</volume>
        <issue>1</issue>
        <year>2019</year>
        <fpage>1</fpage>
        <lpage>54</lpage>
      </element-citation>
    </ref>
    <ref id="bib34">
      <label>34</label>
      <element-citation publication-type="journal" id="sbref31">
        <person-group person-group-type="author">
          <name>
            <surname>Broadley</surname>
            <given-names>R.W.</given-names>
          </name>
          <name>
            <surname>Klenk</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Thies</surname>
            <given-names>S.B.</given-names>
          </name>
          <name>
            <surname>Kenney</surname>
            <given-names>L.P.</given-names>
          </name>
          <name>
            <surname>Granat</surname>
            <given-names>M.H.</given-names>
          </name>
        </person-group>
        <article-title>Methods for the real-world evaluation of fall detection technology: A scoping review</article-title>
        <source>Sensors</source>
        <volume>18</volume>
        <issue>7</issue>
        <year>2018</year>
        <fpage>2060</fpage>
        <pub-id pub-id-type="pmid">29954155</pub-id>
      </element-citation>
    </ref>
    <ref id="bib35">
      <label>35</label>
      <element-citation publication-type="journal" id="sbref32">
        <person-group person-group-type="author">
          <name>
            <surname>Alves</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Castro</surname>
            <given-names>G.Z.</given-names>
          </name>
          <name>
            <surname>Oliveira</surname>
            <given-names>B.A.S.</given-names>
          </name>
          <name>
            <surname>Ferreira</surname>
            <given-names>L.A.</given-names>
          </name>
          <name>
            <surname>Ramírez</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Silva</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Guimarães</surname>
            <given-names>F.G.</given-names>
          </name>
        </person-group>
        <article-title>Explaining machine learning based diagnosis of COVID-19 from routine blood tests with decision trees and criteria graphs</article-title>
        <source>Comput Biol Med</source>
        <volume>132</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">104335</object-id>
      </element-citation>
    </ref>
    <ref id="bib36">
      <label>36</label>
      <element-citation publication-type="journal" id="sbref33">
        <person-group person-group-type="author">
          <name>
            <surname>Chicco</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Jurman</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation</article-title>
        <source>BMC Genom</source>
        <volume>21</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>13</lpage>
      </element-citation>
    </ref>
    <ref id="bib37">
      <label>37</label>
      <element-citation publication-type="journal" id="sbref34">
        <person-group person-group-type="author">
          <name>
            <surname>Kakkar</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>goyal</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Johri</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Artificial Intelligence-Based Approaches for Detection and Classification of Different Classes of Malaria Parasites Using Microscopic Images: A Systematic Review</article-title>
        <source>Arch Comput Methods Eng</source>
        <year>2023</year>
        <fpage>1</fpage>
        <lpage>20</lpage>
      </element-citation>
    </ref>
    <ref id="bib38">
      <label>38</label>
      <element-citation publication-type="journal" id="sbref35">
        <person-group person-group-type="author">
          <name>
            <surname>Lei</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wan</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A deep-learning framework for multi-level peptide–protein interaction prediction</article-title>
        <source>Nat Commun</source>
        <volume>12</volume>
        <issue>1</issue>
        <year>2021</year>
        <fpage>5465</fpage>
        <pub-id pub-id-type="pmid">34526500</pub-id>
      </element-citation>
    </ref>
    <ref id="bib39">
      <label>39</label>
      <element-citation publication-type="journal" id="sbref36">
        <person-group person-group-type="author">
          <name>
            <surname>Negre</surname>
            <given-names>C.F.</given-names>
          </name>
          <name>
            <surname>Morzan</surname>
            <given-names>U.N.</given-names>
          </name>
          <name>
            <surname>Hendrickson</surname>
            <given-names>H.P.</given-names>
          </name>
          <name>
            <surname>Pal</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lisi</surname>
            <given-names>G.P.</given-names>
          </name>
          <name>
            <surname>Loria</surname>
            <given-names>J.P.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Eigenvector centrality for characterization of protein allosteric pathways</article-title>
        <source>Proc Natl Acad Sci USA</source>
        <volume>115</volume>
        <issue>52</issue>
        <year>2018</year>
        <comment>E12201-E8</comment>
      </element-citation>
    </ref>
    <ref id="bib40">
      <label>40</label>
      <element-citation publication-type="journal" id="sbref37">
        <person-group person-group-type="author">
          <name>
            <surname>Khorsand</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Savadi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Naghibzadeh</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Comprehensive host-pathogen protein-protein interaction network analysis</article-title>
        <source>BMC Bioinform</source>
        <volume>21</volume>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>22</lpage>
      </element-citation>
    </ref>
    <ref id="bib41">
      <label>41</label>
      <element-citation publication-type="journal" id="sbref38">
        <person-group person-group-type="author">
          <name>
            <surname>Cui</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Stanley</surname>
            <given-names>H.E.</given-names>
          </name>
        </person-group>
        <article-title>Discovering disease-associated genes in weighted protein–protein interaction networks</article-title>
        <source>Phys A: Stat Mech its Appl</source>
        <volume>496</volume>
        <year>2018</year>
        <fpage>53</fpage>
        <lpage>61</lpage>
      </element-citation>
    </ref>
    <ref id="bib42">
      <label>42</label>
      <element-citation publication-type="journal" id="sbref39">
        <person-group person-group-type="author">
          <name>
            <surname>Hoffmann</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kleine-Weber</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Schroeder</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Krüger</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Herrler</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Erichsen</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SARS-CoV-2 cell entry depends on ACE2 and TMPRSS2 and is blocked by a clinically proven protease inhibitor</article-title>
        <source>cell</source>
        <volume>181</volume>
        <issue>2</issue>
        <year>2020</year>
        <fpage>271</fpage>
        <lpage>280</lpage>
        <comment>e8</comment>
        <pub-id pub-id-type="pmid">32142651</pub-id>
      </element-citation>
    </ref>
    <ref id="bib43">
      <label>43</label>
      <element-citation publication-type="journal" id="sbref40">
        <person-group person-group-type="author">
          <name>
            <surname>Cortese</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Laketa</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>Advanced microscopy technologies enable rapid response to SARS‐CoV‐2 pandemic</article-title>
        <source>Cell Microbiol</source>
        <volume>23</volume>
        <issue>7</issue>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">e13319</object-id>
      </element-citation>
    </ref>
    <ref id="bib44">
      <label>44</label>
      <element-citation publication-type="journal" id="sbref41">
        <person-group person-group-type="author">
          <name>
            <surname>McClain</surname>
            <given-names>M.T.</given-names>
          </name>
          <name>
            <surname>Constantine</surname>
            <given-names>F.J.</given-names>
          </name>
          <name>
            <surname>Henao</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Tsalik</surname>
            <given-names>E.L.</given-names>
          </name>
          <name>
            <surname>Burke</surname>
            <given-names>T.W.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Dysregulated transcriptional responses to SARS-CoV-2 in the periphery</article-title>
        <source>Nat Commun</source>
        <volume>12</volume>
        <issue>1</issue>
        <year>2021</year>
        <fpage>1079</fpage>
        <pub-id pub-id-type="pmid">33597532</pub-id>
      </element-citation>
    </ref>
    <ref id="bib45">
      <label>45</label>
      <element-citation publication-type="journal" id="sbref42">
        <person-group person-group-type="author">
          <name>
            <surname>Hekman</surname>
            <given-names>R.M.</given-names>
          </name>
          <name>
            <surname>Hume</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Goel</surname>
            <given-names>R.K.</given-names>
          </name>
          <name>
            <surname>Abo</surname>
            <given-names>K.M.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Blum</surname>
            <given-names>B.C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Actionable cytopathogenic host responses of human alveolar type 2 cells to SARS-CoV-2</article-title>
        <source>Mol Cell</source>
        <volume>80</volume>
        <issue>6</issue>
        <year>2020</year>
        <fpage>1104</fpage>
        <lpage>1122</lpage>
        <comment>e9</comment>
        <pub-id pub-id-type="pmid">33259812</pub-id>
      </element-citation>
    </ref>
    <ref id="bib46">
      <label>46</label>
      <element-citation publication-type="journal" id="sbref43">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lian</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wuchty</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Transfer learning via multi-scale convolutional neural layers for human-virus protein-protein interaction prediction</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <issue>24</issue>
        <year>2021</year>
        <fpage>4771</fpage>
        <lpage>4778</lpage>
        <pub-id pub-id-type="pmid">34273146</pub-id>
      </element-citation>
    </ref>
    <ref id="bib47">
      <label>47</label>
      <element-citation publication-type="journal" id="sbref44">
        <person-group person-group-type="author">
          <name>
            <surname>Liu-Wei</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Kafkas</surname>
            <given-names>Ş.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Dimonaco</surname>
            <given-names>N.J.</given-names>
          </name>
          <name>
            <surname>Tegnér</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hoehndorf</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>DeepViral: prediction of novel virus-host interactions from protein sequences and infectious disease phenotypes</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <issue>17</issue>
        <year>2021</year>
        <fpage>2722</fpage>
        <lpage>2729</lpage>
        <pub-id pub-id-type="pmid">33682875</pub-id>
      </element-citation>
    </ref>
    <ref id="bib48">
      <label>48</label>
      <element-citation publication-type="journal" id="sbref45">
        <person-group person-group-type="author">
          <name>
            <surname>Barman</surname>
            <given-names>R.K.</given-names>
          </name>
          <name>
            <surname>Saha</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of interactions between viral and host proteins using supervised machine learning methods</article-title>
        <source>PLoS One</source>
        <volume>9</volume>
        <issue>11</issue>
        <year>2014</year>
        <object-id pub-id-type="publisher-id">e112034</object-id>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="sec0175" sec-type="supplementary-material">
    <label>Appendix A</label>
    <title>Supplementary material</title>
    <p id="p0355"><supplementary-material content-type="local-data" id="ec0005"><caption><p>Supplementary material</p></caption><media xlink:href="mmc1.xlsx"/></supplementary-material>.</p>
    <p id="p0360"><supplementary-material content-type="local-data" id="ec0010"><caption><p>Supplementary material</p></caption><media xlink:href="mmc2.xlsx"/></supplementary-material>.</p>
    <p id="p0365"><supplementary-material content-type="local-data" id="ec0015"><caption><p>Supplementary material</p></caption><media xlink:href="mmc3.xlsx"/></supplementary-material>.</p>
  </sec>
  <fn-group>
    <fn id="sec0170" fn-type="supplementary-material">
      <label>Appendix A</label>
      <p id="p0350">Supplementary data associated with this article can be found in the online version at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.csbj.2023.12.010" id="ir0025">doi:10.1016/j.csbj.2023.12.010</ext-link>.</p>
    </fn>
  </fn-group>
</back>
