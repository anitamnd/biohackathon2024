<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 2?>
<front>
  <?epub September-24-2012?>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nucleic Acids Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nucleic Acids Res</journal-id>
    <journal-id journal-id-type="publisher-id">nar</journal-id>
    <journal-id journal-id-type="hwp">nar</journal-id>
    <journal-title-group>
      <journal-title>Nucleic Acids Research</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0305-1048</issn>
    <issn pub-type="epub">1362-4962</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">3592496</article-id>
    <article-id pub-id-type="pmid">23012261</article-id>
    <article-id pub-id-type="doi">10.1093/nar/gks878</article-id>
    <article-id pub-id-type="publisher-id">gks878</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methods Online</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Heterogeneous ensemble approach with discriminative features and
modified-SMOTEbagging for pre-miRNA classification</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Lertampaiporn</surname>
          <given-names>Supatcha</given-names>
        </name>
        <xref ref-type="aff" rid="gks878-AFF1">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Thammarongtham</surname>
          <given-names>Chinae</given-names>
        </name>
        <xref ref-type="aff" rid="gks878-AFF1">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nukoolkit</surname>
          <given-names>Chakarida</given-names>
        </name>
        <xref ref-type="aff" rid="gks878-AFF1">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kaewkamnerdpong</surname>
          <given-names>Boonserm</given-names>
        </name>
        <xref ref-type="aff" rid="gks878-AFF1">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ruengjitchatchawalya</surname>
          <given-names>Marasri</given-names>
        </name>
        <xref ref-type="aff" rid="gks878-AFF1">
          <sup>4</sup>
        </xref>
        <xref ref-type="aff" rid="gks878-AFF1">
          <sup>5</sup>
        </xref>
        <xref ref-type="corresp" rid="gks878-COR1">*</xref>
      </contrib>
    </contrib-group>
    <aff id="gks878-AFF1"><sup>1</sup>Biological Engineering Program, King Mongkut’s
University of Technology Thonburi, Bang Mod, Thung Khru, Bangkok, 10140,
<sup>2</sup>Biochemical Engineering and Pilot Plant Research and Development Unit, National
Center for Genetic Engineering and Biotechnology at King Mongkut’s University of
Technology Thonburi, Bang Khun Thian, Bangkok, 10150, <sup>3</sup>School of Information
Technology, King Mongkut’s University of Technology Thonburi, Bang Mod, Thung Khru,
Bangkok, 10140, <sup>4</sup>School of Bioresources and Technology and
<sup>5</sup>Bioinformatics and Systems Biology Program, King Mongkut’s University of
Technology Thonburi, Bang Khun Thian, Bangkok, 10150, Thailand</aff>
    <author-notes>
      <corresp id="gks878-COR1">*To whom correspondence should be addressed. Tel:
<phone>+66 2 470 7481</phone>; Fax: <fax>+66 2 452 3455</fax>; Email:
<email>marasri.rue@kmutt.ac.th</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>1</month>
      <year>2013</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>24</day>
      <month>9</month>
      <year>2012</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>9</month>
      <year>2012</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the
							<pub-date pub-type="epub"/>. -->
    <volume>41</volume>
    <issue>1</issue>
    <fpage>e21</fpage>
    <lpage>e21</lpage>
    <history>
      <date date-type="received">
        <day>29</day>
        <month>3</month>
        <year>2012</year>
      </date>
      <date date-type="accepted">
        <day>26</day>
        <month>8</month>
        <year>2012</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2012. Published by Oxford University
Press.</copyright-statement>
      <copyright-year>2012</copyright-year>
      <license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/3.0">
        <license-p><!--CREATIVE COMMONS-->This is an Open Access article distributed under the terms of the Creative Commons
Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/">http://creativecommons.org/licenses/by/3.0/</ext-link>), which permits
unrestricted, distribution, and reproduction in any medium, provided the original work
is properly cited.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>An ensemble classifier approach for microRNA precursor (pre-miRNA) classification was
proposed based upon combining a set of heterogeneous algorithms including support vector
machine (SVM), k-nearest neighbors (kNN) and random forest (RF), then aggregating their
prediction through a voting system. Additionally, the proposed algorithm, the
classification performance was also improved using discriminative features,
self-containment and its derivatives, which have shown unique structural robustness
characteristics of pre-miRNAs. These are applicable across different species. By applying
preprocessing methods—both a correlation-based feature selection (CFS) with genetic
algorithm (GA) search method and a modified-Synthetic Minority Oversampling Technique
(SMOTE) bagging rebalancing method—improvement in the performance of this ensemble
was observed. The overall prediction accuracies obtained via 10 runs of 5-fold cross
validation (CV) was 96.54%, with sensitivity of 94.8% and specificity of
98.3%—this is better in trade-off sensitivity and specificity values than
those of other state-of-the-art methods. The ensemble model was applied to animal, plant
and virus pre-miRNA and achieved high accuracy, &gt;93%. Exploiting the
discriminative set of selected features also suggests that pre-miRNAs possess high
intrinsic structural robustness as compared with other stem loops. Our heterogeneous
ensemble method gave a relatively more reliable prediction than those using single
classifiers. Our program is available at <ext-link ext-link-type="uri" xlink:href="http://ncrna-pred.com/premiRNA.html">http://ncrna-pred.com/premiRNA.html</ext-link>.</p>
    </abstract>
    <counts>
      <page-count count="12"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>INTRODUCTION</title>
    <p>MicroRNAs (miRNAs) are small endogenous non-coding RNAs (≈19–25 nt). They play
crucial roles in post-transcriptional regulation of gene expression of plants and animals
(<xref ref-type="bibr" rid="gks878-B1">1</xref>). The miRNAs are expressed at different
levels during cell proliferation, metabolism, development, apoptosis and tumor metastasis
(<xref ref-type="bibr" rid="gks878-B1 gks878-B2">1–2</xref>). In animals, miRNA
biogenesis begins with the transcription of several-hundred-nucleotides-long primary
transcripts called primary miRNAs (pri-miRNAs). An enzyme called Drosha recognizes hairpin
substructures in the pri-miRNAs and cleaves them to produce ∼70-nt long miRNA stem-loop
precursors (pre-miRNAs) (<xref ref-type="bibr" rid="gks878-B3">3</xref>). The pre-miRNAs are
then subsequently processed to yield mature miRNA by Dicer enzyme, which targets pre-miRNAs
on the basis of their hairpin secondary structures, which are considered as a crucial
characteristic for enzyme substrate recognition in miRNA biogenesis pathways (<xref ref-type="bibr" rid="gks878-B4">4</xref>). A number of miRNAs remain undiscovered.
Identification of miRNA genes is one of the most imminent problems towards the understanding
of post-translational gene regulation in both normal development and human pathology (<xref ref-type="bibr" rid="gks878-B5">5</xref>).</p>
    <p>There are two main approaches in miRNA identification: experimental and computational
approaches. The discovery and characterization of novel miRNA genes have proved to be
challenging both experimentally and computationally (<xref ref-type="bibr" rid="gks878-B6">6</xref>). Experimental approaches have successfully identified highly expressed miRNAs
from various tissues. However, cloning methods are biased towards miRNAs that are abundantly
expressed (<xref ref-type="bibr" rid="gks878-B3">3</xref>,<xref ref-type="bibr" rid="gks878-B5">5</xref>,<xref ref-type="bibr" rid="gks878-B7">7</xref>). Computational
methods have been developed to complement experimental approaches in facilitating biologists
for identifying putative miRNA genes. These methods offer the most cost-effective and
time-effective screening approaches to identifying miRNAs. There are two types of
computational techniques: comparative and non-comparative methods. The former is based on
identifying conservation of sequences from closely related species to find homologous
pre-miRNAs. However, a key drawback of this approach is their lack of ability to detect
novel pre-miRNAs that are not homologous to previously identified miRNAs. For the latter,
classification models are trained by machine learning (ML) in identifying non-conserved
miRNAs, both known and novel, based on miRNA characteristics. Numerous <italic>de
novo</italic> non-comparative methods for identifying pre-miRNA hairpins based on single
ML algorithm have been proposed (<xref ref-type="bibr" rid="gks878-B8 gks878-B9 gks878-B10 gks878-B11 gks878-B12 gks878-B13 gks878-B14 gks878-B15 gks878-B16">8–16</xref>). For such methods, stem-loop structures are involved in prediction.
However, the stem-loop structures of non-miRNA sequences, similar to those of pre-miRNAs,
can be found all over the genome. This could lead to a high false positive rate (FPR).
Moreover, there is a risk of over-fitting of an algorithm to the training data. Therefore,
the computational <italic>de novo</italic> method should be improved to obtain a more
efficient and reliable pre-miRNA classification method. To handle the false positive and the
over-fitting, we introduced an ensemble technique in ML to the problem of pre-miRNA
classification. The ensemble, the committee of various algorithms, has been known to provide
more reliable and less false positive results than a single classifier through the agreement
among heterogeneous classifiers. Each single algorithm has its own strengths (and
weaknesses) depending on the induction hypothesis embedded in its learning process; no
single algorithm can perform significantly better than others in all problems and
performance measurements (<xref ref-type="bibr" rid="gks878-B17 gks878-B18 gks878-B19 gks878-B20">17–20</xref>). The voting of
distinct algorithms can reduce the bias occurring in a single learning algorithm and this
can therefore be relatively more generalized in prediction on new unseen data. (<xref ref-type="bibr" rid="gks878-B18">18</xref>,<xref ref-type="bibr" rid="gks878-B21 gks878-B22 gks878-B23">21–23</xref>). Performances of ensemble
ML-based methods have been examined extensively (<xref ref-type="bibr" rid="gks878-B24 gks878-B25 gks878-B26 gks878-B27 gks878-B28 gks878-B29 gks878-B30">24–30</xref>) and they have been proven to be effective in various applications,
such as optical character recognition, face recognition, protein classification and gene
expression analysis (<xref ref-type="bibr" rid="gks878-B18">18</xref>,<xref ref-type="bibr" rid="gks878-B31 gks878-B32">31–32</xref>).</p>
    <p>In general, most ML-based methods rely on known pre-miRNA characteristics as features for
training prediction models. Among these specific features, hairpin secondary structure and
minimum free energy (MFE) of stem-loop hairpins are considered as key features (<xref ref-type="bibr" rid="gks878-B4">4</xref>). However, plant pre-miRNAs have been reported to
have different characteristics from those of animals in MFE distribution, size and stem-loop
structure (<xref ref-type="bibr" rid="gks878-B3">3</xref>,<xref ref-type="bibr" rid="gks878-B14">14</xref>,<xref ref-type="bibr" rid="gks878-B33">33</xref>). Moreover,
MFE of hairpin structure was not a unique characteristic for miRNA because some small
non-coding RNA (ncRNA) also has high negative MFE value similar to those of pre-miRNAs
(<xref ref-type="bibr" rid="gks878-B34">34</xref>). It has been reported that the stem
loops of pre-miRNAs exhibit a significantly high level of genetic robustness in comparison
with other stem-loop sequences (<xref ref-type="bibr" rid="gks878-B35 gks878-B36 gks878-B37">35–37</xref>). The high intrinsic robustness of miRNA stem loops which goes beyond
the intrinsic robustness of other stem-loop structures is likely a consequence of selection
for functionally relevant substructure toward increased robustness (<xref ref-type="bibr" rid="gks878-B38">38</xref>). In this study, we considered various robustness features of
miRNA such as Z-score, <italic>P</italic>-value and self-containment (SC) score. The SC
score is an <italic>in silico</italic> used to measure the structural robustness property of
the RNAs in the face of perturbations. It has been shown that both plant and animal
pre-miRNA hairpins have particularly high SC scores, with right-skewed distribution,
compared with other hairpins. Since the pre-miRNAs need to maintain stable structural
folding through cleavage steps during their biogenesis pathway, the pre-miRNA stem loops
exhibit high SC whereas pseudo-hairpin sequences and other structured RNAs do not (<xref ref-type="bibr" rid="gks878-B39">39</xref>). Therefore, we were interested in exploring
these kinds of robustness characteristics of pre-miRNAs.</p>
    <p>In addition, there are two challenging issues for further enhancement of ensemble
performance. Firstly, irrelevant and redundant features can significantly reduce the
performance of classifiers. Therefore, identification of discriminatory features is
required. Secondly, for training data, the class of interest (minority class) is rare and
has less data than the majority class, which is commonly found in Bioinformatics data,
including pre-miRNA data. In the case of imbalanced data, algorithms aim to maximize overall
accuracy and bias toward the majority class. Thus, rebalancing the imbalanced training data
is a necessary step for improving performance on both sensitivity and specificity.</p>
    <p>This study presents a novel heterogeneous ensemble combining various efficient classifiers
to the problem of pre-miRNA classification. The method, a cooperative combination of
different learning algorithms exposed to different training subsets, can create a high level
of diversity and reduce bias that tends to occur when single individual classifier is used.
Consequently, the ensemble provides a more reliable prediction. Additionally, novel
robustness features were introduced: the SC-base pair composite features served as promising
discriminators in distinguishing real pre-miRNA hairpins from other hairpin sequences with
improved sensitivity and specificity from an original SC feature. Moreover, a feature
selection (FS) method was applied to select only relevant and discriminative features. The
problem of imbalanced data was solved by the modified-Synthetic Minority Oversampling
Technique (SMOTE) bagging method. This enhanced ensemble-based method would effectively
differentiate pre-miRNA from non-miRNA sequences with higher accuracy and better balanced
sensitivity and specificity score, across various organisms, making our model a useful tool
for finding novel animal, plant and virus pre-miRNAs.</p>
  </sec>
  <sec sec-type="materials|methods">
    <title>MATERIALS AND METHODS</title>
    <sec>
      <title>Data set</title>
      <sec>
        <title>Training data</title>
        <p>We randomly selected 600 non-redundant sequences of 1424 <italic>H</italic><italic>omo
sapiens</italic> pre-miRNAs, 200 of 491 of <italic>O</italic><italic>ryza
sativa</italic>, and 200 of 232 of <italic>Arabidopsis thaliana</italic> from the
miRBase version 17 (<xref ref-type="bibr" rid="gks878-B40">40</xref>) as our positive
data sets, where <italic>H. sapiens, O. sativa</italic> and <italic>A. thaliana</italic>
represent animal, monocot plant and dicot plant positive data, respectively.</p>
        <p>The negative training data set was composed of both pseudo-hairpin sequences and other
ncRNAs. 8494 non-redundant pseudo-hairpins were extracted from the protein-coding region
(CDS) according to the human RefSeq genes. The pseudo-hairpins were selected based on
the following criteria: (i) length distribution of pseudo sequences similar to those of
pre-miRNAs, (ii) have a minimum 18 bp on stem structure and (iii) a maximum −18
kcal/mol of free energy. A set of 4000 pseudo-hairpin sequences, randomly selected from
8494 hairpins, were represented as one type of negative training data set. A set of 754
other non-coding RNA (ncRNAs) originally from the Rfam database (<xref ref-type="bibr" rid="gks878-B41">41</xref>) is another type of negative training data, composed of 327
tRNAs (transfer RNAs), 5 sRNAs (small RNAs), 53 snRNAs (small nuclear RNAs), 334 snoRNAs
(small nucleolar RNAs), 32 YRNAs (non-coding RNA components of Ro ribonucleoproteins)
and 3 other miscellaneous RNAs. These non-redundant ncRNA sequences have length between
70 and 150 nt and can form hairpin structures. In this work, four independent testing
sets were used to evaluate the performance of the algorithm. Description of the four
testing data sets is presented in <ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/full/gks878/DC1">Supplementary
Method S1</ext-link>.</p>
      </sec>
    </sec>
    <sec>
      <title>Features</title>
      <p>We gathered 103 features previously introduced in other works (<xref ref-type="bibr" rid="gks878-B8 gks878-B9">8–9</xref>,<xref ref-type="bibr" rid="gks878-B11 gks878-B12">11–12</xref>), including 19 sequence-based features,
24 secondary-structure-based features, 28 base-pair features and 32
triplet-sequence-structure-based features.</p>
      <p>This study, not only included all features used in previously proposed methods, but also
incorporated structural-robustness-related properties into the feature set. We defined new
features—namely ‘the SC-derived feature’ which were SC/(1 − dP),
SC × dP, SC × dP/(1 − dP), SC/tot_bp, SC/Len, SC × MFE/Mean_dG, SC
× zG, SC/NonBP_A, SC/NonBP_C, SC/NonBP_G and SC/NonBP_U—and incorporated them
into the list.</p>
      <p>The list of 125 features used in our study is summarized in <xref ref-type="table" rid="gks878-T1">Table 1</xref>. Detailed descriptions of these features are provided in
<ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/full/gks878/DC1">Supplementary
Method S2</ext-link>. <table-wrap id="gks878-T1" position="float"><label>Table 1.</label><caption><p>List
of 125 features used in this work</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Feature groups</th><th rowspan="1" colspan="1">No. of features</th><th rowspan="1" colspan="1">Feature symbol</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">Sequence-based features</td><td rowspan="1" colspan="1">19</td><td rowspan="1" colspan="1">Len, %G+C,% A+U, %AA, %AC, %AG,
%AU, %CA, %CC, %CG, %CU, %GA,
%GC, %GG, %GU, %UA, %UC, %UG,
%UU</td></tr><tr><td rowspan="1" colspan="1">Secondary structure features</td><td rowspan="1" colspan="1">30</td><td rowspan="1" colspan="1">MFE, efe, MFEI1, MFEI2, MFEI3, MFEI4, dG, dQ, dD, dF, Prob, zG, zQ, zD, zF,
nefe, Freq, diff, dH, dH/L, dS, dS/L, Tm, Tm/L, <bold>MFEI5, MFE/Mean_dG,
dH/loop, dS/loop, Tm/Loop, dQ/Loop</bold></td></tr><tr><td rowspan="1" colspan="1">Base pair features</td><td rowspan="1" colspan="1">32</td><td rowspan="1" colspan="1">dP, zP, div, tot_bp, stem, loop, A-U/L, G-U/L, G-C/L, %A–U/Stem,
%G–C/Stem, %G–U/Stem<italic>,</italic>
Probpair1–10, Avg_BP_stem, NonBP_A, NonBP_C, NonBP_G, NonBP_U, Non_BPP,
<bold>%A–U/BP, %C–G/BP, %G–U/BP,
Avg_BP_Loop</bold></td></tr><tr><td rowspan="1" colspan="1">Triplet sequence structure</td><td rowspan="1" colspan="1">32</td><td rowspan="1" colspan="1">A(((, A((., A(.., A(.(,A.((,A.(.,A..(, A …, C(((, C((., C(.., C(.(,
C.((, C.(., C..(, C…, G(((, G((., G(.., G(.(, G.((, G.(., G..(,
U…, U(((, U((., U(.., U(.(, U.((, U.(., U..(, U…,</td></tr><tr><td rowspan="1" colspan="1">Structural robustness features (SC-derived features)</td><td rowspan="1" colspan="1">12</td><td rowspan="1" colspan="1"><bold>SC, SC/tot_bp, SC/Len, SC × MFE/Mean_dG, SC × dP, SC ×
zG, SC/(1 − dP), SC×dP/(1 − dP), SC/NonBP_A, SC/NonBP_C,
SC/NonBP_G , SC/NonBP_U</bold></td></tr><tr><td rowspan="1" colspan="1">Total</td><td rowspan="1" colspan="1">125</td><td rowspan="1" colspan="1"/></tr></tbody></table><table-wrap-foot><fn id="gks878-TF1"><p>Our additional features are shown in bold.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec sec-type="methods">
      <title>FS methods</title>
      <p>We considered three filter FS methods: ReliefF, Information Gain (InfoGain) and
correlation-based feature selection (CFS) (<xref ref-type="bibr" rid="gks878-B42 gks878-B43 gks878-B44 gks878-B45">42–45</xref>). Details of the FS
determination are described in <ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/full/gks878/DC1">Supplementary
Method S3</ext-link>.</p>
    </sec>
    <sec>
      <title>Algorithm selection</title>
      <p>To select base classifiers for constructing an ensemble, various algorithms were
compared. Eight algorithms—naïve bayes (NB), neural networks (MLP), support
vector machine (SVM), k-nearest neighbors (kNN), decision tree (J48), repeated incremental
pruning to produce error reduction (RIPPER), RBF network (RBFNets) and random forest
(RF)—were considered in our algorithm selection experiment. Each displays a
different inductive bias and learning hypotheses (instance-based, rules, trees and
statistics) and, therefore, provides a potentially more independent and diverse set of
predictions to build upon. The details of algorithms are described in <ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/full/gks878/DC1">Supplementary
Method S4</ext-link>.</p>
    </sec>
    <sec sec-type="methods">
      <title>Ensemble method</title>
      <p>Our heterogeneous ensemble method was implemented using Perl and Java scripts. Our
program was run on a Fedora Linux-based machine. We used Weka (<xref ref-type="bibr" rid="gks878-B46">46</xref>), LIBSVM (<xref ref-type="bibr" rid="gks878-B47">47</xref>,<xref ref-type="bibr" rid="gks878-B48">48</xref>) and R programming (<xref ref-type="bibr" rid="gks878-B49">49</xref>) to build and compare base classifiers. The
computational procedure of our method was illustrated in <xref ref-type="fig" rid="gks878-F1">Figure 1</xref>A. The training process started from collecting positive
and negative data. Each sequence in the training data was extracted as an input vector of
125 features by a feature extraction process. Then, the FS method selected informative and
relevant features and removed irrelevant and redundant features from the 125 feature set.
The sub-sampling methods were applied to rebalance the distribution in the training data
as illustrated in <xref ref-type="fig" rid="gks878-F1">Figure1</xref>B. To handle the
class imbalance in the data set, the resampling techniques, both over-sampling and
under-sampling, were integrated to improve the minority class prediction performance, and
were performed as follows. First, we applied the SMOTE (<xref ref-type="bibr" rid="gks878-B50">50</xref>) with a resampling rate of 50% to increase the
frequency of the minority class by synthesizing 500 new samples of the minority class
(using the parameter k = 5). Under-sampling was applied to create equal class
balance in training subsets by under-sampling the majority class with the same number of
examples of minority class. These resampling methods were called the
‘modified-SMOTEbagging’ method. The method finally gave four class-balanced
training subsets: one subset of ‘miRNA versus ncRNA’ and three subsets of
‘miRNA versus pseudo-hairpin’. After the rebalancing, the chosen algorithms
were then trained on each balanced training subset. As a result, 12 base classifiers (4
SVM, 4 RF, 4 kNN) were combined to form the ensemble. Finally, the predictions of 12
individual classifiers, which were 3 algorithms trained on 4 well-balanced distribution
training data subsets, were voted to obtain the final prediction. <fig id="gks878-F1" position="float"><label>Figure 1.</label><caption><p>(<bold>A</bold>) Overview of proposed ensemble
method: the training process is shown by dark thick arrows. The testing process is
shown in white arrows. (<bold>B</bold>) Rebalancing class distribution procedure:
the imbalanced training data were processed to obtain four subsets of training data
with balanced distribution between positive and negative
classes.</p></caption><graphic xlink:href="gks878f1p"/></fig></p>
    </sec>
    <sec sec-type="methods">
      <title>Performance evaluation methods</title>
      <p>To precisely assess the predictive power of a prediction method and model comparison, we
used several performance measurements already applied extensively in the field of
Bioinformatics. All the performance measures are defined as: <disp-formula><graphic xlink:href="gks878um1"/></disp-formula>
</p>
      <p>The receiver operating characteristic (ROC) curve is a graphic visualization of the
trade-off between the true positive and false positive rates for every possible cut off;
we used an area under the ROC curve (AUC) to compare the performance of classifiers.</p>
    </sec>
  </sec>
  <sec>
    <title>RESULTS AND DISCUSSION</title>
    <sec>
      <title>Predictive performance improvement using SC-derivative features</title>
      <p>Since the choice of features has an impact on predictive performance of classifier, the
discriminative powers for each feature group are compared. The average 5-fold CV (<xref ref-type="bibr" rid="gks878-B51">51</xref>) performance of different feature groups is
shown in the <xref ref-type="table" rid="gks878-T2">Table 2</xref>. The accuracy, commonly
used measurement, is not an appropriate metric to evaluate the performance of a classifier
in class-imbalanced data since the negative class (majority) in training data is much
larger than the positive (minority) class. The geometric mean (Gm) is suitable for
evaluating the performance in this situation where class-imbalanced data still occurs
because it considers performance on both majority and minority classes (<xref ref-type="bibr" rid="gks878-B52">52</xref>). Among the five feature groups in this
study, the SC derivative group showed the most discriminative power with the highest
sensitivity at 84.5%, the highest specificity at 98.4% and the highest Gm at
91.18%. A classifier employing a SC derivative feature group outperformed those
employing other feature groups. Moreover, it outperformed the classifier that utilized all
125 features (Gm of 90.86%). This indicates that the SC derivative feature group is
a strong discriminant between pre-miRNA and non-miRNA sequences. This result was
consistent with previous reports (<xref ref-type="bibr" rid="gks878-B38 gks878-B39">38–39</xref>) in which pre-miRNAs showed high robustness in their structure since
pre-miRNAs need to maintain functional structure in the face of perturbation in their
biogenesis. The real pre-miRNAs exhibit remarkably high SC, which goes beyond the
intrinsic robustness of the stem-loop hairpin structure. <table-wrap id="gks878-T2" position="float"><label>Table 2.</label><caption><p>Predictive performance of each feature groups by
the 5-fold CV</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Feature groups</th><th rowspan="1" colspan="1">No. of features</th><th rowspan="1" colspan="1">Sn</th><th rowspan="1" colspan="1">Sp</th><th rowspan="1" colspan="1">Gm</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">Sequence features</td><td rowspan="1" colspan="1">19</td><td rowspan="1" colspan="1">45.0</td><td rowspan="1" colspan="1">96.3</td><td rowspan="1" colspan="1">65.83</td></tr><tr><td rowspan="1" colspan="1">Structure features</td><td rowspan="1" colspan="1">30</td><td rowspan="1" colspan="1">82.8</td><td rowspan="1" colspan="1">97.6</td><td rowspan="1" colspan="1">89.90</td></tr><tr><td rowspan="1" colspan="1">Base pair feature</td><td rowspan="1" colspan="1">32</td><td rowspan="1" colspan="1">81.5</td><td rowspan="1" colspan="1">97.8</td><td rowspan="1" colspan="1">89.28</td></tr><tr><td rowspan="1" colspan="1">Triplet sequence structure</td><td rowspan="1" colspan="1">32</td><td rowspan="1" colspan="1">77.7</td><td rowspan="1" colspan="1">97.1</td><td rowspan="1" colspan="1">86.86</td></tr><tr><td rowspan="1" colspan="1">SC related feature</td><td rowspan="1" colspan="1">12</td><td rowspan="1" colspan="1">84.5</td><td rowspan="1" colspan="1">98.4</td><td rowspan="1" colspan="1">91.18</td></tr><tr><td rowspan="1" colspan="1">All five feature groups</td><td rowspan="1" colspan="1">125</td><td rowspan="1" colspan="1">84.0</td><td rowspan="1" colspan="1">98.3</td><td rowspan="1" colspan="1">90.86</td></tr><tr><td rowspan="1" colspan="1">Feature SC</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">76.6</td><td rowspan="1" colspan="1">96.9</td><td rowspan="1" colspan="1">86.15</td></tr><tr><td rowspan="1" colspan="1">Feature SC × dP</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">80.5</td><td rowspan="1" colspan="1">98.1</td><td rowspan="1" colspan="1">88.86</td></tr><tr><td rowspan="1" colspan="1">Feature SC/(1 − dP)</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">81.3</td><td rowspan="1" colspan="1">97.9</td><td rowspan="1" colspan="1">89.76</td></tr><tr><td rowspan="1" colspan="1">Feature SCxdP/(1 − dP)</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">82.2</td><td rowspan="1" colspan="1">98.2</td><td rowspan="1" colspan="1">89.84</td></tr><tr><td rowspan="1" colspan="1">Feature SC × MFE/Mean_dG</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">81.9</td><td rowspan="1" colspan="1">97.5</td><td rowspan="1" colspan="1">87.76</td></tr><tr><td rowspan="1" colspan="1">Feature SC × zG</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">78.9</td><td rowspan="1" colspan="1">98.8</td><td rowspan="1" colspan="1">89.36</td></tr><tr><td rowspan="1" colspan="1">Feature SC/tot_bp</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">Feature SC/Len</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">Feature SC/NonBP_A</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">68.5</td><td rowspan="1" colspan="1">98.4</td><td rowspan="1" colspan="1">82.09</td></tr><tr><td rowspan="1" colspan="1">Feature SC/NonBP_C</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">Feature SC/NonBP_G</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">Feature SC/NonBP_U</td><td rowspan="1" colspan="1">1</td><td rowspan="1" colspan="1">48.8</td><td rowspan="1" colspan="1">98.4</td><td rowspan="1" colspan="1">69.29</td></tr></tbody></table><table-wrap-foot><fn id="gks878-TF2"><p>Sn = Sensitivity, Sp = Specificity and Gm
= Geometric mean.</p></fn></table-wrap-foot></table-wrap></p>
      <p>Both plants and human pre-miRNAs are similar in SC profile distribution but differ from
those of ncRNAs and pseudo-hairpin sequences (<xref ref-type="fig" rid="gks878-F2">Figure
2</xref>A). This implies that the SC score can distinguish real pre-miRNAs not only from
pseudo hairpin, but also from other small ncRNAs. The result indicated that miRNAs have
unique robustness in their structures, which evolved from their functional selection, and
this evolved robustness is found in all pre-miRNAs studied in this work. We further
investigated this by calculating average SC values in the training data. The average SC
values of both human (0.86) and plant (0.91) species are significantly higher than those
of other functional RNAs (0.51) and pseudo-hairpin sequences (0.44). This result is
consistent with previous studies reporting that pre-miRNA exhibit high intrinsic
structural invariance with a strong SC score between 0.85 and 0.98, whereas other
stem-loop forming small ncRNAs yield SC ranges (∼0.4–0.6) much lower than the
pre-miRNAs (<xref ref-type="bibr" rid="gks878-B39">39</xref>). Additionally, they observed
correlation relationships between SC and various structural features. Thus, these results
led to the idea of incorporating various structural features into the SC as our novel
features with an aim to maximize specificity and sensitivity. <fig id="gks878-F2" position="float"><label>Figure 2.</label><caption><p>The SC-base pair composite features of
Human_miRNA, Plant_miRNA, Other ncRNAs and Pseudo hairpins in our training dataset.
(<bold>A</bold>) Original SC feature. (<bold>B</bold>) Feature SC × dP.
(<bold>C</bold>) Feature SC/(1 − dP). (<bold>D</bold>) Feature SC ×
dP/(1 − dP).</p></caption><graphic xlink:href="gks878f2p"/></fig></p>
      <p>To further evaluate the performance of individual SC derivative features, the
classification performances gained using individual features of our 11 SC-derived features
were reported (<xref ref-type="table" rid="gks878-T2">Table 2</xref>). SC-base pair
composite features, such as SC × dP, SC/(1 − dP) and SC × dP/(1 −
dP), showed the most discriminative features. We found that the use of so-called SC-base
pair composite features—the incorporation of information about base pairing or
non-base pairing with the SC score—can increase predictive performance by
4–5% in terms of sensitivity value, 1% in terms of specificity and
2–3% in terms of Gm value. By using these three features individually, the
classifier distinguishes real pre-miRNA from other hairpins with higher sensitivity and
specificity than the original SC score.</p>
      <p>In <xref ref-type="fig" rid="gks878-F2">Figure 2</xref>B, C and D, the distribution of
SC-base pair composite features of real pre-miRNA and negative hairpins were well
separated. The human pre-miRNA (<italic>H. sapien</italic>) and plant pre-miRNAs
(<italic>O. sativa</italic> and <italic>A. thaliana</italic>) distributions are similar
but they differed from those of pseudo-hairpins and other ncRNAs. This result indicated
that our SC-base pair combining features were capable of distinguishing real pre-miRNA
from other false hairpins across human and plant species. Among the SC-base pair composite
features, the feature SC × dP/(1 − dP) yields the highest discriminative power
with Gm of 89.84%. The results suggested that using some certain features can give
as good performance as using all of the 125 features (<xref ref-type="table" rid="gks878-T2">Table 2</xref>). This may be due to the fact that there are redundant
and irrelevant features overall. Therefore, it is reasonably suitable to incorporate the
FS method to select only informative, relevant, and non-redundant feature subsets,
plausibly increasing the predictive performance of the classifier and decreasing the
computation time in the feature extraction process. We investigated three statistical
filtering methods based on different criteria, namely ReliefF, InfoGain and CFS + GA.
The filter methods for FS rely on general characteristics of data without involving any
learning algorithms while the wrapper method needs predetermined classifiers in selecting
features. It should be noted that since our method is based on an ensemble system, the
wrapper methods that are dependent upon predetermined classifier were not suitable in this
study.</p>
      <p>To choose the most appropriate FS method, we compared the effectiveness of the 3-fold CV
performance of the three filtering methods (<xref ref-type="table" rid="gks878-T3">Table
3</xref>). ‘All features’ and ‘microPred feature’ were also
shown as a baseline for comparison. The microPred feature is a set of 21 features from
microPred (<xref ref-type="bibr" rid="gks878-B11">11</xref>), not including our additional
features (i.e. SC-derived feature group). In FSs 1 and 2, features were ranked according
to ReliefF and infoGain, respectively. The top 75 ranked individual features of the
InfoGain criterion produced a Gm of 91.35%. For ReliefF, the top 50 ranked
individual features yielded a Gm of 91.40%. The CFS + GA method selected the
subset of 20 features with a Gm of 91.49%. The classifiers with selected feature
sets (FS1, FS2 and FS3) performed better than classifiers with full feature sets. The
possible reasons are some features may be irrelevant and some of them may be redundant due
to their high correlation with others in a large number of features. When using the FS
method to select relevant and informative features that contribute to discrimination
between true and false pre-miRNAs, the performance and robustness of classifiers can be
improved (<xref ref-type="bibr" rid="gks878-B53">53</xref>). Among classifiers with the
different FS method, the classifier with the CFS + GA feature set yielded the highest
Gm and performed better than those from other methods. Thus, we chose the CFS + GA as
a FS method because it gave better overall accuracy and selected a more compact set of
features than the other two methods. A selection of relatively fewer features has the
advantage of being less time consuming in computing features and increasing the classifier
performance by using only informative features. The 20 selected features by the CFS +
GA are the following features: Prob, MFEI1, zG, zP, zQ, dH/Loop, Tm/Loop, AU/L,
Avg_BP_loop, MFEI5, SC, SC × dP, SC × MFE/Mean_dG, SC × dP/(1 −
dP), SC/nonBP_A, Non_BPP, A(((, G(<bold>..</bold>, C <bold>…</bold><bold/>and ProbPair4, which were used later in further experiments. <table-wrap id="gks878-T3" position="float"><label>Table 3.</label><caption><p>The average performance by different
feature selection algorithms on our training data</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Feature subsets</th><th rowspan="1" colspan="1">No. of features</th><th rowspan="1" colspan="1">Sn (%)</th><th rowspan="1" colspan="1">Sp (%)</th><th rowspan="1" colspan="1">Gm (%)</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">All features (No FS)</td><td rowspan="1" colspan="1">125</td><td rowspan="1" colspan="1">84.0</td><td rowspan="1" colspan="1">98.3</td><td rowspan="1" colspan="1">90.86</td></tr><tr><td rowspan="1" colspan="1">microPred features (J–M) (<xref ref-type="bibr" rid="gks878-B10">10</xref>)</td><td rowspan="1" colspan="1">21</td><td rowspan="1" colspan="1">83.0</td><td rowspan="1" colspan="1">97.9</td><td rowspan="1" colspan="1">90.14</td></tr><tr><td rowspan="1" colspan="1">FS1: ReliefF</td><td rowspan="1" colspan="1">50</td><td rowspan="1" colspan="1">84.9</td><td rowspan="1" colspan="1">98.4</td><td rowspan="1" colspan="1">91.40</td></tr><tr><td rowspan="1" colspan="1">FS2: InfoGain</td><td rowspan="1" colspan="1">75</td><td rowspan="1" colspan="1">84.9</td><td rowspan="1" colspan="1">98.3</td><td rowspan="1" colspan="1">91.35</td></tr><tr><td rowspan="1" colspan="1">FS3: CFS + GA</td><td rowspan="1" colspan="1">20</td><td rowspan="1" colspan="1">84.9</td><td rowspan="1" colspan="1">98.6</td><td rowspan="1" colspan="1">91.49</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec>
      <title>Model selection for an efficient ensemble</title>
      <p>To select algorithms for construction of an efficient ensemble, various classification
algorithms—NB, SVM, kNN, MLP, J48, RIPPER, RBFNets and RF—which have been
commonly applied in Bioinformatics, were investigated and compared. Performance of eight
different algorithms on the task of pre-miRNA hairpin classification is summarized in
<xref ref-type="table" rid="gks878-T4">Table 4</xref> as the average 10 × 5-fold
CV. Among the eight algorithms, SVM, kNN and RF models showed their superior performance
in different evaluation metrics. The SVM algorithm gave the highest AUC score on CV. This
is likely due to the fact that the algorithm used support vectors that provide a
hyperplane with a maximal separation between positive and negative samples, giving the
best optimization performance among the eight classifiers. The kNN algorithm yielded the
highest specificity and precision measurements of 99.2 and 96.7%, respectively,
implying that it performed better in correctly identifying the negative class (false miRNA
hairpin sequences) and produced the lowest FPR. The kNN algorithm classified the sample
based on the ‘k’ nearest neighbor samples. It produced a satisfactory result
for negative data, possibly because negative data have features that are more locally
clustered by a closer distance. On the other hand, RF performed most accurately in
identifying the positive class (real miRNA hairpin sequences) by yielding the highest
sensitivity of 86.7%, similar to previous findings in MiPred (<xref ref-type="bibr" rid="gks878-B10">10</xref>). This is possibly due to RF, which combined multiple
decision trees with multiple discriminative rules that can cover the heterogeneity of
characteristics in pre-miRNAs. <table-wrap id="gks878-T4" position="float"><label>Table
4.</label><caption><p>Comparison of the performance of different methods on training
data using 20 selected features</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Algorithms</th><th colspan="6" align="center" rowspan="1">Performance measurement<hr/></th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">ACC</th><th rowspan="1" colspan="1">Sn</th><th rowspan="1" colspan="1">Sp</th><th rowspan="1" colspan="1">PPV</th><th rowspan="1" colspan="1">FPR</th><th rowspan="1" colspan="1">AUC</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">K-nearest neighbors (kNN)</td><td rowspan="1" colspan="1">95.511</td><td rowspan="1" colspan="1">83.3</td><td rowspan="1" colspan="1"><bold>99.2</bold></td><td rowspan="1" colspan="1"><bold>96.7</bold></td><td rowspan="1" colspan="1">0.8</td><td rowspan="1" colspan="1">0.966</td></tr><tr><td rowspan="1" colspan="1">Support vector machine (SVM)</td><td rowspan="1" colspan="1"><bold>95.528</bold></td><td rowspan="1" colspan="1">85.1</td><td rowspan="1" colspan="1">98.6</td><td rowspan="1" colspan="1">94.8</td><td rowspan="1" colspan="1">1.4</td><td rowspan="1" colspan="1"><bold>0.972</bold></td></tr><tr><td rowspan="1" colspan="1">Artificial neural network (MLP)</td><td rowspan="1" colspan="1">95.283</td><td rowspan="1" colspan="1">86.5</td><td rowspan="1" colspan="1">97.9</td><td rowspan="1" colspan="1">92.4</td><td rowspan="1" colspan="1">2.1</td><td rowspan="1" colspan="1">0.964</td></tr><tr><td rowspan="1" colspan="1">Decision tree (J48)</td><td rowspan="1" colspan="1">94.581</td><td rowspan="1" colspan="1">84.4</td><td rowspan="1" colspan="1">97.6</td><td rowspan="1" colspan="1">91.3</td><td rowspan="1" colspan="1">2.4</td><td rowspan="1" colspan="1">0.920</td></tr><tr><td rowspan="1" colspan="1">RBF networks (RBFNets)</td><td rowspan="1" colspan="1">94.352</td><td rowspan="1" colspan="1">86.4</td><td rowspan="1" colspan="1">96.7</td><td rowspan="1" colspan="1">88.7</td><td rowspan="1" colspan="1">3.3</td><td rowspan="1" colspan="1">0.968</td></tr><tr><td rowspan="1" colspan="1">Rule based (RIPPER)</td><td rowspan="1" colspan="1">94.809</td><td rowspan="1" colspan="1">84.0</td><td rowspan="1" colspan="1">98.0</td><td rowspan="1" colspan="1">92.6</td><td rowspan="1" colspan="1">2.0</td><td rowspan="1" colspan="1">0.923</td></tr><tr><td rowspan="1" colspan="1">Naïve bayes (NB)</td><td rowspan="1" colspan="1">93.585</td><td rowspan="1" colspan="1">85.5</td><td rowspan="1" colspan="1">96.0</td><td rowspan="1" colspan="1">86.4</td><td rowspan="1" colspan="1">4.0</td><td rowspan="1" colspan="1">0.955</td></tr><tr><td rowspan="1" colspan="1">Random forest (RF)</td><td rowspan="1" colspan="1">95.283</td><td rowspan="1" colspan="1"><bold>86.7</bold></td><td rowspan="1" colspan="1">97.8</td><td rowspan="1" colspan="1">92.1</td><td rowspan="1" colspan="1">2.2</td><td rowspan="1" colspan="1">0.965</td></tr></tbody></table><table-wrap-foot><fn id="gks878-TF3"><p>Sn = Sensitivity, Sp = Specificity, PPV =
Positive predictive value, ACC = Accuracy, FPR = False positive rate
and AUC = Area under ROC curve. The highest values are in bold.</p></fn></table-wrap-foot></table-wrap></p>
      <p>Consistent with the No Free Lunch (NFL) theorem (<xref ref-type="bibr" rid="gks878-B19">19</xref>), this result strongly suggested that there is no single best algorithm that
is superior to all performance metrics. Based on the evaluation, SVM, RF and kNN
algorithms were chosen as ensemble members because of their best performances in different
metrics: AUC, sensitivity and specificity performance. These three algorithms are
different in the way they learn from data. Selecting diverse algorithms will not only
combine the strengths of multiple algorithms, but will also make individual classifiers in
ensembles disagree with each other. This disagreement among classifiers is utilized by
voting to give a reliable final prediction.</p>
    </sec>
    <sec>
      <title>Class-balance and FS enhancing the ensemble performance</title>
      <p>In the training data set, pre-miRNA is considered to be a minority class, with the ratio
of class distribution being ∼1:5 (miRNA:non-miRNA). It has been shown that the
imbalance of pre-miRNAs training data can affect the accuracy of classifiers (<xref ref-type="bibr" rid="gks878-B11">11</xref>). We performed 10 run of 5-fold CV and
investigated the performance of our three different ensemble models in <xref ref-type="table" rid="gks878-T5">Table 5</xref>. Vote1 is the ensemble of three models
(SVM, kNN, RF) using all features trained on class imbalance data (original data without
performing the resampling techniques). The main difference between Vote1 and Vote2 is the
number of features for building models; Vote2 uses 20 features selected from the FS
method. Performance can be improved using only relevant and informative features. The
ensemble classifiers with the set of selected feature (Vote2) produced better results than
the ensemble classifiers with full feature sets (Vote1). By applying FS, we significantly
improved the performance of our ensemble from 95.48 to 95.81% in terms of accuracy,
and from 0.973 to 0.976 in terms of AUC. <table-wrap id="gks878-T5" position="float"><label>Table
5.</label><caption><p>The 10 × 5 fold CV generalization performance of balanced
and imbalanced ensembles with selected features</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Algorithms</th><th colspan="6" align="center" rowspan="1">Performance measurement<hr/></th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">ACC</th><th rowspan="1" colspan="1">Sn</th><th rowspan="1" colspan="1">Sp</th><th rowspan="1" colspan="1">FPR</th><th rowspan="1" colspan="1">Gm</th><th rowspan="1" colspan="1">AUC</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">Vote 1 (Imbalanced, all features)</td><td rowspan="1" colspan="1">95.48</td><td rowspan="1" colspan="1">84.1</td><td rowspan="1" colspan="1">98.7</td><td rowspan="1" colspan="1">1.3</td><td rowspan="1" colspan="1">91.2</td><td rowspan="1" colspan="1">0.973</td></tr><tr><td rowspan="1" colspan="1">Vote 2 (Imbalanced, 20 selected features)</td><td rowspan="1" colspan="1">95.81</td><td rowspan="1" colspan="1">85.1</td><td rowspan="1" colspan="1">99.1</td><td rowspan="1" colspan="1">0.9</td><td rowspan="1" colspan="1">91.4</td><td rowspan="1" colspan="1">0.976</td></tr><tr><td rowspan="1" colspan="1">Vote 3 (Balanced, 20 selected features)</td><td rowspan="1" colspan="1">96.54</td><td rowspan="1" colspan="1">94.8</td><td rowspan="1" colspan="1">98.3</td><td rowspan="1" colspan="1">1.7</td><td rowspan="1" colspan="1">96.5</td><td rowspan="1" colspan="1">0.996</td></tr></tbody></table><table-wrap-foot><fn id="gks878-TF4"><p>ACC = Accuracy, Sn = Sensitivity, Sp =
Specificity, FPR = False positive rate, Gm = Geometric mean and AUC
= Area under the ROC curve.</p></fn></table-wrap-foot></table-wrap></p>
      <p>Unlike Vote1 and Vote2, Vote3 is an ensemble model with 12 classifiers (4 SVM, 4 kNN and
4 RF) trained on class-balanced data, i.e. the SVM, kNN and RF trained on 4 balanced
training data subsets (3 × 4 =12). Most ML methods assume the balance between
positive and negative classes in data sets and usually perform poorly on imbalanced data
sets because it will maximize the overall prediction accuracy by a bias toward the
majority class (<xref ref-type="bibr" rid="gks878-B52">52</xref>,<xref ref-type="bibr" rid="gks878-B54">54</xref>,<xref ref-type="bibr" rid="gks878-B55">55</xref>). Therefore,
it will misclassify the minority class, in our case, which is the class of interest. To
reduce the risk of the model performing poorly on the minority class (pre-miRNA), we
solved the class imbalance problem at both data and algorithm levels by combining the
SMOTE over-sampling method with the under-sampling method, and integrating them into the
ensemble model. Various resampling methods have their own strengths and drawbacks. It was
previously reported that under-sampling the majority class potentially removes certain
important samples, resulting in loss of useful information. On the other hand, randomly
over-sampling the minority class can lead to over-fitting on multiple copies of minority
class examples (<xref ref-type="bibr" rid="gks878-B50">50</xref>,<xref ref-type="bibr" rid="gks878-B52">52</xref>,<xref ref-type="bibr" rid="gks878-B54">54</xref>). To avoid
the problem of over-fitting, the technique called SMOTE was utilized to generate synthetic
examples along the line segments joining any of the k minority class to their nearest
neighbors; this broadened the decision boundaries for the minority class to spread further
into the majority class space. At the algorithm level, our model is an ensemble of
classifiers, one way to deal with the data imbalanced problem. Comparison of the
effectiveness of several ensemble-based techniques in learning from imbalanced noisy data
has shown that bagging techniques generally outperform boosting in most
cases—bagging improved over individual classifiers is more consistent on various
data sets than boosting (<xref ref-type="bibr" rid="gks878-B30">30</xref>,<xref ref-type="bibr" rid="gks878-B56">56</xref>). Moreover, the positive synergy between
resampling techniques and bagging algorithms has been observed when comparing various
ensemble-based rebalancing techniques. The hybrid approaches of SMOTE and under-sampling
in the bagging-based algorithm, called SMOTEbagging, outperformed others (<xref ref-type="bibr" rid="gks878-B57">57</xref>). The technique is similar to our
imbalance-tackle method, except the SMOTE resampling rate. We set the SMOTE resampling
rate at the constant rate of 50% (the synthetic data were generated for 50%
of the original data in the minority class) to reduce computational time and the amount of
synthetic samples that could possibly degrade the performance of classifiers. Using
modified-SMOTEbagging, we combined the strength of the individual methods while lessening
the drawbacks. The SMOTE method also increased the performance of ensembles by
establishing diversity, one factor necessary in building efficient ensembles. Comparing
Vote2 (imbalanced) and Vote3 (balanced), the sensitivity of Vote3 increased by 10%
(from 85.1 to 94.8%), which is significantly higher than that of Vote2, whereas the
specificity of Vote3 is slightly decreased (&lt;1%) from that of imbalanced class
data.</p>
      <p>By applying rebalancing techniques to handle the imbalanced-class in the training data,
we significantly improved the performance of our ensemble from 95.81 to 96.54% in
terms of accuracy, and from 0.976 to 0.996 in terms of AUC. Vote3 ensemble model with
selected features and trained on class-balanced data yielded the highest accuracy and
balance between sensitivity and specificity value by the voting of 12 diverse and accurate
classifiers. The results suggest that obtaining discriminatory features by the FS method
and rebalancing data distribution by resampling method are essential pre-processing steps
for yielding accurate prediction. Thus, the model Vote3 would be further used in comparing
the performance to other existing methods.</p>
    </sec>
    <sec sec-type="methods">
      <title>Comparison of predictive performance of our ensemble with other methods</title>
      <p>We compared the performance of our ensemble algorithm with those of the other existing
methods (<xref ref-type="bibr" rid="gks878-B8 gks878-B9">8–9</xref>,<xref ref-type="bibr" rid="gks878-B12">12</xref>,<xref ref-type="bibr" rid="gks878-B13">13</xref>), each of which has published results testing on the same data available to
download (the 1st testing data set). The results of the comparison with existing methods
are given in <xref ref-type="table" rid="gks878-T6">Table 6</xref>. Our ensemble
outperformed other methods on three data sets: <italic>TE-H, IE-NC and IE-M</italic>. For
the <italic>IE-NH</italic>, the miPred was slightly better than our method. However,
miPred gave the lowest performance in term of specificity or it did not perform well in
filtering out the negative testing data (the <italic>IE-NC and IE-M</italic>). Specificity
is the performance that the method can identify and filter for the negative class. The
specificity and FPR are correlated: when the method has high specificity, the FPR will be
lower (%FDR = 100 − %Sp). Our method efficiently lowered false
positives with an FPR of 16.7% compared with other methods with the FPR between
17.25–31.32% in <italic>IE-NC</italic> testing data. <table-wrap id="gks878-T6" position="float"><label>Table 6.</label><caption><p>The prediction performance of the
automated classifier, yasMir, miPred, tripletSVM and our method evaluated the same
testing data set</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Test sets</th><th rowspan="1" colspan="1">Automated classifier (<xref ref-type="bibr" rid="gks878-B11">11</xref>)</th><th rowspan="1" colspan="1">yasMiR (<xref ref-type="bibr" rid="gks878-B10">10</xref>)</th><th rowspan="1" colspan="1">miPred (<xref ref-type="bibr" rid="gks878-B7">7</xref>)</th><th rowspan="1" colspan="1">Triplet SVM (<xref ref-type="bibr" rid="gks878-B6">6</xref>)</th><th rowspan="1" colspan="1">Our method</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">TE-H</td><td rowspan="1" colspan="1">94.30</td><td rowspan="1" colspan="1">93.77</td><td rowspan="1" colspan="1">93.50</td><td rowspan="1" colspan="1">87.96</td><td rowspan="1" colspan="1"><bold>98.37</bold></td></tr><tr><td rowspan="1" colspan="1">IE-NH</td><td rowspan="1" colspan="1">94.91</td><td rowspan="1" colspan="1">94.11</td><td rowspan="1" colspan="1"><bold>95.64</bold></td><td rowspan="1" colspan="1">86.15</td><td rowspan="1" colspan="1">95.31</td></tr><tr><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">IE-NC</td><td rowspan="1" colspan="1">77.71</td><td rowspan="1" colspan="1">82.95</td><td rowspan="1" colspan="1">68.68</td><td rowspan="1" colspan="1">78.37</td><td rowspan="1" colspan="1"><bold>83.22</bold></td></tr><tr><td rowspan="1" colspan="1">IE-M</td><td rowspan="1" colspan="1">96.77</td><td rowspan="1" colspan="1"><bold>100</bold></td><td rowspan="1" colspan="1">87.09</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1"><bold>100</bold></td></tr></tbody></table><table-wrap-foot><fn id="gks878-TF5"><p>TE-H (123 human pre-miRNA and 246 pseudo hairpins), IE-NH (1918
pre-miRNA across 40 non-human species and 3836 pseudo hairpins), IE-NC (12 387
functional ncRNAs) and IE-M (31 mRNAs). The values are percentages of correct
prediction for each method on each data set. The highest values are in
bold.</p></fn></table-wrap-foot></table-wrap></p>
      <p>We also used the <italic>TE-CS</italic> data set (as reported in 8,12,15) for
comparison—this was composed of 581 pre-miRNAs. This data set allows us to evaluate
and compare the sensitivity of our method with Triplet-SVM, yasMir and PmirP, trained on
human miRNA hairpin data. As shown in <ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/full/gks878/DC1">Supplementary
Table S3</ext-link>, among the human miRNA hairpin-trained method, our method had the
highest accuracy (98.1%) when compared with the other four methods. yasMir was the
second best with sensitivity of 95.3% followed by PmirP, mirExplorer and
Triplet-SVM with accuracy of 94.0, 92.4 and 90.9%, respectively.</p>
      <p>In order to compare various ML techniques, we used the ‘Common Test’ data set
from mirExplorer (<xref ref-type="bibr" rid="gks878-B58">58</xref>) which allowed our
method to compare with SVM, RF and boosting-based algorithms. As shown in <xref ref-type="table" rid="gks878-T7">Table 7</xref>, our bagging based algorithm performed
better in both sensitivity and specificity value than SVM and RF algorithm based method.
Both sensitivity and specificity of our method is comparable to mirExplorer, a boosting
based method. However, our ensemble performed the best in identifying the 437 multi-loop
pre-miRNAs. Moreover, the performances of our method and MirExplorer in classifying across
species miRNA, 16 species ranging from animals to virus, were reported in <ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/full/gks878/DC1">Supplementary
Table S4</ext-link>. <table-wrap id="gks878-T7" position="float"><label>Table
7.</label><caption><p>Comparison of our method to other method on ‘Common
test’ testing data set of mirExplorer</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Method</th><th rowspan="1" colspan="1">Balance method</th><th rowspan="1" colspan="1">CV</th><th rowspan="1" colspan="1">SE</th><th rowspan="1" colspan="1">SP</th><th rowspan="1" colspan="1">Gm</th><th rowspan="1" colspan="1">ACC(%) Multiloop</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">Triplet-SVM (SVM)</td><td rowspan="1" colspan="1">–</td><td rowspan="1" colspan="1">–</td><td rowspan="1" colspan="1">88.40</td><td rowspan="1" colspan="1">83.50</td><td rowspan="1" colspan="1">0.859</td><td rowspan="1" colspan="1">N/A</td></tr><tr><td rowspan="1" colspan="1">MiPred (random forest)</td><td rowspan="1" colspan="1">–</td><td rowspan="1" colspan="1">–</td><td rowspan="1" colspan="1">84.34</td><td rowspan="1" colspan="1">93.56</td><td rowspan="1" colspan="1">0.888</td><td rowspan="1" colspan="1">N/A</td></tr><tr><td rowspan="1" colspan="1">microPred (SVM)</td><td rowspan="1" colspan="1">SMOTE</td><td rowspan="1" colspan="1">outer 5cv</td><td rowspan="1" colspan="1">90.50</td><td rowspan="1" colspan="1">66.43</td><td rowspan="1" colspan="1">0.775</td><td rowspan="1" colspan="1">54.23</td></tr><tr><td rowspan="1" colspan="1">mirExplorer (AdaBoost)</td><td rowspan="1" colspan="1">SMOTE + undersampling</td><td rowspan="1" colspan="1">outer 10cv</td><td rowspan="1" colspan="1">94.32</td><td rowspan="1" colspan="1">97.11</td><td rowspan="1" colspan="1">0.957</td><td rowspan="1" colspan="1">92.68</td></tr><tr><td rowspan="1" colspan="1">Our method (ensemble)</td><td rowspan="1" colspan="1">Modified-SMOTEBagging</td><td rowspan="1" colspan="1">10x5cv</td><td rowspan="1" colspan="1">95.11</td><td rowspan="1" colspan="1">97.91</td><td rowspan="1" colspan="1">0.965</td><td rowspan="1" colspan="1">97.25</td></tr></tbody></table><table-wrap-foot><fn id="gks878-TF6"><p>SE, SP and ACC represent sensitivity, specificity and accuracy,
respectively.</p></fn></table-wrap-foot></table-wrap></p>
      <p>Besides, it has been known that the plant pre-miRNA is different from animal pre-miRNA in
several aspects, mainly in hairpin loop structure and size, with size ranging from 60 to
500 nt and containing short loops and long stems. In order to compare our ensemble
sensitivity with other existing methods trained on plants pre-miRNAs, we used the same
testing data as PlantMiRNAPred. The comparison of our classifier performance with the
results reported in (<xref ref-type="bibr" rid="gks878-B14">14</xref>) is given in <xref ref-type="table" rid="gks878-T8">Table 8</xref>. As many plant pre-miRNAs contain
multi-loop (<xref ref-type="bibr" rid="gks878-B14">14</xref>), our method can classify
them correctly with the highest accuracy. This can be inferred that our method is
sensitive enough to identify pre-miRNAs with multi-loop. These results suggested that our
method performs with the highest sensitivity across plant and animal species, followed by
the yasMir (<xref ref-type="bibr" rid="gks878-B12">12</xref>), which is the second best
when the 1st and 3rd testing data were tested. As a consequence, the yasMir method was
also included in comparison in the next sections, in which we downloaded the yasMir
program and performed the test on our 2nd and 4th testing data. <table-wrap id="gks878-T8" position="float"><label>Table 8.</label><caption><p>Sensitivity performance on plant specie
pre-miRNAs</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Species</th><th rowspan="1" colspan="1">No. of sequences</th><th colspan="4" align="center" rowspan="1">Accuracy (%)<hr/></th><th rowspan="1" colspan="1">Our method</th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">PlantMiRNA Pred (<xref ref-type="bibr" rid="gks878-B12">12</xref>)</th><th rowspan="1" colspan="1">Triplet-SVM (<xref ref-type="bibr" rid="gks878-B6">6</xref>)</th><th rowspan="1" colspan="1">microPred (<xref ref-type="bibr" rid="gks878-B9">9</xref>)</th><th rowspan="1" colspan="1">yasMir (<xref ref-type="bibr" rid="gks878-B10">10</xref>)</th><th rowspan="1" colspan="1"/></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">ath</td><td rowspan="1" colspan="1">180</td><td rowspan="1" colspan="1">92.22</td><td rowspan="1" colspan="1">76.06</td><td rowspan="1" colspan="1">89.44</td><td rowspan="1" colspan="1">97.78</td><td rowspan="1" colspan="1">99.44</td></tr><tr><td rowspan="1" colspan="1">osa</td><td rowspan="1" colspan="1">397</td><td rowspan="1" colspan="1">94.21</td><td rowspan="1" colspan="1">75.54</td><td rowspan="1" colspan="1">90.43</td><td rowspan="1" colspan="1">96.72</td><td rowspan="1" colspan="1">100</td></tr><tr><td rowspan="1" colspan="1">ptc</td><td rowspan="1" colspan="1">233</td><td rowspan="1" colspan="1">91.85</td><td rowspan="1" colspan="1">75.21</td><td rowspan="1" colspan="1">84.98</td><td rowspan="1" colspan="1">93.99</td><td rowspan="1" colspan="1">96.99</td></tr><tr><td rowspan="1" colspan="1">ppt</td><td rowspan="1" colspan="1">211</td><td rowspan="1" colspan="1">92.42</td><td rowspan="1" colspan="1">71.49</td><td rowspan="1" colspan="1">89.57</td><td rowspan="1" colspan="1">98.10</td><td rowspan="1" colspan="1">98.57</td></tr><tr><td rowspan="1" colspan="1">mtr</td><td rowspan="1" colspan="1">106</td><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">80.18</td><td rowspan="1" colspan="1">95.28</td><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">100</td></tr><tr><td rowspan="1" colspan="1">sbi</td><td rowspan="1" colspan="1">131</td><td rowspan="1" colspan="1">98.47</td><td rowspan="1" colspan="1">69.51</td><td rowspan="1" colspan="1">94.66</td><td rowspan="1" colspan="1">95.42</td><td rowspan="1" colspan="1">100</td></tr><tr><td rowspan="1" colspan="1">zma</td><td rowspan="1" colspan="1">97</td><td rowspan="1" colspan="1">97.94</td><td rowspan="1" colspan="1">66.97</td><td rowspan="1" colspan="1">93.81</td><td rowspan="1" colspan="1">97.94</td><td rowspan="1" colspan="1">96.90</td></tr><tr><td rowspan="1" colspan="1">gma</td><td rowspan="1" colspan="1">83</td><td rowspan="1" colspan="1">98.31</td><td rowspan="1" colspan="1">74.12</td><td rowspan="1" colspan="1">86.75</td><td rowspan="1" colspan="1">96.38</td><td rowspan="1" colspan="1">98.79</td></tr><tr><td rowspan="1" colspan="1">updated aly</td><td rowspan="1" colspan="1">191</td><td rowspan="1" colspan="1">97.91</td><td rowspan="1" colspan="1">70.98</td><td rowspan="1" colspan="1">91.62</td><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">100</td></tr><tr><td rowspan="1" colspan="1">updated gma</td><td rowspan="1" colspan="1">118</td><td rowspan="1" colspan="1">98.31</td><td rowspan="1" colspan="1">79.66</td><td rowspan="1" colspan="1">93.22</td><td rowspan="1" colspan="1">100</td><td rowspan="1" colspan="1">100</td></tr></tbody></table><table-wrap-foot><fn id="gks878-TF7"><p>All methods were tested on the testing data set of
PlantMiRNAPred (<xref ref-type="bibr" rid="gks878-B14">14</xref>).</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec>
      <title>High sensitivity of our ensemble</title>
      <p>We evaluated the predictive power of the ensemble by applying it to predict all known
pre-miRNA taken from miRBase version 17 and up-to-date version 18 (the 2nd testing set).
This testing data is an across species pre-miRNA containing all pre-miRNAs from animal,
plant and virus species. Our ensemble can achieve high accuracy of 92.89, 97.38 and
94.17% when testing across 93 animal species, 52 plant species and 23 virus
species, respectively (<ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/full/gks878/DC1">Supplementary
Table S5</ext-link>). Our methods—trained using human, monocot plant and dicot
plant species—is applicable to animal, plant and virus species with high accuracy.
Although the miRBase is a main miRNA repository, it contains published pre-miRNAs from
both experimental and computational results. In order to test solely on experimentally
verified pre-miRNA, we retrieved pre-miRNAs from the miRNAMap (<xref ref-type="bibr" rid="gks878-B59">59</xref>). The testing results on pre-miRNAs from miRNAMap are given
in <ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/full/gks878/DC1">Supplementary
Table S6</ext-link>. Our method achieved high accuracy of 97.29% when testing on
all experimentally verified pre-miRNA sequences from miRNAMap.</p>
      <p>We compared the performance of our ensemble with its individual classifiers of SVM, kNN
and RF; the results are shown in <ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/full/gks878/DC1">Supplementary
Figure S3</ext-link>. We also included another existing method called yasMir (<xref ref-type="bibr" rid="gks878-B12">12</xref>), which is a SVM-based classifier, in the
plot. As depicted in the plot, the ensemble got better prediction results compared with
single SVM, kNN, RF and yasMir in most testing cases. The ensemble model is a
high-performance approach, relatively, providing superior accuracy—higher than
single classifiers. This is due to the complementary role from each of the 12 classifier
members in our ensemble model. This result is consistent with the previous findings (<xref ref-type="bibr" rid="gks878-B30">30</xref>,<xref ref-type="bibr" rid="gks878-B56">56</xref>) that the bagging-based classifier is almost always more accurate than single
individual classifiers in most testing cases while the boosting-based classifier could be
less accurate than single individual classifiers in some cases.</p>
      <p>Not only does the algorithm affects performance of prediction, but also our
discriminative features, SC and its derivatives, to improve the efficiency of our model.
To give the supported evidences that our novel features would significantly distinguish
real pre-miRNAs from other stem-loop sequences in the testing data, average values of SC
and its SC-base pair composite features, across different groups of organisms in our
testing data, including those of negative data set are calculated and presented in
<ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/full/gks878/DC1">Supplementary
Table S7</ext-link>. Average values of MFE, a well-known feature, across different
groups of organisms were also given. The MFE values of small ncRNAs fall into the range of
−33.16 ± 24.17 similar to those of animal pre-miRNAs. In addition, we
observed high MFE and high variation in MFE distribution of plant pre-miRNAs. This shows
that MFE can be used to distinguish pre-miRNAs from random pseudo hairpins, but cannot
differentiate the real pre-miRNAs from other small stem-loop forming ncRNAs. Consistent
with the training data, the average values of SC and three SC-base pair composite features
of all pre-miRNAs in testing data were significantly higher than those of other negative
hairpin sequences. The distributions of MFE, SC and SC derivative values for the testing
data were plotted as shown in <ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/full/gks878/DC1">Supplementary
Figure S4</ext-link>. In contrast to the MFE, well separations between positive and
negative data were found in SC and SC-base pair composite features. The SC and our three
SC-base pair composite features are useful for distinguishing real pre-miRNA hairpins,
both plant and animal, from pseudo hairpin and other ncRNA sequences effectively.
Moreover, the viral pre-miRNAs, known to evolve rapidly from plant and animal pre-miRNAs
(<xref ref-type="bibr" rid="gks878-B60">60</xref>) also show a similar trend in SC and
SC-base pair composite features to plant and animal pre-miRNAs. This confirmed that the
pre-miRNAs possess unique functional structure that distinguishes them from other hairpin
structures.</p>
    </sec>
    <sec>
      <title>High specificity of our ensemble</title>
      <p>The ability to reduce FPR is essential in the computational identification of pre-miRNA
sequences. To assess the FPR of our ensemble, we compared our method with yasMir, the
second best performance in terms of sensitivity, on the 4th testing data set. The results
showed that the ensemble had the FPR of 6.26% for classifying miRNA from
pseudo-hairpins, 11.65% for classifying miRNA from shuffle sequences, and
16.78% for classifying miRNA from other functional ncRNA (<xref ref-type="table" rid="gks878-T9">Table 9</xref>). This suggested that the method had a low FPR
(11.56%), which was relatively low for scanning pre-miRNA sequences in genomes
compared with the yasMir algorithm. We also applied our method in a more realistic
situation as a computational pipeline for pre-miRNA scanning on the genome scale as
reported in <ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/full/gks878/DC1">Supplementary
Method S5</ext-link>. The ensemble, the voting of multi-expert classifiers, is known as
an effective way of increasing specificity through voting and of giving lower false
positive results than a single classifier. Our <italic>ab-initio</italic> ensemble based
method has proved in this and previous sections that it can predict pre-miRNAs with high
sensitivity and specificity. <table-wrap id="gks878-T9" position="float"><label>Table
9.</label><caption><p>Specificity of our ensemble when applied to the negative
testing data, compared with yasMir (the 2nd best sensitivity from <xref ref-type="table" rid="gks878-T8">Table 8</xref>)</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Negative data</th><th rowspan="1" colspan="1">No. of sequences</th><th colspan="2" align="center" rowspan="1">Our method<hr/></th><th colspan="2" align="center" rowspan="1">yasMir<hr/></th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">Correctly classified (%)</th><th rowspan="1" colspan="1">FPR (%)</th><th rowspan="1" colspan="1">Correctly classified (%)</th><th rowspan="1" colspan="1">FPR (%)</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">Pseudo hairpin</td><td rowspan="1" colspan="1">4494</td><td rowspan="1" colspan="1">93.74</td><td rowspan="1" colspan="1">6.26</td><td rowspan="1" colspan="1">86.91</td><td rowspan="1" colspan="1">13.09</td></tr><tr><td rowspan="1" colspan="1">Shuffle</td><td rowspan="1" colspan="1">21 470</td><td rowspan="1" colspan="1">88.35</td><td rowspan="1" colspan="1">11.65</td><td rowspan="1" colspan="1">83.69</td><td rowspan="1" colspan="1">16.31</td></tr><tr><td rowspan="1" colspan="1">IE-NC (1238ncRNA)</td><td rowspan="1" colspan="1">12 387</td><td rowspan="1" colspan="1">83.22</td><td rowspan="1" colspan="1">16.78</td><td rowspan="1" colspan="1">82.95</td><td rowspan="1" colspan="1">17.05</td></tr><tr><td rowspan="1" colspan="1">Average</td><td rowspan="1" colspan="1">12 784</td><td rowspan="1" colspan="1">88.44</td><td rowspan="1" colspan="1">11.56</td><td rowspan="1" colspan="1">84.52</td><td rowspan="1" colspan="1">15.48</td></tr></tbody></table><table-wrap-foot><fn id="gks878-TF8"><p>Correctly classified (%) is the percent of the correctly
classified as not pre-miRNAs, FPR (%) is the false positive rate.</p></fn></table-wrap-foot></table-wrap></p>
      <p>The accuracy of the method can be affected by the reliability of the training data. A
recent study (<xref ref-type="bibr" rid="gks878-B61">61</xref>) demonstrated that commonly
used positive and negative control data may be unreliable, and provided a new set of
control data: high confidence positive control data with functional evidences and negative
control data with no evidence of processing by Dicer. Our method was also tested with
these novel control data. It yielded accuracy of 100% for positive control and
accuracy of 98.09% for negative data. As given in <ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/full/gks878/DC1">Supplementary
Method S6</ext-link>, our method predicted almost all positive control (127 out of 129)
as pre-miRNA with the highest probability of 1.0, whereas 2 of 129 were predicted as
pre-miRNA with high probability of 0.75. This result again confirmed that our
discriminative features and algorithm work well in identifying bona fide functional
pre-miRNAs.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions">
    <title>CONCLUSION</title>
    <p>Various ML algorithms—including NB, MLP, J48, SVM, kNN, RBFNets, RIPPER and
RF—were applied to discriminate real microRNA precursors from pseudo-hairpin sequences
and other ncRNAs. The comparison performance of each algorithm on the pre-miRNA
classification task was performed. Since different learning algorithms have different
strengths and weaknesses, we proposed to apply a heterogeneous ensemble to improve miRNA
hairpin classification. The heterogeneous ensemble method has shown to improve the
performance in terms of sensitivity-specificity tradeoff. The method contributes towards an
improvement of miRNA hairpin classification by the following reasons. Firstly, this vote of
multiple diverse classifiers could have better and more reliable prediction than a single
classifier since it can reduce the chance of incorrect classification in single algorithms.
Secondly, the ensemble incorporated with the modified-SMOTEbagging techniques is an
effective way to handle class-imbalanced problems occurring in pre-miRNA data. Each base
classifier in the ensemble is trained on a well-balanced subset of the training data, which
makes our model better for classifying the minority class (pre-miRNAs) than those of
class-imbalanced data. Thirdly, the ensemble can give an optimized answer with respect to
sensitivity, specificity and accuracy by selected RF (one member that gives the highest
performance in identifying the positive class), selected kNN (one member that gives the
highest performance in filtering out the negative class) and selected SVM (one algorithm in
the ensemble that can give better tradeoff between true positive and false positive),
respectively. The aggregation of these algorithms increased the possibility that the
ensemble truly represented the characteristics of pre-miRNAs. Finally, our ensemble also
incorporated robust features, that is, our SC-base pair composite features, proven to be the
most informative from the feature set that can efficiently discriminate true pre-miRNA
hairpins.</p>
    <p>Unlike previous methods, ours was trained on the data set containing human and plant
pre-miRNAs. The overall CV prediction accuracy was 96.54% for our ensemble, which
significantly outperformed all other learning methods at 95% confidence level. We
also tested the performance of the ensemble on cross-species data taken from miRBase18. The
results demonstrated that the method performs well across animal, plant and virus species
with accuracy of 92.89, 97.38 and 94.17%, respectively. In conclusion, integrating
the resampling techniques and discriminative feature set to the miRNA heterogeneous ensemble
classification algorithm can improve the accuracy of miRNA hairpin classification.</p>
  </sec>
  <sec>
    <title>SUPPLEMENTARY DATA</title>
    <p><ext-link ext-link-type="uri" xlink:href="http://nar.oxfordjournals.org/cgi/content/full/gks878/DC1">Supplementary
Data</ext-link> are available at NAR Online: Supplementary Tables 1–7, Supplementary
Figures 1–4, Supplementary Methods 1–6, Supplementary Data 1–2 and
Supplementary References [62–73].</p>
  </sec>
  <sec>
    <title>FUNDING</title>
    <p><funding-source>National Research University Project of Thailand’s Office of the
Higher Education Commission</funding-source> [<award-id>54000318</award-id>]. Funding for
open access charge: King Mongkut's University of Technology Thonburi.</p>
    <p><italic>Conflict of interest statement</italic>. None declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="PMC_1" content-type="local-data">
      <caption>
        <title>Supplementary Data</title>
      </caption>
      <media mimetype="text" mime-subtype="html" xlink:href="supp_41_1_e21__index.html"/>
      <media xlink:role="associated-file" mimetype="application" mime-subtype="vnd.ms-excel" xlink:href="supp_gks878_nar-01935-met-g-2012-File001.xlsx"/>
      <media xlink:role="associated-file" mimetype="application" mime-subtype="x-zip-compressed" xlink:href="supp_gks878_nar-01935-met-g-2012-File002.zip"/>
      <media xlink:role="associated-file" mimetype="application" mime-subtype="msword" xlink:href="supp_gks878_Revised_SupplementaryFile_gks878.docx"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <title>ACKNOWLEDGEMENTS</title>
    <p>The authors are thankful to the Bioinformatics &amp; System Biology Program, KMUTT members:
Dr Weerayuth Kittichotirat, Dr Teeraphan Laomettachit and Mr Lee Olsen from KMUTT for their
help in editing and proofreading the manuscript.</p>
  </ack>
  <ref-list>
    <title>REFERENCES</title>
    <ref id="gks878-B1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bartel</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>MicroRNAs: genomics, biogenesis, mechanism, and function</article-title>
        <source>Cell</source>
        <year>2004</year>
        <volume>116</volume>
        <fpage>281</fpage>
        <lpage>297</lpage>
        <pub-id pub-id-type="pmid">14744438</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>MicroRNA biogenesis: coordinated cropping and dicing</article-title>
        <source>Mol. Cell Biol.</source>
        <year>2005</year>
        <volume>6</volume>
        <fpage>376</fpage>
        <lpage>385</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mendes</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Freitas</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sagot</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Current tools for identification of miRNA genes and their
targets</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2009</year>
        <volume>37</volume>
        <fpage>2419</fpage>
        <lpage>2433</lpage>
        <pub-id pub-id-type="pmid">19295136</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ritchie</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Legendre</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gautheret</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>RNA stem-loops: To be or not to be cleaved by RNAse III</article-title>
        <source>RNA</source>
        <year>2007</year>
        <volume>13</volume>
        <fpage>457</fpage>
        <lpage>462</lpage>
        <pub-id pub-id-type="pmid">17299129</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nam</surname>
            <given-names>JW</given-names>
          </name>
          <name>
            <surname>Shin</surname>
            <given-names>KR</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>VN</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>BT</given-names>
          </name>
        </person-group>
        <article-title>Human microRNA prediction through a probabilistic co-learning model of
sequence and structure</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2005</year>
        <volume>33</volume>
        <fpage>3570</fpage>
        <lpage>3581</lpage>
        <pub-id pub-id-type="pmid">15987789</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gerlach</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kriventseva</surname>
            <given-names>EV</given-names>
          </name>
          <name>
            <surname>Rahman</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Vejnar</surname>
            <given-names>CE</given-names>
          </name>
          <name>
            <surname>Zdobnov</surname>
            <given-names>EM</given-names>
          </name>
        </person-group>
        <article-title>miROrtho: computational survey of microRNA genes</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2009</year>
        <volume>37</volume>
        <fpage>D111</fpage>
        <lpage>D117</lpage>
        <pub-id pub-id-type="pmid">18927110</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lindow</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gorodkin</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Principles and limitations of computational microRNA gene and target
finding</article-title>
        <source>DNA Cell Biol.</source>
        <year>2007</year>
        <volume>26</volume>
        <fpage>339</fpage>
        <lpage>351</lpage>
        <pub-id pub-id-type="pmid">17504029</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xue</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Classification of real and pseudo microRNA precursors using local
structure-sequence features and support vector machine</article-title>
        <source>BMC Bioinformatics</source>
        <year>2005</year>
        <volume>6</volume>
        <fpage>310</fpage>
        <pub-id pub-id-type="pmid">16381612</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Loong</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Mishra</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>De novo SVM classification of precursor microRNAs from genomic pseudo
hairpins using global and intrinsic folding measures</article-title>
        <source>Bioinformatics</source>
        <year>2007</year>
        <volume>23</volume>
        <fpage>1321</fpage>
        <lpage>1330</lpage>
        <pub-id pub-id-type="pmid">17267435</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>MiPred: classification of real and pseudo microRNA precursors using random
forest prediction model with combined features</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2007</year>
        <volume>35</volume>
        <fpage>W339</fpage>
        <lpage>W344</lpage>
        <pub-id pub-id-type="pmid">17553836</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Batuwita</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Palade</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>MicroPred: effective classification of pre-miRNAs for human miRNA gene
prediction</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <fpage>989</fpage>
        <lpage>995</lpage>
        <pub-id pub-id-type="pmid">19233894</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pasaila</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Sucila</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mohorianu</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Panţiru</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ciortuz</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>MiRNA recognition with the yasMiR System: the quest for further
improvements</article-title>
        <source>Adv. Exp. Med. Biol.</source>
        <year>2011</year>
        <volume>696</volume>
        <fpage>17</fpage>
        <lpage>25</lpage>
        <pub-id pub-id-type="pmid">21431542</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B13">
      <label>13</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Ionita</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ciortuz</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>MiRNA features for automated classification</article-title>
        <source>Proceedings of the 4th International Soft Computing Application IEEE Computer
Society (SOFA’2010)</source>
        <year>2010</year>
        <publisher-loc>Arad, Romania</publisher-loc>
        <fpage>125</fpage>
        <lpage>130</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xuan</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>PlantMiRNAPred: efficient classification of real and pseudo plant
pre-miRNAs</article-title>
        <source>Bioinformatics</source>
        <year>2011</year>
        <volume>27</volume>
        <fpage>1368</fpage>
        <lpage>1376</lpage>
        <pub-id pub-id-type="pmid">21441575</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>PMirP: a pre-microRNA prediction method based on structure-sequence hybrid
features</article-title>
        <source>Artif. Intell. Med.</source>
        <year>2010</year>
        <volume>49</volume>
        <fpage>127</fpage>
        <lpage>132</lpage>
        <pub-id pub-id-type="pmid">20399081</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ding</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>MiRenSVM: towards better prediction of microRNA precursors using an
ensemble SVM classifier with multi-loop features</article-title>
        <source>GIW2010, BMC Bioinformatics</source>
        <year>2010</year>
        <volume>11</volume>
        <issue>Suppl. 11</issue>
        <fpage>S11</fpage>
      </element-citation>
    </ref>
    <ref id="gks878-B17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Frank</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Hall</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Trigg</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Holmes</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Witten</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Data mining in bioinformatics using Weka</article-title>
        <source>Bioinformatics</source>
        <year>2004</year>
        <volume>20</volume>
        <fpage>2479</fpage>
        <lpage>2481</lpage>
        <pub-id pub-id-type="pmid">15073010</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zomaya</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>A review of ensemble methods in bioinformatics</article-title>
        <source>Curr. Bioinformatics</source>
        <year>2010</year>
        <volume>5</volume>
        <fpage>296</fpage>
        <lpage>308</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wolpert</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Macready</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>No free lunch theorems for optimization</article-title>
        <source>IEEE Trans. Evol. Comput.</source>
        <year>1997</year>
        <volume>1</volume>
        <fpage>67</fpage>
        <lpage>82</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B20">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rice</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>The algorithm selection problem</article-title>
        <source>Adv. Comp.</source>
        <year>1976</year>
        <volume>15</volume>
        <fpage>65</fpage>
        <lpage>118</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Multiple classifier integration for the prediction of protein structural
classes</article-title>
        <source>J. Comput. Chem.</source>
        <year>2009</year>
        <volume>30</volume>
        <fpage>2248</fpage>
        <lpage>2254</lpage>
        <pub-id pub-id-type="pmid">19274708</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B22">
      <label>22</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Kuncheva</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <source>Combining Pattern Classifiers</source>
        <year>2004</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Wiley-Interscience</publisher-name>
      </element-citation>
    </ref>
    <ref id="gks878-B23">
      <label>23</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bian</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>On diversity and accuracy of homogeneous and heterogeneous
ensembles</article-title>
        <source>Int. J. Hyb. Intell. Syst.</source>
        <year>2007</year>
        <volume>4</volume>
        <fpage>103</fpage>
        <lpage>128</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dietterich</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>An experimental comparison of three methods for constructing ensembles of
decision trees: Bagging, boosting, and randomization</article-title>
        <source>Mach. Learn.</source>
        <year>2000</year>
        <volume>40</volume>
        <fpage>139</fpage>
        <lpage>158</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B25">
      <label>25</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Dietterich</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Ensemble methods in machine learning</article-title>
        <source>Proceedings of the First International Workshop on Multiple Classifier Systems
(MSC’00)</source>
        <year>2000</year>
        <publisher-loc>Cagliari, Italy</publisher-loc>
        <fpage>1</fpage>
        <lpage>15</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B26">
      <label>26</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Bagging predictors</article-title>
        <source>Mach. Learn.</source>
        <year>1996</year>
        <volume>26</volume>
        <fpage>123</fpage>
        <lpage>140</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B27">
      <label>27</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Random forests</article-title>
        <source>Mach. Learn.</source>
        <year>2001</year>
        <volume>45</volume>
        <fpage>5</fpage>
        <lpage>32</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B28">
      <label>28</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bauer</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Kohavi</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>An empirical comparison of voting classification algorithms: Bagging,
boosting and variants</article-title>
        <source>Mach. Learn.</source>
        <year>1999</year>
        <volume>36</volume>
        <fpage>525</fpage>
        <lpage>536</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B29">
      <label>29</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dzeroski</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zenko</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Is combining classifiers with stacking better than selecting the best
one</article-title>
        <source>Mach. Learn.</source>
        <year>2004</year>
        <volume>54</volume>
        <fpage>255</fpage>
        <lpage>273</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B30">
      <label>30</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Opitz</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Maclin</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Popular ensemble methods: an empirical study</article-title>
        <source>J. Artif. Intell. Res.</source>
        <year>1999</year>
        <volume>11</volume>
        <fpage>169</fpage>
        <lpage>198</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B31">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Larranaga</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Calvo</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Santana</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Bielza</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Galdiano</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Inza</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Lazano</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Armananzas</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Santafe</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Perez</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Machine learning in bioinformatics</article-title>
        <source>Brief. Bioinformatics</source>
        <year>2005</year>
        <volume>7</volume>
        <fpage>86</fpage>
        <lpage>112</lpage>
        <pub-id pub-id-type="pmid">16761367</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B32">
      <label>32</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Eliis</surname>
            <given-names>LB</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Meta-prediction of protein subcellular localization with reduced
voting</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2007</year>
        <volume>35</volume>
        <fpage>e96</fpage>
        <pub-id pub-id-type="pmid">17670799</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B33">
      <label>33</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thakur</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Wanchana</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bruskiewich</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Quick</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Mosig</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Characterization of statistical features for plant microRNA
prediction</article-title>
        <source>BMC Genomics</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>108</fpage>
        <pub-id pub-id-type="pmid">21324149</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B34">
      <label>34</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Stellwag</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Large scale genome analysis reveals unique features of
microRNAs</article-title>
        <source>Gene</source>
        <year>2009</year>
        <volume>443</volume>
        <fpage>100</fpage>
        <lpage>109</lpage>
        <pub-id pub-id-type="pmid">19422892</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B35">
      <label>35</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Borenstein</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Ruppin</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Direct evolution of genetic robustness in microRNA</article-title>
        <source>Proc. Natl Acad. Sci. USA</source>
        <year>2006</year>
        <volume>103</volume>
        <fpage>6593</fpage>
        <lpage>6598</lpage>
        <pub-id pub-id-type="pmid">16608911</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B36">
      <label>36</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Ni</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bo</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>In silico genetic robustness analysis of secondary structural elements in
the miRNA gene</article-title>
        <source>J. Mol. Evol.</source>
        <year>2008</year>
        <volume>67</volume>
        <fpage>560</fpage>
        <lpage>569</lpage>
        <pub-id pub-id-type="pmid">18941828</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B37">
      <label>37</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Ni</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bo</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>RSRE: RNA structural robustness evaluator</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2007</year>
        <volume>35</volume>
        <fpage>W314</fpage>
        <lpage>W319</lpage>
        <pub-id pub-id-type="pmid">17567615</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B38">
      <label>38</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Price</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Cartweight</surname>
            <given-names>RA</given-names>
          </name>
          <name>
            <surname>Sabath</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Graur</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Azevedo</surname>
            <given-names>RB</given-names>
          </name>
        </person-group>
        <article-title>Neutral evolution of robustness in Drosophila microRNA
precursors</article-title>
        <source>Mol. Biol. Evol.</source>
        <year>2011</year>
        <volume>28</volume>
        <fpage>2115</fpage>
        <lpage>2123</lpage>
        <pub-id pub-id-type="pmid">21285032</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B39">
      <label>39</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Self containment, a property of modular RNA structures, distinguishes
microRNAs</article-title>
        <source>PLoS Comput. Biol.</source>
        <year>2008</year>
        <volume>4</volume>
        <fpage>e1000150</fpage>
        <pub-id pub-id-type="pmid">18725951</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B40">
      <label>40</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Griffiths-Jones</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>The microRNA registry</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2004</year>
        <volume>32</volume>
        <fpage>D109</fpage>
        <lpage>D111</lpage>
        <pub-id pub-id-type="pmid">14681370</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B41">
      <label>41</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Griffiths-Jones</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Moxon</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Marshall</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Khanna</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Eddy</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Bateman</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Rfam: annotating non-coding RNAs in complete genomes</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2005</year>
        <volume>33</volume>
        <fpage>D121</fpage>
        <lpage>D141</lpage>
        <pub-id pub-id-type="pmid">15608160</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B42">
      <label>42</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Toward integrating feature selection algorithms for classification and
clustering</article-title>
        <source>IEEE Trans. Knowl. Data Eng.</source>
        <year>2005</year>
        <volume>17</volume>
        <fpage>491</fpage>
        <lpage>502</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B43">
      <label>43</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hall</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Holmes</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Benchmarking attribute selection techniques for discrete class data
mining</article-title>
        <source>IEEE Trans. Knowl. Data Eng.</source>
        <year>2003</year>
        <volume>15</volume>
        <fpage>1437</fpage>
        <lpage>1447</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B44">
      <label>44</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Šikonja</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Kononenko</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Theoretical and empirical analysis of ReliefF and RReliefF</article-title>
        <source>Mach. Learn.</source>
        <year>2003</year>
        <volume>53</volume>
        <fpage>23</fpage>
        <lpage>69</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B45">
      <label>45</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Saeys</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Inza</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Larranaga</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>A review of feature selection techniques in bioinformatics</article-title>
        <source>Bioinformatics</source>
        <year>2007</year>
        <volume>23</volume>
        <fpage>2507</fpage>
        <lpage>2517</lpage>
        <pub-id pub-id-type="pmid">17720704</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B46">
      <label>46</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Witten</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Frank</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <source>Data Mining: Practical Machine Learning Tools and Techniques</source>
        <year>2005</year>
        <edition>2nd edn</edition>
        <publisher-loc>San Francisco</publisher-loc>
        <publisher-name>Morgan Kaufmann Inc</publisher-name>
      </element-citation>
    </ref>
    <ref id="gks878-B47">
      <label>47</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Chang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <source>LIBSVM : A Library for Support Vector Machines</source>
        <year>2001</year>
        <comment><ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~cjlin/libsvm">http://www.csie.ntu.edu.tw/∼cjlin/libsvm</ext-link> (9 September 2012, date last
accessed)</comment>
      </element-citation>
    </ref>
    <ref id="gks878-B48">
      <label>48</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Manzalawy</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Honavar</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <source>WLSVM : Integrating LibSVM into Weka Environment</source>
        <year>2005</year>
        <comment><ext-link ext-link-type="uri" xlink:href="http://www.cs.iastate.edu/~yasser/wlsvm">http://www.cs.iastate.edu/∼yasser/wlsvm</ext-link> (9 September 2012, date last
accessed)</comment>
      </element-citation>
    </ref>
    <ref id="gks878-B49">
      <label>49</label>
      <element-citation publication-type="book">
        <collab>R. Development Core Team</collab>
        <source>R: A Language and Environment for Statistical Computing, Reference Index Version
2.2.1.</source>
        <year>2005</year>
        <comment><ext-link ext-link-type="uri" xlink:href="http://www.R-project.org">http://www.R-project.org</ext-link> (9 September
2012, date last accessed)</comment>
      </element-citation>
    </ref>
    <ref id="gks878-B50">
      <label>50</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chawla</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Bowyer</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Hall</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Kegelmeyer</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Synthetic Minority Over-sampling Technique</article-title>
        <source>J. Artif. Intell. Res.</source>
        <year>2002</year>
        <volume>16</volume>
        <fpage>321</fpage>
        <lpage>357</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B51">
      <label>51</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kohavi</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A study of cross-validation and bootstrap for accuracy estimation and model
selection</article-title>
        <source>Proceedings of the 14th International Joint Conference on Artificial Intelligence
(IJCAJ’95), Montreal Quebec, Canada</source>
        <year>1995</year>
        <volume>Vol. 2</volume>
        <fpage>1137</fpage>
        <lpage>1143</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B52">
      <label>52</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Chawla</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Maimon</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Rokach</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Data mining for imbalanced datasets: an overview</article-title>
        <source>The Data Mining and Knowledge Discovery Handbook</source>
        <year>2010</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>853</fpage>
        <lpage>867</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B53">
      <label>53</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mucciardi</surname>
            <given-names>AN</given-names>
          </name>
          <name>
            <surname>Gose</surname>
            <given-names>EE</given-names>
          </name>
        </person-group>
        <article-title>A comparison of seven techniques for choosing subsets of pattern
recognition properties</article-title>
        <source>IEEE Trans. Comput.</source>
        <year>1971</year>
        <volume>c-20</volume>
        <fpage>1023</fpage>
        <lpage>1031</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B54">
      <label>54</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hido</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kashima</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Takahashi</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Roughly balanced bagging for imbalanced data</article-title>
        <source>Stat. Anal. Data Min.</source>
        <year>2009</year>
        <volume>2</volume>
        <fpage>412</fpage>
        <lpage>426</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B55">
      <label>55</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chawla</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Sylvester</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Exploiting diversity in ensembles: improving the performance on unbalanced
datasets</article-title>
        <source>LNCS</source>
        <year>2007</year>
        <volume>4472</volume>
        <fpage>397</fpage>
        <lpage>406</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B56">
      <label>56</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Khoshgoftaar</surname>
            <given-names>TM</given-names>
          </name>
          <name>
            <surname>Hulse</surname>
            <given-names>JV</given-names>
          </name>
          <name>
            <surname>Napolitano</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Comparing boosting and bagging techniques with noisy and imbalanced
data</article-title>
        <source>IEEE Trans. Syst. Man Cyber</source>
        <year>2011</year>
        <volume>41</volume>
        <fpage>552</fpage>
        <lpage>568</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B57">
      <label>57</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Galar</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Fernandez</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Barrenechea</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Bustince</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Herrera</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A review on ensembles for the class imbalance problem: bagging-, boosting-,
and hybrid-based approaches</article-title>
        <source>IEEE Trans. Syst. Man Cyber</source>
        <year>2011</year>
        <volume>42</volume>
        <fpage>463</fpage>
        <lpage>484</lpage>
      </element-citation>
    </ref>
    <ref id="gks878-B58">
      <label>58</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Qu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Qu</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>mirExplorer Detecting microRNAs from genome and next generation sequencing
data using the Adaboost method with transition probability matrix and combined
features</article-title>
        <source>RNA Biol.</source>
        <year>2011</year>
        <volume>8</volume>
        <fpage>922</fpage>
        <lpage>934</lpage>
        <pub-id pub-id-type="pmid">21881406</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B59">
      <label>59</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hsu</surname>
            <given-names>PW</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>HD</given-names>
          </name>
          <name>
            <surname>Hsu</surname>
            <given-names>SD</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>LZ</given-names>
          </name>
          <name>
            <surname>Tsou</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Tseng</surname>
            <given-names>CP</given-names>
          </name>
          <name>
            <surname>Stadler</surname>
            <given-names>PF</given-names>
          </name>
          <name>
            <surname>Washietl</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Hofacker</surname>
            <given-names>IL</given-names>
          </name>
        </person-group>
        <article-title>miRNAMap: genomic maps of microRNA genes and their target genes in
mammalian genomes</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2006</year>
        <volume>34</volume>
        <fpage>D135</fpage>
        <lpage>D139</lpage>
        <pub-id pub-id-type="pmid">16381831</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B60">
      <label>60</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ghosh</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Mallick</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Chakrabarti</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Cellular versus viral microRNAs in host–virus
interaction</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2009</year>
        <volume>37</volume>
        <fpage>1035</fpage>
        <lpage>1048</lpage>
        <pub-id pub-id-type="pmid">19095692</pub-id>
      </element-citation>
    </ref>
    <ref id="gks878-B61">
      <label>61</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ritchie</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rasko</surname>
            <given-names>JE</given-names>
          </name>
        </person-group>
        <article-title>Defining and providing robust controls for microRNA
prediction</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <fpage>1058</fpage>
        <lpage>1061</lpage>
        <pub-id pub-id-type="pmid">22408193</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
