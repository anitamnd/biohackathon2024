<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8055224</article-id>
    <article-id pub-id-type="pmid">32706854</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa673</article-id>
    <article-id pub-id-type="publisher-id">btaa673</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Applications Notes</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Systems Biology</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Scalable machine learning-assisted model exploration and inference using Sciope</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Singh</surname>
          <given-names>Prashant</given-names>
        </name>
        <xref ref-type="author-notes" rid="btaa673-FM1"/>
        <xref ref-type="aff" rid="btaa673-aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wrede</surname>
          <given-names>Fredrik</given-names>
        </name>
        <xref ref-type="author-notes" rid="btaa673-FM1"/>
        <xref ref-type="aff" rid="btaa673-aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hellander</surname>
          <given-names>Andreas</given-names>
        </name>
        <xref ref-type="corresp" rid="btaa673-cor1"/>
        <xref ref-type="aff" rid="btaa673-aff1"/>
        <!--<email>andreas.hellander@it.uu.se</email>-->
      </contrib>
    </contrib-group>
    <aff id="btaa673-aff1"><institution>Department of Information Technology, Uppsala University</institution>, 751 05 Uppsala, <country country="SE">Sweden</country></aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Luigi Martelli</surname>
          <given-names>Pier</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <fn id="btaa673-FM1">
        <p>The authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors.</p>
      </fn>
      <corresp id="btaa673-cor1">To whom correspondence should be addressed. <email>andreas.hellander@it.uu.se</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>15</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-07-24">
      <day>24</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>37</volume>
    <issue>2</issue>
    <fpage>279</fpage>
    <lpage>281</lpage>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>2</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>18</day>
        <month>5</month>
        <year>2020</year>
      </date>
      <date date-type="editorial-decision">
        <day>02</day>
        <month>7</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>7</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa673.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Summary</title>
        <p>Discrete stochastic models of gene regulatory networks are fundamental tools for <italic>in silico</italic> study of stochastic gene regulatory networks. Likelihood-free inference and model exploration are critical applications to study a system using such models. However, the massive computational cost of complex, high-dimensional and stochastic modelling currently limits systematic investigation to relatively simple systems. Recently, machine-learning-assisted methods have shown great promise to handle larger, more complex models. To support both ease-of-use of this new class of methods, as well as their further development, we have developed the scalable inference, optimization and parameter exploration (Sciope) toolbox. Sciope is designed to support new algorithms for machine-learning-assisted model exploration and likelihood-free inference. Moreover, it is built ground up to easily leverage distributed and heterogeneous computational resources for convenient parallelism across platforms from workstations to clouds.</p>
      </sec>
      <sec id="s2">
        <title>Availability and implementation</title>
        <p>The Sciope Python3 toolbox is freely available on <ext-link ext-link-type="uri" xlink:href="https://github.com/Sciope/Sciope">https://github.com/Sciope/Sciope</ext-link>, and has been tested on Linux, Windows and macOS platforms.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary information</xref> is available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NIH</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NIH/2R01EB014877-04A1</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Göran Gustafsson foundation</institution>
            <institution-id institution-id-type="DOI">10.13039/100016408</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="3"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Stochastic models of biochemical reaction networks are an integral part of the systems biologist’s toolbox. By formulating discrete models from known or hypothesized molecular interactions, <italic>in silico</italic> analysis of complex biochemical processes is made possible. A key challenge encountered in modelling is characterized by very large uncertainties associated with model parameters. Given an efficient simulation method, two related applications can be discerned. In model parameter space exploration, the modeller’s objective is to use the simulator to screen for different qualitative behaviours displayed by the model under large variations in parameters. Model exploration is often the first step in understanding a system, and applies also when no experimental data are available. In <italic>model inference</italic>, the task is to fit model parameters to observed experimental data. A popular approach for parameter inference in systems biology is Approximate Bayesian Computation (ABC) (<xref rid="btaa673-B7" ref-type="bibr">Marin <italic>et al.</italic>, 2012</xref>). ABC inference requires substantial hyperparameter tuning (such as choosing the prior, tuning acceptance thresholds and distance metrics). ABC can become prohibitively slow for high-dimensional problems and it is of utmost importance to select informative summary statistics. Several open and capable software packages for ABC inference are available, such as PyABC (<xref rid="btaa673-B6" ref-type="bibr">Klinger <italic>et al.</italic>, 2018</xref>). However, traditional methods struggle with high-dimensional problems and stochastic descriptions.</p>
    <p>Machine-learning-assisted methods have been proposed to tackle this problem with a data-driven approach to both exploration and likelihood-free inference. Recently, we presented a human-in-the-loop workflow based on semi-supervised and active learning to aid model exploration (<xref rid="btaa673-B11" ref-type="bibr">Wrede and Hellander, 2019</xref>). For likelihood-free parameter inference, regression approaches using Random Forests (<xref rid="btaa673-B8" ref-type="bibr">Raynal <italic>et al.</italic>, 2019</xref>) and deep artificial neural networks (<xref rid="btaa673-B5" ref-type="bibr">Jiang <italic>et al.</italic>, 2018</xref>; <xref rid="btaa673-B10" ref-type="bibr">Wiqvist <italic>et al.</italic>, 2019</xref>), as well as classification (<xref rid="btaa673-B4" ref-type="bibr">Gutmann <italic>et al.</italic>, 2018</xref>), have been introduced in conjunction with ABC. We saw the need for a software toolbox that (i) focuses on making such ML-assisted methods easy-to-use for practical modelling projects, (ii) supports rapid development of new ML-assisted tools for exploration and inference and (iii) incorporates specific support for stochastic simulations of biochemical reaction networks. We here introduce the SCalable Inference, Optimization and Parameter Exploration (Sciope) toolbox. Sciope provides an integrated environment to generate initial parameter designs, to generate training data (by black-box simulation), to do massive feature generation and dimension reduction, and to build different types of surrogate models such as Gaussian Process models and Convolutional Neural Networks and use them for inference tasks. Sciope is also the main implementation of the human-in-the loop workflow presented in (<xref rid="btaa673-B11" ref-type="bibr">Wrede and Hellander, 2019</xref>). Sciope supports basic ABC routines for completeness, but is not intended to be a complete environment for the many flavours of ABC. However, it supports implementations of novel summary statistic learning via artificial neural networks. It can thus be a good complement for scalable pre-processing for other tools that specializes on ABC, such as PyABC. An overview of Sciope’s core features and contributions is listed in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>.</p>
    <p>The sheer computational cost associated with simulation and feature extraction for complex high-dimensional and stochastic models becomes a bottle-neck both for end-users and method developers. For this reason, Sciope is built with a Dask (Matthew<xref rid="btaa673-B2" ref-type="bibr"> 2015</xref>) backend to support massive parallelism on platforms from laptops to clouds. Sciope is realized as a Python3 toolbox and will form the backend in the next generation of the StochSS software-as-a a service (<xref rid="btaa673-B3" ref-type="bibr">Drawert <italic>et al.</italic>, 2016</xref>).</p>
  </sec>
  <sec>
    <title>2 Overview</title>
    <p>The only requirement to use Sciope is a user-provided black-box simulator that emits time series data in the supported format. Sciope includes wrappers for GillesPy2, a popular package to simulate discrete models of gene regulatory networks. Users in the systems biology application area thus only need to define their model using the Python API or using SBML. <xref ref-type="fig" rid="btaa673-F1">Figure 1A</xref> summarizes the unique features of Sciope. They include model exploration based on semi-supervised learning (<xref rid="btaa673-B11" ref-type="bibr">Wrede and Hellander, 2019</xref>), surrogate modelling, summary statistics selection and state-of-the-art deep neural network architectures to summarize data for ABC inference (<xref rid="btaa673-B1" ref-type="bibr">Åkesson <italic>et al.</italic>, 2020</xref>). Continued development will focus on enabling additional ML-assisted technologies for inference and exploration.
</p>
    <fig id="btaa673-F1" orientation="portrait" position="float">
      <label>Fig. 1.</label>
      <caption>
        <p>(<bold>A</bold>) Sciope is a high-level Python toolbox for scalable, ML-assisted inference and model parameter exploration with a large pool of features and utilities. The user is only required to provide a black-box simulator to be able to use Sciope. To use the distributed mode, the user also needs to setup a Dask cluster. The main supported features of each module, post-processing, assisted machine leaning, model exploration and design and sampling are presented in the figure. Examples are provided under <ext-link ext-link-type="uri" xlink:href="https://github.com/Sciope/Sciope/examples">https://github.com/Sciope/Sciope/examples</ext-link> and in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>. (<bold>B</bold>) From a computational point of view, parameter inference and model exploration workflows share multiple elements, such as repeated simulation of the model with different parameters, and the evaluation of a large number of summary statistics/features for each simulated time series. Both these steps are embarrassingly parallel. Sciope provides a scalable and unified solution for flexible parallel execution across different platforms via a high-level API built around Dask. The complexity of Dask’s low-level task interface is effectively hidden from the users. (<bold>C</bold>) Strong scalability test for the core computational routines (top) for two different parameter bounds (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Information</xref> for scalability efficiency and more details). Weak scalability test for ABC inference with rejection sampling where the number of trials are proportional to the number of cores (bottom)</p>
      </caption>
      <graphic xlink:href="btaa673f1"/>
    </fig>
    <p>A key feature is the easy setup of parallel computational experiments where the complexity is hidden from the user. Inference and model exploration tasks share many computationally expensive core routines, as illustrated in <xref ref-type="fig" rid="btaa673-F1">Figure 1B</xref>. Sciope parallelizes these stages using the Dask task backend, which provides the flexibility to scale out computations in modern cloud environments.</p>
  </sec>
  <sec>
    <title>3 Results</title>
    <p>The <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref> contains several examples of the toolbox workflow on example models from systems biology. They serve to demonstrate the machine learning-enabled exploration and inference capabilities highlighted in <xref ref-type="fig" rid="btaa673-F1">Figure 1A</xref>. Here, we demonstrate the performance of the library when used in distributed mode using cloud resources. We ran an exploration workflow and an ABC inference task for a well-known GRN model of a genetic oscillator (<xref rid="btaa673-B9" ref-type="bibr">Vilar <italic>et al.</italic>, 2002</xref>) (involving 15 kinetic parameters, see <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S1</xref> for parameter bounds).</p>
    <p><italic>Supporting near real-time model exploration workflows</italic>. In machine-learning-assisted model exploration, interactivity is of key importance since a human-in-the-loop guides the workflow and drives the next steps. Both of the visualization tools supported in Sciope, and the human interaction, function optimally with a maximum number of parameter points—a batch—observed in one exploration cycle. However, the wall time to do sampling, simulation and summary statistics extraction for a batch can be considerable for an expensive model. Sciope’s goal concerning model exploration is to converge to a near real-time experience by transiently scaling out the computations to a cloud or cluster.</p>
    <p>As can be seen in <xref ref-type="fig" rid="btaa673-F1">Figure 1C</xref> (top), the runtime for generating a batch of 1000 trial points converges to the runtime of the longest running individual simulation tasks, as illustrated by two different sizes of parameter ranges. The task granularity is one invocation of the simulator and a wide parameter range leads to a larger spread in simulation runtime, see <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S1</xref> for a detailed explanation.</p>
    <p><italic>High-throughput likelihood-free inference</italic>. In comparison with model exploration, ABC inference typically requires a very large number of trials but does not involve a human-in-the loop. Hence, latency is less of a concern. Instead, it is important to support high-throughput of trials for traditional ABC and generation of training data for ML approaches. <xref ref-type="fig" rid="btaa673-F1">Figure 1C</xref> (bottom) illustrates weak scaling during ABC inference. In this experiment, the number of desired accepted samples increases proportionally to the number of cores so that the workload per core stays constant. As can be observed, the runtime decreases only marginally with increased number of accepted samples. As we scale out, we are able to more efficiently handle the computations of longer simulation runtimes, and hence, we observe a slight decrease in the total runtime and variability.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>The work was funded by the NIH [NIH/2R01EB014877-04A1], the eSSENCE strategic collaboration on eScience and the Göran Gustafsson foundation.</p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btaa673_Supplementary_Data</label>
      <media xlink:href="btaa673_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa673-B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Åkesson</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) Convolutional neural networks as summary statistics for approximate bayesian computation. <italic>arXiv preprint, 1802.03426.</italic> arXiv: 1802.03426.</mixed-citation>
    </ref>
    <ref id="btaa673-B2">
      <mixed-citation publication-type="other">Matthew,R. (2015) Dask: Parallel computation with blocked algorithms and task scheduling. In Kathryn,H. and James,B. (eds) <italic>Proceedings of the 14th Python in Science Conference</italic>, pp. 130–36. </mixed-citation>
    </ref>
    <ref id="btaa673-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Drawert</surname><given-names>B.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Stochastic simulation service: bridging the gap between the computational expert and the biologist</article-title>. <source>PLoS Comput. Biol</source>., <volume>12</volume>, <fpage>e1005220</fpage>.<pub-id pub-id-type="pmid">27930676</pub-id></mixed-citation>
    </ref>
    <ref id="btaa673-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gutmann</surname><given-names>M.U.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Likelihood-free inference via classification</article-title>. <source>Stat. Comput</source>., <volume>28</volume>, <fpage>411</fpage>–<lpage>425</lpage>.<pub-id pub-id-type="pmid">31997856</pub-id></mixed-citation>
    </ref>
    <ref id="btaa673-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jiang</surname><given-names>B.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Learning summary statistic for approximate Bayesian computation via deep neural network</article-title>. <source>Stat. Sin</source>., <volume>27</volume>, <fpage>1595</fpage>–<lpage>1618</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa673-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klinger</surname><given-names>E.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>pyABC: distributed, likelihood-free inference</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>3591</fpage>–<lpage>3593</lpage>.<pub-id pub-id-type="pmid">29762723</pub-id></mixed-citation>
    </ref>
    <ref id="btaa673-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marin</surname><given-names>J.-M.</given-names></string-name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Approximate Bayesian computational methods</article-title>. <source>Stat. Comput</source>., <volume>22</volume>, <fpage>1167</fpage>–<lpage>1180</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa673-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raynal</surname><given-names>L.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>ABC random forests for Bayesian parameter inference</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>1720</fpage>–<lpage>1728</lpage>.<pub-id pub-id-type="pmid">30321307</pub-id></mixed-citation>
    </ref>
    <ref id="btaa673-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vilar</surname><given-names>J.M.G.</given-names></string-name></person-group><etal>et al</etal> (<year>2002</year>) 
<article-title>Mechanisms of noise-resistance in genetic oscillators</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>99</volume>, <fpage>5988</fpage>–<lpage>5992</lpage>.<pub-id pub-id-type="pmid">11972055</pub-id></mixed-citation>
    </ref>
    <ref id="btaa673-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wiqvist</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) Partially exchangeable networks and architectures for learning summary statistics in approximate Bayesian computation. In: Kamalika,C. and Ruslan,S. (eds) <italic>International Conference on Machine Learning</italic>, vol. <volume>97</volume>, pp. <fpage>6798</fpage>–<lpage>6807</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa673-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wrede</surname><given-names>F.</given-names></string-name>, <string-name><surname>Hellander</surname><given-names>A.</given-names></string-name></person-group> (<year>2019</year>) 
<article-title>Smart computational exploration of stochastic gene regulatory network models using human-in-the-loop semi-supervised learning</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>5199</fpage>–<lpage>5206</lpage>.<pub-id pub-id-type="pmid">31141124</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
