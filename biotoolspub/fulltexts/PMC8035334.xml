<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName nihms2pmcx2.xsl?>
<?ConverterInfo.Version 1?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Nat Methods?>
<?submitter-system nihms?>
<?submitter-canonical-name Nature Publishing Group?>
<?submitter-canonical-id NATURE-STRUCTUR?>
<?submitter-userid 8068858?>
<?submitter-authority myNCBI?>
<?submitter-login nature-structure?>
<?submitter-name Nature Publishing Group?>
<?domain nihpa?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">101215604</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">32338</journal-id>
    <journal-id journal-id-type="nlm-ta">Nat Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Methods</journal-id>
    <journal-title-group>
      <journal-title>Nature methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1548-7091</issn>
    <issn pub-type="epub">1548-7105</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8035334</article-id>
    <article-id pub-id-type="pmid">33686300</article-id>
    <article-id pub-id-type="doi">10.1038/s41592-021-01080-z</article-id>
    <article-id pub-id-type="manuscript">nihpa1668081</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deep Learning-Based Point-Scanning Super-Resolution Imaging</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Fang</surname>
          <given-names>Linjing</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Monroe</surname>
          <given-names>Fred</given-names>
        </name>
        <xref ref-type="aff" rid="A2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Novak</surname>
          <given-names>Sammy Weiser</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kirk</surname>
          <given-names>Lyndsey</given-names>
        </name>
        <xref ref-type="aff" rid="A3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schiavon</surname>
          <given-names>Cara R.</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yu</surname>
          <given-names>Seungyoon B.</given-names>
        </name>
        <xref ref-type="aff" rid="A4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Tong</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wu</surname>
          <given-names>Melissa</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kastner</surname>
          <given-names>Kyle</given-names>
        </name>
        <xref ref-type="aff" rid="A5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Latif</surname>
          <given-names>Alaa Abdel</given-names>
        </name>
        <xref ref-type="aff" rid="A8">8</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lin</surname>
          <given-names>Zijun</given-names>
        </name>
        <xref ref-type="aff" rid="A8">8</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shaw</surname>
          <given-names>Andrew</given-names>
        </name>
        <xref ref-type="aff" rid="A8">8</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kubota</surname>
          <given-names>Yoshiyuki</given-names>
        </name>
        <xref ref-type="aff" rid="A6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mendenhall</surname>
          <given-names>John</given-names>
        </name>
        <xref ref-type="aff" rid="A3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Zhao</given-names>
        </name>
        <xref ref-type="aff" rid="A7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pekkurnaz</surname>
          <given-names>Gulcin</given-names>
        </name>
        <xref ref-type="aff" rid="A4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Harris</surname>
          <given-names>Kristen</given-names>
        </name>
        <xref ref-type="aff" rid="A3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Howard</surname>
          <given-names>Jeremy</given-names>
        </name>
        <xref ref-type="aff" rid="A8">8</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Manor</surname>
          <given-names>Uri</given-names>
        </name>
        <xref ref-type="aff" rid="A1">1</xref>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="A1"><label>1</label>Waitt Advanced Biophotonics Center, Salk Institute for Biological Studies, La Jolla, CA, USA</aff>
    <aff id="A2"><label>2</label>Wicklow AI Medical Research Initiative, San Francisco, CA, USA</aff>
    <aff id="A3"><label>3</label>Department of Neuroscience, Center for Learning and Memory, Institute for Neuroscience, University of Texas at Austin, Austin, TX, USA</aff>
    <aff id="A4"><label>4</label>Neurobiology Section, Division of Biological Sciences, University of California San Diego, La Jolla, CA, USA</aff>
    <aff id="A5"><label>5</label>Montreal Institute for Learning Algorithms, Université de Montréal, Canada</aff>
    <aff id="A6"><label>6</label>Division of Cerebral Circuitry, National Institute for Physiological Sciences, Okazaki, 444-8787 Japan</aff>
    <aff id="A7"><label>7</label>Texas Advanced Computing Center, University of Texas at Austin, Austin, TX, USA</aff>
    <aff id="A8"><label>8</label>Fast.AI, University of San Francisco Data Institute, San Francisco, CA, USA</aff>
    <author-notes>
      <fn fn-type="con" id="FN1">
        <p id="P1">Author Contributions</p>
        <p id="P2">L.F. designed the research, performed or participated in software development, all experiments and analyses, and wrote the paper; F.M., J.H. designed the research and, together with K.K., A.A.L., Z.L., A.S. contributed to software development; J.M., K.M.H., S.W.N., Y.K. collected EM data; C.R.S., T.Z., M.W. collected mitotracker data; S.Y., G.P. collected neuronal mitochondria data; S.W.N. and L.K. performed vesicle segmentation analysis; C.R.S., S.Y., G.P. performed neuronal mitochrondrial mobiltiy analysis; S.W.N., S.Y., G.P., A.A.L., Z.L., A.S. contributed to data visualization; U.M., K.M.H., Z.Z. contributed with resources; U.M, K.M.H. secured funding; U.M. conceived and designed the research, participated in software development, all experiments and analyses, oversaw the project and wrote the paper.</p>
      </fn>
      <corresp id="CR1"><label>*</label>Corresponding author: <email>umanor@salk.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>25</day>
      <month>2</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>08</day>
      <month>3</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>4</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>08</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <volume>18</volume>
    <issue>4</issue>
    <fpage>406</fpage>
    <lpage>416</lpage>
    <!--elocation-id from pubmed: 10.1038/s41592-021-01080-z-->
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/">http://www.nature.com/authors/editorial_policies/license.html#terms</ali:license_ref>
        <license-p>Users may view, print, copy, and download text and data-mine the content in such documents, for the purposes of academic research, subject always to the full Conditions of use:<uri xlink:href="http://www.nature.com/authors/editorial_policies/license.html#terms">http://www.nature.com/authors/editorial_policies/license.html#terms</uri></license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P3">Point-scanning imaging systems are among the most widely used tools for high-resolution cellular and tissue imaging, benefitting from arbitrarily defined pixel sizes. The resolution, speed, sample preservation, and signal-to-noise ratio (SNR) of point-scanning systems are difficult to optimize simultaneously. We show these limitations can be mitigated via the use of Deep Learning-based supersampling of undersampled images acquired on a point-scanning system, which we term point-scanning super-resolution (PSSR) imaging. We designed a “crappifier” that computationally degrades high SNR, high pixel resolution ground truth images to simulate low SNR, low-resolution counterparts for training PSSR models that can restore real-world undersampled images. For high spatiotemporal resolution fluorescence timelapse data, we developed a “multi-frame” PSSR approach that utilizes information in adjacent frames to improve model predictions. In conclusion, PSSR facilitates point-scanning image acquisition with otherwise unattainable resolution, speed, and sensitivity. All the training data, models, and code for PSSR are publicly available at <ext-link ext-link-type="uri" xlink:href="https://3DEM.org">3DEM.org</ext-link>.</p>
      <sec id="S1">
        <title>Editor’s summary</title>
        <p id="P4">Point-scanning super-resolution imaging uses deep learning to supersample undersampled images and enable time-lapse imaging of subcellular events. An accompanying “crappifier” rapidly generates quality training data for robust performance.</p>
      </sec>
    </abstract>
  </article-meta>
</front>
<body>
  <sec id="S2">
    <title>Introduction</title>
    <p id="P5">An essential tool for understanding the spatiotemporal organization of biological systems, microscopy is nearly synonymous with biology itself. Microscopes suffer from the so-called “eternal triangle of compromise”, which dictates that image resolution, illumination intensity (and consequent sample damage), and imaging speed are all in tension with one another. In other words, it is impossible to optimize one parameter without compromising another. This is particularly problematic for point-scanning systems, e.g. scanning electron (SEM) and laser scanning confocal (LSM) microscopes, for which higher resolution images require higher numbers of sequentially acquired pixels to ensure proper sampling, thus increasing the imaging time and sample damage in direct proportion to the image resolution. Nonetheless, point-scanning systems remain perhaps the most common imaging modality in biological research due to their versatility and ease of use. Thus, the ability to effectively supersample undersampled point-scanning microscope images could be transformative.</p>
    <p id="P6">Deep learning (DL) has been extensively used to “supersample” the pixels in computationally downsampled digital photographs<sup><xref rid="R1" ref-type="bibr">1</xref>–<xref rid="R4" ref-type="bibr">4</xref></sup>. For microscopy, DL has long been established as an invaluable method for image analysis and segmentation<sup><xref rid="R5" ref-type="bibr">5</xref></sup>. More recently, DL has been employed with spectacular results in restoring relatively low SNR or resolution acquisitions<sup><xref rid="R6" ref-type="bibr">6</xref>–<xref rid="R11" ref-type="bibr">11</xref></sup>. DL has also been used for accelerating single-molecule localization microscopy<sup><xref rid="R11" ref-type="bibr">11</xref>–<xref rid="R13" ref-type="bibr">13</xref></sup>. Similarly, low resolution or low SNR EM data has been restored using DL<sup><xref rid="R9" ref-type="bibr">9</xref>,<xref rid="R14" ref-type="bibr">14</xref>–<xref rid="R16" ref-type="bibr">16</xref></sup>. Finally, DL has been used to restore blurry, low optical resolution images to sharp, high “optical” resolution images via either supervised training of fluorescence samples<sup><xref rid="R11" ref-type="bibr">11</xref>,<xref rid="R17" ref-type="bibr">17</xref>,<xref rid="R18" ref-type="bibr">18</xref></sup>, or via more generalizable deconvolution algorithms<sup><xref rid="R19" ref-type="bibr">19</xref>,<xref rid="R20" ref-type="bibr">20</xref></sup>.</p>
    <p id="P7">Increasing the xy pixel resolution of real-world undersampled point-scanning microscope images presents a unique set of both challenges and opportunities: Undersampling point-scanning microscope images in the xy-plane while maintaining a constant pixel dwell time increases the imaging speed and decreases the sample damage, but also results in not just a lower pixel resolution image, but also lower signal-to-noise (SNR), since the total photons or electrons detected are in this case proportional to the number of pixels collected. Thus, restoring undersampled point-scanning microscope images with DL requires simultaneous supersampling and denoising, a challenging task that requires large amounts of high quality training data.</p>
    <p id="P8">To accomplish this, we developed a DL-based training workflow that uses a novel “crappifier” that simultaneously injects noise while downsampling the pixel resolution of high-resolution training data. This crappifier circumvents the need to acquire real-world image pairs for training, which is difficult and expensive for large datasets, and is practically impossible for live samples with quickly moving structures (e.g. subcellular organelles). These crappified images are then paired with their high-resolution counterparts to train models that can successfully supersample and denoise real-world undersampled, noisy images. Remarkably, we found crappifiers employing additive gaussian noise performed best for training models that effectively restore real-world images. We also found that DL restored fluorescence timelapse images of fast-moving subcellular organelles suffer from flickering artifacts when restoring very low resolution, low SNR images. To address this, we developed a “multi-frame” approach for high spatiotemporal resolution timelapse data, which reduces flickering artifacts and generates more accurate super-resolution images by using information from neighboring video frames in timelapse acquisitions. Thus, our <underline>P</underline>oint-<underline>S</underline>canning <underline>S</underline>uper-<underline>R</underline>esolution (PSSR) software provides a practical and powerful framework for simultaneously increasing the sensitivity, pixel resolution, “optical” resolution, and acquisition speed of any point-scanning imaging system.</p>
  </sec>
  <sec id="S3">
    <title>Results</title>
    <p id="P9">Three-dimensional electron microscopy (3DEM) is a powerful technique for determining the volumetric ultrastructure of tissues. In addition to serial section EM (ssEM)<sup><xref rid="R21" ref-type="bibr">21</xref></sup> and focused ion beam SEM (FIB-SEM)<sup><xref rid="R22" ref-type="bibr">22</xref></sup>, one of the most common tools for high throughput 3DEM imaging is serial blockface scanning electron microscopy (SBFSEM)<sup><xref rid="R23" ref-type="bibr">23</xref></sup>, wherein a built-in ultramicrotome iteratively cuts ultrathin sections off the surface of a blockface after it was imaged with a scanning electron probe. This technology facilitates relatively automated, high-throughput 3DEM imaging with minimal post-acquisition image alignment. Unfortunately, higher electron doses cause sample charging, which renders the sample too soft to section reliably (<xref rid="SD12" ref-type="supplementary-material">Supplementary Video 1</xref>). Furthermore, the extremely long imaging times and large file sizes inherent to high-resolution volumetric imaging present a significant bottleneck. For these reasons, most 3DEM datasets are acquired with sub-Nyquist pixel sampling (e.g. pixel sizes ≥ 4nm), which precludes the reliable detection or analysis of smaller subcellular structures, such as ~35nm presynaptic vesicles. While low pixel resolution 3DEM datasets can be suitable for many analyses (e.g. cellular segmentation), the ability to mine targeted regions of pre-existing large datasets for higher resolution ultrastructural information would be extremely valuable. Unfortunately, many 3DEM imaging approaches are destructive, and high-resolution ssEM can be slow and laborious. Thus, the ability to increase the pixel resolution of these 3DEM datasets <italic>post hoc</italic> is highly desirable.</p>
    <p id="P10">Frustrated by our inability to perform SBFSEM imaging with the desired 2nm pixel resolution and SNR necessary to reliably detect presynaptic vesicles, we decided to test whether a deep convolutional neural net model (PSSR) trained on 2nm pixel high-resolution (HR) images could “super-resolve” 8nm pixel low-resolution (LR) images. We also wished to enable imaging with equal pixel dwell times as the HR images, facilitating a 16x increase in imaging speed, but at the cost of a 16x decrease in detected electrons, significantly lowering the SNR. Thus, our PSSR model would need to simultaneously increase the SNR and pixel resolution of the LR images. To train a DL model for this purpose, a very large number of perfectly aligned high- and low-resolution image pairs would be required. However, EM imaging of ultrathin sections results in non-linear distortions that are impossible to predict or control. Therefore, it is extremely difficult and sometimes impossible to perfectly align two sequential acquisitions of the same field of view. Furthermore, sequentially acquiring a large enough number of images for training data is time-intensive and cost-prohibitive. Thus, instead of manually acquiring high- and low-resolution image pairs for training, we opted to generate training data by computationally “crappifying” high-resolution images to be paired with their high-resolution counterparts.</p>
    <p id="P11">We hypothesized that to simulate real-world, low pixel resolution acquisitions with lower electron doses, our “crappifier” must add noise while also decreasing the pixel resolution. To create a suitable crappifier, we performed an ablation study, wherein we compared the performance of models trained with image pairs generated with different crappifiers, while all other factors were kept constant (<xref rid="F1" ref-type="fig">Fig. 1a</xref>–<xref rid="F1" ref-type="fig">e</xref> – see <xref rid="S5" ref-type="sec">Methods</xref> for full details). Specifically, we compared models crappified with no noise (i.e., downsampling only), Poisson noise, Gaussian noise (independently and identically distributed), and Additive-Gaussian distributed noise, respectively. Among all models trained with crappified pairs, “Additive-Gaussian” yielded the best results. The Additive-Gaussian model also outperformed the model trained with manually acquired training pairs (“Real-world”) across all metrics. We further compared “Additive-Gaussian” with “Additive-Gaussian (~80x)”, where we used approximately 80x more training data, which interestingly did not significantly increase the Peak-Signal-to-Noise Ratio (PSNR) or Structural Similarity (SSIM) measurements, but did further increase the resolution as measured by Fourier Ring Correlation (FRC) analysis. Notably, it would have taken &gt;480 hours of imaging time, and cost &gt;$16,000 to acquire the same amount of real-world training data pairs as was used in our 80x Additive-Gaussian model. But by using pre-existing gold standard high-resolution data that was already acquired for separate experimental purposes, we were able to generate training data in only 2 hours, at a cost of only $16 (<xref rid="F1" ref-type="fig">Fig. 1f</xref>). This highlights the utility of the crappifier method, which facilitates the generation of much larger amounts of training data at a fraction of the cost. Therefore, for our training data we “crappified” ~130GB training data of 2nm pixel transmission-mode scanning electron microscope (tSEM<sup><xref rid="R24" ref-type="bibr">24</xref></sup>) images of 40nm ultrathin sections of rat CA1 hippocampal tissue. We then trained our image pairs with a ResNet-based U-Net architecture (<xref rid="F2" ref-type="fig">Fig. 2a</xref>, <xref rid="F6" ref-type="fig">Extended Data Fig. 1</xref> – see <xref rid="S5" ref-type="sec">Methods</xref> and Tables for full details). Using a Mean Squared Error (MSE) loss function yielded excellent results as determined by visual inspection as well as PSNR, SSIM, and FRC analyses. Overall, the PSSR-restored images from our semi-synthetic pairs contained more high-frequency information than our LR images, and yet displayed less noise than both our LR and HR images, making it easier to identify smaller objects, such as 35nm presynaptic vesicles (<xref rid="F2" ref-type="fig">Fig. 2b</xref>).</p>
    <p id="P12">We next tested whether our PSSR model was effective on real-world LR acquisitions. DL-based models are notoriously sensitive to variations in training versus testing data, usually precluding the use of models generated from training images acquired in one condition on images acquired in another (i.e. data generated using a different sample preparation technique, type, or on a different microscope). Similarly, there is a risk that models trained on artificially injected noise will learn how to remove artificial noise yet fail to remove real-world noise.</p>
    <p id="P13">Our training images were generated from 40nm sections of rat CA1 tissue acquired with a tSEM detector. But for our testing data, we acquired HR and LR images of 80nm sections of dentate gyrus tissue from a mouse brain, imaged with a backscatter detector. Based on several metrics including PSNR, SSIM, FRC (<xref rid="F2" ref-type="fig">Fig. 2b</xref>–<xref rid="F2" ref-type="fig">c</xref>), NanoJ-SQUIRREL error mapping analysis (<xref rid="F7" ref-type="fig">Extended Data Fig. 2</xref>)<sup><xref rid="R25" ref-type="bibr">25</xref></sup>, visual inspection, and comparison to the Block-Matching and 3D filtering (BM3D) denoising method (<xref rid="F8" ref-type="fig">Extended Data Fig. 3</xref>)<sup><xref rid="R26" ref-type="bibr">26</xref></sup>, we found PSSR successfully restored real-world LR images (<xref rid="F2" ref-type="fig">Fig. 2c</xref>). Thus, our PSSR model is effective for real-world data, and is not restricted to data acquired in the exact same fashion as the training set.</p>
    <p id="P14">We next asked whether we could sufficiently restore 8nm SBFSEM datasets to 2nm using PSSR, since high quality 2nm SBFSEM imaging is currently difficult or impossible for us to achieve. Using the same PSSR model described above we were able to restore an 8nm pixel SBFSEM 3D dataset to 2nm (<xref rid="F3" ref-type="fig">Fig. 3a</xref>, <xref rid="SD13" ref-type="supplementary-material">Supplementary Video 2</xref>). Remarkably, our PSSR model also worked very well on mouse, rat, and fly samples imaged on four different microscopes in four different labs (<xref rid="F3" ref-type="fig">Fig. 3a</xref>–<xref rid="F3" ref-type="fig">d</xref>). In addition to our SBFSEM and SEM imaging systems, PSSR processing appeared to restore images acquired on a ZEISS FIB-SEM (from the Hess lab at Janelia Farms, <xref rid="F3" ref-type="fig">Fig. 3c</xref>, <xref rid="SD14" ref-type="supplementary-material">Supplementary Video 3</xref>) and a Hitachi Regulus FE-SEM (from the Kubota lab at National Institute for Physiological Sciences, <xref rid="F3" ref-type="fig">Fig. 3d</xref>). PSSR processing also performed well on a 10×10×10nm resolution FIB-SEM fly brain dataset, resulting in a 2×2×10nm resolution dataset with higher SNR and resolution (<xref rid="F3" ref-type="fig">Fig. 3b</xref>). Thus, PSSR can provide 25x super-resolution with useful results, effectively increasing the lateral resolution and speed of FIB-SEM imaging by a factor of at least 25x.</p>
    <p id="P15">The major concern with DL-based image processing is the possibility of false positives (aka “hallucinations”)<sup><xref rid="R5" ref-type="bibr">5</xref>,<xref rid="R9" ref-type="bibr">9</xref>,<xref rid="R27" ref-type="bibr">27</xref>,<xref rid="R28" ref-type="bibr">28</xref></sup>. To further test the accuracy and utility of the PSSR output within a more concrete, biological context, we next randomized LR-Bilinear, LR-PSSR, and HR images, then distributed them to two blinded human experts for manual segmentation of presynaptic vesicles, which are difficult to detect with 8nm pixel resolution images (see <xref rid="SD1" ref-type="supplementary-material">Supplementary Notes</xref> and <xref rid="S5" ref-type="sec">Methods</xref> for full details). We found the LR-PSSR segmentation was significantly more accurate than the LR-Bilinear (<xref rid="F3" ref-type="fig">Fig. 3e</xref>). Interestingly, while the LR-PSSR output significantly reduced false negatives, the LR-PSSR output had a slightly higher number of “false positives” than the LR-Bilinear. However, the variance between the LR-PSSR and HR results was similar to the variance between the two expert human results on HR data (<xref rid="F3" ref-type="fig">Fig. 3e</xref>), which is the current gold standard. Notably, we found PSSR images were much easier to segment - a major bottleneck for analyzing 3DEM datasets (<xref rid="SD15" ref-type="supplementary-material">Supplementary Video 4</xref>).</p>
    <p id="P16">Similar to SBFSEM, laser scanning confocal microscopy also suffers from a direct relationship between pixel resolution and sample damage (i.e. phototoxicity/photobleaching)<sup><xref rid="R29" ref-type="bibr">29</xref></sup>. This can be a major barrier for cell biologists who wish to study the dynamics of smaller structures such as mitochondria, which regularly undergo fission and fusion, but also show increased fission and swelling in response to phototoxicity (<xref rid="SD16" ref-type="supplementary-material">Supplementary Video 5</xref>, <xref rid="F9" ref-type="fig">Extended Data Fig. 4</xref>).</p>
    <p id="P17">Laser scanning microscopy also suffers from an inverse relationship between pixel resolution and imaging speed, making live cell imaging of faster processes (e.g. organelle motility in neurons) challenging if not impossible. Nonetheless, Nyquist sampling criteria often necessitates the use of smaller pixels to resolve smaller structures – this is particularly true for higher resolution imaging methods that depend on post-processing pixel reassignment and/or deconvolution. Thus, we sought to determine whether PSSR might enable acquisitions with decreased pixel resolution in order to optimize the imaging speed and SNR of live laser scanning confocal microscope imaging. Importantly, to train a DL model using image pairs of mitochondria in live cells is virtually impossible because they are constantly moving and changing their shape. Thus, our “crappification” approach is particularly useful generating training data for live cell imaging datasets.</p>
    <p id="P18">To generate our ground truth training dataset we used a ZEISS LSM 880 in Airyscan mode (see <xref rid="S5" ref-type="sec">Online Methods</xref> for more details). Similar to our EM model, an ablation study that compared crappifiers with different noise distributions was conducted (<xref rid="F10" ref-type="fig">Extended Data Fig. 5</xref> – See <xref rid="S5" ref-type="sec">Methods</xref> for full details). We found the crappifier injected with Salt &amp; Pepper and Additive-Gaussian noise yielded overall best performance. For the real-world LR test data, we acquired images in confocal mode at 16x lower pixel resolution with a 2.5 AU pinhole on a PMT confocal detector, without any additional image processing. To ensure minimal phototoxicity, we also decreased the laser power for our LR acquisitions by a factor of 4 or 5 (see <xref rid="T1" ref-type="table">Tables 1</xref>–<xref rid="T2" ref-type="table">2</xref> for more details), resulting in a net laser dose decrease of ~64–80x. Thus, our PSSR model was trained to restore low-resolution, low SNR, low pixel resolution confocal images to high SNR, high pixel resolution, high “optical” (i.e. deblurred) resolution Airyscan-equivalent image quality. To start, we trained on live cell timelapse acquisitions of mitochondria in U2OS cells. As expected, imaging at full resolution resulted in significant bleaching and phototoxicity-induced mitochondrial swelling and fission (<xref rid="SD16" ref-type="supplementary-material">Supplementary Video 5</xref>). However, the LR acquisitions were extremely noisy and pixelated due to undersampling. However, the LR scans showed far less photobleaching (<xref rid="F9" ref-type="fig">Extended Data Fig. 4</xref>). Similar to our EM data, LR-PSSR images had higher SNR and resolution compared to LR acquisitions, as determined by testing on both semi-synthetic and real-world low- versus high-resolution image pairs. Notably, LR-PSSR images also had higher SNR than HR images (<xref rid="F4" ref-type="fig">Fig. 4d</xref>).</p>
    <p id="P19">We observed significant “flickering” in LR-PSSR timelapses (<xref rid="SD17" ref-type="supplementary-material">Supplementary Video 6</xref>, <xref rid="F4" ref-type="fig">Figure 4b</xref>) due to noise-induced variations in signal detection and image reconstruction, causing both false breaks and merges in mitochondrial networks (<xref rid="F4" ref-type="fig">Fig. 4c</xref>, white boxes, <xref rid="F4" ref-type="fig">Fig. 4d</xref>, red and yellow arrows), making it impossible to accurately detect bona fide mitochondrial fission or fusion events. This temporal inconsistency was reflected in neighboring-frame cross-correlation analysis (<xref rid="F4" ref-type="fig">Fig. 4b</xref>, see <xref rid="S5" ref-type="sec">Methods</xref> for full details). One strategy for increasing the SNR of images is to average multiple scans, e.g. “frame averaging” – this method can also be used to reduce “flickering” effects in videos (<xref rid="F4" ref-type="fig">Fig. 4b</xref>). However, this approach is problematic for live imaging of quickly moving objects: If objects move greater than half the distance of the desired spatial resolution between individual frames, <underline><italic toggle="yes">temporal</italic></underline> Nyquist criteria are no longer satisfied, resulting in blurring artifacts and loss of both spatial <underline><italic toggle="yes">and</italic></underline> temporal resolution<sup><xref rid="R30" ref-type="bibr">30</xref>,<xref rid="R31" ref-type="bibr">31</xref></sup>. This loss of information is compounded if spatial Nyquist criteria are also unmet, i.e. when subsampling pixels, as is the case in LR acquisitions. However, although simple frame averaging approaches may lose resolution in exchange for higher SNR, more sophisticated computational approaches can take advantage of multi-frame acquisitions to <underline><italic toggle="yes">increase</italic></underline> the resolution of individual frame reconstructions<sup><xref rid="R32" ref-type="bibr">32</xref>–<xref rid="R35" ref-type="bibr">35</xref></sup>.</p>
    <p id="P20">We hypothesized that the PSSR network could learn the additional information contained in sequential video frames, even when grossly undersampled in both space and time, and could thus be used to reduce flickering artifacts, while also improving the restoration accuracy and resolution of PSSR-processed timelapse videos. To test this hypothesis, we exploited the multi-dimensional capabilities of our PSSR Res-UNet architecture by training on 5 sequential timepoint inputs for each single timepoint output (multi-frame PSSR, or “PSSR-MF”, <xref rid="F4" ref-type="fig">Fig. 4a</xref>). As measured by PSNR, SSIM, FRC, and NanoJ-SQUIRREL error mapping, as well as compared with the BM3D denoising algorithm, PSSR-MF processing of LR acquisitions (LR-PSSR-MF) showed significantly increased resolution and SNR compared to the raw input (LR), 16x bilinear interpolated input (LR-Bilinear), and single-frame PSSR (LR-PSSR-SF) (<xref rid="F4" ref-type="fig">Fig. 4d</xref>, <xref rid="F4" ref-type="fig">4f</xref>, <xref rid="F12" ref-type="fig">Extended Data Fig. 7</xref>–<xref rid="F13" ref-type="fig">8</xref>). Since it is conceivable that simply averaging 5 frames would yield similar improvements due to reduced flickering and increased SNR, we tested whether using a 5-frame rolling average over the LR-PSSR-SF output (LR-PSSR-SF-RA) could yield similar results to LR-PSSR-MF. Although LR-PSSR-SF-RA output displayed reduced flickering (<xref rid="F4" ref-type="fig">Fig. 4b</xref>–<xref rid="F4" ref-type="fig">c</xref>), we found that LR-PSSR-SF-RA performed significantly worse than LR-PSSR-MF in terms of both resolution and accuracy (<xref rid="F4" ref-type="fig">Fig. 4c</xref>, <xref rid="F11" ref-type="fig">Extended Data Fig. 6</xref>, <xref rid="SD17" ref-type="supplementary-material">Supplementary Video 6</xref>). We also compared PSSR models with Content-Aware Image Restoration (CARE), a gold-standard DL-based image restoration algorithm (<xref rid="F4" ref-type="fig">Fig. 4b</xref>–<xref rid="F4" ref-type="fig">c</xref>, <xref rid="F11" ref-type="fig">Extended Data Fig. 6</xref> – see <xref rid="S5" ref-type="sec">Methods</xref> for full details). Specifically, CARE trained results (LR-CARE) and its Rolling Average post-processed version (LR-CARE-RA) were compared with multi-frame PSSR (LR-PSSR-MF), singe-frame PSSR (LR-PSSR-SF) and its Rolling Average post-processed results (LR-PSSR-SF-RA). As expected, the LR-CARE and LR-PSSR-SF results were similar. However, we found the multi-frame PSSR approach yielded better results than CARE, both before and after the Rolling Average processing (<xref rid="F11" ref-type="fig">Extended Data Fig. 6</xref>). We therefore concluded multi-frame PSSR significantly enhances the fidelity and resolution of LR images beyond any standard frame-by-frame image restoration approach.</p>
    <p id="P21">For all time-lapse PSSR we used PSSR-MF and refer to it as PSSR for the remainder of this article. The improved speed, resolution, and SNR enabled us to detect mitochondrial fission events that were not detectable in the LR or LR-Bilinear images (yellow arrows, <xref rid="F4" ref-type="fig">Fig. 4e</xref>, <xref rid="SD18" ref-type="supplementary-material">Supplementary Video 7</xref>). Additionally, the relatively high laser dose during HR acquisitions raises questions as to whether observed fission events are artifacts of phototoxicity. We validated the accuracy of our fission event detection with semi-synthetic data quantified by two expert humans (<xref rid="F4" ref-type="fig">Fig. 4g</xref>–<xref rid="F4" ref-type="fig">h</xref>). We found a significant improvement in detecting fission events with relatively minor increases in false positives (<xref rid="F4" ref-type="fig">Fig. 4h</xref>–<xref rid="F4" ref-type="fig">k</xref>). We again found the variance between the PSSR and HR results was similar to the variance between the two expert human results on HR data. Thus, our PSSR model provides an opportunity to detect very fast mitochondrial fission events with fewer phototoxicity-induced artifacts than standard high-resolution Airyscan imaging using normal confocal optics and detectors.</p>
    <p id="P22">As mentioned above, in addition to phototoxicity issues, the slow speed of HR scanning confocal imaging often results in <underline><italic toggle="yes">temporal</italic></underline> undersampling of fast-moving structures such as motile mitochondria in neurons (<xref rid="SD1" ref-type="supplementary-material">Supplementary Fig. 1</xref>, <xref rid="SD19" ref-type="supplementary-material">Supplementary Videos 8</xref>–<xref rid="SD20" ref-type="supplementary-material">9</xref>). However, relatively fast LR scans do not provide sufficient pixel resolution or SNR to resolve fission or fusion events, or individual mitochondria when they pass one another along a neuronal process, which can result in faulty analysis or data interpretation (<xref rid="SD19" ref-type="supplementary-material">Supplementary Video 8</xref>). Thus, we next tested whether PSSR provided sufficient restoration of undersampled time-lapse imaging of mitochondrial trafficking in neurons.</p>
    <p id="P23">As measured by PSNR, SSIM, FRC, and NanoJ-SQUIRREL error mapping, the overall resolution and SNR improvement provided by PSSR enabled us to resolve adjacent mitochondria as well as fission and fusion events (<xref rid="F5" ref-type="fig">Fig. 5a</xref>–<xref rid="F5" ref-type="fig">c</xref>, <xref rid="F14" ref-type="fig">Extended Data Fig. 9</xref>–<xref rid="F15" ref-type="fig">10</xref>, <xref rid="SD20" ref-type="supplementary-material">Supplementary Video 9</xref>). Since our LR acquisition rates are 16x faster than HR, instantaneous motility details were preserved in LR-PSSR whereas in HR images they were lost (<xref rid="F5" ref-type="fig">Fig. 5d</xref>, <xref rid="SD1" ref-type="supplementary-material">Supplementary Fig. 1</xref>, <xref rid="SD20" ref-type="supplementary-material">Supplementary Video 9</xref>). The overall total distance mitochondria travelled in neuronal processes was the same for both LR and HR (<xref rid="F5" ref-type="fig">Fig. 5f</xref>). However, we were able to obtain unique information about how they translocate when imaging at a 16x higher frame rate (<xref rid="F5" ref-type="fig">Fig. 5g</xref>). Interestingly, a larger range of velocities was identified in LR-PSSR than both LR and HR images. Overall, LR-PSSR and HR provided similar values for the percent time mitochondria spent in motion (<xref rid="F5" ref-type="fig">Fig. 5h</xref>). Smaller distances travelled were easier to detect in our LR-PSSR images, and therefore there was an overall reduction in the percent time mitochondria spent in the paused position in our LR-PSSR data (<xref rid="F5" ref-type="fig">Fig. 5i</xref>). Taken together, these data show PSSR provides a means to detect critical biological events that would not be possible with conventional HR or LR imaging.</p>
  </sec>
  <sec id="S4">
    <title>Discussion</title>
    <p id="P24">We have demonstrated DL-based pixel super-resolution can be a viable strategy of particular value for both optical and electron point-scanning microscopes. Acquiring suitably aligned pairs of high- and low-quality images for training is incredibly expensive and difficult (<xref rid="F1" ref-type="fig">Fig. 1f</xref>). Thus, we introduced a novel “crappifier” for generating noisy, low-resolution training data from high-resolution, high SNR ground truth data. This enables the use of large, pre-existing gold standard datasets for training new models without acquiring any new data. We hope the open-source availability of our crappifier will be reciprocated by open-source sharing of high-quality imaging data, which can then be used to train new DL models. We did not fully explore all possibilities for a crappifier and believe this is an open and fruitful area for future studies.</p>
    <p id="P25">We discovered that DL-based restoration of noisy timelapses suffers from temporal inconsistency (“flickering”) artifacts due to noise-induced randomness in pixel values between frames. To address this, we introduced a multi-frame super-resolution approach that leverages the information in previous and future timepoints to better infer the frame of interest. We found this multi-frame approach not only reduces flickering artifacts, but also provides better overall image restoration for each independent frame. Notably, this approach would be impossible without the crappifier, which provides the ability to generate training data from videos of rapidly moving structures in live cells for which it is impossible to acquire perfectly aligned image pairs.</p>
    <p id="P26">Any output from a DL super-resolution model is a prediction, is never 100% accurate, and is always highly dependent on sufficient correspondence between the training versus experimental data<sup><xref rid="R5" ref-type="bibr">5</xref>,<xref rid="R9" ref-type="bibr">9</xref>,<xref rid="R28" ref-type="bibr">28</xref>,<xref rid="R36" ref-type="bibr">36</xref></sup>. Whether the level of accuracy of a given model for a given dataset is satisfactory is ultimately dependent on the tolerance for error in the measurement being made. For example, our EM PSSR model was validated by segmenting vesicles in presynaptic boutons. But we did not rule out the possibility that other structures or regions in the same sample may not be restored by our model with the necessary accuracy for any arbitrary measurement. Thus, it is essential to validate the accuracy of the model for the specific task at hand before investing further. Similarly, we observed that no single performance metric reliably captures the “best” model. Thus, model performance must be evaluated by a combination of metrics, segmentation, and, of course, visual inspection by human experts. An important future direction may be to develop better metrics for evaluating models.</p>
    <p id="P27">Though the accuracy of DL approaches such as PSSR is technically imperfect, real-world limitations on acquiring ground truth data may render PSSR the best option. Our results show the PSSR approach can in principle enable higher speed and resolution imaging with the fidelity necessary for biological research. The ability to use DL to supersample undersampled images provides an opportunity that extends to any point-scanning system, including ion-based imaging systems<sup><xref rid="R37" ref-type="bibr">37</xref>,<xref rid="R38" ref-type="bibr">38</xref></sup> or high-resolution cryoSTEM<sup><xref rid="R39" ref-type="bibr">39</xref></sup>.</p>
    <p id="P28">For future uses of PSSR, we propose an acquisition scheme wherein a relatively limited number of “ground truth” HR images are acquired for fine-tuning pre-trained models. More importantly, the performance of generalized, unsupervised or “self-supervised” denoising approaches<sup><xref rid="R7" ref-type="bibr">7</xref>,<xref rid="R10" ref-type="bibr">10</xref></sup> as well as DL-enabled deconvolution approaches<sup><xref rid="R19" ref-type="bibr">19</xref>,<xref rid="R20" ref-type="bibr">20</xref></sup> suggests we may one day be able to generate a more generalized model for a specific imaging system, instead of a specific sample type.</p>
    <p id="P29">Structured illumination microscopy, single-molecule localization microscopy, and pixel reassignment microscopy demonstrate the power of configuring optical imaging schemes with a specific post-processing computational strategy in mind. The power of deep convolutional neural networks for post-processing image data presents a new opportunity for redesigning imaging systems to exploit these capabilities to minimize costs traditionally considered necessary for extracting meaningful imaging data. Similarly, automated real-time corrections to the images and real-time feedback to the imaging hardware are now within reach. This is an exciting area of active investigation in our laboratory and others (Lu Mi, Yaron Meirovitch, Jeff Lichtman, Aravinthan Samuel, Nir Shavit, personal communication).</p>
  </sec>
  <sec id="S5">
    <title>Online Methods</title>
    <sec id="S6">
      <title>Semi-synthetic Training Image Generation</title>
      <p id="P30">HR images were acquired using scanning electron or Airyscan confocal microscopes. Due to the variance of image properties (e.g. format, size, dynamic range and depth) in the acquired HR images, data cleaning is indispensable for generating training sets that can be easily accessed during training. In this article, we differentiate the concept of “data sources” and “data sets”, where data sources refer to uncleaned acquired high-resolution images, while data sets refer to images that are generated and preprocessed from data sources. HR data sets were obtained after preprocessing HR images from data sources, LR data sets were generated from HR data sets using a “crappifier” function.</p>
      <sec id="S7">
        <title>Preprocessing.</title>
        <p id="P31">Tiles of predefined sizes (e.g. 256 × 256 and 512 × 512 pixels) were randomly cropped from each frame in image stacks from HR data sources. “Refection padding” was used if the image size in the data sources is smaller than the predefined tile size. All tiles were saved as separate images in <italic>.tif</italic> format, which together formed a HR data set.</p>
      </sec>
      <sec id="S8">
        <title>Image Crappification.</title>
        <p id="P32">A “crappifier” was then used to synthetically degrade the HR data sets to LR images, with the goal of approximating the undesired and unavoidable pixel intensity variation in real-world low-resolution and low SNR images of the same field of view directly taken under an imaging system. These HR images together with their corrupted counterparts served as training pairs to facilitate “deCrappification”. The crappification function can be simple, but it materially improves both the quality and characteristics of PSSR outputs.</p>
        <p id="P33">Image sets were normalized from 0 to 1 before being 16x downsampled in pixel resolution (e.g. a 1000 × 1000 pixel image would be downsampled to 250 × 250 pixels). To mimic the image quality degradation caused by 16x undersampling on a real-world point-scanning imaging system, salt-and-pepper noise, and Gaussian additive noise with specified local variance were randomly injected into the high-resolution images. The degraded images were then rescaled to 8-bit for viewing with normal image analysis software.</p>
        <sec id="S9">
          <title>EM Crappifier</title>
          <p id="P34">Random Gaussian-distributed additive noise (<inline-formula><mml:math display="inline" id="M1"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math></inline-formula>) was injected. The degraded images were then downsampled using spline interpolation of order 1.</p>
        </sec>
        <sec id="S10">
          <title>MitoTracker and Neuronal Mitochondria Crappifier</title>
          <p id="P35">The crappification of MitoTracker and neuronal mitochondria data followed a similar procedure. Salt-and-pepper noise was randomly injected in 0.5% of each image’s pixels replacing them with noise, which was followed by the injection of random Gaussian-distributed additive noise (<inline-formula><mml:math display="inline" id="M2"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math></inline-formula>). The crappified images were then downsampled using spline interpolation of order 1.</p>
        </sec>
      </sec>
      <sec id="S11">
        <title>Data Augmentation.</title>
        <p id="P36">After crappified low-resolution images were generated, we used data augmentation techniques such as random cropping, dihedral affine function, rotation, random zoom to increase the variety and size of our training data<sup><xref rid="R40" ref-type="bibr">40</xref></sup>.</p>
      </sec>
      <sec id="S12">
        <title>Multi-frame Training Pairs.</title>
        <p id="P37">Unlike imaging data of fixed samples, where we use traditional one-to-one high- and low-resolution images as training pairs, for time-lapse movies, five consecutive frames (<inline-formula><mml:math display="inline" id="M3"><mml:msub><mml:mrow><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>) from a HR Airyscan time-lapse movie were synthetically crappified to five LR images (<inline-formula><mml:math display="inline" id="M4"><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>), which together with the HR middle frame at time <inline-formula><mml:math display="inline" id="M5"><mml:mi>t</mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>, form a five-to-one training “pair”. The design of five-to-one training “pairs” leverages the spatiotemporal continuity of dynamic biological behaviors. (<xref rid="F4" ref-type="fig">Fig. 4a</xref>).</p>
      </sec>
    </sec>
    <sec id="S13">
      <title>Crappifier comparison</title>
      <sec id="S14">
        <title>EM crappifier comparison.</title>
        <p id="P38">Four crappifiers including “No noise”, “Poisson”, “Gaussian” and “Additive Gaussian” were used to generate semi-synthetic training pairs from the same set of HR SEM images. The “No noise” crappifier simply downsampled HR image pixel sizes by a factor of 16x (4×4) without adding any noise, while the “Poisson”, “Gaussian” and “Additive Gaussian” crappifiers added random Poisson noise, random Gaussian noise (<inline-formula><mml:math display="inline" id="M6"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.15</mml:mn></mml:math></inline-formula>) and random Gaussian-distributed additive noise (<inline-formula><mml:math display="inline" id="M7"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:math></inline-formula>) respectively, before applying pixel downsampling. “Additive Gaussian (~80x)” used the same crappifier as “Additive Gaussian”, but with ~80x more training data. We also compared the models described above with “Real-world”, a model trained with real-world pairs, whose HR images are the same as the HR images of the semi-synthetically generated training pairs, but whose LR images were manually acquired at the microscope.</p>
      </sec>
      <sec id="S15">
        <title>MitoTracker crappifier comparison.</title>
        <p id="P39">Five crappifiers including “No noise”, “Salt &amp; Pepper”, “Gaussian”, “Additive Gaussian” and “Salt &amp; Pepper + Additive Gaussian” were used to generate semi-synthetic training pairs from the same set of HR Airyscan MitoTracker time-lapse videos. The “No noise” crappifier downsampled HR image pixel sizes by a factor of 16x (4×4) without adding any noise, while the “Salt &amp; Pepper”, “Gaussian”, “Additive Gaussian”, and “Salt &amp; Pepper + Additive Gaussian” crappifiers added random Salt &amp; Pepper noise (0.5%), random Gaussian noise (<inline-formula><mml:math display="inline" id="M8"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.025</mml:mn></mml:math></inline-formula>), random Gaussian-distributed additive noise (<inline-formula><mml:math display="inline" id="M9"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math></inline-formula>), and the combination of “Salt &amp; Pepper” and “Additive Gaussian” respectively, before the bilinear downsampling.</p>
      </sec>
    </sec>
    <sec id="S16">
      <title>Neural Networks</title>
      <sec id="S17">
        <title>Single-frame Neural Network (PSSR-SF).</title>
        <p id="P40">A ResNet-based U-Net was used as our convolutional neural network for training<sup><xref rid="R41" ref-type="bibr">41</xref></sup>. Our U-Net is in the form of encoder-decoder with skip-connections, where the encoder gradually downsizes an input image, followed by the decoder upsampling the image back to its original size. For the EM data, we utilized ResNet pretrained on ImageNet as the encoder. For the design of the decoder, the traditional handcrafted bicubic upscaling filters are replaced with learnable sub-pixel convolutional layers<sup><xref rid="R42" ref-type="bibr">42</xref></sup>, which can be trained specifically for upsampling each feature map optimized in low-resolution parameter space. This upsampling layer design enables better performance and largely reduces computational complexity, but at the same time causes unignorable checkerboard artifacts due to the periodic time-variant property of multirate upsampling filters<sup><xref rid="R43" ref-type="bibr">43</xref></sup>. A blurring technique<sup><xref rid="R44" ref-type="bibr">44</xref></sup> and a weight initialization method known as “sub-pixel convolution initialized to convolution neural network resize (ICNR)”<sup><xref rid="R45" ref-type="bibr">45</xref></sup> designed for the sub-pixel convolution upsampling layers were implemented to remove checkerboard artifacts. In detail, the blurring approach introduces an interpolation kernel of the zero-order hold with the scaling factor after each upsampling layer, the output of which gives out a non-periodic steady-state value, which satisfies a critical condition ensuring a checkerboard artifact-free upsampling scheme<sup><xref rid="R44" ref-type="bibr">44</xref></sup>. Compared to random initialization, in addition to the benefit of removing checkerboard artifacts, ICNR also empowers the model with higher modeling power and higher accuracy<sup><xref rid="R45" ref-type="bibr">45</xref></sup>. A self-attention layer inspired by Zhang et al.<sup><xref rid="R46" ref-type="bibr">46</xref></sup> was added after each convolutional layer.</p>
      </sec>
      <sec id="S18">
        <title>Multi-frame Neural Network (PSSR-MF).</title>
        <p id="P41">A similar yet slightly modified U-Net was used for time-lapse movie training. The input layer was redesigned to take five frames simultaneously while the last layer still produced one frame as output.</p>
      </sec>
    </sec>
    <sec id="S19">
      <title>Training Details</title>
      <sec id="S20">
        <title>Loss Function.</title>
        <p id="P42">MSE loss was used as our loss function.</p>
      </sec>
      <sec id="S21">
        <title>Optimization Methods.</title>
        <p id="P43">Stochastic gradient descent with restarts (SGDR)<sup><xref rid="R47" ref-type="bibr">47</xref></sup> was implemented. Aside from the benefits we are able to get through classic stochastic gradient descent, SGDR resets the learning rate to its initial value at the beginning of each training epoch and allows it to decrease again following the shape of a cosine function, yielding lower loss with higher computational efficiency.</p>
      </sec>
      <sec id="S22">
        <title>Cyclic Learning Rate and Momentum.</title>
        <p id="P44">Instead of having a gradually decreasing learning rate as the training converges, we adopted cyclic learning rates<sup><xref rid="R48" ref-type="bibr">48</xref></sup>, cycling between upper bound and lower bound, which helps oscillate towards a higher learning rate, thus avoiding saddle points in the hyper-dimensional training loss space. In addition, we followed The One Cycle Policy<sup><xref rid="R49" ref-type="bibr">49</xref></sup>, which restricts the learning rate to only oscillate once between the upper and lower bounds. Specifically, the learning rate linearly increases from the lower bound to the upper bound as the momentum decreases from its upper bound to the lower bound linearly. In the second half of the cycle, the learning rate fits a cosine annealing decreasing from the upper bound to zero while the momentum increases from its lower bound to the upper bound following the same annealing. This training technique achieves superior regularization by preventing the network from overfitting during the middle of the learning process, as well as enables super-convergence<sup><xref rid="R50" ref-type="bibr">50</xref></sup> by allowing large learning rates and adaptive momentum.</p>
      </sec>
      <sec id="S23">
        <title>Progressive Resizing (used for EM data only).</title>
        <p id="P45">Progressive resizing was applied during the training of the EM model. Training was executed in two rounds with HR images scaled to xy pixel sizes of 256 × 256 and 512 × 512 and LR images scaled to 64 × 64 and 128 × 128 progressively. The first round was initiated with an ImageNet pretrained ResU-Net, and the model trained from the first round served as the pre-trained model for the second round. The intuition behind this is it quickly reduces the training loss by allowing the model to see lots of images at a small scale during the early stages of training. As the training progresses, the model focuses more on picking up high-frequency features reflected through fine details that are only stored within larger scale images. Therefore, features that are scale-variant can be recognized through the progressively resized learning at each scale.</p>
      </sec>
      <sec id="S24">
        <title>Discriminative Learning Rates (used for EM data only).</title>
        <p id="P46">To better preserve the previously learned information, discriminative learning was applied during each round of training for the purpose of fine-tuning. At the first stage of training, only the parameters from the last layer were trainable after loading a pretrained model, which either came from a large-scaled trained publicly available model (i.e., pretrained ImageNet), or from the previous round of training. The learning rate for this stage <inline-formula><mml:math display="inline" id="M10"><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> was fixed. Parameters from all layers were set as learnable in the second stage. A linearly spaced learning rate range <inline-formula><mml:math display="inline" id="M11"><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> was applied. The learning rate gradually increased across the layers of the entire network architecture. The number of training epochs at each round is noted as <inline-formula><mml:math display="inline" id="M12"><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi> </mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math></inline-formula>, where <inline-formula><mml:math display="inline" id="M13"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math display="inline" id="M14"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> denote the epoch number used at stage one and stage two separately (<xref rid="T3" ref-type="table">Table 3</xref>).</p>
      </sec>
      <sec id="S25">
        <title>Best Model Preservation (used for fluorescence data only).</title>
        <p id="P47">Instead of saving the last model after training a fixed number of epochs, at the end of each training epoch, PSSR checks if the validation loss goes down compared to the loss from the previous epoch and will only update the best model when a lower loss is found. This technique ensures the best model will not be missed due to local loss fluctuation during the training.</p>
      </sec>
      <sec id="S26">
        <title>Elimination of Tiling Artifacts.</title>
        <p id="P48">Testing images often need to be cropped into smaller tiles before being fed into our model due to the memory limit of graphic cards. This creates tiling edge artifacts around the edges of tiles when stitching them back to the original images. A Gaussian blur kernel (<inline-formula><mml:math display="inline" id="M15"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>) was applied to a 10-pixel wide rectangle region centered in each tiling edge to eliminate the artifacts.</p>
      </sec>
      <sec id="S27">
        <title>Technical specifications.</title>
        <p id="P49">Final models were generated using fast.ai v1.0.55 library (<ext-link ext-link-type="uri" xlink:href="https://github.com/fastai/fastai">https://github.com/fastai/fastai</ext-link>), PyTorch on two NVIDIA TITAN RTX GPUs. Initial experiments were conducted using NVIDIA Tesla V100s, NVIDIA Quadro P6000s, NVIDIA Quadro M5000s, NVIDIA Titan Vs, NVIDIA GeForce GTX 1080s, or NVIDIA GeForce RTX 2080Ti GPUs.</p>
      </sec>
    </sec>
    <sec id="S28">
      <title>Evaluation Metrics</title>
      <sec id="S29">
        <title>PSNR and SSIM quantification.</title>
        <p id="P50">Two classic image quality metrics, PSNR and SSIM, known for their properties of pixel-level data fidelity and perceptual quality fidelity correspondingly, were used for the quantification of our paired testing image sets.</p>
        <p id="P51">PSNR is inversely correlated with MSE, numerically reflecting the pixel intensity difference between the reconstruction image and the ground truth image, but it is also famous for poor performance when it comes to estimating human perceptual quality. Instead of traditional error summation methods, SSIM is designed to consider distortion factors like luminance distortion, contrast distortion and loss of correlation when interpreting image quality<sup><xref rid="R51" ref-type="bibr">51</xref></sup>.</p>
      </sec>
      <sec id="S30">
        <title>SNR quantification.</title>
        <p id="P52">SNR was quantified for LSM testing images (<xref rid="F4" ref-type="fig">Fig. 4b</xref> and <xref rid="F5" ref-type="fig">Fig. 5a</xref>) by:
<disp-formula id="FD1"><mml:math display="block" id="M16"><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula>
where <inline-formula><mml:math display="inline" id="M17"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the maximum intensity value in the image, <inline-formula><mml:math display="inline" id="M18"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math display="inline" id="M19"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represent the mean and the standard deviation of the background, respectively<sup><xref rid="R17" ref-type="bibr">17</xref></sup>.</p>
      </sec>
      <sec id="S31">
        <title>Fourier-Ring-Correlation (FRC) analysis.</title>
        <p id="P53">NanoJ-SQUIRREL<sup><xref rid="R25" ref-type="bibr">25</xref></sup> was used to calculate image resolution using FRC method on real-world testing examples with two independent acquisitions of fixed samples (<xref rid="F2" ref-type="fig">Fig. 2b</xref>–<xref rid="F2" ref-type="fig">c</xref>, <xref rid="F4" ref-type="fig">4c</xref> and <xref rid="F5" ref-type="fig">Fig. 5b</xref>).</p>
      </sec>
      <sec id="S32">
        <title>Resolution Scaled Error (RSE) and Resolution Scaled Pearson’s coefficient (RSP).</title>
        <p id="P54">NanoJ-SQUIRREL<sup><xref rid="R25" ref-type="bibr">25</xref></sup> was used to calculate the RSE and RSP for both semi-synthetic and real-world acquired low (LR), bilinear interpolated (LR-Bilinear), and PSSR (LR-PSSR) images versus ground truth high-resolution (HR) images. Difference error maps were also calculated (<xref rid="F7" ref-type="fig">Extended Data Fig. 2</xref>, <xref rid="F12" ref-type="fig">7</xref> and <xref rid="F15" ref-type="fig">10</xref>).</p>
      </sec>
    </sec>
    <sec id="S33">
      <title>EM Imaging and Analysis</title>
      <sec id="S34">
        <title>tSEM high-resolution training data acquisition.</title>
        <p id="P55">Tissue from a perfused 7-month old Long Evans male rat was cut from the left hemisphere, stratum radiatum of CA1 of the hippocampus. The tissue was stained, embedded, and sectioned at 45nm using previously described protocols<sup><xref rid="R52" ref-type="bibr">52</xref></sup>. Sections were imaged using a STEM detector on a ZEISS Supra 40 scanning electron microscope with a 28kV accelerating voltage and an extractor current of 102μA (gun aperture 30μm). Images were acquired with a 2nm pixel size and a field size of 24576 × 24576 pixels with Zeiss ATLAS. The working distance from the specimen to the final lens was 3.7mm, and the dwell time was 1.2μs.</p>
      </sec>
      <sec id="S35">
        <title>EM testing sample preparation and image acquisition.</title>
        <p id="P56">EM data sets were acquired from multiple systems at multiple institutions for this study.</p>
        <p id="P57">For our testing ground truth data, paired LR and HR images of the adult mouse hippocampal dentate gyrus middle molecular layer neuropil were acquired from ultrathin sections (80nm) collected on silicon chips and imaged in a ZEISS Sigma VP FE-SEM<sup><xref rid="R21" ref-type="bibr">21</xref></sup>. All animal work was approved by the Institutional Animal Care and Use Committee (IACUC) of the Salk Institute for Biological Studies. Samples were prepared for EM according the NCMIR protocol<sup><xref rid="R53" ref-type="bibr">53</xref></sup>. Pairs of 4 × 4μm images were collected from the same region at pixels sizes of both 8nm and 2nm using Fibics ATLAS software (InLens detector; 3kV; dwell time, 5.3μs; line averaging, 2; aperture, 30μm; working distance, 2mm).</p>
        <p id="P58">Serial block face scanning electron microscope images were acquired with a Gatan 3View system installed on the ZEISS Sigma VP FE-SEM. Images were acquired using a pixel size of 8nm on a Gatan backscatter detector at 1kV and a current of 221pA. The pixel dwell time was 2μs with an aperture of 30μm and a working distance of 6.81mm. The section thickness was 100nm and the field of view was 24.5 × 24.5μm.</p>
        <p id="P59">Mouse FIB-SEM data sample preparation and image acquisition settings were previously described in the original manuscript the datasets were published<sup><xref rid="R22" ref-type="bibr">22</xref></sup>. Briefly, the images were acquired with 4nm voxel resolution. We downsampled the lateral resolution to 8nm, then applied our PSSR model to the downsampled data to ensure the proper 8-to-2nm transformation for which the PSSR was trained.</p>
        <p id="P60">Fly FIB-SEM data sample preparation and image acquisition settings were previously described in the original manuscript the datasets were published<sup><xref rid="R54" ref-type="bibr">54</xref></sup>. Briefly, images were acquired with 10nm voxel resolution. We first upsampled the xy resolution to 8nm using bilinear interpolation, then applied our PSSR model to the upsampled data to ensure the proper 8-to-2nm transformation for which the PSSR model was originally trained.</p>
        <p id="P61">The rat SEM data sample was acquired from an 8-week old male Wistar rat that was anesthetized with an overdose of pentobarbital (75 mg kg<sup>−1</sup>) and perfused through the heart with 5 – 10ml of a solution of 250 mM sucrose 5mM MgCl<sub>2</sub> in 0.02 M phosphate buffer (pH 7.4) (PB) followed by 200 ml of 4% paraformaldehyde containing 0.2% picric acid and 1% glutaraldehyde in 0.1 M PB. Brains were then removed and oblique horizontal sections (50μm thick) of frontal cortex/striatum were cut on a vibrating microtome (Leica VT1200S, Nussloch, Germany) along the line of the rhinal fissure. The tissue was stained and cut to 50nm sections using ATUMtome (RMC Boeckeler, Tucson, USA) for SEM imaging using the protocol described in the original publication for which the data was acquired<sup><xref rid="R55" ref-type="bibr">55</xref></sup>. The Hitachi Regulus rat SEM data was acquired using a Regulus 8240 FE-SEM with an acceleration voltage of 1.5kV, a dwell time of 3μs, using the backscatter detector with a pixel resolution of 10 × 10nm. We first upsampled the xy resolution to 8nm using bilinear interpolation, then applied our PSSR model to the upsampled data to ensure the proper 8-to-2nm transformation for which the PSSR model was originally trained.</p>
      </sec>
      <sec id="S36">
        <title>EM segmentation and analysis.</title>
        <p id="P62">Image sets generated from the same region of neuropil (LR-Bilinear; LR-PSSR; HR) were aligned rigidly using the ImageJ plugin Linear stack alignment with SIFT<sup><xref rid="R56" ref-type="bibr">56</xref></sup>. Presynaptic axonal boutons (n = 10) were identified and cropped from the image set. The bouton image sets from the three conditions were then assigned randomly generated filenames and distributed to two blinded human experts for manual segmentation of presynaptic vesicles. Vesicles were identified by having a clear and complete membrane, being round in shape, and of approximately 35nm in diameter. For consistency between human experts, vesicles that were embedded in or attached to obliquely sectioned axonal membranes were excluded. Docked and non-docked synaptic vesicles were counted as separate pools. Vesicle counts were recorded and unblinded and grouped by condition and by expert counter. Linear regression analyses were conducted between the counts of the HR images and the corresponding images of the two different LR conditions (LR-Bilinear; LR-PSSR) to determine how closely the counts corresponded between the HR and LR conditions. Linear regression analysis was also used to determine the variability between counters.</p>
      </sec>
    </sec>
    <sec id="S37">
      <title>Fluorescence Imaging and Analysis</title>
      <sec id="S38">
        <title>U2OS cell culture.</title>
        <p id="P63">U2OS cells were purchased from ATCC. Cells were grown in DMEM supplemented with 10% fetal bovine serum at 37 °C with 5% CO<sub>2</sub>. Cells were plated onto either 8-well #1.5 imaging chambers or #1.5 35 mm dishes (Cellvis) coated with 10 μg/mL fibronectin in PBS at 37 °C for 30 minutes prior to plating. 50nm MitoTracker Deep Red or CMXRos Red (ThermoFisher) was added for 30 minutes then washed for at least 30 minutes to allow for recovery time before imaging in FluoroBrite (ThermoFisher) media.</p>
      </sec>
      <sec id="S39">
        <title>Airyscan confocal imaging of U2OS cells.</title>
        <p id="P64">To generate our ground truth training and testing dataset we used a ZEISS Airyscan LSM 880, an advanced confocal microscope that uses a 32-detector array and post-processing pixel reassignment to generate images ~1.7x higher in resolution (~120nm) and ~8x higher SNR than a standard confocal system. All HR ground truth training data were acquired with a 63x objective with at least 2x Nyquist sampling pixel sizes (~50±10nm pixels), then Airyscan processed (deconvolved) using ZEISS Zen software. For the real-world LR test data, we acquired images at 16x lower pixel resolution (~200nm pixel sizes) with a 2.5 AU pinhole on a PMT confocal detector, without any additional image processing. We maintained equal pixel dwell times for the HR versus LR testing acquisitions, resulting in overall 16x shorter exposure times for the LR images. To ensure minimal phototoxicity, we also decreased the laser power for our LR acquisitions by a factor of 4 or 5 (see <xref rid="T1" ref-type="table">Tables 1</xref>–<xref rid="T2" ref-type="table">2</xref> for more details), resulting in a net laser dose decrease of ~64–80x (e.g., 5x lower laser power and 16x shorter exposure time yields a 80x lower laser dose). Furthermore, our LR testing data was not deconvolved and used a much larger effective pinhole size than the HR Airyscan ground truth data, resulting in a blurrier image with lower optical resolution. Cells were imaged with a 63× 1.4 NA oil objective on a ZEISS 880 LSM Airyscan confocal system with an inverted stage and heated incubation system with 5% CO<sub>2</sub> control. For both HR and LR images, equal or lower (when indicated) laser power and equal pixel dwell time of ~1 μs/pixel was used. For testing PSSR-MF, at least 10 sequential frames of fixed samples were acquired with high- and low-resolution settings in order to facilitate PSSR-MF processing.</p>
      </sec>
      <sec id="S40">
        <title>Neuron preparation.</title>
        <p id="P65">Primary hippocampal neurons were prepared from E18 rat (Envigo) embryos as previously described. Briefly, hippocampal tissue was dissected from embryonic brain and further dissociated to single hippocampal neuron by trypsinization with Papain (Roche). The prepared neurons were plated on coverslips (Bellco Glass) coated with 3.33μg/mL laminin (Life Technologies) and 20μg/mL poly-L-Lysine (Sigma) at the density of 7.5 × 10<sup>4</sup> cells/cm<sup>2</sup>. The cells were maintained in Neurobasal medium supplemented with B27 (Life Technologies), penicillin/streptomycin and L-glutamine for 7 – 21 days in vitro. Two days before imaging, the hippocampal neurons were transfected with Lipofectamine 2000 (Life Technologies).</p>
      </sec>
      <sec id="S41">
        <title>Temporal consistency analysis.</title>
        <p id="P66">Given a preprocessed time-lapse movie with <inline-formula><mml:math display="inline" id="M20"><mml:mi>N</mml:mi></mml:math></inline-formula> total number of frames, the cross-correlation coefficient (<inline-formula><mml:math display="inline" id="M21"><mml:mi>∅</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>i</mml:mi><mml:mi>ϵ</mml:mi><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>) was calculated for two neighboring frames (<inline-formula><mml:math display="inline" id="M22"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math display="inline" id="M23"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>) repeatedly across each movie with a step size of <inline-formula><mml:math display="inline" id="M24"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> (<xref rid="F4" ref-type="fig">Fig. 4b</xref>). To ensure comparisons between PSSR output and ground truth data were not biased by high frequency artifacts and noise, each time-lapse was first preprocessed with a Gaussian blur filter (<inline-formula><mml:math display="inline" id="M25"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>)</mml:mo><mml:mo>.</mml:mo><mml:mi> </mml:mi></mml:math></inline-formula></p>
      </sec>
      <sec id="S42">
        <title>Fission event detection and analysis.</title>
        <p id="P67">Given fission events cannot be precisely defined across different evaluators, a HR timelapse of mitotracker stained cells was first given to two human experts as a pilot experiment to examine and correct the inspection performance of all experts. Three conditions (LR-Bilinear; LR-PSSR; HR) of the same Airyscan timelapses (n = 6) were then sequentially assigned to two blinded human experts for mitochondria fission event detection. Fission event counts were recorded and unblinded and grouped by condition and by expert counter. Linear regression analyses were conducted between the counts of the HR images and the corresponding images of the two different LR conditions (LR-Bilinear; LR-PSSR) to determine how closely the counts corresponded between the HR and LR conditions. Linear regression analysis was used to determine the variability between counters.</p>
      </sec>
      <sec id="S43">
        <title>Neuronal mitochondria imaging and kymograph analysis.</title>
        <p id="P68">Live-cell imaging of primary neurons was performed using a Zeiss LSM 880 confocal microscope, enclosed in a temperature control chamber at 37 °C and 5% CO<sub>2</sub>, using a 63x (NA 1.4) oil objective in SR-Airyscan mode (i.e. 0.2 AU virtual pinhole). For low-resolution conditions, images were acquired with a confocal PMT detector with a pinhole size of 2.5 AU at 440 × 440 pixels at 0.5x Nyquist (170nm/pixel) every 270.49ms using a pixel dwell time of 1.2μs and a laser power ranging between 1 – 20 μW. For high-resolution conditions, images were acquired at 1764 × 1764 pixels at 2x Nyquist (~42nm/pixel) every 4.33s using a pixel dwell time of 1.2μs and a laser power of 20μW. Imaging data were collected using Zen Black software. High-resolution images were further processed using Zen Blue’s 2D-SR Airyscan processing. Kymograph analysis of the time-lapse movies were conducted using ImageJ plugin Kymolyzer as described previously<sup><xref rid="R57" ref-type="bibr">57</xref></sup>.</p>
      </sec>
      <sec id="S44">
        <title>Fluorescence photobleaching quantification.</title>
        <p id="P69">Normalized mean intensity over time was measured using Fiji software. Given a time-lapse movie with <inline-formula><mml:math display="inline" id="M26"><mml:mi>N</mml:mi></mml:math></inline-formula> frames, a background region was randomly selected and remained unchanged across frames. The normalized mean intensity (<inline-formula><mml:math display="inline" id="M27"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>) can be expressed as:
<disp-formula id="FD2"><mml:math display="block" id="M28"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>X</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:math></disp-formula>
where <inline-formula><mml:math display="inline" id="M29"><mml:mi>i</mml:mi></mml:math></inline-formula> represents the frame index, <inline-formula><mml:math display="inline" id="M30"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> represents the mean intensity of the selected background region at frame <inline-formula><mml:math display="inline" id="M31"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math display="inline" id="M32"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> represents the intensity mean of the entire frame <inline-formula><mml:math display="inline" id="M33"><mml:mi>i</mml:mi></mml:math></inline-formula>.</p>
      </sec>
    </sec>
    <sec id="S45">
      <title>Comparing PSSR with other methods</title>
      <sec id="S46">
        <title>Block-matching and 3D filtering (BM3D) denoising algorithm.</title>
        <p id="P70">We compared PSSR with BM3D, on both EM and fluorescence MitoTracker data. Application of BM3D before (LR-BM3D-Bilinear) and after (LR-Bilinear-BM3D) bilinear upsampling of pixel sizes were both tested. A wide range of Sigma (<inline-formula><mml:math display="inline" id="M34"><mml:mi>σ</mml:mi><mml:mo>∈</mml:mo><mml:mfenced close="]" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mn>95</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>), the key parameter that defines the assumed zero-mean white Gaussian noise in the images, was thoroughly explored. The exact same test set was used for quantification of PSSR vs. BM3D results.</p>
      </sec>
      <sec id="S47">
        <title>Content-aware Image Restoration (CARE) and Rolling Average (RA) methods.</title>
        <p id="P71">A semi-synthetically crappified dataset of MitoTracker data was used to train both CARE and PSSR networks (PSSR-SF and PSSR-MF) in a consistent manner. We applied the trained models to semi-synthetically generated low-resolution testing time-lapses. Testing data were similarly crappified as training data. Five-frame rolling average processing was further applied to CARE and single-frame testing output.</p>
      </sec>
    </sec>
    <sec sec-type="data-availability" id="S48">
      <title>Data Availability</title>
      <p id="P72">Example training data and pretrained models are included in the GitHub release (<ext-link ext-link-type="uri" xlink:href="https://github.com/BPHO-Salk/PSSR">https://github.com/BPHO-Salk/PSSR</ext-link>). The entirety of our training and testing data sets and data sources are available at Texas Data Repository (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18738/T8/YLCK5A">https://doi.org/10.18738/T8/YLCK5A</ext-link>).</p>
    </sec>
    <sec id="S49">
      <title>Code Availability</title>
      <p id="P73">PSSR source code and documentation are available for download on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/BPHO-Salk/PSSR">https://github.com/BPHO-Salk/PSSR</ext-link>) and are free for use under the BSD 3-Clause License.</p>
    </sec>
    <sec id="S50">
      <title>Reporting summary</title>
      <p id="P74">Further information on research design is available in the Life Sciences Reporting Summary linked to this article.</p>
    </sec>
  </sec>
  <sec sec-type="extended-data" id="S51">
    <title>Extended Data</title>
    <fig id="F6" orientation="portrait" position="anchor">
      <label>Extended Data Fig. 1</label>
      <caption>
        <title>PSSR Neural Network architecture.</title>
        <p id="P75">Shown is the ResNet-34 based U-Net architecture. Single-frame PSSR (PSSR-SF) and multi-frame PSSR (PSSR-MF) have 1 or 5 input channels, separately.</p>
      </caption>
      <graphic xlink:href="nihms-1668081-f0006"/>
    </fig>
    <fig id="F7" orientation="portrait" position="anchor">
      <label>Extended Data Fig. 2</label>
      <caption>
        <title>NanoJ-SQUIRREL error-maps of EM data.</title>
        <p id="P76">NanoJ-SQUIRREL was used to calculate the resolution scaled error (RSE) and resolution scaled Pearson’s coefficient (RSP) for both semi-synthetic and real-world acquired low (LR), bilinear interpolated (LR-Bilinear), and PSSR (LR-PSSR) images versus ground truth high-resolution (HR) images. For these representative images from <xref rid="F2" ref-type="fig">Fig. 2</xref>, the RSE and RSP images are shown along with the difference images for each output.</p>
      </caption>
      <graphic xlink:href="nihms-1668081-f0007"/>
    </fig>
    <fig id="F8" orientation="portrait" position="anchor">
      <label>Extended Data Fig. 3</label>
      <caption>
        <title>Comparison of PSSR vs. BM3D on EM data.</title>
        <p id="P77">PSSR restoration was compared to the Block-matching and 3D filtering (BM3D) denoising algorithm. BM3D was applied to low-resolution real-world SEM images before (LR-BM3D-Bilinear) and after (LR-Bilinear-BM3D) bilinear upsampling. A wide range of Sigma (<inline-formula><mml:math display="inline" id="M35"><mml:mi>σ</mml:mi><mml:mo>∈</mml:mo><mml:mfenced close="]" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mn>95</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, with step size of 5), the key parameter that defines the assumed zero-mean white Gaussian noise in BM3D method, was thoroughly explored. Images of the same region from the LR input, bilinear upsampled, PSSR restored, and Ground truth is displayed in (<bold>a</bold>). Results of LR-BM3D-Bilinear (<bold>b</bold>, top row) and LR-Bilinear-BM3D (b, bottom row) with sigma ranging from [10, 15, … , 35] are shown. PSNR and SSIM results of LR-BM3D-Bilinear and LR-Bilinear-BM3D across the explored range of sigma are plotted in (<bold>c</bold>) and (<bold>d</bold>). Metrics for bilinear-upsampled and PSSR-restored images of the same testing set are shown as dashed lines in orange (LR-Bilinear: PSNR=26.28±0.085; SSIM=0.767±0.0031) and blue (LR-PSSR: PSNR=27.21±0.084; SSIM=0.802 ±0.0026). n=12 independent images for all conditions. Values are shown as mean ± SEM.</p>
      </caption>
      <graphic xlink:href="nihms-1668081-f0008"/>
    </fig>
    <fig id="F9" orientation="portrait" position="anchor">
      <label>Extended Data Fig. 4</label>
      <caption>
        <title>Undersampling significantly reduces photobleaching.</title>
        <p id="P78">U2OS cells stained with mitotracker were imaged every 2 seconds with the same laser power (2.5μW) and pixel dwell time (~1μs), but with 16x lower resolution (196 × 196nm xy pixel size) than full resolution Airyscan acquisitions (~49 × 49nm xy pixel size). Mean intensity plots show the relative rates of fluorescence intensity loss over time (i.e. photobleaching) for LR, LR-PSSR, and HR images.</p>
      </caption>
      <graphic xlink:href="nihms-1668081-f0009"/>
    </fig>
    <fig id="F10" orientation="portrait" position="anchor">
      <label>Extended Data Fig. 5</label>
      <caption>
        <title>Evaluation of crappifiers with different noise injection on mitotracker data.</title>
        <p id="P79">Examples of crappified training images, visualized results and metrics (PSNR, SSIM and FRC resolution) of PSSR-SF models that were trained on high- and low-resolution pairs semi-synthetically generated by crappifiers with different noise injection were presented. <bold>a</bold>, Shown is an example of crappified training images generated by different crappifiers, including “No noise” (no added noise, downsampled pixel size only), Salt &amp; Pepper, Gaussian, Additive Gaussian, and a mixture of Salt &amp; Pepper plus Additive Gaussian noise. High-resolution version of the same region is also included. <bold>b</bold>, Visualized restoration performance of PSSR models that used different crappifiers (No noise, Salt &amp; Pepper, Gaussian, Additive Gaussian, and a mixture of Salt &amp; Pepper plus Additive Gaussian noise). LR input and Ground Truth of the example testing ROI are also shown. PSNR (<bold>c</bold>), SSIM (<bold>d</bold>) and FRC (<bold>e</bold>) quantification show the PSSR model that used “Salt &amp; Pepper + Additive Gaussian” crappifier yielded the best overall performance (n=10 independent timelapses of fixed samples with n=10 timepoints for all conditions). All values are shown as mean ± SEM. P values are specified in the figure for 0.0001&lt;p&lt;0.05. *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001, ****p&lt;0.0001, ns = not significant; Two-sided paired <italic>t</italic>-test.</p>
      </caption>
      <graphic xlink:href="nihms-1668081-f0010"/>
    </fig>
    <fig id="F11" orientation="portrait" position="anchor">
      <label>Extended Data Fig. 6</label>
      <caption>
        <title>Quantitative comparison of CARE and PSSR-SF with PSSR-MF and Rolling Average (RA) methods for timelapse data.</title>
        <p id="P80">PSNR (<bold>a</bold>) and SSIM (<bold>b</bold>) quantification show a decrease in accuracy when applying RA to LR-CARE and LR-PSSR-SF, while multi-frame PSSR provides superior performance compared to LR-PSSR-SF and CARE before and after RA processing. Data points were color-coded based on different cells. See <xref rid="F4" ref-type="fig">Fig. 4c</xref> for visualized comparisons, and <xref rid="SD17" ref-type="supplementary-material">Supplementary Movie 6</xref> for a video comparison of the entire timelapse for CARE, LR-PSSR-SF, LR-PSSR-SF-RA, and LR-PSSR-MF. N=5 independent timelapses with n=30 timepoints each, achieving similar results. All values are shown as mean ± SEM. ****p&lt;0.0001; Two-sided paired <italic>t</italic>-test.</p>
      </caption>
      <graphic xlink:href="nihms-1668081-f0011"/>
    </fig>
    <fig id="F12" orientation="portrait" position="anchor">
      <label>Extended Data Fig. 7</label>
      <caption>
        <title>NanoJ-SQUIRREL error-maps of MitoTracker data.</title>
        <p id="P81">NanoJ-SQUIRREL was used to calculate the resolution scaled error (RSE) and resolution scaled Pearson’s coefficient (RSP) for both semi-synthetic and real-world acquired low (LR), bilinear interpolated (LR-Bilinear), and PSSR (LR-PSSR) images versus ground truth high-resolution (HR) images. For these representative images from <xref rid="F4" ref-type="fig">Fig. 4</xref>, the RSE and RSP images are shown along with the difference images for each output.</p>
      </caption>
      <graphic xlink:href="nihms-1668081-f0012"/>
    </fig>
    <fig id="F13" orientation="portrait" position="anchor">
      <label>Extended Data Fig. 8</label>
      <caption>
        <title>Compare PSSR with BM3D denoising method on mitotracker data.</title>
        <p id="P82">PSSR restored images was compared to results of applying BM3D denoising algorithm to low-resolution real-world mitotracker images before (LR-BM3D-Bilinear) and after (LR-Bilinear-BM3D) bilinear upsampling. A wide range of Sigma (<inline-formula><mml:math display="inline" id="M36"><mml:mi>σ</mml:mi><mml:mo>∈</mml:mo><mml:mfenced close="]" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mn>95</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, with step size of 5) was thoroughly explored. Examples of the same region from the LR input, bilinear upsampled, PSSR-SF restored, PSSR-MF restored, and Ground truth are displayed (<bold>a</bold>, top row). Images from the top 6 results (evaluated by both PSNR and SSIM values) of LR-BM3D-Bilinear (<bold>a</bold>, middle row) and LR-Bilinear-BM3D (<bold>a</bold>, bottom row) are shown. PSNR and SSIM results of LR-BM3D-Bilinear and LR-Bilinear-BM3D across the explored range of sigma are plotted in (<bold>b</bold>) and (<bold>c</bold>). Metrics resulted from bilinearly upsampled, PSSR-SF restored and PSSR-MF restored images of the same testing set are shown as dash lines in orange (LR-Bilinear: PSNR=24.42±0.367; SSIM=0.579 ±0. 0287), blue (LR-PSSR-SF: PSNR=25.72±0.323; SSIM=0.769 ±0.0139) and green (LR-PSSR-MF: PSNR=26.89 ±0.322; SSIM=0.791±0.0133). As it shows, in this fluorescence mitotracker example, BM3D performs better than bilinear upsampling with carefully defined noise distribution, whereas its general performance given both PSNR and SSIM is overall worse than single-frame PSSR (LR-PSSR-SF). Excitably, our multi-frame PSSR (LR-PSSR-MF) yields the best performance. n=10 independent timelapses of fixed samples with n=6–10 timepoints each for all conditions. Values are shown as mean ± SEM.</p>
      </caption>
      <graphic xlink:href="nihms-1668081-f0013"/>
    </fig>
    <fig id="F14" orientation="portrait" position="anchor">
      <label>Extended Data Fig. 9</label>
      <caption>
        <title>NanoJ-SQUIRREL error-maps of neuronal mitochondria data.</title>
        <p id="P83">NanoJ-SQUIRREL was used to calculate the resolution scaled error (RSE) and resolution scaled Pearson’s coefficient (RSP) for both semi-synthetic and real-world acquired low (LR), bilinear interpolated (LR-Bilinear), and PSSR (LR-PSSR) images versus ground truth high-resolution (HR) images. For these representative images from <xref rid="F5" ref-type="fig">Fig. 5</xref>, the RSE and RSP images are shown along with the difference images for each output.</p>
      </caption>
      <graphic xlink:href="nihms-1668081-f0014"/>
    </fig>
    <fig id="F15" orientation="portrait" position="anchor">
      <label>Extended Data Fig. 10</label>
      <caption>
        <title>PSSR facilitates detection of mitochondrial motility and dynamics.</title>
        <p id="P84">Rat hippocampal neurons expressing mito-dsRed were undersampled with a confocal detector using 170nm pixel resolution (LR) to facilitate faster frame rates, then restored with PSSR (LR-PSSR). <bold>a,</bold> before and after time points of the event shown in <xref rid="F5" ref-type="fig">Fig. 5</xref> wherein two adjacent mitochondria pass one another but cannot be resolved in the original low-resolution (LR) or bilinear interpolated (LR-Bilinear) image but are clearly resolved in the LR-PSSR image. <bold>b,</bold> kymographs of a LR vs LR-PSSR timelapse that facilitates the detection of a mitochondrial fission event (yellow arrow).</p>
      </caption>
      <graphic xlink:href="nihms-1668081-f0015"/>
    </fig>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="SD1">
      <label>1</label>
      <media xlink:href="NIHMS1668081-supplement-1.pdf" orientation="portrait" id="d40e2263" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD2">
      <label>1668081_SourceDataFig1</label>
      <p id="P85">Statistical source data for PSNR, SSIM and FRC resolution plots (panel c-e)</p>
      <media xlink:href="NIHMS1668081-supplement-1668081_SourceDataFig1.xlsx" orientation="portrait" id="d40e2269" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD3">
      <label>1668081_SourceDataFig2</label>
      <p id="P86">Statistical source data for PSNR, SSIM and FRC resolution plots of both semi-synthetic (panel b) and real-world testing data (panel c)</p>
      <media xlink:href="NIHMS1668081-supplement-1668081_SourceDataFig2.xlsx" orientation="portrait" id="d40e2275" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD4">
      <label>1668081_SourceDataFig3</label>
      <p id="P87">Statistical source data for EM vesicle segmentation analysis plots (panel e)</p>
      <media xlink:href="NIHMS1668081-supplement-1668081_SourceDataFig3.xlsx" orientation="portrait" id="d40e2281" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD5">
      <label>1668081_SourceDataFig5</label>
      <caption>
        <p id="P88">Statistical source data for PSNR, SSIM and FRC resolution plots of both semi-synthetic and real-world testing data (panel b) and mitochondrion mobility analysis plots (panel f-i)</p>
      </caption>
      <media xlink:href="NIHMS1668081-supplement-1668081_SourceDataFig5.xlsx" orientation="portrait" id="d40e2288" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD6">
      <label>1668081_SourceExtDataFig5</label>
      <caption>
        <p id="P89">Statistical source data for PSNR, SSIM and FRC resolution plots (panel c-e)</p>
      </caption>
      <media xlink:href="NIHMS1668081-supplement-1668081_SourceExtDataFig5.xlsx" orientation="portrait" id="d40e2295" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD7">
      <label>1668081_SourceExtDataFig4</label>
      <caption>
        <p id="P90">Source data for photobleaching intensity plots.</p>
      </caption>
      <media xlink:href="NIHMS1668081-supplement-1668081_SourceExtDataFig4.xlsx" orientation="portrait" id="d40e2302" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD8">
      <label>1668081_SourceDataFig4</label>
      <caption>
        <p id="P91">Statistical source data for flickering quantification plots (panel b), PSNR, SSIM and FRC resolution plots (panel f) and fission event counting plots (panel h-k)</p>
      </caption>
      <media xlink:href="NIHMS1668081-supplement-1668081_SourceDataFig4.xlsx" orientation="portrait" id="d40e2309" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD9">
      <label>1668081_SourceExtDataFig6</label>
      <caption>
        <p id="P92">Statistical source data for PSNR and SSIM plots (panel a-b)</p>
      </caption>
      <media xlink:href="NIHMS1668081-supplement-1668081_SourceExtDataFig6.xlsx" orientation="portrait" id="d40e2316" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD10">
      <label>1668081_SourceExtDataFig8</label>
      <caption>
        <p id="P93">Statistical source data for PSNR and SSIM plots (panel c-d)</p>
      </caption>
      <media xlink:href="NIHMS1668081-supplement-1668081_SourceExtDataFig8.xlsx" orientation="portrait" id="d40e2323" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD11">
      <label>1668081_SourceExtDataFig3</label>
      <caption>
        <p id="P94">Statistical source data for PSNR and SSIM plots (panel c-d)</p>
      </caption>
      <media xlink:href="NIHMS1668081-supplement-1668081_SourceExtDataFig3.xlsx" orientation="portrait" id="d40e2331" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD12">
      <label>1668081_SuppVideo1</label>
      <p id="P95">Comparison of high- and low-resolution serial blockface SEM (SBFSEM) 3View acquisition with 2nm and 8nm pixel resolutions. In the 2nm pixel size image stack, high contrast enabled by relatively higher electron doses ensured high-resolution and high SNR, which unfortunately at the same time caused severe sample damage, resulting in a failure to serially section the tissue after imaging the blockface. On the other hand, low-resolution acquisition at 8nm pixel size facilitated serial blockface imaging, but the compromised resolution and SNR made it impossible to uncover finer structures in the sample.</p>
      <media xlink:href="NIHMS1668081-supplement-1668081_SuppVideo1.mov" orientation="portrait" id="d40e2337" position="anchor" mimetype="video"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD13">
      <label>1668081_SuppVideo2</label>
      <p id="P96">Image restoration achieved by a tSEM-trained PSSR model enables higher resolution SBFSEM imaging. Shown are the lower resolution SBFSEM acquisition input (left) and the PSSR output (right).</p>
      <media xlink:href="NIHMS1668081-supplement-1668081_SuppVideo2.mov" orientation="portrait" id="d40e2343" position="anchor" mimetype="video"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD14">
      <label>1668081_SuppVideo3</label>
      <p id="P97">Resolution restoration achieved by tSEM-trained PSSR model enables higher resolution FIB-SEM acquisition. Shown are the lower resolution FIB-SEM acquisition input (left) and the PSSR output (right).</p>
      <media xlink:href="NIHMS1668081-supplement-1668081_SuppVideo3.mov" orientation="portrait" id="d40e2349" position="anchor" mimetype="video"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD15">
      <label>1668081_SuppVideo4</label>
      <p id="P98">PSSR facilitates efficient 3D segmentation and reconstruction. Shown is the rendering of the 3D reconstruction of multiple biological structures using the PSSR processed FIB-SEM stack described in <xref rid="F3" ref-type="fig">Fig. 3</xref> and <xref rid="SD14" ref-type="supplementary-material">Supplementary Movie 3</xref>. Specifically, this reconstruction includes mitochondria (purple), endoplasmic reticulum (yellow), presynaptic vesicles (gray), the postsynaptic neuron’s plasma membrane (blue), the postsynaptic density (red) and the presynaptic neuron’s plasma membrane (green).</p>
      <media xlink:href="NIHMS1668081-supplement-1668081_SuppVideo4.mov" orientation="portrait" id="d40e2361" position="anchor" mimetype="video"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD16">
      <label>1668081_SuppVideo5</label>
      <p id="P99">Photobleaching and cell stress due to high laser dose during high-resolution live cell imaging. Shown is a 10-minute high-resolution time-lapse movie of a U2OS cell stained with Mitotracker Red imaged with an Airyscan microscope. The live-cell acquisition suffered from photobleaching and phototoxicity as reflected by the steadily decreasing fluorescence intensity over time as well as the swelling and fragmenting mitochondria. Imaging condition: ~35μW laser power, 2s frame rate, 1.15μm pixel size.</p>
      <media xlink:href="NIHMS1668081-supplement-1668081_SuppVideo5.mov" orientation="portrait" id="d40e2367" position="anchor" mimetype="video"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD17">
      <label>1668081_SuppVideo6</label>
      <p id="P100">PSSR-MF reduces flicker observed in single-frame models (LR-CARE and LR-PSSR-SF) without loss of spatiotemporal resolution that occurs with rolling frame averaging (LR-CARE-RA and LR-PSSR-SF-RA). Shown is the restoration performance of single-frame PSSR (LR-PSSR-SF), PSSR-SF with 5-frame Rolling Average (LR-PSSR-SF-RA) and 5-frame multi-frame PSSR (LR-PSSR-MF). Rolling Average alleviated the signal flickering observed in single-frame PSSR at the cost of both temporal and spatial resolution. See <xref rid="F9" ref-type="fig">Extended Data Fig. 4</xref> for more detail.</p>
      <media xlink:href="NIHMS1668081-supplement-1668081_SuppVideo6.mov" orientation="portrait" id="d40e2376" position="anchor" mimetype="video"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD18">
      <label>1668081_SuppVideo7</label>
      <p id="P101">LR input and 5-frame multi-frame PSSR (LR-PSSR). multi-frame PSSSR restores resolution and SNR to Airyscan equivalent quality with no bleaching and higher imaging speed. Shown are PSSR-MF restoration output (right, ~49nm pixels) and its low-resolution acquisition input (left, ~196nm pixels). The digitally magnified region highlights a mitochondrial fission event much more easily detected in the PSSR output.</p>
      <media xlink:href="NIHMS1668081-supplement-1668081_SuppVideo7.mov" orientation="portrait" id="d40e2382" position="anchor" mimetype="video"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD19">
      <label>1668081_SuppVideo8</label>
      <p id="P102">Comparison of high-resolution Airyscan and low-resolution confocal time-lapse acquisition of neuronal mitochondria. Corresponding kymographs are also displayed to illustrate the difference in temporal resolution. The Airyscan acquisition has higher spatial resolution but lower temporal resolution due to lower imaging speed, while confocal acquisition gives higher temporal resolution but lower spatial resolution.</p>
      <media xlink:href="NIHMS1668081-supplement-1668081_SuppVideo8.mov" orientation="portrait" id="d40e2388" position="anchor" mimetype="video"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SD20">
      <label>1668081_SuppVideo9</label>
      <p id="P103">Comparison of PSSR (right) versus bilinear interpolation (left). The enlarged region highlights two adjacent mitochondria passing one another in an axon, the process of which was only resolved in PSSR. Line plot shows the normalized fluorescence intensity of the indicated cross-section.</p>
      <media xlink:href="NIHMS1668081-supplement-1668081_SuppVideo9.mov" orientation="portrait" id="d40e2394" position="anchor" mimetype="video"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S52">
    <title>Acknowledgements</title>
    <p id="P104">The authors thank John Sedat, Terry Sejnowski, Antonio Pinto-Duarte, Florian Jug, Martin Weigert, Kirti Prakash, Stephan Saalfeld, and the entire NSF NeuroNex consortium for invaluable advice and critical feedback on our data and the manuscript. The authors also thank Harald Hess and Shan Xu for sharing their FIB-SEM data. U.M., L.F., T.Z., and S.W.N. are supported by the Waitt Foundation, Core Grant application NCI CCSG (CA014195). U.M. is a Chan-Zuckerberg Initiative Imaging Scientist and supported by NSF NeuroNex Award No. 2014862, and NIH R21 DC018237. C.R.S. is supported by NIH F32 GM137580. J.H. and F.M. are supported by the Wicklow AI in Medicine Research Initiative. K.M.H. is supported by NSF Grant Nos. 1707356 and NSF NeuroNex Award No. 2014862 and NIH/NIMH Grant No. 2R56MH095980–06. Research in the laboratory of G.P. is supported by the Parkinson’s Foundation (PF-JFA-1888), and NIH/NIGMS Grant No. R35GM128823. S.B.Y. is funded by NIH Grant No. T32GM007240. Y.K. was supported by Japan Society for the Promotion of Science KAKENHI Grant 17H06311, 19H03336, and by AMED Grant JP20dm0207084. The authors acknowledge the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing GPU resources that have contributed to the research results reported within this paper. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the NVIDIA Quadro M5000 and NVIDIA Titan V used for this research.</p>
    <p id="P105"><bold>Reviewer recognition statement:</bold> Nature Methods thanks Dr. Jie Tian and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.</p>
  </ack>
  <fn-group>
    <fn id="FN2">
      <p id="P106"><bold>Editor summary:</bold> Point-scanning super-resolution imaging uses deep learning to supersample undersampled images and enable time-lapse imaging of subcellular events. An accompanying “crappifier” rapidly generates quality training data for robust performance.</p>
    </fn>
    <fn id="FN3">
      <p id="P107"><bold>Editor recognition statement:</bold> Rita Strack was the primary editor on this article and managed its editorial process and peer review in collaboration with the rest of the editorial team.</p>
    </fn>
    <fn fn-type="COI-statement" id="FN4">
      <p id="P108">Ethics Declaration</p>
      <p id="P109"><bold>Competing interests</bold> U.M. and L.F. have filed a patent application covering some aspects of this work (International Patent WO2020041517A9: “Systems and methods for enhanced imaging and analysis”, Inventors: Uri Manor, Linjing Fang, published on Oct 01, 2020). The rest of the authors declare no competing interest.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Chen</surname><given-names>J</given-names></name> &amp; <name><surname>Hoi</surname><given-names>SCH</given-names></name>
<article-title>Deep Learning for Image Super-resolution: A Survey</article-title>. <source>eprint arXiv:1902.06068</source>, <comment>arXiv:1902.06068</comment> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="R2">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Jain</surname><given-names>V</given-names></name><etal/><source>Supervised Learning of Image Restoration with Convolutional Networks</source>. (<year>2007</year>).</mixed-citation>
    </ref>
    <ref id="R3">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Romano</surname><given-names>Y</given-names></name>, <name><surname>Isidoro</surname><given-names>J</given-names></name> &amp; <name><surname>Milanfar</surname><given-names>P</given-names></name>. <article-title>RAISR: rapid and accurate image super resolution</article-title>. <source>IEEE Transactions on Computational Imaging</source>
<volume>3</volume>, <fpage>110</fpage>–<lpage>125</lpage> (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="R4">
      <label>4</label>
      <mixed-citation publication-type="confproc"><name><surname>Shrivastava</surname><given-names>A</given-names></name><etal/> in <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>. <fpage>2107</fpage>–<lpage>2116</lpage>.</mixed-citation>
    </ref>
    <ref id="R5">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Moen</surname><given-names>E</given-names></name><etal/><article-title>Deep learning for cellular image analysis</article-title>. <source>Nat Methods</source>, doi:<pub-id pub-id-type="doi">10.1038/s41592-019-0403-1</pub-id> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="R6">
      <label>6</label>
      <mixed-citation publication-type="confproc"><name><surname>Buchholz</surname><given-names>T-O</given-names></name>, <name><surname>Jordan</surname><given-names>M</given-names></name>, <name><surname>Pigino</surname><given-names>G</given-names></name> &amp; <name><surname>Jug</surname><given-names>F</given-names></name> in <conf-name>2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</conf-name>. <fpage>502</fpage>–<lpage>506</lpage> (<publisher-name>IEEE</publisher-name>).</mixed-citation>
    </ref>
    <ref id="R7">
      <label>7</label>
      <mixed-citation publication-type="confproc"><name><surname>Krull</surname><given-names>A</given-names></name>, <name><surname>Buchholz</surname><given-names>T-O</given-names></name> &amp; <name><surname>Jug</surname><given-names>F</given-names></name> in <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>. <fpage>2129</fpage>–<lpage>2137</lpage>.</mixed-citation>
    </ref>
    <ref id="R8">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Buchholz</surname><given-names>TO</given-names></name><etal/><article-title>Content-aware image restoration for electron microscopy</article-title>. <source>Methods Cell Biol</source><volume>152</volume>, <fpage>277</fpage>–<lpage>289</lpage>, doi:<pub-id pub-id-type="doi">10.1016/bs.mcb.2019.05.001</pub-id> (<year>2019</year>).<pub-id pub-id-type="pmid">31326025</pub-id></mixed-citation>
    </ref>
    <ref id="R9">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Weigert</surname><given-names>M</given-names></name><etal/><article-title>Content-aware image restoration: pushing the limits of fluorescence microscopy</article-title>. <source>Nat Methods</source><volume>15</volume>, <fpage>1090</fpage>–<lpage>1097</lpage>, doi:<pub-id pub-id-type="doi">10.1038/s41592-018-0216-7</pub-id> (<year>2018</year>).<pub-id pub-id-type="pmid">30478326</pub-id></mixed-citation>
    </ref>
    <ref id="R10">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Batson</surname><given-names>J</given-names></name> &amp; <name><surname>Royer</surname><given-names>L</given-names></name>. <article-title>Noise2self: Blind denoising by self-supervision</article-title>. <source>arXiv preprint arXiv:1901.11365</source> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="R11">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>Y</given-names></name><etal/><article-title>DLBI: deep learning guided Bayesian inference for structure reconstruction of super-resolution fluorescence microscopy</article-title>. <source>Bioinformatics</source><volume>34</volume>, <comment>i284-i294</comment>, doi:<pub-id pub-id-type="doi">10.1093/bioinformatics/bty241</pub-id> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R12">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Ouyang</surname><given-names>W</given-names></name>, <name><surname>Aristov</surname><given-names>A</given-names></name>, <name><surname>Lelek</surname><given-names>M</given-names></name>, <name><surname>Hao</surname><given-names>X</given-names></name> &amp; <name><surname>Zimmer</surname><given-names>C</given-names></name>. <article-title>Deep learning massively accelerates super-resolution localization microscopy</article-title>. <source>Nat Biotechnol</source>
<volume>36</volume>, <fpage>460</fpage>–<lpage>468</lpage>, doi:<pub-id pub-id-type="doi">10.1038/nbt.4106</pub-id> (<year>2018</year>).<pub-id pub-id-type="pmid">29658943</pub-id></mixed-citation>
    </ref>
    <ref id="R13">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Nelson</surname><given-names>AJ</given-names></name> &amp; <name><surname>Hess</surname><given-names>ST</given-names></name>
<article-title>Molecular imaging with neural training of identification algorithm (neural network localization identification)</article-title>. <source>Microsc Res Tech</source>
<volume>81</volume>, <fpage>966</fpage>–<lpage>972</lpage>, doi:<pub-id pub-id-type="doi">10.1002/jemt.23059</pub-id> (<year>2018</year>).<pub-id pub-id-type="pmid">30242941</pub-id></mixed-citation>
    </ref>
    <ref id="R14">
      <label>14</label>
      <mixed-citation publication-type="book"><name><surname>Heinrich</surname><given-names>L</given-names></name>, <name><surname>Bogovic</surname><given-names>JA</given-names></name> &amp; <name><surname>Saalfeld</surname><given-names>S</given-names></name> in <source>Medical Image Computing and Computer-Assisted Intervention − MICCAI</source>
<year>2017</year>. (eds <name><surname>Maxime</surname><given-names>Descoteaux</given-names></name>et al.) <fpage>135</fpage>–<lpage>143</lpage> (<publisher-name>Springer International Publishing</publisher-name>).</mixed-citation>
    </ref>
    <ref id="R15">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>de Haan</surname><given-names>K</given-names></name>, <name><surname>Ballard</surname><given-names>ZS</given-names></name>, <name><surname>Rivenson</surname><given-names>Y</given-names></name>, <name><surname>Wu</surname><given-names>Y</given-names></name> &amp; <name><surname>Ozcan</surname><given-names>A</given-names></name>. <article-title>Resolution enhancement in scanning electron microscopy using deep learning</article-title>. <source>Scientific Reports</source>
<volume>9</volume>, <fpage>1</fpage>–<lpage>7</lpage> (<year>2019</year>).<pub-id pub-id-type="pmid">30626917</pub-id></mixed-citation>
    </ref>
    <ref id="R16">
      <label>16</label>
      <mixed-citation publication-type="confproc"><name><surname>Sreehari</surname><given-names>S</given-names></name><etal/> in <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</conf-name>. <fpage>88</fpage>–<lpage>96</lpage>.</mixed-citation>
    </ref>
    <ref id="R17">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>H</given-names></name><etal/><article-title>Deep learning enables cross-modality super-resolution in fluorescence microscopy</article-title>. <source>Nat Methods</source><volume>16</volume>, <fpage>103</fpage>–<lpage>110</lpage>, doi:<pub-id pub-id-type="doi">10.1038/s41592-018-0239-0</pub-id> (<year>2019</year>).<pub-id pub-id-type="pmid">30559434</pub-id></mixed-citation>
    </ref>
    <ref id="R18">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>J</given-names></name><etal/><article-title>Three-dimensional residual channel attention networks denoise and sharpen fluorescence microscopy image volumes</article-title>. <source>bioRxiv</source>, <comment>2020.2008.2027.270439</comment>, doi:<pub-id pub-id-type="doi">10.1101/2020.08.27.270439</pub-id> (<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="R19">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Guo</surname><given-names>M</given-names></name><etal/><article-title>Rapid image deconvolution and multiview fusion for optical microscopy</article-title>. <source>Nat Biotechnol</source>, doi:<pub-id pub-id-type="doi">10.1038/s41587-020-0560-x</pub-id> (<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="R20">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Kobayashi</surname><given-names>H</given-names></name>, <name><surname>Solak</surname><given-names>AC</given-names></name>, <name><surname>Batson</surname><given-names>J</given-names></name> &amp; <name><surname>Royer</surname><given-names>LA</given-names></name>
<article-title>Image Deconvolution via Noise-Tolerant Self-Supervised Inversion</article-title>. <source>arXiv preprint arXiv:2006.06156</source> (<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="R21">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Horstmann</surname><given-names>H</given-names></name>, <name><surname>Korber</surname><given-names>C</given-names></name>, <name><surname>Satzler</surname><given-names>K</given-names></name>, <name><surname>Aydin</surname><given-names>D</given-names></name> &amp; <name><surname>Kuner</surname><given-names>T</given-names></name>. <article-title>Serial section scanning electron microscopy (S3EM) on silicon wafers for ultra-structural volume imaging of cells and tissues</article-title>. <source>PLoS One</source>
<volume>7</volume>, <comment>e35172</comment>, doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0035172</pub-id> (<year>2012</year>).</mixed-citation>
    </ref>
    <ref id="R22">
      <label>22</label>
      <mixed-citation publication-type="journal"><name><surname>Xu</surname><given-names>CS</given-names></name><etal/><article-title>Enhanced FIB-SEM systems for large-volume 3D imaging</article-title>. <source>Elife</source><volume>6</volume>, doi:<pub-id pub-id-type="doi">10.7554/eLife.25916</pub-id> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R23">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Denk</surname><given-names>W</given-names></name> &amp; <name><surname>Horstmann</surname><given-names>H</given-names></name>. <article-title>Serial block-face scanning electron microscopy to reconstruct three-dimensional tissue nanostructure</article-title>. <source>PLoS Biol</source>
<volume>2</volume>, <fpage>e329</fpage>, doi:<pub-id pub-id-type="doi">10.1371/journal.pbio.0020329</pub-id> (<year>2004</year>).<pub-id pub-id-type="pmid">15514700</pub-id></mixed-citation>
    </ref>
    <ref id="R24">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Kuwajima</surname><given-names>M</given-names></name>, <name><surname>Mendenhall</surname><given-names>JM</given-names></name>, <name><surname>Lindsey</surname><given-names>LF</given-names></name> &amp; <name><surname>Harris</surname><given-names>KM</given-names></name>
<article-title>Automated transmission-mode scanning electron microscopy (tSEM) for large volume analysis at nanoscale resolution</article-title>. <source>PLoS One</source>
<volume>8</volume>, <comment>e59573</comment>, doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0059573</pub-id> (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="R25">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Culley</surname><given-names>S</given-names></name><etal/><article-title>Quantitative mapping and minimization of super-resolution optical imaging artifacts</article-title>. <source>Nat Methods</source><volume>15</volume>, <fpage>263</fpage>–<lpage>266</lpage>, doi:<pub-id pub-id-type="doi">10.1038/nmeth.4605</pub-id> (<year>2018</year>).<pub-id pub-id-type="pmid">29457791</pub-id></mixed-citation>
    </ref>
    <ref id="R26">
      <label>26</label>
      <mixed-citation publication-type="book"><name><surname>Dabov</surname><given-names>K</given-names></name>, <name><surname>Foi</surname><given-names>A</given-names></name>, <name><surname>Katkovnik</surname><given-names>V</given-names></name> &amp; <name><surname>Egiazarian</surname><given-names>K</given-names></name>
<source>in Image Processing: Algorithms and Systems, Neural Networks, and Machine Learning</source>. <comment>606414</comment> (<publisher-name>International Society for Optics and Photonics</publisher-name>).</mixed-citation>
    </ref>
    <ref id="R27">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>von Chamier</surname><given-names>L</given-names></name>, <name><surname>Laine</surname><given-names>RF</given-names></name> &amp; <name><surname>Henriques</surname><given-names>R</given-names></name>. <article-title>Artificial intelligence for microscopy: what you should know</article-title>. <source>Biochem Soc Trans</source>, doi:<pub-id pub-id-type="doi">10.1042/BST20180391</pub-id> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="R28">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Belthangady</surname><given-names>C</given-names></name> &amp; <name><surname>Royer</surname><given-names>LA</given-names></name>
<article-title>Applications, promises, and pitfalls of deep learning for fluorescence image reconstruction</article-title>. <source>Nat Methods</source>, doi:<pub-id pub-id-type="doi">10.1038/s41592-019-0458-z</pub-id> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="R29">
      <label>29</label>
      <mixed-citation publication-type="journal"><article-title>Phototoxicity revisited</article-title>. <source>Nat Methods</source><volume>15</volume>, <fpage>751</fpage>, doi:<pub-id pub-id-type="doi">10.1038/s41592-018-0170-4</pub-id> (<year>2018</year>).<pub-id pub-id-type="pmid">30275585</pub-id></mixed-citation>
    </ref>
    <ref id="R30">
      <label>30</label>
      <mixed-citation publication-type="journal"><name><surname>Jonkman</surname><given-names>J</given-names></name> &amp; <name><surname>Brown</surname><given-names>CM</given-names></name>
<article-title>Any Way You Slice It-A Comparison of Confocal Microscopy Techniques</article-title>. <source>J Biomol Tech</source>
<volume>26</volume>, <fpage>54</fpage>–<lpage>65</lpage>, doi:<pub-id pub-id-type="doi">10.7171/jbt.15-2602-003</pub-id> (<year>2015</year>).<pub-id pub-id-type="pmid">25802490</pub-id></mixed-citation>
    </ref>
    <ref id="R31">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>Kner</surname><given-names>P</given-names></name>, <name><surname>Chhun</surname><given-names>BB</given-names></name>, <name><surname>Griffis</surname><given-names>ER</given-names></name>, <name><surname>Winoto</surname><given-names>L</given-names></name> &amp; <name><surname>Gustafsson</surname><given-names>MG</given-names></name>
<article-title>Super-resolution video microscopy of live cells by structured illumination</article-title>. <source>Nat Methods</source>
<volume>6</volume>, <fpage>339</fpage>–<lpage>342</lpage>, doi:<pub-id pub-id-type="doi">10.1038/nmeth.1324</pub-id> (<year>2009</year>).<pub-id pub-id-type="pmid">19404253</pub-id></mixed-citation>
    </ref>
    <ref id="R32">
      <label>32</label>
      <mixed-citation publication-type="journal"><name><surname>Wronski</surname><given-names>B</given-names></name><etal/><article-title>Handheld Multi-Frame Super-Resolution</article-title>. <source>arXiv preprint arXiv:1905.03277</source> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="R33">
      <label>33</label>
      <mixed-citation publication-type="journal"><name><surname>Huang</surname><given-names>X</given-names></name><etal/><article-title>Fast, long-term, super-resolution imaging with Hessian structured illumination microscopy</article-title>. <source>Nat Biotechnol</source><volume>36</volume>, <fpage>451</fpage>–<lpage>459</lpage>, doi:<pub-id pub-id-type="doi">10.1038/nbt.4115</pub-id> (<year>2018</year>).<pub-id pub-id-type="pmid">29644998</pub-id></mixed-citation>
    </ref>
    <ref id="R34">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>Carlton</surname><given-names>PM</given-names></name><etal/><article-title>Fast live simultaneous multiwavelength four-dimensional optical microscopy</article-title>. <source>Proc Natl Acad Sci U S A</source><volume>107</volume>, <comment>16016–16022</comment>, doi:<pub-id pub-id-type="doi">10.1073/pnas.1004037107</pub-id> (<year>2010</year>).</mixed-citation>
    </ref>
    <ref id="R35">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Arigovindan</surname><given-names>M</given-names></name><etal/><article-title>High-resolution restoration of 3D structures from widefield images with extreme low signal-to-noise-ratio</article-title>. <source>Proc Natl Acad Sci U S A</source><volume>110</volume>, <comment>17344–17349</comment>, doi:<pub-id pub-id-type="doi">10.1073/pnas.1315675110</pub-id> (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="R36">
      <label>36</label>
      <mixed-citation publication-type="journal"><name><surname>Barbastathis</surname><given-names>G</given-names></name>, <name><surname>Ozcan</surname><given-names>A</given-names></name> &amp; <name><surname>Situ</surname><given-names>G</given-names></name>. <article-title>On the use of deep learning for computational imaging</article-title>. <source>Optica</source>
<volume>6</volume>, <fpage>921</fpage>–<lpage>943</lpage>, doi:<pub-id pub-id-type="doi">10.1364/OPTICA.6.000921</pub-id> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="R37">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Keren</surname><given-names>L</given-names></name><etal/><article-title>MIBI-TOF: A multiplexed imaging platform relates cellular phenotypes and tissue structure</article-title>. <source>Sci Adv</source><volume>5</volume>, <comment>eaax5851</comment>, doi:<pub-id pub-id-type="doi">10.1126/sciadv.aax5851</pub-id> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="R38">
      <label>38</label>
      <mixed-citation publication-type="journal"><name><surname>Arrojo</surname><given-names>EDR</given-names></name><etal/><article-title>Age Mosaicism across Multiple Scales in Adult Tissues</article-title>. <source>Cell Metab</source><volume>30</volume>, <fpage>343</fpage>–<lpage>351</lpage><comment>e343</comment>, doi:<pub-id pub-id-type="doi">10.1016/j.cmet.2019.05.010</pub-id> (<year>2019</year>).<pub-id pub-id-type="pmid">31178361</pub-id></mixed-citation>
    </ref>
    <ref id="R39">
      <label>39</label>
      <mixed-citation publication-type="journal"><name><surname>Wolf</surname><given-names>SG</given-names></name> &amp; <name><surname>Elbaum</surname><given-names>M</given-names></name>. <article-title>CryoSTEM tomography in biology</article-title>. <source>Methods Cell Biol</source>
<volume>152</volume>, <fpage>197</fpage>–<lpage>215</lpage>, doi:<pub-id pub-id-type="doi">10.1016/bs.mcb.2019.04.001</pub-id> (<year>2019</year>).<pub-id pub-id-type="pmid">31326021</pub-id></mixed-citation>
    </ref>
  </ref-list>
  <ref-list>
    <title>Methods-only References</title>
    <ref id="R40">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Perez</surname><given-names>L</given-names></name> &amp; <name><surname>Wang</surname><given-names>J</given-names></name>. <article-title>The effectiveness of data augmentation in image classification using deep learning</article-title>. <source>arXiv preprint arXiv:1712.04621</source> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R41">
      <label>41</label>
      <mixed-citation publication-type="confproc"><name><surname>Ronneberger</surname><given-names>O</given-names></name>, <name><surname>Fischer</surname><given-names>P</given-names></name> &amp; <name><surname>Brox</surname><given-names>T</given-names></name> in <conf-name>International Conference on Medical image computing and computer-assisted intervention</conf-name>. <fpage>234</fpage>–<lpage>241</lpage> (<publisher-name>Springer</publisher-name>).</mixed-citation>
    </ref>
    <ref id="R42">
      <label>42</label>
      <mixed-citation publication-type="confproc"><name><surname>Shi</surname><given-names>W</given-names></name><etal/> in <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>. <fpage>1874</fpage>–<lpage>1883</lpage>.</mixed-citation>
    </ref>
    <ref id="R43">
      <label>43</label>
      <mixed-citation publication-type="confproc"><name><surname>Harada</surname><given-names>Y</given-names></name>, <name><surname>Muramatsu</surname><given-names>S</given-names></name> &amp; <name><surname>Kiya</surname><given-names>H</given-names></name> in <conf-name>9th European Signal Processing Conference (EUSIPCO 1998)</conf-name>. <fpage>1</fpage>–<lpage>4</lpage> (<publisher-name>IEEE</publisher-name>).</mixed-citation>
    </ref>
    <ref id="R44">
      <label>44</label>
      <mixed-citation publication-type="confproc"><name><surname>Sugawara</surname><given-names>Y</given-names></name>, <name><surname>Shiota</surname><given-names>S</given-names></name> &amp; <name><surname>Kiya</surname><given-names>H</given-names></name> in <conf-name>2018 25th IEEE International Conference on Image Processing (ICIP)</conf-name>. <fpage>66</fpage>–<lpage>70</lpage> (<publisher-name>IEEE</publisher-name>).</mixed-citation>
    </ref>
    <ref id="R45">
      <label>45</label>
      <mixed-citation publication-type="journal"><name><surname>Aitken</surname><given-names>A</given-names></name><etal/><article-title>Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize convolution and convolution resize</article-title>. <source>arXiv preprint arXiv:1707.02937</source> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="R46">
      <label>46</label>
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Goodfellow</surname><given-names>I</given-names></name>, <name><surname>Metaxas</surname><given-names>D</given-names></name> &amp; <name><surname>Odena</surname><given-names>A</given-names></name>. <article-title>Self-attention generative adversarial networks</article-title>. <source>arXiv preprint arXiv:1805.08318</source> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R47">
      <label>47</label>
      <mixed-citation publication-type="journal"><name><surname>Loshchilov</surname><given-names>I</given-names></name> &amp; <name><surname>Hutter</surname><given-names>F</given-names></name>. <article-title>Sgdr: Stochastic gradient descent with warm restarts</article-title>. <source>arXiv preprint arXiv:1608.03983</source> (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="R48">
      <label>48</label>
      <mixed-citation publication-type="confproc"><name><surname>Smith</surname><given-names>LN</given-names></name> in <conf-name>2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</conf-name>. <fpage>464</fpage>–<lpage>472</lpage> (<publisher-name>IEEE</publisher-name>).</mixed-citation>
    </ref>
    <ref id="R49">
      <label>49</label>
      <mixed-citation publication-type="journal"><name><surname>Smith</surname><given-names>LN</given-names></name><article-title>A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay</article-title>. <source>arXiv preprint arXiv:1803.09820</source> (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R50">
      <label>50</label>
      <mixed-citation publication-type="journal"><name><surname>Smith</surname><given-names>LN</given-names></name> &amp; <name><surname>Topin</surname><given-names>N</given-names></name>
<source>Super-convergence: Very fast training of residual networks using large learning rates</source>. (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="R51">
      <label>51</label>
      <mixed-citation publication-type="confproc"><name><surname>Hore</surname><given-names>A</given-names></name> &amp; <name><surname>Ziou</surname><given-names>D</given-names></name> in <conf-name>2010 20th International Conference on Pattern Recognition</conf-name>. <fpage>2366</fpage>–<lpage>2369</lpage> (<publisher-name>IEEE</publisher-name>).</mixed-citation>
    </ref>
    <ref id="R52">
      <label>52</label>
      <mixed-citation publication-type="journal"><name><surname>Kuwajima</surname><given-names>M</given-names></name>, <name><surname>Mendenhall</surname><given-names>JM</given-names></name> &amp; <name><surname>Harris</surname><given-names>KM</given-names></name>
<article-title>Large-volume reconstruction of brain tissue from high-resolution serial section images acquired by SEM-based scanning transmission electron microscopy</article-title>. <source>Methods Mol Biol</source>
<volume>950</volume>, <fpage>253</fpage>–<lpage>273</lpage>, doi:<pub-id pub-id-type="doi">10.1007/978-1-62703-137-0_15</pub-id> (<year>2013</year>).<pub-id pub-id-type="pmid">23086880</pub-id></mixed-citation>
    </ref>
    <ref id="R53">
      <label>53</label>
      <mixed-citation publication-type="journal"><name><surname>Deerinck</surname><given-names>TJ</given-names></name>, <name><surname>Bushong</surname><given-names>EA</given-names></name>, <name><surname>Thor</surname><given-names>A</given-names></name> &amp; <name><surname>Ellisman</surname><given-names>MH</given-names></name>
<article-title>NCMIR methods for 3D EM: a new protocol for preparation of biological specimens for serial block face scanning electron microscopy</article-title>. <source>Microscopy</source>, <fpage>6</fpage>–<lpage>8</lpage> (<year>2010</year>).</mixed-citation>
    </ref>
    <ref id="R54">
      <label>54</label>
      <mixed-citation publication-type="journal"><name><surname>Takemura</surname><given-names>SY</given-names></name><etal/><article-title>Synaptic circuits and their variations within different columns in the visual system of Drosophila</article-title>. <source>Proc Natl Acad Sci U S A</source><volume>112</volume>, <comment>13711–13716</comment>, doi:<pub-id pub-id-type="doi">10.1073/pnas.1509820112</pub-id> (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="R55">
      <label>55</label>
      <mixed-citation publication-type="journal"><name><surname>Kubota</surname><given-names>Y</given-names></name><etal/><article-title>A carbon nanotube tape for serial-section electron microscopy of brain ultrastructure</article-title>. <source>Nat Commun</source><volume>9</volume>, <fpage>437</fpage>, doi:<pub-id pub-id-type="doi">10.1038/s41467-017-02768-7</pub-id> (<year>2018</year>).<pub-id pub-id-type="pmid">29382816</pub-id></mixed-citation>
    </ref>
    <ref id="R56">
      <label>56</label>
      <mixed-citation publication-type="journal"><name><surname>Lowe</surname><given-names>DG</given-names></name><article-title>Distinctive Image Features from Scale-Invariant Keypoints</article-title>. <source>International Journal of Computer Vision</source><volume>60</volume>, <fpage>91</fpage>–<lpage>110</lpage>, doi:<pub-id pub-id-type="doi">10.1023/b:Visi.0000029664.99615.94</pub-id> (<year>2004</year>).</mixed-citation>
    </ref>
    <ref id="R57">
      <label>57</label>
      <mixed-citation publication-type="journal"><name><surname>Pekkurnaz</surname><given-names>G</given-names></name>, <name><surname>Trinidad</surname><given-names>JC</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Kong</surname><given-names>D</given-names></name> &amp; <name><surname>Schwarz</surname><given-names>TL</given-names></name>
<article-title>Glucose regulates mitochondrial motility via Milton modification by O-GlcNAc transferase</article-title>. <source>Cell</source>
<volume>158</volume>, <fpage>54</fpage>–<lpage>68</lpage>, doi:<pub-id pub-id-type="doi">10.1016/j.cell.2014.06.007</pub-id> (<year>2014</year>).<pub-id pub-id-type="pmid">24995978</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="F1" orientation="portrait" position="float">
    <label>Fig. 1 |</label>
    <caption>
      <title>Evaluation of crappifiers with different noise injection on EM data.</title>
      <p id="P110"><bold>a</bold>, Different crappifiers applied to high resolution, high SNR images, including “No noise” (no added noise, downsampled pixel size only), “Poisson”, “Gaussian”, and “Additive Gaussian” noise. The real-world acquired low- (LR acquired) and high-resolution (Ground Truth) images are also shown for comparison. Each training set contains 40 image pairs, achieving similar results. <bold>b</bold>, Visualized restoration performance of PSSR models that were trained on each different crappifier (No noise, Poisson, Gaussian, and Additive Gaussian), as well as a model trained with manually acquired low-resolution versions of the same samples used for the high-resolution semi-synthetic training data (“Real-world Training Data”). Results from a model that using the same crappifier as “Additive Gaussian”, but with ~80x more training data (“Additive Gaussian (~80x)”) are also displayed. LR input and Ground truth of the example testing ROI are also shown. Experiments were repeated with 8–16 images, achieving similar results. PSNR (<bold>c</bold>), SSIM (<bold>d</bold>) and resolution as measured by Fourier Ring Correlation analysis (FRC) (<bold>e</bold>) (PSNR and SSIM, n = 8 independent images; FRC resolution, n = 16 independent images). <bold>f</bold>, Shown is a table that compares the devoted time, cost and difficulty level between experiments with manually acquired training pairs and experiments using our crappification method. All values are shown as mean ± SEM. ns = not significant. P values are specified in the figure for 0.0001&lt;p&lt;0.05. *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001, ****p&lt;0.0001, ns = not significant; Two-sided paired <italic>t</italic>-test.</p>
    </caption>
    <graphic xlink:href="nihms-1668081-f0001"/>
  </fig>
  <fig id="F2" orientation="portrait" position="float">
    <label>Fig. 2 |</label>
    <caption>
      <title>Restoration of semi-synthetic and real-world EM testing data using PSSR model trained on semi-synthetically generated training pairs.</title>
      <p id="P111"><bold>a</bold>, Overview of the general workflow. Training pairs were semi-synthetically created by applying a degrading function to the HR images taken from a scanning electron microscope in transmission mode (tSEM) to generate LR counterparts (left column). Semi-synthetic pairs were used as training data through a dynamic ResNet-based U-Net architecture. Layers of the same xy size are in the same color (middle column). Real-world LR and HR image pairs were both manually acquired under a SEM (right column). The output from PSSR (LR-PSSR) when LR is served as input is then compared to HR to evaluate the performance of our trained model. <bold>b</bold>, Restoration performance on semi-synthetic testing pairs from tSEM. Shown is the same field of view of a representative bouton region from the synthetically created LR input with the pixel size of 8nm (left column), a 16x bilinear upsampled image with 2nm pixel size (second column), 16x PSSR upsampled result with 2nm pixel size (third column) and the HR ground truth acquired at the microscope with the pixel size of 2nm (fourth column). A close view of the same vesicle in each image is highlighted. The Peak-Signal-to-Noise-Ratio (PSNR) and the Structural Similarity (SSIM) quantification of the semi-synthetic testing sets are shown (right) (n = 66 independent images). <bold>c</bold>, Restoration results of manually acquired SEM testing pairs. Shown is the comparison of the LR input acquired at the microscope with a pixel size of 8nm (left column), 16x bilinear upsampled image (second column), 16x PSSR upsampled output (third column) and the HR ground truth acquired at the microscope with a pixel size of 2nm (fourth column). Bottom row compares the enlarged region of a presynaptic bouton with one vesicle highlighted in the inset. Graphs comparing PSNR, SSIM and image resolution are also displayed (right). The PSNR and SSIM values were calculated between an upsampled result and its corresponding HR ground truth (n = 42 independent images). Resolution was calculated with the Fourier Ring Correlation (FRC) plugin in NanoJ-SQUIRREL by acquiring two independent images at low and high-resolution (n = 16 independent images). All values are shown as mean ± SEM. P values are specified in the figure for 0.0001&lt;p&lt;0.05. *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001, ****p&lt;0.0001, ns = not significant; Two-sided paired <italic>t</italic>-test.</p>
    </caption>
    <graphic xlink:href="nihms-1668081-f0002"/>
  </fig>
  <fig id="F3" orientation="portrait" position="float">
    <label>Fig. 3 |</label>
    <caption>
      <title>PSSR model is effective for multiple EM modalities and sample types.</title>
      <p id="P112">Shown are representative low-resolution (LR), bilinear interpolated (LR-Bilinear) and PSSR-restored (LR-PSSR) images from mouse brain sections (n = 75 sections in one image stack, xy dimension 240×240 pixels) imaged with a ZEISS Sigma-VP Gatan Serial Blockface SEM system (<bold>a</bold>), fly sections (n = 50 sections in one image stack, xy dimension 1250×1250 pixels) acquired with ZEISS/FEI focused ion beam-SEM (FIB-SEM) (<bold>b</bold>), mouse sections (n = 563 sections in one image stack, xy dimension 240×240 pixels) from ZEISS/FEI FIB-SEM (<bold>c</bold>) and rat sections (one montage, xy dimension 2048×1024 pixels) imaged with a Hitachi Regulus serial section EM (ssEM) (<bold>d</bold>). <bold>e</bold>, Validation of pre-synaptic vesicle detection. LR, LR-Bilinear, LR-PSSR, and ground truth high-resolution (HR) images of a representative bouton region as well as their color-labeled vesicle counts are shown. Vesicles colored with red represents false negatives, blue are false positives, and white are true positives. The percentage of each error type is shown in the pie chart. Docked vesicles were labelled with purple dots. Vesicle counts from two humans were plotted (Dashed line: Human-1, solid line: Human-2), with the average total error ± SEM. displayed above. Experiments were conducted with n=10 independent bouton regions in all conditions, achieving similar results. The linear regression between LR-Bilinear and HR, LR-PSSR and HR, and two human counters of HR are shown in the third row. The equation for the linear regression, the Goodness-of-Fit (<italic>R</italic><sup><italic>2</italic></sup>) and the <italic>p</italic>-value (<italic>p</italic>) of each graph are displayed. Scale bars = 1.5μm.</p>
    </caption>
    <graphic xlink:href="nihms-1668081-f0003"/>
  </fig>
  <fig id="F4" orientation="portrait" position="float">
    <label>Fig. 4 |</label>
    <caption>
      <title>Multi-frame PSSR timelapses of mitochondrial dynamics.</title>
      <p id="P113"><bold>a</bold>, Overview of multi-frame PSSR training data generation method. Five consecutive frames (<inline-formula><mml:math display="inline" id="M37"><mml:msub><mml:mrow><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>) from a HR Airyscan time-lapse movie were synthetically crappified to five LR images (<inline-formula><mml:math display="inline" id="M38"><mml:msub><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>), which together with the HR middle frame at time <inline-formula><mml:math display="inline" id="M39"><mml:mi>t</mml:mi><mml:mi> </mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>, form a five-to-one training “pair”. <bold>b</bold>, Temporal consistency analysis. Neighboring-frame cross-correlation coefficient (<inline-formula><mml:math display="inline" id="M40"><mml:mi>∅</mml:mi><mml:mi> </mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> that corresponds to frame <inline-formula><mml:math display="inline" id="M41"><mml:mi>τ</mml:mi></mml:math></inline-formula> in <inline-formula><mml:math display="inline" id="M42"><mml:mi>x</mml:mi></mml:math></inline-formula>-axis denotes the correlation coefficient of frame <inline-formula><mml:math display="inline" id="M43"><mml:mi>τ</mml:mi><mml:mi> </mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and frame <inline-formula><mml:math display="inline" id="M44"><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> (Left). Absolute error against HR (<inline-formula><mml:math display="inline" id="M45"><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mo>∆</mml:mo><mml:mi>∅</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>) for each condition was compared (<inline-formula><mml:math display="inline" id="M46"><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mo>∆</mml:mo><mml:mi>∅</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>∅</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mi>∅</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math></inline-formula>, right). n = 6 independent timelapses with n = 80–120 timepoints each. Colored shades show standard error. The * sign above LR-PSSR-MF denotes that LR-PSSR-MF is significantly more consistent with HR than all other conditions (P&lt;0.0001). All violin plots show lines at the median and quartiles. <bold>c</bold>, Examples of false mitochondrial network merges (white boxes) due to the severe flickering artifacts in single-frame models (LR-Bilinear, LR-CARE and LR-PSSR-SF), and loss of temporal consistency and resolution (yellow boxes) in models post-processed with a “rolling frame averaging” method (LR-CARE-RA and LR-PSSR-SF-RA). Two consecutive frames of an example region from semi-synthetic acquired low-resolution (LR), bilinearly upsampled (LR-Bilinear), CARE (LR-CARE), 5-frame Rolling Average post-processed CARE output (LR-CARE-RA), single-frame PSSR (LR-PSSR-SF), single-frame PSSR post-processed with a 5-frame Rolling Average (LR-PSSR-RA), 5-frame multi-frame PSSR (LR-PSSR-MF), and ground truth high-resolution (HR-Airyscan) time-lapses are color coded in magenta (<inline-formula><mml:math display="inline" id="M47"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mi>s</mml:mi></mml:math></inline-formula>) and green (<inline-formula><mml:math display="inline" id="M48"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mi>s</mml:mi></mml:math></inline-formula>). Insets show the intensity line plot of the two frames drawn in the center of the white box in each condition. The yellow box shows an example of temporal resolution loss in RA conditions (LR-CARE-RA and LR-PSSR-SF-RA) only. Magenta pixels represent signal that only exists in the t=0s frame, but not in the t=5s, while green pixels represent signal present only in the t=5s frame. <bold>d</bold>, Restoration performance on semi-synthetic and real-world testing pairs. For the semi-synthetic pair, LR was synthetically generated from Airyscan HR movies. Enlarged ROIs show an example of well resolved mitochondrial structures by PSSR, agreeing with Airyscan ground truth images. Red and yellow arrowheads show two false connecting points in LR-Bilinear and LR-PSSR-SF, which were well separated in LR-PSSR-MF. In the real-world example, green arrowheads in the enlarged ROIs highlight a well restored gap between two mitochondria segments in the LR-PSSR-MF output. Normalized line-plot cross-section profile (yellow) highlights false bridging between two neighboring structures in LR-Bilinear and LR-PSSR-SF, which was well separated with our PSSR-MF model. Signal-to-Noise Ratio (SNR) measured using the images in both semi-synthetic and real-world examples are indicated. <bold>e</bold>, PSSR output captured a transient mitochondrial fission event. Shown is a PSSR-restored dynamic mitochondrial fission event, with three key time frames displayed. Arrows highlight the mitochondrial fission site. <bold>f</bold>, PSNR and SSIM quantification of the semi-synthetic (n = 8 independent timelapses with n = 80–120 timepoints each) as well as the real-world (n = 10 independent timelapses of fixed samples with n=10 timepoints each) testing sets discussed in (d). FRC values measured using two independent low- versus high- resolution acquisitions from multiple cells are indicated (n = 10). <bold>g,</bold> Validation of fission event captures using semi-synthetic data. An example of a fission event that was detectable in LR-PSSR but not LR-Bilinear. Experiments were repeated with 8 timelapses, achieving similar results. <bold>h,</bold> For fission event detection, the number of false positives, false negatives, and true positives detected by expert humans was quantified for 8 different timelapses. Distribution was shown in the pie charts. Fission event counts from two humans were plotted (Dashed line: Human-1, solid line: Human-2). <bold>i-k,</bold> Linear regression between LR-Bilinear and HR, LR-PSSR and HR, and two human counters of HR are shown (n = 8 independent timelapses with n = 80–120 timepoints each). The linear regression equation, the Goodness-of-Fit (R<sup>2</sup>) and the p-value of each graph are displayed. All values are shown as mean ± SEM. P values are specified in the figure for 0.0001&lt;p&lt;0.05. *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001, ****p&lt;0.0001, ns = not significant; Two-sided paired <italic>t</italic>-test.</p>
    </caption>
    <graphic xlink:href="nihms-1668081-f0004"/>
  </fig>
  <fig id="F5" orientation="portrait" position="float">
    <label>Fig. 5 |</label>
    <caption>
      <title>Spatiotemporal analysis of mitochondrial motility in neurons.</title>
      <p id="P114">PSSR facilitates high spatiotemporal resolution imaging of mitochondrial motility in neurons. <bold>a</bold>, Comparison of PSSR results (LR-PSSR) versus bilinear interpolation (LR-Bilinear) on semi-synthetic (n = 7 independent timelapse movies with n=100 independent time points each) and real-world testing pairs (n = 6 independent timelapse movies with n=12 independent time points each). Enlarged ROIs from representative images show PSSR resolved two mitochondria in both semi-synthetic and real-world testing sets, quantified by normalized line plot cross-section profiles. SNR <bold>b</bold>, PSNR (top) and SSIM (middle) quantification of the datasets in (a). FRC resolution measured from two independent acquisitions of the real-world overview dataset discussed in (a) is indicated (bottom). <bold>c</bold>, PSSR restoration of LR timelapses resolves mitochondria moving past one another in a neuronal process (arrows indicate direction of movement). <bold>d,</bold> Representative kymographs of mitochondrial motility in hippocampal neurons transfected with Mito-DsRed (n = 7 independent LR timelapse movies processed to LR-PSSR). First frame of each time-lapse movie is shown above each kymograph. Different color arrowheads indicate mitochondria going through fission and fusion events. Each color represents a different mitochondrion. <bold>e,</bold> Enlarged areas of (d), capturing mitochondrial fission and fusion events in real-time. <bold>f-i,</bold> Mitochondrial motility was quantified from time-lapse movies as demonstrated in <xref rid="SD19" ref-type="supplementary-material">Supplementary Video 8</xref>. For each mitochondrial trajectory the total distance mitochondria travelled (f), mitochondrial velocity (g), percent time mitochondria spent in motion (h) and in pause (i) was quantified (n = 76 – 216 mitochondria from 4 neurons and 3 independent experiments). All values are shown as mean ± SEM. P values are specified in the figure for 0.0001&lt;p&lt;0.05. *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001, ****p&lt;0.0001, ns = not significant; Two-sided paired <italic>t</italic>-test (b) and Kruskal-Wallis test followed by Dunn’s multiple comparison test (f-i).</p>
    </caption>
    <graphic xlink:href="nihms-1668081-f0005"/>
  </fig>
  <table-wrap id="T1" position="float" orientation="portrait">
    <label>Table 1:</label>
    <caption>
      <p id="P115">Details of fluorescence PSSR training experiments.</p>
    </caption>
    <table frame="box" rules="all">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="bottom" rowspan="1" colspan="1">Experiment</th>
          <th align="center" valign="top" rowspan="1" colspan="1">U2OS MitoTracker PSSR-SF</th>
          <th align="center" valign="top" rowspan="1" colspan="1">U2OS MitoTracker PSSR-MF</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Neuron Mito-dsRed PSSR-MF</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Input size (x, y, z)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(128, 128, 1)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(128, 128, 5)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(128, 128, 5)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Output size (x, y, z)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(512, 512, 1)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(512, 512, 1)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">(512, 512, 1)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">No. of image pairs for training</td>
          <td align="left" valign="top" rowspan="1" colspan="1">5000</td>
          <td align="left" valign="top" rowspan="1" colspan="1">5000</td>
          <td align="left" valign="top" rowspan="1" colspan="1">3000</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">No. of image pairs for validation</td>
          <td align="left" valign="top" rowspan="1" colspan="1">200</td>
          <td align="left" valign="top" rowspan="1" colspan="1">200</td>
          <td align="left" valign="top" rowspan="1" colspan="1">300</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">No. of GPUs</td>
          <td align="left" valign="top" rowspan="1" colspan="1">2</td>
          <td align="left" valign="top" rowspan="1" colspan="1">2</td>
          <td align="left" valign="top" rowspan="1" colspan="1">2</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Training datasource size (GB)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">9.4</td>
          <td align="left" valign="top" rowspan="1" colspan="1">9.4</td>
          <td align="left" valign="top" rowspan="1" colspan="1">9.5</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Validation datasource size (GB)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.44</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.44</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1.8</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Training dataset size (GB)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1.38</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1.7</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1.03</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Validation dataset size (GB)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.06</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.07</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.1</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Batch size per GPU</td>
          <td align="left" valign="top" rowspan="1" colspan="1">8</td>
          <td align="left" valign="top" rowspan="1" colspan="1">8</td>
          <td align="left" valign="top" rowspan="1" colspan="1">6</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">No. of epochs</td>
          <td align="left" valign="top" rowspan="1" colspan="1">100</td>
          <td align="left" valign="top" rowspan="1" colspan="1">100</td>
          <td align="left" valign="top" rowspan="1" colspan="1">50</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Training time (h)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">3.76</td>
          <td align="left" valign="top" rowspan="1" colspan="1">3.77</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1.25</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Learning rate</td>
          <td align="left" valign="top" rowspan="1" colspan="1">4e-4</td>
          <td align="left" valign="top" rowspan="1" colspan="1">4e-4</td>
          <td align="left" valign="top" rowspan="1" colspan="1">4e-4</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Best model found at epoch</td>
          <td align="left" valign="top" rowspan="1" colspan="1">33</td>
          <td align="left" valign="top" rowspan="1" colspan="1">86</td>
          <td align="left" valign="top" rowspan="1" colspan="1">39</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Normalized to ImageNet statistics?</td>
          <td align="left" valign="top" rowspan="1" colspan="1">No</td>
          <td align="left" valign="top" rowspan="1" colspan="1">No</td>
          <td align="left" valign="top" rowspan="1" colspan="1">No</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">ResNet size</td>
          <td align="left" valign="top" rowspan="1" colspan="1">ResNet34</td>
          <td align="left" valign="top" rowspan="1" colspan="1">ResNet34</td>
          <td align="left" valign="top" rowspan="1" colspan="1">ResNet34</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Loss function</td>
          <td align="left" valign="top" rowspan="1" colspan="1">MSE</td>
          <td align="left" valign="top" rowspan="1" colspan="1">MSE</td>
          <td align="left" valign="top" rowspan="1" colspan="1">MSE</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T2" position="float" orientation="portrait">
    <label>Table 2:</label>
    <caption>
      <p id="P116">Details of fluorescence PSSR testing data for PSNR, SSIM, and error-mapping.</p>
    </caption>
    <table frame="box" rules="all">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th rowspan="3" align="left" valign="top" colspan="1"/>
          <th colspan="3" align="center" valign="top" rowspan="1">U2OS MitoTracker</th>
          <th colspan="3" align="center" valign="top" rowspan="1">Neuron Mito-dsRed</th>
        </tr>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Semi-synthetic</th>
          <th colspan="2" align="left" valign="top" rowspan="1">Real-world</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Semi-synthetic</th>
          <th colspan="2" align="left" valign="top" rowspan="1">Real-world</th>
        </tr>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">HR</th>
          <th align="left" valign="top" rowspan="1" colspan="1">LR</th>
          <th align="left" valign="top" rowspan="1" colspan="1">HR</th>
          <th align="left" valign="top" rowspan="1" colspan="1">HR</th>
          <th align="left" valign="top" rowspan="1" colspan="1">LR</th>
          <th align="left" valign="top" rowspan="1" colspan="1">HR</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Microscopy</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Airyscan</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Confocal</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Airyscan</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Airyscan</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Confocal</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Airyscan</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Laser power (μW)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">35</td>
          <td align="left" valign="top" rowspan="1" colspan="1">7</td>
          <td align="left" valign="top" rowspan="1" colspan="1">28</td>
          <td align="left" valign="top" rowspan="1" colspan="1">82</td>
          <td align="left" valign="top" rowspan="1" colspan="1">11</td>
          <td align="left" valign="top" rowspan="1" colspan="1">55</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Data source size (MB)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1250</td>
          <td align="left" valign="top" rowspan="1" colspan="1">7.15</td>
          <td align="left" valign="top" rowspan="1" colspan="1">200</td>
          <td align="left" valign="top" rowspan="1" colspan="1">5920</td>
          <td align="left" valign="top" rowspan="1" colspan="1">10.7</td>
          <td align="left" valign="top" rowspan="1" colspan="1">305</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Data set size (MB)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">325</td>
          <td align="left" valign="top" rowspan="1" colspan="1">6.39</td>
          <td align="left" valign="top" rowspan="1" colspan="1">191</td>
          <td align="left" valign="top" rowspan="1" colspan="1">1970</td>
          <td align="left" valign="top" rowspan="1" colspan="1">10.4</td>
          <td align="left" valign="top" rowspan="1" colspan="1">305</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Total number of different cells</td>
          <td align="left" valign="top" rowspan="1" colspan="1">6</td>
          <td align="left" valign="top" rowspan="1" colspan="1">10</td>
          <td align="left" valign="top" rowspan="1" colspan="1">10</td>
          <td align="left" valign="top" rowspan="1" colspan="1">7</td>
          <td align="left" valign="top" rowspan="1" colspan="1">10</td>
          <td align="left" valign="top" rowspan="1" colspan="1">10</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T3" position="float" orientation="portrait">
    <label>Table 3:</label>
    <caption>
      <p id="P117">Details of the EM PSSR training experiment.</p>
    </caption>
    <table frame="box" rules="all">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th colspan="4" align="center" valign="top" rowspan="1">EM training data</th>
        </tr>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Round of progressive resizing</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">1</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">2</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">3</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Input size <inline-formula><mml:math display="inline" id="M49"><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula></td>
          <td align="center" valign="middle" rowspan="1" colspan="1">(32, 32, 3)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">(64, 64, 3)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">(128, 128, 3)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Output size <inline-formula><mml:math display="inline" id="M50"><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mi mathvariant="normal"> </mml:mi><mml:mi>z</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula></td>
          <td align="center" valign="middle" rowspan="1" colspan="1">(128,128,3)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">(256, 256, 3)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">(512, 512, 3)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Batch size per GPU</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">8</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">No. of epochs <inline-formula><mml:math display="inline" id="M51"><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal"> </mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math></inline-formula></td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[1,1]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[3,3]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[3,3]</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Learning rate<inline-formula><mml:math display="inline" id="M52"><mml:mi mathvariant="normal"> </mml:mi><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal"> </mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math></inline-formula></td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[1e-3, (1e-5,1e-3)]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[1e-3, (1e-5,1e-3)]</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">[1e-3, (1e-5,1e-4)]</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Training dataset size (GB)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.9</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">15.56</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">62.24</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Validation dataset size (GB)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.96</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.89</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">15.56</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Training datasource size (GB)</td>
          <td colspan="3" align="center" valign="middle" rowspan="1">105</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Validation datasource size (GB)</td>
          <td colspan="3" align="center" valign="middle" rowspan="1">26</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">No. of image pairs for training</td>
          <td colspan="3" align="center" valign="middle" rowspan="1">80,000</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">No. of image pairs for validation</td>
          <td colspan="3" align="center" valign="middle" rowspan="1">20,000</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Total training time (h)</td>
          <td colspan="3" align="center" valign="middle" rowspan="1">16</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Normalized to ImageNet statistics?</td>
          <td colspan="3" align="center" valign="middle" rowspan="1">Yes</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">ResNet size</td>
          <td colspan="3" align="center" valign="middle" rowspan="1">ResNet34</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Loss function</td>
          <td colspan="3" align="center" valign="middle" rowspan="1">MSE</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">No. of GPUs</td>
          <td colspan="3" align="center" valign="middle" rowspan="1">2</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
