<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9310413</article-id>
    <article-id pub-id-type="publisher-id">4825</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-022-04825-5</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SparkGC: Spark based genome compression for large collections of genomes</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Yao</surname>
          <given-names>Haichang</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hu</surname>
          <given-names>Guangyong</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Shangdong</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fang</surname>
          <given-names>Houzhi</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Ji</surname>
          <given-names>Yimu</given-names>
        </name>
        <address>
          <email>jiym@njupt.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <aff id="Aff1"><label>1</label>School of Computer and Software, Nanjing Vocational University of Industry Technology, Nanjing, 210023 China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.453246.2</institution-id><institution-id institution-id-type="ISNI">0000 0004 0369 3615</institution-id><institution>School of Computer Science, </institution><institution>Nanjing University of Posts and Telecommunications, </institution></institution-wrap>Nanjing, 210023 China </aff>
      <aff id="Aff3"><label>3</label>Jiangsu HPC and Intelligent Processing Engineer Research Center, Nanjing, 210003 China </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.453246.2</institution-id><institution-id institution-id-type="ISNI">0000 0004 0369 3615</institution-id><institution>Institute of High Performance Computing and Bigdata, </institution><institution>Nanjing University of Posts and Telecommunications, </institution></institution-wrap>Nanjing, 210023 China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>25</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>25</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>23</volume>
    <elocation-id>297</elocation-id>
    <history>
      <date date-type="received">
        <day>28</day>
        <month>3</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>6</day>
        <month>7</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Since the completion of the Human Genome Project at the turn of the century, there has been an unprecedented proliferation of sequencing data. One of the consequences is that it becomes extremely difficult to store, backup, and migrate enormous amount of genomic datasets, not to mention they continue to expand as the cost of sequencing decreases. Herein, a much more efficient and scalable program to perform genome compression is required urgently. In this manuscript, we propose a new Apache Spark based Genome Compression method called SparkGC that can run efficiently and cost-effectively on a scalable computational cluster to compress large collections of genomes. SparkGC uses Spark’s in-memory computation capabilities to reduce compression time by keeping data active in memory between the first-order and second-order compression. The evaluation shows that the compression ratio of SparkGC is better than the best state-of-the-art methods, at least better by 30%. The compression speed is also at least 3.8 times that of the best state-of-the-art methods on only one worker node and scales quite well with the number of nodes. SparkGC is of significant benefit to genomic data storage and transmission. The source code of SparkGC is publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/haichangyao/SparkGC">https://github.com/haichangyao/SparkGC</ext-link>.</p>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1186/s12859-022-04825-5.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Genome compression</kwd>
      <kwd>Reference-based compression</kwd>
      <kwd>Spark</kwd>
      <kwd>Distributed parallel</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Scientific Research Start-up Foundation of Nanjing Vocational University of Industry Technology</institution>
        </funding-source>
        <award-id>YK21-05-04</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Modern Educational Technology Research Program of Jiangsu Province in 2022</institution>
        </funding-source>
        <award-id>2022-R-98629</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Research Project of Chinese National Light Industry Vocational Education and Teaching Steering Committee in 2021</institution>
        </funding-source>
        <award-id>QGHZW2021066</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>the National Key R&amp;D Program of China</institution>
        </funding-source>
        <award-id>2018AAA0103300</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Genome plays an increasingly important role in human life, and genome technology has become a breakthrough in the treatment of new diseases [<xref ref-type="bibr" rid="CR1">1</xref>]. Based on this, the European Molecular Biology Laboratory (EMBL), Gen-Bank of American National Center for Biotechnology Information (NCBI), and DNA Data Bank of Japan (DDBJ) are updating the database information every day. Because the cost of genome sequencing is continuously reducing, while the efficiency is increasing, the growth of biological data is amazing [<xref ref-type="bibr" rid="CR2">2</xref>]. Such a huge amount of genomic data has posed great challenges to genomic data centers and genomic research institutions, such as in data storage, backup, migration, sharing, etc. [<xref ref-type="bibr" rid="CR3">3</xref>]. The compression of genomic data naturally becomes the best choice to resolve the challenge. Although the general-purpose compression method can also be applied to genomic data, they do not use the characteristics of genomic data, thereby their compression ratio is limited. No matter what general-purpose compression method is used, the compression ratio can only reach 7:1 at most, which cannot completely resolve the challenge [<xref ref-type="bibr" rid="CR4">4</xref>]. In recent years, researchers have proposed many special-purpose genome compression methods. Compared with the general-purpose compression methods, their compression ratio has been greatly improved [<xref ref-type="bibr" rid="CR5">5</xref>].</p>
    <p id="Par3">As a data compression method, the compression ratio is the first evaluated factor in most situations, so this is the direction that genome compression researchers have been working on. But in the face of big data, the compression time is gradually emerging to be an urgent problem for researchers to resolve [<xref ref-type="bibr" rid="CR6">6</xref>]. The compression method is always a trade-off between compression ratio and compression speed. The latest research results show that the compression ratio of pair-wise human genomes compressing has increased to an average of more than 300:1, but the compression time has also increased to more than 10 min per person [<xref ref-type="bibr" rid="CR7">7</xref>]. The time cost may be tolerable when compressing a small amount of genomic data, such as 100 or 1 K human genomes. However, in the scenario of data archiving or data migration in the genomic data centers, it is very common to compress 10K, 100K, or even 1 million human genomes. At this time, the compression time required by current genome compression methods becomes intolerable.</p>
    <p id="Par4">Another problem is that with the continuous increase of genomic data, cloud storage, the storage mode specially designed for big data, has gradually entered the bioinformatics community [<xref ref-type="bibr" rid="CR8">8</xref>]. Cloud storage enables users to use storage facilities on demand without the huge cost of building and maintaining expensive infrastructure. With recent services by Amazon and alike, it is possible to rent almost arbitrary configurations, which look logically as a single machine, but is in fact distributed. In this setup, when storing the genomic big data, the user does not need to care about using the cloud, since the infrastructure is hidden from the user. Although most genome compression methods can still be used in the cloud, they are almost single machine algorithms, which cannot fully utilize the computing power of distributed nodes. Developing a genome compression method that can be executed directly in distributed parallel systems has become a better solution [<xref ref-type="bibr" rid="CR9">9</xref>].</p>
    <p id="Par5">Another advantage of studying parallel and scalable genome compression algorithms is that they can implement more complex compression schemes with high space-time requirements. The key factors of referential genome compression algorithms have been developed from maximum exact match (MEM) search to the prediction or calculation of precise differences between sequences. Solving the differences between two sequences is a global optimization problem, and solving the differences among large collections of sequences is an approximate NP-hard problem. They both have high computational complexity [<xref ref-type="bibr" rid="CR6">6</xref>]. Distributed parallel algorithms can speed up these solvings that make the research of genome compression have more space.</p>
    <p id="Par6">In recent years, several distributed parallel frameworks have emerged to efficiently manage and process large datasets. The most popular of which are Hadoop [<xref ref-type="bibr" rid="CR10">10</xref>] and Spark [<xref ref-type="bibr" rid="CR11">11</xref>]. Both of them are open source big data frameworks of Apache. The core designs of the Hadoop framework are Hadoop Distributed File System (HDFS) and MapReduce. HDFS provides storage for massive data, while MapReduce provides the calculation for massive data. The MapReduce framework has a limitation on programmability though, as it requires the programmer to write code where the Map phase is always followed by the Reduce phase. Moreover, it saves intermediate data to the disk between Map phase and Reduce phase, which increases disk access overhead. Spark is a general parallel framework like MapReduce. It is open source by UC Berkeley AMP lab. Spark has the advantages of MapReduce and can also use HDFS as the distributed file system. But different from MapReduce, Spark allows programmers to perform many other transformations besides just Map and Reduce, while keeping data in memory between these transformations. These distributed parallel frameworks are different from the multi-core parallel schemes, which can be realized by modifying the code slightly. Only if the architectural details and the specific aspects of the considered framework are carefully taken into account for the algorithm design and implementation, genome compression can be developed well on these frameworks.</p>
    <p id="Par7">In this manuscript, we propose and implement a Spark based genome compression (SparkGC) method that allows running efficiently and cost-effectively on a scalable computational cluster to compress large collections of genomes. SparkGC utilizes Spark’s in-memory computation capabilities to improve the performance of genome compression. The contributions of this manuscript are summarized as follows:<list list-type="bullet"><list-item><p id="Par8">We proposed Spark based genome compression scheme for the first time. Although there are some genome compression methods based on distributed parallel computing, such as FastDRC [<xref ref-type="bibr" rid="CR12">12</xref>], they are all based on MapReduce framework. To our best knowledge, there is no large collections of genomes compression method based on Spark. Our method proved the feasibility of compressing large collections of genomes via Spark. Furthermore, our research results indicated that Spark is more suitable for the iterative compression of large collections of genomes.</p></list-item><list-item><p id="Par9">We designed and implemented SparkGC: a production-quality and highly scalable large collections of genomes compression method. SparkGC meticulously designs the RDD (Resilient Distributed Datasets) transformations to keep data active in memory among the whole compression process. SparkGC improves the compression speed for about 4 times on the cluster with only one worker node and scales excellently by increasing the number of worker nodes.</p></list-item><list-item><p id="Par10">We optimized the framework by using Kyro serialization and broadcast variables compression that enable SparkGC to compress 1100 human genomes on a common computer with just 24 GB of RAM. We optimized the encoding scheme for the mapping results that makes SparkGC achieve the best compression ratio among all the state-of-the-art compression methods.</p></list-item></list></p>
    <p id="Par11">The remainder of this manuscript is organized as follows. In “<xref rid="Sec2" ref-type="sec">Related works</xref>” section, we discuss the related works on genome compression and its parallelization. “<xref rid="Sec3" ref-type="sec">Methodology</xref>” section presents the methodology of SparkGC. This is followed by “<xref rid="Sec8" ref-type="sec">Results</xref>” section, which evaluates the performance of the proposed algorithms, including compression ratio, compression speed, scalability, robustness, and trade-off. We finally conclude the manuscript with “<xref rid="Sec14" ref-type="sec">Conclusions</xref>” section.</p>
  </sec>
  <sec id="Sec2">
    <title>Related works</title>
    <p id="Par12">DNAZip [<xref ref-type="bibr" rid="CR13">13</xref>] proposed in 2009 compressed James Watson’s 3 GB genome to 4 MB, so small, that it even can be sent by email attachment. The high compression ratio of the referential genome compression immediately aroused researchers’ interest, made more and more researchers focus on referential genome compression research and obtain many achievements. Table <xref rid="Tab1" ref-type="table">1</xref> summarises the related works of this paper. More works about genome compression can be referenced in review articles [<xref ref-type="bibr" rid="CR14">14</xref>–<xref ref-type="bibr" rid="CR16">16</xref>].<table-wrap id="Tab1"><label>Table 1</label><caption><p>Summary of the related works of this paper</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Year</th><th align="left">Name</th><th align="left">Methodology</th><th align="left">Characteristics</th><th align="left">Parallelization</th></tr></thead><tbody><tr><td align="left">2009</td><td align="left">DNAZip [<xref ref-type="bibr" rid="CR13">13</xref>]</td><td align="left">A serial of compression techniques (Variable integer (VINT), Delta positions (DELTA), SNP mapping (DBSNP), K-mer partitioning (KMER)) are taken together to reduce the size of a single genome</td><td align="left">The SNP database dbSNP [<xref ref-type="bibr" rid="CR24">24</xref>] and the mapping results between reference and target sequence have to be input as prerequisites, that limits its practicability</td><td align="left">Serial</td></tr><tr><td align="left">2012</td><td align="left">BlockCompression [<xref ref-type="bibr" rid="CR25">25</xref>]</td><td align="left">The reference and target sequence are divided into fixed-length blocks. Matching are performed between the blocks</td><td align="left"><p>Compressed suffix tree is employed to save memory.</p><p>Straightforward approximate matching is used to improve matching rate</p></td><td align="left">Block-processing can be distributed on several CPUs</td></tr><tr><td align="left">2013</td><td align="left">FRESCO [<xref ref-type="bibr" rid="CR17">17</xref>]</td><td align="left"><p>Suffix tree is used to index the reference sequence.</p><p>The base after the exact match is saved as mutation</p></td><td align="left">Three schemes (selecting a good reference, reference rewriting, and second-order compression) were proposed to improve the compression ratio</td><td align="left">Serial</td></tr><tr><td align="left">2015</td><td align="left">COGI [<xref ref-type="bibr" rid="CR18">18</xref>]</td><td align="left">COGI transforms the genomic sequences to a bitmap, then applies a rectangular partition coding algorithm to compress the binary image</td><td align="left"><p>The reference sequence is selected using techniques based on co-occurrence entropy and multi-scale entropy.</p><p>Compressing multiple sequences is supported by COGI, but the compression ratio decreases dramatically</p></td><td align="left">Serial</td></tr><tr><td align="left">2015</td><td align="left">GDC2 [<xref ref-type="bibr" rid="CR26">26</xref>]</td><td align="left">GDC2 is developed to compress large collections of genomes. Second-order compression scheme and variable integer encoding scheme are employed to reduce the size of compressed files</td><td align="left">GDC 2 is implemented in a multithreaded fashion. By default, GDC 2 uses 4 threads: 3 for the first level Ziv–Lempel factoring and 1 for the second-level factoring and arithmetic coding</td><td align="left">Multithreaded parallel</td></tr><tr><td align="left">2015</td><td align="left">iDoComp [<xref ref-type="bibr" rid="CR27">27</xref>]</td><td align="left"><p>Suffix array is used to index the reference sequence.</p><p>Greedy matching scheme is used to match the reference and the target sequence</p></td><td align="left">Suffix array has to be pre-computed and stored in the hard drive before compression</td><td align="left">Serial</td></tr><tr><td align="left">2016</td><td align="left">NRGC [<xref ref-type="bibr" rid="CR28">28</xref>]</td><td align="left">NRGC uses the score based placement technique to quantify the differences between genome sequences, so as to obtain the best position of each target block on the reference blocks</td><td align="left">NRGC has strict requirements on the similarity between the reference sequence and target sequence, which is prone to compression failure</td><td align="left">Serial</td></tr><tr><td align="left">2017</td><td align="left">HiRGC [<xref ref-type="bibr" rid="CR29">29</xref>]</td><td align="left">In the pre-processing stage, HiRGC separates the target sequence file into the identifier, the length of each line, position intervals of lowercase letters and the letter ‘N’, special letters and base letters, and then different compression schemes are used to compress them according to their characteristics</td><td align="left">The greedy matching scheme generates some suboptimal matching result</td><td align="left">Serial</td></tr><tr><td align="left">2018</td><td align="left">SCCG [<xref ref-type="bibr" rid="CR30">30</xref>]</td><td align="left">SCCG optimized the greedy matching scheme of HiRGC. It combines the greedy matching with the segmentation matching used in NRGC, matches the target sequence to the corresponding reference segmentation first, improves the compression ratio</td><td align="left">The compression time and memory consumption increase significantly</td><td align="left">Serial</td></tr><tr><td align="left">2019</td><td align="left">HRCM [<xref ref-type="bibr" rid="CR21">21</xref>]</td><td align="left">HRCM supports both pair-wise sequence compression and multiple sequences compression. When multiple sequences are compressed, optimized second-order compression scheme is used to improve compression ratio</td><td align="left">HRCM balances well the compression speed, compression ratio, and robustness, especially for large collections of genomes compression</td><td align="left">Serial</td></tr><tr><td align="left">2020</td><td align="left">memRGC [<xref ref-type="bibr" rid="CR7">7</xref>]</td><td align="left"><p>bfMEM algorithm [<xref ref-type="bibr" rid="CR31">31</xref>] is used to save the compression time and memory usage.</p><p>memRGC extends the MEMs if there are less than two SNPs between MEMs, that improves the compression ratio</p></td><td align="left">INDEL (INsertion and DELetion) and more than two SNPs are omitted in the approximate matching of memRGC</td><td align="left">multithreaded parallel</td></tr><tr><td align="left">2021</td><td align="left">HadoopHRCM [<xref ref-type="bibr" rid="CR22">22</xref>]</td><td align="left">HDFS and Map/Reduce architecture is employed to improve the compression speed of HRCM</td><td align="left">Distributed parallel computing technology is introduced to the FASTA compression</td><td align="left">Hadoop</td></tr></tbody></table></table-wrap></p>
    <p id="Par13">In recent decade, the performance of referential genome compression method continues improving, including compression ratio, robustness, scalability and applicability. The object of compression is also extended from single sequence to large collections of sequences. Researchers improve the compression method from every stage of compression process, such as sequence pre-processing, reference selection, index building, matching scheme, parallel scheme, etc. Specifically, in terms of sequence pre-processing, earlier proposed methods, such as FRESCO [<xref ref-type="bibr" rid="CR17">17</xref>] and COGI [<xref ref-type="bibr" rid="CR18">18</xref>], converted all characters to lowercases or uppercases and treated all non-base characters as ‘n’. The pre-processing scheme reduces the matching complexity, but losses some information. Recent proposed methods basically preserved all information of target sequences, that is, achieved lossless compression. In terms of reference selection, COGI uses the technology based on co-occurrence and multi-scale entropy. FRESCO uses the technology based on RsbitX. RCC [<xref ref-type="bibr" rid="CR19">19</xref>] and ECC [<xref ref-type="bibr" rid="CR6">6</xref>] cluster the target genomes, and choose the centroid of each cluster as the reference sequence. In terms of index building, researchers employed different technologies (e.g. suffix tree, suffix array, hash array, compressed suffix tree, etc.) to adapt different scenarios. In terms of matching scheme, researchers proposed greedy matching, segmentation matching and approximate matching schemes. Due to the improving technologies and schemes, the compression ratio is getting better and better, from dozens [<xref ref-type="bibr" rid="CR20">20</xref>] to more than 400:1 [<xref ref-type="bibr" rid="CR6">6</xref>]. When the second-order compression scheme is employed, the compression ratio achieves even more than 2000:1 [<xref ref-type="bibr" rid="CR21">21</xref>]. However, the cost is the increasing time complexity. With the exponential increase of genomic data, intolerable compression time emerges to be a problem that compression researchers have to work hard to resolve. Therefore, in order to reduce compression time, some researchers started to employ parallel technology. But the most used parallel technology is the straightforward multithreaded parallel technology. With the development of the big data processing technology, some Hadoop based genome compression method are proposed [<xref ref-type="bibr" rid="CR22">22</xref>].But generally speaking, the research based on big data processing technologies still has a lot of work to be done. So far, to our best knowledge, there is no published research on Spark based genome compression, but only some Spark based genome analysis achievements [<xref ref-type="bibr" rid="CR23">23</xref>].</p>
  </sec>
  <sec id="Sec3">
    <title>Methodology</title>
    <p id="Par14">This section will firstly introduce the architecture of SparkGC, and then describe separately how does SparkGC parallelize the compression tasks, and finally introduce the decompression.</p>
    <sec id="Sec4">
      <title>Architecture</title>
      <p id="Par15">The architecture for large collections of genomes compression based on Spark is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Spark architecture divides the computing cluster to master nodes and worker nodes. The compression algorithm is deployed on the master node, but the scheduling mechanism of Spark is migrating the computing tasks to nodes closest to the data, so the compression tasks will be scheduled to worker nodes. The <italic>Driver</italic> component of Spark executes the main function of genome compression in SparkGC. <italic>TaskScheduler</italic> component partitions the compression tasks and schedules them to each executor. Executor is a process running on worker node to execute compression tasks and cache intermediate results of RDD transformation. The master node reads the reference sequence from HDFS or local file system, and builds the index of the reference sequence. The driver broadcasts the reference sequence and its index as broadcast variables. The executor stores the broadcast variables to the <italic>BlockManager</italic> component. Broadcast variable is a shared variable mechanism of Spark. It enables the programs to send large size read-only data to all worker nodes. The worker nodes read the to-be-compressed sequences from HDFS before the compression and write the compressed results to HDFS after compression.<fig id="Fig1"><label>Fig. 1</label><caption><p>Architecture of Spark based genome compression</p></caption><graphic xlink:href="12859_2022_4825_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec5">
      <title>Pre-processing</title>
      <p id="Par16">The data flow of SparkGC is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. After each sequence file is read into memory, pre-processing of the sequence file follows closely. The sequence file is divided into two parts, base data, and auxiliary data. Base data refers to the base sequence composed of uppercase ACGT, and auxiliary data is the identifier, line break characters, special characters, and other information contained in the sequence. Because SparkGC is a lossless genome compression method, the auxiliary data of the to-be-compressed sequence cannot be lost. They are compressed independently with specific coding schemes at the pre-processing stage. The base data of the to-be-compressed sequence file saved as RDD to the memory of worker nodes for compression tasks. It is the first RDD of SparkGC data flow, so the RDD is indexed as RDD1 in this article. RDD is a logical entity of Spark. It is used as a whole, but actually the data of RDD are distributed in the memory of different worker nodes. The base data of one to-be-compressed sequence is one partition of RDD1. Each partition maps to a processing thread, which ensures that the process of these partitions is independent and concurrent.<fig id="Fig2"><label>Fig. 2</label><caption><p>Data flow of SparkGC</p></caption><graphic xlink:href="12859_2022_4825_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par17">The hash index built for the reference sequence data is on the master node. However, all worker nodes require reference sequence data and its hash index, so broadcast variables need to be created for them. In order to reduce the size of broadcast variables, Kryo [<xref ref-type="bibr" rid="CR32">32</xref>] is used for serialization and compression of broadcast variables. The broadcast variables are set to read-only, which will not cause thread-safety problems. Then the broadcast variables are sent and cached in the memory of each worker node to prepare for the compression stage.</p>
    </sec>
    <sec id="Sec6">
      <title>Compression</title>
      <p id="Par18">The compression stage of SparkGC contains two steps which are referred to as the first-order compression and the second-order compression. The main task of the first-order compression is mapping the to-be-compressed sequences to the reference sequence based on the hash index, that generates the MEMs. The MEMs obtained at this stage are encoded as the tuple &lt; position, length &gt;. The mismatched sequence data is stored as the original characters. Therefore, after the first-level mapping, the original sequence data is converted to a new sequence composed of triple &lt; position, length, mismatched string&gt;. All the mapped results are not saved to the file system, but saved in the memory for the second-order compression. They are the second RDD of the data flow of SparkGC, i.e., RDD2. The mapped results of one to-be-compressed sequence are saved as one partition of RDD2. The partition is represented as &lt; <italic>key, value</italic> &gt;, where the <italic>key</italic> is the partition number and the <italic>value</italic> is the mapped results. The partition number is used to identify the sequence ID, so as to ensure the sequence order in subsequent processing. If the data amount of the <italic>value</italic> exceeds the memory limitation of the worker node, Kryo serialization will be used again to compress the data to prevent compression failure because of insufficient memory, that makes the compression of large collections of genomes successful on an ordinary computer.</p>
      <p id="Par19">After the first-order compression, a part of the compressed sequences will be regarded as the references of the second-order compression, hereinafter they are referred to as the second-order references. The second-order references are filtered out according to the sequence ID. Then, the hash index for the second-order references is built. The second-order references and their hash indexes are cached in memory as RDD3. The partitions of RDD3 are distributed on different worker nodes. Because like the first-order compression, all worker nodes need to use the second-order references and their hash indexes at the second-order compression stage. Therefore, all partitions in RDD3 are merged into one partition, i.e., RDD4. The master node then creates the broadcast variable based on RDD4 and sends it to all worker nodes. The merging will generate shuffle, that results in network transmission and disk access. However, after the first-order compression, the size of the compressed sequence has been reduced by more than 100 times compared to the original size, so the amount of shuffle data is not large. The first-order compressed results RDD2 is also kept in memory for the second-level matching.</p>
      <p id="Par20">The essence of the second-level matching is mapping the first-order compressed sequences to the second-order references by order using the hash indexes. The first-order compressed sequences are read from RDD2. The second-order references and their hash indexes are read from broadcast variables. After the second-level matching, the original first-level mapped results are converted to the second-level mapped results composed of triples &lt; sequence ID, position, length&gt; and triples &lt; position, length, mismatched string&gt;. Lastly, the second-order references are also compressed. The <italic>i</italic>-th second-order reference is mapped with the first to the (<italic>i</italic>-1)-th second-order references. In this way the second-order references can be losslessly reconstructed.</p>
      <p id="Par21">After the second-level mapping, all the mapped results are written to HDFS, one file for one sequence. These files will be downloaded to the local file system, compressed by BSC compression algorithm (<ext-link ext-link-type="uri" xlink:href="http://libbsc.com/">http://libbsc.com/</ext-link>). So far, the whole compression is completed. SparkGC compression algorithm is summarized in Algorithm 1.<graphic position="anchor" xlink:href="12859_2022_4825_Figa_HTML" id="MO3"/></p>
    </sec>
    <sec id="Sec7">
      <title>Decompression</title>
      <p id="Par22">Firstly, the compressed file is decompressed by BSC decompression algorithm to get all the second-order compressed data. The reference sequence is read and extracted in the same way as compression. But unlike compression, there is no need to build any hash index in decompression. So decompression and compression is asymmetric. Decompression requires much less memory and time. The target sequences are read by order, their base data and auxiliary data are reconstructed respectively. It is worth to note that SparkGC supports decompressing the sequences interested without decompressing all the sequences every time. The overview of the decompression is shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. If the target sequence is one of the second-order references, the <italic>i</italic>-th target sequence only depends on its previous <italic>i</italic>-1 sequences. The target sequence can be totally decompressed without decompressing the remainder sequences. If the target sequence is not one of the second-order references, its decompression only depends on all the second-order references, other sequences don’t need to be decompressed. For large genomic datasets, this will save decompressors a lot of time.<fig id="Fig3"><label>Fig. 3</label><caption><p>Overview of the decompression of SparkGC</p></caption><graphic xlink:href="12859_2022_4825_Fig3_HTML" id="MO4"/></fig></p>
      <p id="Par23">Because more than 95% of decompression time is I/O time, the actual limitation of the decompression is the hard disk write speed. That is different from the compression which is CPU-bound and memory-bound. It has no effect to parallelize the decompression. Therefore, SparkGC does not implement the parallelization of decompression.</p>
    </sec>
  </sec>
  <sec id="Sec8">
    <title>Results</title>
    <p id="Par24">We evaluate the performance of SparkGC in this section. SparkGC was run on the cluster with 4 worker nodes and 1 master node. Each node is a common computer configured with 2 × 2.8 GHz Intel Xeon E5-2680 (20 cores) and 32 GB RAM. SparkGC was run over YARN (Yet Another Resource Negotiator) in the platform.</p>
    <p id="Par25">The datasets we selected firstly were 1000 Genome Project [<xref ref-type="bibr" rid="CR33">33</xref>] which contains 1092 human genomes. In addition, we supplemented another 10 genomes HG13, HG16, HG17, HG18, HG19, HG38, K131 (the abbreviation of KOREF_20090131), K224 (the abbreviation of KOREF_20090224) [<xref ref-type="bibr" rid="CR34">34</xref>], YH [<xref ref-type="bibr" rid="CR35">35</xref>], and Huref [<xref ref-type="bibr" rid="CR36">36</xref>]. These 10 human genomes are derived from different sequencing teams using different methods in different periods. They have different characteristics so that they are widely used in genome compression algorithms evaluation [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR30">30</xref>]. Therefore, our datasets totally contain 1102 human genomes and the total file size is about 3.11 TB. All the datasets can be downloaded from open access FTP server. Details of these datasets are provided in the Additional file <xref rid="MOESM1" ref-type="media">1</xref>.</p>
    <sec id="Sec9">
      <title>Compression ratio</title>
      <p id="Par26">As a compression method, the compression ratio is always the first factor to be evaluated. Firstly we arbitrarily selected HG16 as the reference to compress other 1100 human genomes. In the robustness section, we evaluated the compression performance under different references. In addition, we also tested the compression ratio and compression time of the state-of-the-art genome compression methods in recent 4 years. They compressed the same 1100 human genomes under the same reference run on the same computers. These compression methods are HiRGC [<xref ref-type="bibr" rid="CR29">29</xref>] proposed in 2017, SCCG [<xref ref-type="bibr" rid="CR30">30</xref>] proposed in 2018, HRCM [<xref ref-type="bibr" rid="CR21">21</xref>] proposed in 2019, and memRGC [<xref ref-type="bibr" rid="CR7">7</xref>] proposed in 2020. Their compression ratios are shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. Details of the experimental results are provided in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S3.<fig id="Fig4"><label>Fig. 4</label><caption><p>Compression ratio of SparkGC and the state-of-the-art methods</p></caption><graphic xlink:href="12859_2022_4825_Fig4_HTML" id="MO5"/></fig></p>
      <p id="Par27">We can see from the Fig. <xref rid="Fig4" ref-type="fig">4</xref>, SparkGC achieved the best compression ratio among all the compression methods. The compression ratio of SparkGC is about 2347:1, it compressed the 3.11 TB original data to about 1387 MB. Compared to HiRGC, SCCG, and memRGC, the compression ratio is improved by 673%, 653%, and 582%. The compression ratio of SparkGC is greatly improved mainly because of the second-order compression scheme. After the first-level matching of each to-be-compressed sequence to the reference sequence, the matched results are not written to file, but saved in memory as intermediate data. Part of these intermediate sequences are selected as the second-order references to build hash index, then each first-order compressed sequence is compressed again according to the second-order hash index matrix. This compression scheme fully utilizes the similarity among the to-be-compressed sequences, greatly reduces the size of the compressed file. HRCM is also a second-order compression method, but compared to HRCM, the compression ratio of SparkGC is improved by 31%. The reason is that SparkGC uses BSC compression algorithm to compress the second-order compressed sequences.</p>
    </sec>
    <sec id="Sec10">
      <title>Compression speed</title>
      <p id="Par28">The compression speed of compressing 1100 human genomes using HG16 as the reference sequence is shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. Details of the experimental results are provided in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S4. Because SparkGC is a distributed parallel method, it will distribute the compression tasks on multiple nodes and run at the same time. In order to be as fair as possible, in this experiment, the number of worker nodes of SparkGC was set as 1, that is, SparkGC only used one worker node to perform the compression. The compression speed of multiple nodes will be illustrated in the scalability section. All compression time of this paper corresponds to the ‘real’ or wall-clock elapsed time. Each experiment was executed 3 times and the average time was recorded.<fig id="Fig5"><label>Fig. 5</label><caption><p>Compression speed of SparkGC and the state-of-the-art methods</p></caption><graphic xlink:href="12859_2022_4825_Fig5_HTML" id="MO6"/></fig></p>
      <p id="Par29">As can be seen from Fig. <xref rid="Fig5" ref-type="fig">5</xref>, although SparkGC only used one worker node to execute the compression, the compression speed achieved more than 58 MB/s, which is much higher than the best state-of-the-art methods. It only took 15.53 h to complete the compression of 3.11 TB genomic data. The compression speed is 5.63 times of HiRGC, 21.75 times of SCCG, 11.21 times of memRGC, and 3.85 times of HRCM. SCCG takes the most time, more than 14 days. It is hard to tolerate so much time to compress 1100 human genomes. SparkGC reduced the compression time of several days required by other methods to just more than half a day. The reason why SparkGC can achieve such high speed on one node is that the algorithm will make full use of the multi-thread of a single node automatically for compressing.</p>
    </sec>
    <sec id="Sec11">
      <title>Scalability</title>
      <p id="Par30">The biggest advantage of SparkGC is not the performance on a single node, but its high scalability, which is the advantage that other methods do not have. We did a series of experiments to evaluate the scalability of SparkGC. Firstly we evaluated the compression speed of all chromosomes on the cluster with an increasing number of worker nodes activated, ranging from 1 to 4. The compression ratio of SparkGC does not correlate with the number of worker nodes, so the increasing number of worker nodes will not change the compression ratio. The total compression time of all chromosomes under different numbers of worker nodes is shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. Here we observe that, with the increasing number of the worker nodes, the compression time decreases greatly. When the number of worker nodes is 4, SparkGC was able to compress the 3.11 TB genomic data to about 1387 MB in less than 6 h. The compression speed is about 151 MB/s.<fig id="Fig6"><label>Fig. 6</label><caption><p>Total compression time under different number of worker nodes</p></caption><graphic xlink:href="12859_2022_4825_Fig6_HTML" id="MO7"/></fig></p>
      <p id="Par31">In terms of runtime and parallelism, the following experiment evaluated the compression process of SparkGC from four stages:<list list-type="order"><list-item><p id="Par32">Pre-processing stage completes the reading and hash index building of the reference sequence, and creating the broadcast variables.</p></list-item><list-item><p id="Par33">First-order compression stage completes the first-level matching of all the to-be-compressed sequences to the reference sequence, then shuffles all the second-order references and their hash indexes, and creates the broadcast variables.</p></list-item><list-item><p id="Par34">Second-order compression stage completes the second-level matching of all the first-order compressed sequences.</p></list-item><list-item><p id="Par35">Post-processing stage downloads all the matched result files to the local file system and compresses them by BSC compression algorithm, then cleans up tasks on all worker nodes.</p></list-item></list></p>
      <p id="Par36">In these four stages, the first stage and the last stage cannot be parallelized, they are run only on the master node. Only the second stage and the third stage can be parallelized. Therefore, to evaluate the change of runtime of different stages with the increasing number of worker nodes, we illustrate the compression time of Chromosome 1 (abbreviate as Chr1) and Chromosome 13 (abbreviate as Chr13) in detail, as shown in Table <xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Runtime of different parts on different numbers of worker nodes</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Chromosome</th><th align="left" rowspan="2">Stage</th><th align="left" colspan="2">1 worker node</th><th align="left" colspan="2">2 worker nodes</th><th align="left" colspan="2">3 worker nodes</th><th align="left" colspan="2">4 worker nodes</th></tr><tr><th align="left">Time (s)</th><th align="left">%</th><th align="left">Time (s)</th><th align="left">%</th><th align="left">Time (s)</th><th align="left">%</th><th align="left">Time (s)</th><th align="left">%</th></tr></thead><tbody><tr><td align="left" rowspan="5">Chr1</td><td align="left">Pre-processing</td><td char="." align="char">112</td><td char="." align="char">1.79</td><td char="." align="char">116</td><td char="." align="char">3.84</td><td char="." align="char">124</td><td char="." align="char">5.27</td><td char="." align="char">126</td><td char="." align="char">6.49</td></tr><tr><td align="left">First-order</td><td char="." align="char">5577</td><td char="." align="char">89.23</td><td char="." align="char">2618</td><td char="." align="char">86.72</td><td char="." align="char">2007</td><td char="." align="char">85.37</td><td char="." align="char">1648</td><td char="." align="char">84.81</td></tr><tr><td align="left">Second-order</td><td char="." align="char">531</td><td char="." align="char">8.50</td><td char="." align="char">254</td><td char="." align="char">8.41</td><td char="." align="char">188</td><td char="." align="char">8.00</td><td char="." align="char">136</td><td char="." align="char">7.00</td></tr><tr><td align="left">Post-processing</td><td char="." align="char">30</td><td char="." align="char">0.48</td><td char="." align="char">31</td><td char="." align="char">1.03</td><td char="." align="char">32</td><td char="." align="char">1.36</td><td char="." align="char">32</td><td char="." align="char">1.70</td></tr><tr><td align="left">Total</td><td char="." align="char">6250</td><td char="." align="char">100</td><td char="." align="char">3019</td><td char="." align="char">100</td><td char="." align="char">2351</td><td char="." align="char">100</td><td char="." align="char">1943</td><td char="." align="char">100</td></tr><tr><td align="left" rowspan="5">Chr13</td><td align="left">Pre-processing</td><td char="." align="char">62</td><td char="." align="char">3.58</td><td char="." align="char">70</td><td char="." align="char">6.42</td><td char="." align="char">70</td><td char="." align="char">9.06</td><td char="." align="char">70</td><td char="." align="char">10.74</td></tr><tr><td align="left">First-order</td><td char="." align="char">1520</td><td char="." align="char">87.76</td><td char="." align="char">921</td><td char="." align="char">84.50</td><td char="." align="char">625</td><td char="." align="char">80.85</td><td char="." align="char">512</td><td char="." align="char">78.53</td></tr><tr><td align="left">Second-order</td><td char="." align="char">120</td><td char="." align="char">6.93</td><td char="." align="char">69</td><td char="." align="char">6.33</td><td char="." align="char">47</td><td char="." align="char">6.08</td><td char="." align="char">39</td><td char="." align="char">5.98</td></tr><tr><td align="left">Post-processing</td><td char="." align="char">30</td><td char="." align="char">1.73</td><td char="." align="char">30</td><td char="." align="char">2.75</td><td char="." align="char">31</td><td char="." align="char">4.01</td><td char="." align="char">31</td><td char="." align="char">4.75</td></tr><tr><td align="left">Total</td><td char="." align="char">1732</td><td char="." align="char">100</td><td char="." align="char">1090</td><td char="." align="char">100</td><td char="." align="char">773</td><td char="." align="char">100</td><td char="." align="char">652</td><td char="." align="char">100</td></tr></tbody></table></table-wrap></p>
      <p id="Par37">It can be seen from Table <xref rid="Tab2" ref-type="table">2</xref> that the pre-processing stage and the post-processing stage did not save time with the increase of worker nodes, on the contrary, their runtime increased with the increase of worker nodes. Because with the increase of worker nodes, the program needs to initialize all worker nodes, and broadcast variables need to be sent to all worker nodes, which increases the runtime. Similarly, at the post-processing stage, the increase of worker nodes will lead to a longer clean-up time. Observing the first-order compression stage and the second-order compression stage will find that these two stages scaled quite well. For example, to Chr1, when the number of worker nodes was 1, 2, and 4, the average first-order compression time of each chromosome was 5, 2.4, and 1.5 s respectively, and the average second-order compression time of each chromosome was 0.48, 0.23, and 0.12 s respectively. To Chr13, it was 1.38, 0.83, and 0.46 s respectively at the first-order compression stage and 0.11, 0.06, and 0.03 s respectively at the second-order compression stage. The runtime was almost decreasing at the linear speed. From the percentage of runtime at each stage, the percentage of serial computing was gradually increasing, while the percentage of parallel computing was gradually decreasing, so the overall runtime showed a sublinear downward trend.</p>
      <p id="Par38">The above experiments are all compressing 1100 human genomes. We are very interested in how the compression ratio and compression speed of SparkGC scale with the number of the to-be-compressed sequences. Because in the actual compression scenario, SparkGC will compress any size of genomic data sets. We evaluated the compression ratio and compression speed of Chr1 and Chr13 when the sequence number was 200, 400, 600, 800, and 1000 respectively, as shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. In this experiment, the number of worker nodes was 3.<fig id="Fig7"><label>Fig.7</label><caption><p>Compression performance to the different number of target sequences</p></caption><graphic xlink:href="12859_2022_4825_Fig7_HTML" id="MO8"/></fig></p>
      <p id="Par39">We can see from Fig. <xref rid="Fig7" ref-type="fig">7</xref> that SparkGC also scaled quite well to the number of the to-be-compressed sequences. The compression ratio and compression speed of Chr1 and Chr13 both increased with the increase of the to-be-compressed sequences. The compression ratio of Chr1 gradually increased from 1756:1 to 2390:1, and that of Chr13 increased from 2085:1 to 2871:1. The compression speed of Chr1 increased gradually from 65 MB/s to 106 MB/s, and that of Chr13 increased from 100 MB/s to 139 MB/s. The compression ratio of SparkGC increases with the increase of the to-be-compressed sequences is because in that case, the percentage of the second-order references decreases. The default number of the second-order references of SparkGC is 40. The compression ratio of the second-order references is low, because the <italic>i</italic>-th second-order reference is compressed only using the first to the (<italic>i</italic>-1)-th sequences as references. So the smaller <italic>i</italic> is, the smaller the compression ratio is. The compression speed of SparkGC also increases with the increase of the to-be-compressed sequences, because the percentage of the first-order compression time and the second-order compression time increases. The compression speed and compression ratio of Chr13 is higher than Chr1 is because of the different similarity of different chromosomes. Generally, the greater the similarity of chromosomes, the higher the compression ratio and the faster the compression speed [<xref ref-type="bibr" rid="CR37">37</xref>].</p>
    </sec>
    <sec id="Sec12">
      <title>Robustness</title>
      <p id="Par40">The compression ratio of the referential compression method is easily affected by the reference sequence. Therefore, some researchers studied the reference selection [<xref ref-type="bibr" rid="CR6">6</xref>] [<xref ref-type="bibr" rid="CR17">17</xref>] [<xref ref-type="bibr" rid="CR19">19</xref>]. However, with the increase of the to-be-compressed sequences, the selection of reference sequences becomes hard due to its high time complexity. We evaluated the affection of different references to the performance of SparkGC by selecting 6 genomes with different characteristics to compress 1100 human genomes. The compressed size and compression time of Chr1 and Chr13 under different references are shown in Tables <xref rid="Tab3" ref-type="table">3</xref> and <xref rid="Tab4" ref-type="table">4</xref> respectively. In the two tables, AVG represents the average value under different references, it is computed by (<xref rid="Equ1" ref-type="">1</xref>).<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$AVG = {{\sum\nolimits_{i = 1}^{n} {s_{i} } } \mathord{\left/ {\vphantom {{\sum\nolimits_{i = 1}^{n} {s_{i} } } n}} \right. \kern-\nulldelimiterspace} n}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="true">/</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:math><graphic xlink:href="12859_2022_4825_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s_{i}$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4825_Article_IEq1.gif"/></alternatives></inline-formula> represents the compressed file size, and the <italic>n</italic> value is the number of references. SD is the standard deviation of all values, represents the degree of dispersion. It is computed by (<xref rid="Equ2" ref-type="">2</xref>).<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$SD = \sqrt {\frac{1}{n}\sum\nolimits_{i = 1}^{n} {\left( {x_{i} - \overline{x} } \right)^{2} } }$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mover><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math><graphic xlink:href="12859_2022_4825_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq2"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overline{x}$$\end{document}</tex-math><mml:math id="M8"><mml:mover><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2022_4825_Article_IEq2.gif"/></alternatives></inline-formula> represents the AVG value of the case. It can be seen from Table <xref rid="Tab3" ref-type="table">3</xref> that the compression ratio of SparkGC achieved the best results under all reference sequences, and the influence of reference on SparkGC was very small. The difference betweeen the maximum and minimum compressed size of Chr1 is 15 MB, and of Chr13 is 8 MB. Compared with the original data of 264,994 MB and 122,492 MB respectively, these differences can be ignored. The compressed results of HiRGC, SCCG, and memRGC under different reference sequences are quite different, especially to Chr1. The maximum compressed size of HiRGC, SCCG, and memRGC of Chr1 is 5.15 times, 5.24 times, and 5.75 times that of the minimum compressed size, respectively. When HG13 was the reference sequence, SCCG even failed to compress all sequences. The reason why SparkGC is less affected by the reference sequence is that if the similarity between the reference sequence and the to-be-compressed sequence is low, many identical mismatched fragments will be generated after the first-level matching, and these mismatched fragments will be matched and compressed in the second-level matching, so the compression result has little relationship with the similarity between the reference sequence and the to-be-compressed sequence. HRCM is also one of the second-order compression methods, so HRCM also performs well in robustness.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Compressed size under different references</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Chromosome</th><th align="left" rowspan="2">Original size (MB)</th><th align="left" rowspan="2">Method</th><th align="left" colspan="6">Compressed size (MB) under different references</th><th align="left" rowspan="2">AVG</th><th align="left" rowspan="2">SD</th></tr><tr><th align="left">HG13</th><th align="left">HG16</th><th align="left">K131</th><th align="left">YH</th><th align="left">Huref</th><th align="left">HG00096</th></tr></thead><tbody><tr><td align="left" rowspan="5">Chr1</td><td align="left" rowspan="5">264,994</td><td align="left">HiRGC</td><td align="left">2474</td><td char="." align="char">1313</td><td char="." align="char">828</td><td char="." align="char">750</td><td char="." align="char">1026</td><td char="." align="char">480</td><td align="left">1145</td><td align="left">647</td></tr><tr><td align="left">SCCG</td><td align="left">2430</td><td char="." align="char">1284</td><td char="." align="char">775</td><td char="." align="char">706</td><td char="." align="char">986</td><td char="." align="char">464</td><td align="left">1107</td><td align="left">643</td></tr><tr><td align="left">memRGC</td><td align="left">2324</td><td char="." align="char">1192</td><td char="." align="char">650</td><td char="." align="char">593</td><td char="." align="char">887</td><td char="." align="char">406</td><td align="left">1009</td><td align="left">638</td></tr><tr><td align="left">HRCM</td><td align="left">126</td><td char="." align="char">140</td><td char="." align="char">156</td><td char="." align="char">154</td><td char="." align="char">151</td><td char="." align="char">136</td><td align="left">144</td><td align="left">11</td></tr><tr><td align="left">SparkGC</td><td align="left"><bold>123</bold></td><td char="." align="char"><bold>115</bold></td><td char="." align="char"><bold>130</bold></td><td char="." align="char"><bold>128</bold></td><td char="." align="char"><bold>125</bold></td><td char="." align="char"><bold>115</bold></td><td align="left"><bold>123</bold></td><td align="left"><bold>6</bold></td></tr><tr><td align="left" rowspan="5">Chr13</td><td align="left" rowspan="5">122,492</td><td align="left">HiRGC</td><td align="left">333</td><td char="." align="char">296</td><td char="." align="char">413</td><td char="." align="char">394</td><td char="." align="char">314</td><td char="." align="char">219</td><td align="left">328</td><td align="left">64</td></tr><tr><td align="left">SCCG</td><td align="left">/</td><td char="." align="char">289</td><td char="." align="char">390</td><td char="." align="char">375</td><td char="." align="char">308</td><td char="." align="char">219</td><td align="left">/</td><td align="left">/</td></tr><tr><td align="left">memRGC</td><td align="left">288</td><td char="." align="char">254</td><td char="." align="char">333</td><td char="." align="char">319</td><td char="." align="char">262</td><td char="." align="char">190</td><td align="left">274</td><td align="left">47</td></tr><tr><td align="left">HRCM</td><td align="left">48</td><td char="." align="char">56</td><td char="." align="char">66</td><td char="." align="char">65</td><td char="." align="char">63</td><td char="." align="char">57</td><td align="left">59</td><td align="left">6</td></tr><tr><td align="left">SparkGC</td><td align="left"><bold>44</bold></td><td char="." align="char"><bold>44</bold></td><td char="." align="char"><bold>52</bold></td><td char="." align="char"><bold>52</bold></td><td char="." align="char"><bold>50</bold></td><td char="." align="char"><bold>45</bold></td><td align="left"><bold>48</bold></td><td align="left"><bold>3</bold></td></tr></tbody></table><table-wrap-foot><p>‘/’ indicates the method fails to compress the chromosome. Bold indicates the best value of the case</p></table-wrap-foot></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Compression time under different references</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Chromosome</th><th align="left" rowspan="2">Method</th><th align="left" colspan="6">Compression time (hour) under different references</th><th align="left" rowspan="2">AVG</th><th align="left" rowspan="2">SD</th></tr><tr><th align="left">HG13</th><th align="left">HG16</th><th align="left">K131</th><th align="left">YH</th><th align="left">Huref</th><th align="left">HG00096</th></tr></thead><tbody><tr><td align="left" rowspan="5">Chr1</td><td align="left">HiRGC</td><td align="left">11.95</td><td char="." align="char">7.40</td><td char="." align="char">8.75</td><td char="." align="char">8.82</td><td char="." align="char">10.01</td><td char="." align="char">6.55</td><td align="left">8.91</td><td align="left">1.75</td></tr><tr><td align="left">SCCG</td><td align="left">21.25</td><td char="." align="char">21.91</td><td char="." align="char">37.73</td><td char="." align="char">37.11</td><td char="." align="char">39.86</td><td char="." align="char">36.66</td><td align="left">32.42</td><td align="left">7.73</td></tr><tr><td align="left">memRGC</td><td align="left">20.49</td><td char="." align="char">12.05</td><td char="." align="char">16.09</td><td char="." align="char">18.45</td><td char="." align="char">16.53</td><td char="." align="char">11.07</td><td align="left">15.78</td><td align="left">3.32</td></tr><tr><td align="left">HRCM</td><td align="left">11.19</td><td char="." align="char">8.28</td><td char="." align="char">9.60</td><td char="." align="char">8.40</td><td char="." align="char">9.76</td><td char="." align="char">5.72</td><td align="left">8.82</td><td align="left">1.69</td></tr><tr><td align="left">SparkGC</td><td align="left"><bold>0.70</bold></td><td char="." align="char"><bold>0.54</bold></td><td char="." align="char"><bold>0.57</bold></td><td char="." align="char"><bold>0.49</bold></td><td char="." align="char"><bold>0.63</bold></td><td char="." align="char"><bold>0.40</bold></td><td align="left"><bold>0.56</bold></td><td align="left"><bold>0.10</bold></td></tr><tr><td align="left" rowspan="5">Chr13</td><td align="left">HiRGC</td><td align="left">2.67</td><td char="." align="char">2.43</td><td char="." align="char">2.37</td><td char="." align="char">2.45</td><td char="." align="char">2.48</td><td char="." align="char">2.41</td><td align="left">2.47</td><td align="left">0.10</td></tr><tr><td align="left">SCCG</td><td align="left">/</td><td char="." align="char">24.15</td><td char="." align="char">18.61</td><td char="." align="char">23.44</td><td char="." align="char">12.37</td><td char="." align="char">10.17</td><td align="left">/</td><td align="left">/</td></tr><tr><td align="left">memRGC</td><td align="left">9.07</td><td char="." align="char">8.13</td><td char="." align="char">14.62</td><td char="." align="char">9.82</td><td char="." align="char">11.25</td><td char="." align="char">8.23</td><td align="left">10.19</td><td align="left">2.25</td></tr><tr><td align="left">HRCM</td><td align="left">1.64</td><td char="." align="char">1.43</td><td char="." align="char">1.47</td><td char="." align="char">1.40</td><td char="." align="char">1.44</td><td char="." align="char">1.25</td><td align="left">1.44</td><td align="left">0.11</td></tr><tr><td align="left">SparkGC</td><td align="left"><bold>0.2</bold></td><td char="." align="char"><bold>0.18</bold></td><td char="." align="char"><bold>0.19</bold></td><td char="." align="char"><bold>0.17</bold></td><td char="." align="char"><bold>0.16</bold></td><td char="." align="char"><bold>0.15</bold></td><td align="left"><bold>0.18</bold></td><td align="left"><bold>0.01</bold></td></tr></tbody></table><table-wrap-foot><p>‘/’ indicates the method fails to compress the chromosome. Bold indicates the best value of the case</p></table-wrap-foot></table-wrap></p>
      <p id="Par41">As can be seen from Table <xref rid="Tab4" ref-type="table">4</xref>, SparkGC performed better in the robustness of compression time. The SD values of Chr1 and Chr13 are only 0.1 and 0.01 respectively, which are far lower than other compression methods. In terms of compression time, the maximum and smallest minimum compression time of Chr1 are 42 min and 24 min respectively; the maximum and minimum compression time of Chr13 are 20 min and 9 min respectively, and the difference is very small.</p>
    </sec>
    <sec id="Sec13">
      <title>Discussion</title>
      <p id="Par42">Data compression is always a trade-off between compression ratio and compression speed, so is SparkGC. When the reference sequence is determined, the most important factor affecting compression ratio and compression speed is the number of the second-order references. In the second-level matching, the more the second-order references are, the greater the probability of matching successfully, so the compression ratio is higher. However, the cost is that the shuffle time and the hash index building time of the second-order references, the transferring time of broadcast variables, and the search time in hash index will be longer. We evaluated the trade-off between compression ratio and compression speed of SparkGC under 7 different numbers of the second-order references, as shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>. In this experiment, we evenly selected 8 chromosomes of our datasets for compressing. They are Chr1, Chr4, Chr7, Chr10, Chr13, Chr16, Chr19, and Chr22. The total file disk size of these chromosomes is about 1 TB.<fig id="Fig8"><label>Fig. 8</label><caption><p>Trade-off between compression ratio and compression speed</p></caption><graphic xlink:href="12859_2022_4825_Fig8_HTML" id="MO9"/></fig></p>
      <p id="Par43">We can see from the figure, with the increase of the number of second-order references, the compressed size decreases, and the compression time increases. The compressed size decreases from 611 MB when the number of second-order references is 10 to 460 MB when the number of second-order references is 70, the reduction rate is 24.7%. However, the compression time increases from 116 to 133 min, with an increase of 14.7%. Therefore, the compressors can choose the appropriate number of the second-order references according to their own needs.</p>
      <p id="Par44">In order to expand the applicability of the method, we developed sub-modules to compress FASTQ sequence based on the proposed methodology. Furthermore, we used data sets generated by different sequencing technologies including new and traditional ones to evaluate the FASTQ compression modules. The sequencing technologies we selected are Illumina, PacBio, and Oxford Nanopore. Details of the data sets are provided in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S2. The compression ratio and speed are shown in Fig. <xref rid="Fig9" ref-type="fig">9</xref>.<fig id="Fig9"><label>Fig. 9</label><caption><p>Compression ratio and speed on FASTQ data sets</p></caption><graphic xlink:href="12859_2022_4825_Fig9_HTML" id="MO10"/></fig></p>
      <p id="Par45">From Fig. <xref rid="Fig9" ref-type="fig">9</xref>, it is shown that SparkGC successfully work on all test data sets. However, the performance changes with different sequencing technologies. It obtains superior performance on the second generation sequencing platform Illumina to the third generation sequencing platforms PacBio and Oxford Nanopore. The reason is that the third generation sequencing technologies obtain longer read length, but which is accompanied by a relatively higher error rate. The error bases result in more fragments when matching the reference sequence, which affects the compression performance. In addition, the unfixed read length value also consumes a certain amount of storage space.</p>
      <p id="Par46">The evaluation of a genome compression method must take into account the main memory usage. Compared with the stand-alone programs, it is more complex to discuss the memory usage of SparkGC. Because the tasks of the master node and each worker node are different, the memory usage is different. The master node is responsible for reading reference sequences, the aggregation of the first-order compression results, and the hash index building. The memory footprint of the master node is affected by the size of the reference sequence and the number of second-order references. The worker node is responsible for reading the to-be-compressed sequences, the first-order compression, and the second-order compression. The memory usage is related to the number of to-be-compressed sequences. From our experimental observation, compressing 1100 human genomes consumes the most memory. However, whether on the master node or worker node, the memory footprint of SparkGC is less than 20 GB.</p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Conclusions</title>
    <p id="Par47">This research proposes and implements a genome compression method based on Apache Spark. It can run efficiently on a multi-node cluster to compress large collections of genomes. Compared to the state-of-the-art genome compression methods, the compression ratio and speed are both recognizably improved. Besides, the method has good scalability and robustness. It will greatly benefit the storage of large genomic datasets. However, it should be noted that developing Spark based programs is not a trivial task. As such, they have largely only been embraced in the technology sector. Making Spark based genome compression method easy to use and extend for more non-computer science professionals is our goal at the next stage.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec15">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2022_4825_MOESM1_ESM.docx">
            <caption>
              <p><bold>Additional file 1</bold>. Details of the data sets and experimental results.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank the anonymous reviewers for their valuable comments and suggestions to improve the quality of our manuscript.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>HY and YJ wrote the main manuscript text. GH prepared the figures and tables. SL did the experiments. HF wrote the code. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was partially funded by the National Key R&amp;D Program of China (2018AAA0103300), The Natural Science Foundation of the Jiangsu Higher Education Institutions of China (22KJB520001), Modern Educational Technology Research Program of Jiangsu Province in 2022 (2022-R-98629), Scientific Research Start-up Foundation of Nanjing Vocational University of Industry Technology (YK21-05-04), Research Project of Chinese National Light Industry Vocational Education and Teaching Steering Committee in 2021 (QGHZW2021066).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The datasets analysed during the current study are all available in the public server and can be downloaded freely. Details of these datasets are provided in the Additional file <xref rid="MOESM1" ref-type="media">1</xref>. The source code of the current study is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/haichangyao/SparkGC">https://github.com/haichangyao/SparkGC</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par48">Not applicable.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par49">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par50">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pathak</surname>
            <given-names>GA</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Miller-Fleming</surname>
            <given-names>TW</given-names>
          </name>
          <name>
            <surname>Wendt</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Ehsan</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hou</surname>
            <given-names>KC</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>ZY</given-names>
          </name>
          <name>
            <surname>Gopalan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Yengo</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Integrative genomic analyses identify susceptibility genes underlying COVID-19 hospitalization</article-title>
        <source>Nat Commun</source>
        <year>2021</year>
        <volume>12</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1038/s41467-021-24824-z</pub-id>
        <pub-id pub-id-type="pmid">33397941</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Minirmd: accurate and fast duplicate removal tool for short reads via multiple minimizers</article-title>
        <source>Bioinformatics</source>
        <year>2021</year>
        <volume>37</volume>
        <issue>11</issue>
        <fpage>1604</fpage>
        <lpage>1606</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa915</pub-id>
        <pub-id pub-id-type="pmid">33112385</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Hamming-shifting graph of genomic short reads: efficient construction and its application for compression</article-title>
        <source>Plos Comput Biol</source>
        <year>2021</year>
        <volume>17</volume>
        <issue>7</issue>
        <fpage>e1009229</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1009229</pub-id>
        <pub-id pub-id-type="pmid">34280186</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Light-weight reference-based compression of FASTQ data</article-title>
        <source>BMC Bioinform</source>
        <year>2015</year>
        <volume>16</volume>
        <issue>1</issue>
        <fpage>188</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-015-0628-7</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wen</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Chu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>LW-FQZip 2: a parallelized reference-based compression of FASTQ files</article-title>
        <source>BMC Bioinform</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>179</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-017-1588-x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Sketch distance-based clustering of chromosomes for large genome database compression</article-title>
        <source>BMC Genomics</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>Suppl 10</issue>
        <fpage>978</fpage>
        <pub-id pub-id-type="doi">10.1186/s12864-019-6310-0</pub-id>
        <pub-id pub-id-type="pmid">31888458</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Allowing mutations in maximal matches boosts genome compression performance</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <issue>18</issue>
        <fpage>4675</fpage>
        <lpage>4681</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa572</pub-id>
        <pub-id pub-id-type="pmid">33118018</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ceri</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Pinoli</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Data science for genomic data management: challenges, resources</article-title>
        <source>Exp SN Comput Sci</source>
        <year>2020</year>
        <volume>1</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1007/s42979-019-0007-y</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Pasquale De Luca SF, Luca Landolfi, Annabella Di Mauro. Distributed genomic compression in MapReduce paradigm. In: International conference on internet and distributed computing systems (IDCS). 2019; Springer: 369–378.</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dean</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ghemawat</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>MapReduce: a flexible data processing tool</article-title>
        <source>Commun ACM</source>
        <year>2010</year>
        <volume>53</volume>
        <issue>1</issue>
        <fpage>72</fpage>
        <lpage>77</lpage>
        <pub-id pub-id-type="doi">10.1145/1629175.1629198</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zaharia</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Chowdhury</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Franklin</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Shenker</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Stoica</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Spark: cluster computing with working sets</article-title>
        <source>HotCloud</source>
        <year>2010</year>
        <volume>10</volume>
        <issue>10</issue>
        <fpage>95</fpage>
        <lpage>105</lpage>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Ji Y, Fang H, Yao H, He J, Chen S, Li K, Liu S. FastDRC: Fast and Scalable Genome Compression Based on Distributed and Parallel Processing. In: International conference on algorithms and architectures for parallel processing (ICA3PP). 2020; Springer: 313–319.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Christley</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Human genomes as email attachments</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <issue>2</issue>
        <fpage>274</fpage>
        <lpage>275</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btn582</pub-id>
        <pub-id pub-id-type="pmid">18996942</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hosseini</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pratas</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Pinho</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A survey on data compression methods for biological sequences</article-title>
        <source>Information</source>
        <year>2016</year>
        <volume>7</volume>
        <issue>4</issue>
        <fpage>56</fpage>
        <lpage>76</lpage>
        <pub-id pub-id-type="doi">10.3390/info7040056</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Law</surname>
            <given-names>BN-F</given-names>
          </name>
        </person-group>
        <article-title>Application of signal processing for DNA sequence compression</article-title>
        <source>IET Signal Process</source>
        <year>2019</year>
        <volume>13</volume>
        <issue>6</issue>
        <fpage>569</fpage>
        <lpage>580</lpage>
        <pub-id pub-id-type="doi">10.1049/iet-spr.2018.5392</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hernaez</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pavlichin</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Weissman</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ochoa</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Genomic data compression</article-title>
        <source>Ann Rev Biomed Data Sci</source>
        <year>2019</year>
        <volume>2</volume>
        <fpage>19</fpage>
        <lpage>37</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev-biodatasci-072018-021229</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wandelt</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Leser</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>FRESCO: referential compression of highly similar sequences</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinf</source>
        <year>2013</year>
        <volume>10</volume>
        <issue>5</issue>
        <fpage>1275</fpage>
        <lpage>1288</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2013.122</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>CoGI: towards compressing genomes as an image</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinf</source>
        <year>2015</year>
        <volume>12</volume>
        <issue>6</issue>
        <fpage>1275</fpage>
        <lpage>1285</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2015.2430331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cheng</surname>
            <given-names>K-O</given-names>
          </name>
          <name>
            <surname>Law</surname>
            <given-names>N-F</given-names>
          </name>
          <name>
            <surname>Siu</surname>
            <given-names>W-C</given-names>
          </name>
        </person-group>
        <article-title>Clustering-based compression for population DNA sequences</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinf</source>
        <year>2017</year>
        <volume>16</volume>
        <issue>1</issue>
        <fpage>208</fpage>
        <lpage>221</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2017.2762302</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Kuruppu S, Puglisi SJ, Zobel J. Relative Lempel-Ziv compression of genomes for large-scale storage and retrieval. In: International conference on string processing and information retrieval (SPIRE). 2010; Springer, pp 201–206.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yao</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>HRCM: an efficient hybrid referential compression method for genomic big data</article-title>
        <source>Biomed Res Int</source>
        <year>2019</year>
        <volume>2019</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yao</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Parallel compression for large collections of genomes</article-title>
        <source>Concurr Comput Pract Exp</source>
        <year>2022</year>
        <volume>34</volume>
        <issue>2</issue>
        <fpage>e6339</fpage>
        <pub-id pub-id-type="doi">10.1002/cpe.6339</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mushtaq</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ahmed</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Al-Ars</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>SparkGA2: production-quality memory-efficient Apache Spark based genome analysis framework</article-title>
        <source>PLoS ONE</source>
        <year>2019</year>
        <volume>14</volume>
        <issue>12</issue>
        <fpage>e0224784</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0224784</pub-id>
        <pub-id pub-id-type="pmid">31805063</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sherry</surname>
            <given-names>ST</given-names>
          </name>
          <name>
            <surname>Ward</surname>
            <given-names>M-H</given-names>
          </name>
          <name>
            <surname>Kholodov</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Baker</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Phan</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Smigielski</surname>
            <given-names>EM</given-names>
          </name>
          <name>
            <surname>Sirotkin</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>dbSNP: the NCBI database of genetic variation</article-title>
        <source>Nucleic Acids Res</source>
        <year>2001</year>
        <volume>29</volume>
        <issue>1</issue>
        <fpage>308</fpage>
        <lpage>311</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/29.1.308</pub-id>
        <pub-id pub-id-type="pmid">11125122</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wandelt</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Leser</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>Adaptive efficient compression of genomes</article-title>
        <source>Algorithms Mol Biol</source>
        <year>2012</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>30</fpage>
        <lpage>39</lpage>
        <pub-id pub-id-type="doi">10.1186/1748-7188-7-30</pub-id>
        <pub-id pub-id-type="pmid">23146997</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Deorowicz</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Danek</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Niemiec</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>GDC 2: compression of large collections of genomes</article-title>
        <source>Sci Rep</source>
        <year>2015</year>
        <volume>5</volume>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1038/srep11565</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ochoa</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Hernaez</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Weissman</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>iDoComp: a compression scheme for assembled genomes</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>31</volume>
        <issue>5</issue>
        <fpage>626</fpage>
        <lpage>633</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu698</pub-id>
        <pub-id pub-id-type="pmid">25344501</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Saha</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Rajasekaran</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>NRGC: a novel referential genome compression algorithm</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <issue>22</issue>
        <fpage>3405</fpage>
        <lpage>3412</lpage>
        <?supplied-pmid 27485445?>
        <pub-id pub-id-type="pmid">27485445</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>High-speed and high-ratio referential genome compression</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <issue>21</issue>
        <fpage>3364</fpage>
        <lpage>3372</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx412</pub-id>
        <pub-id pub-id-type="pmid">28651329</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shi</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>High efficiency referential genome compression algorithm</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>35</volume>
        <issue>12</issue>
        <fpage>2058</fpage>
        <lpage>2065</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty934</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Fast detection of maximal exact matches via fixed sampling of query K-mers and Bloom filtering of index K-mers</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <issue>22</issue>
        <fpage>4560</fpage>
        <lpage>4567</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz273</pub-id>
        <pub-id pub-id-type="pmid">30994891</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Zhao Y, Hu F, Chen H. An adaptive tuning strategy on spark based on in-memory computation characteristics. In: International conference on advanced communication technology. 2016; IEEE:1.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <collab>Consortium TGP</collab>
        </person-group>
        <article-title>An integrated map of genetic variation from 1092 human genomes</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>491</volume>
        <fpage>56</fpage>
        <lpage>65</lpage>
        <pub-id pub-id-type="doi">10.1038/nature11632</pub-id>
        <pub-id pub-id-type="pmid">23128226</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ahn</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>TH</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ghang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>BC</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>SY</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>WY</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The first Korean genome sequence and analysis: full genome sequencing for a socio-ethnic group</article-title>
        <source>Genome Res</source>
        <year>2009</year>
        <volume>19</volume>
        <issue>9</issue>
        <fpage>1622</fpage>
        <lpage>1629</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.092197.109</pub-id>
        <pub-id pub-id-type="pmid">19470904</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>The diploid genome sequence of an Asian individual</article-title>
        <source>Nature</source>
        <year>2008</year>
        <volume>456</volume>
        <issue>7218</issue>
        <fpage>60</fpage>
        <lpage>68</lpage>
        <pub-id pub-id-type="doi">10.1038/nature07484</pub-id>
        <pub-id pub-id-type="pmid">18987735</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Levy</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sutton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ng</surname>
            <given-names>PC</given-names>
          </name>
          <name>
            <surname>Feuk</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Halpern</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Walenz</surname>
            <given-names>BP</given-names>
          </name>
          <name>
            <surname>Axelrod</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kirkness</surname>
            <given-names>EF</given-names>
          </name>
          <name>
            <surname>Denisov</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>The diploid genome sequence of an individual human</article-title>
        <source>PLoS Biol</source>
        <year>2007</year>
        <volume>5</volume>
        <issue>10</issue>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pbio.0050254</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fernando</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Vinicius</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Sebastian</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Ulf</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Alysson</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Luis</surname>
            <given-names>HE</given-names>
          </name>
        </person-group>
        <article-title>On-demand indexing for referential compression of dna sequences</article-title>
        <source>PLoS ONE</source>
        <year>2015</year>
        <volume>10</volume>
        <issue>7</issue>
        <fpage>e0132460</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0132460</pub-id>
        <pub-id pub-id-type="pmid">26146838</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
