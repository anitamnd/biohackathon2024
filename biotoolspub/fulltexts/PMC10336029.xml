<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10336029</article-id>
    <article-id pub-id-type="pmid">37421399</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btad420</article-id>
    <article-id pub-id-type="publisher-id">btad420</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Applications Note</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Gene Expression</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>LSMMD-MA: scaling multimodal data integration for single-cell genomics data analysis</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0521-621X</contrib-id>
        <name>
          <surname>Meng-Papaxanthos</surname>
          <given-names>Laetitia</given-names>
        </name>
        <aff><institution>Google Research, Brain Team, Google</institution>, <addr-line>Brandschenkestrasse 110</addr-line>, Zurich 8002, <country country="CH">Switzerland</country></aff>
        <xref rid="btad420-cor1" ref-type="corresp"/>
        <!--lpapaxanthos@google.com-->
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7630-1251</contrib-id>
        <name>
          <surname>Zhang</surname>
          <given-names>Ran</given-names>
        </name>
        <aff><institution>Department of Genome Sciences, University of Washington</institution>, <addr-line>3720 15th Ave NE</addr-line>, Seattle, WA 98195, <country country="US">United States</country></aff>
        <aff><institution>eScience Institute, University of Washington</institution>, <addr-line>3910 15th Ave NE</addr-line>, Seattle, WA 98195, <country country="US">United States</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-0759-9063</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Gang</given-names>
        </name>
        <aff><institution>Department of Genome Sciences, University of Washington</institution>, <addr-line>3720 15th Ave NE</addr-line>, Seattle, WA 98195, <country country="US">United States</country></aff>
        <aff><institution>eScience Institute, University of Washington</institution>, <addr-line>3910 15th Ave NE</addr-line>, Seattle, WA 98195, <country country="US">United States</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1934-0588</contrib-id>
        <name>
          <surname>Cuturi</surname>
          <given-names>Marco</given-names>
        </name>
        <aff><institution>Google Research, Brain Team, Google</institution>, <addr-line>8 Rue de Londres</addr-line>, Paris 75009, <country country="FR">France</country></aff>
        <aff><institution>Apple ML Research, Apple</institution>, <addr-line>7 Av. d’Iéna</addr-line>, Paris 75116, <country country="FR">France</country></aff>
        <xref rid="btad420-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-7283-4715</contrib-id>
        <name>
          <surname>Noble</surname>
          <given-names>William Stafford</given-names>
        </name>
        <aff><institution>Department of Genome Sciences, University of Washington</institution>, <addr-line>3720 15th Ave NE</addr-line>, Seattle, WA 98195, <country country="US">United States</country></aff>
        <aff><institution>Paul G. Allen School of Computer Science and Engineering, University of Washington</institution>, <addr-line>185 E Stevens Way NE</addr-line>, Seattle, WA 98195, <country country="US">United States</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-9510-8441</contrib-id>
        <name>
          <surname>Vert</surname>
          <given-names>Jean-Philippe</given-names>
        </name>
        <aff><institution>Google Research, Brain Team, Google</institution>, <addr-line>8 Rue de Londres</addr-line>, Paris 75009, <country country="FR">France</country></aff>
        <aff><institution>Owkin, Inc.</institution>, <addr-line>14/16 Bd Poissonnière</addr-line>, Paris 75009, <country country="FR">France</country></aff>
        <xref rid="btad420-cor1" ref-type="corresp"/>
        <!--jean-philippe.vert@owkin.com-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Mathelier</surname>
          <given-names>Anthony</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btad420-cor1">Corresponding authors. Google Research, Brain Team, Google, Zurich 8002, Switzerland. E-mail: <email>lpapaxanthos@google.com</email> (L.M.-P.); Google Research, Brain Team, Google, Paris 75009, France. E-mail: <email>jean-philippe.vert@owkin.com</email> (J.-P.V.)</corresp>
      <fn id="btad420-FM1">
        <p>Work done while at Google.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <month>7</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-07-08">
      <day>08</day>
      <month>7</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>08</day>
      <month>7</month>
      <year>2023</year>
    </pub-date>
    <volume>39</volume>
    <issue>7</issue>
    <elocation-id>btad420</elocation-id>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>3</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>25</day>
        <month>6</month>
        <year>2023</year>
      </date>
      <date date-type="editorial-decision">
        <day>26</day>
        <month>6</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>11</day>
        <month>7</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btad420.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Modality matching in single-cell omics data analysis—i.e. matching cells across datasets collected using different types of genomic assays—has become an important problem, because unifying perspectives across different technologies holds the promise of yielding biological and clinical discoveries. However, single-cell dataset sizes can now reach hundreds of thousands to millions of cells, which remain out of reach for most multimodal computational methods.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We propose <monospace>LSMMD-MA</monospace>, a large-scale Python implementation of the <monospace>MMD-MA</monospace> method for multimodal data integration. In <monospace>LSMMD-MA</monospace>, we reformulate the <monospace>MMD-MA</monospace> optimization problem using linear algebra and solve it with KeOps, a CUDA framework for symbolic matrix computation in Python. We show that <monospace>LSMMD-MA</monospace> scales to a million cells in each modality, two orders of magnitude greater than existing implementations.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p><monospace>LSMMD-MA</monospace> is freely available at <ext-link xlink:href="https://github.com/google-research/large_scale_mmdma" ext-link-type="uri">https://github.com/google-research/large_scale_mmdma</ext-link> and archived at <ext-link xlink:href="https://doi.org/10.5281/zenodo.8076311" ext-link-type="uri">https://doi.org/10.5281/zenodo.8076311</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NIH</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>UM1 HG011531</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="3"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Modality matching in single-cell genomics data analysis can enhance our understanding of the relationships between cellular modalities and help us resolve cell states. In this problem, single-cell measurements collected using two or more different types of assays are projected into a shared space or are otherwise matched across modalities, with the goal of achieving insights into the joint multimodal dataset. Most existing multimodal models rely on learning cell representations in each modality in a joint low-dimensional space (<xref rid="btad420-B21" ref-type="bibr">Welch <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btad420-B2" ref-type="bibr">Cao <italic toggle="yes">et al.</italic> 2020</xref>, <xref rid="btad420-B9" ref-type="bibr">Jin <italic toggle="yes">et al.</italic> 2020</xref>, <xref rid="btad420-B20" ref-type="bibr">Stark <italic toggle="yes">et al.</italic> 2020</xref>, <xref rid="btad420-B5" ref-type="bibr">Gayoso <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btad420-B8" ref-type="bibr">Hao <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btad420-B16" ref-type="bibr">Raimundo <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btad420-B3" ref-type="bibr">Cao and Gao 2022</xref>). <monospace>MMD-MA</monospace> (<xref rid="btad420-B10" ref-type="bibr">Liu <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btad420-B18" ref-type="bibr">Singh <italic toggle="yes">et al.</italic> 2020</xref>) is one such method that has shown promising results on datasets containing several thousand cells in each modality. However, thanks to new single-cell technologies, the size of single-cell datasets has increased significantly in the past 2 years, now reaching several hundreds of thousands to millions of cells (<xref rid="btad420-B14" ref-type="bibr">Papatheodorou <italic toggle="yes">et al.</italic> 2020</xref>, <xref rid="btad420-B8" ref-type="bibr">Hao <italic toggle="yes">et al.</italic> 2021</xref>, <xref rid="btad420-B17" ref-type="bibr">Rozenblatt-Rosen <italic toggle="yes">et al.</italic> 2021</xref>). These datasets cannot be analysed by current implementations of <monospace>MMD-MA</monospace> due to memory issues.</p>
    <p>More precisely, <monospace>MMD-MA</monospace> (<xref rid="btad420-B10" ref-type="bibr">Liu <italic toggle="yes">et al.</italic> 2019</xref>) is a multimodal approach that maps each cell in each modality to a shared, low-dimensional representation space. The linear mappings from the input spaces to the representation space are learned by minimizing an objective function composed of several terms: (i) a “matching” term based on the squared maximum mean discrepancy (MMD) with a Gaussian radial basis function (RBF) kernel to ensure that the different modalities overlap in the representation space, (ii) two “noncollapsing” penalties to prevent trivial solutions, and (iii) two “distortion” penalties to ensure that as much information from the input data as possible is captured in the shared representation. More details about the method are provided <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix Section A</xref>. However, current implementations of <monospace>MMD-MA</monospace> (<xref rid="btad420-B10" ref-type="bibr">Liu <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btad420-B18" ref-type="bibr">Singh <italic toggle="yes">et al.</italic> 2020</xref>) scale quadratically as a function of the number of cells in memory and runtime, which is prohibitive for datasets with more than a few thousand samples (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Table SA4</xref>).</p>
    <p>To increase the scalibility of <monospace>MMD-MA</monospace>, we introduce <monospace>LSMMD-MA</monospace>, a reformulation and PyTorch implementation of <monospace>MMD-MA</monospace> that overcomes the memory explosion issue. To achieve this, we (i) reformulate <monospace>MMD-MA</monospace>’s optimization problem in the primal, which is beneficial when the number of cells is larger than the number of informative features and (ii) implement the MMD matching term with the CUDA-based KeOps library for symbolic matrices (<xref rid="btad420-B4" ref-type="bibr">Charlier <italic toggle="yes">et al.</italic> 2021</xref>), tailored to handle matrices that do not fit in RAM or GPU memory. The resulting algorithm scales only linearly in memory with the number of cells and can handle up to a million cells in each modality.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Reformulating <monospace>MMD-MA</monospace> in the primal</title>
      <p>Let <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (respectively, <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) be the data matrix of the first (respectively, second) modality, where <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (respectively, <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) is the number of cells in the first (respectively, second) modality and <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (respectively, <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>) is the number of features in the first (respectively, second) modality. The goal of <monospace>MMD-MA</monospace> is to learn two mappings from the input spaces <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> to a shared representation space <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. We focus specifically on linear mappings, as in the original publications (<xref rid="btad420-B10" ref-type="bibr">Liu <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btad420-B18" ref-type="bibr">Singh <italic toggle="yes">et al.</italic> 2020</xref>), where mappings are parameterized with dual variables <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> such that the embedding of the first (respectively, second) modality is <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (respectively, <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>). Instead, we equivalently parameterize the mappings by primal variables <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, such that the embedding of the first (respectively, second) modality is <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (respectively, <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>). We can then rewrite the <monospace>MMD-MA</monospace> optimization problem in the primal:
where <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are hyperparameters. Under the assumption that <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi><mml:mo>≫</mml:mo><mml:mi>p</mml:mi><mml:mo>≫</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> for each modality, efficiently implementing the primal loss (Equation 1) scales better than implementing the dual loss, as shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table A2</xref>. The primal loss does not require the computation and storage of the linear kernel matrices <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, which are O(<inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) in time and memory, and the penalty and distortion terms are not O(<inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) in runtime anymore. However, computing the MMD term remains O(<inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) in runtime and memory if we implement it naively. A description of <monospace>MMD-MA</monospace> in the dual and a comparison between the formulations of <monospace>MMD-MA</monospace> and <monospace>LSMMD-MA</monospace> are available in <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix Sections A and B</xref>.</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M1" display="block" overflow="scroll">
          <mml:mrow>
            <mml:mtable>
              <mml:mtr columnalign="left">
                <mml:mtd columnalign="left">
                  <mml:mrow>
                    <mml:mrow>
                      <mml:munder>
                        <mml:mrow>
                          <mml:mi>min</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>W</mml:mi>
                              </mml:mrow>
                              <mml:mi>x</mml:mi>
                            </mml:msub>
                          </mml:mrow>
                          <mml:mo>,</mml:mo>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>W</mml:mi>
                              </mml:mrow>
                              <mml:mi>y</mml:mi>
                            </mml:msub>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:munder>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>L</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mtext>primal</mml:mtext>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>W</mml:mi>
                        </mml:mrow>
                        <mml:mi>x</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo>,</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>W</mml:mi>
                        </mml:mrow>
                        <mml:mi>y</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:munder>
                        <mml:mrow>
                          <mml:mi>min</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>W</mml:mi>
                              </mml:mrow>
                              <mml:mi>x</mml:mi>
                            </mml:msub>
                          </mml:mrow>
                          <mml:mo>,</mml:mo>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>W</mml:mi>
                              </mml:mrow>
                              <mml:mi>y</mml:mi>
                            </mml:msub>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:munder>
                    </mml:mrow>
                    <mml:mo stretchy="false">[</mml:mo>
                    <mml:mi mathvariant="italic">MMD</mml:mi>
                    <mml:mrow>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mo stretchy="false">(</mml:mo>
                            <mml:mi>X</mml:mi>
                            <mml:mrow>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>W</mml:mi>
                                </mml:mrow>
                                <mml:mi>x</mml:mi>
                              </mml:msub>
                            </mml:mrow>
                            <mml:mo>,</mml:mo>
                            <mml:mi>Y</mml:mi>
                            <mml:mrow>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>W</mml:mi>
                                </mml:mrow>
                                <mml:mi>y</mml:mi>
                              </mml:msub>
                            </mml:mrow>
                            <mml:mo stretchy="false">)</mml:mo>
                          </mml:mrow>
                        </mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:msup>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mtd>
              </mml:mtr>
              <mml:mtr columnalign="left">
                <mml:mtd columnalign="left">
                  <mml:mrow>
                    <mml:mo>+</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mo>λ</mml:mo>
                        </mml:mrow>
                        <mml:mn>1</mml:mn>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mtext>pen</mml:mtext>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>W</mml:mi>
                        </mml:mrow>
                        <mml:mi>x</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mtext>pen</mml:mtext>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>W</mml:mi>
                        </mml:mrow>
                        <mml:mi>y</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                    <mml:mo stretchy="false">)</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mo>λ</mml:mo>
                        </mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mtext>dis</mml:mtext>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mi>X</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>W</mml:mi>
                        </mml:mrow>
                        <mml:mi>x</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mtext>dis</mml:mtext>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mi>Y</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>W</mml:mi>
                        </mml:mrow>
                        <mml:mi>y</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                    <mml:mo stretchy="false">)</mml:mo>
                    <mml:mo stretchy="false">]</mml:mo>
                    <mml:mo>,</mml:mo>
                  </mml:mrow>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:mrow>
        </mml:math>
      </disp-formula>
    </sec>
    <sec>
      <title>2.2 Using KeOps</title>
      <p>To overcome the O(<inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>) memory burden of computing the MMD term, we implement it using the CUDA-based Map-Reduce scheme of KeOps (<xref rid="btad420-B4" ref-type="bibr">Charlier <italic toggle="yes">et al.</italic> 2021</xref>). This allows us to compute the MMD term without instantiating the <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> Gaussian RBF kernel in memory, using symbolic matrix computation with O(<italic toggle="yes">n</italic>) memory complexity, as detailed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix Section D</xref>. KeOps therefore optimizes (Equation 1) with a linear memory complexity and also improves runtime by a significant multiplicative factor when the number of samples is &gt;1000.</p>
    </sec>
    <sec>
      <title>2.3 Implementation</title>
      <p>We make four algorithms available, including <monospace>LS-MMDMA</monospace> and three variants: primal formulation without KeOps, dual formulation with KeOps, and dual formulation without KeOps (an efficient implementation of the original algorithm). The code is implemented in PyTorch (<xref rid="btad420-B15" ref-type="bibr">Paszke <italic toggle="yes">et al.</italic> 2019</xref>) and can run on CPU or GPU. The package is open source with an Apache license, available at github.com/google-research/large_scale_mmdma. It is referenced on PyPI and can be installed with the command: <monospace>pip install lsmmdma</monospace>. Details about I/O, command line instructions and tutorials are given in the <monospace>Readme.md</monospace> and in the <monospace>examples</monospace> folder.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results and conclusion</title>
    <p>We tested the scalability of the implementation of <monospace>LSMMD-MA</monospace> (primal formulation with KeOps) against three comparison partners (primal formulation without KeOps, dual formulations with KeOps and dual formulation without KeOps) and against the two original implementations (<xref rid="btad420-B10" ref-type="bibr">Liu <italic toggle="yes">et al.</italic> 2019</xref>, <xref rid="btad420-B18" ref-type="bibr">Singh <italic toggle="yes">et al.</italic> 2020</xref>) which focus on the dual formulation in TensorFlow (<xref rid="btad420-B1" ref-type="bibr">Abadi <italic toggle="yes">et al.</italic> 2016</xref>) and PyTorch (<xref rid="btad420-B15" ref-type="bibr">Paszke <italic toggle="yes">et al.</italic> 2019</xref>), respectively. Additionally, <xref rid="btad420-B18" ref-type="bibr">Singh <italic toggle="yes">et al.</italic> (2020)</xref> proposes to use the linear time approximation of MMD for large numbers of samples (<inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>5000</mml:mn></mml:mrow></mml:math></inline-formula>) (see Lemma 14 in <xref rid="btad420-B7" ref-type="bibr">Gretton <italic toggle="yes">et al.</italic> 2012</xref>).</p>
    <p>We ran all algorithms on simulated datasets of different sizes where the latent space is shaped as a branch (see lsmmdma/data/data_pipeline.py in GitHub and <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix Section G</xref> for more details). All algorithms were run for 500 epochs and a low-dimensional representation of dimension <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>. A V100 GPU (16GB) was used for the experiments. We observe that <monospace>LSMMD-MA</monospace>, using the primal formulation and KeOps, scales to one million cells in each modality, whereas the original implementations runs out of memory for &gt;14 000 cells (<xref rid="btad420-F1" ref-type="fig">Fig. 1</xref>). In the same figure, we can also notice that our dual implementations are faster than the original implementations irrespective of the number of samples. We also show that <monospace>LSMMD-MA</monospace> obtains good accuracies, as measured by the Fraction Of Samples Closer than The True Match (FOSCTTM) (<xref rid="btad420-B10" ref-type="bibr">Liu <italic toggle="yes">et al.</italic> 2019</xref>), on a selection of the simulated datasets (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix Section E</xref>). <monospace>MMD-MA</monospace> and <monospace>LSMMD-MA</monospace> have the same optimal objective values as <monospace>LSMMD-MA</monospace> is a reformulation of <monospace>MMD-MA</monospace>. Furthermore, we show that both algorithms obtain similar FOSCTTM performance on 12 synthetic datasets with varying numbers of features and samples (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix Section B</xref>).</p>
    <fig position="float" id="btad420-F1">
      <label>Figure 1.</label>
      <caption>
        <p>(A) Fastest MMD-MA variant as a function of the number of samples and the number of features. The black region in the top right corner means that all variants ran out of memory. (B) Runtime as a function of number of cells for different implementations of <monospace>MMD-MA</monospace>, when the dimension <italic toggle="yes">p</italic> of the input data varies. The black and green dotted lines with cross markers correspond to the original implementations of <monospace>MMD-MA</monospace> as written by  <xref rid="btad420-B10" ref-type="bibr">Liu <italic toggle="yes">et al.</italic> (2019)</xref> (black) and <xref rid="btad420-B18" ref-type="bibr">Singh <italic toggle="yes">et al.</italic> (2020)</xref> (green). The runtime for different values of <italic toggle="yes">p</italic>, from 100 to 10 000, is shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix Fig. A1</xref>.</p>
      </caption>
      <graphic xlink:href="btad420f1" position="float"/>
    </fig>
    <p>As a proof of principle, we also ran <monospace>LSMMD-MA</monospace> on a real-world CITE-seq dataset containing 90 261 human bone marrow mononuclear cells, with 13 953 gene IDs for the gene expression modality and 134 proteins for the protein marker modality (<xref rid="btad420-B13" ref-type="bibr">Luecken <italic toggle="yes">et al.</italic> 2021</xref>). We obtain an FOSCTTM of 0.22 after 100 000 epochs (10.3 h) with <monospace>LSMMD-MA</monospace>, which would have been infeasible with previous versions of <monospace>MMD-MA</monospace>. More details about the preprocessing of the dataset and the hyperparameters are available in <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix Section G</xref>. We additionally compared <monospace>LSMMD-MA</monospace> with baselines such as Procrustes superimposition (with and without aligned data) (<xref rid="btad420-B6" ref-type="bibr">Gower 1975</xref>), LIGER (PyLiger) (<xref rid="btad420-B11" ref-type="bibr">Liu <italic toggle="yes">et al.</italic> 2020</xref>, <xref rid="btad420-B12" ref-type="bibr">Lu and Welch 2022</xref>) and Harmonic alignment (<xref rid="btad420-B19" ref-type="bibr">Stanley <italic toggle="yes">et al.</italic> 2020</xref>) and show that <monospace>LSMMD-MA</monospace> is competitive (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix Section H</xref> for detail).</p>
    <p>These results suggest that an optimized implementation, exploiting the primal formulation and taking advantage of the KeOps library, are key to building a multimodal model that scales to the size of current single-cell datasets.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btad420_Supplementary_Data</label>
      <media xlink:href="btad420_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>The authors thank the anonymous reviewers for their valuable suggestions.</p>
  </ack>
  <sec>
    <title>Supplementary data</title>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Conflict of interest</title>
    <p>None declared.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the NIH award UM1 HG011531.</p>
  </sec>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>The simulations used in this article are available in GitHub at <ext-link xlink:href="https://github.com/google-research/large_scale_mmdma" ext-link-type="uri">https://github.com/google-research/large_scale_mmdma</ext-link> and the CITE-seq dataset is available from NCBI GEO under accession GSE194122.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btad420-B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Abadi</surname><given-names>M</given-names></string-name>, <string-name><surname>Agarwal</surname><given-names>A</given-names></string-name>, <string-name><surname>Barham</surname><given-names>P</given-names></string-name></person-group><etal>et al</etal> TensorFlow: large-scale machine learning on heterogeneous systems. [Computer software]. <italic toggle="yes">arXiv preprint arXiv:1603.04467</italic>, 2016. <ext-link xlink:href="https://www.tensorflow.org" ext-link-type="uri">https://www.tensorflow.org</ext-link>.</mixed-citation>
    </ref>
    <ref id="btad420-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cao</surname><given-names>K</given-names></string-name>, <string-name><surname>Bai</surname><given-names>X</given-names></string-name>, <string-name><surname>Hong</surname><given-names>Y</given-names></string-name></person-group><etal>et al</etal><article-title>Unsupervised topological alignment for single-cell multi-omics integration</article-title>. <source>Bioinformatics</source><year>2020</year>;<volume>36</volume>:<fpage>i48</fpage>–<lpage>56</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa443</pub-id>.<pub-id pub-id-type="pmid">32657382</pub-id></mixed-citation>
    </ref>
    <ref id="btad420-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cao</surname><given-names>Z-J</given-names></string-name>, <string-name><surname>Gao</surname><given-names>G.</given-names></string-name></person-group><article-title>Multi-omics integration and regulatory inference for unpaired single-cell data with a graph-linked unified embedding framework</article-title>. <source>Nat Biotechnol</source><year>2022</year>;<volume>40</volume>:<fpage>1458</fpage>–<lpage>66</lpage>.<pub-id pub-id-type="pmid">35501393</pub-id></mixed-citation>
    </ref>
    <ref id="btad420-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Charlier</surname><given-names>B</given-names></string-name>, <string-name><surname>Feydy</surname><given-names>J</given-names></string-name>, <string-name><surname>Glaunès</surname><given-names>JA</given-names></string-name></person-group><etal>et al</etal><article-title>Kernel operations on the GPU, with autodiff, without memory overflows</article-title>. <source>J Mach Learn Res</source><year>2021</year>;<volume>22</volume>:<fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation>
    </ref>
    <ref id="btad420-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gayoso</surname><given-names>A</given-names></string-name>, <string-name><surname>Steier</surname><given-names>Z</given-names></string-name>, <string-name><surname>Lopez</surname><given-names>R</given-names></string-name></person-group><etal>et al</etal><article-title>Joint probabilistic modeling of single-cell multi-omic data with totalVI</article-title>. <source>Nat Methods</source><year>2021</year>;<volume>18</volume>:<fpage>272</fpage>–<lpage>82</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-020-01050-x</pub-id>.<pub-id pub-id-type="pmid">33589839</pub-id></mixed-citation>
    </ref>
    <ref id="btad420-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gower</surname><given-names>JC.</given-names></string-name></person-group><article-title>Generalized procrustes analysis</article-title>. <source>Psychometrika</source><year>1975</year>;<volume>40</volume>:<fpage>33</fpage>–<lpage>51</lpage>.</mixed-citation>
    </ref>
    <ref id="btad420-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gretton</surname><given-names>A</given-names></string-name>, <string-name><surname>Borgwardt</surname><given-names>KM</given-names></string-name>, <string-name><surname>Rasch</surname><given-names>MJ</given-names></string-name></person-group><etal>et al</etal><article-title>A kernel two-sample test</article-title>. <source>J Mach Learn Res</source><year>2012</year>;<volume>13</volume>:<fpage>723</fpage>–<lpage>73</lpage>.</mixed-citation>
    </ref>
    <ref id="btad420-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hao</surname><given-names>Y</given-names></string-name>, <string-name><surname>Hao</surname><given-names>S</given-names></string-name>, <string-name><surname>Andersen-Nissen</surname><given-names>E</given-names></string-name></person-group><etal>et al</etal><article-title>Integrated analysis of multimodal single-cell data</article-title>. <source>Cell</source><year>2021</year>;<volume>184</volume>:<fpage>3573</fpage>–<lpage>87.e29</lpage>.<pub-id pub-id-type="pmid">34062119</pub-id></mixed-citation>
    </ref>
    <ref id="btad420-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jin</surname><given-names>S</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>L</given-names></string-name>, <string-name><surname>Nie</surname><given-names>Q.</given-names></string-name></person-group><article-title>scAI: an unsupervised approach for the integrative analysis of parallel single-cell transcriptomic and epigenomic profiles</article-title>. <source>Genome Biol</source><year>2020</year>;<volume>21</volume>:<fpage>1</fpage>–<lpage>19</lpage>. <pub-id pub-id-type="doi">10.1186/s13059-020-1932-8</pub-id>.</mixed-citation>
    </ref>
    <ref id="btad420-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>J</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Singh</surname><given-names>R</given-names></string-name></person-group><etal>et al</etal> Jointly embedding multiple single-cell omics measurements. In: <italic toggle="yes">19th International Workshop on Algorithms in Bioinformatics (WABI 2019)</italic>, volume 143 of <italic toggle="yes">Leibniz International Proceedings in Informatics (LIPIcs)</italic> Niagara Falls, NY, USA, Vol. <volume>10</volume>, p.<fpage>1</fpage>–<lpage>10</lpage>, <year>2019</year>. <pub-id pub-id-type="doi">10.4230/LIPIcs.WABI.2019.10</pub-id>.</mixed-citation>
    </ref>
    <ref id="btad420-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>J</given-names></string-name>, <string-name><surname>Gao</surname><given-names>C</given-names></string-name>, <string-name><surname>Sodicoff</surname><given-names>J</given-names></string-name></person-group><etal>et al</etal><article-title>Jointly defining cell types from multiple single-cell datasets using liger</article-title>. <source>Nat Protoc</source><year>2020</year>;<volume>15</volume>:<fpage>3632</fpage>–<lpage>62</lpage>.<pub-id pub-id-type="pmid">33046898</pub-id></mixed-citation>
    </ref>
    <ref id="btad420-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lu</surname><given-names>L</given-names></string-name>, <string-name><surname>Welch</surname><given-names>JD.</given-names></string-name></person-group><article-title>Pyliger: scalable single-cell multi-omic data integration in python</article-title>. <source>Bioinformatics</source><year>2022</year>;<volume>38</volume>:<fpage>2946</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">35561174</pub-id></mixed-citation>
    </ref>
    <ref id="btad420-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luecken</surname><given-names>MD</given-names></string-name>, <string-name><surname>Burkhardt</surname><given-names>DB</given-names></string-name>, <string-name><surname>Cannoodt</surname><given-names>R</given-names></string-name></person-group><etal>et al</etal> A sandbox for prediction and integration of DNA, RNA, and proteins in single cells. In: J. Vanschoren and S. Yeung (eds.), <italic toggle="yes">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks</italic>, Vol. <volume>1</volume>. Curran, <year>2021</year>.</mixed-citation>
    </ref>
    <ref id="btad420-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Papatheodorou</surname><given-names>I</given-names></string-name>, <string-name><surname>Moreno</surname><given-names>P</given-names></string-name>, <string-name><surname>Manning</surname><given-names>J</given-names></string-name></person-group><etal>et al</etal><article-title>Expression atlas update: from tissues to single cells</article-title>. <source>Nucleic Acids Res</source><year>2020</year>;<volume>48</volume>:<fpage>D77</fpage>–<lpage>83</lpage>.<pub-id pub-id-type="pmid">31665515</pub-id></mixed-citation>
    </ref>
    <ref id="btad420-B15">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Paszke</surname><given-names>A</given-names></string-name>, <string-name><surname>Gross</surname><given-names>S</given-names></string-name>, <string-name><surname>Massa</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal><part-title>Pytorch: an imperative style, high-performance deep learning library</part-title>. In: H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett (eds.), <source>Advances in Neural Information Processing Systems</source><italic toggle="yes">, Vancouver, Canada</italic> Vol. <volume>32</volume>. <publisher-name>Curran Associates, Inc</publisher-name>., <year>2019</year>, <fpage>8024</fpage>–<lpage>35</lpage>.</mixed-citation>
    </ref>
    <ref id="btad420-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raimundo</surname><given-names>F</given-names></string-name>, <string-name><surname>Meng-Papaxanthos</surname><given-names>L</given-names></string-name>, <string-name><surname>Vallot</surname><given-names>C</given-names></string-name></person-group><etal>et al</etal><article-title>Machine learning for single-cell genomics data analysis</article-title>. <source>Curr Opin Syst Biol</source><year>2021</year>;<volume>26</volume>:<fpage>64</fpage>–<lpage>71</lpage>.</mixed-citation>
    </ref>
    <ref id="btad420-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rozenblatt-Rosen</surname><given-names>O</given-names></string-name>, <string-name><surname>Shin</surname><given-names>JW</given-names></string-name>, <string-name><surname>Rood</surname><given-names>JE</given-names></string-name></person-group><etal>et al</etal>; <collab>Human Cell Atlas Standards and Technology Working Group</collab>. <article-title>Building a high-quality human cell atlas</article-title>. <source>Nat Biotechnol</source><year>2021</year>;<volume>39</volume>:<fpage>149</fpage>–<lpage>53</lpage>.<pub-id pub-id-type="pmid">33500565</pub-id></mixed-citation>
    </ref>
    <ref id="btad420-B18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Singh</surname><given-names>R</given-names></string-name>, <string-name><surname>Demetci</surname><given-names>P</given-names></string-name>, <string-name><surname>Bonora</surname><given-names>G</given-names></string-name></person-group><etal>et al</etal> Unsupervised manifold alignment for single-cell multi-omics data. In: <italic toggle="yes">Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics</italic>, p.<fpage>1</fpage>–<lpage>10</lpage>, <year>2020</year>. <pub-id pub-id-type="doi">10.1101/2020.06.13.149195</pub-id>.</mixed-citation>
    </ref>
    <ref id="btad420-B19">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Stanley</surname><given-names>JS</given-names><suffix>III</suffix></string-name>, <string-name><surname>Gigante</surname><given-names>S</given-names></string-name>, <string-name><surname>Wolf</surname><given-names>G</given-names></string-name></person-group><etal>et al</etal> Harmonic alignment. In: <italic toggle="yes">Proceedings of the 2020 SIAM International Conference on Data Mining</italic>, p.<fpage>316</fpage>–<lpage>24</lpage>. SIAM, <year>2020</year>.</mixed-citation>
    </ref>
    <ref id="btad420-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stark</surname><given-names>SG</given-names></string-name>, <string-name><surname>Ficek</surname><given-names>J</given-names></string-name>, <string-name><surname>Locatello</surname><given-names>F</given-names></string-name></person-group><etal>et al</etal>; 12 <collab>Tumor Profiler Consortium</collab>. <article-title>SCIM: universal single-cell matching with unpaired feature sets</article-title>. <source>Bioinformatics</source><year>2020</year>;<volume>36</volume>:<fpage>i919</fpage>–<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa843</pub-id>.<pub-id pub-id-type="pmid">33381818</pub-id></mixed-citation>
    </ref>
    <ref id="btad420-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Welch</surname><given-names>JD</given-names></string-name>, <string-name><surname>Kozareva</surname><given-names>V</given-names></string-name>, <string-name><surname>Ferreira</surname><given-names>A</given-names></string-name></person-group><etal>et al</etal><article-title>Single-cell multi-omic integration compares and contrasts features of brain cell identity</article-title>. <source>Cell</source><year>2019</year>;<volume>177</volume>:<fpage>1873</fpage>–<lpage>87.e17</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2019.05.006</pub-id>.<pub-id pub-id-type="pmid">31178122</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
