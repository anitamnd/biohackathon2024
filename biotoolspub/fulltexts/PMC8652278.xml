<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archivearticle1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">R Soc Open Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">R Soc Open Sci</journal-id>
    <journal-id journal-id-type="publisher-id">RSOS</journal-id>
    <journal-id journal-id-type="hwp">royopensci</journal-id>
    <journal-title-group>
      <journal-title>Royal Society Open Science</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2054-5703</issn>
    <publisher>
      <publisher-name>The Royal Society</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8652278</article-id>
    <article-id pub-id-type="pmid">34909208</article-id>
    <article-id pub-id-type="doi">10.1098/rsos.202182</article-id>
    <article-id pub-id-type="publisher-id">rsos202182</article-id>
    <article-categories>
      <subj-group subj-group-type="discipline-codes">
        <compound-subject>
          <compound-subject-part content-type="code">1008</compound-subject-part>
        </compound-subject>
        <compound-subject>
          <compound-subject-part content-type="code">1001</compound-subject-part>
        </compound-subject>
      </subj-group>
      <subj-group subj-group-type="subject-codes">
        <compound-subject>
          <compound-subject-part content-type="code">175</compound-subject-part>
        </compound-subject>
        <compound-subject>
          <compound-subject-part content-type="code">22</compound-subject-part>
        </compound-subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Mathematics</subject>
      </subj-group>
      <subj-group subj-group-type="type-of-publication">
        <subject>Research Articles</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CLARITY: comparing heterogeneous data using dissimilarity</article-title>
      <alt-title alt-title-type="short">CLARITY: comparing heterogeneous data using dissimilarity</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0002-5311-6213</contrib-id>
        <name>
          <surname>Lawson</surname>
          <given-names>Daniel J.</given-names>
        </name>
        <email>dan.lawson@bristol.ac.uk</email>
        <xref rid="af1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="af2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Solanki</surname>
          <given-names>Vinesh</given-names>
        </name>
        <xref rid="af3" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yanovich</surname>
          <given-names>Igor</given-names>
        </name>
        <xref rid="af4" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dellert</surname>
          <given-names>Johannes</given-names>
        </name>
        <xref rid="af5" ref-type="aff">
          <sup>5</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0001-8678-8852</contrib-id>
        <name>
          <surname>Ruck</surname>
          <given-names>Damian</given-names>
        </name>
        <xref rid="af6" ref-type="aff">
          <sup>6</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Endicott</surname>
          <given-names>Phillip</given-names>
        </name>
        <xref rid="af7" ref-type="aff">
          <sup>7</sup>
        </xref>
      </contrib>
      <aff id="af1">
        <label>
          <sup>1</sup>
        </label>
        <addr-line>Institute of Statistical Sciences, School of Mathematics, <institution>University of Bristol</institution>, Bristol, <country>UK</country></addr-line>
      </aff>
      <aff id="af2">
        <label>
          <sup>2</sup>
        </label>
        <addr-line>Integrative Epidemiology Unit, Population Health Sciences, <institution>University of Bristol</institution>, Bristol, <country>UK</country></addr-line>
      </aff>
      <aff id="af3">
        <label>
          <sup>3</sup>
        </label>
        <addr-line>Independent researcher</addr-line>
      </aff>
      <aff id="af4">
        <label>
          <sup>4</sup>
        </label>
        <addr-line>Department of English and American Studies, <institution>Vienna University</institution>, Vienna, <country>Austria</country></addr-line>
      </aff>
      <aff id="af5">
        <label>
          <sup>5</sup>
        </label>
        <addr-line>Seminar für Sprachwissenschaft; DFG Center ‘Words, Bones, Genes, Tools’, <institution>University of Tübingen</institution>, Tübingen, <country>Germany</country></addr-line>
      </aff>
      <aff id="af6">
        <label>
          <sup>6</sup>
        </label>
        <addr-line>Department of Anthropology, <institution>University of Tennessee</institution>, Knoxville, TN, <country>USA</country></addr-line>
      </aff>
      <aff id="af7">
        <label>
          <sup>7</sup>
        </label>
        <addr-line>Unité Eco-Anthropologie (EA), <institution>Muséum National d’Histoire Naturelle</institution>, 17 place du Trocadero, Paris 75016, <country>France</country></addr-line>
      </aff>
    </contrib-group>
    <author-notes>
      <fn fn-type="other">
        <p>Electronic supplementary material is available online at <uri xlink:href="https://doi.org/10.6084/m9.figshare.c.5733215">https://doi.org/10.6084/m9.figshare.c.5733215</uri>.</p>
      </fn>
    </author-notes>
    <pub-date publication-format="electronic" date-type="pub">
      <day>8</day>
      <month>12</month>
      <year>2021</year>
      <string-date>December 8, 2021</string-date>
    </pub-date>
    <pub-date publication-format="electronic" date-type="collection">
      <month>12</month>
      <year>2021</year>
    </pub-date>
    <volume>8</volume>
    <issue>12</issue>
    <elocation-id>202182</elocation-id>
    <history>
      <date date-type="received">
        <day>4</day>
        <month>12</month>
        <year>2020</year>
        <string-date>December 4, 2020</string-date>
      </date>
      <date date-type="accepted">
        <day>29</day>
        <month>10</month>
        <year>2021</year>
        <string-date>October 29, 2021</string-date>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 The Authors.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Published by the Royal Society under the terms of the Creative Commons Attribution License <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>, which permits unrestricted use, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="rsos.202182.pdf"/>
    <abstract>
      <p>Integrating datasets from different disciplines is hard because the data are often qualitatively different in meaning, scale and reliability. When two datasets describe the same entities, many scientific questions can be phrased around whether the (dis)similarities between entities are conserved across such different data. Our method, CLARITY, quantifies consistency across datasets, identifies where inconsistencies arise and aids in their interpretation. We illustrate this using three diverse comparisons: gene methylation versus expression, evolution of language sounds versus word use, and country-level economic metrics versus cultural beliefs. The non-parametric approach is robust to noise and differences in scaling, and makes only weak assumptions about how the data were generated. It operates by decomposing similarities into two components: a ‘structural’ component analogous to a clustering, and an underlying ‘relationship’ between those structures. This allows a ‘structural comparison’ between two similarity matrices using their predictability from ‘structure’. Significance is assessed with the help of re-sampling appropriate for each dataset. The software, CLARITY, is available as an R package from <uri xlink:href="https://github.com/danjlawson/CLARITY">github.com/danjlawson/CLARITY</uri>.</p>
    </abstract>
    <kwd-group>
      <kwd>linguistics</kwd>
      <x xml:space="preserve">, </x>
      <kwd>visualization</kwd>
      <x xml:space="preserve">, </x>
      <kwd>comparitive statistics</kwd>
    </kwd-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Deutsche Forschungsgemeinschaft</institution>
            <institution-id>http://dx.doi.org/10.13039/501100001659</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>2237</award-id>
        <award-id>391377018</award-id>
      </award-group>
    </funding-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Welcome Trust</institution>
          </institution-wrap>
        </funding-source>
        <award-id>WT104125MA</award-id>
      </award-group>
    </funding-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Horizon 2020 Framework Programme</institution>
            <institution-id>http://dx.doi.org/10.13039/100010661</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>834050</award-id>
        <award-id>873207</award-id>
      </award-group>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <label>1<x xml:space="preserve">. </x></label>
    <title>Introduction</title>
    <p>The need to compare different sources of information about the same subjects arises in most quantitative sciences. With sufficient effort, it is always possible to construct a model that accounts for data of arbitrary complexity. But without this time-consuming work, can we visualize the data to determine whether the different sources describe the same qualitative phenomena?</p>
    <p>Many datasets are best expressed in terms of similarities or differences between subjects, and are frequently compared by plotting the resulting matrices side by side. Examples include the co-evolution of language and culture [<xref rid="RSOS202182C1" ref-type="bibr">1</xref>], as well as genetics and phenotype [<xref rid="RSOS202182C2" ref-type="bibr">2</xref>], which are all linked through their geographical constraints and shared history. Further uses include identifying brain function using neural activity patterns [<xref rid="RSOS202182C3" ref-type="bibr">3</xref>], understanding disease through comparing the expression of genes with biomarkers [<xref rid="RSOS202182C4" ref-type="bibr">4</xref>], toxicology prediction comparing the activation of biological pathways [<xref rid="RSOS202182C5" ref-type="bibr">5</xref>] and understanding bacterial function by comparing nucleotide variation to that of amino acids [<xref rid="RSOS202182C6" ref-type="bibr">6</xref>].</p>
    <p>We describe a new method that is computationally efficient and can be applied whenever similarities or dissimlarities can be defined. We present three diverse examples to demonstrate the potential utility of this method. The first is comparing gene methylation with expression in simulated data containing anomalies; the second compares lexical change with phonetic change data in linguistics, and the third examines the interaction between culture and economics. Beyond providing a new method with extremely wide applicability, this paper aims to focus attention on the problem area of <italic toggle="yes">structural comparisons</italic> in general.</p>
    <sec id="s1a">
      <label>1.1<x xml:space="preserve">. </x></label>
      <title>The purpose of CLARITY</title>
      <p><xref rid="RSOS202182F1" ref-type="fig">Figure 1</xref> is a ‘graphical abstract’ to illustrate what CLARITY is designed to detect. Rather than learning a model, CLARITY identifies features of one dataset that are anomalous in the second—marginalizing out structures present in both. It does this from information on subjects, i.e. labelled entities on which we have data such as countries, languages, genes, etc.—which are matched across datasets. It looks for structures—that can be loosely thought of as clusters or shared variance components—that are present in the second dataset, but were not in the first. It reports by identifying subjects with large residuals that ‘persist’, i.e. cannot be predicted from the first dataset, for a wide range of complexity of representation.
<fig position="float" id="RSOS202182F1"><label>Figure 1<x xml:space="preserve">. </x></label><caption><p>What is CLARITY For? CLARITY compares a <italic toggle="yes">reference</italic> dataset (i) with a <italic toggle="yes">target</italic> dataset (ii–iii) to perform a <italic toggle="yes">structural comparison</italic>. (<italic toggle="yes">a</italic>) <italic toggle="yes">Structure</italic> is defined in terms of the similarity between <italic toggle="yes">subjects</italic> (here letters a–p) that in this example fall into <italic toggle="yes">clusters</italic>. These clusters have a <italic toggle="yes">relationship</italic>, here the distance between clusters. (<italic toggle="yes">b</italic>) The data are quantified as a <italic toggle="yes">similarity matrix</italic> between all subjects. We learn the Structure by minimizing residuals in the reference (<italic toggle="yes">b</italic>-i) and predict the target similarity (<italic toggle="yes">b</italic>-ii,<italic toggle="yes">b</italic>-iii) by relearning the relationship. In (ii) the Structure is the same as the reference but the relationship changes. In (iii), a subject also changes cluster, leading to a structural change. (<italic toggle="yes">c</italic>) This is captured in CLARITY using a residual <italic toggle="yes">persistence chart</italic>. This quantifies how well we predict each subject at a range of representation complexity (here, number of clusters). This allows CLARITY to identify structural change separately to relationship change, for diverse models and data. Box 1 defines the italicized terms and §2.1 discusses this figure.</p></caption><graphic xlink:href="rsos202182f01" position="float"/></fig></p>
      <p>CLARITY works with dissimilarities (and equivalently, similarities), which until they are formally defined (see §4.2), can be thought of as generalizing distances between all pairs of items. Similarities can often be defined even when the data does not form a convenient space, e.g. neural spike trains [<xref rid="RSOS202182C7" ref-type="bibr">7</xref>] or genetic relatedness [<xref rid="RSOS202182C8" ref-type="bibr">8</xref>]. Similarities are more general than covariances and make a richer representation than a tree—all trees can be represented as a distance, but the converse is not true. They can be defined on regular feature matrices, or on richer spaces, and are robust to the inherent complexity of the data. However, the better chosen a similarity measure is, the better empirical performance can be expected.</p>
      <p>CLARITY should have wide application across many disciplines. The paper is written to allow non-specialists to gain insight into the approach and its correct interpretation. Users of the methodology should read the Results and Discussion sections, which include simulated and real examples that should be insightful for specialists and non-specialists of the application area. Further mathematical justification and technical details are available in Methods.</p>
    </sec>
    <sec id="s1b">
      <label>1.2<x xml:space="preserve">. </x></label>
      <title>Overview of comparison approaches</title>
      <p>How different is the information provided about <italic toggle="yes">the same</italic> subjects in two datasets? For what follows, we are interested in the relationship between the subjects, rather than particular features in the datasets, and we assume that we have enough information to build a meaningful similarity matrix between the subjects.</p>
      <p>The gold standard approach involves <italic toggle="yes">generative modelling</italic>, in which the joint model for both datasets is specified. Examples include host–parasite coevolution [<xref rid="RSOS202182C9" ref-type="bibr">9</xref>] and comparing linguistic and genetic data [<xref rid="RSOS202182C10" ref-type="bibr">10</xref>]. However, each analysis is bespoke, requiring an expert modeller able to specify a joint model for the two datasets.</p>
      <p>If the datasets take a matrix form, then <italic toggle="yes">testing</italic> whether two matrices are statistically equivalent is another natural starting point. For this, Mantel’s test [<xref rid="RSOS202182C11" ref-type="bibr">11</xref>] and related approaches [<xref rid="RSOS202182C12" ref-type="bibr">12</xref>], can be used. However, for the sort of scientific investigation that we are considering here, the null hypothesis that the two datasets have ‘the same’ distribution, or, in the case of shared historical processes, are ‘independent’ of each other, can often be rejected <italic toggle="yes">a priori</italic>. We are thus interested in richer comparisons, able to highlight specific subjects that behave differently in two datasets.</p>
      <p>Data can be directly compared by transforming one to look like the other. When applied to matrices, an important class are <italic toggle="yes">Procrustes transformations</italic> [<xref rid="RSOS202182C13" ref-type="bibr">13</xref>], which use rotation, translation and scaling [<xref rid="RSOS202182C14" ref-type="bibr">14</xref>] to perform the maximally achievable matching. Procrustes transformations have been used for testing matrix equality under transformation [<xref rid="RSOS202182C15" ref-type="bibr">15</xref>] and are often combined with initial rank reduction via <italic toggle="yes">Spectral decomposition</italic> for matrix comparison (e.g. [<xref rid="RSOS202182C16" ref-type="bibr">16</xref>]).</p>
      <p>If we are not constructing an explicit model of both datasets, nor testing whether they are identical, then the remaining options revolve around constructing summaries that can be compared. Many methods exist to compare <italic toggle="yes">covariance matrices</italic>. Testing [<xref rid="RSOS202182C17" ref-type="bibr">17</xref>] is again straightforward. Metrics comparing covariance matrices exist [<xref rid="RSOS202182C18" ref-type="bibr">18</xref>], while spectral methods, such as common principal component analysis [<xref rid="RSOS202182C19" ref-type="bibr">19</xref>], allow theoretical statements to be made about the results of a comparison [<xref rid="RSOS202182C20" ref-type="bibr">20</xref>].</p>
      <p>Another important class of summary is <italic toggle="yes">tree-based methods</italic> that represent each dataset as a tree, which can be compared using standard metrics. These include topological distance [<xref rid="RSOS202182C21" ref-type="bibr">21</xref>,<xref rid="RSOS202182C22" ref-type="bibr">22</xref>] and tree-space [<xref rid="RSOS202182C23" ref-type="bibr">23</xref>], and the approach is implemented in popular packages such as ‘phangorn’ [<xref rid="RSOS202182C24" ref-type="bibr">24</xref>] in R. The downside is that handling model uncertainty is difficult, with only some types of tree being stable to small changes in the data [<xref rid="RSOS202182C25" ref-type="bibr">25</xref>]. Often the data are not completely hierarchical—for example, tree-based methods can be misleading when the data have a mixture element to them [<xref rid="RSOS202182C26" ref-type="bibr">26</xref>]. Conversely, whilst mixtures might be compared using fixed-dimensional mixture-based methods [<xref rid="RSOS202182C27" ref-type="bibr">27</xref>,<xref rid="RSOS202182C28" ref-type="bibr">28</xref>], this can be misleading when the data have an hierarchical element to them [<xref rid="RSOS202182C29" ref-type="bibr">29</xref>].</p>
      <p>With CLARITY we are addressing scientific questions that relate to which similarity structures are present in two datasets. There are other scientific questions that might be asked. For example, canonical correlation analysis (CCA) [<xref rid="RSOS202182C30" ref-type="bibr">30</xref>,<xref rid="RSOS202182C31" ref-type="bibr">31</xref>] and related approaches can be applied on datasets with matched features, as in e.g. ecology [<xref rid="RSOS202182C32" ref-type="bibr">32</xref>] and machine learning [<xref rid="RSOS202182C33" ref-type="bibr">33</xref>,<xref rid="RSOS202182C34" ref-type="bibr">34</xref>]. CCA addresses the question of which features in one dataset are important for understanding another. Because of this focus on features, CCA cannot be used directly in any of the simulations or real datasets that we consider below. Qualitatively, this is because the datasets can match perfectly if the number of features is higher than the number of subjects.</p>
    </sec>
  </sec>
  <sec id="s2">
    <label>2<x xml:space="preserve">. </x></label>
    <title>Results</title>
    <sec id="s2a">
      <label>2.1<x xml:space="preserve">. </x></label>
      <title>High-level view of CLARITY for comparing data from different sources</title>
      <p>This section contains a high-level mathematical description of the sort of comparison CLARITY is useful for. Technical mathematics is left for the Methods, §4.2, but at a high level, the key concepts are given in <xref rid="RSOS202182BX1" ref-type="boxed-text">box 1</xref>.</p>
      <boxed-text id="RSOS202182BX1" position="float">
        <label>Box 1:</label>
        <caption>
          <title>Important concepts in CLARITY.</title>
        </caption>
        <p><bold>Similarity</bold><italic toggle="yes"><bold>Y</bold></italic><bold>:</bold> A <italic toggle="yes">d</italic> × <italic toggle="yes">d</italic> matrix comparing all subjects, for which ‘closer’ subjects have large pairwise values. A typical example is a covariance. CLARITY operates entirely equivalently with dissimilarities for which ‘close’ implies small values; a typical example is the Euclidean distance.</p>
        <p><bold>Structure</bold><bold><italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub></bold><bold>:</bold> A <italic toggle="yes">d</italic> × <italic toggle="yes">k</italic> matrix, providing a representation of each subject in <italic toggle="yes">k</italic> dimensions. The choice of <italic toggle="yes">structure</italic> is the most important choice in CLARITY. It is learned subject to constraints on <italic toggle="yes">A</italic>, conditional on the relationship. Three important Structures are (i) a clustering, with entries of <italic toggle="yes">A</italic> being 0 except when subject <italic toggle="yes">i</italic> is in cluster <italic toggle="yes">k</italic> in which case <italic toggle="yes">A</italic><sub><italic toggle="yes">ik</italic></sub> = 1; (ii) a Mixture: rows of <italic toggle="yes">A</italic><sub><italic toggle="yes">ij</italic></sub> sum to 1 and <italic toggle="yes">A</italic><sub><italic toggle="yes">ij</italic></sub> are non-negative; and (iii) an unconstrained subspace, in which case <italic toggle="yes">A</italic> is the top <italic toggle="yes">k</italic> eigenvectors, as seen in spectral methods (singular value decomposition, SVD and principal components analysis, PCA).</p>
        <p><bold>Relationship</bold><bold><italic toggle="yes">X</italic><sup>(<italic toggle="yes">k</italic>)</sup></bold><bold>:</bold> The <italic toggle="yes">relationship</italic> between structures depends on the structure. We again learn it by minimizing the error conditional on the structure. In this paper we do not consider constraints. For a clustering, the relationship is the similarity between clusters, or the similarity between latent clusters in a mixture model. It describes the ‘branch lengths’ of a tree. For PCA, the relationship is a rotation, translation and scaling of the matrix of singular values.</p>
        <p><bold>Structural comparison:</bold> The <italic toggle="yes">structure</italic> learned from the reference <italic toggle="yes">Y</italic><sub>1</sub> at each complexity is used to predict the target <italic toggle="yes">Y</italic><sub>2</sub>. <italic toggle="yes">Y</italic><sub>2</sub> may be numerically quite different if the relationships are different. However, as long as the datasets can be predicted in this sense then we say that the matrices are defined to be <italic toggle="yes">structurally similar</italic>; this will happen if the same clusters, mixtures or eigenvalues are important in both datasets and describe the same subjects.</p>
        <p><bold>Residual persistence charts:</bold> We use graphical summaries to present useful scientific insights, focusing on structures that persist over a range of model complexity. These are inspired by the concept of persistent homology from Topological Data Analysis [<xref rid="RSOS202182C35" ref-type="bibr">35</xref>]. When the complexity <italic toggle="yes">k</italic> is sufficiently high, every (full rank) dataset can predict every other, so the focus is on which structures in <italic toggle="yes">Y</italic><sub>2</sub> are explained late in the sequence defined by <italic toggle="yes">Y</italic><sub>1</sub>, i.e. persist. This is captured by the residual persistence <italic toggle="yes">P</italic><sub><italic toggle="yes">ik</italic></sub>, a matrix whose entries for data subject <italic toggle="yes">i</italic> at a complexity <italic toggle="yes">k</italic>, are the sum (over <italic toggle="yes">j</italic>) of the squared residuals.</p>
      </boxed-text>
      <p>CLARITY allows comparison of arbitrary datasets for which the same set of <italic toggle="yes">d</italic> subjects are observed. It represents the similarity of a <italic toggle="yes">reference</italic> dataset <italic toggle="yes">Y</italic><sub>1</sub> non-parametrically using increasingly rich representations of <italic toggle="yes">complexity</italic>
<italic toggle="yes">k</italic> ≤ <italic toggle="yes">d</italic>. At each <italic toggle="yes">k</italic> we learn a <italic toggle="yes">structure</italic>
<italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub>, which is a <italic toggle="yes">d</italic> × <italic toggle="yes">k</italic> matrix, and a <italic toggle="yes">relationship</italic>
<italic toggle="yes">X</italic><sup>(<italic toggle="yes">k</italic>)</sup> between the structures, which is a <italic toggle="yes">k</italic> × <italic toggle="yes">k</italic> matrix. Learning is by minimizing the squared error of the estimate <inline-formula><mml:math id="IM1"><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, which is the sum of the squared residuals.</p>
      <p>
        <bold>Example:</bold>
        <italic toggle="yes">In <xref rid="RSOS202182F1" ref-type="fig">figure 1</xref>b Y<sub>1</sub> is a similarity. <xref rid="RSOS202182F1" ref-type="fig">Figure 1</xref>a treats A<sub>k</sub> as a clustering learned from the Reference Similarity Y<sub><italic toggle="yes">1</italic></sub> (<xref rid="RSOS202182F1" ref-type="fig">figure 1</xref><italic toggle="yes">b</italic>). The relationship X<sup>(k)</sup> is then the similarity between clusters, and the complexity is the number of clusters <italic toggle="yes">k</italic>.</italic>
      </p>
      <p>We make a <italic toggle="yes">structural comparison</italic> between the reference <italic toggle="yes">Y</italic><sub>1</sub> and the target <italic toggle="yes">Y</italic><sub>2</sub>, by keeping the <italic toggle="yes">same structure</italic>
<italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub> but fitting a new relationship <inline-formula><mml:math id="IM2"><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> to make a prediction <inline-formula><mml:math id="IM3"><mml:msubsup><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>. The procedure is:
<list list-type="simple"><list-item><label>1<x xml:space="preserve"> </x></label><p>Construct (dis)<italic toggle="yes">similarity</italic> matrices: both a reference <italic toggle="yes">Y</italic><sub>1</sub> and a target <italic toggle="yes">Y</italic><sub>2</sub>.</p></list-item><list-item><label>2<x xml:space="preserve"> </x></label><p>Learn structure: Learn <inline-formula><mml:math id="IM4"><mml:msubsup><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> in terms of structure <italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub> and relationship <inline-formula><mml:math id="IM5"><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> to best predict <italic toggle="yes">Y</italic><sub>1</sub> for a range of complexities <italic toggle="yes">k</italic> by minimizing the total error, subject to constraints.</p></list-item><list-item><label>3<x xml:space="preserve"> </x></label><p>Predict conditional on structure: Predict <italic toggle="yes">Y</italic><sub>2</sub> using <inline-formula><mml:math id="IM6"><mml:msubsup><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> which uses <italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub> at each complexity <italic toggle="yes">k</italic>.</p></list-item><list-item><label>4<x xml:space="preserve"> </x></label><p>Evaluate prediction: examine the residuals <inline-formula><mml:math id="IM7"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> as a function of <italic toggle="yes">k</italic>. Visually report residuals that are present for many <italic toggle="yes">k</italic>.</p></list-item></list><bold>Example, continued:</bold>
<italic toggle="yes">In <xref rid="RSOS202182F1" ref-type="fig">figure 1</xref>a A<sub>k</sub> are clusterings learned from the reference similarity Y<sub><italic toggle="yes">1</italic></sub> with each of k = 1, …, d clusters. We also learn the relationship <inline-formula><mml:math id="IM8"><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> i.e. similarity between clusters in that dataset. We then predict Y<sub><italic toggle="yes">2</italic></sub> from each set of clusters A<sub>k</sub>, but learn a completely new relationship between clusters <inline-formula><mml:math id="IM9"><mml:msubsup><mml:mi>X</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>. With few clusters, we have a bad representation of Y<sub><italic toggle="yes">1</italic></sub> while with many clusters we overfit it. We are therefore interested in subjects that are poorly explained for many intermediate clusterings, i.e. those with persistent residuals.</italic></p>
      <p>The CLARITY model uses a range of <italic toggle="yes">complexities</italic>
<italic toggle="yes">k</italic> to represent data. The full set of structures therefore quantifies a rich range of models. For example, it can be constrained to a hierarchical clustering (i.e. a tree), and in §2.4 we show that if the data are tree-like, the structure can be interpreted in terms of a ‘soft tree’ (<xref rid="RSOS202182F2" ref-type="fig">figure 2</xref>), which can capture deviations from a strict tree model.
<fig position="float" id="RSOS202182F2"><label>Figure 2<x xml:space="preserve">. </x></label><caption><p>What is complexity in CLARITY? CLARITY uses a non-parametric representation that captures a wide class of model, which is interpretable if the truth is interpretable. This is demonstrated with a hierarchical simulated dataset (see §4.6). (<italic toggle="yes">a</italic>) Subjects are generated as belonging to a cluster under a ‘true structure’. Each feature is shared (with noise) with all descendent subjects in the tree, so may represent any branch of the tree, each of which is assigned a colour. CLARITY models the Similarity <italic toggle="yes">Y</italic> between samples. (<italic toggle="yes">b</italic>) In such a hierarchical dataset, an inferred mixture Structure at each complexity <italic toggle="yes">k</italic> relates to a ‘soft hierarchy’, i.e. a set of mixtures <italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub> with <italic toggle="yes">k</italic> components whose latent clusters approximately represent the branches of the tree. Adding components explains different branches, until eventually only noise is explained. (Parameters: <italic toggle="yes">d</italic> = 100, true <italic toggle="yes">k</italic> = 10, <italic toggle="yes">σ</italic> = 0.005.)</p></caption><graphic xlink:href="rsos202182f02" position="float"/></fig></p>
      <p>
        <bold>Example, continued:</bold>
        <italic toggle="yes">In <xref rid="RSOS202182F1" ref-type="fig">figure 1</xref>c, persistencies at different complexities k are plotted. Observe that at high k, residuals and consequently persistences largely perish. In (ii), we have the same clustering (i.e. structure) but a different relationship, and in c(ii) the large decrease in persistences happens at k = 4, i.e. as soon as complexity k reaches the true number of clusters in the data. In contrast, in c(iii), the data are generated under a clustering that differs by one subject from the structure (i.e. the clustering) from the first dataset. In this condition, persistences remain high until much higher complexity, especially so for the anomalous subject i.</italic>
      </p>
      <p><bold>Key decisions when using CLARITY:</bold> There are just two substantive choices to make when running CLARITY. First, how to construct the similarity matrices <italic toggle="yes">Y</italic><sub>1</sub> and <italic toggle="yes">Y</italic><sub>2</sub>, which may be determined by the data. Second, the choice of structure <italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub>, for which the two main approaches we provide are SVD, which is computationally convenient, or a mixture model, which is more interpretable. We have not found a case for using clusters in practice, as they are not robust. There is one final technical choice, which is the statistical procedure for assessing significance, which we address in §4.4. Good practice for all choices is discussed in §3.1.</p>
    </sec>
    <sec id="s2b">
      <label>2.2<x xml:space="preserve">. </x></label>
      <title>Learning structure in CLARITY</title>
      <p>Two similarity matrices are structurally similar if one can be predicted from the other, using the partial representation we have defined as <italic toggle="yes">structure</italic>. CLARITY is comparing similarity matrices <italic toggle="yes">Y</italic> which requires a <italic toggle="yes">quadratic</italic> model (in <italic toggle="yes">A</italic>) rather than the more familiar linear model
<disp-formula id="RSOS202182UM1"><mml:math id="DM1" display="block"><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>X</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic toggle="yes">A</italic> and <italic toggle="yes">X</italic> are intended to be ‘simpler’ (concretely, <italic toggle="yes">k</italic>-dimensional) approximations to <italic toggle="yes">Y</italic>.</p>
      <p>
        <bold>Example, continued:</bold>
        <italic toggle="yes">When A is a clustering, then rows of A are subjects and columns are clusters. Row i of <italic toggle="yes">A</italic> is a vector with value 1 if subject i is in cluster j. We then predict the similarity Y<sub>ij</sub> with an estimate <inline-formula><mml:math id="IM10"><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> between subject i and j by finding which cluster c<sub>i</sub> and c<sub>j</sub> each is in, and replacing it with the similarity between those clusters, X(c<sub>i</sub>, c<sub>j</sub>).</italic>
      </p>
      <p>We always seek to minimize a loss <italic toggle="yes">L</italic>(<italic toggle="yes">A</italic>, <italic toggle="yes">X</italic>) which is the (squared) error. Setting <inline-formula><mml:math id="IM11"><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, the loss is
<disp-formula id="RSOS202182UM2"><mml:math id="DM2" display="block"><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:munderover><mml:mi>R</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p>If this minimization is unconstrained, then (see Methods §4.2) <italic toggle="yes">A</italic> is the matrix of the (first <italic toggle="yes">k</italic>) eigenvectors and <italic toggle="yes">X</italic> the diagonal matrix of (first <italic toggle="yes">k</italic>) singular values of <italic toggle="yes">Y</italic>. <italic toggle="yes">Y</italic><sub>2</sub> is structurally similar to <italic toggle="yes">Y</italic><sub>1</sub> if it can be predicted using this learned <italic toggle="yes">A</italic> and a new <italic toggle="yes">X</italic><sub>2</sub>. Technically, if <italic toggle="yes">Y</italic><sub>2</sub> is poorly predicted at complexity <italic toggle="yes">k</italic> then it is not close to the subspace spanned by the first <italic toggle="yes">k</italic> principal components of variation in <italic toggle="yes">Y</italic><sub>1</sub>.</p>
      <p>When <italic toggle="yes">A</italic> is constrained to be a mixture (that is, its elements are non-negative and sum to one), <italic toggle="yes">X</italic> describes the similarity between ‘latent clusters’, and the rows of <italic toggle="yes">A</italic> describe mixtures between these clusters. Similarity matrices are hence ‘structurally similar’ if they can be described by the same mixture. This mixture model is interpretable, as we demonstrate in the simulation study. Specifically, a ‘structural difference’ at complexity <italic toggle="yes">k</italic> means that the latent clusters of <italic toggle="yes">Y</italic><sub>2</sub> are not in the <italic toggle="yes">k</italic> most important latent clusters of <italic toggle="yes">Y</italic><sub>1</sub>. Further, the subjects that are poorly predicted, i.e. whose cluster membership is not captured, can be read off from the residual matrix.</p>
      <p>Persistences and residuals decrease with model complexity and are affected by correlations between similarities. Despite the complexities of working with a similarity matrix, in theorem 4.2 (Methods §4.8) we prove that the model is stable in the presence of noise, so that if two datasets were resampled then their residuals based on our notion of structure are not expected to change by a large amount. The theoretical and simulation results together demonstrate that the CLARITY paradigm is performing a meaningful comparison.</p>
    </sec>
    <sec id="s2c">
      <label>2.3<x xml:space="preserve">. </x></label>
      <title>Structure of the examples</title>
      <p>We now present use cases. First we consider a simulation study for a mixture of trees model in §2.4, to demonstrate how CLARITY can be used to identify differences in large-scale structure. In §2.5, we consider simulated data based on methylation and gene expression data to give an example in which CLARITY can be used for anomaly detection. Section 2.6 is a real-data example from linguistics, which is smaller in scale and therefore more subtle in interpretation. Finally, §2.7 examines the relationship between culture and economics at the country scale. The take-home messages from these examples are summarized in §3.1.</p>
    </sec>
    <sec id="s2d">
      <label>2.4<x xml:space="preserve">. </x></label>
      <title>Comparing simulated hierarchical mixtures</title>
      <p>Hierarchical data are common and naturally interpretable using CLARITY. In this section we simulate subjects related by a tree and insert an interpretable structural difference between two datasets. The relationship between structures includes features such as the branch lengths of the tree. The structure itself is defined by the membership of subjects in the clusters. Both are detectable with CLARITY but changing structure creates a much larger effect in the data.</p>
      <sec id="s2d1">
        <label>2.4.1<x xml:space="preserve">. </x></label>
        <title>Simulation model</title>
        <p>The model creates data that are generated with <italic toggle="yes">N</italic> = 100 subjects observed at <italic toggle="yes">L</italic> = 2000 features, grouped into <italic toggle="yes">k</italic> = 10 clusters related via a tree. These data are used as a reference for a CLARITY model. In <italic toggle="yes">Scenario A</italic>, we construct a target by regenerating the tree with the same topology but altering the branch lengths, and resimulating data. In <italic toggle="yes">Scenario B</italic>, we construct a target with the branch lengths changed as in Scenario A, but additionally change the structure <italic toggle="yes">A</italic>. See §4.6 for details.</p>
      </sec>
      <sec id="s2d2">
        <label>2.4.2<x xml:space="preserve">. </x></label>
        <title>Hierarchical mixture inference</title>
        <p><xref rid="RSOS202182F1" ref-type="fig">Figure 1</xref> shows that CLARITY is insensitive to changes in the heatmaps themselves, but remains sensitive to changes in structure. <xref rid="RSOS202182F2" ref-type="fig">Figure 2</xref> shows how the CLARITY mixture model infers detailed structure of <italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub> capturing the clusters present in the data when the tree is ‘cut’ at different heights. <xref rid="RSOS202182F3" ref-type="fig">Figure 3</xref> illustrates how this is achieved.
<fig position="float" id="RSOS202182F3"><label>Figure 3<x xml:space="preserve">. </x></label><caption><p>Interpreting residuals and persistences using simulated data, for <italic toggle="yes">d</italic> = 100 subjects from <italic toggle="yes">k</italic> = 10 clusters. Dissimilarities shown are averaged within clusters, whilst residuals and persistences are summed to create population values. (<italic toggle="yes">a</italic>) Learned tree and dissimilarity matrix. (<italic toggle="yes">b</italic>) Residual persistence chart for the learned data. (<italic toggle="yes">c</italic>) Squared residuals and (<italic toggle="yes">d</italic>) residual persistence, for new simulated data with the same structure as in (<italic toggle="yes">a</italic>). (<italic toggle="yes">e</italic>) Squared residuals and (<italic toggle="yes">f</italic>) residual persistence, for simulated data with a different structure to (<italic toggle="yes">a</italic>), for which some subjects in cluster t2 are a mixture with t9. For (<italic toggle="yes">c</italic>–<italic toggle="yes">f</italic>) lack of significance at <italic toggle="yes">p</italic> = 0.01 is illustrated by drawing a smaller rectangle. (<italic toggle="yes">g</italic>,<italic toggle="yes">h</italic>) Replicated results averaged over 200 simulations. (<italic toggle="yes">g</italic>) ‘excess persistence’ which is the residual persistence of samples in the recipient cluster—i.e. t2 in the tree in (<italic toggle="yes">e</italic>)—with the mean residual persistence of the other samples subtracted. (<italic toggle="yes">h</italic>) Summed squared residuals for different parts of the residual matrix. Shown is the ‘recipient’ compared with the ‘donor’—t9 in (<italic toggle="yes">e</italic>)—as well as the recipient compared with all non-donor samples, and the average residuals for all pairs of samples that were neither recipient nor donor. Simulation settings: <italic toggle="yes">σ</italic> = 0.05, <italic toggle="yes">β</italic> = 0.5, for which in ‘different structure’, half of the recipient cluster is affected by the mixture.</p></caption><graphic xlink:href="rsos202182f03" position="float"/></fig></p>
        <p>When we use the <italic toggle="yes">Y</italic><sub>1</sub> structure (<xref rid="RSOS202182F3" ref-type="fig">figure 3</xref><italic toggle="yes">a</italic>) for prediction of the second similarity matrix <italic toggle="yes">Y</italic><sub>2</sub> (<xref rid="RSOS202182F3" ref-type="fig">figure 3</xref><italic toggle="yes">c</italic>,<italic toggle="yes">e</italic>), several situations may occur. In Scenario A-1, the trees share the same split ordering, and any differences will be completely absorbed by differences between <italic toggle="yes">X</italic><sub>1</sub> and <italic toggle="yes">X</italic><sub>2</sub>. The residuals and persistences of <italic toggle="yes">Y</italic><sub>2</sub> will be distributed as for <italic toggle="yes">Y</italic><sub>1</sub> (see §4.4 for how this is estimated). In scenario A-2, the trees share the same topology but the split order differs. The structures in <italic toggle="yes">Y</italic><sub>2</sub> may not appear in exactly the same order as in <italic toggle="yes">Y</italic><sub>1</sub> but still all appear in the top <italic toggle="yes">k</italic> &lt; <italic toggle="yes">k</italic><sub>max</sub> = 10 structures representing the tree. The residuals and persistences may be larger at lower complexity, as happens in figures <xref rid="RSOS202182F3" ref-type="fig">3</xref><italic toggle="yes">c</italic>,<italic toggle="yes">d</italic> and <xref rid="RSOS202182F1" ref-type="fig">1</xref><italic toggle="yes">c</italic>, but the entire difference can be explained at some complexity threshold. Things are different in Scenario B when <italic toggle="yes">Y</italic><sub>2</sub> has a different topology to <italic toggle="yes">Y</italic><sub>1</sub>—perhaps containing mixtures as in <xref rid="RSOS202182F3" ref-type="fig">figure 3</xref><italic toggle="yes">e</italic>,<italic toggle="yes">f</italic> or new clusters, such as <xref rid="RSOS202182F1" ref-type="fig">figure 1</xref><italic toggle="yes">a</italic>. Only then will important structure be absent until much higher <italic toggle="yes">k</italic> and this will result in high <italic toggle="yes">and persistent</italic> residuals for the affected data (figures <xref rid="RSOS202182F3" ref-type="fig">3</xref><italic toggle="yes">c</italic> and <xref rid="RSOS202182F1" ref-type="fig">1</xref>(iii)).</p>
        <p>The persistence <italic toggle="yes">P</italic> in <xref rid="RSOS202182F3" ref-type="fig">figure 3</xref><italic toggle="yes">f</italic> identifies the clusters that are affected by the structural change: cluster group t2 has significantly inflated <italic toggle="yes">P</italic>. Examining the residuals themselves at a specific <italic toggle="yes">k</italic>, <xref rid="RSOS202182F3" ref-type="fig">figure 3</xref><italic toggle="yes">e</italic> identifies the two clusters affected, which have highest off-diagonal shared residuals. In addition, they show that the ‘recipient’ cluster t2 has consistently high pairwise residuals. The ‘donor’ cluster t9 does not have exceptional residuals overall, but does have the highest pairwise residual with the ‘recipient’ t2. Of note is that low-dimensional representations (<italic toggle="yes">k</italic> = 1, 2) are not helpful because there is high intrinsic variability (i.e. these persistences are large but not significant). We must have a ‘good enough model’ of <italic toggle="yes">Y</italic><sub>1</sub> before it is useful to understand <italic toggle="yes">Y</italic><sub>2</sub>.</p>
        <p>This interpretation is robustly replicated in simulations, as is shown in <xref rid="RSOS202182F3" ref-type="fig">figure 3</xref><italic toggle="yes">g</italic>,<italic toggle="yes">h</italic> for 200 different mixture-of-tree simulations. Specifically,
<list list-type="simple"><list-item><label>—<x xml:space="preserve"> </x></label><p>Persistence is high in ‘recipient’ clusters of <italic toggle="yes">Y</italic><sub>2</sub> containing a mixture of two different signals of the structure found in <italic toggle="yes">Y</italic><sub>1</sub>.</p></list-item><list-item><label>—<x xml:space="preserve"> </x></label><p>Squared residuals of the recipient cluster are high with all clusters that are topologically close to affected subjects. This happens both under the original structure and the ‘new’ structure in which the ‘recipient’ and ‘donor’ clusters are close.</p></list-item><list-item><label>—<x xml:space="preserve"> </x></label><p>Persistence for ‘donor’ clusters is not exceptional, but they are identifiable from their very high residuals with the recipient cluster.</p></list-item></list>In this way, the residuals for tree-like data can be interpreted topologically by first identifying clusters of subjects that experience a high residual persistence. The source of the mixture can be identified from which clusters (that are dissimilar in the reference) have increased residuals with these subjects.</p>
        <p>Whilst the Mixture model allows interpretation of how structural differences can be detected, both the Mixture model and SVD model make comparable predictions. Electronic supplementary material, figure S1 shows that the <italic toggle="yes">same</italic> structural similarity information is learned from the SVD model as in the Mixture model. The models predict <italic toggle="yes">Y</italic><sub>1</sub> with near identical performance. Further, they both agree that the presence of different structure leads to poorer prediction of <italic toggle="yes">Y</italic><sub>2</sub> from <italic toggle="yes">Y</italic><sub>1</sub> for a wide range of <italic toggle="yes">k</italic>.</p>
        <p>In terms of computational complexity, the SVD method is dominated by the SVD (<italic toggle="yes">O</italic>(<italic toggle="yes">d</italic><sup>3</sup>)). For reference, it takes 6.75 min to run our SVD model for <italic toggle="yes">d</italic> = 5000 on a personal laptop, most of which is computing the SVD. The mixture model is dominated by a <italic toggle="yes">d</italic> × <italic toggle="yes">d</italic> matrix inversion (<italic toggle="yes">O</italic>(<italic toggle="yes">d</italic><sup>3</sup>) or better) but is in practice slower as the convergence time of the iterative algorithm scales with <italic toggle="yes">d</italic>.</p>
      </sec>
    </sec>
    <sec id="s2e">
      <label>2.5<x xml:space="preserve">. </x></label>
      <title>Comparing simulated gene methylation to gene expression</title>
      <p>This example focuses on identifying <italic toggle="yes">anomalies</italic>: subjects (here loci) that behave differently in one dataset compared with another. The CLARITY framing using a similarity matrix means that the datasets need not have features in common.</p>
      <p>It is very common in epigenetics to wish to compare different measurements on genes. These are often measured on different scales—methylation is a proportion whilst expression is a positive number—and writing a formal model is hard. Further, the raw data may not be available to each researcher as they are identifiable and sensitive. It is therefore very helpful if summaries, such as those based on similarities, can be compared.</p>
      <p>To emphasize the utility of CLARITY we focus on an epigenetic simulation model from the literature [<xref rid="RSOS202182C36" ref-type="bibr">36</xref>], in which Methylation and Expression data are generated from <italic toggle="yes">independent</italic> case-control experiments that describe the same set of genetic loci, i.e. positions in the genome, that correspond to known genes. These loci are our subjects. We do not claim that this is a realistic model, only that it has received attention.</p>
      <p>Methylation is a chemical process that reduces the accessibility of DNA for transcription, and is therefore negatively associated with gene expression. It can be measured using high-throughput arrays (e.g. [<xref rid="RSOS202182C37" ref-type="bibr">37</xref>,<xref rid="RSOS202182C38" ref-type="bibr">38</xref>]) that target known methylation loci. Similarly, gene expression can be measured using high-throughput arrays by capturing the transcribed RNA [<xref rid="RSOS202182C39" ref-type="bibr">39</xref>]. However, both approaches are subject to a variety of noise, including inter-cell and inter-subject variability, high stochasticity in measurement from the amplification process, and so on. This variability adds to signal from causative factors of interest. The simulation therefore assumes only a −5% average correlation.</p>
      <p>The simulation is for a case-control study in which some people were controls, and others had a tumour. Methylation is simulated in different classes (‘hyper’ and ‘hypo’ methylated loci) which interact with case-control status. Expression is simulated conditional on methylation, with large and random noise. Finally, anomalous loci are chosen in which the relationship between methylation and expression is reversed. See Methods §4.5 for full details.</p>
      <p>The important feature of this set-up is that the set of people considered to quantify methylation and expression are independent. The only thing they share in common is the set of loci on which data are reported. <xref rid="RSOS202182F4" ref-type="fig">Figure 4</xref> illustrates this dataset and highlights the ability of CLARITY to extract the anomalous loci, despite the considerable structure in this dataset.
<fig position="float" id="RSOS202182F4"><label>Figure 4<x xml:space="preserve">. </x></label><caption><p>A simulated Methylation/Expression heatmap comparison from [<xref rid="RSOS202182C36" ref-type="bibr">36</xref>] with added anomalies (see Methods §4.5). (<italic toggle="yes">a</italic>) A reference dataset is used to learn structure; here, simulated Methylation patterns across the genome. (<italic toggle="yes">b</italic>) The structure is used to predict the similarities in the target dataset; here, gene expression data at the same loci, with inserted anomalies. The data need not describe the same features (here, samples with Tumour/Control status). (<italic toggle="yes">c</italic>) <italic toggle="yes">Persistent residuals</italic> over a range of <italic toggle="yes">l</italic> indicate which subjects (here, loci) are structurally different between the target data and the reference data, accurately identifying the anomalies.</p></caption><graphic xlink:href="rsos202182f04" position="float"/></fig></p>
      <p><xref rid="RSOS202182F4" ref-type="fig">Figure 4</xref> shows the two input datasets <italic toggle="yes">Y</italic><sub>1</sub> and <italic toggle="yes">Y</italic><sub>2</sub>, which contain some clear visual structures, and specifically loci that differ between Tumour and Control. It then shows the <italic toggle="yes">persistent residuals</italic> as estimated by CLARITY. A subset of these are ‘anomalies’, one cluster of which (top) has lower expression than expected in Tumour samples, the other has lower expression in Controls. Both are clearly highlighted via the Persistence chart (<xref rid="RSOS202182F4" ref-type="fig">figure 4</xref><italic toggle="yes">c</italic>) over a range of <italic toggle="yes">k</italic>, even though the changes are individually small.</p>
    </sec>
    <sec id="s2f">
      <label>2.6<x xml:space="preserve">. </x></label>
      <title>Two types of language change</title>
      <p>In this example, we also search for <italic toggle="yes">anomalies</italic>, but in contrast to the previous case, our finding here is both surprising and scientifically significant. It therefore merits explicit hypothesis testing, as well as a number of further checks making sure that the effect we find is not spurious. We compare two different, but non-independent types of language change: phonetic, or sound, change; and lexical, or word-replacement, change. There is no question that sound (i.e. phonetic) change and word-replacement (i.e. lexical) change are correlated because they result from a shared historical process: both types of change are inherent to the transmission of language from one generation of speakers to the next. There are two general sources for both phonetic and lexical change. First, each language changes on its own as time proceeds, even in complete isolation from external influences [<xref rid="RSOS202182C40" ref-type="bibr">40</xref>]. Second, languages can influence each other when there are multilingual people, with this process being called <italic toggle="yes">language contact</italic> [<xref rid="RSOS202182C41" ref-type="bibr">41</xref>].</p>
      <p>In the linguistic literature, it is often argued qualitatively that phonetic and lexical change can be unevenly favoured by different social situations of language transmission and contact (e.g. [<xref rid="RSOS202182C42" ref-type="bibr">42</xref>]). Here, we use CLARITY to provide a quantitative test, the first such experiment known to us. <xref rid="RSOS202182F7" ref-type="fig">Figure 7</xref><italic toggle="yes">a</italic>,<italic toggle="yes">b</italic>, demonstrates that the two change types induce closely aligned similarities between language. While this is not surprising, given that the same factors influence both types of linguistic change, we show that, despite this high degree of correlation, there are still detectable ‘structural’ differences between lexical and phonetic changes. Two components of our comparison are crucial: (i) we use a large dataset that enables us to apply CLARITY significance testing and thus derive a statistically credible result; (ii) allowing the CLARITY ‘relationship’ to differ for phonetic and lexical, we effectively permit the rates of both types of change to differ considerably without the matrices becoming structurally dissimilar; in other words, the null hypothesis of CLARITY is broad enough that its rejection would be informative.</p>
      <p>The true history of both phonetic and lexical change is unknown, but can be conceptualized as a graph that consists of a ‘vertical’ inheritance via a tree that captures independent change, and ‘horizontal’ edges that capture change through language contact. Such a graph induces a similarity matrix between languages. Many graphs may induce the same matrix, making direct inference of the history graph impossible in the general case, <xref rid="RSOS202182F5" ref-type="fig">figure 5</xref><italic toggle="yes">a</italic>,<italic toggle="yes">b</italic>. But the similarity matrix does allow us to distinguish between different <italic toggle="yes">classes</italic> of graphs (cf. <xref rid="RSOS202182F5" ref-type="fig">figure 5</xref><italic toggle="yes">a</italic>–<italic toggle="yes">c</italic>). With CLARITY, we can infer changes to the structure of the underlying graph without explicitly learning that graph. With two matrices representing similarity due to phonetic versus lexical change, we can use CLARITY to find out whether phonetic and lexical change go hand in hand. Our null hypothesis is that lexical and phonetic change are aligned, because they ultimately stem from the same interactions between and within speech communities. It is the rejection of this null hypothesis that would be scientifically interesting. This is an appropriate set-up for applying CLARITY, which looks for evidence of differences between two (dis)similarity matrices.
<fig position="float" id="RSOS202182F5"><label>Figure 5<x xml:space="preserve">. </x></label><caption><p>Some history-of-change graphs are not distinguishable from dissimilarity matrices, and others are. Let each of <italic toggle="yes">A</italic>, <italic toggle="yes">B</italic> and <italic toggle="yes">C</italic> be a feature descending from the root node, with a constant rate of change. Then the probability of a mismatch between <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic>, <italic toggle="yes">p</italic><sub><italic toggle="yes">X</italic>≠<italic toggle="yes">Y</italic></sub>, is a weighted sum of terms 1 − e<sup>−<italic toggle="yes">t</italic></sup>, where <italic toggle="yes">t</italic> is the length of a path between <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic> in the graph, and the weights are given by the probabilities a given path was taken, determined by admixture proportions <italic toggle="yes">α</italic> and <italic toggle="yes">β</italic> in (<italic toggle="yes">b</italic>,<italic toggle="yes">c</italic>). Graphs (<italic toggle="yes">a</italic>,<italic toggle="yes">b</italic>) differ, but with specific values for the times of splits and <italic toggle="yes">α</italic> can lead to exactly the same probabilities of mismatch, and thus the same dissimilarity matrices. Graph (<italic toggle="yes">c</italic>), in contrast, leads to a different dissimilarity matrix over any choice of split times and <italic toggle="yes">β</italic> as long as <italic toggle="yes">β</italic> is not 0.</p></caption><graphic xlink:href="rsos202182f05" position="float"/></fig></p>
      <p>In the limit of an infinite number of linguistic features, there exists a ‘true’ matrix induced by the history-of-change graph. But in practice we have to work with matrices estimated from a finite amount of data. To achieve a reasonable estimate, we need many individual features, which in practice requires automatic methods for inferring both phonetic and lexical similarity. Automatic methods incur an inherent error at the level of individual features, but processing many features results in reasonably stable estimates of similarity matrices. Specifically, in cross-validation, we show that however we divide our features into two halves, the halves can predict each other with great success, i.e. carry very similar information.</p>
      <p>Furthermore, an important technical detail is that our automatic lexical-similarity recognition operates on word-to-word phonetic similarity scores. This dependence amplifies the correlation between the lexical and phonetic similarities, thereby working in favour of the null hypothesis. This way, we can be confident that a rejection of the null hypothesis would correspond to a true real-world difference in ‘structure’. Finally, automatic cognate detection cannot distinguish between cognates inherited from the last common ancestor and words that have been borrowed between sister branches after divergence. For recovering the true tree part of the language-family history, this is a drawback; for us, this is a benefit, as it captures language contact relations in the lexical-change data, just as it affects the phonetic-change data.</p>
      <p>Our data come from one of the largest existing historical-linguistic datasets, NorthEuraLex v. 0.9 [<xref rid="RSOS202182C43" ref-type="bibr">43</xref>], which stores phonetic transcriptions of words expressing 1016 different meanings in over a 100 languages. We focus on the 36 Indo-European languages in NorthEuraLex, for which we computed measures of both phonetic and lexical dissimilarity using a state-of-the-art method [<xref rid="RSOS202182C44" ref-type="bibr">44</xref>], as discussed in more detail in §4.7.</p>
      <p>Tables in <xref rid="RSOS202182F6" ref-type="fig">figure 6</xref> illustrate how the phonetic and lexical similarities work in practice, using 10 meanings and words for their expression in English, Danish and German. Historical linguists have established that English and German have a more recent common ancestor than either has with Danish (i.e. in the vertical ‘backbone tree’). After their respective divergences, the three languages experienced complex language-contact patterns, which can be conceptualized as horizontal edges in their historical graphs.
<fig position="float" id="RSOS202182F6"><label>Figure 6<x xml:space="preserve">. </x></label><caption><p>(<italic toggle="yes">a</italic>) Phonetic and lexical word-to-word similarities for 10 meanings in English (EN), Danish (DA) and German (DE). Columns ph:A-B list inferred phonetic similarity between languages <italic toggle="yes">A</italic> and <italic toggle="yes">B</italic> accounting for regular sound correspondences. Columns le:A-B list word-to-word lexical similarity, namely cognacy (i.e. descending from the same ancestral word), inferred via clustering based on phonetic similarity scores as in ph:A-B. See §4.7 for details of the measures. (<italic toggle="yes">b</italic>,<italic toggle="yes">c</italic>) The phonetic and lexical similarity matrices induced by the data in (<italic toggle="yes">a</italic>). (<italic toggle="yes">d</italic>,<italic toggle="yes">e</italic>) Same, induced by the full data.</p></caption><graphic xlink:href="rsos202182f06" position="float"/></fig></p>
      <p>We discuss three rows in <xref rid="RSOS202182F6" ref-type="fig">figure 6</xref> to illustrate the individual-feature diversity behind the general similarities. The meaning BEARD in <xref rid="RSOS202182F6" ref-type="fig">figure 6</xref><italic toggle="yes">a</italic> is straightforward. The English and German words descend from the same ancestral word, experiencing no lexical-replacement events. In linguistic parlance, these words are cognates. This common ancestry means that their phonetic shape descends from one and the same shape of the ancestral word, and thus causes them to be phonetically similar in the real world. Our phonetic-similarity algorithm correctly recovers that information (column ph:EN-DE), and based on this high level of similarity, our lexical cognate-detection algorithm also declares them lexically similar (‘yes’, or 1, in le:EN-DE). The Danish word for BEARD is historically unrelated, and is indeed not similar to either German or English word phonetically. For the meaning HEAD, it is the English and Danish words that are true cognates, and are inferred to be both phonetically and lexically similar by the algorithms. But the German word for HEAD is accidentally much closer phonetically to the Danish word than to the English one. The more the phonetic systems of two languages are alike, the more frequent and pronounced such accidental similarities of unrelated words will be on average. Finally, the meaning GREEN illustrates a different property of phonetic similarity. For GREEN, all three languages use cognate words, and are correctly inferred to do so by the lexical algorithm. But their phonetic similarities to each other differ. The English vowel in ‘green’ is closer to that in German ‘grün’ than Danish ‘grøn’. For cognate words, phonetic similarity depends on how sound changes operated on what was initially one and the same word.</p>
      <p>Schematically, our automatically inferred similarity measures Phon and Lex can be characterized as follows. Lex = TrueLex + <italic toggle="yes">ε</italic><sub>lex</sub>, where TrueLex is the true lexical similarity, and <italic toggle="yes">ε</italic><sub>lex</sub> is the error from the automatic cognacy-detection algorithm. Phon = TrueLex∗unrelated.phon.sim + (1 − TrueLex)∗cognate.phon.sim + <italic toggle="yes">ε</italic><sub>phon</sub>, where we condition on whether the relevant words are related (with TrueLex impressionistically viewed as probability) or not; unrelated.phon.sim and cognate.phon.sim are two types of phonetic word-to-word similarities discussed in the previous paragraph; and <italic toggle="yes">ε</italic><sub>phon</sub> the error of the phonetic algorithm. This informal representation shows how our empirical measures Phon and Lex are even more dependent than the real-world quantities TrueLex, unrelated.phon.sim and cognate.phon.sim. In turn, the latter three are also dependent because they are the result of language-change processes in the same speaker communities.</p>
      <p><xref rid="RSOS202182F7" ref-type="fig">Figure 7</xref><italic toggle="yes">a</italic>,<italic toggle="yes">b</italic> shows our similarity matrices for Phonetic and Lexical, which look very much alike. <xref rid="RSOS202182F7" ref-type="fig">Figure 7</xref><italic toggle="yes">c</italic>,<italic toggle="yes">d</italic> shows the CLARITY analysis, which uncovers structural differences between the matrices in (<italic toggle="yes">a</italic>,<italic toggle="yes">b</italic>). The differences between Phonetic and Lexical are declared <italic toggle="yes">statistically significant</italic> by the CLARITY hypothesis-testing procedure based on resampling. In the figure, significant persistences are marked by a brighter colour and a larger rectangle. They only concern one direction of prediction, namely Phon from Lex, and only some groups of languages; the most affected ones are the Slavic and Scandinavian subfamilies. The differences are also <italic toggle="yes">scientifically significant</italic>, which we determined by analysing the residuals at individual <italic toggle="yes">k</italic>, shown in electronic supplementary material, figure S2: outstanding residuals remained of the order of 0.1–0.2 s.d. (computed from all similarities in the matrix) in individual cells even at the highest <italic toggle="yes">k</italic>s. We believe this is a moderate, yet considerable difference. Our additional checks also included establishing that CLARITY decomposition captures signal rather than noise up to the maximal <italic toggle="yes">k</italic> (electronic supplementary material, figure S3); checking whether self-similarity affects inference (electronic supplementary material, figure S4) examining significances at a stricter <italic toggle="yes">p</italic> = 0.01 (electronic supplementary material, figure S5) and examining the persistence curves for the resampled matrices to make sure there were no anomalies (electronic supplementary material, figures S6 and S7). We conclude that the effect CLARITY finds, captured in the visual summary in <xref rid="RSOS202182F7" ref-type="fig">figure 7</xref><italic toggle="yes">c</italic>,<italic toggle="yes">d</italic>, is a real one.
<fig position="float" id="RSOS202182F7"><label>Figure 7<x xml:space="preserve">. </x></label><caption><p>(<italic toggle="yes">a</italic>,<italic toggle="yes">b</italic>) Phonetic and Lexical similarity matrices. The three large orange-red clusters correspond, left to right, to the Slavic, Germanic and Romance language subfamilies. (<italic toggle="yes">c</italic>,<italic toggle="yes">d</italic>) Persistence diagrams. Statistical significance for cells is visually highlighted by colour saturation and by the size of the background rectangle. Compare the columns for <monospace>oss</monospace> (Ossetian) and <monospace>slv</monospace> (Slovenian) in (<italic toggle="yes">c</italic>). The column for <monospace>oss</monospace> has no significant cells. The column for <monospace>slv</monospace> has significant cells up to <italic toggle="yes">k</italic> = 23 and non-significant ones higher than that.</p></caption><graphic xlink:href="rsos202182f07" position="float"/></fig></p>
      <p>This result obtained by CLARITY is striking because it is based on very subtle distinctions in the observed data. To the bare eye, the Phonetic and Lexical distance matrices <xref rid="RSOS202182F7" ref-type="fig">figure 7</xref><italic toggle="yes">a</italic>,<italic toggle="yes">b</italic> are quite similar, because the two sets of features are not independent of each other. Despite high correlation, CLARITY allowed us to discover a clear difference between the two processes of language change.</p>
      <p>How should we interpret this finding? In terms of the informal representation above, our results about Lex and Phon indicates that the real-world quantities TrueLex on the one hand, and unrelated.phon.sim and cognate.phon.sim on the other, are affected by subtly different historical processes. Given that Phon depends on a superset of real-world quantities that Lex depends on, it is not surprising that we only find the effect in the direction of predicting Phon from Lex. The difference between the two is considerable, but is only discovered with statistical significance for groups of languages closely related to each other, such as the Slavic and Scandinavian languages. This might be a real-world phenomenon: perhaps in the long run, the phonetic and lexical change processes average out and start looking very similar). It could also be an artefact of our algorithms for estimating similarity: as all computational-historical-linguistic algorithms, they inherently tend to be more accurate for more closely related languages, so it might be that we see significant mismatches between Phonetic and Lexical only where our estimates can be sharp enough. We leave solving this question to future research.</p>
      <p>We conclude this case study with the following strong thesis: qualitative linguistic research into small sets of linguistic features should refrain from generalizing its results to the overall workings of different types of change. The statistical mismatch between Phonetic and Lexical that we found was subtle and required a large dataset to be discovered.</p>
    </sec>
    <sec id="s2g">
      <label>2.7<x xml:space="preserve">. </x></label>
      <title>Predicting culture from economics</title>
      <p>In this example, we use the ‘structure’ from economic properties of countries to predict cultural values. The purpose of this case study is to demonstrate how exploratory analysis with CLARITY can be performed on the level of individual anomalous residuals. Overall, country-level economics and culture similarity matrices are significantly different, as is to be expected. But examining the CLARITY residual structure, we identify particularly interesting anomalies.</p>
      <p>For Economics, the World Bank [<xref rid="RSOS202182C45" ref-type="bibr">45</xref>] provides a range of features primarily relating to wealth, inequality, trade and the like for over 200 countries. The World and European Values Surveys (WEVS) [<xref rid="RSOS202182C46" ref-type="bibr">46</xref>,<xref rid="RSOS202182C47" ref-type="bibr">47</xref>] provides features on Culture—that is, people’s attitudes and beliefs regarding topics like religion, prosociality, openness to out-groups, justifiability of homosexuality, political engagement and trust in national institutions. There are 104 countries shared between these data that represent our subjects for this case study. The ‘Cultural’ dissimilarity matrix, <xref rid="RSOS202182F8" ref-type="fig">figure 8</xref><italic toggle="yes">a</italic> is constructed from a dimensionality-reduced dataset of nine cultural factors from WEVS from <italic toggle="yes">ca</italic> 2000 CE, as described by Ruck <italic toggle="yes">et al</italic>. [<xref rid="RSOS202182C48" ref-type="bibr">48</xref>]. For ‘Economics’, we retained the 284 indicators of the World Bank dataset with less than 40% missingness, standardized to unit variance, capped extreme values at 10 s.d., mean imputed, and computed the ‘Economic’ pairwise distance matrix. The raw dissimilarity matrices are shown in electronic supplementary material, figure S8.
<fig position="float" id="RSOS202182F8"><label>Figure 8<x xml:space="preserve">. </x></label><caption><p>Predicting economic properties from cultural properties. (<italic toggle="yes">a</italic>) The dissimilarity <italic toggle="yes">Y</italic><sub>2</sub> for Culture. The order of countries is determined by clustering based on the dissimilarity, so countries that are close on the <italic toggle="yes">x</italic>-axis are, other things being equal, close to each other in the Culture dataset. (<italic toggle="yes">b</italic>) Persistence of Culture predicted from Economics. The dashed line indicates the complexity <italic toggle="yes">k</italic> used in (<italic toggle="yes">c</italic>). (<italic toggle="yes">c</italic>) Residuals for Culture predicted from Economics at <italic toggle="yes">k</italic> = 15.</p></caption><graphic xlink:href="rsos202182f08" position="float"/></fig></p>
      <p>Cultural values are known to predict economic outcomes such as GDP <italic toggle="yes">per capita</italic> [<xref rid="RSOS202182C48" ref-type="bibr">48</xref>,<xref rid="RSOS202182C49" ref-type="bibr">49</xref>], economic inequality [<xref rid="RSOS202182C50" ref-type="bibr">50</xref>] and the balance of agriculture-industrial-service sectors within the economy [<xref rid="RSOS202182C51" ref-type="bibr">51</xref>]. Conversely, a country’s economic performance predicts cultural factors such as religiosity [<xref rid="RSOS202182C52" ref-type="bibr">52</xref>] and book writing [<xref rid="RSOS202182C53" ref-type="bibr">53</xref>]. CLARITY does not presuppose a causal model and therefore the choice of reference and target should not be interpreted as a causal claim without additional information.</p>
      <p>Because we have only nine features for Culture, it cannot be used to identify persistent residuals in Economics with CLARITY as the maximum complexity is <italic toggle="yes">k</italic><sub>max</sub> = min(<italic toggle="yes">k</italic>, <italic toggle="yes">d</italic>) = 9. Instead we ask whether Culture (<xref rid="RSOS202182F8" ref-type="fig">figure 8</xref><italic toggle="yes">a</italic>) can be predicted from Economic data, in which <italic toggle="yes">k</italic><sub>max</sub> = <italic toggle="yes">d</italic> = 104. The persistence chart (<xref rid="RSOS202182F8" ref-type="fig">figure 8</xref><italic toggle="yes">b</italic>) makes it clear that Culture is incompletely predicted from Economics, as almost all Persistences are significant. Conversely, not all residuals are significant. The residual matrix at a specific <italic toggle="yes">k</italic> (<xref rid="RSOS202182F8" ref-type="fig">figure 8</xref><italic toggle="yes">c</italic>) identifies the most important mismatches between Culture and Economics, which may be worthy of further study using other methods and/or data. There are two main classes: anomalies and clusters.</p>
      <p>One anomaly is Andorra, a small European country in the Pyrenees between Spain and France, which belongs culturally, to the cluster of Scandinavian and economically strong Western European countries, <xref rid="RSOS202182F8" ref-type="fig">figure 8</xref><italic toggle="yes">a</italic>. But predicting Culture from Economics in <xref rid="RSOS202182F8" ref-type="fig">figure 8</xref><italic toggle="yes">c</italic>, we see highly negative (blue) residuals between Andorra and its cultural cluster. (Remember that absence of significance simply means we do not have <italic toggle="yes">enough</italic> evidence to reject the null hypothesis based on the available data.) Examining the raw data in electronic supplementary material, figure S8, shows Andorra to be an Economic outlier, clustering with other territories with complex sovereignty that may influence data gathering: Taiwan, the Turkish region of Cyprus, and Northern Ireland.</p>
      <p>Now consider Vietnam and Uzbekistan. Their columns of residuals in <xref rid="RSOS202182F8" ref-type="fig">figure 8</xref><italic toggle="yes">c</italic> are all green, meaning they are farther away on Culture than expected from Economics from all (!) other countries. These two countries are the only ones with all-green residual columns in <xref rid="RSOS202182F8" ref-type="fig">figure 8</xref><italic toggle="yes">c</italic>. Examining the raw data in <xref rid="RSOS202182F8" ref-type="fig">figure 8</xref><italic toggle="yes">a</italic> and electronic supplementary material, figure S8, as well as the Economic PCs in electronic supplementary material, figure S9, we see that they are not particularly close in either Culture or Economics to any other countries, but that there is little relation between which countries are closer in either. Future research may thus want to first confirm that the cultural unusualness is not spurious (based e.g. on coding errors and the like in the datasets as processed), and second, to study what could be driving these anomalous profiles.</p>
      <p>There are countries that share residual structure. Countries in Latin America, such as Argentina, Uruguay and Puerto Rico, have large European descended populations [<xref rid="RSOS202182C54" ref-type="bibr">54</xref>,<xref rid="RSOS202182C55" ref-type="bibr">55</xref>], so are culturally similar to Europe because cultural values percolate along linguistic and religious pathways [<xref rid="RSOS202182C56" ref-type="bibr">56</xref>–<xref rid="RSOS202182C58" ref-type="bibr">58</xref>]. However, Latin American countries have a relatively smaller GDP and have high economic inequality; they are therefore more culturally similar to Europe than Economics predict.</p>
      <p>The Cultural data contain associations that are perhaps surprising; for example, Japan is culturally similar to the Czech Republic. Correcting for Economics, <xref rid="RSOS202182F8" ref-type="fig">figure 8</xref><italic toggle="yes">c</italic>, reveals that this cannot be fully explained away by Economic similarity (the relevant cell is light blue, though not significant). In contrast, Poland and the Czech Republic are neighbouring countries that cluster very differently in Culture, and have large persistences. In this case, Economics may be playing an even smaller role than in the comparison Japan–Czech Republic: the residual Poland–Czech Republic is significant and shows they are much farther from each other on Culture than we could expect from Economics.</p>
      <p>To conclude, we have described how CLARITY analysis identifies interesting anomalies in one dataset that stand out when making predictions based on the other dataset. In this case, the prediction is poor, so considerable follow-up analysis would be needed to draw strong conclusions. The assessment of statistical significance was of secondary importance: we concentrated on systematic patterns in the CLARITY residuals that are likely to be of scientific significance, and the presence of statistically significant individual cells only confirmed the insights. This strategy is particularly useful for datasets based on a relatively small number of features, which will often be the case in social sciences: the statistical power of such data would be in the general case limited.</p>
    </sec>
    <sec id="s2h">
      <label>2.8<x xml:space="preserve">. </x></label>
      <title>Summary of the examples</title>
      <p>We summarize the lessons from the four case studies above. In §2.4, we show how CLARITY behaves on data from a simulation where we purposefully manipulated the ‘structure’ of the compared datasets. This case study builds intuitions about what CLARITY output would look like in different real-world scenarios. In another simulation study in §2.5, we demonstrate that CLARITY correctly identifies anomalous links between two datasets generated by an existing epigenetic model that did not conform to our definition of Structure. In the linguistic study on real-world data in §2.6, we report the first-of-its-kind quantitative finding that phonetic and lexical language change operates in subtly different ways, though both occur in the same human communities and are subject to similar constraining factors. This example is the only one in this paper where explicit hypothesis testing is of primary importance. Finally, in the example on culture and economics in §2.7, we illustrated how CLARITY residuals can be explored to detect interesting anomalies that can then be selected for further in-depth study.</p>
      <p>Overall, we intend this set of examples to show how CLARITY can be a versatile tool for both exploratory data analysis and for explicit hypothesis testing, allowing us to make subtle and fine-grained comparisons between paired datasets stemming from the same subjects, but consisting of very different features, sometimes generated within different scientific disciplines. In the next section, we further discuss best practice and provide advice for interpretation of CLARITY results.</p>
    </sec>
  </sec>
  <sec id="s3">
    <label>3<x xml:space="preserve">. </x></label>
    <title>Discussion</title>
    <sec id="s3a">
      <label>3.1<x xml:space="preserve">. </x></label>
      <title>Best practice</title>
      <p>CLARITY is simple to use and has very few moving parts. There are, however, a few critical decisions:
<list list-type="simple"><list-item><label>1.<x xml:space="preserve"> </x></label><p>designing the input (dis)similarity <italic toggle="yes">Y</italic>, for which the user should take care;</p></list-item><list-item><label>2.<x xml:space="preserve"> </x></label><p>choosing a structure representation <italic toggle="yes">A</italic>, for which the defaults provide good performance;</p></list-item><list-item><label>3.<x xml:space="preserve"> </x></label><p>whether to use null hypothesis statistical testing, to identify statistically significant deviations; and</p></list-item><list-item><label>4.<x xml:space="preserve"> </x></label><p>how to interpret the results.</p></list-item></list></p>
      <sec id="s3a1">
        <label>3.1.1<x xml:space="preserve">. </x></label>
        <title>How to choose a (dis)similarity</title>
        <p>CLARITY can work with rich input data <italic toggle="yes">Y</italic>. However, the simpler the measure, the more reliable the inference will be. Therefore, if you can compute a regular covariance or Euclidean distance, this will be more interpretable.</p>
        <p>The <italic toggle="yes">self-similarity</italic> should be correctly quantified. Broadly, this should mean, ‘if we removed any excess similarity that is unique to the subject, how similar would it be to itself?’ We implemented a ‘diagonal removal’ for this purpose, which sets the diagonal to the next-highest value. This is generally recommended, and was applied in the simulations, Methylation and Culture examples. We also provide a (slower) iterative model in which <inline-formula><mml:math id="IM12"><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>X</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>D</mml:mi></mml:math></inline-formula> is iteratively solved for a diagonal matrix <italic toggle="yes">D</italic>. This was used in the Linguistic example. We did not find any qualitative difference between the methods. Sensitivity analysis will confirm for users whether working with the raw similarity, or diagonal-corrected similarity, impacts inference.</p>
        <p>Whether to <italic toggle="yes">centre</italic> the data is an important decision. Centring changes whether any difference in mean is used in the comparison, and is therefore a modelling decision. We only recommend centring if the mean is known <italic toggle="yes">a priori</italic> to be unimportant or misleading. We only centred the Culture/Economics data, which required feature standardization. Again, we recommend either careful justification of the choice to centre, or sensitivity analysis to confirm it does not matter. Conversely, <italic toggle="yes">scaling</italic> will typically not affect the inference because any change can be accounted for in <italic toggle="yes">X</italic>.</p>
        <p>We note a clear distinction between two classes of data. The first, represented in the Language example, is where every subject is distinct, or the total amount of data is ‘small’. In this case, self-similarity correction is important as it affects <italic toggle="yes">A</italic> closely. Further, persistences have relatively small ranges of <italic toggle="yes">k</italic> to persist over, and therefore inference is subtle. Conversely, the second class has a ‘large’ number of subjects, such as the Methylation example. In this case the effect of the diagonal becomes negligible as the data grows, as can be verified in a sensitivity analysis.</p>
      </sec>
      <sec id="s3a2">
        <label>3.1.2<x xml:space="preserve">. </x></label>
        <title>How to choose the structure</title>
        <p>In general, the unconstrained SVD-based structure is recommended; it scales to large data, and provides the best overall fit. Although the representation of structure <italic toggle="yes">A</italic> is a linear embedding using only the SVD, it is flexible in prediction of <italic toggle="yes">Y</italic><sub>2</sub> due to using <italic toggle="yes">k</italic><sup>2</sup> free parameters in <italic toggle="yes">X</italic><sub>2</sub>.</p>
        <p>We only recommend the mixture model when the data are small, and suspected to lie very close to some interpretable model such as a tree containing mixtures. The residuals should be close to identical between the methods, with the important difference being the interpretation of <italic toggle="yes">A</italic>. We note that unlike in many mixture model inference methods, there is nothing in our loss function that encourages <italic toggle="yes">A</italic> to be close to the boundaries (i.e. some <italic toggle="yes">A</italic><sub><italic toggle="yes">ij</italic></sub> are close to 0 or 1), except the initial conditions. This feature should therefore be used with further validation, as part of the hypothesis generation process.</p>
      </sec>
      <sec id="s3a3">
        <label>3.1.3<x xml:space="preserve">. </x></label>
        <title>Whether to perform null hypothesis statistical testing</title>
        <p>CLARITY is in general a hypothesis generating tool and much of the value can be obtained without any use of statistical testing. For many datasets, statistical power will dominate (such as the Culture example) and many persistences are expected to be significant. We then care about practical significance, and hypothesis testing is somewhat spurious. Similarly, in the Methylation example, clear persistent residuals were observed and these require no testing to confirm anomalies.</p>
        <p>Conversely, especially with small datasets, statistical power is limiting (such as the Language example), in which case we wish to check that a given persistence is significant. The statistical test that we provide compares <italic toggle="yes">Y</italic><sub>2</sub> with the distribution of left-out data from <italic toggle="yes">Y</italic><sub>1</sub>. It does so by first mean centring and scaling each dataset <italic toggle="yes">to the reference</italic>, and then applying the standard CLARITY correction of learning-independent Relationships. This encodes an implicit assumption that the signal and noise are of the same scale, and therefore should only be used if this is plausible.</p>
        <p>Null hypothesis testing is a non-trivial process to implement because it requires that the user is able to either create independent features, or able to provide a set of bootstrapped samples generated on pseudo-independent samples, for both the reference and the test dataset. This comes with a computational cost, as of the order of 200 pseudo-independent replicates are required to confidently reject at the 0.05 level.</p>
        <p>We emphasize that the principal use case is the identification of large persistent residuals which CLARITY will only create where structure has changed, and does not require testing.</p>
      </sec>
      <sec id="s3a4">
        <label>3.1.4<x xml:space="preserve">. </x></label>
        <title>How to interpret persistences and residuals</title>
        <p>We have noted that high residuals at a single complexity <italic toggle="yes">k</italic> might capture only a small change in the importance of a particular relationship, such as two clusters getting further apart. Conversely, CLARITY is easiest to interpret when residuals are large and persist across a range of complexities. This will typically highlight structural anomalies in certain subjects.</p>
        <p>There are two phenomena that require care. The first is that the sums of squared residuals are not monotonic for a particular subject (but are overall). For example, <xref rid="RSOS202182F1" ref-type="fig">figure 1</xref><italic toggle="yes">c</italic>(iii) shows residuals increasing for subject <italic toggle="yes">i</italic> up to complexity 3, and then decreasing; similarly, subject <italic toggle="yes">l</italic> has high residual at complexity 12–13. This occurs when the change to the structure fits other, nearby but different, subjects which can ‘drag’ that subject away from its target.</p>
        <p>The second important phenomenon is that residuals need not be largest in the subject that has changed structure. Because the inferred relationship <italic toggle="yes">X</italic><sub>2</sub> is chosen to minimize the total squared residuals, the model may make nearby subjects have the highest residuals. The important thing to note is that this still creates some residual excess in the target subject, which can be identified by looking at the matrix of residuals (e.g. <xref rid="RSOS202182F8" ref-type="fig">figure 8</xref><italic toggle="yes">c</italic>).</p>
      </sec>
    </sec>
    <sec id="s3b">
      <label>3.2<x xml:space="preserve">. </x></label>
      <title>Conclusion</title>
      <p>CLARITY can be applied to any pair of datasets in which subjects are matched and so can be used in a wide variety of situations. We demonstrated this in very different fields: epigenetics, linguistics and bridging sociology and economics. In these examples, we have recovered differences, supported by well-documented evidence and generated new hypotheses.</p>
      <p>The software requires very little technical knowledge to employ, there are no tuning parameters, and the output can be presented in a simple, interpretable chart we called the residual persistence. This identifies the clusters and subjects that are poorly predicted, and allows interpretation of which other clusters they may share additional structure with. We suggest that the same approach may yield valuable insights when applied to other fields of interest, and that the results will generate hypotheses for further investigation through the application of additional, statistically robust, methods.</p>
      <p>CLARITY is fast, and, for prediction, is limited only by the cost of computing a singular value decomposition. We showed via simulation that the SVD approach is representing the structure in the data very similarly to a mixture model, for which we presented a novel algorithm based on a multiplicative update rule. The mixture model correctly identifies hierarchical structure, clusters and mixtures when these are present in the data and so permits the interrogation of why a particular prediction may have been made. We were unable to find tools that were able to perform an analogous structural comparison and, therefore, have not performed statistical recall and efficiency benchmarking. While we could have run the models listed in the introduction, they have different null hypotheses and purposes. Some provide qualitatively different information to CLARITY, while others test for equality of the similarities, which is an implausible null hypothesis for our examples. While CLARITY is currently performing a unique function in terms of information extraction from complex data, we anticipate that the problem may be addressed in other ways, and that the insights which can be automatically extracted can be extended.</p>
    </sec>
  </sec>
  <sec sec-type="methods" id="s4">
    <label>4<x xml:space="preserve">. </x></label>
    <title>Methods</title>
    <sec id="s4a">
      <label>4.1<x xml:space="preserve">. </x></label>
      <title>Notation</title>
      <p>The notation we use is largely standard. Matrices are denoted by upper case letters. The set of all <italic toggle="yes">d</italic> × <italic toggle="yes">k</italic> matrices with real entries is denoted by <inline-formula><mml:math id="IM13"><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. If <inline-formula><mml:math id="IM14"><mml:mi>Y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a matrix, its (<italic toggle="yes">i</italic>, <italic toggle="yes">j</italic>)-entry is denoted by <italic toggle="yes">Y</italic><sub><italic toggle="yes">ij</italic></sub>. The quantity ‖<italic toggle="yes">Y</italic>‖<sub><italic toggle="yes">F</italic></sub> denotes the Frobenius norm of <italic toggle="yes">Y</italic>, i.e.
<disp-formula id="RSOS202182UM3"><mml:math id="DM3" display="block"><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>Y</mml:mi><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:math></disp-formula></p>
    </sec>
    <sec id="s4b">
      <label>4.2<x xml:space="preserve">. </x></label>
      <title>Structural representation</title>
      <p>In general, CLARITY can be applied to any pairs of matrices. In practice, the utility of the subsequent results does depend on how the matrix was constructed and the ‘best practice’ input is likely to be a distance matrix corresponding to a reasonable metric, such as Euclidean. In limited experimentation, asymmetry does not appear to be a major problem but the class of matrices we can prove that CLARITY is sensible for is the following.</p>
      <p>A dissimilarity matrix is defined to be any symmetric matrix <inline-formula><mml:math id="IM15"><mml:mi>Y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> of full rank consisting of non-negative entries. Let <italic toggle="yes">Y</italic><sub>1</sub> and <italic toggle="yes">Y</italic><sub>2</sub> be a pair of dissimilarity matrices in <inline-formula><mml:math id="IM16"><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. For each natural number <italic toggle="yes">k</italic> ≤ <italic toggle="yes">d</italic>, we initially seek matrices <inline-formula><mml:math id="IM17"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="IM18"><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> such that the quantity
<disp-formula id="RSOS202182UM4"><mml:math id="DM4" display="block"><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula>is minimized. Note that the squared error discussed in the text is the squared Frobenius norm and is minimized at the same <italic toggle="yes">A</italic> and <italic toggle="yes">X</italic>.</p>
      <p>The product <inline-formula><mml:math id="IM19"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is to be viewed as the best rank <italic toggle="yes">k</italic> approximation of <italic toggle="yes">Y</italic><sub>1</sub> in Frobenius norm subject to whatever constraints may be placed on both <italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub> and <inline-formula><mml:math id="IM20"><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> and it affords a structural reduction of <italic toggle="yes">Y</italic><sub>1</sub> at dimension <italic toggle="yes">k</italic> as motivated by the following proposition.</p>
      <statement id="st1">
        <title>Proposition 4.1.</title>
        <p><italic toggle="yes">Let</italic><inline-formula><mml:math id="IM21"><mml:mi>Y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula><italic toggle="yes">be a dissimilarity matrix and let</italic> (<italic toggle="yes">A</italic>, <italic toggle="yes">X</italic>) <italic toggle="yes">be a pair of matrices such that</italic>
<disp-formula id="RSOS202182UM5"><mml:math id="DM5" display="block"><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>Y</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mi>X</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo></mml:math></disp-formula><italic toggle="yes">is minimized, where</italic>
<italic toggle="yes">A</italic>
<italic toggle="yes">has full column rank. Then</italic>
<disp-formula id="RSOS202182UM6"><mml:math id="DM6" display="block"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mi>Y</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula><italic toggle="yes">where</italic>
<italic toggle="yes">P</italic><sub><italic toggle="yes">A</italic></sub>
<italic toggle="yes">denotes the orthogonal projection operator onto</italic>
<inline-formula><mml:math id="IM22"><mml:mrow><mml:mi mathvariant="normal">im</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>.</p>
      </statement>
      <statement id="st2">
        <title>Proof.</title>
        <p>Define the objective function
<disp-formula id="RSOS202182UM7"><mml:math id="DM7" display="block"><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo> </mml:mo><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>Y</mml:mi><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mi>X</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:math></disp-formula>Taking matrix derivatives with respect to <italic toggle="yes">X</italic> gives the condition
<disp-formula id="RSOS202182UM8"><mml:math id="DM8" display="block"><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mi>X</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:math></disp-formula>at a critical point (<italic toggle="yes">A</italic>, <italic toggle="yes">X</italic>). If <italic toggle="yes">A</italic> has full column rank, the matrix <italic toggle="yes">A</italic><sup><italic toggle="yes">T</italic></sup><italic toggle="yes">A</italic> is invertible and it is possible to solve for <italic toggle="yes">X</italic> by
<disp-formula id="RSOS202182UM9"><mml:math id="DM9" display="block"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mi>Y</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic toggle="yes">A</italic><sup>+</sup> : = (<italic toggle="yes">A</italic><sup><italic toggle="yes">T</italic></sup><italic toggle="yes">A</italic>)<sup>−1</sup><italic toggle="yes">A</italic><sup><italic toggle="yes">T</italic></sup> is the generalized (Moore–Penrose) inverse of <italic toggle="yes">A</italic>. Then
<disp-formula id="RSOS202182UM10"><mml:math id="DM10" display="block"><mml:mi>A</mml:mi><mml:mi>X</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mi>Y</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mi>Y</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math></disp-formula> ▪</p>
      </statement>
      <p>Given the above structural reduction of <italic toggle="yes">Y</italic><sub>1</sub>, we seek to find the extent to which it is capable of predicting the matrix <italic toggle="yes">Y</italic><sub>2</sub>. To this end, we find a matrix <inline-formula><mml:math id="IM23"><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> such that
<disp-formula id="RSOS202182UM11"><mml:math id="DM11" display="block"><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula>is minimized and we examine both the residual matrix
<disp-formula id="RSOS202182UM12"><mml:math id="DM12" display="block"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:math></disp-formula>and element-wise norms of it.</p>
      <p>If <italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub> has full column rank, the argument in proposition 4.1 gives that <inline-formula><mml:math id="IM24"><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> where <inline-formula><mml:math id="IM25"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> denotes the orthogonal projection onto <inline-formula><mml:math id="IM26"><mml:mrow><mml:mi mathvariant="normal">im</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>.</p>
    </sec>
    <sec id="s4c">
      <label>4.3<x xml:space="preserve">. </x></label>
      <title>Learning structure</title>
      <p>We consider two methods that differ only in the manner in which the initial optimization problem stated above is solved. Our <italic toggle="yes">SVD model</italic> uses singular value decomposition to solve analytically for <italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub> and <inline-formula><mml:math id="IM27"><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, and it is possible to do this precisely because these matrices are assumed to be unconstrained. Our <italic toggle="yes">Mixture model</italic> constrains the matrix <italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub> to have rows taken from a probability simplex (but does not constrain <inline-formula><mml:math id="IM28"><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>), and an optimum is obtained numerically via an iterative procedure.</p>
      <sec id="s4c1">
        <label>4.3.1<x xml:space="preserve">. </x></label>
        <title>SVD-based solution</title>
        <p>Suppose that we have singular value decomposition
<disp-formula id="RSOS202182UM13"><mml:math id="DM13" display="block"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mo> </mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mo> </mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mo> </mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic toggle="yes">σ</italic><sub><italic toggle="yes">j</italic></sub> denotes the <italic toggle="yes">j</italic>-largest singular value of <italic toggle="yes">Y</italic><sub>1</sub>. The matrix product <inline-formula><mml:math id="IM29"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> can have rank at most <italic toggle="yes">k</italic>, and by the Eckart–Young theorem [<xref rid="RSOS202182C59" ref-type="bibr">59</xref>],
<disp-formula id="RSOS202182UM14"><mml:math id="DM14" display="block"><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo> </mml:mo><mml:mo>:</mml:mo><mml:mo> </mml:mo><mml:mrow><mml:mi mathvariant="normal">rk</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>Y</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>Y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="IM30"><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is defined to be the truncation of the SVD of <italic toggle="yes">Y</italic><sub>1</sub> to its top <italic toggle="yes">k</italic> singular values, i.e.
<disp-formula id="RSOS202182UM15"><mml:math id="DM15" display="block"><mml:msub><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mo> </mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mo> </mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mo> </mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:math></disp-formula>We set <inline-formula><mml:math id="IM31"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mo>…</mml:mo><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula>. The matrix <inline-formula><mml:math id="IM32"><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> is then the top left-hand <italic toggle="yes">k</italic> × <italic toggle="yes">k</italic> block of <inline-formula><mml:math id="IM33"><mml:msub><mml:mi>Σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, and
<disp-formula id="RSOS202182UM16"><mml:math id="DM16" display="block"><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      </sec>
      <sec id="s4c2">
        <label>4.3.2<x xml:space="preserve">. </x></label>
        <title>Solution under a simplicial constraint</title>
        <p>We assume that the entries of <italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub> are non-negative and that the rows of <italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub> sum to 1. This constraint is motivated by mixture modelling. A solution is sought via an iterative gradient descent procedure using multiplicative update rules based on the approach of Lee &amp; Seung [<xref rid="RSOS202182C60" ref-type="bibr">60</xref>].</p>
        <p>Specifically, we derive a multiplicative update rule for <italic toggle="yes">A</italic> given <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic><sub>1</sub> and then solve for <italic toggle="yes">X</italic> given <italic toggle="yes">A</italic> and <italic toggle="yes">Y</italic><sub>1</sub>. These two steps are applied to convergence. This particular model does not appear to have been solved previously in the literature, and this solution is relatively efficient.</p>
        <p>Given <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic><sub>1</sub>, consider the objective function
<disp-formula id="RSOS202182UM17"><mml:math id="DM17" display="block"><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo> </mml:mo><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mi>X</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:math></disp-formula>If <italic toggle="yes">A</italic><sub><italic toggle="yes">t</italic></sub> is the current estimate of <italic toggle="yes">A</italic> at iteration <italic toggle="yes">t</italic>, taking matrix derivatives of <inline-formula><mml:math id="IM34"><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> leads to the update rule
<disp-formula id="RSOS202182UM18"><mml:math id="DM18" display="block"><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where
<disp-formula id="RSOS202182UM19"><mml:math id="DM19" display="block"><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:math></disp-formula>and
<disp-formula id="RSOS202182UM20"><mml:math id="DM20" display="block"><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula>and <italic toggle="yes">X</italic><sub><italic toggle="yes">t</italic></sub> denotes the estimate of <italic toggle="yes">X</italic> at the <italic toggle="yes">t</italic>th iteration. If <italic toggle="yes">A</italic><sub><italic toggle="yes">t</italic>+1</sub> has full column rank, then we solve for <italic toggle="yes">X</italic><sub><italic toggle="yes">t</italic>+1</sub> by use of the generalized inverse; i.e.
<disp-formula id="RSOS202182UM21"><mml:math id="DM21" display="block"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:math></disp-formula>If <italic toggle="yes">A</italic><sub><italic toggle="yes">t</italic>+1</sub> does not have full column rank, a multiplicative update rule is used to update <italic toggle="yes">X</italic><sub><italic toggle="yes">t</italic></sub> derived analogously, i.e.
<disp-formula id="RSOS202182UM22"><mml:math id="DM22" display="block"><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where
<disp-formula id="RSOS202182UM23"><mml:math id="DM23" display="block"><mml:msubsup><mml:mi>N</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msubsup><mml:mo> </mml:mo><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula>and
<disp-formula id="RSOS202182UM24"><mml:math id="DM24" display="block"><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msubsup><mml:mo> </mml:mo><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math></disp-formula>Empirically, the row-sums are approximately stable in this algorithm, but it does <italic toggle="yes">not</italic> guarantee that the rows sum to 1. Therefore, at each iteration we renormalize the rows to enforce this property. The row sums are not in general identifiable. In practice, disabling this normalization does not allow the row sums to drift significantly, except in cases where the model is a very poor approximation to the data. Poor model fit may cause the algorithm to terminate because it cannot find the local optima.</p>
        <p>The following algorithm describes this rule, using ○ to denote the entry-wise product of two matrices.</p>
        <p>
          <inline-graphic xlink:href="rsos202182f09.jpg"/>
        </p>
      </sec>
    </sec>
    <sec id="s4d">
      <label>4.4<x xml:space="preserve">. </x></label>
      <title>Statistical significance</title>
      <p>For simple datasets consisting of <italic toggle="yes">d</italic> subjects about which we observe <italic toggle="yes">L</italic> features, significance is measured using a statistical resampling procedure implemented in the CLARITY package. More complex datasets where similarities are computed in a complex way, and not read straightforwardly off matches between features—for example, as for our linguistic data—can still be quantified via resampling. In such cases, the data are bootstrapped externally and provided to the software as a set of matrices. In this procedure, we sample <italic toggle="yes">L</italic>/2 of the <italic toggle="yes">L</italic> features (columns) of the data <italic toggle="yes">D</italic><sub>1</sub>, which is a <italic toggle="yes">d</italic> × <italic toggle="yes">L</italic> matrix. We then compute a ‘sampled reference’ (dis)similarity matrix, and from the remaining <italic toggle="yes">L</italic>/2, a ‘sampled target’ (dis)similarity matrix. We then replicate the downsampling procedure on the target data and obtain a (dis)similarity matrix. We then mean centre and scale both sampled target and downsampled original target matrices into the sampled reference matrix, and evaluate test statistics <italic toggle="yes">f</italic> (squared residuals and persistences). This is repeated <italic toggle="yes">n</italic><sub><italic toggle="yes">bs</italic></sub> times.</p>
      <p>We compute a regularized empirical <italic toggle="yes">p</italic>-value <inline-formula><mml:math id="IM46"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mi>f</mml:mi><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo></mml:math></inline-formula>
<inline-formula><mml:math id="IM47"><mml:mi>f</mml:mi><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, formed from the probability that a sample from the true target is as great or greater than the resampled reference. This procedure is necessary because bootstrap resampling [<xref rid="RSOS202182C61" ref-type="bibr">61</xref>] is not straightforwardly valid for similarity matrices.</p>
      <p>Whilst this procedure correctly estimates which structures of <italic toggle="yes">Y</italic><sub>2</sub> are not predicted by <italic toggle="yes">Y</italic><sub>1</sub>, it does not distinguish between structures that are generated by signal versus noise. Because we are not interested in predicting noise, we further need to detect it. Estimating values of <italic toggle="yes">k</italic> associated with structure is straightforward by simple cross-validation, because we have already constructed many random resamples of the data. We can therefore predict fold-2 of <italic toggle="yes">Y</italic><sub>2</sub> from a CLARITY model learned in fold-1 of <italic toggle="yes">Y</italic><sub>2</sub> and estimate <inline-formula><mml:math id="IM48"><mml:mrow><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> from the minimum cross-validation error.</p>
      <p>Because <inline-formula><mml:math id="IM49"><mml:mrow><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is a point estimate subject to variation, we obtain <italic toggle="yes">n</italic><sub><italic toggle="yes">bs</italic></sub> samples <inline-formula><mml:math id="IM50"><mml:msub><mml:mrow><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and implement a soft threshold, the ‘probability that complexity <italic toggle="yes">k</italic> is describing structure’ <inline-formula><mml:math id="IM51"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>k</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, i.e. the proportion of bootstrap samples that have an estimate at least as large as <italic toggle="yes">k</italic>. We then report the complete CLARITY <italic toggle="yes">p</italic>-value <italic toggle="yes">p</italic><sub><italic toggle="yes">c</italic></sub>, quantifying the ‘probability of observing a test-statistic for <italic toggle="yes">Y</italic><sub>2</sub>, this extreme or more so, under the null hypothesis that <italic toggle="yes">Y</italic><sub>2</sub> is a mean scaled (dimension <italic toggle="yes">k</italic> rotation, translation and scaled) version of <italic toggle="yes">Y</italic><sub>1</sub> within which Complexity <italic toggle="yes">k</italic> describes Structure’. This is:
<disp-formula id="RSOS202182UM25"><mml:math id="DM25" display="block"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mi>f</mml:mi><mml:mo> </mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>which is close to 0 only if both <italic toggle="yes">p</italic>(<italic toggle="yes">k</italic>) is close to 1 and <italic toggle="yes">p</italic>(<italic toggle="yes">f</italic>(<italic toggle="yes">Y</italic><sub>2</sub>)|<italic toggle="yes">f</italic>(<italic toggle="yes">Y</italic><sub>1</sub>)) is close to 0.</p>
      <p>Because the <italic toggle="yes">p</italic>-values are highly correlated, and there is a multiple testing burden, the <italic toggle="yes">p</italic>-values should not be used to test for the presence of any difference in structure between <italic toggle="yes">Y</italic><sub>1</sub> and <italic toggle="yes">Y</italic><sub>2</sub>. In particular, it can be that <italic toggle="yes">Y</italic><sub>1</sub> and <italic toggle="yes">Y</italic><sub>2</sub> are substantially different, but this does not result in any particular cell in the persistence diagram having a <italic toggle="yes">p</italic>-value at the appropriate multiple-testing level. In other words, on the level of <italic toggle="yes">Y</italic><sub>1</sub> and <italic toggle="yes">Y</italic><sub>2</sub> viewed globally, testing individual residuals has very low power; testing individual persistences has low power. A more powerful test would be for <italic toggle="yes">f</italic> being the Frobenius norm of the whole matrix, but this has limited scientific value as it discards the scientific significance of the results.</p>
      <p>We provide a formal statistical significance procedure to add weight to the identification of scientifically significant results, but emphasize that these are different concepts.</p>
    </sec>
    <sec id="s4e">
      <label>4.5<x xml:space="preserve">. </x></label>
      <title>Methylation/expression simulated data</title>
      <p>For <xref rid="RSOS202182F4" ref-type="fig">figure 4</xref>, we use the simulation data from [<xref rid="RSOS202182C36" ref-type="bibr">36</xref>] (electronic supplementary material, figure S3) which is based on real methylation and expression patterns.</p>
      <p>First each locus is assigned a methylation class, 300 are ‘hypo’methylated (low) and 700 are ‘hyper’methylated (high). They are then given a mean methylation, <italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub> ∼ <italic toggle="yes">U</italic>(0.1, 0.4) and <italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub> ∼ <italic toggle="yes">U</italic>(0.55, 0.85) for hypo/hyper, respectively. Then ‘Tumour’ cases are assigned methylation <italic toggle="yes">m</italic><sub><italic toggle="yes">ij</italic></sub> ∼ <italic toggle="yes">U</italic>(<italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub> − <italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub>/2, <italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub> + <italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub>/2) or <italic toggle="yes">m</italic><sub><italic toggle="yes">ij</italic></sub> ∼ <italic toggle="yes">U</italic>(<italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub> − (1 − <italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub>)/2, <italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub> + (1 − <italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub>)/2) for hypo/hyper. Control cases follow the same distribution but shift towards the mean by replacing <italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub> with <italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub> + 0.2 (if <italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub> &lt; 0.3), or <italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub> − 0.2 (if <italic toggle="yes">μ</italic><sub><italic toggle="yes">i</italic></sub> &gt; 0.7) or with equal probability of positive or negative shift ±0.2 otherwise.</p>
      <p>To create Expression data, the procedure generates <italic toggle="yes">σ</italic><sub><italic toggle="yes">i</italic></sub> ∼ <italic toggle="yes">U</italic>(0.5, 0.9) for each locus and then sets <inline-formula><mml:math id="IM52"><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:msubsup><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. The reported expression <italic toggle="yes">e</italic><sub><italic toggle="yes">ij</italic></sub> are then centred and scaled.</p>
      <p>We then chose two segments of 10 loci and moved the two classes towards each other; the top anomaly loci have ‘tumour’ expression altered, and the bottom have ‘control’ altered, by setting <inline-formula><mml:math id="IM53"><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">anomaly</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. This simulates loci that behave differently in Methylation data to in Expression data. These parameters induce an average <inline-formula><mml:math id="IM54"><mml:mo>−</mml:mo><mml:mn>5</mml:mn><mml:mtext>%</mml:mtext></mml:math></inline-formula> correlation between methylation and expression, i.e. the association is weak.</p>
    </sec>
    <sec id="s4f">
      <label>4.6<x xml:space="preserve">. </x></label>
      <title>Simulation details</title>
      <p>For §2.4, we generate a coalescent tree <inline-formula><mml:math id="IM55"><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> using ‘rcoal’ from the package ‘ape’ [<xref rid="RSOS202182C62" ref-type="bibr">62</xref>] for R [<xref rid="RSOS202182C63" ref-type="bibr">63</xref>]. The ‘true’ <italic toggle="yes">A</italic> is a vector of zeroes except for the cluster membership <italic toggle="yes">k</italic> of <italic toggle="yes">i</italic>, for which <italic toggle="yes">A</italic><sub><italic toggle="yes">ik</italic></sub> = 1. We then simulate a matrix <italic toggle="yes">D</italic><sub>0</sub> consisting of <italic toggle="yes">k</italic> rows (clusters) and <italic toggle="yes">L</italic> columns (features) by allowing features to drift in a correlated manner under a random-walk model using the function ‘rTraitCont’ from the package ‘ape’. This generates a ‘true’ <inline-formula><mml:math id="IM56"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Dist</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. To generate a feature <italic toggle="yes">d</italic> for a sample with mixture <italic toggle="yes">a</italic>, we simulate features <inline-formula><mml:math id="IM57"><mml:mi>d</mml:mi><mml:mo> </mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi>D</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, from which we can compute <inline-formula><mml:math id="IM58"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Dist</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>.</p>
      <p>In <italic toggle="yes">Scenario A</italic>, we make <inline-formula><mml:math id="IM59"><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> into a non-ultrametric tree by randomly perturbing the branch lengths of <inline-formula><mml:math id="IM60"><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> by multiplying each by a <italic toggle="yes">U</italic>(0.1, 2) variable. We generate <italic toggle="yes">Y</italic><sup>(2)</sup> as above from <inline-formula><mml:math id="IM61"><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>.</p>
      <p>In <italic toggle="yes">Scenario B</italic>, we make <inline-formula><mml:math id="IM62"><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> as in Scenario A. Then one additional <italic toggle="yes">mixture edge</italic> is added at random. This is done by choosing a tip of the tree <italic toggle="yes">i</italic>, choosing a second tip <italic toggle="yes">j</italic> at least the median distance from the first tip, and setting <italic toggle="yes">A</italic>[, <italic toggle="yes">i</italic>] ← (1 − <italic toggle="yes">β</italic>)<italic toggle="yes">A</italic>[, <italic toggle="yes">j</italic>] and <italic toggle="yes">A</italic>[, <italic toggle="yes">j</italic>] = <italic toggle="yes">A</italic>[, <italic toggle="yes">j</italic>] + <italic toggle="yes">βA</italic>[, <italic toggle="yes">i</italic>]. This edge affects a proportion <italic toggle="yes">r</italic> of the subjects in cluster <italic toggle="yes">i</italic>. If <italic toggle="yes">r</italic> = 0 or <italic toggle="yes">r</italic> = 1 this becomes a relationship change rather than a structural change, because all of the samples in the cluster adopt a new relationship with the remaining clusters (though the relationship is no longer a tree). We use <italic toggle="yes">r</italic> = 0.5 throughout.</p>
    </sec>
    <sec id="s4g">
      <label>4.7<x xml:space="preserve">. </x></label>
      <title>Language example details</title>
      <p>We compute <bold>phonetic similarities</bold> using the <italic toggle="yes">information-weighted distance with sound correspondences</italic> (IWDSC) method [<xref rid="RSOS202182C44" ref-type="bibr">44</xref>]. First, we estimate global sound similarity scores, based on the whole NorthEuraLex 0.9 dataset with 107 languages. This provides us with an idea of which sounds in the data generally tend to be close. Implicitly, the employed inference method makes those sounds close that appear in words that are probably historically related. In other words, global sound similarities are not directly about articulatory or auditory similarities (i.e. how humans produce and perceive different sounds), but rather estimate ‘historical similarity’, thus implicitly tracking processes of language change.</p>
      <p>After obtaining global sound similarities, we compute local sound similarity scores for each language–language pair in our 36-language Indo-European subset of NorthEuraLex. This works similarly to global similarity scores, but now only taking into account data from those two languages. Both global and local sound similarity scores are based on mutual information. In particular, the local scores declare such sounds similar which are highly predictable from the sounds in the word expressing the same meaning in the other language.</p>
      <p>To obtain overall language–language Phonetic scores, we first build word-to-word similarity scores based on sound-to-sound similarity scores. Crucially, we discount the weight of the sounds in highly regular parts of words, e.g. the infinitive ending in German verbs such as geb*en* ‘to give’ and leb*en* ‘to live’ [<xref rid="RSOS202182C64" ref-type="bibr">64</xref>]. This way, we discount the regular grammatical elements: they carry information about the grammar of a language, but not about its individual words. We also normalize by word length. To get aggregate language-to-language similarities out of word-to-word similarities, we simply average.</p>
      <p>Language-to-language <bold>lexical similarity</bold> is defined as cognate overlap: the share of words in the relevant two languages that were inferred to have the same ancestral word. We produce automatic cognacy judgements by applying unweighted pair group method with arithmetic mean (UPGMA) clustering to the word-to-word phonetic similarity scores within each meaning, a method shown to currently produce state-of-the-art automatic cognacy judgements [<xref rid="RSOS202182C44" ref-type="bibr">44</xref>].</p>
      <p>Both Phonetic and Lexical similarities that we compute are based on word-to-word phonetic similarity scores. The cognate clustering step that takes us from word-to-word similarities to cognate overlap aims to uncover, automatically, information about the word-replacement change. Phonetic and Lexical information is bound to be <bold>highly correlated</bold>. First, the change of two types occurs in the same communities subject to the same historical processes. For example, both Phonetic and Lexical change accumulate with time, so two speech communities that split earlier will be more dissimilar on both Phonetic and Lexical change than two speech communities with a later split, other things being equal. Second, when two languages retain a common ancestral word, simply by virtue of stemming from the same proto-word, the two modern words are going to be more phonetically similar than two randomly selected phonetic sequences from the two languages. Thus higher levels of true cognate overlap will lead to higher levels of phonetic similarities.</p>
      <p>Finally, in addition to these two real-world drivers of correlation, in our computational analysis we infer lexical overlap based on low-level phonetic similarity. It is a common and effective practice in computational historical linguistics, and only slightly inferior to expert-coded information for at least some types of practical inference [<xref rid="RSOS202182C65" ref-type="bibr">65</xref>]. But we do expect to miss some true cognates that changed phonetically so much as to be not statistically identifiable from the raw data without additional expert knowledge. This makes our estimated dissimilarity matrices for Phonetic and Lexical still more correlated than the corresponding ground-truth matrices would be. This makes it all the more striking that despite a strong correlation between Phonetic and Lexical, stemming from both natural and analysis-induced sources, we find a robust and convincing effect of mismatch using CLARITY.</p>
      <p>To assess <bold>significance</bold>, we use the method described in §4.4, dividing the data into two halves by meaning, and computing independently a similarity matrix from each half. When doing that, we always use the same global similarity scores, which represent the properties of a much larger sample of 107 languages, taken as a proxy for languages of the world in general.</p>
      <p>We use <bold>cross-validation</bold> to assess whether CLARITY decomposition at higher complexities <italic toggle="yes">k</italic> still captures signal rather than noise. For the pairs of matrices based on two halves of the data, we predict one based on the other (so Phonetic from Phonetic and Lexical from Lexical). This shows (electronic supplementary material, figure S3) that even at the highest <italic toggle="yes">k</italic> we do not have overfitting to noise. Therefore, the full range of <italic toggle="yes">k</italic> in the main analysis is interpretable.</p>
      <p>The CLARITY set-up in this example implies that the diagonal values in our similarity matrices might not be amenable to successful modelling. In the main text above, we report the results where we discount the diagonal when doing CLARITY decomposition. We checked whether the pattern we found depended on this choice. Electronic supplementary material, figure S4 shows CLARITY persistences with no special treatment for the diagonal, and those are basically the same as those in <xref rid="RSOS202182F7" ref-type="fig">figure 7</xref>. Similarly, we checked (electronic supplementary material, figure S5) that lowering the significance threshold from 0.05 to 0.01 does not change the result. Finally, in electronic supplementary material, figures S6 and S7), we checked that the image representation of persistence did not affect our inference, and that resampling the persistence curves for the matrices based on half the data, used for significance testing. Taken together, these further checks convince us that our reported result is real and not spurious.</p>
    </sec>
    <sec id="s4h">
      <label>4.8<x xml:space="preserve">. </x></label>
      <title>Mathematical validity of structural comparison</title>
      <p>The matrices <italic toggle="yes">Y</italic><sub>1</sub> and <italic toggle="yes">Y</italic><sub>2</sub> are typically observed with non-independent noise, and so there is a need for the various quantities of interest to be stable under perturbation—that is, that a small change in the data does not result in a large change to the inference. The following result describes the stability of the residual matrix under perturbation of <italic toggle="yes">Y</italic><sub>1</sub> and <italic toggle="yes">Y</italic><sub>2</sub> for the SVD-based solution.</p>
      <statement id="st3">
        <title>Theorem 4.2.</title>
        <p><italic toggle="yes">Let</italic><inline-formula><mml:math id="IM63"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula><italic toggle="yes">be symmetric matrices such that</italic> ‖<italic toggle="yes">Y</italic><sub>2</sub> − <italic toggle="yes">Y</italic>′<sub>2</sub>‖<sub><italic toggle="yes">F</italic></sub> , ‖<italic toggle="yes">Y</italic><sub>1</sub> − <italic toggle="yes">Y</italic>′<sub>1</sub>‖<sub><italic toggle="yes">F</italic></sub> ≤ <italic toggle="yes">ε</italic>. <italic toggle="yes">Suppose that we have singular value decompositions</italic>
<inline-formula><mml:math id="IM64"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>Σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>
<italic toggle="yes">and</italic>
<inline-formula><mml:math id="IM65"><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:msubsup><mml:mi>Σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi> </mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. Let <italic toggle="yes">A</italic><sub><italic toggle="yes">k</italic></sub>
<italic toggle="yes">and</italic>
<italic toggle="yes">A</italic>′<sub><italic toggle="yes">k</italic></sub>
<italic toggle="yes">be the matrices obtained by taking the first</italic>
<italic toggle="yes">k</italic>
<italic toggle="yes">columns of</italic>
<italic toggle="yes">U</italic><sub>1</sub>
<italic toggle="yes">and</italic>
<italic toggle="yes">U</italic><sub>1</sub>′, <italic toggle="yes">respectively</italic>.
<list list-type="simple"><list-item><label>1.<x xml:space="preserve"> </x></label><p><italic toggle="yes">If</italic><inline-formula><mml:math id="IM66"><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula><italic toggle="yes">with</italic><inline-formula><mml:math id="IM67"><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi> </mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula><italic toggle="yes">defined analogously for</italic><italic toggle="yes">Y</italic>′<sub>2</sub><italic toggle="yes">then</italic><disp-formula id="RSOS202182UM26"><mml:math id="DM26" display="block"><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi> </mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>ϵ</mml:mi><mml:mo>.</mml:mo></mml:math></disp-formula></p></list-item><list-item><label>2.<x xml:space="preserve"> </x></label><p><italic toggle="yes">Suppose that</italic><italic toggle="yes">Y</italic><sub>1</sub><italic toggle="yes">has eigenvalues</italic><inline-formula><mml:math id="IM68"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mo>⋯</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula><italic toggle="yes">and let</italic><italic toggle="yes">δ</italic><sub><italic toggle="yes">k</italic></sub> := <italic toggle="yes">λ</italic><sub><italic toggle="yes">k</italic></sub> − <italic toggle="yes">λ</italic><sub><italic toggle="yes">k</italic>+1</sub>
<italic toggle="yes">for each natural number</italic>
<italic toggle="yes">k</italic> &lt; <italic toggle="yes">d</italic>. <italic toggle="yes">Then</italic>
<disp-formula id="RSOS202182UM27"><mml:math id="DM27" display="block"><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi> </mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>ϵ</mml:mi></mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p></list-item></list></p>
      </statement>
      <p>The proof of theorem 4.2 may be found in appendix A. Theorem 4.2 can be used for statistical purposes as follows. If <italic toggle="yes">Y</italic>′<sub>1</sub> and <italic toggle="yes">Y</italic>′<sub>2</sub> are sampled matrices that are believed to be close to their population counterparts <italic toggle="yes">Y</italic><sub>1</sub>, <italic toggle="yes">Y</italic><sub>2</sub> (for example, when dealing with covariances), then given suitably sized eigengaps <italic toggle="yes">δ</italic><sub><italic toggle="yes">k</italic></sub> and <italic toggle="yes">δ</italic><sub><italic toggle="yes">k</italic></sub>′ for <italic toggle="yes">Y</italic><sub>1</sub> and <italic toggle="yes">Y</italic><sub>1</sub>′, respectively, the Frobenius norm of the estimated residual matrix is close to that of the true residual matrix. Specifically, simple manipulation of the inequalities established in theorem 4.2 leads to the deviation inequality
<disp-formula id="RSOS202182UM28"><mml:math id="DM28" display="block"><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mi> </mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi> </mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mo>≤</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mn>5</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mi>ϵ</mml:mi></mml:math></disp-formula>where ‖<italic toggle="yes">Y</italic><sub>2</sub> − <italic toggle="yes">Y</italic>′<sub>2</sub>‖<sub><italic toggle="yes">F</italic></sub> , ‖<italic toggle="yes">Y</italic><sub>1</sub> − <italic toggle="yes">Y</italic>′<sub>1</sub>‖<sub><italic toggle="yes">F</italic></sub> ≤ <italic toggle="yes">ε</italic>.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material position="float" content-type="local-data">
      <media xlink:href="rsos202182_review_history.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec id="s5">
    <title>Data accessibility</title>
    <p>Data and relevant code (R packages <italic toggle="yes">CLARITY</italic> and <italic toggle="yes">CLARITYsim</italic>) for this research work are stored in GitHub: <uri xlink:href="https://github.com/danjlawson/CLARITY">github.com/danjlawson/CLARITY</uri> and have been archived within the Zenodo repository: <uri xlink:href="https://doi.org/10.5281/zenodo.5172063">https://doi.org/10.5281/zenodo.5172063</uri>.</p>
  </sec>
  <sec id="s6">
    <title>Competing interests</title>
    <p>We declare we have no competing interests.</p>
  </sec>
  <sec id="s7">
    <title>Funding</title>
    <p>D.J.L. is funded by the Wellcome Trust and Royal Society Sir Henry Dale Fellowship, grant no. WT104125MA. D.J.L. and P.E. are supported by the OCSEAN grant funded by the EU Research Executive Agency (Horizon 2020 MSCA RISE 2019 number 873207). J.D. has been supported by the German Research Foundation (DFG) under FOR 2237 ‘Words, Bones, Genes, Tools’ and by the European Research Council (ERC) under the Horizon 2020 research and innovation programme (CrossLingference, grant agreement no. 834050, I.Y. has been supported by the German Research Foundation (DFG) under Emmy-Noether-NWG 391377018 and under FOR 2237 ‘Words, Bones, Genes, Tools’.</p>
  </sec>
  <app-group>
    <app>
      <title>Appendix A. Proofs</title>
      <p>
        <bold>A.1. Notation</bold>
      </p>
      <p>In addition to the notation already introduced, if <italic toggle="yes">A</italic> is a matrix, its spectral norm is denoted by ‖<italic toggle="yes">A</italic>‖<sub>2</sub>. The singular values <inline-formula><mml:math id="IM69"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>…</mml:mo></mml:math></inline-formula> of <italic toggle="yes">A</italic> are listed in non-increasing order, and so ‖<italic toggle="yes">A</italic>‖<sub>2</sub> = <italic toggle="yes">σ</italic><sub>1</sub>(<italic toggle="yes">A</italic>). If <italic toggle="yes">v</italic> is a vector, its Euclidean norm is denoted by ‖<italic toggle="yes">v</italic>‖. If <italic toggle="yes">A</italic> is a matrix, its vectorization (the vector obtained by stacking the columns of <italic toggle="yes">A</italic>) is denoted by <inline-formula><mml:math id="IM70"><mml:mrow><mml:mi mathvariant="sans-serif">V</mml:mi><mml:mi mathvariant="sans-serif">e</mml:mi><mml:mi mathvariant="sans-serif">c</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>.</p>
      <p>
        <bold>A.2. Preliminary facts</bold>
      </p>
      <p>Recall that for any matrices <italic toggle="yes">A</italic>, <italic toggle="yes">B</italic> and <italic toggle="yes">C</italic> where the product <italic toggle="yes">ABC</italic> exists, we have the identity <inline-formula><mml:math id="IM71"><mml:mrow><mml:mi mathvariant="normal">Vec</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">Vec</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> where ⊗ denotes the Kronecker product of two matrices. This identity is useful in what follows.</p>
      <p>If <italic toggle="yes">V</italic>, <italic toggle="yes">V</italic>′ are <italic toggle="yes">d</italic> × <italic toggle="yes">k</italic> matrices with orthonormal columns, we have a vector (cos<sup>−1</sup>(<italic toggle="yes">σ</italic><sub>1</sub>), …, cos <sup>−1</sup>(<italic toggle="yes">σ</italic><sub><italic toggle="yes">k</italic></sub>))<sup><italic toggle="yes">T</italic></sup> of principal angles, where the <italic toggle="yes">σ</italic><sub><italic toggle="yes">j</italic></sub> are the singular values of <italic toggle="yes">V</italic><sup>′<italic toggle="yes">T</italic></sup><italic toggle="yes">V</italic>. Let <inline-formula><mml:math id="IM72"><mml:mi>Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> denote the <italic toggle="yes">r</italic> × <italic toggle="yes">r</italic> diagonal matrix with the <italic toggle="yes">j</italic>th diagonal entry given by the <italic toggle="yes">j</italic>th principal angle. The matrices <inline-formula><mml:math id="IM73"><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="IM74"><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> are defined entry-wise. The perturbation bounds established rely on the following variant of the Davis–Kahan theorem [<xref rid="RSOS202182C66" ref-type="bibr">66</xref>].</p>
      <statement id="st4">
        <title>Theorem A.1.</title>
        <p><italic toggle="yes">Let</italic><inline-formula><mml:math id="IM75"><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula><italic toggle="yes">be symmetric matrices with eigenvalues</italic><inline-formula><mml:math id="IM76"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mo>⋯</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula><italic toggle="yes">and</italic><inline-formula><mml:math id="IM77"><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>≥</mml:mo><mml:mo>⋯</mml:mo><mml:mo>≥</mml:mo><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:math></inline-formula>, <italic toggle="yes">respectively. Fix</italic> 1 ≤ <italic toggle="yes">r</italic> ≤ <italic toggle="yes">s</italic> ≤ <italic toggle="yes">d</italic>
<italic toggle="yes">and suppose that</italic>
<italic toggle="yes">δ</italic><sub><italic toggle="yes">r</italic>,<italic toggle="yes">s</italic></sub> : = min (<italic toggle="yes">λ</italic><sub><italic toggle="yes">r</italic>−1</sub> − <italic toggle="yes">λ</italic><sub><italic toggle="yes">r</italic></sub>, <italic toggle="yes">λ</italic><sub><italic toggle="yes">s</italic></sub> − <italic toggle="yes">λ</italic><sub><italic toggle="yes">s</italic>+1</sub>) &gt; 0, <italic toggle="yes">where</italic>
<italic toggle="yes">λ</italic><sub>0</sub> : = ∞ <italic toggle="yes">and</italic>
<italic toggle="yes">λ</italic><sub><italic toggle="yes">d</italic>+1</sub> : = −∞. <italic toggle="yes">Put</italic>
<italic toggle="yes">p</italic> = <italic toggle="yes">s</italic> − <italic toggle="yes">r</italic> + 1 <italic toggle="yes">and define</italic>
<inline-formula><mml:math id="IM78"><mml:mi>V</mml:mi><mml:mo> </mml:mo><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mo>…</mml:mo><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo> </mml:mo><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mo>…</mml:mo><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula>, <italic toggle="yes">both with orthonormal columns, satisfying</italic>
<italic toggle="yes">Yv</italic><sub><italic toggle="yes">j</italic></sub> = <italic toggle="yes">λ</italic><sub><italic toggle="yes">j</italic></sub>
<italic toggle="yes">v</italic><sub><italic toggle="yes">j</italic></sub>
<italic toggle="yes">and</italic>
<italic toggle="yes">Y</italic>′<italic toggle="yes">v</italic>′<sub><italic toggle="yes">j</italic></sub> = <italic toggle="yes">λ</italic>′<sub><italic toggle="yes">j</italic></sub>
<italic toggle="yes">v</italic>′<sub><italic toggle="yes">j</italic></sub>
<italic toggle="yes">for each</italic>
<inline-formula><mml:math id="IM79"><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:math></inline-formula>. <italic toggle="yes">Then</italic>
<disp-formula id="RSOS202182UM29"><mml:math id="DM29" display="block"><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>Y</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>Y</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      </statement>
      <p>
        <bold>A.3. Proof of theorem 4.2</bold>
      </p>
      <p>
        <list list-type="simple">
          <list-item>
            <label>1.<x xml:space="preserve"> </x></label>
            <p>Let <inline-formula><mml:math id="IM80"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>. Then
<disp-formula id="RSOS202182UM30"><mml:math id="DM30" display="block"><mml:mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace" displaystyle="true"><mml:mtr><mml:mtd><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">Vec</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>≤</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⊗</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>ϵ</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>and the claim follows by the triangle inequality.</p>
          </list-item>
          <list-item>
            <label>2.<x xml:space="preserve"> </x></label>
            <p>Let <inline-formula><mml:math id="IM81"><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub></mml:math></inline-formula>. Then,
<disp-formula id="RSOS202182UM31"><mml:math id="DM31" display="block"><mml:mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace" displaystyle="true"><mml:mtr><mml:mtd><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>≤</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>≤</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>⊗</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">Vec</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⊗</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">Vec</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>≤</mml:mo><mml:mn>2</mml:mn><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>Moreover,
<disp-formula id="RSOS202182UM32"><mml:math id="DM32" display="block"><mml:mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace" displaystyle="true"><mml:mtr><mml:mtd><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mo>≤</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi><mml:mi mathvariant="sans-serif">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>≤</mml:mo><mml:mn>8</mml:mn><mml:mrow><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:msubsup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mfrac></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where theorem A.1 has been used to obtain the last inequality. The claim follows by the triangle inequality.</p>
          </list-item>
        </list>
      </p>
    </app>
  </app-group>
  <ref-list>
    <title>References</title>
    <ref id="RSOS202182C1">
      <label>1<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sokal</surname><given-names>RR</given-names></string-name></person-group>. <year>1988</year><article-title>Genetic, geographic, and linguistic distances in Europe</article-title>. <source>Proc. Natl Acad. Sci. USA</source><volume><bold>85</bold></volume>, <fpage>1722</fpage>-<lpage>1726</lpage>. (<pub-id pub-id-type="doi">10.1073/pnas.85.5.1722</pub-id>)<pub-id pub-id-type="pmid">3422760</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C2">
      <label>2<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Creanza</surname><given-names>N</given-names></string-name>, <string-name><surname>Ruhlen</surname><given-names>M</given-names></string-name>, <string-name><surname>Pemberton</surname><given-names>TJ</given-names></string-name>, <string-name><surname>Rosenberg</surname><given-names>NA</given-names></string-name>, <string-name><surname>Feldman</surname><given-names>MW</given-names></string-name>, <string-name><surname>Ramachandran</surname><given-names>S</given-names></string-name></person-group>. <year>2015</year><article-title>A comparison of worldwide phonemic and genetic variation in human populations</article-title>. <source>Proc. Natl Acad. Sci. USA</source><volume><bold>112</bold></volume>, <fpage>1265</fpage>-<lpage>1272</lpage>. (<pub-id pub-id-type="doi">10.1073/pnas.1424033112</pub-id>)<pub-id pub-id-type="pmid">25605893</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C3">
      <label>3<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kriegeskorte</surname><given-names>N</given-names></string-name>, <string-name><surname>Mur</surname><given-names>M</given-names></string-name>, <string-name><surname>Bandettini</surname><given-names>P</given-names></string-name></person-group>. <year>2008</year><article-title>Representational similarity analysis—connecting the branches of systems neuroscience</article-title>. <source>Front. Syst. Neurosci.</source><volume><bold>2</bold></volume>, <fpage>4</fpage>. (<pub-id pub-id-type="doi">10.3389/neuro.01.016.2008</pub-id>)<pub-id pub-id-type="pmid">19104670</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C4">
      <label>4<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grigoriadis</surname><given-names>A</given-names></string-name>, <string-name><surname>Mackay</surname><given-names>A</given-names></string-name>, <string-name><surname>Noel</surname><given-names>E</given-names></string-name>, <string-name><surname>Wu</surname><given-names>PJ</given-names></string-name>, <string-name><surname>Natrajan</surname><given-names>R</given-names></string-name>, <string-name><surname>Frankum</surname><given-names>J</given-names></string-name>, <string-name><surname>Reis-Filho</surname><given-names>JS</given-names></string-name>, <string-name><surname>Tutt</surname><given-names>A</given-names></string-name></person-group>. <year>2012</year><article-title>Molecular characterisation of cell line models for triple-negative breast cancers</article-title>. <source>BMC Genomics</source><volume><bold>13</bold></volume>, <fpage>619</fpage>. (<pub-id pub-id-type="doi">10.1186/1471-2164-13-619</pub-id>)<pub-id pub-id-type="pmid">23151021</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C5">
      <label>5<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Römer</surname><given-names>M</given-names></string-name>, <string-name><surname>Eichner</surname><given-names>J</given-names></string-name>, <string-name><surname>Metzger</surname><given-names>U</given-names></string-name>, <string-name><surname>Templin</surname><given-names>MF</given-names></string-name>, <string-name><surname>Plummer</surname><given-names>S</given-names></string-name>, <string-name><surname>Ellinger-Ziegelbauer</surname><given-names>H</given-names></string-name>, <string-name><surname>Zell</surname><given-names>A</given-names></string-name></person-group>. <year>2014</year><article-title>Cross-platform toxicogenomics for the prediction of non-genotoxic hepatocarcinogenesis in rat</article-title>. <source>PLoS ONE</source><volume><bold>9</bold></volume>, <fpage>e97640</fpage>. (<pub-id pub-id-type="doi">10.1371/journal.pone.0097640</pub-id>)<pub-id pub-id-type="pmid">24830643</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C6">
      <label>6<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>S-M</given-names></string-name>, <string-name><surname>Bu</surname><given-names>L</given-names></string-name>, <string-name><surname>Laidemitt</surname><given-names>MR</given-names></string-name>, <string-name><surname>Lu</surname><given-names>L</given-names></string-name>, <string-name><surname>Mutuku</surname><given-names>MW</given-names></string-name>, <string-name><surname>Mkoji</surname><given-names>GM</given-names></string-name>, <string-name><surname>Loker</surname><given-names>ES</given-names></string-name></person-group>. <year>2018</year><article-title>Complete mitochondrial and rDNA complex sequences of important vector species of Biomphalaria, obligatory hosts of the human-infecting blood fluke, <italic toggle="yes">Schistosoma mansoni</italic></article-title>. <source>Sci. Rep.</source><volume><bold>8</bold></volume>, <fpage>7341</fpage>. (<pub-id pub-id-type="doi">10.1038/s41598-018-25463-z</pub-id>)<pub-id pub-id-type="pmid">29743617</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C7">
      <label>7<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Victor</surname><given-names>JD</given-names></string-name>, <string-name><surname>Purpura</surname><given-names>KP</given-names></string-name></person-group>. <year>1997</year><article-title>Metric-space analysis of spike trains: theory, algorithms and application</article-title>. <source>Netw.: Comput. Neural Syst.</source><volume><bold>8</bold></volume>, <fpage>127</fpage>-<lpage>164</lpage>. (<pub-id pub-id-type="doi">10.1088/0954-898X_8_2_003</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C8">
      <label>8<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lawson</surname><given-names>DJ</given-names></string-name>, <string-name><surname>Hellenthal</surname><given-names>G</given-names></string-name>, <string-name><surname>Myers</surname><given-names>S</given-names></string-name>, <string-name><surname>Falush</surname><given-names>D</given-names></string-name></person-group>. <year>2012</year><article-title>Inference of population structure using dense haplotype data</article-title>. <source>PLoS Genet.</source><volume><bold>8</bold></volume>, <fpage>e1002453</fpage>. (<pub-id pub-id-type="doi">10.1371/journal.pgen.1002453</pub-id>)<pub-id pub-id-type="pmid">22291602</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C9">
      <label>9<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brooks</surname><given-names>DR</given-names></string-name></person-group>. <year>1979</year><article-title>Testing the context and extent of host-parasite coevolution</article-title>. <source>Syst. Biol.</source><volume><bold>28</bold></volume>, <fpage>299</fpage>-<lpage>307</lpage>. (<pub-id pub-id-type="doi">10.1093/sysbio/28.3.299</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C10">
      <label>10<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amorim</surname><given-names>CEG</given-names></string-name>, <string-name><surname>Bisso-Machado</surname><given-names>R</given-names></string-name>, <string-name><surname>Ramallo</surname><given-names>V</given-names></string-name>, <string-name><surname>Bortolini</surname><given-names>MC</given-names></string-name>, <string-name><surname>Bonatto</surname><given-names>SL</given-names></string-name>, <string-name><surname>Salzano</surname><given-names>FM</given-names></string-name>, <string-name><surname>Hünemeier</surname><given-names>T</given-names></string-name></person-group>. <year>2013</year><article-title>A Bayesian approach to genome/linguistic relationships in native South Americans</article-title>. <source>PLoS ONE</source><volume><bold>8</bold></volume>, <fpage>e64099</fpage>. (<pub-id pub-id-type="doi">10.1371/journal.pone.0064099</pub-id>)<pub-id pub-id-type="pmid">23696865</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C11">
      <label>11<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mantel</surname><given-names>N</given-names></string-name></person-group>. <year>1967</year><article-title>The detection of disease clustering and a generalized regression approach</article-title>. <source>Cancer Res.</source><volume><bold>27</bold></volume>, <fpage>209</fpage>-<lpage>220</lpage>.<pub-id pub-id-type="pmid">6018555</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C12">
      <label>12<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Smouse</surname><given-names>PE</given-names></string-name>, <string-name><surname>Long</surname><given-names>JC</given-names></string-name>, <string-name><surname>Sokal</surname><given-names>RR</given-names></string-name></person-group>. <year>1986</year><article-title>Multiple regression and correlation extensions of the Mantel test of matrix correspondence</article-title>. <source>Syst. Zool.</source><volume><bold>35</bold></volume>, <fpage>627</fpage>-<lpage>632</lpage>. (<pub-id pub-id-type="doi">10.2307/2413122</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C13">
      <label>13<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hurley</surname><given-names>JR</given-names></string-name>, <string-name><surname>Cattell</surname><given-names>RB</given-names></string-name></person-group>. <year>1962</year><article-title>The Procrustes program: producing direct rotation to test a hypothesized factor structure</article-title>. <source>Behav. Sci.</source><volume><bold>7</bold></volume>, <fpage>258</fpage>-<lpage>262</lpage>. (<pub-id pub-id-type="doi">10.1002/bs.v7:2</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C14">
      <label>14<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schneider</surname><given-names>JW</given-names></string-name>, <string-name><surname>Borlund</surname><given-names>P</given-names></string-name></person-group>. <year>2007</year><article-title>Matrix comparison, part 2: measuring the resemblance between proximity measures or ordination results by use of the Mantel and Procrustes statistics</article-title>. <source>J. Am. Soc. Inf. Sci. Technol.</source><volume><bold>58</bold></volume>, <fpage>1596</fpage>-<lpage>1609</lpage>. (<pub-id pub-id-type="doi">10.1002/(ISSN)1532-2890</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C15">
      <label>15<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jackson</surname><given-names>DA</given-names></string-name></person-group>. <year>1995</year><article-title>PROTEST: a PROcrustean Randomization TEST of community environment concordance</article-title>. <source>Ecoscience</source><volume><bold>2</bold></volume>, <fpage>297</fpage>-<lpage>303</lpage>. (<pub-id pub-id-type="doi">10.1080/11956860.1995.11682297</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C16">
      <label>16<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peres-Neto</surname><given-names>PR</given-names></string-name>, <string-name><surname>Jackson</surname><given-names>DA</given-names></string-name></person-group>. <year>2001</year><article-title>How well do multivariate data sets match? The advantages of a Procrustean superimposition approach over the Mantel test</article-title>. <source>Oecologia</source><volume><bold>129</bold></volume>, <fpage>169</fpage>-<lpage>178</lpage>. (<pub-id pub-id-type="doi">10.1007/s004420100720</pub-id>)<pub-id pub-id-type="pmid">28547594</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C17">
      <label>17<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steiger</surname><given-names>JH</given-names></string-name></person-group>. <year>1980</year><article-title>Tests for comparing elements of a correlation matrix</article-title>. <source>Psychol. Bull.</source><volume><bold>87</bold></volume>, <fpage>245</fpage>-<lpage>251</lpage>. (<pub-id pub-id-type="doi">10.1037/0033-2909.87.2.245</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C18">
      <label>18<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Förstner</surname><given-names>W</given-names></string-name>, <string-name><surname>Moonen</surname><given-names>B</given-names></string-name></person-group>. <year>2003</year><comment>A metric for covariance matrices. In <italic toggle="yes">Geodesy: the challenge of the 3rd Millennium</italic> (eds EW Grafarend, FW Krumm, VS Schwarze), pp. 299–309. Berlin, Germany: Springer</comment>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C19">
      <label>19<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Flury</surname><given-names>B</given-names></string-name></person-group>. <year>1988</year><source>Common principal components &amp; related multivariate models</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>John Wiley &amp; Sons, Inc</publisher-name>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C20">
      <label>20<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Flury</surname><given-names>BN</given-names></string-name></person-group>. <year>1986</year><article-title>Asymptotic theory for common principal component analysis</article-title>. <source>Ann. Stat.</source><volume><bold>14</bold></volume>, <fpage>418</fpage>-<lpage>430</lpage>. (<pub-id pub-id-type="doi">10.1214/aos/1176349930</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C21">
      <label>21<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bille</surname><given-names>P</given-names></string-name></person-group>. <year>2005</year><article-title>A survey on tree edit distance and related problems</article-title>. <source>Theor. Comput. Sci.</source><volume><bold>337</bold></volume>, <fpage>217</fpage>-<lpage>239</lpage>. (<pub-id pub-id-type="doi">10.1016/j.tcs.2004.12.030</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C22">
      <label>22<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Penny</surname><given-names>D</given-names></string-name>, <string-name><surname>Hendy</surname><given-names>MD</given-names></string-name></person-group>. <year>1985</year><article-title>The use of tree comparison metrics</article-title>. <source>Syst. Zool.</source><volume><bold>34</bold></volume>, <fpage>75</fpage>-<lpage>82</lpage>. (<pub-id pub-id-type="doi">10.2307/2413347</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C23">
      <label>23<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nye</surname><given-names>TMW</given-names></string-name>, <string-name><surname>Tang</surname><given-names>X</given-names></string-name>, <string-name><surname>Weyenberg</surname><given-names>G</given-names></string-name>, <string-name><surname>Yoshida</surname><given-names>R</given-names></string-name></person-group>. <year>2017</year><article-title>Principal component analysis and the locus of the Fréchet mean in the space of phylogenetic trees</article-title>. <source>Biometrika</source><volume><bold>104</bold></volume>, <fpage>901</fpage>-<lpage>922</lpage>. (<pub-id pub-id-type="doi">10.1093/biomet/asx047</pub-id>)<pub-id pub-id-type="pmid">29422694</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C24">
      <label>24<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schliep</surname><given-names>KP</given-names></string-name></person-group>. <year>2011</year><article-title>phangorn: phylogenetic analysis in R</article-title>. <source>Bioinformatics</source><volume><bold>27</bold></volume>, <fpage>592</fpage>-<lpage>593</lpage>. (<pub-id pub-id-type="doi">10.1093/bioinformatics/btq706</pub-id>)<pub-id pub-id-type="pmid">21169378</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C25">
      <label>25<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carlsson</surname><given-names>G</given-names></string-name>, <string-name><surname>Mémoli</surname><given-names>F</given-names></string-name></person-group>. <year>2010</year><article-title>Characterization, stability and convergence of hierarchical clustering methods</article-title>. <source>J. Mach. Learn. Res.</source><volume><bold>11</bold></volume>, <fpage>1425</fpage>-<lpage>1470</lpage>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C26">
      <label>26<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mossel</surname><given-names>E</given-names></string-name></person-group>. <year>2005</year><article-title>Phylogenetic MCMC algorithms are misleading on mixtures of trees</article-title>. <source>Science</source><volume><bold>309</bold></volume>, <fpage>2207</fpage>-<lpage>2209</lpage>. (<pub-id pub-id-type="doi">10.1126/science.1115493</pub-id>)<pub-id pub-id-type="pmid">16195459</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C27">
      <label>27<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Mahalanabis</surname><given-names>S</given-names></string-name>, <string-name><surname>Štefankovič</surname><given-names>D</given-names></string-name></person-group>. <year>2009</year><comment>Approximating <italic toggle="yes">L</italic><sub>1</sub>-distances between mixture distributions using random projections. In <italic toggle="yes">Proc. of the Meet. on Analytic Algorithmics and Combinatorics</italic>, pp. 75–84. Philadelphia, PA: Society for Industrial and Applied Mathematics</comment>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C28">
      <label>28<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Tipping</surname><given-names>ME</given-names></string-name></person-group>. <year>1999</year><comment>Deriving cluster analytic distance functions from Gaussian mixture models. In <italic toggle="yes">ICANN99, September</italic>, pp. 815–820. IEEE</comment>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C29">
      <label>29<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lawson</surname><given-names>DJ</given-names></string-name>, <string-name><surname>Van Dorp</surname><given-names>L</given-names></string-name>, <string-name><surname>Falush</surname><given-names>D</given-names></string-name></person-group>. <year>2018</year><article-title>A tutorial on how not to over-interpret STRUCTURE and ADMIXTURE bar plots</article-title>. <source>Nat. Commun.</source><volume><bold>9</bold></volume>, <fpage>3258</fpage>. (<pub-id pub-id-type="doi">10.1038/s41467-018-05257-7</pub-id>)<pub-id pub-id-type="pmid">30108219</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C30">
      <label>30<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hotelling</surname><given-names>H</given-names></string-name></person-group>. <year>1936</year><article-title>Relations between two sets of variates</article-title>. <source>Biometrika</source><volume><bold>28</bold></volume>, <fpage>321</fpage>-<lpage>327</lpage>. (<pub-id pub-id-type="doi">10.1093/biomet/28.3-4.321</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C31">
      <label>31<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Seber</surname><given-names>GA</given-names></string-name></person-group>. <year>2009</year><source>Multivariate observations</source>, <volume>vol. 252</volume>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>John Wiley &amp; Sons</publisher-name>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C32">
      <label>32<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ter Braak</surname><given-names>CJ</given-names></string-name></person-group>. <year>1987</year><article-title>The analysis of vegetation-environment relationships by canonical correspondence analysis</article-title>. <source>Vegetatio</source><volume><bold>69</bold></volume>, <fpage>69</fpage>-<lpage>77</lpage>. (<pub-id pub-id-type="doi">10.1007/BF00038688</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C33">
      <label>33<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hardoon</surname><given-names>DR</given-names></string-name>, <string-name><surname>Szedmak</surname><given-names>S</given-names></string-name>, <string-name><surname>Shawe-Taylor</surname><given-names>J</given-names></string-name></person-group>. <year>2004</year><article-title>Canonical correlation analysis: an overview with application to learning methods</article-title>. <source>Neural Comput.</source><volume><bold>16</bold></volume>, <fpage>2639</fpage>-<lpage>2664</lpage>. (<pub-id pub-id-type="doi">10.1162/0899766042321814</pub-id>)<pub-id pub-id-type="pmid">15516276</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C34">
      <label>34<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Raghu</surname><given-names>M</given-names></string-name>, <string-name><surname>Gilmer</surname><given-names>J</given-names></string-name>, <string-name><surname>Yosinski</surname><given-names>J</given-names></string-name>, <string-name><surname>Sohl-Dickstein</surname><given-names>J</given-names></string-name></person-group>. <year>2017</year><comment>SVCCA: singular vector canonical correlation analysis for deep learning dynamics and interpretability. In <italic toggle="yes">Advances in neural information processing systems</italic>, vol. 30 (eds I Guyon, UV Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett), pp. 6076–6085. New York, NY: Curran Associates, Inc</comment>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C35">
      <label>35<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wasserman</surname><given-names>L</given-names></string-name></person-group>. <year>2018</year><article-title>Topological data analysis</article-title>. <source>Annu. Rev. Stat. Appl.</source><volume><bold>5</bold></volume>, <fpage>501</fpage>-<lpage>532</lpage>. (<pub-id pub-id-type="doi">10.1146/statistics.2018.5.issue-1</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C36">
      <label>36<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gu</surname><given-names>Z</given-names></string-name>, <string-name><surname>Eils</surname><given-names>R</given-names></string-name>, <string-name><surname>Schlesner</surname><given-names>M</given-names></string-name></person-group>. <year>2016</year><article-title>Complex heatmaps reveal patterns and correlations in multidimensional genomic data</article-title>. <source>Bioinformatics</source><volume><bold>32</bold></volume>, <fpage>2847</fpage>-<lpage>2849</lpage>. (<pub-id pub-id-type="doi">10.1093/bioinformatics/btw313</pub-id>)<pub-id pub-id-type="pmid">27207943</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C37">
      <label>37<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bibikova</surname><given-names>M</given-names></string-name><etal>et al.</etal></person-group><year>2011</year><article-title>High density DNA methylation array with single CpG site resolution</article-title>. <source>Genomics</source><volume><bold>98</bold></volume>, <fpage>288</fpage>-<lpage>295</lpage>. (<pub-id pub-id-type="doi">10.1016/j.ygeno.2011.07.007</pub-id>)<pub-id pub-id-type="pmid">21839163</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C38">
      <label>38<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Min</surname><given-names>JL</given-names></string-name><etal>et al.</etal></person-group><year>2021</year><article-title>Genomic and phenomic insights from an atlas of genetic effects on DNA methylation</article-title>. <source>Nat. Genet.</source><volume><bold>53</bold></volume>, <fpage>1311</fpage>-<lpage>1321</lpage>.<pub-id pub-id-type="pmid">34493871</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C39">
      <label>39<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lockhart</surname><given-names>DJ</given-names></string-name>, <string-name><surname>Winzeler</surname><given-names>EA</given-names></string-name></person-group>. <year>2000</year><article-title>Genomics, gene expression and DNA arrays</article-title>. <source>Nature</source><volume><bold>405</bold></volume>, <fpage>827</fpage>-<lpage>836</lpage>. (<pub-id pub-id-type="doi">10.1038/35015701</pub-id>)<pub-id pub-id-type="pmid">10866209</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C40">
      <label>40<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Trask</surname><given-names>L</given-names></string-name>, <string-name><surname>Millar</surname><given-names>RM</given-names></string-name></person-group>. <year>2015</year><source>Trask’s historical linguistics</source>, <edition>3rd edn</edition>. <publisher-loc>London, UK</publisher-loc>: <publisher-name>Routledge</publisher-name>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C41">
      <label>41<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Matras</surname><given-names>Y</given-names></string-name></person-group>. <year>2009</year><source>Contact linguistics</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C42">
      <label>42<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Thomason</surname><given-names>SG</given-names></string-name>, <string-name><surname>Kaufman</surname><given-names>T</given-names></string-name></person-group>. <year>1988</year><source>Language contact, creolization and genetic linguistics</source>. <publisher-loc>Berkeley, CA</publisher-loc>: <publisher-name>University of California Press</publisher-name>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C43">
      <label>43<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dellert</surname><given-names>J</given-names></string-name><etal>et al</etal></person-group>. <year>2020</year><article-title>NorthEuraLex: a wide-coverage lexical database of Northern Eurasia</article-title>. <source>Lang. Resour. Eval.</source><volume><bold>54</bold></volume>, <fpage>273</fpage>-<lpage>301</lpage>. (<pub-id pub-id-type="doi">10.1007/s10579-019-09480-6</pub-id>)<pub-id pub-id-type="pmid">32214931</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C44">
      <label>44<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Dellert</surname><given-names>J</given-names></string-name></person-group>. <year>2018</year><comment>Combining information-weighted sequence alignment and sound correspondence models for improved cognate detection. In <italic toggle="yes">Proc. of the 27th Int. Conf. on Computational Linguistics</italic>, pp. 3123–3133. Santa Fe, USA: Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C45">
      <label>45<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><collab>World Bank</collab>. <year>2018</year><comment>World development indicators</comment>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C46">
      <label>46<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><collab>EVS</collab>. <year>2011</year><comment>European values study</comment>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C47">
      <label>47<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><collab>WVS</collab>. <year>2017</year><comment>World value survey—what we do</comment>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C48">
      <label>48<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruck</surname><given-names>DJ</given-names></string-name>, <string-name><surname>Bentley</surname><given-names>RA</given-names></string-name>, <string-name><surname>Lawson</surname><given-names>DJ</given-names></string-name></person-group>. <year>2018</year><article-title>Religious change preceded economic change in the 20th century</article-title>. <source>Sci. Adv.</source><volume><bold>4</bold></volume>, <fpage>eaar8680</fpage>. (<pub-id pub-id-type="doi">10.1126/sciadv.aar8680</pub-id>)<pub-id pub-id-type="pmid">30035222</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C49">
      <label>49<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gorodnichenko</surname><given-names>Y</given-names></string-name>, <string-name><surname>Roland</surname><given-names>G</given-names></string-name></person-group>. <year>2016</year><article-title>Culture, institutions and the wealth of nations</article-title>. <source>Rev. Econ. Stat.</source><volume><bold>99</bold></volume>, <fpage>402</fpage>-<lpage>416</lpage>. (<pub-id pub-id-type="doi">10.1162/REST_a_00599</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C50">
      <label>50<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nikolaev</surname><given-names>B</given-names></string-name>, <string-name><surname>Boudreaux</surname><given-names>C</given-names></string-name>, <string-name><surname>Salahodjaev</surname><given-names>R</given-names></string-name></person-group>. <year>2017</year><article-title>Are individualistic societies less equal? Evidence from the parasite stress theory of values</article-title>. <source>J. Econ. Behav. Organ.</source><volume><bold>138</bold></volume>, <fpage>30</fpage>-<lpage>49</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jebo.2017.04.001</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C51">
      <label>51<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Inglehart</surname><given-names>R</given-names></string-name>, <string-name><surname>Welzel</surname><given-names>C</given-names></string-name></person-group>. <year>2005</year><source>Modernization, cultural change, and democracy: the human development sequence</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C52">
      <label>52<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sinding Bentzen</surname><given-names>J</given-names></string-name></person-group>. <year>2019</year><article-title>Acts of God? Religiosity and natural disasters across subnational world districts</article-title>. <source>Econ. J.</source><volume><bold>129</bold></volume>, <fpage>2295</fpage>-<lpage>2321</lpage>. (<pub-id pub-id-type="doi">10.1093/ej/uez008</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C53">
      <label>53<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bentley</surname><given-names>RA</given-names></string-name>, <string-name><surname>Acerbi</surname><given-names>A</given-names></string-name>, <string-name><surname>Ormerod</surname><given-names>P</given-names></string-name>, <string-name><surname>Lampos</surname><given-names>V</given-names></string-name></person-group>. <year>2014</year><article-title>Books average previous decade of economic misery</article-title>. <source>PLoS ONE</source><volume><bold>9</bold></volume>, <fpage>e83147</fpage>. (<pub-id pub-id-type="doi">10.1371/journal.pone.0083147</pub-id>)<pub-id pub-id-type="pmid">24416159</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C54">
      <label>54<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><collab>CIA</collab>. <year>2018</year><comment>The world factbook 2018</comment>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C55">
      <label>55<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Putterman</surname><given-names>L</given-names></string-name>, <string-name><surname>Weil</surname><given-names>DN</given-names></string-name></person-group>. <year>2010</year><article-title>Post-1500 population flows and the long run determinants of economic growth and inequality</article-title>. <source>Q. J. Econ.</source><volume><bold>125</bold></volume>, <fpage>1627</fpage>-<lpage>1682</lpage>. (<pub-id pub-id-type="doi">10.1162/qjec.2010.125.4.1627</pub-id>)<pub-id pub-id-type="pmid">24478530</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C56">
      <label>56<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Matthews</surname><given-names>LJ</given-names></string-name>, <string-name><surname>Edmonds</surname><given-names>J</given-names></string-name>, <string-name><surname>Wildman</surname><given-names>WJ</given-names></string-name>, <string-name><surname>Nunn</surname><given-names>CL</given-names></string-name></person-group>. <year>2013</year><article-title>Cultural inheritance or cultural diffusion of religious violence? A quantitative case study of the radical reformation</article-title>. <source>Relig. Brain Behav.</source><volume><bold>3</bold></volume>, <fpage>3</fpage>-<lpage>15</lpage>. (<pub-id pub-id-type="doi">10.1080/2153599X.2012.707388</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C57">
      <label>57<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Matthews</surname><given-names>LJ</given-names></string-name>, <string-name><surname>Passmore</surname><given-names>S</given-names></string-name>, <string-name><surname>Richard</surname><given-names>PM</given-names></string-name>, <string-name><surname>Gray</surname><given-names>RD</given-names></string-name>, <string-name><surname>Atkinson</surname><given-names>QD</given-names></string-name></person-group>. <year>2016</year><article-title>Shared cultural history as a predictor of political and economic changes among nation states</article-title>. <source>PLoS ONE</source><volume><bold>11</bold></volume>, <fpage>e0152979</fpage>. (<pub-id pub-id-type="doi">10.1371/journal.pone.0152979</pub-id>)<pub-id pub-id-type="pmid">27110713</pub-id></mixed-citation>
    </ref>
    <ref id="RSOS202182C58">
      <label>58<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Spolaore</surname><given-names>E</given-names></string-name>, <string-name><surname>Wacziarg</surname><given-names>R</given-names></string-name></person-group>. <year>2013</year><article-title>How deep are the roots of economic development?</article-title><source>J. Econ. Lit.</source><volume><bold>51</bold></volume>, <fpage>325</fpage>-<lpage>369</lpage>. (<pub-id pub-id-type="doi">10.1257/jel.51.2.325</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C59">
      <label>59<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eckart</surname><given-names>C</given-names></string-name>, <string-name><surname>Young</surname><given-names>G</given-names></string-name></person-group>. <year>1936</year><article-title>The approximation of one matrix by another of lower rank</article-title>. <source>Psychometrika</source><volume><bold>1</bold></volume>, <fpage>211</fpage>-<lpage>218</lpage>. (<pub-id pub-id-type="doi">10.1007/BF02288367</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C60">
      <label>60<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Lee</surname><given-names>DD</given-names></string-name>, <string-name><surname>Seung</surname><given-names>HS</given-names></string-name></person-group>. <year>2001</year><comment>Algorithms for non-negative matrix factorization. In <italic toggle="yes">Advances in neural information processing systems</italic>, vol. 13 (eds TK Leen, TG Dietterich, V Tresp), pp. 556–562. Cambridge, MA: MIT Press</comment>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C61">
      <label>61<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Efron</surname><given-names>B</given-names></string-name>, <string-name><surname>Tibshirani</surname><given-names>RJ</given-names></string-name>, <string-name><surname>Tibshirani</surname><given-names>RJ</given-names></string-name></person-group>. <year>1994</year><source>An introduction to the bootstrap</source>. <publisher-loc>London, UK</publisher-loc>: <publisher-name>Chapman and Hall/CRC</publisher-name>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C62">
      <label>62<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Paradis</surname><given-names>E</given-names></string-name><etal>et al.</etal></person-group><year>2011</year><comment>Package ‘ape’: analysis of phylogenetics and evolution. See <uri xlink:href="http://cran.r-project.org/web/packages/ape/ape.pdf">http://cran.r-project.org/web/packages/ape/ape.pdf</uri>, pp. 1–222</comment>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C63">
      <label>63<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="book"><collab>R Core Team</collab>. <year>2018</year><source>R: a language and environment for statistical computing</source>. <publisher-loc>Vienna, Austria</publisher-loc>: <publisher-name>R Foundation for Statistical Computing</publisher-name>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C64">
      <label>64<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dellert</surname><given-names>J</given-names></string-name>, <string-name><surname>Buch</surname><given-names>A</given-names></string-name></person-group>. <year>2018</year><article-title>A new approach to concept basicness and stability as a window to the robustness of concept list rankings</article-title>. <source>Lang. Dyn. Change</source><volume><bold>8</bold></volume>, <fpage>157</fpage>-<lpage>181</lpage>. (<pub-id pub-id-type="doi">10.1163/22105832-00802001</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSOS202182C65">
      <label>65<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Rama</surname><given-names>T</given-names></string-name>, <string-name><surname>List</surname><given-names>J-M</given-names></string-name>, <string-name><surname>Wahle</surname><given-names>J</given-names></string-name>, <string-name><surname>Jäger</surname><given-names>G</given-names></string-name></person-group>. <year>2018</year><comment>Are automatic methods for cognate detection good enough for phylogenetic reconstruction in historical linguistics? In <italic toggle="yes">Proc. of NAACL-HLT 2018</italic>, pp. 393–400. Association for Computational Linguistics</comment>.</mixed-citation>
    </ref>
    <ref id="RSOS202182C66">
      <label>66<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname><given-names>T</given-names></string-name>, <string-name><surname>Samworth</surname><given-names>RJ</given-names></string-name></person-group>. <year>2015</year><article-title>A useful variant of the Davis–Kahan theorem for statisticians</article-title>. <source>Biometrika</source><volume><bold>102</bold></volume>, <fpage>315</fpage>-<lpage>323</lpage>. (<pub-id pub-id-type="doi">10.1093/biomet/asv008</pub-id>)</mixed-citation>
    </ref>
  </ref-list>
</back>
