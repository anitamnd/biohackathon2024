<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8097688</article-id>
    <article-id pub-id-type="pmid">33016991</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa855</article-id>
    <article-id pub-id-type="publisher-id">btaa855</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Gene Expression</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>selectBoost: a general algorithm to enhance the performance of variable selection methods</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0837-8281</contrib-id>
        <name>
          <surname>Bertrand</surname>
          <given-names>Frédéric</given-names>
        </name>
        <xref rid="btaa855-cor1" ref-type="corresp"/>
        <!--frederic.bertrand@utt.fr-->
        <aff><institution>Institut de Recherche Mathématique Avancée, CNRS UMR 7501, Labex IRMIA, Université de Strasbourg</institution>, Strasbourg, <country country="FR">France</country></aff>
        <aff><institution>Université de Technologie de Troyes, ICD, ROSAS, M2S</institution>, Troyes, <country country="FR">France</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Aouadi</surname>
          <given-names>Ismaïl</given-names>
        </name>
        <aff><institution>ImmunoRhumatologie Moléculaire, INSERM UMR_S 1109, LabEx TRANSPLANTEX, Centre de Recherche d’Immunologie et d’Hématologie, Fédération de Médecine Translationnelle de Strasbourg (FMTS), Université de Strasbourg</institution>, Strasbourg, <country country="FR">France</country></aff>
        <aff><institution>Laboratoire International Associé (LIA) INSERM, Strasbourg (France) - Nagano (Japan)</institution>, Strasbourg, <country country="FR">France</country></aff>
        <aff><institution>Fédération Hospitalo-Universitaire (FHU) OMICARE, Laboratoire Central d’Immunologie, Pôle de Biologie, Nouvel Hôpital Civil, Hôpitaux Universitaires de Strasbourg</institution>, Strasbourg, <country country="FR">France</country></aff>
        <xref rid="btaa855-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jung</surname>
          <given-names>Nicolas</given-names>
        </name>
        <aff><institution>Institut de Recherche Mathématique Avancée, CNRS UMR 7501, Labex IRMIA, Université de Strasbourg</institution>, Strasbourg, <country country="FR">France</country></aff>
        <aff><institution>ImmunoRhumatologie Moléculaire, INSERM UMR_S 1109, LabEx TRANSPLANTEX, Centre de Recherche d’Immunologie et d’Hématologie, Fédération de Médecine Translationnelle de Strasbourg (FMTS), Université de Strasbourg</institution>, Strasbourg, <country country="FR">France</country></aff>
        <xref rid="btaa855-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Carapito</surname>
          <given-names>Raphael</given-names>
        </name>
        <aff><institution>ImmunoRhumatologie Moléculaire, INSERM UMR_S 1109, LabEx TRANSPLANTEX, Centre de Recherche d’Immunologie et d’Hématologie, Fédération de Médecine Translationnelle de Strasbourg (FMTS), Université de Strasbourg</institution>, Strasbourg, <country country="FR">France</country></aff>
        <aff><institution>Laboratoire International Associé (LIA) INSERM, Strasbourg (France) - Nagano (Japan)</institution>, Strasbourg, <country country="FR">France</country></aff>
        <aff><institution>Fédération Hospitalo-Universitaire (FHU) OMICARE, Laboratoire Central d’Immunologie, Pôle de Biologie, Nouvel Hôpital Civil, Hôpitaux Universitaires de Strasbourg</institution>, Strasbourg, <country country="FR">France</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Vallat</surname>
          <given-names>Laurent</given-names>
        </name>
        <aff><institution>ImmunoRhumatologie Moléculaire, INSERM UMR_S 1109, LabEx TRANSPLANTEX, Centre de Recherche d’Immunologie et d’Hématologie, Fédération de Médecine Translationnelle de Strasbourg (FMTS), Université de Strasbourg</institution>, Strasbourg, <country country="FR">France</country></aff>
        <aff><institution>Fédération Hospitalo-Universitaire (FHU) OMICARE, Laboratoire Central d’Immunologie, Pôle de Biologie, Nouvel Hôpital Civil, Hôpitaux Universitaires de Strasbourg</institution>, Strasbourg, <country country="FR">France</country></aff>
        <xref rid="btaa855-cor2" ref-type="corresp"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bahram</surname>
          <given-names>Seiamak</given-names>
        </name>
        <aff><institution>ImmunoRhumatologie Moléculaire, INSERM UMR_S 1109, LabEx TRANSPLANTEX, Centre de Recherche d’Immunologie et d’Hématologie, Fédération de Médecine Translationnelle de Strasbourg (FMTS), Université de Strasbourg</institution>, Strasbourg, <country country="FR">France</country></aff>
        <aff><institution>Laboratoire International Associé (LIA) INSERM, Strasbourg (France) - Nagano (Japan)</institution>, Strasbourg, <country country="FR">France</country></aff>
        <aff><institution>Fédération Hospitalo-Universitaire (FHU) OMICARE, Laboratoire Central d’Immunologie, Pôle de Biologie, Nouvel Hôpital Civil, Hôpitaux Universitaires de Strasbourg</institution>, Strasbourg, <country country="FR">France</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Maumy-Bertrand</surname>
          <given-names>Myriam</given-names>
        </name>
        <aff><institution>Institut de Recherche Mathématique Avancée, CNRS UMR 7501, Labex IRMIA, Université de Strasbourg</institution>, Strasbourg, <country country="FR">France</country></aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Lenore</surname>
          <given-names>Cowen</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <fn id="btaa855-FM1">
        <p>Ismaïl Aouadi and Nicolas Jung authors wish it to be known that these authors contributed equally.</p>
      </fn>
      <corresp id="btaa855-cor2">Present address: IRFAC, INSERM UMR_S1113, FMTS, Université de Strasbourg, Strasbourg, France</corresp>
      <corresp id="btaa855-cor1">To whom correspondence should be addressed. <email>frederic.bertrand@utt.fr</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>3</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-10-05">
      <day>05</day>
      <month>10</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>10</month>
      <year>2020</year>
    </pub-date>
    <volume>37</volume>
    <issue>5</issue>
    <fpage>659</fpage>
    <lpage>668</lpage>
    <history>
      <date date-type="received">
        <day>19</day>
        <month>7</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>02</day>
        <month>9</month>
        <year>2020</year>
      </date>
      <date date-type="editorial-decision">
        <day>19</day>
        <month>9</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>21</day>
        <month>9</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa855.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>With the growth of big data, variable selection has become one of the critical challenges in statistics. Although many methods have been proposed in the literature, their performance in terms of recall (sensitivity) and precision (predictive positive value) is limited in a context where the number of variables by far exceeds the number of observations or in a highly correlated setting.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>In this article, we propose a general algorithm, which improves the precision of any existing variable selection method. This algorithm is based on highly intensive simulations and takes into account the correlation structure of the data. Our algorithm can either produce a confidence index for variable selection or be used in an experimental design planning perspective. We demonstrate the performance of our algorithm on both simulated and real data. We then apply it in two different ways to improve biological network reverse-engineering.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Code is available as the <monospace><bold>SelectBoost</bold></monospace> package on the CRAN, <ext-link xlink:href="https://cran.r-project.org/package=SelectBoost" ext-link-type="uri">https://cran.r-project.org/package=SelectBoost</ext-link>. Some network reverse-engineering functionalities are available in the Patterns CRAN package, <ext-link xlink:href="https://cran.r-project.org/package=Patterns" ext-link-type="uri">https://cran.r-project.org/package=Patterns</ext-link>.</p>
      </sec>
      <sec id="s7">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Agence Nationale de la Recherche</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001665</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>ANR-11-LABX-0070_TRANSPLANTEX</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>INSERM</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001677</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>UMR_S 1109</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Institut Universitaire de France</institution>
            <institution-id institution-id-type="DOI">10.13039/501100004795</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>MSD-Avenir</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>European regional development fund</institution>
            <institution-id institution-id-type="DOI">10.13039/501100008530</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Agence Nationale de la Recherche</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001665</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>ANR-11-LABX-0055_IRMIA</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>CNRS</institution>
            <institution-id institution-id-type="DOI">10.13039/100012681</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>UMR 7501</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>French HPC Center ROMEO</institution>
          </institution-wrap>
        </funding-source>
        <award-id>UR 201923174L</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="10"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Technological innovations make it possible to measure large amounts of data in a single observation. As a consequence, problems in which the number <italic toggle="yes">P</italic> of variables is larger than the number <italic toggle="yes">N</italic> of observations have become common. As reviewed by <xref rid="btaa855-B20" ref-type="bibr">Fan and Li (2006)</xref>, such situations arise in many fields from fundamental sciences to social science, and variable selection is required to tackle these issues. For example, in biology/medicine, thousands of messenger RNA (mRNA) expressions (<xref rid="btaa855-B31" ref-type="bibr">Lipshutz <italic toggle="yes">et al</italic>., 1999</xref>) may be potential predictors of some disease. Moreover, in such studies, the correlation between variables is often very strong (<xref rid="btaa855-B41" ref-type="bibr">Segal <italic toggle="yes">et al</italic>., 2003</xref>), and variable selection methods often fail to make the distinction between the informative variables and those which are not. Similarly, inference of gene regulatory networks from perturbation data can enhance the insights of a biological system (<xref rid="btaa855-B35" ref-type="bibr">Morgan <italic toggle="yes">et al</italic>., 2019</xref>). In this article, we propose a general algorithm that enhances model selection in correlated variables.</p>
    <p>First, we will assume a statistical model with a response variable <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> (with the symbol ‘’’ as the transposed), a variable matrix of size <italic toggle="yes">N </italic>×<italic toggle="yes"> P</italic>, <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mn>1.</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and a vector of parameters <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo>β</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mo>β</mml:mo><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>. Then, we will assume that the vector of parameters <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo>β</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mo>β</mml:mo><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula> is sparse. In other words, we will assume that <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>β</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> except for a quite small proportion of elements of the vector. We note <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mi mathvariant="script">S</mml:mi></mml:math></inline-formula> as the set of indices for which <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>β</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mi>q</mml:mi><mml:mo>&lt;</mml:mo><mml:mo>∞</mml:mo></mml:mrow></mml:math></inline-formula> is the cardinality of this set <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mi mathvariant="script">S</mml:mi></mml:math></inline-formula>. Without any loss of generality, we will assume that <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>β</mml:mo><mml:mi>p</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> if and only if <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
    <p>When dealing with a problem of variable selection, one of the goals is the estimation of the support, in which you want <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script">S</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> to be close to one, with <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script">S</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>k</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. Here, our interest is mainly as follows, i.e. in identifying the correct support <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mi mathvariant="script">S</mml:mi></mml:math></inline-formula>. This kind of issue arises in many fields, e.g. in biology, where it is of greatest interest to discover which specific molecules are involved in a disease (<xref rid="btaa855-B20" ref-type="bibr">Fan and Li, 2006</xref>).</p>
    <p>There is a vast literature dealing with the problem of variable selection in both statistical and machine-learning areas (<xref rid="btaa855-B20" ref-type="bibr">Fan and Li, 2006</xref>; <xref rid="btaa855-B21" ref-type="bibr">Fan and Lv, 2010</xref>). The main variable selection methods can be gathered in the common framework of penalized likelihood. The estimate <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is then given by:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mo>ℓ</mml:mo><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mtext>pen</mml:mtext></mml:mrow><mml:mo>λ</mml:mo></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo>β</mml:mo><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>ℓ</mml:mo><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the log-likelihood function, <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mtext>pen</mml:mtext></mml:mrow><mml:mo>λ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is a penalty function and <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:mo>λ</mml:mo><mml:mo>∈</mml:mo><mml:mo>ℝ</mml:mo></mml:mrow></mml:math></inline-formula> is the regularization parameter. As the goal is to obtain a sparse estimation of the vector of parameters <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">β</mml:mi></mml:math></inline-formula>, a natural choice for the penalty function is to use the so-called <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> norm (<inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mo>.</mml:mo><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>), which corresponds to the number of non-vanishing elements of a vector:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mtext>pen</mml:mtext></mml:mrow><mml:mo>λ</mml:mo></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>:</mml:mo></mml:mtd><mml:mtd><mml:mo>ℝ</mml:mo></mml:mtd><mml:mtd><mml:mo>↦</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>λ</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mi>x</mml:mi></mml:mtd><mml:mtd><mml:mo>↦</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mtext>pen</mml:mtext></mml:mrow><mml:mo>λ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>λ</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext><mml:mo> </mml:mo><mml:mi>x</mml:mi><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mtext>pen</mml:mtext></mml:mrow><mml:mo>λ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>else</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>which induces <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mtext>pen</mml:mtext></mml:mrow><mml:mo>λ</mml:mo></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo>β</mml:mo><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>λ</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. For example, when <italic toggle="yes">λ</italic> =1, we get the Akaike Information Criterion (AIC) (<xref rid="btaa855-B2" ref-type="bibr">Akaike, 1974</xref>) and when <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:mo>λ</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>, we get the Bayesian Information Criterion (BIC) (<xref rid="btaa855-B40" ref-type="bibr">Schwarz, 1978</xref>).</p>
    <p>Many different penalties can be found in the literature. Solving this problem with <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">‖</mml:mi><mml:mo>.</mml:mo><mml:msub><mml:mi mathvariant="normal">‖</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> as part of the penalty is an NP-hard problem (<xref rid="btaa855-B21" ref-type="bibr">Fan and Lv, 2010</xref>; <xref rid="btaa855-B36" ref-type="bibr">Natarajan, 1995</xref>). It cannot be used in practice when <italic toggle="yes">P</italic> becomes large, even when it is employed with some search strategy like forward regression, stepwise regression (<xref rid="btaa855-B26" ref-type="bibr">Hocking, 1976</xref>) and genetic algorithms (<xref rid="btaa855-B30" ref-type="bibr">Koza <italic toggle="yes">et al</italic>., 1999</xref>). <xref rid="btaa855-B16" ref-type="bibr">Donoho and Elad (2003)</xref> showed that relaxing <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mo>.</mml:mo><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> to norm <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mo>.</mml:mo><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> ends, under some assumptions, to the same estimation. This result encourages the use of a wide range of penalties based on different norms. For example, the case where <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mtext>pen</mml:mtext></mml:mrow><mml:mo>λ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo>β</mml:mo><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>λ</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mo>β</mml:mo><mml:mi>p</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> is the lasso estimator (<xref rid="btaa855-B43" ref-type="bibr">Tibshirani, 1996</xref>) [or equivalently Basis Pursuit Denoising (<xref rid="btaa855-B10" ref-type="bibr">Chen <italic toggle="yes">et al</italic>., 2001</xref>)] whereas <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mtext>pen</mml:mtext></mml:mrow><mml:mo>λ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo>β</mml:mo><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>λ</mml:mo><mml:msubsup><mml:mo>β</mml:mo><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> leads to the Ridge estimator (<xref rid="btaa855-B27" ref-type="bibr">Hoerl and Kennard, 1970</xref>). Nevertheless, the penalty term induces variable selection only if:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi>x</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mtext>dpen</mml:mtext></mml:mrow><mml:mo>λ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:math></disp-formula></p>
    <p><xref rid="E3" ref-type="disp-formula">Equation (3)</xref> explains why the lasso regression allows for variable selection, while the Ridge regression does not. The lasso regression is, however, known to lead to a biased estimate (<xref rid="btaa855-B51" ref-type="bibr">Zou, 2006</xref>). The Smoothly Clipped Absolute Deviation (SCAD) (<xref rid="btaa855-B19" ref-type="bibr">Fan, 1997</xref>), Minimax Concave Penalty (<xref rid="btaa855-B48" ref-type="bibr">Zhang, 2010</xref>) or adaptive lasso (<xref rid="btaa855-B51" ref-type="bibr">Zou, 2006</xref>) penalties all address this problem. The popularity of such variable selection methods is linked to fast algorithms like Least-Angle Regression Selection (<xref rid="btaa855-B17" ref-type="bibr">Efron <italic toggle="yes">et al</italic>., 2004</xref>), coordinate descent or Penalized Linear Unbiased Selection (<xref rid="btaa855-B48" ref-type="bibr">Zhang, 2010</xref>).</p>
    <p>Nevertheless, the goal of identifying the correct support of the regression is complicated and the reason why variable selection methods fail to select the set of non-zero variables <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mi mathvariant="script">S</mml:mi></mml:math></inline-formula> can be summarized in two words: linear correlation. Choosing the lasso regression as a special case, <xref rid="btaa855-B49" ref-type="bibr">Zhao and Yu (2006)</xref> stated that if an irrelevant predictor is highly correlated with the predictors in the true model, lasso may not be able to distinguish it from the true predictors with any amount of data and any amount of regularization. <xref rid="btaa855-B49" ref-type="bibr">Zhao and Yu (2006)</xref> [and simultaneously <xref rid="btaa855-B51" ref-type="bibr">Zou (2006)</xref>] found an almost necessary and sufficient condition for lasso sign consistency (i.e. selecting the non-zero variables with the correct sign). This condition is known as ‘irrepresentable condition’:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:msub><mml:mo>′</mml:mo><mml:mrow><mml:mi mathvariant="normal">∖</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:msub><mml:mo>′</mml:mo><mml:mi mathvariant="script">S</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mtext>sgn</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi mathvariant="normal">∖</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo>β</mml:mo><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. In other words, when <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:mtext>sgn</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, this can be seen as the regression of each variable, which is not in <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mi mathvariant="script">S</mml:mi></mml:math></inline-formula> over the variables, which are in <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mi mathvariant="script">S</mml:mi></mml:math></inline-formula>. As all variables in the matrix <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mi mathvariant="bold">X</mml:mi></mml:math></inline-formula> are centered, the absolute sum of the regression parameters should be smaller than 1 to satisfy this ‘irrepresentable condition’.</p>
    <p>Facing this issue, existing variable selection methods can be split into two categories:
</p>
    <list list-type="bullet">
      <list-item>
        <p>those which are ‘regularized’ and try to give similar coefficients to correlated variables [e.g. elastic net (<xref rid="btaa855-B52" ref-type="bibr">Zou and Hastie, 2005</xref>)],</p>
      </list-item>
      <list-item>
        <p>those which are not ‘regularized’ and pick up one variable among a set of correlated variables [e.g. the lasso (<xref rid="btaa855-B43" ref-type="bibr">Tibshirani, 1996</xref>)].</p>
      </list-item>
    </list>
    <p>The former group can further be split into methods in which groups of correlation are known, such as the group lasso (<xref rid="btaa855-B22" ref-type="bibr">Friedman <italic toggle="yes">et al</italic>., 2010a</xref>; <xref rid="btaa855-B47" ref-type="bibr">Yuan and Lin, 2006</xref>) and those in which groups are not known as in the elastic net (<xref rid="btaa855-B52" ref-type="bibr">Zou and Hastie, 2005</xref>). The latter combines the <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and the <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> norm and takes advantage of both. Non-regularized methods will select some co-variables among a group of correlated variables while regularized methods will select all variables in the same group with similar coefficients.</p>
    <p>The main idea of our algorithm is to consider that any observed value of a group of linearly correlated variables of the <bold>X</bold> matrix is the independent realization of a given random function. This common random function is then used to perturb the observed values of the relevant correlated variables. Strictly speaking, the use of noise to determine the informative variables is not a new idea. For example, it has been shown that adding random pseudo-variables decreases over-fitting (<xref rid="btaa855-B46" ref-type="bibr">Wu <italic toggle="yes">et al</italic>., 2007</xref>). In the case where <italic toggle="yes">P</italic> &gt;<italic toggle="yes"> N</italic>, the pseudo-variables are generated either with a standard normal distribution <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> or by using permutations on the matrix <bold>X</bold> (<xref rid="btaa855-B46" ref-type="bibr">Wu <italic toggle="yes">et al.</italic>, 2007</xref>). Another approach consists of adding noise to the response variable and leads to similar results (<xref rid="btaa855-B32" ref-type="bibr">Luo <italic toggle="yes">et al</italic>., 2006</xref>). The rationale of this method is based on the work of <xref rid="btaa855-B14" ref-type="bibr">Cook and Stefanski (1994)</xref>, which introduces the simulation-based algorithm SIMEX (<xref rid="btaa855-B14" ref-type="bibr">Cook and Stefanski, 1994</xref>). Adding noise to the matrix <bold>X</bold> has already been used in the context of microarrays (<xref rid="btaa855-B9" ref-type="bibr">Chen <italic toggle="yes">et al</italic>., 2007</xref>). Simsel (<xref rid="btaa855-B18" ref-type="bibr">Eklund and Zwanzig, 2012</xref>) is an algorithm that both adds noise to variables and uses random pseudo-variables. One new and inspiring approach is stability selection (<xref rid="btaa855-B34" ref-type="bibr">Meinshausen and Bühlmann, 2010</xref>) in which the variable selection method is applied on sub-samples, and informative variables are defined as variables which have a high probability of being selected. Bootstrapping has been applied to the lasso on both the response variable and the matrix <bold>X</bold> with better results in the former case (<xref rid="btaa855-B3" ref-type="bibr">Bach <italic toggle="yes">et al</italic>., 2008</xref>). A random lasso, in which variables are weighted with random weights, has also been introduced (<xref rid="btaa855-B45" ref-type="bibr">Wang <italic toggle="yes">et al</italic>., 2011</xref>).</p>
    <p>In this article, following the idea of using simulation to enhance the variable selection methods, we propose the SelectBoost algorithm. Unlike other algorithms reviewed above, it takes into account the correlation structure of the data. Furthermore, our algorithm is motivated by the fact that in the case of non-regularized variable selection methods, if a group contains variables that are highly correlated together, one of them will be chosen with precision.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <p>The SelectBoost algorithm has been designed in a general framework in order to avoid to select non-predictive correlated features. The main goal is to improve the predictive positive value (PPV), i.e. the proportion of selected variables which truly belong to <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mi mathvariant="script">S</mml:mi></mml:math></inline-formula>.</p>
    <sec>
      <title>2.1 Generate new perturbed design matrix</title>
      <p>As we assume that the variables are centered and that <inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>, we know that <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Indeed, the normalization puts the variables on the unit sphere <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The process of centering can be seen as a projection on the hyperplane <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> with the unit vector as normal vector. Moreover, the intersection between <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is <inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. We further define the following isomorphism:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mo>ϕ</mml:mo></mml:mtd><mml:mtd><mml:mo>:</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mo>→</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>↦</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is an orthogonal base of <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="script">H</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE49"><mml:math id="IM49" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the canonical base of <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. We define:
<disp-formula id="E6"><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi mathvariant="bold">e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:msub><mml:mi mathvariant="bold">e</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi mathvariant="bold">e</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:msub><mml:mi mathvariant="bold">e</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>with <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold">e</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> the canonical base of <inline-formula id="IE52"><mml:math id="IM52" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. Note that <inline-formula id="IE53"><mml:math id="IM53" display="inline" overflow="scroll"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="script">S</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, and that is why we can work in <inline-formula id="IE54"><mml:math id="IM54" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and then return in <inline-formula id="IE55"><mml:math id="IM55" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
      <p>Here, we make the assumption that a group of correlated variables are independent realizations of the same multivariate Gaussian distribution. As the variables are normalized with respect to the <inline-formula id="IE56"><mml:math id="IM56" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> norm, we will use the von Mises–Fisher distribution (<xref rid="btaa855-B42" ref-type="bibr">Sra, 2012</xref>) in <inline-formula id="IE57"><mml:math id="IM57" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> thanks to the isomorphism <inline-formula id="IE58"><mml:math id="IM58" display="inline" overflow="scroll"><mml:mo>ϕ</mml:mo></mml:math></inline-formula> in order to generate new perturbed design matrix. The probability density function of the von Mises–Fisher distribution for the random <italic toggle="yes">P</italic>-dimensional unit vector <inline-formula id="IE59"><mml:math id="IM59" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:math></inline-formula> is given by:
<disp-formula id="E7"><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>;</mml:mo><mml:mo mathvariant="bold-italic">μ</mml:mo><mml:mo>,</mml:mo><mml:mo>κ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>κ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>κ</mml:mo><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>′</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE60"><mml:math id="IM60" display="inline" overflow="scroll"><mml:mrow><mml:mo>κ</mml:mo><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo>μ</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mo>μ</mml:mo><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and the normalization constant <inline-formula id="IE61"><mml:math id="IM61" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>κ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is equal to:
<disp-formula id="E8"><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>κ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mo>κ</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>π</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>κ</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">I<sub>v</sub></italic> denotes the modified Bessel function of the first kind and order <italic toggle="yes">v</italic> (<xref rid="btaa855-B1" ref-type="bibr">Abramowitz and Stegun, 1972</xref>). We denote by <inline-formula id="IE62"><mml:math id="IM62" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE63"><mml:math id="IM63" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>κ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> the maximum likelihood estimators of the <inline-formula id="IE64"><mml:math id="IM64" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">μ</mml:mi></mml:math></inline-formula> and <italic toggle="yes">κ</italic> parameters.</p>
      <p>The multivariate Gaussian distribution assumption is not restrictive. As long as the group of correlated variables is independent realizations of the same distribution, the SelectBoost algorithm can be applied: either directly to assess the stability of the selected variables with perturbed datasets with an increasing noise level, which is the core idea behind the SelectBoost algorithm, or after replacing the von Mises–Fisher distribution with a more relevant one.</p>
    </sec>
    <sec>
      <title>2.2 The SelectBoost algorithm</title>
      <p>To use the SelectBoost algorithm, we need a grouping method <inline-formula id="IE65"><mml:math id="IM65" display="inline" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> depending on a user-provided constant <inline-formula id="IE66"><mml:math id="IM66" display="inline" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. This constant determines the strength of the grouping effect. The grouping method maps each variable index <inline-formula id="IE67"><mml:math id="IM67" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> to an element of <inline-formula id="IE68"><mml:math id="IM68" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo>}</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> [with <inline-formula id="IE69"><mml:math id="IM69" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> the powerset of the set <italic toggle="yes">S</italic>, i.e. the set which contains all the subsets of <italic toggle="yes">S</italic>]. Concretely, <inline-formula id="IE70"><mml:math id="IM70" display="inline" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the set of all variables, which are considered to be linked to the variable <inline-formula id="IE71"><mml:math id="IM71" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE72"><mml:math id="IM72" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the submatrix of <bold>X</bold> containing the columns which indices are in <inline-formula id="IE73"><mml:math id="IM73" display="inline" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We impose the following constraints to the grouping function:
<disp-formula id="E9"><label>(6)</label><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mo>∀</mml:mo><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>P</mml:mi><mml:mo>}</mml:mo><mml:mo>:</mml:mo><mml:mi>g</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>p</mml:mi><mml:mo>}</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mtext>and</mml:mtext><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mi>g</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>P</mml:mi><mml:mo>}</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>Furthermore, we need to have a selection method:
<disp-formula id="E10"><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="normal">select</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mo>:</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mo>→</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>which maps the design matrix <bold>X</bold> and the response variable <bold>y</bold> to a 0–1 vector of length <italic toggle="yes">P</italic> with 1 at position <italic toggle="yes">p</italic> if the method selects the variable <italic toggle="yes">p</italic> and 0 otherwise. We then use the von Mises–Fisher distribution to generate replacement of the original variables by some simulations (see Algorithm 1) to create <italic toggle="yes">B</italic> new design matrices <inline-formula id="IE74"><mml:math id="IM74" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The SelectBoost algorithm then applies the variable selection method <italic toggle="yes">select</italic> to each of these matrices and returns a vector of length <italic toggle="yes">P</italic> with the frequency of apparition of each variable. The frequency of apparition of variable <inline-formula id="IE75"><mml:math id="IM75" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, noted <italic toggle="yes">ζ<sub>p</sub></italic> is assumed to be an estimator of the probability <inline-formula id="IE76"><mml:math id="IM76" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">ℙ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for this variable to be in <inline-formula id="IE77"><mml:math id="IM77" display="inline" overflow="scroll"><mml:mi mathvariant="script">S</mml:mi></mml:math></inline-formula>. The choice of <italic toggle="yes">c</italic><sub>0</sub> is crucial. On the one hand, when this constant is too large, the model is not perturbed enough. On the other hand, when this constant is too small, variables are chosen at random.</p>
      <p>The SelectBoost algorithm returns the vector <inline-formula id="IE78"><mml:math id="IM78" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo>ζ</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mo>ζ</mml:mo><mml:mi>P</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>. Each of these values has to be compared to a threshold <inline-formula id="IE79"><mml:math id="IM79" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>ζ</mml:mo><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to determine which variables are selected: we choose to select a variable <italic toggle="yes">p</italic> if <inline-formula id="IE80"><mml:math id="IM80" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>ζ</mml:mo><mml:mi>p</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mo>ζ</mml:mo><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The simulation study showed that the choice of the threshold is critical and the algorithm can be improved if we enforce that the <italic toggle="yes">ζ<sub>p</sub></italic> values—as functions of <italic toggle="yes">c</italic><sub>0</sub>—are non-increasing, see <xref rid="btaa855-F1" ref-type="fig">Figure 1</xref> bottom. This additional requirement makes sense: the more variables the resampling process involves—with smaller <italic toggle="yes">c</italic><sub>0</sub>— the less a given variable will be selected.
</p>
      <fig position="float" id="btaa855-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Top: evolution of the recall, PPV and <italic toggle="yes">F</italic>-score as a function of <inline-formula id="IE81"><mml:math id="IM81" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> for LASSO-based SelectBoost and AICc model selection criterion for Type1 simulated data with a non-increasing post-processing step and a threshold <inline-formula id="IE82"><mml:math id="IM82" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>ζ</mml:mo><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. If <inline-formula id="IE83"><mml:math id="IM83" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>≤</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula> models are empty. Bottom: the distribution of the PPV for a 0.25 threshold and <inline-formula id="IE84"><mml:math id="IM84" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mtext>mean</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>90</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for SPLS-based SelectBoost, Type1 data and raw SelectBoost (left) or SelectBoost with a non-increasing post-processing step (right)</p>
        </caption>
        <graphic xlink:href="btaa855f1" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.3 Choosing the parameters of the algorithm</title>
      <p>We first have to choose the grouping function. One of the simplest ways to define a grouping function <inline-formula id="IE85"><mml:math id="IM85" display="inline" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the following:
<disp-formula id="E11"><label>(7)</label><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>q</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo>}</mml:mo><mml:mo> </mml:mo><mml:mo>|</mml:mo><mml:mo> </mml:mo><mml:mo>|</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mo>|</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>}</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>In other words, the correlation group of the variable <italic toggle="yes">p</italic> is determined by variables whose correlation with <inline-formula id="IE86"><mml:math id="IM86" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is at least <italic toggle="yes">c</italic><sub>0</sub>. In another way, the structure of correlation may further be taken into account using graph community clustering. Let <inline-formula id="IE87"><mml:math id="IM87" display="inline" overflow="scroll"><mml:mi mathvariant="bold-italic">C</mml:mi></mml:math></inline-formula> be the correlation matrix of matrix <inline-formula id="IE88"><mml:math id="IM88" display="inline" overflow="scroll"><mml:mi mathvariant="bold">X</mml:mi></mml:math></inline-formula>. Let define <inline-formula id="IE89"><mml:math id="IM89" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mo>ˇ</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> as follows:
<disp-formula id="E12"><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>ˇ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>ˇ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtext>if</mml:mtext></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>ˇ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mtext>and</mml:mtext><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>otherwise</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>Then, we apply a community clustering algorithm on the undirected network with weighted adjacency matrix defined by <inline-formula id="IE90"><mml:math id="IM90" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mo>ˇ</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. Using a graph community clustering algorithm is helpful with large datasets while still clustering similar variables together. For instance, the fast greedy modularity optimization algorithm for finding community structure (<xref rid="btaa855-B13" ref-type="bibr">Clauset <italic toggle="yes">et al</italic>., 2004</xref>) runs in essentially linear time for many real-world networks given that they are sparse and hierarchical.</p>
      <p>Once the grouping function is chosen, we have to choose parameter <italic toggle="yes">c</italic><sub>0</sub>. Due to the constraints in <xref rid="E9" ref-type="disp-formula">Equation (6)</xref>, the SelectBoost algorithm results in the initial variable selection method when <inline-formula id="IE91"><mml:math id="IM91" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. As we will show in the next section, the smaller the parameter <italic toggle="yes">c</italic><sub>0</sub>, the higher the precision of the resulting selected variables. On the other hand, it is obvious that the probability of choosing none of the variables (i.e. resulting in the choice of an empty set) increases as the parameter <italic toggle="yes">c</italic><sub>0</sub> decreases. In the perspective of experimental planning, the choice of <italic toggle="yes">c</italic><sub>0</sub> should result of a compromise between precision and proportion of active identified variables. Hence, the <italic toggle="yes">c</italic><sub>0</sub> parameter can be used to introduce a confidence index <italic toggle="yes">γ<sub>p</sub></italic> related to the variable <inline-formula id="IE92"><mml:math id="IM92" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>:
<disp-formula id="E13"><label>(8)</label><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mo>γ</mml:mo><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:mi>min</mml:mi><mml:mo>⁡</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="script">S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mtext>hence</mml:mtext><mml:mo> </mml:mo><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mo>γ</mml:mo><mml:mi>p</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mn>1.</mml:mn></mml:mrow></mml:math></disp-formula></p>
      <p>
        <boxed-text id="btaa855-BOX1" position="float">
          <sec>
            <title><bold>Algorithm 1</bold> Pseudo-code for the SelectBoost algorithm</title>
            <p><bold>Require:</bold>
 <inline-formula id="IE93"><mml:math id="IM93" display="inline" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">select</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>
 <inline-formula id="IE94"><mml:math id="IM94" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mo>←</mml:mo><mml:msub><mml:mn>0</mml:mn><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p>
            <p> <bold>for</bold><inline-formula id="IE95"><mml:math id="IM95" display="inline" overflow="scroll"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula><bold>do</bold></p>
            <p>  <inline-formula id="IE96"><mml:math id="IM96" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>←</mml:mo><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula></p>
            <p>  <bold>for</bold><inline-formula id="IE97"><mml:math id="IM97" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula><bold>do</bold></p>
            <p>   <inline-formula id="IE98"><mml:math id="IM98" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo>.</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>←</mml:mo><mml:msup><mml:mo>ϕ</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>random</mml:mtext><mml:mo>-</mml:mo><mml:mtext>vMF</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>κ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p>
            <p>  <bold>end for</bold></p>
            <p>  <inline-formula id="IE99"><mml:math id="IM99" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mo>←</mml:mo><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">select</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p>
            <p> <bold>end for</bold></p>
            <p> <inline-formula id="IE100"><mml:math id="IM100" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mo>←</mml:mo><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula></p>
          </sec>
        </boxed-text>
      </p>
    </sec>
  </sec>
  <sec>
    <title>3 Numerical studies</title>
    <p>We benchmarked the algorithm with a large simulation study with four data generation processes and three real datasets (Table <xref rid="btaa855-T1" ref-type="table">1</xref>). Generated datasets are available upon request and real datasets are available either upon request or online.
</p>
    <list list-type="order">
      <list-item>
        <p>Simulation with 1000 variables and one linear response. A cluster of 50 variables is linked to the response.</p>
      </list-item>
      <list-item>
        <p>Simulation with 1000 variables and one binary response. A cluster of 50 variables is linked to the response.</p>
      </list-item>
      <list-item>
        <p>Data are 200 uncorrelated (‘unlinked’) single nucleotide polymorphisms (SNPs) with simulated genotypes, in which the first 20 of them affect the outcome with three covariates; 400 observations;</p>
      </list-item>
      <list-item>
        <p>Data are 100 uncorrelated (‘unlinked’) SNPs with simulated genotypes, in which the first 10 of them affect the outcome with two covariates; 750 observations.</p>
      </list-item>
      <list-item>
        <p>The leukemia dataset (<xref rid="btaa855-B24" ref-type="bibr">Golub <italic toggle="yes">et al</italic>., 1999</xref>) is the preprocessed data of <xref rid="btaa855-B15" ref-type="bibr">Dettling (2004)</xref> retrieved from the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> accompanying <xref rid="btaa855-B23" ref-type="bibr">Friedman <italic toggle="yes">et al</italic>. (2010b</xref>).</p>
      </list-item>
      <list-item>
        <p>The Huntington dataset is a real dataset with 28 087 variables observed on 69 individuals. We first applied independent filtering and removed 10 370 variables. We applied the SelectBoost algorithm to 17 717 variables observed on 69 individuals.</p>
      </list-item>
      <list-item>
        <p>The melanoma dataset is the GSE78220 dataset from <xref rid="btaa855-B28" ref-type="bibr">Hugo <italic toggle="yes">et al</italic>. (2016)</xref>.</p>
      </list-item>
    </list>
    <table-wrap position="float" id="btaa855-T1">
      <label>Table 1.</label>
      <caption>
        <p>Summary of the types of datasets used to benchmark the SelectBoost algorithm</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="center" span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th rowspan="1" colspan="1">Name</th>
            <th rowspan="1" colspan="1">Data</th>
            <th rowspan="1" colspan="1">Individuals</th>
            <th rowspan="1" colspan="1">Variables</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">Type1</td>
            <td rowspan="1" colspan="1">Simulated</td>
            <td rowspan="1" colspan="1">100</td>
            <td rowspan="1" colspan="1">1000</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Type2</td>
            <td rowspan="1" colspan="1">Simulated</td>
            <td rowspan="1" colspan="1">100</td>
            <td rowspan="1" colspan="1">1000</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Type3</td>
            <td rowspan="1" colspan="1">Simulated</td>
            <td rowspan="1" colspan="1">400</td>
            <td rowspan="1" colspan="1">203</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Type4</td>
            <td rowspan="1" colspan="1">Simulated</td>
            <td rowspan="1" colspan="1">750</td>
            <td rowspan="1" colspan="1">102</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Leukemia</td>
            <td rowspan="1" colspan="1">Observed</td>
            <td rowspan="1" colspan="1">72</td>
            <td rowspan="1" colspan="1">3571</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Huntington</td>
            <td rowspan="1" colspan="1">Observed</td>
            <td rowspan="1" colspan="1">69</td>
            <td rowspan="1" colspan="1">17 717</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Melanoma</td>
            <td rowspan="1" colspan="1">Observed</td>
            <td rowspan="1" colspan="1">28</td>
            <td rowspan="1" colspan="1">25 268</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>For Types 1 and 2, the number of variables is 1000, and the number of observations is 100. The data are generated from a cluster simulation (<xref rid="btaa855-B4" ref-type="bibr">Bair <italic toggle="yes">et al</italic>., 2006</xref>; <xref rid="btaa855-B6" ref-type="bibr">Bastien <italic toggle="yes">et al</italic>., 2015</xref>). Only 50 first predictors are linked to the response <italic toggle="yes">Y</italic> and the last 950 variables are randomly generated from a standard normal distribution. For Example 3, the response variable is linear but was turned into a binary variable (+1 when <inline-formula id="IE101"><mml:math id="IM101" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and –1 when <inline-formula id="IE102"><mml:math id="IM102" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>).</p>
    <p>Examples 1 and 3 are linear regression examples whereas 2, 4, 5, 6 and 7 are logistic regression ones, for which, we will assume a logistic model with a binary response variable (<xref rid="btaa855-B37" ref-type="bibr">Peng <italic toggle="yes">et al</italic>., 2002</xref>).</p>
    <p>We provide results for 12 different settings based on 10 different models, see <xref rid="sup1" ref-type="supplementary-material">Supplementary Section S1</xref> for more details.
</p>
    <list list-type="order">
      <list-item>
        <p>Linear regression (seven types): SPLS [<xref rid="btaa855-B12" ref-type="bibr">Chun and Keles (2010)</xref>, with raw and bootstrap corrected coefficients], LASSO, adaptive LASSO, enet and adaptive enet with model choice based on information criteria (AICc, BIC, GCV, Cp), LASSO with model choice based on 5-fold cross-validation (<inline-formula id="IE103"><mml:math id="IM103" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>λ</mml:mo><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mo>λ</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mtext>se</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) and varbvs linear (<xref rid="btaa855-B11" ref-type="bibr">Carbonetto and Stephens, 2012</xref>; <xref rid="btaa855-B25" ref-type="bibr">Guan and Stephens, 2011</xref>; <xref rid="btaa855-B50" ref-type="bibr">Zhou <italic toggle="yes">et al</italic>., 2013</xref>).</p>
      </list-item>
      <list-item>
        <p>Logistic regression (five types): logistic LASSO (glmnet based) with model choice based on 5-fold cross-validation, logistic LASSO (glmnet based) with model choice based on information criteria (AICc, BIC), varbvs binomial, SPLSda (<xref rid="btaa855-B12" ref-type="bibr">Chun and Keles, 2010</xref>) and sgpls (<xref rid="btaa855-B12" ref-type="bibr">Chun and Keles, 2010</xref>).</p>
      </list-item>
    </list>
    <p>The SelectBoost algorithm is based on correlated resampling and hence random. We wanted to assess both the stability and performance of the algorithm. As a consequence, for the four types of simulated data, we focused both on what may be called a repeatability study (a given dataset was analyzed 100 times to estimate the variation only due to the fact that the algorithm is random) and a reproducibility study (100 different datasets were generated and analyzed to estimate the variability due to both data simulation—from the same data generation—and the fact that the algorithm is random).</p>
    <p>The repeatability issue raised was raised, for instance by <xref rid="btaa855-B7" ref-type="bibr">Boulesteix (2014)</xref> and <xref rid="btaa855-B33" ref-type="bibr">Magnanensi <italic toggle="yes">et al</italic>. (2017)</xref> for PLS models. For those models, random split cross-validation is known to have poor repeatability. We used two types of grouping functions (either determined by variables whose correlation with <italic toggle="yes">x<sub>p</sub></italic> is at least <italic toggle="yes">c</italic><sub>0</sub> -gdirect- or community clustering-based -gcc-).</p>
    <p>The cost (memory and time) of the random generation step can be limited thanks to a sparse correlated resampling feature. The remaining cost of the algorithm is <inline-formula id="IE104"><mml:math id="IM104" display="inline" overflow="scroll"><mml:mrow><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mi>c</mml:mi><mml:mn>0</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Tim</mml:mi><mml:msub><mml:mi mathvariant="normal">e</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> with <italic toggle="yes">B</italic> the number of resampling and <italic toggle="yes">Nc</italic>0 the number of c0 values that are investigated and Time<sub>1</sub> the time to fit the model once.</p>
    <p>To demonstrate the performance of the SelectBoost method, we compared our method with stability selection (<xref rid="btaa855-B34" ref-type="bibr">Meinshausen and Bühlmann, 2010</xref>) and with a naive version of our algorithm, naiveSelectBoost. The naiveSelectBoost algorithm works as follows: estimate <italic toggle="yes">β</italic> with any variable selection method then if <inline-formula id="IE105"><mml:math id="IM105" display="inline" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, as defined in <xref rid="E11" ref-type="disp-formula">Equation (7)</xref> e.g. is not reduced to <italic toggle="yes">p</italic>, shrink to 0. The naiveSelectBoost algorithm is similar to the SelectBoost algorithm, except that it does not take into account the error, which is made choosing at random a variable among a set of correlated variables.</p>
    <p>We use four indicators to evaluate the abilities of our method on simulated data. We define:
</p>
    <list list-type="bullet">
      <list-item>
        <p>recall as the ratio of the number of correctly identified variables (i.e. <inline-formula id="IE106"><mml:math id="IM106" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE107"><mml:math id="IM107" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>β</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) over the number of variables that should have been discovered (i.e. <inline-formula id="IE108"><mml:math id="IM108" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>β</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>).</p>
      </list-item>
      <list-item>
        <p>precision as the ratio of correctly identified variables (i.e. <inline-formula id="IE109"><mml:math id="IM109" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE110"><mml:math id="IM110" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>β</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) over the number of identified variables (i.e. <inline-formula id="IE111"><mml:math id="IM111" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>).</p>
      </list-item>
      <list-item>
        <p><italic toggle="yes">F</italic>-score as the following ratio:
<disp-formula id="E14"><mml:math id="M14" display="block" overflow="scroll"><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mtext>recall</mml:mtext><mml:mo>×</mml:mo><mml:mtext>precision</mml:mtext></mml:mrow><mml:mrow><mml:mtext>recall</mml:mtext><mml:mo>+</mml:mo><mml:mtext>precision</mml:mtext></mml:mrow></mml:mfrac><mml:mo>·</mml:mo></mml:mrow></mml:math></disp-formula></p>
      </list-item>
      <list-item>
        <p>selection as the average number of identified variables (i.e. <inline-formula id="IE112"><mml:math id="IM112" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>).</p>
      </list-item>
    </list>
    <p>Note that our interest is focused on precision, as our goal is to select reliable variables. As stated before, when <italic toggle="yes">c</italic><sub>0</sub> is decreasing toward zero, we expect a profit in precision and a decrease in recall. We also compute the <italic toggle="yes">F</italic>-score, which combines both recall and precision. As an improvement of precision comes with a decrease in the number of identified variables, the best method is the one with the highest precision for a given level of selection.</p>
    <sec>
      <title>3.1 Results of the numerical studies</title>
      <p>We show the evolution of the four criteria (recall, precision, <italic toggle="yes">F</italic>-score and selection) with regards to the decrease of <italic toggle="yes">c</italic><sub>0</sub>. When <inline-formula id="IE113"><mml:math id="IM113" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, the SelectBoost algorithm is equivalent to the initial variable selection method. We introduce a post-processing step to enforce that, for a given variable, the proportion of selection is non-increasing. It is the expected behavior since the correlated resampling is not meant to increase the probability of selection for a variable. Such an increase may happen for small <italic toggle="yes">c</italic><sub>0</sub> values when a variable that is not linked with the response is mixed with a variable that is linked to the response. For all the simulations, this post-processing step increases the PPV of the SelectBoost algorithm, see <xref rid="btaa855-F1" ref-type="fig">Figure 1</xref>. As our primary focus is PPV, we recommend the use of this post-processing step. More details can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Graphs S7</xref>–<xref rid="sup1" ref-type="supplementary-material">S174</xref>.</p>
      <p>We created precision-recall plots to display the effects of the algorithm on the performance of all the models and criteria used for a given dataset. Identical model fitting criteria share the same colors. The arrows point toward decreasing <italic toggle="yes">c</italic><sub>0</sub> values. Direct grouping and community grouping lead to similar results, <xref rid="btaa855-F2" ref-type="fig">Figure 2</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S1</xref>, <xref rid="sup1" ref-type="supplementary-material">S3</xref> and <xref rid="sup1" ref-type="supplementary-material">S4</xref>. These Figures also show that the results for a single dataset repeated 100 times are similar to results for 100 different datasets. The Zoom <italic toggle="yes">l</italic> sequence, which is a 10-step regular grid from the <inline-formula id="IE114"><mml:math id="IM114" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> quantile—the maximum value—to <inline-formula id="IE115"><mml:math id="IM115" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>90</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>—the quantile of order 0.9—achieves high PPV, <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S299</xref>.
</p>
      <fig position="float" id="btaa855-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>Recall-precision curve. All models and criteria non-increasing SelectBoost. Type 1 data. Direct grouping. A total of 100 different datasets. <inline-formula id="IE116"><mml:math id="IM116" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>ζ</mml:mo><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula></p>
        </caption>
        <graphic xlink:href="btaa855f2" position="float"/>
      </fig>
      <p><xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S5</xref> displays an example of raw SelectBoost (without the non-increasing post-processing step) for direct grouping and 100 different datasets that should be compared to <xref rid="btaa855-F2" ref-type="fig">Figure 2</xref>. This effect is even stronger smaller values for the <inline-formula id="IE117"><mml:math id="IM117" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>ζ</mml:mo><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> threshold. The non-increasing post-processing step greatly improves the results of the algorithm and leads to monotonic relationships between the recall, the precision and the <italic toggle="yes">c</italic><sub>0</sub> value. PPV benefit less from smaller values for the <inline-formula id="IE118"><mml:math id="IM118" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>ζ</mml:mo><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> threshold, <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S6</xref>.</p>
      <p>All the results of the simulation study showed the good performance and stability of the algorithm, which we then applied once to each of the real datasets. The results of the gdirect and gcc based SelectBoost are similar, the gcc based being a bit more time consuming than the gdirect one. In the following of this article, we reported results and figures for gdirect-based SelectBoost.</p>
      <p>According to our simulation studies, <xref rid="sup1" ref-type="supplementary-material">Supplementary Graphs S7</xref>–<xref rid="sup1" ref-type="supplementary-material">S174</xref>, one should choose <italic toggle="yes">c</italic><sub>0</sub> between <inline-formula id="IE119"><mml:math id="IM119" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>90</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE120"><mml:math id="IM120" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, see <xref rid="btaa855-F1" ref-type="fig">Figure 1</xref> top. In our simulation studies, we used an 11-steps <italic toggle="yes">c</italic><sub>0</sub> sequence, but, according to our results, it could be limited to 6 steps [from mean(<inline-formula id="IE121"><mml:math id="IM121" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>90</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) to <inline-formula id="IE122"><mml:math id="IM122" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>] for the biggest datasets. The value of <italic toggle="yes">B</italic> should not be lower than 10. <italic toggle="yes">B</italic> = 50 or <italic toggle="yes">B</italic> = 100 will provide more stable results. As a consequence, the minimal time cost of the SelectBoost algorithm will be 60 times the time cost of the regular model fit, which could be afforded in almost every case. The parallel processing support of the SelectBoost package can help to reduce this time. Hence, the SelectBoost seems feasible with most of the datasets and even omics datasets as we did in our simulation study with the three real datasets.</p>
      <p>Hence, to assess the performance of the SelectBoost algorithm, we performed comprehensive numerical studies. As stated before, the SelectBoost algorithm can be applied to any existing variable selection method.</p>
      <p><xref rid="btaa855-F1" ref-type="fig">Figure 1</xref> top shows the result for the lasso selection with a penalty parameter chosen using information criteria for Type 1 datasets. In this example, we improve the precision up to 1. Moreover, as shown by <xref rid="btaa855-F1" ref-type="fig">Figures 1</xref> and <xref rid="btaa855-F3" ref-type="fig">3</xref>, the proportion of models, for which the precision reaches one, increases with the decrease of <italic toggle="yes">c</italic><sub>0</sub>. The <italic toggle="yes">F</italic>-score increases, remains either stable or shows a small decrease indicating that the increase of PPV compensates the loss in recall.
</p>
      <fig position="float" id="btaa855-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>Top: The average number of identified variables is plotted as a function of the proportion of correctly identified variables for Type1 simulated data and all models. Middle and bottom: effect of the SelectBoost algorithm wrt <inline-formula id="IE123"><mml:math id="IM123" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> for adaptive elastic net and AICc model selection criterion with <italic toggle="yes">c</italic><sub>0</sub> in the range <inline-formula id="IE124"><mml:math id="IM124" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>90</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> for 100 different (middle, reproducibility) or 100 identical (bottom, repeatability) Type3 simulated data with a non-increasing post-processing step and a threshold <inline-formula id="IE125"><mml:math id="IM125" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>ζ</mml:mo><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Only results for non-empty models are shown</p>
        </caption>
        <graphic xlink:href="btaa855f3" position="float"/>
      </fig>
      <p>In the previous section, we mentioned the possibility of using SelectBoost to obtain a confidence index, corresponding to one minus the lowest <italic toggle="yes">c</italic><sub>0</sub> for which a variable is selected. For each <italic toggle="yes">c</italic><sub>0</sub>, we plotted the average number of selected variables as a function of the proportion of correctly identified variables (<xref rid="btaa855-F3" ref-type="fig">Fig. 3</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S300</xref>–<xref rid="sup1" ref-type="supplementary-material">S304</xref>). As expected, the proportion of correctly identified variables increases with the increase of the confidence index and with the decrease of the average number of identified variables. Therefore, the proportion of non-predictive features decreases with the increase of the confidence index.</p>
      <p>The SelectBoost algorithm shows its superiority over the naive SelectBoost algorithm. The error made when choosing a variable randomly among a set of correlated variables leads to more incorrect choices of variables. While the intensive simulation of our algorithm allows taking into account this error, the naiveSelectBoost does not.</p>
      <p>Finally, we compare the SelectBoost algorithm with stability selection. Stability selection uses a resampling algorithm to determine which of the variables included in the model are robust. In our simulation, stability selection shows performance with high precision but also low recall. Moreover, in contrast to the SelectBoost algorithm, stability selection does not allow to choose a convenient precision-PPV trade-off.</p>
      <p>The timings of the algorithm can be found on <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S175–S222</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Application to three real datasets</title>
    <p>We applied our algorithm to three real datasets. We studied, with respect to the threshold, the number of non-zero variables, the number of variables selected by SelectBoost and their ratio. We found results that were concordant with those of the simulated datasets. <xref rid="btaa855-F4" ref-type="fig">Figure 4</xref> displays those results for a SGPLS-based SelectBoost of the Leukemia dataset with a 0.25 threshold (<inline-formula id="IE126"><mml:math id="IM126" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>ζ</mml:mo><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula>). See <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S247–S298</xref>.
</p>
    <fig position="float" id="btaa855-F4">
      <label>Fig. 4.</label>
      <caption>
        <p>% of non-zero coefficients wrt to <italic toggle="yes">c</italic><sub>0</sub> for SGPLS-based SelectBoost models of the leukemia datasets and threshold <inline-formula id="IE127"><mml:math id="IM127" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>ζ</mml:mo><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula></p>
      </caption>
      <graphic xlink:href="btaa855f4" position="float"/>
    </fig>
    <p>We report the results for the RNA-Seq dataset providing mRNA expressions from Huntington’s disease and neurologically normal individuals. This dataset was downloaded from the GEO database under accession number GSE64810 (<ext-link xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE64810" ext-link-type="uri">https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi? acc=GSE64810</ext-link>). This dataset contains 20 Huntington’s disease cases and 49 neurologically normal controls and includes 28 087 genes as explanatory variables. An independent filtering (<xref rid="btaa855-B8" ref-type="bibr">Bourgon <italic toggle="yes">et al</italic>., 2010</xref>) preprocessing step was first performed using data-based filtering for replicated high-throughput transcriptome sequencing experiments (<xref rid="btaa855-B38" ref-type="bibr">Rau <italic toggle="yes">et al</italic>., 2013</xref>). Then, we applied the lasso selection method to this reduced dataset (see <xref rid="btaa855-F5" ref-type="fig">Fig. 5</xref> left for the whole path of the solution). We used cross-validation to choose the appropriate level of penalization [i.e. the <italic toggle="yes">λ</italic> parameter in <xref rid="E3" ref-type="disp-formula">Equation (3)]</xref>.
</p>
    <fig position="float" id="btaa855-F5">
      <label>Fig. 5.</label>
      <caption>
        <p>Colors: the green is for the most reliable variables selected by the SelectBoost algorithm [confidence index of 0.3; orange is for intermediate confidence (0.25) and red for low confidence (0.15)]. Left: evolution of the coefficients in the lasso regression when the regularization parameter <italic toggle="yes">λ</italic> is varying. For the <italic toggle="yes">λ</italic> range shown, the red, orange and green lines stick to zero. Right: evolution of the probability of being in the support of the regression when the confidence index is varying. The dotted line represents the 0.95 threshold</p>
      </caption>
      <graphic xlink:href="btaa855f5" position="float"/>
    </fig>
    <p>We then applied our SelectBoost algorithm on the lasso method with penalty parameter chosen by cross-validation. We use a range for the <italic toggle="yes">c</italic><sub>0</sub> parameter starting from 1 to 0.7 with steps of 0.05, which corresponds to a confidence index from 0 to 0.3. For each step, the probability of being included in the support <inline-formula id="IE128"><mml:math id="IM128" display="inline" overflow="scroll"><mml:mi mathvariant="script">S</mml:mi></mml:math></inline-formula> was calculated with 200 simulations as described in Algorithm 1. We set the threshold of being in the support to 0.95 to avoid numerical instability. We classify the selected variables into three categories: those that are identified for each confidence index from 0 to 0.15 (red), those identified from 0 to 0.25 (orange) and those identified from 0 to 0.3 (green). The last category contains the most reliable variables selected by the SelectBoost algorithm because these variables are identified from low to high confidence index.</p>
    <p>With the lasso selection method, 15 variables were selected. Among them, four genes were identified by SelectBoost into the three different categories of confidence index (see <xref rid="btaa855-F5" ref-type="fig">Fig. 5</xref> right): two genes for low confidence (red) (ANXA3 and INTS12), one gene for intermediate confidence (orange) (NUB1) and one gene for high confidence (green) (PUS3).</p>
    <p>The interesting point, in these three examples, is that the identified variables are neither the first variables selected by the lasso nor the variables with the highest coefficients (see <xref rid="btaa855-F5" ref-type="fig">Fig. 5</xref> left). This result demonstrates that our algorithm can be advantageous to select variables with high confidence and not just to select variables with the highest coefficients.</p>
    <p>Finally, we decided to assess the differential expression of these genes between patients and controls, using the <monospace>limma</monospace> package (Linear Models for Microarray and RNA-Seq Data) (<xref rid="btaa855-B39" ref-type="bibr">Ritchie <italic toggle="yes">et al</italic>., 2015</xref>). The four identified genes are significantly down-expressed by neurologically healthy controls confirming the result of a logistic model with these four genes.</p>
  </sec>
  <sec>
    <title>5 Robust reverse-engineering of networks</title>
    <p>Sparsity is a well-known feature of most biological networks (<xref rid="btaa855-B5" ref-type="bibr">Barabási <italic toggle="yes">et al</italic>., 2003</xref>). An actor can only be regulated by a small number of other actors, whereas it may regulate any number of other actors. Hence, variable selection methods, such as the lasso, ensure that sparsity feature and are often core components of most of the biological network reverse-engineering tools. As a consequence, we propose to apply the SelectBoost algorithm in two different ways in order to improve the biological network reverse-engineering: as a post-processing step after the inference was made or during the inference itself in order to select the most stable predictors for each node in the network. When used as a post-processing step, one can assess for any of the inferred links between the actors of the network, its confidence index against correlated resampling of the predictors. When used during the inference step, one can infer a model that is only built with links with a high enough confidence index. The former is implemented in the SelectBoost package as a new method for the Cascade package<xref rid="btaa855-B29" ref-type="bibr"> (Jung <italic toggle="yes">et al</italic>, 2014) </xref>. The latter is implemented in the new Patterns CRAN package as a dedicated fitting function and is especially useful when trying to find targets for biological intervention that are strongly related to markers of some diseases through the reverse-engineered network and useful and reliable links.</p>
    <p>We benchmarked those two uses of the algorithm with a particular type of biological networks that we have been using for several years: cascade networks (<xref rid="btaa855-B44" ref-type="bibr">Vallat <italic toggle="yes">et al</italic>., 2013</xref>).</p>
    <p>For the post-inference processing, we first fit a model to a cascade network using the Cascade package inference function. Then, we compute confidence indices for the inferred links using the SelectBoost algorithm, more details, as well as the code, of the simulations can be found in the vignette of the package ‘Towards Confidence Estimates in Cascade Networks using the SelectBoost Package’, available at <ext-link xlink:href="https://fbertran.github.io/SelectBoost/articles/confidence-indices-Cascade-networks.html" ext-link-type="uri">https://fbertran.github.io/SelectBoost/articles/confidence-indices-Cascade-networks.html</ext-link>. An example of those results is shown in <xref rid="btaa855-F6" ref-type="fig">Figure 6</xref> with a cascade network for four time points and four groups of 25 actors.
</p>
    <fig position="float" id="btaa855-F6">
      <label>Fig. 6.</label>
      <caption>
        <p>Post-inference analysis of an inferred cascade network. Dark values are tantamount to low confidence. Bright values are tantamount to high confidence. Confidence ranges from 0 (lowest) to 1 (highest). The lower triangular part of the matrix is an area with the highest confidence (1) since we know—and assume so in the model—that for cascade networks those links must be =0</p>
      </caption>
      <graphic xlink:href="btaa855f6" position="float"/>
    </fig>
    <p>For the use of the SelectBoost algorithm during the fitting step of a cascade network reverse-engineering, we used the Patterns package. Benchmark results were reported as sensitivity, positive predictive value and <italic toggle="yes">F</italic>-score, shown in <xref rid="btaa855-F7" ref-type="fig">Figure 7</xref>; the code, the simulation details and the remaining results are part of a vignette of the package ‘Benchmarking the SelectBoost Package for Network Reverse Engineering’, that is available at <ext-link xlink:href="https://fbertran.github.io/SelectBoost/articles/benchmarking-SelectBoost-networks.html" ext-link-type="uri">https://fbertran.github.io/SelectBoost/articles/benchmarking-SelectBoost-networks.html</ext-link>.
</p>
    <fig position="float" id="btaa855-F7">
      <label>Fig. 7.</label>
      <caption>
        <p><italic toggle="yes">F</italic>-score as a function of the thresholding value: if an inferred coefficient for the network is less than the thresholding value, then it is set to 0. The SelectBoost algorithm is compared to both stability selection and the regular lasso. The upper row displays results for the unweighted version of the algorithms, whereas the lower row displays results for their weighted counterparts</p>
      </caption>
      <graphic xlink:href="btaa855f7" position="float"/>
    </fig>
    <p>We created an unweighted or a weighted version of the algorithm. The weighted version of the algorithm enables the user to include weights in the model, which means to favor or disfavor some links between the actors, in order, for instance, to take into account biological knowledge.</p>
    <p>The results shown in <xref rid="btaa855-F7" ref-type="fig">Figure 7</xref> of the simulation study are a comparison to a standard set up for stability selection and regular lasso both for an unweighted version of the algorithms and a highly correctly weighted version of the same algorithms.</p>
    <p>By highly correctly weighted, we mean that we included influential weights in the model accordingly to the links that existed in the network that was used for data simulation. This network was randomized from one simulation to another. This weighted setting was used to determine if including correct biological knowledge would help the reverse-engineering algorithm to retrieve the correct network. If correct biological knowledge is included in the model, all three fitting functions lead to similar and outstanding results for the <italic toggle="yes">F</italic>-score criterion without even requiring the need to search for an optimal thresholding value as we had to do with the Cascade package.</p>
    <p>For each simulated dataset, vertical dots are displayed to show the optimal threshold level that should be used to maximize the <italic toggle="yes">F</italic>-score. It is computed with respect to the actual values that are unknown for real datasets. Without weights, SelectBoost shrinks the range of optimal values when compared to the lasso or stability selection. With correct weights, none of the methods still requires to use a cut-off value to maximize <italic toggle="yes">F</italic>-score.</p>
    <p>In an unweighted setting, the SelectBoost version of the fitting process shows better performance than stability selection and the lasso as long as the cut-off value is &lt;0.4, which is about the double of the optimal thresholding value.</p>
  </sec>
  <sec>
    <title>6 Conclusion</title>
    <p>We introduce the SelectBoost algorithm that relies intensive computations to select variables with high precision (PPV). The user of SelectBoost can apply this algorithm to produce a confidence index or choose an appropriate precision-selection trade-off to select variables with high confidence and avoid selecting non-predictive features. The main idea behind our algorithm is to take into account the correlation structure of the data and thus use intensive computation to select reliable variables.</p>
    <p>The choice of the threshold <inline-formula id="IE129"><mml:math id="IM129" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>ζ</mml:mo><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is critical since such a choice leads to two effects.
</p>
    <list list-type="bullet">
      <list-item>
        <p>With a high threshold value—nearing the maximum value of 1—: an increase of the PPV while limiting the decrease of the <italic toggle="yes">F</italic>-score.</p>
      </list-item>
      <list-item>
        <p>With a low or medium threshold value—nearing the mid value of 0.5—: an increase in recall while limiting the decrease of the <italic toggle="yes">F</italic>-score.</p>
      </list-item>
    </list>
    <p>We will want the first property to retrieve the stable core of the predictors for models that are known to randomly choose between correlated variables, such as the lasso or adaptive lasso. . Whereas, we will want the second property for models that scarcely select variable, such as variable selection model using variational approximation methods for binary response (<monospace>varbvs</monospace>). A non-constant threshold should be also and investigated by those that would like to introduce corrections, for instance FDR-like, such as Holm–Bonferroni, in the variable selection process.</p>
    <p>We prove the performance of our algorithm through simulation studies in various settings. To get the best results, we recommend the use of <italic toggle="yes">c</italic><sub>0</sub> in the range of mean(<inline-formula id="IE130"><mml:math id="IM130" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>90</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) to <inline-formula id="IE131"><mml:math id="IM131" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> with the non-increasing post-processing step. It could be useful to decrease the lower bound to <inline-formula id="IE132"><mml:math id="IM132" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mn>90</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for the smallest datasets. The user should never use a <italic toggle="yes">c</italic><sub>0</sub> value too close to the empty model zone to avoid a decrease in precision. We succeed in improving the PPV, whenever it was possible, of all the 12 selection methods with relative stability on recall and <italic toggle="yes">F</italic>-score. If the PPV was already nearing 1, then there is almost no negative effect on the PPV and recall when applying SelectBoost.</p>
    <p>Our results open the perspective of a precision-selection trade-off which may be very useful in some situations where many regressions have to be made (e.g. network reverse-engineering with one regression made per node of the network). In such a context, our algorithm may even be used in an experimental design approach.</p>
    <p>The application to three real datasets allowed us to show that the most reliable variables are not necessarily those with the highest coefficients. The SelectBoost algorithm is a powerful tool that can be used in every situation where reliable and robust variable selection has to be made.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by grants from the Agence Nationale de la Recherche (ANR) [ANR-11-LABX-0070_TRANSPLANTEX]; the INSERM [UMR_S 1109]; the Institut Universitaire de France (IUF) and the MSD-Avenir grant AUTOGEN, all to S.B.; the European regional development fund (European Union) INTERREG V program (project number 3.2 TRIDIAG) to R.C. and S.B.; the Agence Nationale de la Recherche (ANR) [ANR-11-LABX-0055_IRMIA]; the CNRS [UMR 7501] to F.B. and M.M.-B.; and by the French HPC Center ROMEO [UR 201923174L] to F.B.</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btaa855_Supplementary_Data</label>
      <media xlink:href="btaa855_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa855-B1">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Abramowitz</surname>
 <given-names>M.</given-names></string-name>, <string-name><surname>Stegun</surname><given-names>I.A.</given-names></string-name></person-group> (<year>1972</year>) <source>Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables</source>. Vol. <volume>55</volume>, 
<publisher-name>National Bureau of Standards Applied Mathematics Series, Dover Publications Inc., New York</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btaa855-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Akaike</surname>
 <given-names>H.</given-names></string-name></person-group> (<year>1974</year>) 
<article-title>A new look at the statistical model identification</article-title>. <source>IEEE Trans. Automat. Contr</source>., <volume>19</volume>, <fpage>716</fpage>–<lpage>723</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bach</surname>
 <given-names>F.R.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2008</year>) Bolasso: model consistent lasso estimation through the bootstrap. In: <italic toggle="yes">Proceedings of the 25th International Conference on Machine Learning</italic>, Helsinki, Finland. pp. <fpage>33</fpage>–<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bair</surname>
 <given-names>E.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2006</year>) 
<article-title>Prediction by supervised principal components</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>101</volume>, <fpage>119</fpage>–<lpage>137</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Barabási</surname>
 <given-names>A.L.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2003</year>) <part-title>Emergence of scaling in complex networks</part-title>. In: <person-group person-group-type="editor"><string-name><surname>Bornholdt</surname><given-names>S.</given-names></string-name>, <string-name><surname>Schuster</surname><given-names>H.G.</given-names></string-name></person-group> (eds) <source>Handbook of Graphs and Networks: From the Genome to the Internet</source>. 
<publisher-name>Wiley-VCH</publisher-name>, 
<publisher-loc>Weinheim</publisher-loc>, pp. <fpage>69</fpage>–<lpage>84</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bastien</surname>
 <given-names>P.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2015</year>) 
<article-title>Deviance residuals-based sparse PLS and sparse kernel PLS regression for censored data</article-title>. <source>Bioinformatics</source>, <volume>31</volume>, <fpage>397</fpage>–<lpage>404</lpage>.<pub-id pub-id-type="pmid">25286920</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Boulesteix</surname>
 <given-names>A.-L.</given-names></string-name></person-group> (<year>2014</year>) Accuracy estimation for PLS and related methods via resampling-based procedures. In: <source>PLS–14 Book of Abstracts, Paris, France</source>. pp. <fpage>13</fpage>–<lpage>14</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bourgon</surname>
 <given-names>R.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2010</year>) 
<article-title>Independent filtering increases detection power for high-throughput experiments</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>107</volume>, <fpage>9546</fpage>–<lpage>9551</lpage>.<pub-id pub-id-type="pmid">20460310</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Chen</surname>
 <given-names>L.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2007</year>) Noise-based feature perturbation as a selection method for microarray data. In: <source>Bioinformatics Research and Applications, Atlanta, GA, USA</source>. pp. <fpage>237</fpage>–<lpage>247</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>
 <given-names>S.S.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2001</year>) 
<article-title>Atomic decomposition by basis pursuit</article-title>. <source>SIAM Rev</source>., <volume>43</volume>, <fpage>129</fpage>–<lpage>159</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carbonetto</surname>
 <given-names>P.</given-names></string-name>, <string-name><surname>Stephens</surname><given-names>M.</given-names></string-name></person-group> (<year>2012</year>) 
<article-title>Scalable variational inference for Bayesian variable selection in regression, and its accuracy in genetic association studies</article-title>. <source>Bayesian Anal</source>., <volume>7</volume>, <fpage>73</fpage>–<lpage>108</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chun</surname>
 <given-names>H.</given-names></string-name>, <string-name><surname>Keles</surname><given-names>S.</given-names></string-name></person-group> (<year>2010</year>) 
<article-title>Sparse partial least squares regression for simultaneous dimension reduction and variable selection</article-title>. <source>J. R. Stat. Soc. Series B Stat. Methodol</source>., <volume>72</volume>, <fpage>3</fpage>–<lpage>25</lpage>.<pub-id pub-id-type="pmid">20107611</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clauset</surname>
 <given-names>A.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2004</year>) 
<article-title>Finding community structure in very large networks</article-title>. <source>Phys. Rev. E</source>, <volume>70</volume>, <fpage>066111</fpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cook</surname>
 <given-names>J.R.</given-names></string-name>, <string-name><surname>Stefanski</surname><given-names>L.A.</given-names></string-name></person-group> (<year>1994</year>) 
<article-title>Simulation-extrapolation estimation in parametric measurement error models</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>89</volume>, <fpage>1314</fpage>–<lpage>1328</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dettling</surname>
 <given-names>M.</given-names></string-name></person-group> (<year>2004</year>) 
<article-title>BagBoosting for tumor classification with gene expression data</article-title>. <source>Bioinformatics</source>, <volume>20</volume>, <fpage>3583</fpage>–<lpage>3593</lpage>.<pub-id pub-id-type="pmid">15466910</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Donoho</surname>
 <given-names>D.L.</given-names></string-name>, <string-name><surname>Elad</surname><given-names>M.</given-names></string-name></person-group> (<year>2003</year>) 
<article-title>Optimally sparse representation in general (nonorthogonal) dictionaries via L1 minimization</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>100</volume>, <fpage>2197</fpage>–<lpage>2202</lpage>.<pub-id pub-id-type="pmid">16576749</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Efron</surname>
 <given-names>B.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2004</year>) 
<article-title>Least angle regression</article-title>. <source>Ann. Stat</source>., <volume>32</volume>, <fpage>407</fpage>–<lpage>499</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eklund</surname>
 <given-names>M.</given-names></string-name>, <string-name><surname>Zwanzig</surname><given-names>S.</given-names></string-name></person-group> (<year>2012</year>) 
<article-title>SimSel: a new simulation method for variable selection</article-title>. <source>J. Stat. Comput. Simul</source>., <volume>82</volume>, <fpage>515</fpage>–<lpage>527</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fan</surname>
 <given-names>J.</given-names></string-name></person-group> (<year>1997</year>) 
<article-title>Comments on “Wavelets in statistics: a review” by A. Antoniadis</article-title>. <source>Stat. Meth. Appl</source>., <volume>6</volume>, <fpage>131</fpage>–<lpage>138</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B20">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Fan</surname>
 <given-names>J.</given-names></string-name>, <string-name><surname>Li</surname><given-names>R.</given-names></string-name></person-group> (<year>2006</year>) Statistical challenges with high dimensionality: feature selection in knowledge discovery. In: <source>Proceedings International Congress of Mathematicitans</source>. <volume>Vol. 3</volume>, pp. <fpage>595</fpage>–<lpage>622</lpage>, 
<publisher-name>European Mathematical Society</publisher-name>, 
<publisher-loc>Zúrich</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btaa855-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fan</surname>
 <given-names>J.</given-names></string-name>, <string-name><surname>Lv</surname><given-names>J.</given-names></string-name></person-group> (<year>2010</year>) 
<article-title>A selective overview of variable selection in high dimensional feature space</article-title>. <source>Stat. Sin</source>., <volume>20</volume>, <fpage>101</fpage>–<lpage>148</lpage>.<pub-id pub-id-type="pmid">21572976</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedman</surname>
 <given-names>J.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2010</year>a) 
<article-title>A note on the group lasso and a sparse group lasso</article-title>. <source>arXiv preprint arXiv: 1001.0736</source>.</mixed-citation>
    </ref>
    <ref id="btaa855-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedman</surname>
 <given-names>J.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2010</year>b) 
<article-title>Regularization paths for generalized linear models via coordinate descent</article-title>. <source>J. Stat. Softw</source>., <volume>33</volume>, <fpage>1</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">20808728</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Golub</surname>
 <given-names>T.R.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>1999</year>) 
<article-title>Molecular classification -of cancer: class discovery and class prediction by gene expression monitoring</article-title>. <source>Science</source>, <volume>286</volume>, <fpage>531</fpage>–<lpage>537</lpage>.<pub-id pub-id-type="pmid">10521349</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guan</surname>
 <given-names>Y.</given-names></string-name>, <string-name><surname>Stephens</surname><given-names>M.</given-names></string-name></person-group> (<year>2011</year>) 
<article-title>Bayesian variable selection regression for genome-wide association studies and other large-scale problems</article-title>. <source>Ann. Appl. Stat</source>., <volume>5</volume>, <fpage>1780</fpage>–<lpage>1815</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hocking</surname>
 <given-names>R.R.</given-names></string-name></person-group> (<year>1976</year>) 
<article-title>A Biometrics invited paper. The analysis and selection of variables in linear regression</article-title>. <source>Biometrics</source>, <volume>32</volume>, <fpage>1</fpage>–<lpage>49</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoerl</surname>
 <given-names>A.E.</given-names></string-name>, <string-name><surname>Kennard</surname><given-names>R.W.</given-names></string-name></person-group> (<year>1970</year>) 
<article-title>Ridge regression: biased estimation for nonorthogonal problems</article-title>. <source>Technometrics</source>, <volume>12</volume>, <fpage>55</fpage>–<lpage>67</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hugo</surname>
 <given-names>W.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2016</year>) 
<article-title>Genomic and transcriptomic features of response to anti-PD-1 therapy in metastatic melanoma</article-title>. <source>Cell</source>, <volume>165</volume>, <fpage>35</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">26997480</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jung</surname>
 <given-names>N.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2014</year>) 
<article-title>Cascade: a R package to study, predict and simulate the diffusion of a signal through a temporal gene network</article-title>. <source>Bioinformatics</source>, <volume>30</volume>, <fpage>571</fpage>–<lpage>573</lpage>.<pub-id pub-id-type="pmid">24307703</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Koza</surname>
 <given-names>J.R.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>1999</year>) <source>Genetic Programming as a Darwinian Invention Machine</source>. 
<publisher-name>Springer, Heidelberg</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btaa855-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lipshutz</surname>
 <given-names>R.J.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>1999</year>) 
<article-title>High density synthetic oligonucleotide arrays</article-title>. <source>Nat. Genet</source>., <volume>21</volume>, <fpage>20</fpage>–<lpage>24</lpage>.<pub-id pub-id-type="pmid">9915496</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luo</surname>
 <given-names>X.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2006</year>) 
<article-title>Tuning variable selection procedures by adding noise</article-title>. <source>Technometrics</source>, <volume>48</volume>, <fpage>165</fpage>–<lpage>175</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Magnanensi</surname>
 <given-names>J.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2017</year>) 
<article-title>A new universal resample-stable bootstrap-based stopping criterion for PLS component construction</article-title>. <source>Stat. Comput</source>., <volume>27</volume>, <fpage>757</fpage>–<lpage>718</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meinshausen</surname>
 <given-names>N.</given-names></string-name>, <string-name><surname>Bühlmann</surname><given-names>P.</given-names></string-name></person-group> (<year>2010</year>) 
<article-title>Stability selection</article-title>. <source>J. R. Stat. Soc. Series B Stat. Methodol</source>., <volume>72</volume>, <fpage>417</fpage>–<lpage>473</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morgan</surname>
 <given-names>D.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2019</year>) 
<article-title>A generalized framework for controlling FDR in gene regulatory network inference</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>1026</fpage>–<lpage>1032</lpage>.<pub-id pub-id-type="pmid">30169550</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Natarajan</surname>
 <given-names>B.K.</given-names></string-name></person-group> (<year>1995</year>) 
<article-title>Sparse approximate solutions to linear systems</article-title>. <source>SIAM J. Comput</source>., <volume>24</volume>, <fpage>227</fpage>–<lpage>234</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname>
 <given-names>C.J.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2002</year>) 
<article-title>An introduction to logistic regression analysis and reporting</article-title>. <source>J. Educ. Res</source>., <volume>96</volume>, <fpage>3</fpage>–<lpage>14</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rau</surname>
 <given-names>A.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2013</year>) 
<article-title>Data-based filtering for replicated high-throughput transcriptome sequencing experiments</article-title>. <source>Bioinformatics</source>, <volume>29</volume>, <fpage>2146</fpage>–<lpage>2152</lpage>.<pub-id pub-id-type="pmid">23821648</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ritchie</surname>
 <given-names>M.E.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2015</year>) 
<article-title>limma powers differential expression analyses for RNA-sequencing and microarray studies</article-title>. <source>Nucleic Acids Res</source>., <volume>43</volume>, <fpage>e47</fpage>.<pub-id pub-id-type="pmid">25605792</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwarz</surname>
 <given-names>G.</given-names></string-name></person-group> (<year>1978</year>) 
<article-title>Estimating the dimension of a model</article-title>. <source>Ann. Stat</source>., <volume>6</volume>, <fpage>461</fpage>–<lpage>464</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Segal</surname>
 <given-names>M.R.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2003</year>) 
<article-title>Regression approaches for microarray data analysis</article-title>. <source>J. Comput. Biol</source>., <volume>10</volume>, <fpage>961</fpage>–<lpage>980</lpage>.<pub-id pub-id-type="pmid">14980020</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sra</surname>
 <given-names>S.</given-names></string-name></person-group> (<year>2012</year>) 
<article-title>A short note on parameter approximation for von Mises-Fisher distributions: and a fast implementation of I s (x)</article-title>. <source>Comput. Stat</source>., <volume>27</volume>, <fpage>177</fpage>–<lpage>190</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tibshirani</surname>
 <given-names>R.</given-names></string-name></person-group> (<year>1996</year>) 
<article-title>Regression shrinkage and selection via the lasso</article-title>. <source>J. R. Stat. Soc. Series B Methodol</source>., <volume>58</volume>, <fpage>267</fpage>–<lpage>288</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vallat</surname>
 <given-names>L.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2013</year>) 
<article-title>Reverse-engineering the genetic circuitry of a cancer cell with predicted intervention in chronic lymphocytic leukemia</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>110</volume>, <fpage>459</fpage>–<lpage>464</lpage>.<pub-id pub-id-type="pmid">23267079</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>
 <given-names>S.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2011</year>) 
<article-title>Random lasso</article-title>. <source>Ann. Appl. Stat</source>., <volume>5</volume>, <fpage>468</fpage>–<lpage>485</lpage>.<pub-id pub-id-type="pmid">22997542</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname>
 <given-names>Y.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2007</year>) 
<article-title>Controlling variable selection by the addition of pseudovariables</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>102</volume>, <fpage>235</fpage>–<lpage>243</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yuan</surname>
 <given-names>M.</given-names></string-name>, <string-name><surname>Lin</surname><given-names>Y.</given-names></string-name></person-group> (<year>2006</year>) 
<article-title>Model selection and estimation in regression with grouped variables</article-title>. <source>J. R. Stat. Soc. Series B Stat. Methodol</source>., <volume>68</volume>, <fpage>49</fpage>–<lpage>67</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>
 <given-names>C.</given-names></string-name></person-group> (<year>2010</year>) 
<article-title>Nearly unbiased variable selection under minimax concave penalty</article-title>. <source>Ann. Stat</source>., <volume>38</volume>, <fpage>894</fpage>–<lpage>942</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhao</surname>
 <given-names>P.</given-names></string-name>, <string-name><surname>Yu</surname><given-names>B.</given-names></string-name></person-group> (<year>2006</year>) 
<article-title>On model selection consistency of lasso</article-title>. <source>J. Mach. Learn. Res</source>., <volume>7</volume>, <fpage>2541</fpage>–<lpage>2563</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname>
 <given-names>X.</given-names></string-name></person-group>
 <etal>et al</etal> (<year>2013</year>) 
<article-title>Polygenic modeling with Bayesian sparse linear mixed models</article-title>. <source>PLoS Genet</source>., <volume>9</volume>, <fpage>e1003264</fpage>.<pub-id pub-id-type="pmid">23408905</pub-id></mixed-citation>
    </ref>
    <ref id="btaa855-B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zou</surname>
 <given-names>H.</given-names></string-name></person-group> (<year>2006</year>) 
<article-title>The adaptive lasso and its oracle properties</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>101</volume>, <fpage>1418</fpage>–<lpage>1429</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa855-B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zou</surname>
 <given-names>H.</given-names></string-name>, <string-name><surname>Hastie</surname><given-names>T.</given-names></string-name></person-group> (<year>2005</year>) 
<article-title>Regularization and variable selection via the elastic net</article-title>. <source>J. R. Stat. Soc. Series B Stat. Methodol</source>., <volume>67</volume>, <fpage>301</fpage>–<lpage>320</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
