<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neuroinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neuroinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5196</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9171328</article-id>
    <article-id pub-id-type="doi">10.3389/fninf.2022.862805</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Technology and Code</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>vol2Brain: A New Online Pipeline for Whole Brain MRI Analysis</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Manjón</surname>
          <given-names>José V.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/316112/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Romero</surname>
          <given-names>José E.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Vivo-Hernando</surname>
          <given-names>Roberto</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rubio</surname>
          <given-names>Gregorio</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1751012/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Aparici</surname>
          <given-names>Fernando</given-names>
        </name>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>de la Iglesia-Vaya</surname>
          <given-names>Mariam</given-names>
        </name>
        <xref rid="aff5" ref-type="aff">
          <sup>5</sup>
        </xref>
        <xref rid="aff6" ref-type="aff">
          <sup>6</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/385089/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Coupé</surname>
          <given-names>Pierrick</given-names>
        </name>
        <xref rid="aff7" ref-type="aff">
          <sup>7</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1236779/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Instituto de Aplicaciones de las Tecnologías de la Información y de las Comunicaciones Avanzadas (ITACA), Universitat Politècnica de València</institution>, <addr-line>Valencia</addr-line>, <country>Spain</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Instituto de Automática e Informática Industrial, Universitat Politècnica de València</institution>, <addr-line>Valencia</addr-line>, <country>Spain</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Departamento de Matemática Aplicada, Universitat Politècnica de València</institution>, <addr-line>Valencia</addr-line>, <country>Spain</country></aff>
    <aff id="aff4"><sup>4</sup><institution>Área de Imagen Medica, Hospital Universitario y Politécnico La Fe</institution>, <addr-line>Valencia</addr-line>, <country>Spain</country></aff>
    <aff id="aff5"><sup>5</sup><institution>Unidad Mixta de Imagen Biomédica FISABIO-CIPF, Fundación Para el Fomento de la Investigación Sanitario y Biomédica de la Comunidad Valenciana</institution>, <addr-line>Valencia</addr-line>, <country>Spain</country></aff>
    <aff id="aff6"><sup>6</sup><institution>Centro de Investigación Biomédica en Red de Salud Mental, ISC III</institution>, <addr-line>València</addr-line>, <country>Spain</country></aff>
    <aff id="aff7"><sup>7</sup><institution>Centre National de la Recherche Scientifique, Univ. Bordeaux, Bordeaux INP, Laboratoire Bordelais de Recherche en Informatique, UMR5800, PICTURA</institution>, <addr-line>Talence</addr-line>, <country>France</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Fatos Tunay Yarman Vural, Middle East Technical University, Turkey</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Guray Erus, University of Pennsylvania, United States; Nagesh Adluru, University of Wisconsin-Madison, United States</p>
      </fn>
      <corresp id="c001">*Correspondence: José V. Manjón <email>jmanjon@fis.upv.es</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>24</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>16</volume>
    <elocation-id>862805</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>1</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>07</day>
        <month>4</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Manjón, Romero, Vivo-Hernando, Rubio, Aparici, de la Iglesia-Vaya and Coupé.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Manjón, Romero, Vivo-Hernando, Rubio, Aparici, de la Iglesia-Vaya and Coupé</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Automatic and reliable quantitative tools for MR brain image analysis are a very valuable resource for both clinical and research environments. In the past few years, this field has experienced many advances with successful techniques based on label fusion and more recently deep learning. However, few of them have been specifically designed to provide a dense anatomical labeling at the multiscale level and to deal with brain anatomical alterations such as white matter lesions (WML). In this work, we present a fully automatic pipeline (vol2Brain) for whole brain segmentation and analysis, which densely labels (<italic>N</italic> &gt; 100) the brain while being robust to the presence of WML. This new pipeline is an evolution of our previous volBrain pipeline that extends significantly the number of regions that can be analyzed. Our proposed method is based on a fast and multiscale multi-atlas label fusion technology with systematic error correction able to provide accurate volumetric information in a few minutes. We have deployed our new pipeline within our platform volBrain (<ext-link xlink:href="http://www.volbrain.upv.es" ext-link-type="uri">www.volbrain.upv.es</ext-link>), which has been already demonstrated to be an efficient and effective way to share our technology with the users worldwide.</p>
    </abstract>
    <kwd-group>
      <kwd>segmentation</kwd>
      <kwd>brain</kwd>
      <kwd>analysis</kwd>
      <kwd>MRI</kwd>
      <kwd>cloud</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>Ministerio de Economía, Industria y Competitividad, Gobierno de España</institution>
            <institution-id institution-id-type="doi">10.13039/501100010198</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id award-type="contract" rid="cn001">DPI2017-87743-R</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn002">
          <institution-wrap>
            <institution>Agence Nationale de la Recherche</institution>
            <institution-id institution-id-type="doi">10.13039/501100001665</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id award-type="contract" rid="cn002">ANR-18-CE45-0013</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="6"/>
      <equation-count count="4"/>
      <ref-count count="35"/>
      <page-count count="11"/>
      <word-count count="7284"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Introduction</title>
    <p>Quantitative brain image analysis based on MRI has become more and more popular over the last decade due to its high potential to better understand subtle changes in the normal and pathological human brain. The exponential increase in the current neuroimaging data availability and the complexity of the methods to analyze them make the development of novel approaches necessary to address challenges related to the new “Big Data” paradigm (Van Horn and Toga, <xref rid="B32" ref-type="bibr">2014</xref>). Thus, automatic, robust, and reliable methods for automatic brain analysis will have a major role in the near future, most of them being powered by cost-effective cloud-based platforms.</p>
    <p>Specifically, MRI brain structure volume estimation is being increasingly used to better understand the normal brain evolution (Coupé et al., <xref rid="B9" ref-type="bibr">2017</xref>) or the progression of many neurological pathologies such as multiple sclerosis (MS, Commowick et al., <xref rid="B8" ref-type="bibr">2018</xref>) or Alzheimer's disease (Coupé et al., <xref rid="B11" ref-type="bibr">2019</xref>).</p>
    <p>The quantitative estimation of the different brain structure volumes requires automatic, robust, and reliable segmentation of such structures. As manual delineation of the full brain is unfeasible for routine brain analysis (this task is too tedious, time-consuming, and prone to reproducibility errors), many segmentation methods have been proposed over the years. Some of them were initially focused at the tissue level such as the famous Statistical Parametric mapping (SPM) (Ashburner and Friston, <xref rid="B2" ref-type="bibr">2005</xref>). However, this level of detail may be insufficient to detect subtle changes in specific brain structures at early stages of the disease.</p>
    <p>For example, hippocampus and lateral ventricle volumes can be used as early biomarkers of Alzheimer's disease. At this scale, also, cortical and subcortical gray matter (sGM) structures are of special interest for the neuroimaging community. Classic neuroimaging tools such as the well-known FSL package (Jenkinson et al., <xref rid="B20" ref-type="bibr">2012</xref>) or Freesurfer (Fischl et al., <xref rid="B16" ref-type="bibr">2002</xref>) have been widely used over the last 2 decades. More recently, multi-atlas label fusion segmentation techniques have been extensively applied, thanks to their ability to combine multiple atlas information minimizing mislabeling due to inaccurate registrations (Coupé et al., <xref rid="B10" ref-type="bibr">2011</xref>; Wang and Yushkevich, <xref rid="B34" ref-type="bibr">2013</xref>; Manjón et al., <xref rid="B25" ref-type="bibr">2014</xref>; Romero et al., <xref rid="B28" ref-type="bibr">2015</xref>).</p>
    <p>However, segmentation of the whole brain into a large number of structures is still a very challenging problem even for modern multi-atlas based methods (Wang and Yushkevich, <xref rid="B34" ref-type="bibr">2013</xref>; Cardoso et al., <xref rid="B6" ref-type="bibr">2015</xref>; Ledig et al., <xref rid="B21" ref-type="bibr">2015</xref>). The problems encountered are (1) the need of a large set of densely manually labeled brain scans and (2) the large amount of computational time needed to combine all those labeled scans to produce the final segmentation. Fortunately, a fast framework based on collaborative patch-matching was recently proposed (Giraud et al., <xref rid="B17" ref-type="bibr">2016</xref>) to reduce the computational time required by multi-atlas patch-based methods.</p>
    <p>More recently, deep leaning methods have also been proposed for brain structure segmentation. Those methods are mainly patch-based (Wachinger et al., <xref rid="B33" ref-type="bibr">2018</xref>) or 2D (slice-based) (Roy et al., <xref rid="B29" ref-type="bibr">2019</xref>) due to current GPU memory limitations. The current state-of-the-art whole brain deep learning methods are based on ensembles of local neural networks such as the SLANT method (Huo et al., <xref rid="B19" ref-type="bibr">2019</xref>), or more recently the Assemblynet method (Coupé et al., <xref rid="B12" ref-type="bibr">2020</xref>).</p>
    <p>The aim of this study is to present a new software pipeline for whole brain analysis that we have called vol2Brain. It is based on an optimized multi-atlas label fusion scheme that has a reduced execution time, thanks to the use of our fast collaborative patch-matching approach, which has been specifically designed to deal with both normal appearing and lesioned brains (a feature that most of preceding methods ignored). This pipeline automatically provides volumetric brain information at different scales in a very simple manner through a web-based service not requiring any installation or technical requirements in a similar manner as previously done by our volBrain platform that since 2015 has processed more than 360,000 brains online worldwide. In the following sections, the new pipeline will be described, and some evidences of its quality will be presented.</p>
  </sec>
  <sec sec-type="materials and methods" id="s2">
    <title>Materials and Methods</title>
    <sec>
      <title>Dataset Description</title>
      <p>In our proposed method, we used an improved version of the full Neuromorphometrics dataset (<ext-link xlink:href="http://www.neuromorphometrics.com" ext-link-type="uri">http://www.neuromorphometrics.com</ext-link>), which consists of 114 manually segmented brain MR volumes corresponding to subjects with ages covering almost the full lifespan (from 5 to 96 years). Dense neuroanatomical manual labeling of MRI brain scans was performed at Neuromorphometrics, Inc., following the methods described in the study by Caviness et al. (<xref rid="B7" ref-type="bibr">1999</xref>).</p>
      <p>The original MRI scans were obtained from the following sources: (1) the Open Access Series of Imaging Studies (OASIS) project website (<ext-link xlink:href="http://www.oasis-brains.org/" ext-link-type="uri">http://www.oasis-brains.org/</ext-link>) (<italic>N</italic> = 30), (2) the Child and Adolescent NeuroDevelopment Initiative (CANDI) Neuroimaging Access Point (<ext-link xlink:href="http://www.nitrc.org/projects/candi_share" ext-link-type="uri">http://www.nitrc.org/projects/candi_share</ext-link>) (<italic>N</italic> = 13), (3) the Alzheimer's Disease Neuroimaging Initiative (ADNI) project website (<ext-link xlink:href="http://adni.loni.usc.edu/data-samples/access-data/" ext-link-type="uri">http://adni.loni.usc.edu/data-samples/access-data/</ext-link>) (<italic>N</italic> = 30), (4) the McConnell Brain Imaging Center (<ext-link xlink:href="http://www.bic.mni.mcgill.ca/ServicesAtlases/Colin27Highres/" ext-link-type="uri">http://www.bic.mni.mcgill.ca/ServicesAtlases/Colin27Highres/</ext-link>) (<italic>N</italic> = 1), and (5) the 20Repeats dataset (<ext-link xlink:href="http://www.oasis-brains.org/" ext-link-type="uri">http://www.oasis-brains.org/</ext-link>) (<italic>N</italic> = 40).</p>
      <p>Before manual labeling, all the images were preprocessed with an automated bias field inhomogeneity correction (Arnold et al., <xref rid="B1" ref-type="bibr">2001</xref>) and geometrically normalized using three anatomical landmarks [anterior commissure (AC), posterior commissure (PC), and mid-sagittal point]. The scans were reoriented and resliced so that anatomical labeling could be done in coronal planes that follow the AC-PC axis. The manual outlining was performed using an in-house software called the NVM and the exact specification of each region of interest is defined in (1) Neuromorphometrics' General Segmentation Protocol (<ext-link xlink:href="http://neuromorphometrics.com/Seg/" ext-link-type="uri">http://neuromorphometrics.com/Seg/</ext-link>) and (2) the BrainCOLOR Cortical Parcellation Protocol (<ext-link xlink:href="http://Neuromorphometrics.com/ParcellationProtocol_2010-04-05.PDF" ext-link-type="uri">http://Neuromorphometrics.com/ParcellationProtocol_2010-04-05.PDF</ext-link>). It has to be noted that the exact protocols used to label the scans evolved over time. Because of this, not all anatomical regions were labeled in every group (label number range: max = 142, min = 136).</p>
    </sec>
    <sec>
      <title>Dataset Correction</title>
      <p>Right after downloading the Neuromorphometrics dataset, we performed a rigorous quality control of the dataset. We discovered that this dataset presented several issues that had to be corrected before using it.</p>
      <sec>
        <title>Image Resolution, Orientation, and Size</title>
        <p>After checking each individual file, we found that they had different acquisition orientations (coronal, sagittal, and axial). They also have different resolutions (1 ×1 ×1, 0.95 ×0.93 ×1.2, 1.26 ×1.24 ×12, etc.) and different volume sizes (256 ×256 ×307, 256 ×256 ×299, 256 ×256 ×160, etc.). To standardize them, we registered all image and corresponding label files to the MNI152 space using ANTS software, which resulted in a homogeneous dataset with axial orientation, 1 ×1 ×1 mm<sup>3</sup> voxel resolution, and a volume size of 181 ×217 ×181 voxels. We also checked the image quality and we removed 14 cases from the original dataset that presented strong image artifacts and severe blurring effects. This resulted in a final dataset of 100 cases.</p>
      </sec>
      <sec>
        <title>Inconsistent and Different Number of Labels</title>
        <p>The selected 100 files from the previous step had 129 common labels from a total of 142 labels. After analyzing these 13 inconsistent labels, we decided to treat each of them in a specific manner according to the detected issue. Label file description assigns label numbers from 1 to 207. However, we found that labels 228, 229, 230, and 231 were present in some files. After checking them, we realized that labels 228 and 229 on the left corresponded to a right basal foreground (labels 75 and 76) and so we renumbered them. Labels 230 and 231 just represented few pixels in three of the cases and therefore were removed. Labels 63 and 64 (right and left vessel) were not present in all the cases (not always visible) and we decided to renumber them as a part of the putamen (labels 57 and 58), as they were located inside. We removed label 69 (optic chiasm) because it was not present in all the cases and its delineation was very inconsistent. Labels 71, 72, and 73 (cerebellar vermal lobules I-V, VI-VII, and VIII-X) were present in 74 of the 100 cases, and we decided to re-segment the inconsistent cases so that all the cases have these labels (details are given in the following section). Label 78 (corpus callosum) was only present in 25 cases, and we decided to relabel it as right and left white matter (WM, labels 44 and 45). Label 15 (5th ventricle) was very tiny and only present in a few cases (13); thus, it was relabeled as lateral ventricles (labels 51 and 52). Finally, we decided to add two new labels that we found important, i.e., external cerebrospinal fluid (CSF) (labeled as 1) and left and right WM lesions (labels 53 and 54). Details on how these labels were added are provided in the following section. After all the cleanup, the final dataset had a consistent number of 135 labels (refer to <xref rid="SM1" ref-type="supplementary-material">Appendix</xref>).</p>
      </sec>
      <sec>
        <title>Labeling Errors</title>
        <p>Once the dataset had a homogeneous number of labels, we inspected them to check their quality. After inspecting the dataset visually, we found that the boundaries of all the structures in sagittal and axial planes were very irregular. This is probably due to the fact that the original manual delineation was performed in the coronal plane. However, one of the main problems we found was the fact that cortical gray matter (cGM) was severely overestimated, and correspondingly, the CSF and WM were underestimated. This fact has been already highlighted by other researchers (Huo et al., <xref rid="B18" ref-type="bibr">2017</xref>) who pointed out this problem in the context of cortical thickness estimation. The same problem arises in the cerebellum, although it is a bit less pronounced. To solve this problem (Huo et al., <xref rid="B18" ref-type="bibr">2017</xref>), an automatic fusion of the original GM/WM maps was used, and partial volume maps were generated by the TOADS method (Bazin and Pham, <xref rid="B4" ref-type="bibr">2008</xref>) to correct the cortical labels. In this study, we have followed a different approach based on the original manual segmentation and the intensity information.</p>
        <p>First, we combined all the 135 labels into seven different classes (CSF, cGM, cerebral white matter (cWM), sGM, cerebellar gray matter (ceGM), cerebellar white matter (ceWM), and brain stem (BS)]. External CSF was not labeled in the Neuromorphometrics dataset, so we added it using volBrain (Manjón and Coupé, <xref rid="B22" ref-type="bibr">2016</xref>) (we copied CSF label to those pixels that had label 0 in the original label file). Then, the median value of cGM and cWM was estimated and used to generate the partial volume maps using a linear mixing model (Manjón et al., <xref rid="B26" ref-type="bibr">2008</xref>). Voxels in the cGM and cWM interface were relabeled according to their partial volume content (e.g., a cGM voxel with a cWM partial volume coefficient bigger than its corresponding cGM partial volume coefficient was relabeled as cWM). The same process was repeated for the CSF/cGM interface, the ceGM/ceWM interface, and the ceGM/CSF interface. To ensure the regularity of the new label maps, each partial volume map was regularized using a non-local means filter (Coupé et al., <xref rid="B13" ref-type="bibr">2018</xref>). Finally, each case was visually revised and small labeling errors were manually corrected using the ITK-SNAP software. Most of the corrections were related with cGM in the upper part of the brain, and misclassifications of WM lesions were termed as cGM and CSF-related corrections. <xref rid="F1" ref-type="fig">Figure 1</xref> shows an example of the cGM/cWM tissue maps before and after the correction.</p>
        <fig position="float" id="F1">
          <label>Figure 1</label>
          <caption>
            <p>Example of cGM tissue correction. From right to left: Reference T1 image, original cGM map, corrected cGM map, and map of changes (white means inclusion and black means removal of pf voxels). In the bottom row, a close up is shown to better highlight the differences.</p>
          </caption>
          <graphic xlink:href="fninf-16-862805-g0001" position="float"/>
        </fig>
        <p>After the tissue correction, the original structure labels were automatically relabeled to match the new tissue maps. Specifically, those voxels that kept the same tissue type before and after the correction kept their original labels and those that changed were automatically labeled according to the most likely label considering their position and intensity. Results were visually reviewed to assess its correctness and manually corrected when necessary. Finally, we realized that sGM structures showed important segmentation errors and we decided to re-segment them using volBrain automatic segmentation followed by manual correction when needed. <xref rid="F2" ref-type="fig">Figure 2</xref> shows an example of the final relabeling result.</p>
        <fig position="float" id="F2">
          <label>Figure 2</label>
          <caption>
            <p>Top row shows the original labeling and bottom row shows the corrected labeling. Note that the external CSF label has been added to the labeling protocol.</p>
          </caption>
          <graphic xlink:href="fninf-16-862805-g0002" position="float"/>
        </fig>
      </sec>
      <sec>
        <title>LesionBrain Dataset</title>
        <p>One of the main goals of the proposed pipeline was to make it robust to the presence of WM lesions that normally are misclassified as gray matter (GM) in pathological brains. To this end, we included not only healthy cases but also subjects with WM lesions in our library. Specifically, 32 of the 100 cases of the previously described Neuromorphometrics dataset had WM visible lesions with a lesion load ranging from moderate to severe. We are aware that WM lesions can appear anywhere in the brain, but it is also known that they have <italic>a priori</italic> probability to be located in the periventricular areas among others (Coupé et al., <xref rid="B13" ref-type="bibr">2018</xref>).</p>
        <p>We found though that the number of cases with lesions on the dataset was not enough to capture the diversity of WM lesion distribution, so we decided to expand the dataset using a manually labeled MS dataset. We previously used this dataset to develop a MS segmentation method (Coupé et al., <xref rid="B13" ref-type="bibr">2018</xref>).</p>
        <p>This dataset is composed of 43 patients with MS who underwent 3T 3D-T1w MPRAGE and 3D-Fluid-Attenuated Inversion Recovery (FLAIR) MRI. We used only the T1 images, as this is the input modality of our proposed pipeline. To further increase the size of the dataset, we included the left-right flipped version of the images and labels resulting in an extended dataset of 86 cases.</p>
      </sec>
    </sec>
    <sec>
      <title>Vol2Brain Pipeline Description</title>
      <p>The vol2Brain pipeline is a set of image processing tasks dedicated to improve the quality of the input data and to set them into a specific geometric and intensity space, to segment the different structures and to generate useful volumetric information (refer to <xref rid="F3" ref-type="fig">Figure 3</xref> for a general overview). The vol2Brain pipeline is based on the following steps:</p>
      <list list-type="order">
        <list-item>
          <p>Preprocessing</p>
        </list-item>
        <list-item>
          <p>Multiscale labeling and cortical thickness estimation</p>
        </list-item>
        <list-item>
          <p>Report and csv generation</p>
        </list-item>
      </list>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p>vol2Brain pipeline scheme. In the first row, the preprocessing for any new subject is presented. In the second row, the results of the ICC extraction, structure, and tissue segmentations jointly with the cortical thickness estimation are presented. Finally, in the third row, the volumetric information is extracted and presented.</p>
        </caption>
        <graphic xlink:href="fninf-16-862805-g0003" position="float"/>
      </fig>
      <sec>
        <title>Preprocessing</title>
        <p>We have used the same preprocessing steps as those described in the volBrain pipeline (Manjón and Coupé, <xref rid="B22" ref-type="bibr">2016</xref>), as it has been demonstrated to be very robust (based in our experience processing more than 360,000 subjects worldwide). This preprocessing consists of the following steps. To improve the image quality, first, the raw image is denoised using the Spatially Adaptive Non-Local Means (SANLM) filter (Manjón et al., <xref rid="B23" ref-type="bibr">2010</xref>) and inhomogeneity is corrected using the N4 method (Tustison et al., <xref rid="B31" ref-type="bibr">2010</xref>). The resulting image is then affinely registered to the Montreal Neurological Institute (MNI) space using the ANTS software (Avants et al., <xref rid="B3" ref-type="bibr">2008</xref>). The image in the MNI space has a size of 181 ×217 ×181 voxels with 1 mm<sup>3</sup> voxel resolution. Then, we used an inhomogeneity correction based on SPM8 (Ashburner and Friston, <xref rid="B2" ref-type="bibr">2005</xref>) toolbox, as this model-based method has proven to be quite robust once the data are located at the MNI space. Finally, we normalized the images as per intensity by applying a piecewise linear tissue mapping based on the TMS method (Manjón et al., <xref rid="B26" ref-type="bibr">2008</xref>) as described in the study by Manjón and Coupé (<xref rid="B22" ref-type="bibr">2016</xref>). It is worth to note that the library images were also normalized as per intensity using the described approach so that both library and the case to be segmented share a common geometrical and intensity space.</p>
      </sec>
      <sec>
        <title>Multiscale Labeling and Cortical Thickness Estimation</title>
        <p>After the preprocessing, the images are ready to be segmented and measured. This segmentation is performed in several stages.</p>
        <sec>
          <title>ICC Extraction</title>
          <p>The first step in the segmentation process is the intracranial cavity extraction (ICC). This is obtained using the NICE method (Manjón et al., <xref rid="B25" ref-type="bibr">2014</xref>). NICE method is based on a multi-scale non-local label fusion scheme. Details of the NICE method can be found in the study by Manjón et al. (<xref rid="B25" ref-type="bibr">2014</xref>). To further improve the quality of the original NICE method, we have increased the size of the original volBrain template library from 100 to 300 cases using the 100 cases of the vol2Brain library and their left-right mirrored version.</p>
        </sec>
        <sec>
          <title>Full Brain Structure Segmentation</title>
          <p>The dense segmentation of the full brain is based on a multiscale version of the non-local patch-based label fusion technique (Coupé et al., <xref rid="B10" ref-type="bibr">2011</xref>) wherein patches of the subject to be segmented are compared with patches of the training library to look for similar patterns within a predefined search volume to assign the proper label <italic>v</italic> as can be seen in the following equation:</p>
          <disp-formula id="E1">
            <label>(1)</label>
            <mml:math id="M1" overflow="scroll">
              <mml:mtable class="eqnarray" columnalign="left">
                <mml:mtr>
                  <mml:mtd>
                    <mml:mi>v</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                    <mml:mtext> </mml:mtext>
                    <mml:mo>=</mml:mo>
                    <mml:mtext> </mml:mtext>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mstyle displaystyle="true">
                          <mml:munderover accentunder="false" accent="false">
                            <mml:mrow>
                              <mml:mo>∑</mml:mo>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>s</mml:mi>
                              <mml:mo>=</mml:mo>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>N</mml:mi>
                            </mml:mrow>
                          </mml:munderover>
                        </mml:mstyle>
                        <mml:mstyle displaystyle="true">
                          <mml:munder class="msub">
                            <mml:mrow>
                              <mml:mo>∑</mml:mo>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>j</mml:mi>
                              <mml:mo>∈</mml:mo>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>V</mml:mi>
                                </mml:mrow>
                                <mml:mrow>
                                  <mml:mi>i</mml:mi>
                                </mml:mrow>
                              </mml:msub>
                            </mml:mrow>
                          </mml:munder>
                        </mml:mstyle>
                        <mml:mi>w</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>x</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>i</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                            <mml:mo>,</mml:mo>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>x</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>s</mml:mi>
                                <mml:mo>,</mml:mo>
                                <mml:mi>j</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mrow>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>y</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>s</mml:mi>
                            <mml:mo>,</mml:mo>
                            <mml:mi>j</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mstyle displaystyle="true">
                          <mml:munderover accentunder="false" accent="false">
                            <mml:mrow>
                              <mml:mo>∑</mml:mo>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>s</mml:mi>
                              <mml:mtext> </mml:mtext>
                              <mml:mo>=</mml:mo>
                              <mml:mtext> </mml:mtext>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>N</mml:mi>
                            </mml:mrow>
                          </mml:munderover>
                        </mml:mstyle>
                        <mml:mstyle displaystyle="true">
                          <mml:munder class="msub">
                            <mml:mrow>
                              <mml:mo>∑</mml:mo>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>j</mml:mi>
                              <mml:mo>∈</mml:mo>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>V</mml:mi>
                                </mml:mrow>
                                <mml:mrow>
                                  <mml:mi>i</mml:mi>
                                </mml:mrow>
                              </mml:msub>
                            </mml:mrow>
                          </mml:munder>
                        </mml:mstyle>
                        <mml:mi>w</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="false">(</mml:mo>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>x</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>i</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                            <mml:mo>,</mml:mo>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>x</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>s</mml:mi>
                                <mml:mo>,</mml:mo>
                                <mml:mi>j</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mrow>
                          <mml:mo stretchy="false">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
          </disp-formula>
          <p>where <italic>V</italic><sub><italic>i</italic></sub> corresponds to the search volume, <italic>N</italic> is the number of subjects in the templates library, <italic>y</italic><sub><italic>s, j</italic></sub> is the label of the voxel <italic>x</italic><sub><italic>s, j</italic></sub> at the position <italic>j</italic> in the library subject <italic>s</italic>, and <italic>w(x</italic><sub><italic>i</italic></sub><italic>, x</italic><sub><italic>s, j</italic></sub><italic>)</italic> is the patch similarity defined as:</p>
          <disp-formula id="E2">
            <label>(2)</label>
            <mml:math id="M2" overflow="scroll">
              <mml:mtable class="eqnarray" columnalign="left">
                <mml:mtr>
                  <mml:mtd>
                    <mml:mi>w</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mo>,</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>s</mml:mi>
                            <mml:mo>,</mml:mo>
                            <mml:mi>j</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                    <mml:mtext> </mml:mtext>
                    <mml:mo>=</mml:mo>
                    <mml:mtext> </mml:mtext>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mo class="qopname">exp</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mfrac>
                          <mml:mrow>
                            <mml:mo>-</mml:mo>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>D</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>i</mml:mi>
                                <mml:mo>,</mml:mo>
                                <mml:mi>j</mml:mi>
                                <mml:mo>,</mml:mo>
                                <mml:mi>s</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:msup>
                              <mml:mrow>
                                <mml:mi>h</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mn>2</mml:mn>
                              </mml:mrow>
                            </mml:msup>
                          </mml:mrow>
                        </mml:mfrac>
                      </mml:mrow>
                    </mml:msup>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
          </disp-formula>
          <disp-formula id="E3">
            <label>(3)</label>
            <mml:math id="M3" overflow="scroll">
              <mml:mtable class="eqnarray" columnalign="left">
                <mml:mtr>
                  <mml:mtd>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>D</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>j</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>s</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mtext> </mml:mtext>
                    <mml:mo>=</mml:mo>
                    <mml:mtext> </mml:mtext>
                    <mml:mo>|</mml:mo>
                    <mml:mo>|</mml:mo>
                    <mml:mi>P</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                    <mml:mtext> </mml:mtext>
                    <mml:mo>-</mml:mo>
                    <mml:mtext> </mml:mtext>
                    <mml:mi>P</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>s</mml:mi>
                            <mml:mo>,</mml:mo>
                            <mml:mi>j</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                    <mml:mo>|</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mo>|</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
          </disp-formula>
          <p>where P<italic>(x</italic><sub><italic>i</italic></sub><italic>)</italic> is the patch centered at <italic>x</italic><sub><italic>i</italic></sub>, <italic>P(x</italic><sub><italic>s, j</italic></sub><italic>)</italic> is the patch centered at <italic>x</italic><sub><italic>j</italic></sub> in the templates, and ||.||<sub>2</sub> is the normalized L2 norm (normalized by the number of elements) calculated from the distance between each pair of voxels from both patches <italic>P(x</italic><sub><italic>i</italic></sub><italic>)</italic> and <italic>P(x</italic><sub><italic>s, j</italic></sub><italic>)</italic>. <italic>h</italic> is a normalization parameter that is estimated from the minimum of all patch distances within the search volume.</p>
          <p>However, exhaustive patch comparison process is very time-consuming (even in reduced neighborhoods, i.e., when the search volume V is small). To reduce the computational burden of this process, we have used a multiscale adaptation of the OPAL method (Giraud et al., <xref rid="B17" ref-type="bibr">2016</xref>) previously proposed in the study by Romero et al. (<xref rid="B27" ref-type="bibr">2017</xref>), which takes benefit from the concept of Approximate Nearest Neighbor Fields (ANNF). To further speed up the process, we processed only those voxels that were segmented as ICC by the NICE method.</p>
          <p>In patch-based segmentation, the patch size is a key parameter that is strongly related to the structure to be segmented and image resolution. It can be seen in the literature that multi-scale approaches improve segmentation results (Manjón et al., <xref rid="B25" ref-type="bibr">2014</xref>). In the OPAL method (Giraud et al., <xref rid="B17" ref-type="bibr">2016</xref>), independent and simultaneous multi-scale and multi-feature artificial neural networks (ANN) fields were computed. Thus, we have followed a multi-scale approach in which several different ANNs are computed for different patch sizes resulting in different label probability maps that have to be combined. In this study, two patch sizes are used, and an adaptive weighting scheme is proposed to fuse these maps (Equation 3).</p>
          <disp-formula id="E4">
            <label>(4)</label>
            <mml:math id="M4" overflow="scroll">
              <mml:mrow>
                <mml:mi>p</mml:mi>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>l</mml:mi>
                <mml:mo stretchy="false">)</mml:mo>
                <mml:mtext> </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mtext> </mml:mtext>
                <mml:mi>α</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:msub>
                  <mml:mi>p</mml:mi>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                    <mml:mtext> </mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>l</mml:mi>
                <mml:mo stretchy="false">)</mml:mo>
                <mml:mtext> </mml:mtext>
                <mml:mo>+</mml:mo>
                <mml:mtext> </mml:mtext>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mn>1</mml:mn>
                <mml:mtext> </mml:mtext>
                <mml:mo>−</mml:mo>
                <mml:mtext> </mml:mtext>
                <mml:mi>α</mml:mi>
                <mml:mo stretchy="false">)</mml:mo>
                <mml:msub>
                  <mml:mi>p</mml:mi>
                  <mml:mn>2</mml:mn>
                </mml:msub>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mi>l</mml:mi>
                <mml:mo stretchy="false">)</mml:mo>
              </mml:mrow>
            </mml:math>
          </disp-formula>
          <p>where <italic>p</italic><sub>1</sub><italic>(l)</italic> is the probability map of patch-size 3 ×3 ×3 volxels for label <italic>l, p</italic><sub>2</sub><italic>(l)</italic> is the probability map of patch-size 5 ×5 ×5 voxels for label l, <italic>p(l)</italic> is the final probability map for label l, and α ϵ [0,1] is the probability mixing coefficient.</p>
        </sec>
        <sec>
          <title>Systematic Error Correction</title>
          <p>Any segmentation method is subject to both random and systematic errors. The first error type can be typically minimized by using bootstrapped estimations. Fortunately, the non-local label fusion technique estimates the voxel label averaging the votes of many patches, which naturally reduces the random classification error. Unfortunately, systematic errors cannot be reduced using this strategy, as they are not random. However, due to its nature, this systematic bias can be learned, and later, this knowledge can be used to correct the segmentation output (Wang and Yushkevich, <xref rid="B34" ref-type="bibr">2013</xref>).</p>
          <p>In the study by Romero et al. (<xref rid="B27" ref-type="bibr">2017</xref>), we proposed an error corrector method based on a patch-based ensemble of neural networks (PEC for Patch-based Ensemble Corrector) to increase the segmentation accuracy by reducing the systematic errors. Specifically, a shallow neural network ensemble is trained with image patches of sizes 3 ×3 ×3 voxels (fully sampled) and 7 ×7 ×7 voxels (subsampled by skipping two voxels at each dimension) from the T1w images, the automatic segmentations, a distance map value, and their x, y, and z coordinates at MNI152 space. The distance map we used is calculated for the whole structure as the distance in voxels to the structure contour. This results in a vector of 112 features that are mapped to a patch of manual segmentations of size 3 ×3 ×3 voxels. We used a multilayer perceptron with two hidden layers of size 83 and 55 neurons resulting in a network with a topology of 112 ×83 ×55 ×27 neurons. An ensemble of 10 neural networks was trained using a boosting strategy. Each new network was trained with a different subset of data, which was selected by giving a higher probability of selection to those samples that were misclassified in the previous ensemble. More details can be found in the original study (Romero et al., <xref rid="B27" ref-type="bibr">2017</xref>).</p>
        </sec>
      </sec>
      <sec>
        <title>Multiscale Label Generation</title>
        <p>Once the full brain segmentation is performed, different scale versions were computed by combining several labels to generate more generic ones and allowing a multiscale brain analysis. The 135 labels were combined to create a tissue-type segmentation map, including eight different tissues [CSF, cGM, cWM, sGM, ceGM, ceWM, BS, and white matter lesions (WML)]. The cGM and cWM maps will be later used to compute the cortical thickness. Also, cerebrum lobe maps were created by combining cortical GM structures. These maps will be used later to compute the lobe-specific volumes and thickness.</p>
      </sec>
      <sec>
        <title>Cortical Thickness Estimation</title>
        <p>To estimate the cGM thickness, we have used the DiReCT method. DiReCT was introduced in the study by Das et al. (<xref rid="B15" ref-type="bibr">2009</xref>) and was made available in ANTs under the program named <italic>KellyKapowski</italic>. This method is based on the use of a dense non-linear registration to estimate the distance between the inner and the outer parts of the cGM. Cortical thickness per cortical label and per lobe were estimated from the thickness map and the corresponding segmentation maps (Tustison et al., <xref rid="B30" ref-type="bibr">2014</xref>).</p>
      </sec>
      <sec>
        <title>Report Generation</title>
        <p>The output produced by the vol2Brain pipeline consists in a pdf and csv files. These files summarize the volumes and asymmetry ratios estimated from the images. If the user provides sex and age of the submitted subject, population-based normal volumes and asymmetry bounds for all structures are added for reference purposes. These normality bounds were automatically estimated from the IXI dataset (<ext-link xlink:href="https://brain-development.org/ixi-dataset/" ext-link-type="uri">https://brain-development.org/ixi-dataset/</ext-link>), which contains almost 600 normal subjects covering most of the adult lifespan. We are aware that one of the most important sources of variability is the use of different scanners to build the normative values (although the use of our preprocessing reduces this variability). In the near future, we will extend the dataset to have a larger and more representative sample of the population as we already did for the volBrain pipeline (Coupé et al., <xref rid="B9" ref-type="bibr">2017</xref>).</p>
        <p>Furthermore, the user can access to its user area through volBrain website to download the resulting nifti files containing the segmentations at different scales (both in native and MNI space). An example of the volumetric report produced by vol2Brain is shown in <xref rid="SM1" ref-type="supplementary-material">Appendix</xref>.</p>
      </sec>
    </sec>
  </sec>
  <sec id="s3">
    <title>Experiments and Results</title>
    <p>In this section, some experimental results are shown to highlight the accuracy and reproducibility of the proposed pipeline. A leave-two-out procedure was performed for the 100 subjects of the library (i.e., excluding the case to be segmented and its mirrored version). In the dataset, there are 19 cases that were scanned and labeled twice for the purpose of reproducibility estimation. In this case, a leave-four-out procedure was applied to avoid any problem (i.e., excluding the case to be segmented and its mirrored version of the two acquisitions of the same subject). To measure the segmentation quality, the dice index (Zijdenbos et al., <xref rid="B35" ref-type="bibr">1994</xref>) was computed by comparing the manual segmentations with the segmentations obtained with our method. A visual example of the automatic segmentation results is shown in <xref rid="F4" ref-type="fig">Figure 4</xref>.</p>
    <fig position="float" id="F4">
      <label>Figure 4</label>
      <caption>
        <p>Example results of vol2Brain. T1 image, ICC mask, brain tissues, lobes, and structures.</p>
      </caption>
      <graphic xlink:href="fninf-16-862805-g0004" position="float"/>
    </fig>
    <sec>
      <title>Results</title>
      <p>Since presenting dice results of the 135 labels would be impractical, we have decided to show the average results for cortical and non-cortical labels as done in previous studies (Wang and Yushkevich, <xref rid="B34" ref-type="bibr">2013</xref>). In <xref rid="T1" ref-type="table">Table 1</xref>, the results of the proposed method are shown with and without the corrective learning step (PEC) to show the impact that this postprocessing has in the final results (it improved the results in all the cases).</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>Proposed method dice results.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>All labels</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Cortical labels</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Non-cortical labels</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Our method</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8190 ± 0.0300</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.7912 ± 0.0397</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8929 ± 0.0173</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Our method + PEC</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.8262</bold>
                <bold>±0.0257</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.7996</bold>
                <bold>±0.0347</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.8969</bold>
                <bold>±0.0157</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The mean dice is evaluated on all the considered labels (135 without background). *Best results highligthed in bold</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>To further explore the results, we separated them by dataset, as it is well-known that results within the same dataset are normally better than across the datasets. This allows to explore the generalization capabilities of the proposed method. Results are summarized in <xref rid="T2" ref-type="table">Table 2</xref>. As can be seen, results of the OASIS dataset were the best among the datasets. This makes perfect sense, as precisely, this dataset is the largest. CANDI dataset showed the worst results. This dataset had the worst image quality, which somehow explains these results.</p>
      <table-wrap position="float" id="T2">
        <label>Table 2</label>
        <caption>
          <p>Proposed method overall dice results for the full dataset and for each of the subsets.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>All</bold>
                <break/>
                <bold>(<italic>N</italic> = 100)</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1"><bold>OASI</bold>S<break/><bold>(<italic>N</italic> = 68)</bold></th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>CANDI</bold>
                <break/>
                <bold>(<italic>N</italic> = 6)</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>ADNI</bold>
                <break/>
                <bold>(<italic>N</italic> = 25)</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>COLIN</bold>
                <break/>
                <bold>(<italic>N</italic> = 1)</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">0.8262 ± 0.0257</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.8353 ± 0.0233</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.7831± 0.0326</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.8111 ± 0.0142</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.8353</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>One of the objectives of the proposed method was to be able to deal with images with white mater lesions. This is fundamental, as if we do not take into account those regions, they are normally misclassified as a cGM or sGM (which also affects the cortical thickness estimation) (Dadar et al., <xref rid="B14" ref-type="bibr">2021</xref>). The results of WM lesion segmentation are summarized in <xref rid="T3" ref-type="table">Table 3</xref> (left and right lesions were considered together). We separated the results by lesion volume, as it is well-known that small lesions are more difficult to segment than the big ones (Manjón et al., <xref rid="B24" ref-type="bibr">2018</xref>).</p>
      <table-wrap position="float" id="T3">
        <label>Table 3</label>
        <caption>
          <p>Proposed method lesion dice results.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Small (<italic>N</italic> = 76)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Medium (<italic>N</italic> = 21)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Big (<italic>N</italic> = 3)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Avg (<italic>N</italic> = 100)</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Lesion</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.5767 ± 0.1486</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8281 ± 0.0500</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8467 ± 0.0524</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.6440 ± 0.1589</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>*Small (&lt;4 ml), Medium (4–18 ml), Big (&gt;18 ml)</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>Once the full brain is segmented into 135 labels, those labels are grouped together to provide information at different anatomical scales. Specifically, eight different tissue labels are generated. Dice results are summarized in <xref rid="T4" ref-type="table">Table 4</xref>.</p>
      <table-wrap position="float" id="T4">
        <label>Table 4</label>
        <caption>
          <p>Proposed method dice results for each brain tissue.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>CSF</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>cGM</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>cWM</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>sGM</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">0.9006 ± 0.0307</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.9543 ± 0.0144</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.9669 ± 0.0131</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.9518 ± 0.0114</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>ceGM</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>ceWM</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>BS</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>Lesion</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">0.9644 ± 0.0172</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.9448 ± 0.0363</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.9693 ± 0.0137</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.6440 ± 0.1589</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>Method Reproducibility</title>
      <p>A very important feature for a measurement method is its reproducibility. To measure the reproducibility of the proposed method, we used a subset of our library. Specifically, we used 19 cases of the OASIS subset that were scanned and labeled twice. In this case, we have two sources of variability, which are related to the inter-image changes and manual labeling differences. To measure the reproducibility, we computed the dice coefficient between the two different segmentations (of each case and its repetition). This was done for both the manual segmentation (that we used as a reference) and the automatic one. Results are summarized in <xref rid="T5" ref-type="table">Table 5</xref>. As can be seen, the proposed method showed a slightly superior reproducibility than manual labeling.</p>
      <table-wrap position="float" id="T5">
        <label>Table 5</label>
        <caption>
          <p>Proposed method dice results.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>All labels</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Cortical labels</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Non-cortical labels</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">vol2Brain</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8405 ± 0.0181</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8234 ± 0.0206</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8856 ± 0.0158</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Manual</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8368 ± 0.0171</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8198 ± 0.0200</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8818 ± 0.0163</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The mean dice is evaluated on all the considered labels (135 without background)</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>Method Comparison</title>
      <p>It is difficult to compare the proposed method with similar state-of-the-art methods such as Freesurfer, as the labeling protocol is slightly different. For this reason, we have used as a freely available and well-known method called Joint Label Fusion as a reference (Wang and Yushkevich, <xref rid="B34" ref-type="bibr">2013</xref>). This method is a state-of-the-art multi-atlas segmentation approach. To make it fully comparable, we used the corrected cases of our library as the atlas library. We summarized the results of the comparison in <xref rid="T6" ref-type="table">Table 6</xref>. We compared our proposed method with two versions of the JLF approach, one using an affine registered library (linear) and another using a non-linear registered library. It is worth to note the proposed method uses only a linearly registered library (i.e., no non-linear registration was used). As can be noticed, the proposed method was far superior to both versions.</p>
      <table-wrap position="float" id="T6">
        <label>Table 6</label>
        <caption>
          <p>Proposed method dice results compared with the results of two versions of JLF method.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>All labels</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Cortical labels</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Non-cortical labels</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">vol2Brain</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8262 ± 0.0257</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.7996 ± 0.0347</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8969 ± 0.0157</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">JLF (linear)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.7369 ± 0.0292</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.7016 ± 0.0337</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8305 ± 0.0241</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">JLF (non-linear)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.7591 ± 0.0252</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.7327 ± 0.0288</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8291 ± 0.0228</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>Computational Time</title>
      <p>The proposed method takes around 20 min on average to complete the whole pipeline (including cortical thickness estimation and report generation). JLF method takes around only 2 h for structure segmentations without cortical thickness estimation (excluding the preprocessing, which includes several hours of non-linear registration depending on the number of atlases used). Freesurfer normally takes around 6 h to perform the complete analysis (which also includes surface extraction).</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>Discussion</title>
    <p>We have presented a new pipeline for full brain segmentation (vol2Brain) that is able to segment the brain into 135 different regions in a very efficient and accurate manner. The proposed method also integrates these 135 regions to provide measures at different anatomical scales, including brain tissues and lobes. It also provides cortical thickness measurements per cortical structure and lobe displayed into an automatic report summarizing the results (refer to <xref rid="SM1" ref-type="supplementary-material">Appendix</xref>).</p>
    <p>To create vol2Brain pipeline, we had to create a template library that integrates all the anatomical information needed to perform the labeling process. This was a long and laborious work, as the original library obtained from Neuromorphometrics did not meet the required quality and we had to invest a significant amount of time to make it ready to use. To create this library, we homogenized the image resolution, orientation, and size of the images, removed and relabeled inconsistent labels, and corrected systematic labeling errors. Besides, we extended the labeling protocol by adding external CSF and WM lesions. As a result, we generated a highly consistent and high-quality library that not only allowed to develop the current proposed pipeline but will also be a valuable resource for future developments.</p>
    <p>The proposed method is based on patch-based multi-atlas label fusion technology. Specifically, we have used an optimized version of non-local label fusion called OPAL that efficiently finds patch matches needed to label each voxel in the brain by reducing the required time to label the full brain from hours to minutes. To further improve the results, we have used a patch-based error corrector, which has been previously used in other segmentation problems such as hippocampus subfield labeling (Romero et al., <xref rid="B27" ref-type="bibr">2017</xref>) or cerebellum lobules (Carass et al., <xref rid="B5" ref-type="bibr">2018</xref>).</p>
    <p>We measured the results of the proposed pipeline using a LOO methodology and achieved an average dice value of 0.8262. This result was obtained from four different sub-datasets ranking from 0.7831 to 0.8353 showing a good generalization of the proposed method. This result was quite close to the manual intraobserver accuracy that was estimated as 0.8363 using a reduced dataset. We also compared the proposed method with a related currently available state-of-the-art method for full brain labeling. We demonstrated that vol2Brain was not only far superior to the linear (0.8262 vs. 0.7369) and nonlinear (0.8262 vs. 0.7591) versions of JLF method but also more efficient with a temporal cost of minutes compared with hours.</p>
    <p>The proposed vol2Brain pipeline is already available through our volBrain platform (<ext-link xlink:href="https://volbrain.upv.es" ext-link-type="uri">https://volbrain.upv.es</ext-link>). As compared to the rest of the volBrain platform pipelines, this pipeline receives an anonymized and compressed nifti file (a T1-weighted image in the case of vol2Brain) through the website and reports the results 20 min later by sending an email to the user. The user can also download the segmentation nifti files through the user area of volBrain platform (an example of the pdf report is shown in <xref rid="SM1" ref-type="supplementary-material">Appendix</xref>).</p>
    <p>We hope that the accuracy and efficiency of the proposed method and the ease of use through the volBrain platform will boost the anatomical analysis of the normal and pathological brain (especially on those cases with WM lesions).</p>
  </sec>
  <sec sec-type="conclusions" id="s5">
    <title>Conclusion</title>
    <p>In this study, we present a novel pipeline to densely segment the brain and to provide measurements of different features at different anatomical scales in an accurate and efficient manner. The proposed pipeline has been compared with a state-of-the-art-related method showing competitive results in terms of accuracy and computational time. Finally, we hope that the online accessibility of the proposed pipeline will facilitate the access of any user around the world to the proposed pipeline making their MRI data analysis simpler and more efficient.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data Availability Statement</title>
    <p>The datasets presented in this article are not publicly available because the dataset is currently protected by a license. Requests to access the datasets should be directed to the corresponding author.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>JM and PC designed and implemented the software. JR helped in the experiments and coding. RV-H, GR, MI-V, and FA helped in the library definition and report generation. All authors writed and reviewed the paper. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-information" id="s8">
    <title>Funding</title>
    <p>This research was supported by the Spanish DPI2017-87743-R grant from the Ministerio de Economia, Industria y Competitividad of Spain. This work was benefited from the support of the project DeepvolBrain of the French National Research Agency (ANR-18-CE45-0013). This study was achieved within the context of the Laboratory of Excellence TRAIL ANR-10-LABX-57 for the BigDataBrain project.</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s9">
    <title>Publisher's Note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
</body>
<back>
  <ack>
    <p>We thank the Investments for the future Program IdEx Bordeaux (ANR-10-IDEX- 03- 02, HL-MRI Project), Cluster of excellence CPU, and the CNRS.</p>
  </ack>
  <sec sec-type="supplementary-material" id="s10">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fninf.2022.862805/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fninf.2022.862805/full#supplementary-material</ext-link></p>
    <supplementary-material id="SM1" position="float" content-type="local-data">
      <media xlink:href="Data_Sheet_1.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnold</surname><given-names>J. B.</given-names></name><name><surname>Liow</surname><given-names>J. S.</given-names></name><name><surname>Schaper</surname><given-names>K. A.</given-names></name><name><surname>Stern</surname><given-names>J. J.</given-names></name><name><surname>Sled</surname><given-names>J. G.</given-names></name><name><surname>Shattuck</surname><given-names>D. W.</given-names></name><etal/></person-group>. (<year>2001</year>). <article-title>Qualitative and quantitative evaluation of six algorithms for correcting intensity nonuniformity effects</article-title>. <source>Neuroimage</source><volume>13</volume>, <fpage>931</fpage>–<lpage>943</lpage>. <pub-id pub-id-type="doi">10.1006/nimg.2001.0756</pub-id><?supplied-pmid 11304088?><pub-id pub-id-type="pmid">11304088</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashburner</surname><given-names>J.</given-names></name><name><surname>Friston</surname><given-names>K. J.</given-names></name></person-group> (<year>2005</year>). <article-title>Unified segmentation</article-title>. <source>Neuroimage</source>
<volume>26</volume>, <fpage>839</fpage>–<lpage>851</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.02.018</pub-id><?supplied-pmid 15955494?><pub-id pub-id-type="pmid">15955494</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>B. B.</given-names></name><name><surname>Epstein</surname><given-names>C. L.</given-names></name><name><surname>Grossman</surname><given-names>M.</given-names></name><name><surname>Gee</surname><given-names>J. C.</given-names></name></person-group> (<year>2008</year>). <article-title>Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</article-title>. <source>Med. Image Anal.</source>
<volume>12</volume>, <fpage>26</fpage>–<lpage>41</lpage>
<pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id><?supplied-pmid 17659998?><pub-id pub-id-type="pmid">17659998</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bazin</surname><given-names>P. L.</given-names></name><name><surname>Pham</surname><given-names>D. L.</given-names></name></person-group> (<year>2008</year>). <article-title>Homeomorphic brain image segmentation with topological and statistical atlases</article-title>. <source>Med. Image Anal</source>. <volume>12</volume>, <fpage>616</fpage>–<lpage>625</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2008.06.008</pub-id><?supplied-pmid 18640069?><pub-id pub-id-type="pmid">18640069</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carass</surname><given-names>A.</given-names></name><name><surname>Cuzzocreo</surname><given-names>J. L.</given-names></name><name><surname>Han</surname><given-names>S.</given-names></name><name><surname>Hernandez-Castillo</surname><given-names>C. R.</given-names></name><name><surname>Rasser</surname><given-names>P. E.</given-names></name><name><surname>Ganz</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Comparing fully automated state-of-the-art cerebellum parcellation from Magnetic Resonance Imaging</article-title>. <source>Neuroimage</source><volume>183</volume>, <fpage>150</fpage>–<lpage>172</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.08.003</pub-id><?supplied-pmid 30099076?><pub-id pub-id-type="pmid">30099076</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cardoso</surname><given-names>M. J.</given-names></name><name><surname>Modat</surname><given-names>M.</given-names></name><name><surname>Wolz</surname><given-names>R.</given-names></name><name><surname>Melbourne</surname><given-names>A.</given-names></name><name><surname>Cash</surname><given-names>D.</given-names></name><name><surname>Rueckert</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>geodesic information flows: spatially-variant graphs and their application to segmentation and fusion</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>34</volume>, <fpage>1976</fpage>–<lpage>1988</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2015.2418298</pub-id><?supplied-pmid 25879909?><pub-id pub-id-type="pmid">25879909</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caviness</surname><given-names>V. S.</given-names></name><name><surname>Lange</surname><given-names>N. T.</given-names></name><name><surname>Makris</surname><given-names>N.</given-names></name><name><surname>Herbert</surname><given-names>M. R.</given-names></name><name><surname>Kennedy</surname><given-names>D. N.</given-names></name></person-group> (<year>1999</year>). <article-title>MRI based brain volumetrics: emergence of a developmental brain science</article-title>. <source>Brain Dev.</source>
<volume>21</volume>, <fpage>289</fpage>–<lpage>295</lpage>. <pub-id pub-id-type="doi">10.1016/S0387-7604(99)00022-4</pub-id><?supplied-pmid 10413014?><pub-id pub-id-type="pmid">10413014</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Commowick</surname><given-names>O.</given-names></name><name><surname>Istace</surname><given-names>A.</given-names></name><name><surname>Kain</surname><given-names>M.</given-names></name><name><surname>Laurent</surname><given-names>B.</given-names></name><name><surname>Leray</surname><given-names>F.</given-names></name><name><surname>Simon</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Objective evaluation of multiple sclerosis lesion segmentation using a data management and processing infrastructure</article-title>. <source>Sci. Rep.</source><volume>8</volume>, <fpage>13650</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-018-31911-7</pub-id><?supplied-pmid 30209345?><pub-id pub-id-type="pmid">30209345</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coupé</surname><given-names>P</given-names></name><name><surname>Catheline</surname><given-names>G.</given-names></name><name><surname>Lanuza</surname><given-names>E.</given-names></name><name><surname>Manjón</surname><given-names>J. V.</given-names></name></person-group> (<year>2017</year>). <article-title>Towards a unified analysis of brain maturation and aging across the entire lifespan: a MRI analysis</article-title>. <source>Human Brain Mapping</source>
<volume>38</volume>, <fpage>5501</fpage>–<lpage>5518</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.23743</pub-id><?supplied-pmid 28737295?><pub-id pub-id-type="pmid">28737295</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coupé</surname><given-names>P</given-names></name><name><surname>Manjón</surname><given-names>J. V.</given-names></name><name><surname>Fonov</surname><given-names>V.</given-names></name><name><surname>Pruessner</surname><given-names>J.</given-names></name><name><surname>Robles</surname><given-names>M.</given-names></name><name><surname>Collins</surname><given-names>D. L.</given-names></name></person-group> (<year>2011</year>). <article-title>Patch-based segmentation using expert priors: application to hippocampus and ventricle segmentation</article-title>. <source>Neuroimage</source>
<volume>54</volume>, <fpage>940</fpage>–<lpage>954</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.09.018</pub-id><?supplied-pmid 20851199?><pub-id pub-id-type="pmid">20851199</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Coupé</surname><given-names>P</given-names></name><name><surname>Manjón</surname><given-names>J. V.</given-names></name><name><surname>Lanuza</surname><given-names>E.</given-names></name><name><surname>Catheline</surname><given-names>G.</given-names></name></person-group> (<year>2019</year>). <article-title>Timeline of brain alterations in Alzheimer's disease across the entire lifespan</article-title>. <source>Sci. Rep</source>. 9, 3998. <pub-id pub-id-type="doi">10.1038/s41598-019-39809-8</pub-id><?supplied-pmid 30850617?></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coupé</surname><given-names>P</given-names></name><name><surname>Mansencal</surname><given-names>B.</given-names></name><name><surname>Clément</surname><given-names>M.</given-names></name><name><surname>Giraud</surname><given-names>R.</given-names></name><name><surname>Denis de Senneville</surname><given-names>B.</given-names></name><name><surname>Ta</surname><given-names>V. T.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>AssemblyNet: a large ensemble of CNNs for 3D whole brain MRI segmentation</article-title>. <source>Neuroimage</source><volume>219</volume>, <fpage>117026</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117026</pub-id><?supplied-pmid 32522665?><pub-id pub-id-type="pmid">32522665</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Coupé</surname><given-names>P</given-names></name><name><surname>Tourdias</surname><given-names>T.</given-names></name><name><surname>Linck</surname><given-names>P.</given-names></name><name><surname>Romero</surname><given-names>J. E.</given-names></name><name><surname>Manjon</surname><given-names>J. V.</given-names></name></person-group> (<year>2018</year>). <source>LesionBrain: An Online Tool for White Matter Lesion, Segmentation. PatchMI workshop, MICCA2018</source> (<publisher-loc>Granada</publisher-loc>). <pub-id pub-id-type="doi">10.1007/978-3-030-00500-9_11</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dadar</surname><given-names>M.</given-names></name><name><surname>Potvin</surname><given-names>O.</given-names></name><name><surname>Camicioli</surname><given-names>R.</given-names></name><name><surname>Duchesne</surname><given-names>S.</given-names></name></person-group> (<year>2021</year>). <article-title>Beware of white matter hyperintensities causing systematic errors in FreeSurfer gray matter segmentations!</article-title>. <source>Hum. Brain Mapp.</source>
<volume>42</volume>, <fpage>2734</fpage>–<lpage>2745</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.25398</pub-id><?supplied-pmid 33783933?><pub-id pub-id-type="pmid">33783933</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Das</surname><given-names>S. R.</given-names></name><name><surname>Avants</surname><given-names>B. B.</given-names></name><name><surname>Grossman</surname><given-names>M.</given-names></name><name><surname>Gee</surname><given-names>J. C.</given-names></name></person-group> (<year>2009</year>). <article-title>Registration based cortical thickness measurement</article-title>. <source>Neuroimage</source>
<volume>45</volume>, <fpage>867</fpage>–<lpage>879</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.12.016</pub-id><?supplied-pmid 19150502?><pub-id pub-id-type="pmid">19150502</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B.</given-names></name><name><surname>Salat</surname><given-names>D. H.</given-names></name><name><surname>Busa</surname><given-names>E.</given-names></name><name><surname>Albert</surname><given-names>M.</given-names></name><name><surname>Dieterich</surname><given-names>M.</given-names></name><name><surname>Haselgrove</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2002</year>). <article-title>Whole brain segmentation: automated labeling of neuroanatomical structures in the human brain</article-title>. <source>Neuron</source><volume>33</volume>, <fpage>341</fpage>–<lpage>355</lpage>. <pub-id pub-id-type="doi">10.1016/S0896-6273(02)00569-X</pub-id><?supplied-pmid 11832223?><pub-id pub-id-type="pmid">11832223</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname><given-names>R.</given-names></name><name><surname>Ta</surname><given-names>V. T.</given-names></name><name><surname>Papadakis</surname><given-names>N.</given-names></name><name><surname>Manjón</surname><given-names>J. V.</given-names></name><name><surname>Collins</surname><given-names>D. L.</given-names></name><name><surname>Coupé</surname><given-names>P.</given-names></name></person-group> (<year>2016</year>). <article-title>An optimized PatchMatch for multi-scale and multi-feature label fusion</article-title>. <source>Neuroimage</source>
<volume>124</volume>, <fpage>770</fpage>–<lpage>782</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.07.076</pub-id><?supplied-pmid 26244277?><pub-id pub-id-type="pmid">26244277</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huo</surname><given-names>Y.</given-names></name><name><surname>Plassard</surname><given-names>A. J.</given-names></name><name><surname>Carass</surname><given-names>A.</given-names></name><name><surname>Resnick</surname><given-names>S. M.</given-names></name><name><surname>Pham</surname><given-names>D. L.</given-names></name><name><surname>Prince</surname><given-names>J. L.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Consistent cortical reconstruction and multi-atlas brain segmentation</article-title>. <source>Neuroimage</source><volume>138</volume>, <fpage>197</fpage>–<lpage>210</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.05.030</pub-id><?supplied-pmid 27184203?><pub-id pub-id-type="pmid">27184203</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huo</surname><given-names>Y.</given-names></name><name><surname>Xu</surname><given-names>Z.</given-names></name><name><surname>Xiong</surname><given-names>Y.</given-names></name><name><surname>Aboud</surname><given-names>K.</given-names></name><name><surname>Parvathaneni</surname><given-names>P.</given-names></name><name><surname>Bao</surname><given-names>S.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>3D whole brain segmentation using spatially localized atlas network tiles</article-title>. <source>Neuroimage</source><volume>194</volume>, <fpage>105</fpage>–<lpage>119</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.03.041</pub-id><?supplied-pmid 30910724?><pub-id pub-id-type="pmid">30910724</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M.</given-names></name><name><surname>Beckmann</surname><given-names>C. F.</given-names></name><name><surname>Behrens</surname><given-names>T. E.</given-names></name><name><surname>Woolrich</surname><given-names>M. W.</given-names></name><name><surname>Smith</surname><given-names>S. M.</given-names></name></person-group> (<year>2012</year>). <article-title>FSL</article-title>. <source>Neuroimage</source>
<volume>62</volume>, <fpage>782</fpage>–<lpage>790</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.015</pub-id><?supplied-pmid 21979382?><pub-id pub-id-type="pmid">21979382</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ledig</surname><given-names>C.</given-names></name><name><surname>Heckemann</surname><given-names>R. A.</given-names></name><name><surname>Hammers</surname><given-names>A.</given-names></name><name><surname>Lopez</surname><given-names>J. C.</given-names></name><name><surname>Newcombe</surname><given-names>V.</given-names></name><name><surname>Makropoulos</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Robust whole-brain segmentation: application to traumatic brain injury</article-title>. <source>Med. Image Anal.</source><volume>21</volume>, <fpage>40</fpage>–<lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.media.2014.12.003</pub-id><?supplied-pmid 25596765?><pub-id pub-id-type="pmid">25596765</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manjón</surname><given-names>J. V.</given-names></name><name><surname>Coupé</surname><given-names>P</given-names></name></person-group>. (<year>2016</year>). <article-title>volBrain: an online MRI brain volumetry system</article-title>. <source>Front. Neuroinform.</source><volume>10</volume>, <fpage>1</fpage>–<lpage>30</lpage>. <pub-id pub-id-type="doi">10.3389/fninf.2016.00030</pub-id><?supplied-pmid 27512372?><pub-id pub-id-type="pmid">26834620</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manjón</surname><given-names>J. V.</given-names></name><name><surname>Coupé</surname><given-names>P.</given-names></name><name><surname>Martí-Bonmatí</surname><given-names>L.</given-names></name><name><surname>Robles</surname><given-names>M.</given-names></name><name><surname>Collins</surname><given-names>L.</given-names></name></person-group> (<year>2010</year>). <article-title>Adaptive non-local means denoising of MR images with spatially varying noise levels</article-title>. <source>J. Magn. Reson. Imaging</source>
<volume>31</volume>, <fpage>192</fpage>–<lpage>203</lpage>. <pub-id pub-id-type="doi">10.1002/jmri.22003</pub-id><?supplied-pmid 20027588?><pub-id pub-id-type="pmid">20027588</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manjón</surname><given-names>J. V.</given-names></name><name><surname>Coupé</surname><given-names>P.</given-names></name><name><surname>Raniga</surname><given-names>P.</given-names></name><name><surname>Xi</surname><given-names>Y.</given-names></name><name><surname>Desmond</surname><given-names>P.</given-names></name><name><surname>Fripp</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>MRI white matter lesion segmentation using an ensemble of neural networks and overcomplete patch-based voting</article-title>. <source>Comput. Med. Imaging Graphics</source><volume>69</volume>, <fpage>43</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1016/j.compmedimag.2018.05.001</pub-id><?supplied-pmid 30172092?><pub-id pub-id-type="pmid">30172092</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manjón</surname><given-names>J. V.</given-names></name><name><surname>Eskildsen</surname><given-names>S. F.</given-names></name><name><surname>Coupé</surname><given-names>P.</given-names></name><name><surname>Romero</surname><given-names>J. E.</given-names></name><name><surname>Collins</surname><given-names>D. L.</given-names></name><name><surname>Robles</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>). <article-title>Non-local intracranial cavity extraction</article-title>. <source>IJBI</source>
<volume>2014</volume>, <fpage>820205</fpage>. <pub-id pub-id-type="doi">10.1155/2014/820205</pub-id><?supplied-pmid 25328511?><pub-id pub-id-type="pmid">25328511</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manjón</surname><given-names>J. V.</given-names></name><name><surname>Tohka</surname><given-names>J.</given-names></name><name><surname>García-Martí</surname><given-names>G.</given-names></name><name><surname>Carbonell-Caballero</surname><given-names>J.</given-names></name><name><surname>Lull</surname><given-names>J. J.</given-names></name><name><surname>Martí-Bonmatí</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2008</year>). <article-title>Robust MRI brain tissue parameter estimation by multistage outlier rejection</article-title>. <source>Magn. Reson. Med.</source><volume>59</volume>, <fpage>866</fpage>–<lpage>873</lpage>. <pub-id pub-id-type="doi">10.1002/mrm.21521</pub-id><?supplied-pmid 18383286?><pub-id pub-id-type="pmid">18383286</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romero</surname><given-names>J. E.</given-names></name><name><surname>Coupé</surname><given-names>P.</given-names></name><name><surname>Giraud</surname><given-names>R.</given-names></name><name><surname>Ta</surname><given-names>V. T.</given-names></name><name><surname>Fonov</surname><given-names>V.</given-names></name><name><surname>Park</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>CERES: a new cerebellum lobule segmentation method</article-title>. <source>Neuroimage</source><volume>147</volume>, <fpage>916</fpage>–<lpage>924</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.11.003</pub-id><?supplied-pmid 27833012?><pub-id pub-id-type="pmid">27833012</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romero</surname><given-names>J. E.</given-names></name><name><surname>Manjón</surname><given-names>J. V.</given-names></name><name><surname>Tohka</surname><given-names>J.</given-names></name><name><surname>Coupé</surname><given-names>P.</given-names></name><name><surname>Robles</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>Non-local automatic brain hemisphere segmentation</article-title>. <source>Magn. Reson. Imaging</source>
<volume>33</volume>, <fpage>474</fpage>–<lpage>484</lpage>. <pub-id pub-id-type="doi">10.1016/j.mri.2015.02.005</pub-id><?supplied-pmid 25660644?><pub-id pub-id-type="pmid">25660644</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>A. G.</given-names></name><name><surname>Conjeti</surname><given-names>S.</given-names></name><name><surname>Navab</surname><given-names>N.</given-names></name><name><surname>Wachinger</surname><given-names>C.</given-names></name></person-group> (<year>2019</year>). <article-title>QuickNAT: a fully convolutional network for quick and accurate segmentation of neuroanatomy</article-title>. <source>Neuroimage</source>
<volume>186</volume>, <fpage>713</fpage>–<lpage>727</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.11.042</pub-id><?supplied-pmid 30502445?><pub-id pub-id-type="pmid">30502445</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>N.</given-names></name><name><surname>Cook</surname><given-names>P.</given-names></name><name><surname>Klein</surname><given-names>A.</given-names></name><name><surname>Song</surname><given-names>G.</given-names></name><name><surname>Das</surname><given-names>S. R.</given-names></name><name><surname>Duda</surname><given-names>J. T.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Large-scale evaluation of ANTs and FreeSurfer cortical thickness measurements</article-title>. <source>Neuroimage</source><volume>1</volume>, <fpage>166</fpage>–<lpage>179</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.05.044</pub-id><?supplied-pmid 24879923?><pub-id pub-id-type="pmid">24879923</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>N. J.</given-names></name><name><surname>Avants</surname><given-names>B. B.</given-names></name><name><surname>Cook</surname><given-names>P. A.</given-names></name><name><surname>Zheng</surname><given-names>Y.</given-names></name><name><surname>Egan</surname><given-names>A.</given-names></name><name><surname>Yushkevich</surname><given-names>P. A.</given-names></name></person-group> (<year>2010</year>). <article-title>N4ITK: improved N3 bias correction</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>29</volume>, <fpage>1310</fpage>–<lpage>1320</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id><?supplied-pmid 20378467?><pub-id pub-id-type="pmid">20378467</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Horn</surname><given-names>J. D.</given-names></name><name><surname>Toga</surname><given-names>A. W.</given-names></name></person-group> (<year>2014</year>). <article-title>Human neuroimaging as a “Big Data” science</article-title>. <source>Brain Imaging Behav</source>. <volume>8</volume>, <fpage>323</fpage>–<lpage>331</lpage>. <pub-id pub-id-type="doi">10.1007/s11682-013-9255-y</pub-id><?supplied-pmid 24113873?><pub-id pub-id-type="pmid">24113873</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wachinger</surname><given-names>C.</given-names></name><name><surname>Reuter</surname><given-names>M.</given-names></name><name><surname>Klein</surname><given-names>T.</given-names></name></person-group> (<year>2018</year>). <article-title>DeepNAT: Deep convolutional neural network for segmenting neuroanatomy</article-title>. <source>Neuroimage</source>
<volume>170</volume>, <fpage>434</fpage>–<lpage>445</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.02.035</pub-id><?supplied-pmid 28223187?><pub-id pub-id-type="pmid">28223187</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Yushkevich</surname><given-names>P.</given-names></name></person-group> (<year>2013</year>). <article-title>Multi-atlas segmentation with joint label fusion and corrective learning—an open source implementation</article-title>. <source>Front. Neuroinform.</source>
<volume>7</volume>, <fpage>27</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2013.00027</pub-id><?supplied-pmid 24319427?><pub-id pub-id-type="pmid">24319427</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zijdenbos</surname><given-names>A. P.</given-names></name><name><surname>Dawant</surname><given-names>B. M.</given-names></name><name><surname>Margolin</surname><given-names>R. A.</given-names></name><name><surname>Palmer</surname><given-names>A. C.</given-names></name></person-group> (<year>1994</year>). <article-title>Morphometric analysis of white matter lesions in MR images: method and validation</article-title>. <source>IEEE Trans. Med. Imaging</source>
<volume>13</volume>, <fpage>716</fpage>–<lpage>724</lpage>. <pub-id pub-id-type="doi">10.1109/42.363096</pub-id><?supplied-pmid 18218550?><pub-id pub-id-type="pmid">18218550</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
