<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neurosci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neurosci</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neurosci.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroscience</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1662-4548</issn>
    <issn pub-type="epub">1662-453X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7201293</article-id>
    <article-id pub-id-type="doi">10.3389/fnins.2020.00125</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Technology and Code</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>BraTS Toolkit: Translating BraTS Brain Tumor Segmentation Algorithms Into Clinical and Scientific Practice</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Kofler</surname>
          <given-names>Florian</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/819137/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Berger</surname>
          <given-names>Christoph</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/819465/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Waldmannstetter</surname>
          <given-names>Diana</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/863025/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lipkova</surname>
          <given-names>Jana</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/806592/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ezhov</surname>
          <given-names>Ivan</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/862469/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tetteh</surname>
          <given-names>Giles</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/906556/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kirschke</surname>
          <given-names>Jan</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/353765/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zimmer</surname>
          <given-names>Claus</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/362661/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wiestler</surname>
          <given-names>Benedikt</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn002">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/460024/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Menze</surname>
          <given-names>Bjoern H.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn002">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/604278/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Image-Based Biomedical Modeling, Department of Informatics, Technical University of Munich</institution>, <addr-line>Munich</addr-line>, <country>Germany</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Department of Neuroradiology, Klinikum rechts der Isar</institution>, <addr-line>Munich</addr-line>, <country>Germany</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Kamran Avanaki, Wayne State University, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Suyash P. Awate, Indian Institute of Technology Bombay, India; Guido Gerig, NYU Tandon School of Engineering, United States</p>
      </fn>
      <corresp id="c001">*Correspondence: Florian Kofler <email>florian.kofler@tum.de</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Brain Imaging Methods, a section of the journal Frontiers in Neuroscience</p>
      </fn>
      <fn fn-type="other" id="fn002">
        <p>†These authors have contributed equally to this work and share senior authorship</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>29</day>
      <month>4</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>14</volume>
    <elocation-id>125</elocation-id>
    <history>
      <date date-type="received">
        <day>30</day>
        <month>9</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>31</day>
        <month>1</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2020 Kofler, Berger, Waldmannstetter, Lipkova, Ezhov, Tetteh, Kirschke, Zimmer, Wiestler and Menze.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Kofler, Berger, Waldmannstetter, Lipkova, Ezhov, Tetteh, Kirschke, Zimmer, Wiestler and Menze</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Despite great advances in brain tumor segmentation and clear clinical need, translation of state-of-the-art computational methods into clinical routine and scientific practice remains a major challenge. Several factors impede successful implementations, including data standardization and preprocessing. However, these steps are pivotal for the deployment of state-of-the-art image segmentation algorithms. To overcome these issues, we present BraTS Toolkit. BraTS Toolkit is a holistic approach to brain tumor segmentation and consists of three components: First, the BraTS Preprocessor facilitates data standardization and preprocessing for researchers and clinicians alike. It covers the entire image analysis workflow prior to tumor segmentation, from image conversion and registration to brain extraction. Second, BraTS Segmentor enables orchestration of BraTS brain tumor segmentation algorithms for generation of fully-automated segmentations. Finally, Brats Fusionator can combine the resulting candidate segmentations into consensus segmentations using fusion methods such as majority voting and iterative SIMPLE fusion. The capabilities of our tools are illustrated with a practical example to enable easy translation to clinical and scientific practice.</p>
    </abstract>
    <kwd-group>
      <kwd>brain tumor segmentation</kwd>
      <kwd>anonymization</kwd>
      <kwd>MRI data preprocessing</kwd>
      <kwd>medical imaging</kwd>
      <kwd>brain extraction</kwd>
      <kwd>BraTS</kwd>
      <kwd>glioma</kwd>
    </kwd-group>
    <counts>
      <fig-count count="5"/>
      <table-count count="0"/>
      <equation-count count="0"/>
      <ref-count count="32"/>
      <page-count count="8"/>
      <word-count count="4591"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Advances in deep learning have led to unprecedented opportunities for computer-aided image analysis. In image segmentation, the introduction of the U-Net architecture (Ronneberger et al., <xref rid="B29" ref-type="bibr">2015</xref>) and subsequently developed variations like the V-Net (Milletari et al., <xref rid="B26" ref-type="bibr">2016</xref>) or the 3D U-Net (Çiçek et al., <xref rid="B5" ref-type="bibr">2016</xref>) have yielded algorithms for brain tumor segmentation that achieve a performance comparable to experienced human raters (Dvorak and Menze, <xref rid="B7" ref-type="bibr">2015</xref>; Menze et al., <xref rid="B22" ref-type="bibr">2015a</xref>; Bakas et al., <xref rid="B3" ref-type="bibr">2018</xref>). A recent retrospective analysis of a large, multi-center cohort of glioblastoma patients convincingly demonstrated that objective assessment of tumor response via U-Net-based segmentation outperforms the assessment by human readers in terms of predicting patient survival (Kickingereder et al., <xref rid="B16" ref-type="bibr">2019</xref>; Kofler et al., <xref rid="B17" ref-type="bibr">2019</xref>), suggesting a potential benefit of implementing these algorithms into clinical routine.</p>
    <p>Recent works present diverse approaches toward brain tumor segmentation and analysis. Jena and Awate (<xref rid="B14" ref-type="bibr">2019</xref>) introduced a Deep-Neural-Network for image segmentation with uncertainty estimates based on Bayesian decision theory. Shboul et al. (<xref rid="B32" ref-type="bibr">2019</xref>) deployed feature-guided radiomics for glioblastoma segmentation and survival prediction. Jungo et al. (<xref rid="B15" ref-type="bibr">2018</xref>) analyzed the impact of inter-rater variability and fusion techniques for ground truth generation on uncertainty estimation. Shah et al. (<xref rid="B31" ref-type="bibr">2018</xref>) combined strong and weak supervision in training of their segmentation network to reduce overall supervision cost. Cheplygina et al. (<xref rid="B4" ref-type="bibr">2019</xref>) created an overview of Machine Learning methods in medical image analysis employing less or unconventional kinds of supervision.</p>
    <p>In earlier years researchers experimented with a variety of approaches to tackle brain tumor segmentation (Prastawa et al., <xref rid="B28" ref-type="bibr">2003</xref>; Menze et al., <xref rid="B24" ref-type="bibr">2010</xref>, <xref rid="B23" ref-type="bibr">2015b</xref>; Geremia et al., <xref rid="B10" ref-type="bibr">2012</xref>), however in recent years the field is increasingly dominated by convolutional neural networks (CNN). This is also reflected in the contributions to the Multimodel Brain Tumor Segmentation Benchmark (BraTS) challenge (Bakas et al., <xref rid="B3" ref-type="bibr">2018</xref>). The BraTS challenge (Menze et al., <xref rid="B22" ref-type="bibr">2015a</xref>; Bakas et al., <xref rid="B2" ref-type="bibr">2017</xref>) was introduced in 2012 at the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), evaluating different algorithms for automated brain tumor segmentation. Therefore, every year the BraTS organizers provide a set of MRI scans, consisting of T1, T1c, T2, and FLAIR images from low- and high-grade glioma patients, coming with the corresponding ground truth segmentations.</p>
    <p>Nonetheless, the computational methods presented in the BraTS challenge have not found their way into clinical and scientific practice. While the individual reasons vary, there are some key obstacles that impede the successful implementation of these algorithms. First of all, the availability of data for training, especially of high-quality, well-annotated data, is limited. Additionally, data protection as well as ethical barriers, complicate the development of centralized solutions, making local solutions strongly preferable. Furthermore, there are knowledge and skill barriers, when it comes to the conduction of setting up necessary preprocessing of data, while time and resources are limited.</p>
    <p>While individual solutions for several of these problems exist, such as containerization for simplified distribution of code or public datasets, these are oftentimes fragmented and hence difficult to combine. Centralizing these efforts holds promise for making advances in image analysis easily available for broad implementation. Here we introduce three components to tackle these problems. First <italic>BraTS preprocessor</italic> facilitates data standardization and preprocessing for researchers and clinicians alike. Building upon that, varying tumor segmentations can be obtained from multiple algorithms with <italic>BraTS Segmentor</italic>. Finally, <italic>BraTS Fusionator</italic> can fuse these candidate segmentations into consensus segmentations by majority voting and iterative SIMPLE (Langerak et al., <xref rid="B18" ref-type="bibr">2010</xref>) fusion. Together our tools represent <italic>BraTS Toolkit</italic> and enable a holistic approach integrating all the steps necessary for brain tumor image analysis.</p>
  </sec>
  <sec sec-type="methods" id="s2">
    <title>2. Methods</title>
    <p>We developed BraTS Toolkit to get from raw DICOM data to fully automatically generate tumor segmentations in NIFTI format. The toolkit consists of three modular components. <xref ref-type="fig" rid="F1">Figure 1</xref> visualizes how a typical brain tumor segmentation pipeline can be realized using the toolkit. The data is first preprocessed using the BraTS Preprocessor, then candidate segmentations are obtained from the BraTS Segmentor and finally fused via the BraTS Fusionator. Each component can be replaced with custom solutions to account for local requirements<xref ref-type="fn" rid="fn0001"><sup>1</sup></xref>. A key design principle of the software is that all data processing happens locally to comply with data privacy and protection regulations.</p>
    <fig id="F1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>Illustration of a typical dataflow to get from raw MRI scans to segmented brain tumors by combining the three components of the BraTS Toolkit. After preprocessing the raw MRI scans using the BraTS Preprocessor, the data is passed to the BraTS Segmentor, where arbitrary state-of-the-art models from the BraTS algorithmic repository can be used for segmentation. With BraTS Fusionator, multiple candidate segmentations may then be fused to obtain a consensus segmentation. As the Toolkit is designed to be completely modular and with clearly defined interfaces, each component can be replaced with custom solutions if required.</p>
      </caption>
      <graphic xlink:href="fnins-14-00125-g0001"/>
    </fig>
    <p>BraTS Toolkit comes as a python package and can be deployed either via Python or by using the integrated command line interface (CLI). As the software is subject to ongoing development and improvement this work focuses on more abstract descriptions of the software's fundamental design principles. To ease deployment in scientific and clinical practice an up-to-date user guide with installation and usage instructions can be found here: <ext-link ext-link-type="uri" xlink:href="https://neuronflow.github.io/BraTS-Toolkit/">https://neuronflow.github.io/BraTS-Toolkit/</ext-link>.</p>
    <p>Users that prefer an easier approach can alternatively use the BraTS Preprocessor's graphical user interface (GUI) to take care of the data preprocessing<xref ref-type="fn" rid="fn0002"><sup>2</sup></xref>. The GUI is constantly improved in a close feedback loop with radiologists from the department of Neuroradiology at Klinikum Rechts der Isar (Technical University of Munich) to address the needs of clinical practitioners. Depending on the community's feedback, we plan to additionally provide graphical user interfaces for BraTS Segmentor and BraTS Fusionator in the future. Therefore, BraTS Toolkit features update mechanisms to ensure that users have access to the latest features.</p>
    <sec>
      <title>2.1. Component One: BraTS Preprocessor</title>
      <p>BraTS Preprocessor provides image conversion, registration, and anonymization functionality. The starting point to use BraTS Preprocessor is to have T1, T1c, T2, and FLAIR imaging data in NIFTI format. DICOM files can be converted to NIFTI format using the embedded dcm2niix conversion software (Li et al., <xref rid="B20" ref-type="bibr">2016</xref>).</p>
      <p>The main output of BraTS Preprocessor consists of the anonymized image data of all four modalities in BraTS space. Moreover, it generates the original input images converted to BraTS space, anonymized data in native space, defacing/skullstripping masks for anonymization, registration matrices to convert between BraTS and native space and overview images of the volumes' slices in png format. <xref ref-type="fig" rid="F2">Figure 2</xref> depicts the data-processing in detail.</p>
      <fig id="F2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Illustration of the data-processing. We start with a T1, T1c, T2, and FLAIR volume. In a first step we co-register all modalities to the T1 image. Depending on the chosen mode, we then compute the brain segmentation or defacing mask in T1-space. To morph the segmented images in native space, we transform the mask to the respective native spaces and multiply it with the volumes. For obtaining the segmented images in BraTS space, we transform the masks and volumes to the BraTS space using a brain atlas. We then apply the masks to the volumes.</p>
        </caption>
        <graphic xlink:href="fnins-14-00125-g0002"/>
      </fig>
      <p>BraTS Preprocessor handles standardization and preprocessing of brain MRI data using a classical front- and back end software architecture. <xref ref-type="fig" rid="F3">Figure 3</xref> illustrates the GUI variant's software architecture, which enables users without programming knowledge to handle MRI data pre-processing steps.</p>
      <fig id="F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>BraTS Preprocessor software architecture (GUI variant). The front end is implemented by a Vue.js web application packaged via Electron.js. To ensure a constant runtime environment the Python based back end resides in a Docker container (Merkel, <xref rid="B25" ref-type="bibr">2014</xref>). Redis Queue allows for load balancing and parallelization of the processing. The architecture enables two-way communication between front end and back end by implementing Socket.IO on the former and Flask-Socket.IO on the latter. In contrast to this the python package's front end is implemented using python-socketio.</p>
        </caption>
        <graphic xlink:href="fnins-14-00125-g0003"/>
      </fig>
      <p>Advanced Normalization Tools (ANTs) (Avants et al., <xref rid="B1" ref-type="bibr">2011</xref>) are deployed for linear registration and transformation of images into BraTS space, independent of the selected mode. In order to achieve proper anonymization of the image data there are four different processing modes to account for different local requirements and hardware configurations:</p>
      <list list-type="order">
        <list-item>
          <p>GPU brain-extraction mode</p>
        </list-item>
        <list-item>
          <p>CPU brain-extraction mode</p>
        </list-item>
        <list-item>
          <p>GPU defacing mode (under development)</p>
        </list-item>
        <list-item>
          <p>CPU defacing mode</p>
        </list-item>
      </list>
      <p>Brain extraction is implemented by means of HD-BET (Isensee et al., <xref rid="B13" ref-type="bibr">2019</xref>) using GPU or CPU, respectively. HD-BET is a deep learning based brain extraction method, which is trained on glioma patients and therefore particularly well-suited for our task. In case the available RAM is not sufficient the CPU mode automatically falls back to ROBEX (Iglesias et al., <xref rid="B11" ref-type="bibr">2011</xref>). ROBEX is another robust, but slightly less accurate, skull-stripping method that requires less RAM than HD-BET, when running on CPU.</p>
      <p>Alternatively, the BraTS Preprocessor features GPU and CPU defacing modes for users who find brain-extraction too destructive. Defacing on the CPU is implemented via Freesurfer's mri-deface (Fischl, <xref rid="B9" ref-type="bibr">2012</xref>), while deep-learning based defacing on the GPU is currently under development.</p>
    </sec>
    <sec>
      <title>2.2. Component Two: BraTS Segmentor</title>
      <p>The Segmentor module provides a standardized control interface for the BraTS algorithmic repository<xref ref-type="fn" rid="fn0003"><sup>3</sup></xref> (Bakas et al., <xref rid="B3" ref-type="bibr">2018</xref>). This repository is a collection of Docker images, each containing a Deep Learning model and accompanying code designed for the BraTS challenge. Each model has a rigidly defined interface to hand data to the model and retrieve segmentation results from the model. This enables the application of state-of-the-art models for brain tumor segmentation on new data without the need to install additional software or to train a model from scratch. However, even though the algorithmic repository provides unified models, it is still up to the interested user to download and run each Docker image individually as well as manage the input and output. This final gap in the pipeline is closed by the Segmentor, which enables less experienced users to download, run and evaluate any model in the BraTS algorithmic repository. It provides a front end to manage all available containers and run them on arbitrary data, as long as the data conforms to the BraTS format. To this end, the Segmentor provides a command line interface to process data with any or all of the available Docker images in the repository while ensuring proper handling of the files. Its modular structure also allows anyone to extend the code, include other Docker containers or include it as a Python package.</p>
    </sec>
    <sec>
      <title>2.3. Component Three: BraTS Fusionator</title>
      <p>The Segmentor module can generate multiple segmentations for a given set of images which usually vary in accuracy and without prior knowledge, a user might be unsure which segmentation is the most accurate. The Fusionator module provides two methods to combine this arbitrary number of segmentation candidates into one final fusion which represents the consensus of all available segmentations. There are two main methods offered: Majority voting and the selective and iterative method for performance level estimation (SIMPLE) proposed by Langerak et al. (<xref rid="B18" ref-type="bibr">2010</xref>). Both methods take all available candidate segmentations produced by the algorithms of the repository and combine each label to generate a final fusion. In majority voting, a class is assigned to a given voxel if at least half of the candidate segmentations agree that this voxel is of a certain class. This is repeated for each class to generate the complete segmentation. The SIMPLE fusion works as follows: First, a majority vote fusion with all candidate segmentations is performed. Secondly, each candidate segmentation is compared to the current consensus fusion and the resulting overlap score (a standard DICE measure in the proposed method) is used as a weight for the majority voting. This causes the candidate segmentations with higher estimated accuracy to have a higher influence on the final result. Lastly, each candidate segmentation with an accuracy below a certain threshold is dropped out after each iteration. This iterative process is stopped once the consensus fusion converges. After repeating the processes for each label, a final segmentation is obtained.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>3. Results</title>
    <p>The broad availability of Python, Electron.js, and Docker allows us to support all major operating systems with an easy installation process. Users can choose to process data using the command line (CLI) or through the user friendly graphical user interface (GUI). Depending on the available hardware, multiple threads are run to efficiently use the system's resources.</p>
    <sec>
      <title>3.1. Practicality in Clinical and Scientific Practice</title>
      <p>To test the practicality of BraTS Toolkit we conducted a brain tumor segmentation experiment on 191 patients of the BraTS 2016 dataset. As a first step we generated candidate tumor segmentations. BraTS Segmentor allowed us to rapidly obtain tumor delineations from ten different algorithms of the BraTS algorithmic repository (Bakas et al., <xref rid="B3" ref-type="bibr">2018</xref>). The standardized user interface of BraTS Segmentor abstracts all the required background knowledge regarding docker and the particularities of the algorithms. In the next step we used BraTS Fusionator to fuse the generated segmentations by consensus voting. <xref ref-type="fig" rid="F4">Figure 4</xref> shows that fusion by iterative SIMPLE and class-wise majority voting had a slight advantage over single algorithms. This effect was particularly driven by removal of false positives as illustrated for an exemplary patient in <xref ref-type="fig" rid="F5">Figure 5</xref>. BraTS Toolkit enabled us to conduct the experiment in a user-friendly way. With only a few lines of Python code we were able to obtain segmentation results in a fully-automated fashion. This impression was confirmed by experiments on further in house data-sets where we also deployed the CLI and GUI variants of all three BraTS Toolkit components with great feedback from clinical and scientific practitioners. Users especially appreciated the increased robustness and precision of consensus segmentations compared to existing single algorithm solutions.</p>
      <fig id="F4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Evaluation of the segmentation results on the BraTS 2016 data set for whole tumor labels on <italic>n</italic> = 191 evaluated test cases. We generated candidate segmentations with ten different algorithms. Segmentation methods are sorted in descending order by mean dice score. The two fusion methods, iterative SIMPLE (sim) and class-wise majority voting displayed on the left, outperformed individual algorithms depicted further right. The red horizontal line shows the SIMPLE median dice score (<italic>M</italic> = 0.863) for better comparison.</p>
        </caption>
        <graphic xlink:href="fnins-14-00125-g0004"/>
      </fig>
      <fig id="F5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Single algorithm vs. iterative SIMPLE consensus segmentation. T2 scans with segmented labels by exemplary candidate algorithms from <bold>(A)</bold> Pawar et al. (<xref rid="B27" ref-type="bibr">2018</xref>), <bold>(B)</bold> Sedlar (<xref rid="B30" ref-type="bibr">2018</xref>), and <bold>(C)</bold> Isensee et al. (<xref rid="B12" ref-type="bibr">2017</xref>) (Green: edema; Red: necrotic region/non-enhancing tumor; Yellow: enhancing tumor). <bold>(D)</bold> Shows a consensus segmentation obtained using the iterative SIMPLE fusion. Notice the false positives marked with white circles on the candidate segmentations. These outliers are effectively reduced in the fusion segmentation shown in <bold>(D)</bold>.</p>
        </caption>
        <graphic xlink:href="fnins-14-00125-g0005"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>4. Discussion</title>
    <p>Overall, the BraTS Toolkit is a step toward the democratization of automatic brain tumor segmentation. By lowering resource and knowledge barriers, users can effectively disseminate dockerized brain tumor segmentation algorithms collected through the BraTS challenge. Thus, it makes objective brain tumor volumetry, which has been demonstrated to be superior to traditional image assessment (Kickingereder et al., <xref rid="B16" ref-type="bibr">2019</xref>), readily available for scientific and clinical use.</p>
    <p>Currently, BraTS segmentation algorithms and therefore BraTS Segmentor require each of T1, T1c, T2, and FLAIR sequences to be present. In practice, this can become a limiting factor due to errors in data acquisition or incomplete protocols leading to missing modalities. Recent efforts try to bridge this gap by using machine learning techniques to reconstruct missing image modalities (e.g., Dorent et al., <xref rid="B6" ref-type="bibr">2019</xref>; Li et al., <xref rid="B19" ref-type="bibr">2019</xref>).</p>
    <p>Other crucial aspects of data preprocessing are the lack of standards for pulse sequences across different scanners and manufacturers, and absence of data acquisition protocols' harmonization in general. For the moment, we address this only with primitive image standardization strategies as described in <xref ref-type="fig" rid="F2">Figure 2</xref>. However, in clinical and scientific practice, we already found our application to be very robust across different data sources. Brain extraction with HD-BET also proved to be sound for patients from multiple institutions with different pathologies (Isensee et al., <xref rid="B13" ref-type="bibr">2019</xref>).</p>
    <p>These limitations are in fact some of the key motivations for our initiative. We strive to provide researchers with tools to build comprehensive databases which capture more of the data variability in magnetic resonance imaging. In the longterm this will enable the development of more precise algorithms. With BraTS Toolkit clinicians can actively contribute to this process.</p>
    <p>Through well-defined interfaces, the resulting output from our software can be integrated seamlessly with further downstream software to create new scientific and medical applications such as but not limited to, fully-automatic MR reporting<xref ref-type="fn" rid="fn0004"><sup>4</sup></xref> or tumor growth modeling (Ezhov et al., <xref rid="B8" ref-type="bibr">2019</xref>; Lipková et al., <xref rid="B21" ref-type="bibr">2019</xref>). Another promising future direction is to focus on integration with the local PACS to enable streamlined processing of imaging data directly from the radiologist's workplace.</p>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data Availability Statement</title>
    <p>The datasets generated for this study are available on request to the corresponding author.</p>
  </sec>
  <sec id="s6">
    <title>Author Contributions</title>
    <p>FK conceptualized the BraTS Toolkit, programmed the BraTS Preprocessor and contributed to paper writing. CB programmed and conceptualized the BraTS Fusionator and BraTS Segmentor and contributed to paper writing. DW, JL, IE, and JK conceptualized the BraTS Preprocessor and contributed to paper writing. GT conceptualized the BraTS Preprocessor software architecture and contributed to paper writing. CZ conceptualized the BraTS Preprocessor and provided feedback on the BraTS Fusionator. BW and BM conceptualized the BraTS Toolkit and contributed to paper writing.</p>
  </sec>
  <sec id="s7">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="fn0001">
      <p><sup>1</sup>As an example users who do not want to generate tumor segmentations on their own hardware using the BraTS Segmentor, can alternatively try our experimental web technology based solution nicknamed the Kraken: <ext-link ext-link-type="uri" xlink:href="https://neuronflow.github.io/kraken/">https://neuronflow.github.io/kraken/</ext-link>.</p>
    </fn>
    <fn id="fn0002">
      <p><sup>2</sup>For an up-to-date installation and user guide please refer to: <ext-link ext-link-type="uri" xlink:href="https://neuronflow.github.io/BraTS-Preprocessor/">https://neuronflow.github.io/BraTS-Preprocessor/</ext-link>.</p>
    </fn>
    <fn id="fn0003">
      <p>
        <sup>3</sup>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/BraTS/Instructions/blob/master/Repository_Links.md#brats-algorithmic-repository">https://github.com/BraTS/Instructions/blob/master/Repository_Links.md#brats-algorithmic-repository</ext-link>
      </p>
    </fn>
    <fn id="fn0004">
      <p><sup>4</sup>Our Kraken web service can be seen as an an exemplary prototype for this (for the moment it is not for clinical use, but for research and entertainment purposes only). The Kraken is able to send automatically generated segmentation and volumetry reports to the user's email address: <ext-link ext-link-type="uri" xlink:href="https://neuronflow.github.io/kraken/">https://neuronflow.github.io/kraken/</ext-link>.</p>
    </fn>
  </fn-group>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> BM, BW, and FK are supported through the SFB 824, subproject B12. Supported by Deutsche Forschungsgemeinschaft (DFG) through TUM International Graduate School of Science and Engineering (IGSSE), GSC 81. With the support of the Technical University of Munich–Institute for Advanced Study, funded by the German Excellence Initiative.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>B. B.</given-names></name><name><surname>Tustison</surname><given-names>N. J.</given-names></name><name><surname>Song</surname><given-names>G.</given-names></name><name><surname>Cook</surname><given-names>P. A.</given-names></name><name><surname>Klein</surname><given-names>A.</given-names></name><name><surname>Gee</surname><given-names>J. C.</given-names></name></person-group> (<year>2011</year>). <article-title>A reproducible evaluation of ants similarity metric performance in brain image registration</article-title>. <source>NeuroImage</source>
<volume>54</volume>, <fpage>2033</fpage>–<lpage>2044</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.09.025</pub-id><?supplied-pmid 20851191?><pub-id pub-id-type="pmid">20851191</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakas</surname><given-names>S.</given-names></name><name><surname>Akbari</surname><given-names>H.</given-names></name><name><surname>Sotiras</surname><given-names>A.</given-names></name><name><surname>Bilello</surname><given-names>M.</given-names></name><name><surname>Rozycki</surname><given-names>M.</given-names></name><name><surname>Kirby</surname><given-names>J. S.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Advancing the cancer genome atlas glioma mri collections with expert segmentation labels and radiomic features</article-title>. <source>Sci. Data</source><volume>4</volume>:<fpage>170117</fpage>. <pub-id pub-id-type="doi">10.1038/sdata.2017.117</pub-id><?supplied-pmid 28872634?><pub-id pub-id-type="pmid">28872634</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakas</surname><given-names>S.</given-names></name><name><surname>Reyes</surname><given-names>M.</given-names></name><name><surname>Jakab</surname><given-names>A.</given-names></name><name><surname>Bauer</surname><given-names>S.</given-names></name><name><surname>Rempfler</surname><given-names>M.</given-names></name><name><surname>Crimi</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge</article-title>. <source>arXiv preprint arXiv:1811.02629</source>.</mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheplygina</surname><given-names>V.</given-names></name><name><surname>de Bruijne</surname><given-names>M.</given-names></name><name><surname>Pluim</surname><given-names>J. P.</given-names></name></person-group> (<year>2019</year>). <article-title>Not-so-supervised: a survey of semi-supervised, multi-instance, and transfer learning in medical image analysis</article-title>. <source>Med. Image Analysis</source>
<volume>54</volume>, <fpage>280</fpage>–<lpage>296</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2019.03.009</pub-id><?supplied-pmid 30959445?><pub-id pub-id-type="pmid">30959445</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Çiçek</surname><given-names>Ö.</given-names></name><name><surname>Abdulkadir</surname><given-names>A.</given-names></name><name><surname>Lienkamp</surname><given-names>S. S.</given-names></name><name><surname>Brox</surname><given-names>T.</given-names></name><name><surname>Ronneberger</surname><given-names>O.</given-names></name></person-group> (<year>2016</year>). <article-title>“3d u-net: learning dense volumetric segmentation from sparse annotation,”</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Springer</publisher-loc>), <fpage>424</fpage>–<lpage>432</lpage>.</mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dorent</surname><given-names>R.</given-names></name><name><surname>Joutard</surname><given-names>S.</given-names></name><name><surname>Modat</surname><given-names>M.</given-names></name><name><surname>Ourselin</surname><given-names>S.</given-names></name><name><surname>Vercauteren</surname><given-names>T.</given-names></name></person-group> (<year>2019</year>). <article-title>“Hetero-modal variational encoder-decoder for joint modality completion and segmentation,”</article-title> in <source>Medical Image Computing and Computer Assisted Intervention–MICCAI 2019</source> eds D. Shen, T. Liu, T. M. Peters, L. H. Staib, C. Essert, S. Zhou, P.-T. Yap, and A. Khan (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>), <fpage>74</fpage>–<lpage>82</lpage>.</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dvorak</surname><given-names>P.</given-names></name><name><surname>Menze</surname><given-names>B.</given-names></name></person-group> (<year>2015</year>). <article-title>“Local structure prediction with convolutional neural networks for multimodal brain tumor segmentation”</article-title>, in <source>Medical Computer Vision: Algorithms for Big Data</source>, (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>), <fpage>59</fpage>–<lpage>71</lpage>.</mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ezhov</surname><given-names>I.</given-names></name><name><surname>Lipkova</surname><given-names>J.</given-names></name><name><surname>Shit</surname><given-names>S.</given-names></name><name><surname>Kofler</surname><given-names>F.</given-names></name><name><surname>Collomb</surname><given-names>N.</given-names></name><name><surname>Lemasson</surname><given-names>B.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>“Neural parameters estimation for brain tumor growth modeling,”</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>), <fpage>787</fpage>–<lpage>795</lpage>.</mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B.</given-names></name></person-group> (<year>2012</year>). <article-title>Freesurfer</article-title>. <source>Neuroimage</source>
<volume>62</volume>, <fpage>774</fpage>–<lpage>781</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id><?supplied-pmid 22248573?><pub-id pub-id-type="pmid">22248573</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Geremia</surname><given-names>E.</given-names></name><name><surname>Menze</surname><given-names>B. H.</given-names></name><name><surname>Ayache</surname><given-names>N.</given-names></name></person-group> (<year>2012</year>). <article-title>“Spatial decision forests for glioma segmentation in multi-channel mr images,”</article-title> in <source>MICCAI Challenge on Multimodal Brain Tumor Segmentation</source>, (<publisher-loc>Citeseer</publisher-loc>), <fpage>34</fpage>.</mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iglesias</surname><given-names>J. E.</given-names></name><name><surname>Liu</surname><given-names>C.-Y.</given-names></name><name><surname>Thompson</surname><given-names>P. M.</given-names></name><name><surname>Tu</surname><given-names>Z.</given-names></name></person-group> (<year>2011</year>). <article-title>Robust brain extraction across datasets and comparison with publicly available methods</article-title>. <source>IEEE Trans. Med. Imaging</source>, <volume>30</volume>, <fpage>1617</fpage>–<lpage>1634</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2011.2138152</pub-id><?supplied-pmid 21880566?><pub-id pub-id-type="pmid">21880566</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Isensee</surname><given-names>F.</given-names></name><name><surname>Kickingereder</surname><given-names>P.</given-names></name><name><surname>Bonekamp</surname><given-names>D.</given-names></name><name><surname>Bendszus</surname><given-names>M.</given-names></name><name><surname>Wick</surname><given-names>W.</given-names></name><name><surname>Schlemmer</surname><given-names>H.-P.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>“Brain tumor segmentation using large receptive field deep convolutional neural networks,”</article-title> in <source>Bildverarbeitung für die Medizin 2017</source> (<publisher-loc>Springer</publisher-loc>), <fpage>86</fpage>–<lpage>91</lpage>.</mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Isensee</surname><given-names>F.</given-names></name><name><surname>Schell</surname><given-names>M.</given-names></name><name><surname>Tursunova</surname><given-names>I.</given-names></name><name><surname>Brugnara</surname><given-names>G.</given-names></name><name><surname>Bonekamp</surname><given-names>D.</given-names></name><name><surname>Neuberger</surname><given-names>U.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Automated brain extraction of multisequence MRI using artificial neural networks</article-title>. <source>Human Brain Mapping</source>, (<publisher-name>Wiley Online Library</publisher-name>), <volume>40</volume>, <fpage>4952</fpage>–<lpage>964</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.24750</pub-id><?supplied-pmid 31403237?><pub-id pub-id-type="pmid">31403237</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jena</surname><given-names>R.</given-names></name><name><surname>Awate</surname><given-names>S. P.</given-names></name></person-group> (<year>2019</year>). <article-title>“A bayesian neural net to segment images with uncertainty estimates and good calibration,”</article-title> in <source>International Conference on Information Processing in Medical Imaging</source> (<publisher-loc>Springer</publisher-loc>), <fpage>3</fpage>–<lpage>15</lpage>.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jungo</surname><given-names>A.</given-names></name><name><surname>Meier</surname><given-names>R.</given-names></name><name><surname>Ermis</surname><given-names>E.</given-names></name><name><surname>Blatti-Moreno</surname><given-names>M.</given-names></name><name><surname>Herrmann</surname><given-names>E.</given-names></name><name><surname>Wiest</surname><given-names>R.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>“On the effect of inter-observer variability for a reliable estimation of uncertainty of medical image segmentation,”</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>), <fpage>682</fpage>–<lpage>690</lpage>.</mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kickingereder</surname><given-names>P.</given-names></name><name><surname>Isensee</surname><given-names>F.</given-names></name><name><surname>Tursunova</surname><given-names>I.</given-names></name><name><surname>Petersen</surname><given-names>J.</given-names></name><name><surname>Neuberger</surname><given-names>U.</given-names></name><name><surname>Bonekamp</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Automated quantitative tumour response assessment of mri in neuro-oncology with artificial neural networks: a multicentre, retrospective study</article-title>. <source>Lancet Oncol</source>. <volume>20</volume>, <fpage>728</fpage>–<lpage>740</lpage>. <pub-id pub-id-type="doi">10.1016/S1470-2045(19)30098-1</pub-id><?supplied-pmid 30952559?><pub-id pub-id-type="pmid">30952559</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kofler</surname><given-names>F.</given-names></name><name><surname>Paetzold</surname><given-names>J.</given-names></name><name><surname>Ezhov</surname><given-names>I.</given-names></name><name><surname>Shit</surname><given-names>S.</given-names></name><name><surname>Krahulec</surname><given-names>D.</given-names></name><name><surname>Kirschke</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>“A baseline for predicting glioblastoma patient survival time with classical statistical models and primitive features ignoring image information,”</article-title> in <source>International MICCAI Brainlesion Workshop</source> (<publisher-loc>Springer</publisher-loc>).</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langerak</surname><given-names>T. R.</given-names></name><name><surname>van der Heide</surname><given-names>U. A.</given-names></name><name><surname>Kotte</surname><given-names>A. N.</given-names></name><name><surname>Viergever</surname><given-names>M. A.</given-names></name><name><surname>Van Vulpen</surname><given-names>M.</given-names></name><name><surname>Pluim</surname><given-names>J. P.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>Label fusion in atlas-based segmentation using a selective and iterative method for performance level estimation (simple)</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>29</volume>, <fpage>2000</fpage>–<lpage>2008</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2010.2057442</pub-id><?supplied-pmid 20667809?><pub-id pub-id-type="pmid">20667809</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H.</given-names></name><name><surname>Paetzold</surname><given-names>J. C.</given-names></name><name><surname>Sekuboyina</surname><given-names>A.</given-names></name><name><surname>Kofler</surname><given-names>F.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Kirschke</surname><given-names>J. S.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>“Diamondgan: unified multi-modal generative adversarial networks for mri sequences synthesis,”</article-title> in <source>Medical Image Computing and Computer Assisted Intervention–MICCAI 2019</source>, eds D. Shen, T. Liu, T. M. Peters, L. H. Staib, C. Essert, S. Zhou, P.-T. Yap, and A. Khan (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>), <fpage>795</fpage>–<lpage>803</lpage>.</mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Morgan</surname><given-names>P. S.</given-names></name><name><surname>Ashburner</surname><given-names>J.</given-names></name><name><surname>Smith</surname><given-names>J.</given-names></name><name><surname>Rorden</surname><given-names>C.</given-names></name></person-group> (<year>2016</year>). <article-title>The first step for neuroimaging data analysis: DICOM to NIfTI conversion</article-title>. <source>J. Neurosci. Methods</source>
<volume>264</volume>, <fpage>47</fpage>–<lpage>56</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.03.001</pub-id><?supplied-pmid 26945974?><pub-id pub-id-type="pmid">26945974</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lipková</surname><given-names>J.</given-names></name><name><surname>Angelikopoulos</surname><given-names>P.</given-names></name><name><surname>Wu</surname><given-names>S.</given-names></name><name><surname>Alberts</surname><given-names>E.</given-names></name><name><surname>Wiestler</surname><given-names>B.</given-names></name><name><surname>Diehl</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Personalized radiotherapy design for glioblastoma: integrating mathematical tumor models, multimodal scans, and bayesian inference</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>38</volume>, <fpage>1875</fpage>–<lpage>1884</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2019.2902044</pub-id><?supplied-pmid 30835219?><pub-id pub-id-type="pmid">30835219</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menze</surname><given-names>B. H.</given-names></name><name><surname>Jakab</surname><given-names>A.</given-names></name><name><surname>Bauer</surname><given-names>S.</given-names></name><name><surname>Kalpathy-Cramer</surname><given-names>J.</given-names></name><name><surname>Farahani</surname><given-names>K.</given-names></name><name><surname>Kirby</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2015a</year>). <article-title>The multimodal brain tumor image segmentation benchmark (brats)</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>34</volume>, <fpage>1993</fpage>–<lpage>2024</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2014.2377694</pub-id><?supplied-pmid 25494501?><pub-id pub-id-type="pmid">25494501</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menze</surname><given-names>B. H.</given-names></name><name><surname>Van Leemput</surname><given-names>K.</given-names></name><name><surname>Lashkari</surname><given-names>D.</given-names></name><name><surname>Riklin-Raviv</surname><given-names>T.</given-names></name><name><surname>Geremia</surname><given-names>E.</given-names></name><name><surname>Alberts</surname><given-names>E.</given-names></name><etal/></person-group>. (<year>2015b</year>). <article-title>A generative probabilistic model and discriminative extensions for brain lesion segmentation—with application to tumor and stroke</article-title>. <source>IEEE Trans. Med. Imaging</source><volume>35</volume>, <fpage>933</fpage>–<lpage>946</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2015.2502596</pub-id><?supplied-pmid 26599702?><pub-id pub-id-type="pmid">26599702</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Menze</surname><given-names>B. H.</given-names></name><name><surname>Van Leemput</surname><given-names>K.</given-names></name><name><surname>Lashkari</surname><given-names>D.</given-names></name><name><surname>Weber</surname><given-names>M.-A.</given-names></name><name><surname>Ayache</surname><given-names>N.</given-names></name><name><surname>Golland</surname><given-names>P.</given-names></name></person-group> (<year>2010</year>). <article-title>“A generative model for brain tumor segmentation in multi-modal images,”</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Berlin; Heidelberg</publisher-loc>: <publisher-name>Springer Berlin Heidelberg</publisher-name>), <fpage>151</fpage>–<lpage>159</lpage>.<?supplied-pmid 20879310?></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merkel</surname><given-names>D.</given-names></name></person-group> (<year>2014</year>). <article-title>Docker: lightweight linux containers for consistent development and deployment</article-title>. <source>Linux J.</source>
<volume>2014</volume>:<fpage>2</fpage>.</mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Milletari</surname><given-names>F.</given-names></name><name><surname>Navab</surname><given-names>N.</given-names></name><name><surname>Ahmadi</surname><given-names>S.-A.</given-names></name></person-group> (<year>2016</year>). <article-title>“V-net: fully convolutional neural networks for volumetric medical image segmentation,”</article-title> in <source>2016 Fourth International Conference on 3D Vision (3DV)</source> (<publisher-loc>IEEE</publisher-loc>), <fpage>565</fpage>–<lpage>571</lpage>.</mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pawar</surname><given-names>K.</given-names></name><name><surname>Chen</surname><given-names>Z.</given-names></name><name><surname>Shah</surname><given-names>N. J.</given-names></name><name><surname>Egan</surname><given-names>G.</given-names></name></person-group> (<year>2018</year>). <article-title>“Residual encoder and convolutional decoder neural network for glioma segmentation,”</article-title> in <source>Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries</source>, eds A. Crimi, S. Bakas, H. Kuijf, B. Menze, and M. Reyes (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>), <fpage>263</fpage>–<lpage>273</lpage>.</mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prastawa</surname><given-names>M.</given-names></name><name><surname>Bullitt</surname><given-names>E.</given-names></name><name><surname>Moon</surname><given-names>N.</given-names></name><name><surname>Van Leemput</surname><given-names>K.</given-names></name><name><surname>Gerig</surname><given-names>G.</given-names></name></person-group> (<year>2003</year>). <article-title>Automatic brain tumor segmentation by subject specific modification of atlas priors1</article-title>. <source>Acad. Radiol</source>. <volume>10</volume>, <fpage>1341</fpage>–<lpage>1348</lpage>. <pub-id pub-id-type="doi">10.1016/S1076-6332(03)00506-3</pub-id><?supplied-pmid 14697002?><pub-id pub-id-type="pmid">14697002</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O.</given-names></name><name><surname>Fischer</surname><given-names>P.</given-names></name><name><surname>Brox</surname><given-names>Thomas.</given-names></name></person-group> (<year>2015</year>). <article-title>“U-net: Convolutional networks for biomedical image segmentation,”</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>, (<publisher-name>Springer</publisher-name>), <fpage>234</fpage>–<lpage>241</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sedlar</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>). <article-title>“Brain tumor segmentation using a multi-path cnn based method,”</article-title> in <source>Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries</source>, eds A. Crimi, S. Bakas, H. Kuijf, B. Menze, and M. Reyes (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>), <fpage>403</fpage>–<lpage>422</lpage>.</mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shah</surname><given-names>M. P.</given-names></name><name><surname>Merchant</surname><given-names>S.</given-names></name><name><surname>Awate</surname><given-names>S. P.</given-names></name></person-group> (<year>2018</year>). <article-title>“Ms-net: mixed-supervision fully-convolutional networks for full-resolution segmentation,”</article-title> in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Springer</publisher-loc>), <fpage>379</fpage>–<lpage>387</lpage>.</mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shboul</surname><given-names>Z. A.</given-names></name><name><surname>Alam</surname><given-names>M.</given-names></name><name><surname>Vidyaratne</surname><given-names>L.</given-names></name><name><surname>Pei</surname><given-names>L.</given-names></name><name><surname>Elbakary</surname><given-names>M. I.</given-names></name><name><surname>Iftekharuddin</surname><given-names>K. M.</given-names></name></person-group> (<year>2019</year>). <article-title>Feature-guided deep radiomics for glioblastoma patient survival prediction</article-title>. <source>Front. Neurosci</source>. <volume>13</volume>:<fpage>966</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2019.00966</pub-id><?supplied-pmid 31619949?><pub-id pub-id-type="pmid">31619949</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
