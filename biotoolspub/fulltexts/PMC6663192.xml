<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with OASIS Tables with MathML3 v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archive-oasis-article1-mathml3.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jpoasis-nisons2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Netw Neurosci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Netw Neurosci</journal-id>
    <journal-id journal-id-type="publisher-id">netn</journal-id>
    <journal-title-group>
      <journal-title>Network Neuroscience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2472-1751</issn>
    <publisher>
      <publisher-name>MIT Press</publisher-name>
      <publisher-loc>One Rogers Street, Cambridge, MA 02142-1209USAjournals-info@mit.edu</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6663192</article-id>
    <article-id pub-id-type="publisher-id">netn_a_00091</article-id>
    <article-id pub-id-type="doi">10.1162/netn_a_00091</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Articles</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Exact topological inference of the resting-state brain networks in twins</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Chung</surname>
          <given-names>Moo K.</given-names>
        </name>
        <xref ref-type="corresp" rid="cor1">*</xref>
        <aff id="aff1">University of Wisconsin, Madison, WI, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lee</surname>
          <given-names>Hyekyoung</given-names>
        </name>
        <aff id="aff2">Seoul National University, Seoul, Korea</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>DiChristofano</surname>
          <given-names>Alex</given-names>
        </name>
        <aff id="aff3">Washington University, St. Louis, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ombao</surname>
          <given-names>Hernando</given-names>
        </name>
        <aff id="aff4">King Abdullah University of Science and Technology, Thuwal, Saudi Arabia</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Solo</surname>
          <given-names>Victor</given-names>
        </name>
        <aff id="aff5">University of New South Wales, Sydney, Australia</aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <fn fn-type="COI-statement">
        <p>Competing Interests: The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor1">* Corresponding Author: <email xlink:href="mailto:mkchung@wisc.edu">mkchung@wisc.edu</email></corresp>
      <fn>
        <p>Handling Editor: Paul Expert</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>01</day>
      <month>7</month>
      <year>2019</year>
      <string-date>2019</string-date>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>3</volume>
    <issue>3</issue>
    <issue-title>Focus Feature: Topological Neuroscience</issue-title>
    <fpage>674</fpage>
    <lpage>694</lpage>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>8</month>
        <year>2018</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>4</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2019 Massachusetts Institute of Technology</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Massachusetts Institute of Technology</copyright-holder>
      <license license-type="open-access">
        <license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. For a full description of the license, please visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/legalcode">https://creativecommons.org/licenses/by/4.0/legalcode</ext-link>.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="netn-03-674.pdf"/>
    <abstract>
      <p>A cycle in a brain network is a subset of a connected component with redundant additional connections. If there are many cycles in a connected component, the connected component is more densely connected. Whereas the number of connected components represents the integration of the brain network, the number of cycles represents how strong the integration is. However, it is unclear how to perform statistical inference on the number of cycles in the brain network. In this study, we present a new statistical inference framework for determining the significance of the number of cycles through the Kolmogorov-Smirnov (KS) distance, which was recently introduced to measure the similarity between networks across different filtration values by using the zeroth Betti number. In this paper, we show how to extend the method to the first Betti number, which measures the number of cycles. The performance analysis was conducted using the random network simulations with ground truths. By using a twin imaging study, which provides biological ground truth, the methods are applied in determining if the number of cycles is a statistically significant heritable network feature in the resting-state functional connectivity in 217 twins obtained from the Human Connectome Project. The <monospace>MATLAB</monospace> codes as well as the connectivity matrices used in generating results are provided at <ext-link ext-link-type="uri" xlink:href="http://www.stat.wisc.edu/~mchung/TDA">http://www.stat.wisc.edu/∼mchung/TDA</ext-link>.</p>
    </abstract>
    <abstract abstract-type="author-summary">
      <title>Author Summary</title>
      <p>In this paper, we propose a new topological distance based on the Kolmogorov-Smirnov (KS) distance that is adapted for brain networks, and compare them against other topological network distances including the Gromov-Hausdorff (GH) distances. KS-distance is recently introduced to measure the similarity between networks across different filtration values by using the zeroth Betti number, which measures the number of connected components. In this paper, we show how to extend the method to the first Betti number, which measures the number of cycles. The performance analysis was conducted using random network simulations with ground truths. Using a twin imaging study, which provides biological ground truth (of network differences), we demonstrate that the KS distances on the zeroth and first Betti numbers have the ability to determine heritability.</p>
    </abstract>
    <kwd-group kwd-group-type="text">
      <title>Keywords</title>
      <kwd>Functional brain networks</kwd>
      <x xml:space="preserve">, </x>
      <kwd>Cycles</kwd>
      <x xml:space="preserve">, </x>
      <kwd>Betti numbers</kwd>
      <x xml:space="preserve">, </x>
      <kwd>Persistent homology</kwd>
      <x xml:space="preserve">, </x>
      <kwd>Twin imaging studies</kwd>
      <x xml:space="preserve">, </x>
      <kwd>Resting-state fMRI</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health</institution>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>UL1TR000427</award-id>
      </award-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health</institution>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>EB022856</award-id>
      </award-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>National Research Foundation of Korea</institution>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003725</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NRF-2016R1D1A1B03935463</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="6"/>
      <table-count count="3"/>
      <equation-count count="38"/>
      <ref-count count="93"/>
      <page-count count="21"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>citation</meta-name>
        <meta-value>Chung, M. K., Lee, H., DiChristofano, A., Ombao, H., &amp; Solo, V. (2019). Exact topological inference of the resting-state brain networks in twins. <italic>Network Neuroscience</italic>, <italic>3</italic>(3), 674–694. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/netn_a_00091">https://doi.org/10.1162/netn_a_00091</ext-link></meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec>
    <title>INTRODUCTION</title>
    <p>The modular structure and connected components are the fundamental topological features of a brain network. Brain networks with a higher number of connected components have many disjointed clusters, and the transfer of information will likely be impeded. Modular structures are often studied through the Q-modularity in graph theory (Meunier, Lambiotte, Fornito, Ersche, &amp; Bullmore, <xref rid="bib65" ref-type="bibr">2009</xref>; Newman, Barabasi, &amp; Watts, <xref rid="bib66" ref-type="bibr">2006</xref>) and the zeroth Betti number in <xref rid="def1" ref-type="def">persistent homology</xref> (Carlsson &amp; Memoli, <xref rid="bib15" ref-type="bibr">2008</xref>; Carlsson &amp; Mémoli, <xref rid="bib16" ref-type="bibr">2010</xref>; Chung, Vilalta-Gil, Lee, Rathouz, Lahey, &amp; Zald, <xref rid="bib29" ref-type="bibr">2017b</xref>; Chung, Luo, Leow, Adluru, Alexander, Richard, &amp; Goldsmith, <xref rid="bib28" ref-type="bibr">2018b</xref>; Lee, Chung, &amp; Lee, <xref rid="bib54" ref-type="bibr">2014</xref>).</p>
    <p>Persistent homology provides a coherent framework for obtaining higher order topological features beyond modular structures (Edelsbrunner &amp; Harer, <xref rid="bib33" ref-type="bibr">2008</xref>; Zomorodian &amp; Carlsson, <xref rid="bib93" ref-type="bibr">2005</xref>). A brain network can be treated as the 1-skeleton of a simplicial complex, where the 0-dimensional hole is the connected component, and the 1-dimensional hole is a cycle. The number of <italic>k</italic>-dimensional holes is called the <italic>k</italic>-th Betti number and denoted as <italic>β</italic><sub><italic>k</italic></sub> (Lee et al., <xref rid="bib54" ref-type="bibr">2014</xref>; Lee, Chung, Kang, Choi, Kim, &amp; Lee, <xref rid="bib55" ref-type="bibr">2018</xref>; Petri, Expert, Turkheimer, Carhart-Harris, Nutt, Hellyer, &amp; Vaccarino, <xref rid="bib69" ref-type="bibr">2014</xref>; Sizemore, Giusti, Kahn, Vettel, Betzel, &amp; Bassett, <xref rid="bib76" ref-type="bibr">2018</xref>). In this study, we will study higher order topological changes of brain networks using cycles. The cycle structure in networks is important for information propagation, redundancy, and feedback loops (Lind, Gonzalez, &amp; Herrmann, <xref rid="bib61" ref-type="bibr">2005</xref>). If a cycle exists in the network, information can be delivered using two different redundant paths and interpreted as redundant connections. Alternately, it can be viewed as diffusing the spread of information and creating information bottlenecks (Tarjan, <xref rid="bib85" ref-type="bibr">1972</xref>).</p>
    <p>Although cycles in a network have been widely studied in graph theory, especially in path analysis, they are rarely used in brain network analysis (Sporns, <xref rid="bib81" ref-type="bibr">2003</xref>; Sporns, Tononi, &amp; Edelman, <xref rid="bib82" ref-type="bibr">2000</xref>). Existing graph analysis packages such as Brain Connectivity (<ext-link ext-link-type="uri" xlink:href="http://sites.google.com/site/bctnet">http://sites.google.com/site/bctnet</ext-link>) do not provide any tools related to cycles. Traditionally, cycles are often computed using the brute-force depth-first search algorithm (Tarjan, <xref rid="bib85" ref-type="bibr">1972</xref>). In standard graph theoretic approaches, graph theory features are measured mainly by determining the difference in graph theory features such as assortativity, betweenness centrality, small-worldness, and network homogeneity (Bullmore &amp; Sporns, <xref rid="bib11" ref-type="bibr">2009</xref>; Rubinov &amp; Sporns, <xref rid="bib74" ref-type="bibr">2010</xref>; Rubinov, Knock, Stam, Micheloyannis, Harris, Williams, &amp; Breakspear, <xref rid="bib73" ref-type="bibr">2009</xref>; Uddin, Kelly, Biswal, Margulies, Shehzad, Shaw, Ghaffari, Rotrosen, Adler, Castellanos, &amp; Milham, <xref rid="bib89" ref-type="bibr">2008</xref>). Comparison of graph theory features appears to reveal changes of structural or functional connectivity associated with different clinical populations (Rubinov &amp; Sporns, <xref rid="bib74" ref-type="bibr">2010</xref>). Since weighted brain networks are difficult to interpret and visualize, they are often turned into binary networks by thresholding edge weights (He, Chen, &amp; Evans, <xref rid="bib44" ref-type="bibr">2008</xref>; Wijk, Stam, &amp; Daffertshofer, <xref rid="bib91" ref-type="bibr">2010</xref>). However, the thresholds for the edge weights are often chosen arbitrarily and produce results that could alter the network topology and thus make comparisons difficult. To obtain the proper optimal threshold where comparisons can be made, the multiple comparison correction over every possible edge has been proposed (Rubinov et al., <xref rid="bib73" ref-type="bibr">2009</xref>; Wijk et al., <xref rid="bib91" ref-type="bibr">2010</xref>). However, the resulting binary graph is extremely sensitive depending on the chosen <italic>p</italic> value or threshold value. Others tried to control the sparsity of edges in the network in obtaining the binary network (Achard &amp; Bullmore, <xref rid="bib1" ref-type="bibr">2007</xref>; Bassett, <xref rid="bib5" ref-type="bibr">2006</xref>; He et al., <xref rid="bib44" ref-type="bibr">2008</xref>; Lee, Kang, Chung, Kim, &amp; Lee, <xref rid="bib58" ref-type="bibr">2012</xref>; Wijk et al., <xref rid="bib91" ref-type="bibr">2010</xref>). However, one encounters the problem of thresholding sparse parameters. Thus existing methods for binarizing weighted networks cannot escape the inherent problem of arbitrary thresholding.</p>
    <p>There is currently no widely accepted criteria for thresholding networks. Instead of trying to find an optimal threshold that gives rise to a single network that may not be suitable for comparing clinical populations, cognitive conditions, or different studies, <italic>why not use each network produced from every threshold?</italic> Motivated by this simple question, a new multiscale hierarchical network modeling framework based on persistent homology has been proposed (Cassidy, Rae, &amp; Solo, <xref rid="bib18" ref-type="bibr">2015</xref>; Chung, Hanson, Lee, Adluru, Alexander, Davidson, &amp; Pollak, <xref rid="bib24" ref-type="bibr">2013</xref>; Giusti, Pastalkova, Curto, &amp; Itskov, <xref rid="bib41" ref-type="bibr">2015</xref>; Lee, Chung, Kang, Kim, &amp; Lee, <xref rid="bib56" ref-type="bibr">2011a</xref>, <xref rid="bib57" ref-type="bibr">2011b</xref>; Lee et al., <xref rid="bib58" ref-type="bibr">2012</xref>; Petri, Scolamiero, Donato, &amp; Vaccarino, <xref rid="bib70" ref-type="bibr">2013</xref>; Petri et al., <xref rid="bib69" ref-type="bibr">2014</xref>; Sizemore, Giusti, &amp; Bassett, <xref rid="bib75" ref-type="bibr">2016</xref>; Sizemore et al., <xref rid="bib76" ref-type="bibr">2018</xref>; Stolz, Harrington, &amp; Porter, <xref rid="bib83" ref-type="bibr">2017</xref>). Persistent homology, a branch of computational topology (Carlsson &amp; Memoli, <xref rid="bib15" ref-type="bibr">2008</xref>; Edelsbrunner &amp; Harer, <xref rid="bib33" ref-type="bibr">2008</xref>; Edelsbrunner, Letscher, &amp; Zomorodian, <xref rid="bib34" ref-type="bibr">2000</xref>), provides a more coherent mathematical framework for measuring network distance than the conventional method of simply taking the difference between graph theoretic features or the norm of the connectivity matrices. Instead of looking at networks at a fixed scale, as is usually done in many standard brain network analysis, persistent homology observes the changes of topological features of the network over multiple resolutions and scales (Edelsbrunner &amp; Harer, <xref rid="bib33" ref-type="bibr">2008</xref>; Horak, Maletić, &amp; Rajković, <xref rid="bib45" ref-type="bibr">2009</xref>; Zomorodian &amp; Carlsson, <xref rid="bib93" ref-type="bibr">2005</xref>). In doing so, it reveals the most persistent topological features that are robust under noise perturbations. This robustness in performance under different scales is needed for most network distances that are parameter and scale dependent.</p>
    <p>In persistent homology–based brain network analysis, instead of analyzing networks at one fixed threshold that may not be optimal, we build the collection of nested networks over every possible threshold by using the <xref rid="def2" ref-type="def"><italic>graph filtration</italic></xref>, a persistent homological construct (Chung et al., <xref rid="bib24" ref-type="bibr">2013</xref>; Lee et al., <xref rid="bib56" ref-type="bibr">2011a</xref>, <xref rid="bib58" ref-type="bibr">2012</xref>). The graph filtration is a threshold-free framework for analyzing a family of graphs but requires hierarchically building specific nested subgraph structures. The graph filtration shares similarities to the existing multithresholding or multiresolution network models that use many different arbitrary thresholds or scales (Achard, Salvador, Whitcher, Suckling, &amp; Bullmore, <xref rid="bib2" ref-type="bibr">2006</xref>; He et al., <xref rid="bib44" ref-type="bibr">2008</xref>; Kim, Adluru, Chung, Okonkwo, Johnson, Bendlin, &amp; Singh, <xref rid="bib50" ref-type="bibr">2015</xref>; Lee et al., <xref rid="bib58" ref-type="bibr">2012</xref>; Supekar, Menon, Rubin, Musen, &amp; Greicius, <xref rid="bib84" ref-type="bibr">2008</xref>). Such approaches are mainly used to visually display the dynamic pattern of how graph theoretic features change over different thresholds, and the pattern of change is rarely quantified. Persistent homology can be used to quantify such dynamic patterns in a more coherent mathematical framework. Recently, various persistent homological network approaches have been proposed. In Giusti et al. (<xref rid="bib41" ref-type="bibr">2015</xref>) and Sizemore et al. (<xref rid="bib75" ref-type="bibr">2016</xref>, <xref rid="bib76" ref-type="bibr">2018</xref>), graph filtration was developed on cliques. In Petri et al. (<xref rid="bib70" ref-type="bibr">2013</xref>), weighted clique rank homology was developed. In Petri et al. (<xref rid="bib69" ref-type="bibr">2014</xref>), the concept of homological scaffolds was developed and applied to the resting-state fMRI.</p>
    <p>In persistent homology, there are various metrics that have been proposed to measure similarity and distances, including the bottleneck, Gromov-Hausdorff (GH), and Wasserstein distances (Chazal, Cohen-Steiner, Guibas, Mémoli, &amp; Oudot, <xref rid="bib19" ref-type="bibr">2009</xref>; Kerber, Morozov, &amp; Nigmetov, <xref rid="bib48" ref-type="bibr">2017</xref>; Tuzhilin, <xref rid="bib87" ref-type="bibr">2016</xref>), the complex vector method (Di Fabio &amp; Ferri, <xref rid="bib32" ref-type="bibr">2015</xref>), and the persistence kernel (Ibanez-Marcelo, Campioni, Manzoni, Santarcangelo, &amp; Petri, <xref rid="bib46" ref-type="bibr">2018a</xref>; Ibanez-Marcelo, Campioni, Phinyomark, Petri, &amp; Santarcangelo, <xref rid="bib47" ref-type="bibr">2018b</xref>; Kusano, Hiraoka, &amp; Fukumizu, <xref rid="bib53" ref-type="bibr">2016</xref>). Among them, the bottleneck and GH distances are possibly the two most popular distances that were originally used to measure distance between two <xref rid="def3" ref-type="def">metric spaces</xref> (Tuzhilin, <xref rid="bib87" ref-type="bibr">2016</xref>). They were later adapted to measure distances in persistent homology, dendrograms (Carlsson &amp; Memoli, <xref rid="bib15" ref-type="bibr">2008</xref>; Carlsson &amp; Mémoli, <xref rid="bib16" ref-type="bibr">2010</xref>; Chazal et al., <xref rid="bib19" ref-type="bibr">2009</xref>), and brain networks (Lee et al., <xref rid="bib57" ref-type="bibr">2011b</xref>, <xref rid="bib58" ref-type="bibr">2012</xref>). The probability distributions of bottleneck and GH-distances are unknown. Thus, the statistical inference on them can only be done through resampling techniques such as permutations (Lee et al., <xref rid="bib58" ref-type="bibr">2012</xref>; Lee, Kang, Chung, Lim, Kim, &amp; Lee, <xref rid="bib59" ref-type="bibr">2017</xref>), which often cause serious computational bottlenecks for large-scale networks.</p>
    <p>To bypass the computational bottleneck associated with resampling large-scale networks, the Kolmogorov-Smirnov (KS) distance was introduced (Chung et al., <xref rid="bib24" ref-type="bibr">2013</xref>, 1; Lee et al., <xref rid="bib59" ref-type="bibr">2017</xref>). The advantage of using KS-distance is that its gives results that are easier to interpret than those obtained from less intuitive distances from persistent homology. Furthermore because of its simplicity in construction, it is possible to determine its probability distribution exactly without resampling (Chung et al., <xref rid="bib29" ref-type="bibr">2017b</xref>). However, the KS-distance has been only applied to the number of connected components <italic>β</italic><sub>0</sub>, and it is unclear how to apply to the number of cycles <italic>β</italic><sub>1</sub> in graphs and networks. In this paper, for the first time, we show how to extend the KS-distance by performing statistical inference on <italic>β</italic><sub>1</sub>. This is achieved by establishing the monotonic property of the number of cycles over graph filtration. The monotonicity is then used in constructing the KS-distance for topologically differentiating two networks. Subsequently, the method is applied to the large-scale resting-state twin fMRI study in determining the heritability of the number of cycles.</p>
  </sec>
  <sec>
    <title>CORRELATION BRAIN NETWORK</title>
    <p>The edge weight, which measures the strength of a connection, is usually given by a similarity measure between the observed data on the nodes in brain networks. Various similarity measures have been proposed. The correlation or mutual information between measurements for the biological or metabolic network and the frequency of contact between actors for the social network have been used as edge weights (Bassett, Meyer-Lindenberg, Achard, Duke, &amp; Bullmore, <xref rid="bib6" ref-type="bibr">2006</xref>; Bien &amp; Tibshirani, <xref rid="bib7" ref-type="bibr">2011</xref>; Li, Liu, Li, Qin, Li, Yu, &amp; Jiang, <xref rid="bib60" ref-type="bibr">2009</xref>; McIntosh &amp; Gonzalez-Lima, <xref rid="bib64" ref-type="bibr">1994</xref>; Newman &amp; Watts, <xref rid="bib67" ref-type="bibr">1999</xref>; Song, Havlin, &amp; Makse, <xref rid="bib80" ref-type="bibr">2005</xref>). In particular, the Pearson correlation has been most widely used as edge weights in functional brain network modeling.</p>
    <p>Consider a weighted graph with node set <italic>V</italic> = {1, …, <italic>p</italic>} and edge weights <italic>w</italic> = (<italic>w</italic><sub><italic>ij</italic></sub>) between nodes <italic>i</italic> and <italic>j</italic>. Let <bold>x</bold><sub><italic>j</italic></sub> = (<italic>x</italic><sub>1<italic>j</italic></sub>, ⋯, <italic>x</italic><sub><italic>nj</italic></sub>)<sup>⊤</sup> ∈ ℝ<sup><italic>n</italic></sup> be <italic>n</italic> × 1 measurement vector on node <italic>j</italic>. Let us center and normalize data <bold>x</bold><sub><italic>j</italic></sub> such that<disp-formula><mml:math id="m1"><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>∥</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mstyle displaystyle="true"><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:math></disp-formula>Then we can show that <italic>ρ</italic><sub><italic>ij</italic></sub> = <inline-formula><mml:math id="m2"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula><bold>x</bold><sub><italic>j</italic></sub> is the Pearson correlation between <bold>x</bold><sub><italic>i</italic></sub> and <bold>x</bold><sub><italic>j</italic></sub> (Chung, Hanson, Ye, Davidson, &amp; Pollak, <xref rid="bib25" ref-type="bibr">2015</xref>). Note that correlations are invariant under scale and translations. Naturally, we are interested in using correlations or their simple functions such as<disp-formula><mml:math id="m3"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="1em"/><mml:mtext>or</mml:mtext><mml:mspace width="1em"/><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula>as edge weights. Among possible functions of correlations,<disp-formula id="E1"><mml:math id="m4"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><label>(1)</label></disp-formula>satisfies triangle inequality <italic>w</italic><sub><italic>ij</italic></sub> ≤ <italic>w</italic><sub><italic>ik</italic></sub> + <italic>w</italic><sub><italic>kj</italic></sub> and other metric properties (Chung, Lee, Solo, Davidson, &amp; Pollak, <xref rid="bib26" ref-type="bibr">2017a</xref>). Having metric distances facilitates more mathematically coherent interpretation of brain networks and offers many nice mathematical properties. With such edge weight <italic>w</italic>, 𝒳 = (<italic>V</italic>, <italic>w</italic>) forms a metric space. In the simulation studies in this paper, <xref ref-type="disp-formula" rid="E1">Equation 1</xref> is used as the edge weights.</p>
  </sec>
  <sec>
    <title>GRAPH FILTRATION</title>
    <p>All topological network distances that will be introduced in later sections are based on filtrations on graphs by thresholding edge weights.</p>
    <p>
      <statement id="dfn1">
        <label>Definition 1</label>
        <p>Given weighted network 𝒳 = (<italic>V</italic>, <italic>w</italic>) with positive edge weight <italic>w</italic> = (<italic>w</italic><sub><italic>ij</italic></sub>), the binary network 𝒳<sub><italic>ϵ</italic></sub> = (<italic>V</italic>, <italic>w</italic><sub><italic>ϵ</italic></sub>) is a graph consisting of the node set <italic>V</italic> and the binary edge weights <italic>w</italic><sub><italic>ϵ</italic></sub> given by<disp-formula><mml:math id="m5"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>ε</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mfenced open="{" close=""><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mtext>if</mml:mtext><mml:mspace width="2pt"/><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>ε</mml:mi><mml:mo>;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mtext>otherwise</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
      </statement>
    </p>
    <p>Any edge weight less than or equal to <italic>ϵ</italic> is made into zero while edge weights larger than <italic>ϵ</italic> are made into one. Lee et al. (<xref rid="bib57" ref-type="bibr">2011b</xref>, 1) defines the binary graphs by thresholding above, that is, <italic>w</italic><sub><italic>ij</italic>,<italic>ϵ</italic></sub> = 1 if <italic>w</italic><sub><italic>ij</italic></sub> &lt;= <italic>ϵ</italic>, which is consistent with the definition of the Rips filtration. However, in brain imaging, the higher value of <italic>w</italic><sub><italic>ij</italic></sub> indicates stronger connectivity. Thus, we are thresholding below and leave out stronger connections (Chung et al., <xref rid="bib24" ref-type="bibr">2013</xref>, 1).</p>
    <p>Note <italic>w</italic><sub><italic>ϵ</italic></sub> is the adjacency matrix of 𝒳<sub><italic>ϵ</italic></sub>, which is a simplicial complex consisting of 0-simplices (nodes) and 1-simplices (edges) (Ghrist, <xref rid="bib39" ref-type="bibr">2008</xref>). By increasing the filtration value <italic>ϵ</italic>, we are deleting more edges, so the size of the edge set decreases. Thus, the binary network satisfies the monotonic subset property<disp-formula><mml:math id="m6"><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⊃</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⊃</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⊃</mml:mo><mml:mo>⋯</mml:mo></mml:math></disp-formula>for any <italic>ϵ</italic><sub>0</sub> ≤ <italic>ϵ</italic><sub>1</sub> ≤ <italic>ϵ</italic><sub>2</sub> ⋯. Equivalently, we also have<disp-formula><mml:math id="m7"><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>⊂</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>⊂</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>⊂</mml:mo><mml:mo>⋯</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>The sequence of such nested multiscale graphs is defined as the graph filtration (Lee et al., <xref rid="bib57" ref-type="bibr">2011b</xref>, 1). Note that 𝒳<sub>0</sub> is the complete graph and 𝒳<sub>∞</sub> is the node set <italic>V</italic>. For a graph with <italic>p</italic> nodes, the maximum number of edges is (<italic>p</italic><sup>2</sup> − <italic>p</italic>)/2, which is obtained in a complete graph. If we order the edge weights in increasing order, we have the sorted edge weights:<disp-formula><mml:math id="m8"><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:munder><mml:mrow><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mo>⋯</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>max</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic>q</italic> ≤ (<italic>p</italic><sup>2</sup> − <italic>p</italic>)/2. The subscript <sub>( )</sub> denotes the order statistic. Hence, we simply construct the graph filtration at the edge weights<disp-formula id="E2"><mml:math id="m9"><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>⊃</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⊃</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>⊃</mml:mo><mml:mo>⋯</mml:mo><mml:mo>⊃</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math><label>(2)</label></disp-formula></p>
    <p>The condition of having unique edge weights is not restrictive in practice. Assuming edge weights to follow some continuous distribution, the probability of any two edges being equal is zero. The finiteness and uniqueness of the filtration levels over finite graphs are intuitively clear by themselves and are implicitly assumed in software packages such as javaPlex (Adams, Tausz, &amp; Vejdemo-Johansson, <xref rid="bib3" ref-type="bibr">2014</xref>).</p>
  </sec>
  <sec>
    <title>BETTI NUMBERS</title>
    <p>In persistent homology, the <italic>k</italic>-th Betti number is often referred to as the number of <italic>k</italic>-dimensional holes (Lee et al., <xref rid="bib54" ref-type="bibr">2014</xref>, 1; Petri et al., <xref rid="bib69" ref-type="bibr">2014</xref>; Sizemore et al., <xref rid="bib76" ref-type="bibr">2018</xref>). In network setting, the 0-th Betti number is the number of connected components and the 1st Betti number is the number of cycles. During graph filtration, we can show that <italic>β</italic><sub>0</sub> and <italic>β</italic><sub>1</sub> monotonically change. Although it is not true in general (Bobrowski &amp; Kahle, <xref rid="bib9" ref-type="bibr">2014</xref>), on the graph filtration <xref ref-type="disp-formula" rid="E2">(2)</xref>, <italic>β</italic><sub>0</sub> and <italic>β</italic><sub>1</sub> numbers have very stable monotonic increases and decreases respectively.</p>
    <p>
      <statement id="thm1">
        <label>Theorem 1</label>
        <p>In a graph, Betti numbers <italic>β</italic><sub>0</sub> and <italic>β</italic><sub>1</sub> are monotone over graph filtration on edge weights.</p>
      </statement>
    </p>
    <p>
      <statement>
        <p><italic>Proof</italic>. Under graph filtration <xref ref-type="disp-formula" rid="E2">(2)</xref>, the edges are deleted one at a time. Since an edge has only two end points, the deletion of the edge disconnects the graph into at most two. Thus, the number of connected components (<italic>β</italic><sub>0</sub>) always increases, and the increase is at most by one. The Euler characteristic <italic>χ</italic> of the graph is given by (Adler, Bobrowski, Borman, Subag, &amp; Weinberger, <xref rid="bib4" ref-type="bibr">2010</xref>)<disp-formula><mml:math id="m10"><mml:mi>χ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic>p</italic> and <italic>q</italic> are the number of nodes and edges respectively. Thus,<disp-formula><mml:math id="m11"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>q</mml:mi><mml:mo>.</mml:mo></mml:math></disp-formula>Note <italic>p</italic> is fixed over the filtration but <italic>q</italic> is decreasing by one while <italic>β</italic><sub>0</sub> increases at most by one. Hence, <italic>β</italic><sub>1</sub> always decreases and the decrease is at most by one.</p>
      </statement>
    </p>
    <p><xref rid="thm1" ref-type="statement">Theorem 1</xref> is related to the incremental Betti number computation over a simplical complex (Boissonnat &amp; Teillaud, <xref rid="bib10" ref-type="bibr">2006</xref>). Once we compute <italic>β</italic><sub>0</sub> number, <italic>β</italic><sub>1</sub> number is simply given by <italic>β</italic><sub>0</sub> − <italic>p</italic> + <italic>q</italic> without additional computation. For the computation of <italic>β</italic><sub>0</sub>, it is not necessary to perform graph filtration for infinitely many possible filtration values. The maximum possible number of filtration level needed for computing <italic>β</italic><sub>0</sub> is one plus the number of unique edge weights. In the case of trees, <italic>β</italic><sub>0</sub> computation is exactly given.</p>
    <p>
      <statement id="thm2">
        <label>Theorem 2</label>
        <p>For tree 𝒯 = (<italic>V</italic>, <italic>w</italic>) with <italic>p</italic> ≥ 2 nodes and unique positive edge weights<disp-formula><mml:math id="m12"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mo>⋯</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula>the zeroth Betti number <italic>β</italic><sub>0</sub> over graph filtration <xref ref-type="disp-formula" rid="E2">(2)</xref> is given by<disp-formula><mml:math id="m13"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      </statement>
    </p>
    <p>The proof is given in Chung et al. (<xref rid="bib25" ref-type="bibr">2015</xref>). Note a tree with <italic>p</italic> nodes has <italic>p</italic> − 1 edges. For a graph that is not possible, it may not be possible to analytically represent <italic>β</italic><sub>0</sub> over a filtration like <xref ref-type="statement" rid="thm2">Theorem 2</xref>. In general, <italic>β</italic><sub>0</sub> can be numerically computed using the single linkage dendrogram (SLD) (Lee et al., <xref rid="bib58" ref-type="bibr">2012</xref>), the Dulmage-Mendelsohn decomposition (Chung, Adluru, Dalton, Alexander, &amp; Davidson, <xref rid="bib22" ref-type="bibr">2011</xref>; Pothen &amp; Fan, <xref rid="bib72" ref-type="bibr">1990</xref>), or the simplical complex method (Carlsson &amp; Memoli, <xref rid="bib15" ref-type="bibr">2008</xref>; de Silva &amp; Ghrist, <xref rid="bib31" ref-type="bibr">2007</xref>; Edelsbrunner, Letscher, &amp; Zomorodian, <xref rid="bib35" ref-type="bibr">2002</xref>). In this study, we computed <italic>β</italic><sub>0</sub> over filtration by using the Dulmage-Mendelsohn decomposition.</p>
  </sec>
  <sec>
    <title>SINGLE LINKAGE CLUSTERING</title>
    <p>The <italic>β</italic><sub>0</sub> computation is related to single linkage clustering and dendrogram construction (Carlsson, <xref rid="bib12" ref-type="bibr">2009</xref>; Carlsson, De Silva, &amp; Morozov, <xref rid="bib13" ref-type="bibr">2009</xref>; Carlsson, Singh, &amp; Zomorodian, <xref rid="bib17" ref-type="bibr">2009b</xref>; Chowdhury &amp; Mémoli, <xref rid="bib21" ref-type="bibr">2016</xref>; Khalid, Kim, Chung, Ye, &amp; Jeon, <xref rid="bib49" ref-type="bibr">2014</xref>). In single linkage clustering, the single linkage distance (SLD) <italic>s</italic><sub><italic>ij</italic></sub> between the closest nodes in the two disjoint connected components <bold>R</bold><sub>1</sub> and <bold>R</bold><sub>2</sub> is given by<disp-formula><mml:math id="m14"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math></disp-formula>In this study, the square-root of 1 correlation is used as edge weight <italic>w</italic><sub><italic>kl</italic></sub>.</p>
    <p>Every edge connecting a node in <bold>R</bold><sub>1</sub> to a node in <bold>R</bold><sub>2</sub> has the same SLD. The SLD is then used to construct the single linkage matrix (SLM) <italic>S</italic> = (<italic>s</italic><sub><italic>ij</italic></sub>) (<xref ref-type="fig" rid="F1">Figure 1</xref>). SLM shows how connected components are merged locally and can be used in constructing a dendrogram over filtration. If the single linkage distance <italic>s</italic><sub><italic>ij</italic></sub> is larger than the current filtration value <italic>ϵ</italic><sub><italic>k</italic></sub> but smaller than the next filtration value <italic>ϵ</italic><sub><italic>k</italic>+1</sub>, that is, <italic>ϵ</italic><sub><italic>k</italic></sub> ≤ <italic>s</italic><sub><italic>ij</italic></sub> &lt; <italic>ϵ</italic><sub><italic>k</italic>+1</sub>. Then components <bold>R</bold><sub>1</sub> and <bold>R</bold><sub>2</sub> will be connected at the next filtration value <italic>ϵ</italic><sub><italic>k</italic>+1</sub>. The sequence of how components are merged during the graph filtration is identical to the sequence of the merging in the dendrogram construction (Lee et al., <xref rid="bib58" ref-type="bibr">2012</xref>). By tracing how each of the connected components are merged, we can compute <italic>β</italic><sub>0</sub>. In the single linkage clustering, instead of deleting edges, we are connecting nodes over increasing edge weights.</p>
    <fig id="F1" orientation="portrait" position="float">
      <label><bold>Figure 1.</bold> </label>
      <caption>
        <p>(A) Toy network, (B) its dendrogram, (C) the distance matrix <italic>w</italic> based on Euclidean distance, and (D) the single linkage matrix <italic>S</italic>. In the case of dendrogram construction, the graph filtration is done by connecting nodes over increasing edge weights.</p>
      </caption>
      <graphic xlink:href="netn-03-674-g001"/>
    </fig>
    <p>SLM is an <italic>ultrametric</italic>, which is a metric space satisfying the stronger triangle inequality <italic>s</italic><sub><italic>ij</italic></sub> ≤ max(<italic>s</italic><sub><italic>ik</italic></sub>, <italic>s</italic><sub><italic>kj</italic></sub>) (Carlsson &amp; Mémoli, <xref rid="bib16" ref-type="bibr">2010</xref>). Thus the dendrogram can be represented as an ultrametric space 𝒟 = (<italic>V</italic>, <italic>S</italic>), which is again a metric space. In persistent homology, the Gromov-Hausdorff (GH) distance has been mainly used in quantifying the dendrogram shape differences (Carlsson &amp; Mémoli, <xref rid="bib16" ref-type="bibr">2010</xref>; Chung et al., <xref rid="bib26" ref-type="bibr">2017a</xref>; Lee et al., <xref rid="bib57" ref-type="bibr">2011b</xref>, 1). The GH-distance between dendrograms 𝒟<sup>1</sup> = (<italic>V</italic>, <italic>S</italic><sup>1</sup>) and 𝒟<sup>2</sup> = (<italic>V</italic>, <italic>S</italic><sup>2</sup>) with SLM <italic>S</italic><sup>1</sup> = (<inline-formula><mml:math id="m15"><mml:msubsup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>) and <italic>S</italic><sup>2</sup> = (<inline-formula><mml:math id="m16"><mml:msubsup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>) is given by<disp-formula><mml:math id="m17"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:munder><mml:mrow><mml:mo>max</mml:mo></mml:mrow><mml:mrow><mml:mo>∀</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>For the statistical inference on GH-distance, resampling techniques such as jackknife or <xref rid="def4" ref-type="def">permutation tests</xref> are often used ((Lee et al., <xref rid="bib58" ref-type="bibr">2012</xref>), 1). In this study, we will use the permutation test.</p>
  </sec>
  <sec>
    <title>BOTTLENECK DISTANCE</title>
    <p>The bottleneck distance is perhaps the most often used distance in persistent homology, although it is <italic>rarely</italic> used for brain networks. In persistent homology, the topology of underlying data can be represented by the birth and death of topological features, such as the number of connected components or cycles (Carlsson, Ishkhanov, De Silva, &amp; Zomorodian, <xref rid="bib14" ref-type="bibr">2008</xref>). During the filtration, these topological features appear and disappear. If a topological feature appears at the threshold <italic>ξ</italic> and disappears at <italic>τ</italic>, it can be encoded into a point, (<italic>ξ</italic>, <italic>τ</italic>) (0 ≤ <italic>ξ</italic> ≤ <italic>τ</italic> &lt; ∞) in ℝ<sup>2</sup>. If <italic>m</italic> number of connected components or cycles appear during the filtration of a network 𝒳 = (<italic>V</italic>, <italic>w</italic>), the homology group can be represented by a point set<disp-formula><mml:math id="m18"><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfenced open="{" close="}"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula>This scatter plot is called a persistence diagram (PD) (Cohen-Steiner, Edelsbrunner, &amp; Harer, <xref rid="bib30" ref-type="bibr">2007</xref>).</p>
    <p>Given two networks 𝒳<sup>1</sup> = (<italic>V</italic><sup>1</sup>, <italic>w</italic><sup>1</sup>) with <italic>m</italic> features and 𝒳<sup>2</sup> = (<italic>V</italic><sup>2</sup>, <italic>w</italic><sup>2</sup>) with <italic>n</italic> features, PDs<disp-formula><mml:math id="m19"><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfenced open="{" close="}"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced></mml:math></disp-formula>and<disp-formula><mml:math id="m20"><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfenced open="{" close="}"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced></mml:math></disp-formula>are obtained through the filtration (Lee et al., <xref rid="bib58" ref-type="bibr">2012</xref>). The bottleneck distance between the networks is defined as the bottleneck distance of the corresponding PDs (Cohen-Steiner et al., <xref rid="bib30" ref-type="bibr">2007</xref>):<disp-formula id="E3"><mml:math id="m21"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>inf</mml:mo></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mo>sup</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:munder><mml:mo>∥</mml:mo><mml:msubsup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mo>∞</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><label>(3)</label></disp-formula>where <inline-formula><mml:math id="m22"><mml:msubsup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = (<inline-formula><mml:math id="m23"><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="m24"><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>) ∈ 𝒫(𝒳<sup>1</sup>) and <italic>γ</italic> is a bijection from 𝒫(𝒳<sup>1</sup>) to 𝒫(𝒳<sup>2</sup>). The infimum is taken over all possible bijections. If <inline-formula><mml:math id="m25"><mml:msubsup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = (<inline-formula><mml:math id="m26"><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="m27"><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>) = <italic>γ</italic>(<inline-formula><mml:math id="m28"><mml:msubsup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>) for some <italic>i</italic> and <italic>j</italic>, <italic>L</italic><sub>∞</sub>-norm is given by<disp-formula><mml:math id="m29"><mml:mo>∥</mml:mo><mml:msubsup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mo>∞</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>Note <xref ref-type="disp-formula" rid="E3">Equation 3</xref> assumes <italic>m</italic> = <italic>n</italic> such that the bijection <italic>γ</italic> exists. Suppose two networks share the same node set, that is, <italic>V</italic><sup>1</sup> = <italic>V</italic><sup>2</sup>, with <italic>p</italic> nodes and the same number of <italic>q</italic> unique edge weights. If the graph filtration is performed on two networks, the number of connected components and cycles that appear and disappear during the filtration is <italic>p</italic> and 1 − <italic>p</italic> + <italic>q</italic>, respectively. Thus, their persistence diagrams always have the same number of points. The bijection <italic>γ</italic> is determined by the bipartite graph matching algorithm (Cohen-Steiner et al., <xref rid="bib30" ref-type="bibr">2007</xref>; Edelsbrunner &amp; Harer, <xref rid="bib33" ref-type="bibr">2008</xref>).</p>
    <p>If <italic>m</italic> ≠ <italic>n</italic>, there is no one-to-one correspondence between two PDs. Then, auxiliary points<disp-formula><mml:math id="m30"><mml:mo stretchy="false">(</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula>and<disp-formula><mml:math id="m31"><mml:mo stretchy="false">(</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula>that are orthogonal projections to the diagonal line <italic>ξ</italic> = <italic>τ</italic> in 𝒫(𝒳<sup>1</sup>) and 𝒫(𝒳<sup>2</sup>) are added to 𝒫(𝒳<sup>2</sup>) and 𝒫(𝒳<sup>1</sup>), respectively, to make the identical number of points in PDs.</p>
    <p>The bottleneck distance does not directly measure the distance between two metric spaces 𝒳<sup>1</sup> = (<italic>V</italic><sup>1</sup>, <italic>w</italic><sup>1</sup>) and 𝒳<sup>2</sup> = (<italic>V</italic><sup>2</sup>, <italic>w</italic><sup>2</sup>), but measures the distance between their corresponding persistence diagrams 𝒫(𝒳<sup>1</sup>) and 𝒫(𝒳<sup>1</sup>). In practice, the bottleneck distance has been often used since it is a lower bound on the GH-distance and it is easier to compute (Chazal et al., <xref rid="bib19" ref-type="bibr">2009</xref>). Since the brain regions that form the network nodes are matched across the networks through predefined parcellations in brain network studies, the GH-distance can be computed easily. Thus, in this study, we will only use the GH-distance and not show the result of the bottleneck distance in the simulation study.</p>
  </sec>
  <sec>
    <title>PERMUTATION TEST ON NETWORK DISTANCES</title>
    <p>Statistical inference on network distances can be done using resampling techniques such as the permutation test (Chung et al., <xref rid="bib24" ref-type="bibr">2013</xref>; Efron, <xref rid="bib36" ref-type="bibr">1982</xref>; Lee et al., <xref rid="bib58" ref-type="bibr">2012</xref>). The permutation test is perhaps the most widely used nonparametric test procedure in the sciences (Chung et al., <xref rid="bib29" ref-type="bibr">2017b</xref>; Nichols &amp; Holmes, <xref rid="bib68" ref-type="bibr">2002</xref>; Thompson, Cannon, Narr, van Erp, Poutanen, Huttunen, Lonnqvist, Standertskjold-Nordenstam, Kaprio, &amp; Khaledy, <xref rid="bib86" ref-type="bibr">2001</xref>; Zalesky, Fornito, Harding, Cocchi, Yücel, Pantelis, &amp; Bullmore, <xref rid="bib92" ref-type="bibr">2010</xref>). It is known as the exact test in brain imaging since the distribution of the test statistic under the null hypothesis can be exactly computed if we can calculate all possible values of the test statistic under every possible permutation.</p>
    <p>Here we explain the permutation test procedure that was used for network distances. The usual setting in brain imaging applications is a two-sample comparison. Suppose there are <italic>m</italic> measurement in Group 1 on node set <italic>V</italic> of size <italic>p</italic>. Denote the data matrix as <inline-formula><mml:math id="m32"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>. The edge weights of Group 1 are given by <italic>f</italic>(<bold>x</bold><sup>1</sup>) for some function <italic>f</italic> and the metric space is given by 𝒳<sup>1</sup> = (<italic>V</italic>, <italic>f</italic>(<bold>X</bold><sup>1</sup>)). Suppose there are <italic>n</italic> measurement in Group 2 on the identical node set <italic>V</italic>. Denote data matrix as <inline-formula><mml:math id="m33"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and the corresponding metric space as 𝒳<sup>1</sup> = (<italic>V</italic>, <italic>f</italic>(<bold>X</bold><sup>1</sup>)). We test the statistical significance of network distance <italic>D</italic>(𝒳<sup>1</sup>, 𝒳<sup>2</sup>) under the null hypothesis <italic>H</italic><sub>0</sub>:<disp-formula><mml:math id="m34"><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mspace width="0.3em"/><mml:mtext>vs.</mml:mtext><mml:mspace width="0.3em"/><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:math></disp-formula></p>
    <p>The permutation test is done as follows. Under <italic>H</italic><sub>0</sub>, one can concatenate the data matrices<disp-formula><mml:math id="m35"><mml:mi mathvariant="bold">X</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula>and then permute the indices of the row vectors of <bold>X</bold> in the symmetric group of degree <italic>m</italic> + <italic>n</italic>, that is, <italic>S</italic><sub><italic>m</italic>+<italic>n</italic></sub> (Kondor, Howard, &amp; Jebara, <xref rid="bib51" ref-type="bibr">2007</xref>). Denote the <italic>i</italic>-th permuted data matrix as <bold>X</bold><sub><italic>σ</italic>(<italic>i</italic>)</sub> = (<italic>x</italic><sub><italic>σ</italic>(<italic>i</italic>),<italic>j</italic></sub>), where <italic>σ</italic> ∈ <italic>S</italic><sub><italic>m</italic>+<italic>n</italic></sub>. Then we split <bold>X</bold><sub><italic>σ</italic>(<italic>i</italic>)</sub> into submatrices such that<disp-formula><mml:math id="m36"><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="m37"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="m38"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> are of sizes <italic>m</italic> × <italic>p</italic> and <italic>n</italic> × <italic>p</italic> respectively. Let <inline-formula><mml:math id="m39"><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = (<italic>V</italic>, <italic>f</italic>(<inline-formula><mml:math id="m40"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>)) and <inline-formula><mml:math id="m41"><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> = (<italic>V</italic>, <italic>f</italic>(<inline-formula><mml:math id="m42"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>)) be weighted networks where the rows of the data matrices are permuted across the groups. Then we have distance <italic>D</italic>(<inline-formula><mml:math id="m43"><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="m44"><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>) for each permutation. The fraction of permutations <italic>D</italic>(<inline-formula><mml:math id="m45"><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula><mml:math id="m46"><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>) that is larger than <italic>D</italic>(𝒳<sup>1</sup>, 𝒳<sup>2</sup>) gives the estimate for the <italic>p</italic> value.</p>
    <p>Unfortunately, generating every possible permutation for whole images is still extremely time consuming even for a modest sample size. The number of permutations exponentially increases, and it is impractical to generate every possible permutation. In the permutation test, only a small fraction of possible permutations are generated, and the statistical significance is computed approximately. In most studies, on the order of 1% of total permutations were often used, mainly due to the computational bottleneck of generating permutations (Thompson et al., <xref rid="bib86" ref-type="bibr">2001</xref>; Zalesky et al., <xref rid="bib92" ref-type="bibr">2010</xref>). In Zalesky et al. (<xref rid="bib92" ref-type="bibr">2010</xref>), 5,000 permutations out of possible <inline-formula><mml:math id="m47"><mml:mfenced open="(" close=")"><mml:mfrac linethickness="0"><mml:mrow><mml:mn>27</mml:mn></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:mfrac></mml:mfenced></mml:math></inline-formula> = 17,383,860 permutations (2.9%) were used. In Thompson et al. (<xref rid="bib86" ref-type="bibr">2001</xref>), 1 million permutations out of <inline-formula><mml:math id="m48"><mml:mfenced open="(" close=")"><mml:mfrac linethickness="0"><mml:mrow><mml:mn>40</mml:mn></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:mfrac></mml:mfenced></mml:math></inline-formula> possible permutations (0.07%) were generated using a super computer. In our study, we have 131 MZ and 77 DZ twins. The possible number of permutations is <inline-formula><mml:math id="m49"><mml:mfenced open="(" close=")"><mml:mfrac linethickness="0"><mml:mrow><mml:mn>208</mml:mn></mml:mrow><mml:mrow><mml:mn>77</mml:mn></mml:mrow></mml:mfrac></mml:mfenced></mml:math></inline-formula>. This is a number so large, we cannot exactly represent it in computing systems such as MATLAB and R. Even the 1% of <inline-formula><mml:math id="m50"><mml:mfenced open="(" close=")"><mml:mfrac linethickness="0"><mml:mrow><mml:mn>208</mml:mn></mml:mrow><mml:mrow><mml:mn>77</mml:mn></mml:mrow></mml:mfrac></mml:mfenced></mml:math></inline-formula> is about 1.96 × 10<sup>56</sup>, which is still astronomically large and beyond the computing capability of the most computers. On the other hand, the proposed KS-distance method computes for all possible permutations combinatorially and completely bypasses the computational bottleneck. There is no computational cost involved in the KS-distance and the computation is done in a few seconds. Furthermore, the method computes <italic>p</italic> values exactly and it is not approximate.</p>
  </sec>
  <sec>
    <title>KOLMOGOROV-SMIRNOV DISTANCE</title>
    <p>Recently, the <xref rid="def5" ref-type="def">Kolmogorov-Smirnov (KS) distance</xref> has been successfully applied in quantifying the change of <italic>β</italic><sub>0</sub> number over graph filtration as a way to quantify brain networks without thresholding (Chung et al., <xref rid="bib26" ref-type="bibr">2017a</xref>, <xref rid="bib29" ref-type="bibr">2017b</xref>). The main advantage of the method is that it avoids using the computationally costly and time consuming permutation test for large-scale networks. In this paper, we show how to apply KS-distance in quantifying the change of the <italic>β</italic><sub>1</sub> number over graph filtration as well.</p>
    <p>In this study, the square root of 1 correlation is used as edge weights. Given two networks 𝒳<sup>1</sup> = (<italic>V</italic>, <italic>w</italic><sup>1</sup>) and 𝒳<sup>2</sup> = (<italic>V</italic>, <italic>w</italic><sup>2</sup>), KS distances between 𝒳<sup>1</sup> and 𝒳<sup>2</sup> for Betti numbers <italic>β</italic><sub>0</sub> and <italic>β</italic><sub><italic>j</italic></sub> are defined as (Chung et al., <xref rid="bib24" ref-type="bibr">2013</xref>; Lee et al., <xref rid="bib59" ref-type="bibr">2017</xref>):<disp-formula><mml:math id="m51"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>sup</mml:mo></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic>β</italic><sub><italic>j</italic></sub>(<inline-formula><mml:math id="m52"><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) is the <italic>j</italic>-th Betti number for binary network <inline-formula><mml:math id="m53"><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>. The distance <italic>D</italic><sub><italic>KS</italic></sub> can be discretely approximated using the finite number of filtrations:<disp-formula><mml:math id="m54"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>sup</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>If we choose enough of <italic>q</italic> such that <italic>ϵ</italic><sub><italic>j</italic></sub> are all the sorted edge weights, then<disp-formula><mml:math id="m55"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula>(Chung et al., <xref rid="bib29" ref-type="bibr">2017b</xref>). This is possible since there are only up to <italic>p</italic>(<italic>p</italic> − 1)/2 number of unique edges in a graph with <italic>p</italic> nodes and the monotone function increases discretely but <italic>not continuously</italic>. In practice, <italic>ϵ</italic><sub><italic>j</italic></sub> may be chosen uniformly or a divide-and-conquer strategy can be used to adaptively grid the filtration values. Then the probability distribution of <italic>D</italic><sub><italic>q</italic></sub> can be computed exactly by combinatorial means.</p>
    <p>
      <statement id="thm3">
        <label>Theorem 3</label>
        <p><disp-formula><mml:math id="m56"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mfenced open="(" close=")"><mml:mfrac linethickness="0"><mml:mrow><mml:mn>2</mml:mn><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mfrac></mml:mfenced></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic>A</italic><sub><italic>u</italic>,<italic>v</italic></sub> satisfies <italic>A</italic><sub><italic>u</italic>,<italic>v</italic></sub> = <italic>A</italic><sub><italic>u</italic>−1,<italic>v</italic></sub> + <italic>A</italic><sub><italic>u</italic>,<italic>v</italic>−1</sub> with the boundary condition <italic>A</italic><sub>0,<italic>v</italic></sub> = <italic>A</italic><sub><italic>u</italic>,0</sub> = 1 within band |<italic>u</italic> − <italic>v</italic>| &lt; <italic>d</italic> and initial condition <italic>A</italic><sub>0,0</sub> = 0 for <italic>u</italic>, <italic>v</italic> ≥ 1.</p>
      </statement>
    </p>
    <p>The proof is given in Chung et al. (<xref rid="bib29" ref-type="bibr">2017b</xref>).</p>
    <p>
      <statement id="exm1">
        <label>Example 1</label>
        <p><italic>P</italic>(<italic>D</italic><sub>3</sub> ≥ 2) is computed sequentially as follows (<xref ref-type="fig" rid="F2">Figure 2</xref>). We start with the bottom left corner <italic>A</italic><sub>0,0</sub> = 0 and move right or up toward the upper corner<disp-formula><mml:math id="m57"><mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mo>→</mml:mo><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mo>⋯</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>The probability is then <italic>P</italic>(<italic>D</italic><sub>3</sub> ≥ 2) = 1 − 8/<inline-formula><mml:math id="m58"><mml:mfenced open="(" close=")"><mml:mfrac linethickness="0"><mml:mrow><mml:mn>6</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac></mml:mfenced></mml:math></inline-formula> = 0.6. The computational complexity of the combinatorial inference is 𝒪(<italic>q</italic> log <italic>q</italic>) for sorting and 𝒪(<italic>q</italic><sup>2</sup>) for computing <italic>A</italic><sub><italic>q</italic>,<italic>q</italic></sub> in the grid while the permutation test requires exponential run time.</p>
      </statement>
    </p>
    <fig id="F2" orientation="portrait" position="float">
      <label><bold>Figure 2.</bold> </label>
      <caption>
        <p>In this example, <italic>A</italic><sub><italic>u</italic>,<italic>v</italic></sub> is computed within the boundary (dotted red line) from (0, 0) to (3, 3).</p>
      </caption>
      <graphic xlink:href="netn-03-674-g002"/>
    </fig>
    <p>When <italic>q</italic> is too large, it may not be possible to represent and compute <inline-formula><mml:math id="m59"><mml:mfenced open="(" close=")"><mml:mfrac linethickness="0"><mml:mrow><mml:mn>2</mml:mn><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mfrac></mml:mfenced></mml:math></inline-formula> in all the digits. For large <italic>q</italic>, use the asymptotic probability distribution <italic>D</italic><sub><italic>q</italic></sub> given by Chung et al. (<xref rid="bib29" ref-type="bibr">2017b</xref>):<disp-formula id="E4"><mml:math id="m60"><mml:munder><mml:mrow><mml:mo>lim</mml:mo></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>→</mml:mo><mml:mo>∞</mml:mo></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>q</mml:mi></mml:mrow></mml:msqrt><mml:mo>≥</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mstyle displaystyle="true"><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>∞</mml:mo></mml:mrow></mml:munderover></mml:mstyle><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:math><label>(4)</label></disp-formula>The <italic>p</italic> value of the test statistic under the null is then computed as<disp-formula><mml:math id="m61"><mml:mi>p</mml:mi><mml:mspace width="0.3em"/><mml:mtext>value</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>8</mml:mn><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>18</mml:mn><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msup><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>where the observed value <italic>d</italic><sub><italic>o</italic></sub> is the least integer greater than <italic>D</italic><sub><italic>q</italic></sub>/<inline-formula><mml:math id="m62"><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>q</mml:mi></mml:mrow></mml:msqrt></mml:math></inline-formula> in the data.</p>
  </sec>
  <sec>
    <title>COMPARISONS</title>
    <p>Six network distances (<italic>L</italic><sub>1</sub>, <italic>L</italic><sub>2</sub>, <italic>L</italic><sub>∞</sub>, GH and KS on <italic>β</italic><sub>0</sub> and <italic>β</italic><sub>1</sub>) were compared in simulation studies. For the review of various brain network distances, refer to Chung et al. (<xref rid="bib26" ref-type="bibr">2017a</xref>). We also used the popular Q-modularity function for community detection in graph theory (Girvan &amp; Newman, <xref rid="bib40" ref-type="bibr">2002</xref>; Meunier et al., <xref rid="bib65" ref-type="bibr">2009</xref>; Newman et al., <xref rid="bib66" ref-type="bibr">2006</xref>). The difference in Q-modularity functions was used as the distance measure. The simulations below were independently performed 100 times. We used <italic>p</italic> = 20,100,500 nodes and <italic>n</italic> = 5 images in each group, which made it possible for permutations to be exactly <inline-formula><mml:math id="m63"><mml:mfenced open="(" close=")"><mml:mfrac linethickness="0"><mml:mrow><mml:mn>5</mml:mn><mml:mo>+</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:mfrac></mml:mfenced></mml:math></inline-formula> = 252 (<xref ref-type="fig" rid="F3">Figure 3</xref>). The small number of permutations enables us to compare the performance of distances exactly. Through the simulations, <italic>σ</italic> = 0.1 was universally used as network variability.</p>
    <fig id="F3" orientation="portrait" position="float">
      <label><bold>Figure 3.</bold> </label>
      <caption>
        <p>Randomly simulated correlation matrices with different topological structures with <italic>k</italic> = 2, 4, 5, 10. For <italic>k</italic> = 4, 10, two different randomly generated networks are shown.</p>
      </caption>
      <graphic xlink:href="netn-03-674-g003"/>
    </fig>
    <p>The data vector <bold>x</bold><sub><italic>i</italic></sub> at node <italic>i</italic> was simulated as identical and independently distributed multivariate normal across <italic>i</italic>, that is, <bold>x</bold><sub><italic>i</italic></sub> ∼ <italic>N</italic>(0, <italic>I</italic><sub><italic>n</italic></sub>) with <italic>n</italic> by <italic>n</italic> identity matrix <italic>I</italic><sub><italic>n</italic></sub> as the covariance matrix. This gives the correlation matrix <italic>C</italic><sup>1</sup> = (<inline-formula><mml:math id="m64"><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>) = (<italic>corr</italic>(<bold>x</bold><sub><italic>i</italic></sub>, <bold>x</bold><sub><italic>j</italic></sub>)). The edge weights were given by <inline-formula><mml:math id="m65"><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msqrt></mml:math></inline-formula>. The data vector <bold>y</bold><sub><italic>i</italic></sub> at node <italic>i</italic> that produced node dependency was simulated by adding additional dependency to <bold>x</bold><sub><italic>i</italic></sub> through a hierarchical linear model or <xref rid="def6" ref-type="def">mixed-effect model</xref> (Pinehiro &amp; Bates, <xref rid="bib71" ref-type="bibr">2002</xref>; Snijders, Spreen, &amp; Zwaagstra, <xref rid="bib79" ref-type="bibr">1995</xref>). This is a standard simulation technique for introducing dependency structures in random simulations. The hierarchical linear model enables us to explicitly model the data vector at each node and simulate the amount of dependency between nodes, providing detailed control over the topological structures in the correlation matrices. Data vector <bold>y</bold><sub><italic>i</italic></sub> at node <italic>i</italic> will be simulated using <bold>x</bold><sub><italic>i</italic></sub> as follows.<disp-formula><mml:math id="m66"><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd/><mml:mtd columnalign="left"><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>This introduces a topological structure of connectedness through statistical dependency. Although we did not try here, a far more complex dependency structure is also possible. In our simulation <italic>c</italic> = <italic>p</italic>/<italic>k</italic> = 10, 5, 4, 2 and <italic>k</italic> = <italic>p</italic>/<italic>c</italic> = 2, 4, 5, 10 are used (<xref ref-type="fig" rid="F3">Figure 3</xref>). Subsequently, we have the correlation matrix <italic>C</italic><sup>2</sup> = (<inline-formula><mml:math id="m67"><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>) = (<italic>corr</italic>(<bold>y</bold><sub><italic>i</italic></sub>, <bold>y</bold><sub><italic>j</italic></sub>)) and the subsequent edge weights <inline-formula><mml:math id="m68"><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:msqrt></mml:math></inline-formula>.</p>
    <sec>
      <title>No Network Difference</title>
      <p>It was expected there was no network difference between networks generated using the same parameters and initial data vectors <bold>x</bold><sub><italic>i</italic></sub> in the above model. For example, <xref ref-type="fig" rid="F3">Figure 3</xref> shows two simulated networks generated with the same parameters <italic>k</italic> = 4, 10. We compared networks with the same parameter <italic>k</italic>: 4 vs. 4, 5 vs. 5 and 10 vs. 10. It is expected we should not able to detect the network differences. The performance results were given in terms of the false positive error rate computed as the fraction of simulations that gave <italic>p</italic> value below 0.05 (<xref rid="T1" ref-type="table">Table 1</xref>). For all the distances except KS-distance, the permutation test was used. Since there were five samples in each group, the total number of permutations was <inline-formula><mml:math id="m69"><mml:mfenced open="(" close=")"><mml:mfrac linethickness="0"><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:mfrac></mml:mfenced></mml:math></inline-formula> = 272, making the permutation test exact and the comparisons accurate. All the distances performed very well including Q-modularity. KS-distance was overly sensitive and was producing up to 7% false positives. However, for 0.05 level test, it is expected that there is 5% chance of producing false positives. Thus, KS-distance is producing only 2% above the expected error rate.</p>
      <table-wrap id="T1" orientation="portrait" position="float">
        <label><bold>Table 1.</bold> </label>
        <caption>
          <p>The <italic>p</italic> = 20 nodes simulation results given in terms of false positive and negative error rates.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr valign="bottom">
              <th align="left" rowspan="1" colspan="1"><italic>p</italic> = 20</th>
              <th align="center" rowspan="1" colspan="1">
                <italic>L</italic>
                <sub>1</sub>
              </th>
              <th align="center" rowspan="1" colspan="1">
                <italic>L</italic>
                <sub>2</sub>
              </th>
              <th align="center" rowspan="1" colspan="1">
                <italic>L</italic>
                <sub>∞</sub>
              </th>
              <th align="center" rowspan="1" colspan="1">GH</th>
              <th align="center" rowspan="1" colspan="1">KS (<italic>β</italic><sub>0</sub>)</th>
              <th align="center" rowspan="1" colspan="1">KS (<italic>β</italic><sub>1</sub>)</th>
              <th align="center" rowspan="1" colspan="1">Q</th>
            </tr>
          </thead>
          <tbody>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">4 vs. 4</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.04</td>
              <td align="center" rowspan="1" colspan="1">0.01</td>
              <td align="center" rowspan="1" colspan="1">0.05</td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">5 vs. 5</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.07</td>
              <td align="center" rowspan="1" colspan="1">0.01</td>
              <td align="center" rowspan="1" colspan="1">0.06</td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">10 vs. 10</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.04</td>
            </tr>
            <tr valign="top">
              <td colspan="8" rowspan="1"> </td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">4 vs. 5</td>
              <td align="center" rowspan="1" colspan="1">0.63</td>
              <td align="center" rowspan="1" colspan="1">0.40</td>
              <td align="center" rowspan="1" colspan="1">0.33</td>
              <td align="center" rowspan="1" colspan="1">0.15</td>
              <td align="center" rowspan="1" colspan="1">0.27</td>
              <td align="center" rowspan="1" colspan="1">0.06</td>
              <td align="center" rowspan="1" colspan="1">0.9</td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">2 vs. 4</td>
              <td align="center" rowspan="1" colspan="1">0.71</td>
              <td align="center" rowspan="1" colspan="1">0.48</td>
              <td align="center" rowspan="1" colspan="1">0.42</td>
              <td align="center" rowspan="1" colspan="1">0.53</td>
              <td align="center" rowspan="1" colspan="1">0.18</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.95</td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">5 vs. 10</td>
              <td align="center" rowspan="1" colspan="1">0.94</td>
              <td align="center" rowspan="1" colspan="1">0.80</td>
              <td align="center" rowspan="1" colspan="1">0.78</td>
              <td align="center" rowspan="1" colspan="1">0.72</td>
              <td align="center" rowspan="1" colspan="1">0.44</td>
              <td align="center" rowspan="1" colspan="1">0.24</td>
              <td align="center" rowspan="1" colspan="1">0.96</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The <italic>p</italic> = 20 simutation might be too small a network to extract topologically distinct features that are used in topological distances. Thus, we increased the number of nodes to <italic>p</italic> = 100 (<xref rid="T2" ref-type="table">Table 2</xref>). All the network distances except KS-distances performed reasonably well. KS-distances seem to be overly sensitive to slight topological change in large topological structures that were present in <italic>k</italic> = 2, 4, 5 cases. As <italic>k</italic> increases, KS-distances seem to perform reasonably well.</p>
      <table-wrap id="T2" orientation="portrait" position="float">
        <label><bold>Table 2.</bold> </label>
        <caption>
          <p>The <italic>p</italic> = 100 nodes simulation results given in terms of false positive and negative error rates.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr valign="bottom">
              <th align="left" rowspan="1" colspan="1"><italic>p</italic> = 100</th>
              <th align="center" rowspan="1" colspan="1">
                <italic>L</italic>
                <sub>1</sub>
              </th>
              <th align="center" rowspan="1" colspan="1">
                <italic>L</italic>
                <sub>2</sub>
              </th>
              <th align="center" rowspan="1" colspan="1">
                <italic>L</italic>
                <sub>∞</sub>
              </th>
              <th align="center" rowspan="1" colspan="1">GH</th>
              <th align="center" rowspan="1" colspan="1">KS (<italic>β</italic><sub>0</sub>)</th>
              <th align="center" rowspan="1" colspan="1">KS (<italic>β</italic><sub>1</sub>)</th>
              <th align="center" rowspan="1" colspan="1">Q</th>
            </tr>
          </thead>
          <tbody>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">4 vs. 4</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.26</td>
              <td align="center" rowspan="1" colspan="1">0.54</td>
              <td align="center" rowspan="1" colspan="1">0.03</td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">5 vs. 5</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.14</td>
              <td align="center" rowspan="1" colspan="1">0.43</td>
              <td align="center" rowspan="1" colspan="1">0.05</td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">10 vs. 10</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.05</td>
              <td align="center" rowspan="1" colspan="1">0.05</td>
              <td align="center" rowspan="1" colspan="1">0.05</td>
            </tr>
            <tr valign="top">
              <td colspan="8" rowspan="1"> </td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">4 vs. 5</td>
              <td align="center" rowspan="1" colspan="1">0.51</td>
              <td align="center" rowspan="1" colspan="1">0.37</td>
              <td align="center" rowspan="1" colspan="1">0.35</td>
              <td align="center" rowspan="1" colspan="1">0.16</td>
              <td align="center" rowspan="1" colspan="1">0.11</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.93</td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">2 vs. 4</td>
              <td align="center" rowspan="1" colspan="1">0.66</td>
              <td align="center" rowspan="1" colspan="1">0.45</td>
              <td align="center" rowspan="1" colspan="1">0.57</td>
              <td align="center" rowspan="1" colspan="1">0.61</td>
              <td align="center" rowspan="1" colspan="1">0.03</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.91</td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">5 vs. 10</td>
              <td align="center" rowspan="1" colspan="1">0.94</td>
              <td align="center" rowspan="1" colspan="1">0.86</td>
              <td align="center" rowspan="1" colspan="1">0.79</td>
              <td align="center" rowspan="1" colspan="1">0.72</td>
              <td align="center" rowspan="1" colspan="1">0.11</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.98</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>Network Differences</title>
      <p>We generated networks with parameter <italic>k</italic> = 2, 4, 5, 10 with <italic>p</italic> = 20 nodes simulation (<xref ref-type="fig" rid="F3">Figure 3</xref>). Since topological structures were different, the distances are expected to differentiate the networks. The performance results were given in terms of the false negative error rate computed as the fraction of simulations that give <italic>p</italic> value above 0.05 (<xref rid="T1" ref-type="table">Table 1</xref>). All the distances including Q-modularity performed badly, although KS-distance performed the best. Since graph theory features are <italic>not explicitly</italic> designed to measure network distances, they do not usually perform well when there are large topological differences.</p>
      <p>We increased the number of nodes to <italic>p</italic> = 100. All the network distances including Q-modularity were still performing badly except KS-distances (<xref rid="T2" ref-type="table">Table 2</xref>). KS-distance on the number of cycles seems to be the best network distance to use when there are network topology differences, although it has tendency to produce false positives when there is no difference.</p>
      <p>In terms of computation, distance methods based on the permutation test took about 950 seconds (16 minutes) for 100 nodes, while the KS-like test procedure only took about 20 seconds in a computer. The results given in <xref rid="T1" ref-type="table">Tables 1</xref>–<xref rid="T3" ref-type="table">3</xref> may slightly change if different random networks are generated. We also performed the simulation study on the 500 nodes to see the effect of increased network sizes (<xref rid="T3" ref-type="table">Table 3</xref>). The proposed KS-distance on both <italic>β</italic><sub>0</sub> and <italic>β</italic><sub>1</sub> are not necessarily performing well in the case of no network differences. Again the KS-distance is too sensitive and detecting minute network differences. On the other hand, in the case of actual network differences, the KS-distances are performing exceptionally well compared with other network differences.</p>
      <table-wrap id="T3" orientation="portrait" position="float">
        <label><bold>Table 3.</bold> </label>
        <caption>
          <p>The <italic>p</italic> = 500 nodes simulation results given in terms of false positive and negative error rates.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr valign="bottom">
              <th align="left" rowspan="1" colspan="1"><italic>p</italic> = 500</th>
              <th align="center" rowspan="1" colspan="1">
                <italic>L</italic>
                <sub>1</sub>
              </th>
              <th align="center" rowspan="1" colspan="1">
                <italic>L</italic>
                <sub>2</sub>
              </th>
              <th align="center" rowspan="1" colspan="1">
                <italic>L</italic>
                <sub>∞</sub>
              </th>
              <th align="center" rowspan="1" colspan="1">GH</th>
              <th align="center" rowspan="1" colspan="1">KS (<italic>β</italic><sub>0</sub>)</th>
              <th align="center" rowspan="1" colspan="1">KS (<italic>β</italic><sub>1</sub>)</th>
              <th align="center" rowspan="1" colspan="1">Q</th>
            </tr>
          </thead>
          <tbody>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">4 vs. 4</td>
              <td align="center" rowspan="1" colspan="1">0.04</td>
              <td align="center" rowspan="1" colspan="1">0.05</td>
              <td align="center" rowspan="1" colspan="1">0.06</td>
              <td align="center" rowspan="1" colspan="1">0.08</td>
              <td align="center" rowspan="1" colspan="1">0.20</td>
              <td align="center" rowspan="1" colspan="1">0.26</td>
              <td align="center" rowspan="1" colspan="1">0.02</td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">5 vs. 5</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.13</td>
              <td align="center" rowspan="1" colspan="1">0.20</td>
              <td align="center" rowspan="1" colspan="1">0.02</td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">10 vs. 10</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.06</td>
              <td align="center" rowspan="1" colspan="1">0.18</td>
              <td align="center" rowspan="1" colspan="1">0.05</td>
            </tr>
            <tr valign="top">
              <td colspan="8" rowspan="1"> </td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">4 vs. 5</td>
              <td align="center" rowspan="1" colspan="1">0.20</td>
              <td align="center" rowspan="1" colspan="1">0.20</td>
              <td align="center" rowspan="1" colspan="1">0.20</td>
              <td align="center" rowspan="1" colspan="1">0.20</td>
              <td align="center" rowspan="1" colspan="1">0.11</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.20</td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">2 vs. 4</td>
              <td align="center" rowspan="1" colspan="1">0.14</td>
              <td align="center" rowspan="1" colspan="1">0.11</td>
              <td align="center" rowspan="1" colspan="1">0.14</td>
              <td align="center" rowspan="1" colspan="1">0.12</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.17</td>
            </tr>
            <tr valign="top">
              <td align="left" rowspan="1" colspan="1">5 vs. 10</td>
              <td align="center" rowspan="1" colspan="1">0.20</td>
              <td align="center" rowspan="1" colspan="1">0.18</td>
              <td align="center" rowspan="1" colspan="1">0.19</td>
              <td align="center" rowspan="1" colspan="1">0.16</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.00</td>
              <td align="center" rowspan="1" colspan="1">0.20</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
  </sec>
  <sec>
    <title>APPLICATION</title>
    <p>As an application, we show how to apply KS-distances in understanding heritability of brain networks. Because of their unique relationship, twin imaging studies allow researchers to examine genetic and environmental influences easily <italic>in vivo</italic> (Blokland, McMahon, Thompson, Martin, de Zubicaray, &amp; Wright, <xref rid="bib8" ref-type="bibr">2011</xref>; Chiang, McMahon, de Zubicaray, Martin, Hickie, Toga, Wright, &amp; Thompson, <xref rid="bib20" ref-type="bibr">2011</xref>; Glahn, Winkler, Kochunov, Almasy, Duggirala, Carless, Curran, Olvera, Laird, Smith, Beckmann, Fox, &amp; Blangero, <xref rid="bib42" ref-type="bibr">2010</xref>; McKay, Knowles, Winkler, Sprooten, Kochunov, Olvera, Curran, Kent Jr., Carless, Göring, Dyer, Duggirala, Almasy, Fox, Blangero, &amp; Glahn, <xref rid="bib63" ref-type="bibr">2014</xref>; Smit, Stam, Posthuma, Boomsma, &amp; De Geus, <xref rid="bib77" ref-type="bibr">2008</xref>). Monozygotic (MZ) twins share 100% of genes, whereas dizygotic (DZ) twins share 50% of genes (Chung et al., <xref rid="bib29" ref-type="bibr">2017b</xref>). The difference between MZ and DZ twins measures the degree of genetic and environmental influence. Twin imaging studies are very useful for understanding the extent to which brain networks are influenced by genetic factors. This information can then be later used to develop better ways to prevent and treat disorders and maladaptive behaviors.</p>
    <sec>
      <title>Dataset and Image Preprocessing</title>
      <p>We used the resting-state fMRI of 271 twin pairs from the Human Connectome Project (Van Essen, Ugurbil, Auerbach, Barch, Behrens, Bucholz, Chang, Chen, Corbetta, &amp; Curtiss, <xref rid="bib90" ref-type="bibr">2012</xref>). Out of a total 271 twin pairs, we only used genetically confirmed 131 MZ twin pairs (age 29.3 ± 3.3 years, 56M/75F) and 77 same-sex DZ twin pairs (age 29.1 ± 3.5 years, 30M/47F) in this study. Since the discrepancy between self-reported and genotype-verified zygosity was fairly high at 13% of all the available data, 19 MZ and 19 DZ twin pairs that do not have genotyping were excluded. We additionally excluded 35 twin pairs with missing fMRI data.</p>
      <p>fMRI were collected on a customized Siemens 3T Connectome Skyra scanner, using a gradient-echo-planar imaging (EPI) sequence with multiband factor = 8, TR = 720 ms, TE = 33.1 ms, flip angle = 52°, 104 × 90 (RO×PE) matrix size, 72 slices, and 2-mm isotropic voxels; 1,200 volumes were obtained over a 14 min, 33 sec scanning session. fMRI data has undergone spatial and temporal preprocessing including motion and physiological noise removal (Smith et al., <xref rid="bib78" ref-type="bibr">2013</xref>). Using the resting-state fMRI, we employed the Automated Anatomical Labeling (AAL) brain template to parcellate the brain volume into 116 regions (Tzourio-Mazoyer, Landeau, Papathanassiou, Crivello, Etard, Delcroix, Mazoyer, &amp; Joliot, <xref rid="bib88" ref-type="bibr">2002</xref>). The fMRI were then averaged across voxels in each brain region for each subject. The averaged fMRI signal in each parcellation was then temporally smoothed using the cosine series representation as follows (Chung, Adluru, Lee, Lazar, Lainhart, &amp; Alexander, <xref rid="bib23" ref-type="bibr">2010</xref>; Gritsenko, Lindquist, Kirk, &amp; Chung, <xref rid="bib43" ref-type="bibr">2018</xref>).</p>
      <p>Given fMRI time series at the <italic>i</italic>-th parcellation <italic>ζ</italic><sub><italic>i</italic></sub>(<italic>t</italic>) at time <italic>t</italic>, we scaled it to fit to unit interval [0, 1]. Then subtracted its mean over time <inline-formula><mml:math id="m70"><mml:msubsup><mml:mrow><mml:mo>∫</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>
<italic>ζ</italic><sub><italic>i</italic></sub>(<italic>t</italic>) <italic>dt</italic>. Then the resulting scaled and translated time series was represented as<disp-formula><mml:math id="m71"><mml:msub><mml:mrow><mml:mi>ζ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover></mml:mstyle><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.3em"/><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic>ψ</italic><sub>0</sub>(<italic>t</italic>) = 1, <italic>ψ</italic><sub><italic>l</italic></sub>(<italic>t</italic>) = <inline-formula><mml:math id="m72"><mml:msqrt><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msqrt></mml:math></inline-formula> cos(<italic>lπt</italic>) were cosine basis functions and <italic>c</italic><sub><italic>li</italic></sub> were coefficients estimated in the least squares fashion. For our study, <italic>k</italic> = 119 was used such that fMRI were compressed into 10% of the original data size; <italic>k</italic> = 119 expansion increased the signal-to-noise ratio (SNR) as measured by the ratio of variabilities by 81% in average over all 116 brain regions and 416 subjects, that is, SNR = 1.81. The resulting real-valued Fourier coefficient vector <bold>c</bold><sub><italic>i</italic></sub> = (<italic>c</italic><sub>0<italic>i</italic></sub>, <italic>c</italic><sub>1<italic>i</italic></sub>, ⋯, <italic>c</italic><sub><italic>ki</italic></sub>) was then used to represent the fMRI in each parcellation as 120 features in the spectral domain.</p>
    </sec>
    <sec>
      <title>Twin Correlations</title>
      <p>The subject level connectivity matrix <italic>C</italic> = (<italic>c</italic><sub><italic>ij</italic></sub>) was computed by correlating 120 features in the spectral domain. Between <italic>i</italic>- and <italic>j</italic>-th parcellations, the connectivity was measured by correlating <bold>c</bold><sub><italic>i</italic></sub> and <bold>c</bold><sub><italic>j</italic></sub> over 120 features, that is, <italic>c</italic><sub><italic>ij</italic></sub> = <italic>corr</italic>(<bold>c</bold><sub><italic>i</italic></sub>, <bold>c</bold><sub><italic>j</italic></sub>). From the individual correlation matrices <italic>C</italic>, we computed pairwise twin correlations in each group at the edge level. The resulting group level twin correlations matrices <italic>C</italic><sub><italic>MZ</italic></sub> = (<inline-formula><mml:math id="m73"><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) and <italic>C</italic><sub><italic>DZ</italic></sub> = (<inline-formula><mml:math id="m74"><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) are nonsymmetric cross-correlation matrices. Since there is no preference in the order of twins, we symmetrize them by<disp-formula><mml:math id="m75"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:math></disp-formula>and<disp-formula><mml:math id="m76"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>.</mml:mo></mml:math></disp-formula>Then we are interested in knowing the extent of the genetic influence on resting-state functional brain network and its statistical significance. For this, we use the widely used ACE genetic model (Falconer &amp; Mackay, <xref rid="bib37" ref-type="bibr">1995</xref>) that mainly uses <xref rid="def7" ref-type="def">heritability index</xref> (HI), which determines the amount of variation (in terms of percentage) due to genetic influence in a population. HI is often estimated using Falconer’s formula (Falconer &amp; Mackay, <xref rid="bib37" ref-type="bibr">1995</xref>) as a baseline. MZ twins share 100% of genes, whereas DZ twins share 50% of genes. Thus, the additive genetic factor <italic>A</italic>, the common environmental factor <italic>C</italic> for each twin type are related as<disp-formula id="E5"><mml:math id="m77"><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo></mml:math><label>(5)</label></disp-formula><disp-formula id="E6"><mml:math id="m78"><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo></mml:math><label>(6)</label></disp-formula>where <italic>corr</italic>(<inline-formula><mml:math id="m79"><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) and <italic>corr</italic>(<inline-formula><mml:math id="m80"><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>) are the pairwise correlation within MZ and same-sex DZ twins at edge between <italic>i</italic> and <italic>j</italic>. Solving <xref ref-type="disp-formula" rid="E5">Equation 5</xref> and <xref ref-type="disp-formula" rid="E6">Equation 6</xref>, we obtain the additive genetic factor, that is, HI given by<disp-formula><mml:math id="m81"><mml:mtext>HI</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula></p>
      <p>The network differences between MZ and DZ twins are considered as mainly contributed to heritability and can be used to determine the statistical significance of HI (Chung et al., <xref rid="bib26" ref-type="bibr">2017</xref>, <xref rid="bib27" ref-type="bibr">2018</xref>). The KS-distance was computed by taking 1 − <italic>C</italic><sub><italic>MZ</italic></sub> and 1 − <italic>C</italic><sub><italic>DZ</italic></sub> as edge weights.</p>
      <p>In most brain imaging studies, 5,000–1,000,000 permutations are often used, which puts the total number of generated permutations to usually less than 0.01 to 1% of all possible permutations. In Zalesky et al. (<xref rid="bib92" ref-type="bibr">2010</xref>), 5,000 permutations are out of a possible <inline-formula><mml:math id="m82"><mml:mfenced open="(" close=")"><mml:mfrac linethickness="0"><mml:mrow><mml:mn>27</mml:mn></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:mfrac></mml:mfenced></mml:math></inline-formula> = 17,383,860 permutations (2.9%) used. In Thompson et al. (<xref rid="bib86" ref-type="bibr">2001</xref>), for instance, 1 million permutations out of <inline-formula><mml:math id="m83"><mml:mfenced open="(" close=")"><mml:mfrac linethickness="0"><mml:mrow><mml:mn>40</mml:mn></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:mfrac></mml:mfenced></mml:math></inline-formula> possible permutations (0.07%) were generated using a super computer. In Lee et al. (<xref rid="bib59" ref-type="bibr">2017</xref>), 5,000 permutations out of a possible <inline-formula><mml:math id="m84"><mml:mfenced open="(" close=")"><mml:mfrac linethickness="0"><mml:mrow><mml:mn>33</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mfrac></mml:mfenced></mml:math></inline-formula> = 92,561,040 permutations (0.005%) were used. Since we have 131 MZ and 77 DZ pairs, the total number of possible permutation is <inline-formula><mml:math id="m85"><mml:mfenced open="(" close=")"><mml:mfrac linethickness="0"><mml:mrow><mml:mn>271</mml:mn></mml:mrow><mml:mrow><mml:mn>131</mml:mn></mml:mrow></mml:mfrac></mml:mfenced></mml:math></inline-formula>, which is larger than 10<sup>80</sup>. Even if we generate only 0.01% of 10<sup>80</sup> of all possible permutations, 10<sup>76</sup> permutations are still too large for most desktop computers. Thus, we choose the KS-distance for measuring the network distance. Although the probability distribution of the KS-distance is actually based on the permutation test but the probability is computed combinatorially, bypassing the need for resampling. KS-distance in our study only took a few seconds to compute the <italic>p</italic> value.</p>
    </sec>
    <sec>
      <title>Results</title>
      <p>We used <italic>β</italic><sub>0</sub> and <italic>β</italic><sub>1</sub> in computing KS-distances. Let <italic>ϕ</italic> ∘ <italic>C</italic><sub><italic>MZ</italic></sub> = (<italic>ϕ</italic>(<inline-formula><mml:math id="m86"><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>)) and <italic>ϕ</italic> ∘ <italic>C</italic><sub><italic>DZ</italic></sub> = (<italic>ϕ</italic>(<inline-formula><mml:math id="m87"><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>Z</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>)) for some monotone function <italic>ϕ</italic>. Then KS-distance between <italic>C</italic><sub><italic>MZ</italic></sub> and <italic>C</italic><sub><italic>DZ</italic></sub> is equivalent to KS-distance between 1 − <italic>C</italic><sub><italic>MZ</italic></sub> and 1 − <italic>C</italic><sub><italic>DZ</italic></sub> as well as between <italic>ϕ</italic> ∘ (1 − <italic>C</italic><sub><italic>MZ</italic></sub>) and <italic>ϕ</italic> ∘ (1 − <italic>C</italic><sub><italic>DZ</italic></sub>). Thus, we simply built filtrations over <italic>C</italic><sub><italic>MZ</italic></sub> and <italic>C</italic><sub><italic>DZ</italic></sub> and computed KS-distance without using the square-root of 1 - correlation. We used 101 filtration values between 0 and 1 at 0.01 increment (<xref ref-type="fig" rid="F4">Figure 4</xref>). This gives a reasonably accurate estimate of the maximum gap in the <italic>β</italic><sub><italic>i</italic></sub>-plots between the twins (<xref ref-type="fig" rid="F5">Figure 5</xref>). For <italic>β</italic><sub>0</sub>-plots, the maximum gap is 82, which gives the <italic>p</italic> value smaller than 10<sup>−24</sup>. For <italic>β</italic><sub>1</sub>-plots, the maximum gap is 3,647, which gives the <italic>p</italic> value smaller than 10<sup>−32</sup>. At the same correlation value, MZ twins are more connected than DZ twins. Also MZ twins have more cycles than DZ twins. Such huge topological differences are contributed to heritability.</p>
      <fig id="F4" orientation="portrait" position="float">
        <label><bold>Figure 4.</bold> </label>
        <caption>
          <p>Correlation network filtration thresholded at the indicated correlation values. MZ-twins (top) shows higher correlation connections compared with DZ-twins (bottom). Such connectivity difference is contributed to heritability.</p>
        </caption>
        <graphic xlink:href="netn-03-674-g004"/>
      </fig>
      <fig id="F5" orientation="portrait" position="float">
        <label><bold>Figure 5.</bold> </label>
        <caption>
          <p><xref rid="def8" ref-type="def">Betti-plots</xref> showing Betti numbers over correlation <italic>ϵ</italic> as filtration. MZ twins (top) shows more higher correlation connections and cycles compared with DZ twins (bottom).</p>
        </caption>
        <graphic xlink:href="netn-03-674-g005"/>
      </fig>
      <p><xref ref-type="fig" rid="F6">Figure 6</xref>, which displays the HI index thresholded at 100% heritability, shows MZ twins far more similar compared with DZ twins in many connections, suggesting that genes influence the development of these connections. The most heritable connections include the left frontal gyrus, left and right middle frontal gyri, left superior frontal gyrus, left parahippocampal gyrus, left and right thalami, left and right caudate, and nuclei among many other regions. Most regions overlap with highly heritable regions observed in other twins brain-imaging studies (Fan, Fossella, Sommer, Wu, &amp; Posner, <xref rid="bib38" ref-type="bibr">2003</xref>; Glahn et al., <xref rid="bib42" ref-type="bibr">2010</xref>; Gritsenko et al., <xref rid="bib43" ref-type="bibr">2018</xref>). Moreover, the findings here are somewhat consistent with a previous study on diffusion tensor imaging on twins from our group (Chung, Luo, Adluru, Alexander, Richard, &amp; Goldsmith, <xref rid="bib27" ref-type="bibr">2018a</xref>; Chung et al., <xref rid="bib28" ref-type="bibr">2018b</xref>), showing that many regions of both resting-state functional and structural connections are heritable at the same time. The left and right caudate nuclei are identified as the most heritable hub nodes in our study.</p>
      <fig id="F6" orientation="portrait" position="float">
        <label><bold>Figure 6.</bold> </label>
        <caption>
          <p>Most highly heritable connections. The connections with 100% heritability are only shown.</p>
        </caption>
        <graphic xlink:href="netn-03-674-g006"/>
      </fig>
      <p>The <monospace>MATLAB</monospace> codes for the simulation study as well as the connectivity matrices <italic>C</italic><sub><italic>MZ</italic></sub> and <italic>C</italic><sub><italic>DZ</italic></sub> used in generating results are given at <ext-link ext-link-type="uri" xlink:href="http://www.stat.wisc.edu/~mchung/TDA">http://www.stat.wisc.edu/∼mchung/TDA</ext-link>.</p>
    </sec>
  </sec>
  <sec>
    <title>DISCUSSION</title>
    <sec>
      <title>The Limitation of KS-distances</title>
      <p>Currently KS-distance is applied to Betti numbers <italic>β</italic><sub>0</sub> and <italic>β</italic><sub>1</sub> separately. It may be possible to construct a new topological distance that uses the combination of both <italic>β</italic><sub>0</sub> and <italic>β</italic><sub>1</sub> and come up with topologically more sensitive distances. One possible approach is to use the convex combination <italic>α</italic><inline-formula><mml:math id="m88"><mml:msubsup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> + (1 − <italic>α</italic>)<inline-formula><mml:math id="m89"><mml:msubsup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>, where <inline-formula><mml:math id="m90"><mml:msubsup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is KS-distance for <italic>β</italic><sub><italic>i</italic></sub> and 0 ≤ <italic>α</italic> ≤ 1. This is beyond the scope of this paper and left as a future study.</p>
    </sec>
    <sec>
      <title>Other Network Distances</title>
      <p>The network distances used in this study are not just any other distances but metrics. Since there are almost infinitely many possible similarity measures and distances we can use in networks, the performance of the chosen distance is important in discrimination tasks, which we have shown in simulation studies. The determination of the optimal distance is related to <italic>metric learning</italic>, an area of supervised machine learning in which the goal is to learn from data an optimal similarity function that measures how similar two objects are (Ktena, Parisot, Ferrante, Rajchl, Lee, Glocker, &amp; Rueckert, <xref rid="bib52" ref-type="bibr">2018</xref>; Lowe, <xref rid="bib62" ref-type="bibr">1995</xref>). This is left as a future study.</p>
    </sec>
    <sec>
      <title>Computational Issues</title>
      <p>The total number of permutations in permuting two groups of size <italic>q</italic> each is <inline-formula><mml:math id="m91"><mml:mfenced open="(" close=")"><mml:mfrac linethickness="0"><mml:mrow><mml:mn>2</mml:mn><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mfrac></mml:mfenced></mml:math></inline-formula> ∼ <inline-formula><mml:math id="m92"><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:math></inline-formula>. Even for small <italic>q</italic> = 10, more than tens of thousands of permutations are needed for the accurate approximation of the <italic>p</italic> value. The main advantage of KS-distance over all other distance measures is that it avoids numerically performing the permutation test and avoids generating tens of thousands of permutations. Although the probability distribution of the KS-distance is actually based on the permutation test, the probability is computed combinatorially. We believe that it is possible to develop similar theoretical results for other distance measures and come up with a method for avoiding a resampling-based method for statistical inference.</p>
    </sec>
  </sec>
  <sec>
    <title>ACKNOWLEDGMENTS</title>
    <p>We thank Yuan Wang of University of South Carolina, Peter Bebunik of University of Florida, Bala Krishnamoorthy of Washington State University, Dustin Pluta of University of California-Irvine, Alex Leow of University of Illinois-Chicago, and Martin Lindquist of Johns Hopkins University for valuable discussions. We also thank Andrey Gritsenko and Gregory Kirk of University of Wisconsin-Madison for logistic support and image preprocessing help.</p>
  </sec>
  <sec>
    <title>AUTHOR CONTRIBUTIONS</title>
    <p>Moo Chung: Conceptualization; Data curation; Formal analysis; Funding acquisition; Investigation; Methodology; Project administration; Resources; Software; Supervision; Validation; Visualization; Writing - Original Draft; Writing - Review &amp; Editing. Hyekyoung Lee: Investigation; Methodology; Validation; Visualization; Writing - Original Draft. Alex DiChristofano: Investigation. Hernando Ombao: Writing - Review &amp; Editing. Victor Solo: Conceptualization; Methodology; Writing - Review &amp; Editing.</p>
  </sec>
  <sec>
    <title>FUNDING INFORMATION</title>
    <p>Moo Chung, National Institutes of Health (<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.13039/100000002">http://dx.doi.org/10.13039/100000002</ext-link>), Award ID: EB022856. Hyekyoung Lee, National Research Foundation of Korea (<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.13039/501100003725">http://dx.doi.org/10.13039/501100003725</ext-link>), Award ID: NRF-2016R1D1A1B03935463.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>TECHNICAL TERMS</title>
    <def-list>
      <def-item id="def1">
        <term>Persistent homology:</term>
        <def>
          <p>A topological data analysis technique for computing topological features at different spatial resolutions.</p>
        </def>
      </def-item>
      <def-item id="def2">
        <term>Graph filtration:</term>
        <def>
          <p>A collection of nested graphs.</p>
        </def>
      </def-item>
      <def-item id="def3">
        <term>Metric space:</term>
        <def>
          <p>A set with a metric defined on the set.</p>
        </def>
      </def-item>
      <def-item id="def4">
        <term>Permutation test:</term>
        <def>
          <p>Determines the statistical significance by calculating all possible values of the test statistic under all possible rearrangements of the samples.</p>
        </def>
      </def-item>
      <def-item id="def5">
        <term>Kolmogorov-Smirnov (KS) distance:</term>
        <def>
          <p>A distance between the empirical distributions of two samples.</p>
        </def>
      </def-item>
      <def-item id="def6">
        <term>Mixed-effect model:</term>
        <def>
          <p>A model with both fixed and random effect terms.</p>
        </def>
      </def-item>
      <def-item id="def7">
        <term>Heritability index:</term>
        <def>
          <p>A number between 0 and 1 that measures the amount of genetic contribution.</p>
        </def>
      </def-item>
      <def-item id="def8">
        <term>Betti-plots:</term>
        <def>
          <p>Displays the change of Betti numbers over filtration values.</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ref-list>
    <title>REFERENCES</title>
    <ref id="bib1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Achard</surname><given-names>S.</given-names></name>, &amp; <name name-style="western"><surname>Bullmore</surname><given-names>E.</given-names></name></person-group> (<year>2007</year>). <article-title>Efficiency and cost of economical brain functional networks</article-title>. <source>PLoS Computational Biology</source>, <volume>3</volume>(<issue>2</issue>), <fpage>e17</fpage>.<pub-id pub-id-type="pmid">17274684</pub-id></mixed-citation>
    </ref>
    <ref id="bib2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Achard</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Salvador</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Whitcher</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Suckling</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Bullmore</surname><given-names>E.</given-names></name></person-group> (<year>2006</year>). <article-title>A resilient, low-frequency, small-world human brain functional network with highly connected association cortical hubs</article-title>. <source>The Journal of Neuroscience</source>, <volume>26</volume>, <fpage>63</fpage>–<lpage>72</lpage>.<pub-id pub-id-type="pmid">16399673</pub-id></mixed-citation>
    </ref>
    <ref id="bib3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Adams</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Tausz</surname><given-names>A.</given-names></name>, &amp; <name name-style="western"><surname>Vejdemo-Johansson</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>). <article-title>javaPlex: A research software package for persistent (co) homology</article-title>. In <source>Mathematical Software–icms 2014</source> (pp. <fpage>129</fpage>–<lpage>136</lpage>). <publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Adler</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Bobrowski</surname><given-names>O.</given-names></name>, <name name-style="western"><surname>Borman</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Subag</surname><given-names>E.</given-names></name>, &amp; <name name-style="western"><surname>Weinberger</surname><given-names>S.</given-names></name></person-group> (<year>2010</year>). <article-title>Persistent homology for random fields and complexes</article-title>. In <source>Borrowing Strength: Theory Powering Applications–A festschrift for Lawrence D. Brown</source> (pp. <fpage>124</fpage>–<lpage>143</lpage>). <publisher-name>Institute of Mathematical Statistics</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bassett</surname><given-names>D.</given-names></name></person-group> (<year>2006</year>). <article-title>Small-world brain networks</article-title>. <source>The Neuroscientist</source>, <volume>12</volume>, <fpage>512</fpage>–<lpage>523</lpage>.<pub-id pub-id-type="pmid">17079517</pub-id></mixed-citation>
    </ref>
    <ref id="bib6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bassett</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Meyer-Lindenberg</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Achard</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Duke</surname><given-names>T.</given-names></name>, &amp; <name name-style="western"><surname>Bullmore</surname><given-names>E.</given-names></name></person-group> (<year>2006</year>). <article-title>Adaptive reconfiguration of fractal small-world human brain functional networks</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>103</volume>, <fpage>19518</fpage>–<lpage>19523</lpage>.</mixed-citation>
    </ref>
    <ref id="bib7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bien</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Tibshirani</surname><given-names>R.</given-names></name></person-group> (<year>2011</year>). <article-title>Hierarchical clustering with prototypes via minimax linkage</article-title>. <source>Journal of American Statistical Association</source>, <volume>106</volume>, <fpage>1075</fpage>–<lpage>1084</lpage>.</mixed-citation>
    </ref>
    <ref id="bib8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Blokland</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>McMahon</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Thompson</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Martin</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>de Zubicaray</surname><given-names>G.</given-names></name>, &amp; <name name-style="western"><surname>Wright</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <article-title>Heritability of working memory brain activation</article-title>. <source>The Journal of Neuroscience</source>, <volume>31</volume>, <fpage>10882</fpage>–<lpage>10890</lpage>.<pub-id pub-id-type="pmid">21795540</pub-id></mixed-citation>
    </ref>
    <ref id="bib9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bobrowski</surname><given-names>O.</given-names></name>, &amp; <name name-style="western"><surname>Kahle</surname><given-names>M.</given-names></name></person-group> (<year>2014</year>). <article-title>Topology of random geometric complexes: A survey</article-title>. <source>Journal of Applied and Computational Topology</source>, <fpage>1</fpage>–<lpage>34</lpage>.</mixed-citation>
    </ref>
    <ref id="bib10">
      <mixed-citation publication-type="book"><person-group person-group-type="editor"><name name-style="western"><surname>Boissonnat</surname><given-names>J.-D.</given-names></name>, &amp; <name name-style="western"><surname>Teillaud</surname><given-names>M.</given-names></name></person-group> (Eds.). (<year>2006</year>). <source>Effective computational geometry for curves and surfaces</source>. <publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bullmore</surname><given-names>E.</given-names></name>, &amp; <name name-style="western"><surname>Sporns</surname><given-names>O.</given-names></name></person-group> (<year>2009</year>). <article-title>Complex brain networks: Graph theoretical analysis of structural and functional systems</article-title>. <source>Nature Review Neuroscience</source>, <volume>10</volume>, <fpage>186</fpage>–<lpage>198</lpage>.<pub-id pub-id-type="pmid">19190637</pub-id></mixed-citation>
    </ref>
    <ref id="bib12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Carlsson</surname><given-names>G.</given-names></name></person-group> (<year>2009</year>). <article-title>Topology and data</article-title>. <source>Bulletin of the American Mathematical Society</source>, <volume>46</volume>, <fpage>255</fpage>–<lpage>308</lpage>.</mixed-citation>
    </ref>
    <ref id="bib13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Carlsson</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>De Silva</surname><given-names>V.</given-names></name>, &amp; <name name-style="western"><surname>Morozov</surname><given-names>D.</given-names></name></person-group> (<year>2009</year>). <article-title>Zigzag persistent homology and real-valued functions</article-title>. In <source>Proceedings of the Twenty-fifth Annual Symposium on Computational Geometry</source> (pp. <fpage>247</fpage>–<lpage>256</lpage>).</mixed-citation>
    </ref>
    <ref id="bib14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Carlsson</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Ishkhanov</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>De Silva</surname><given-names>V.</given-names></name>, &amp; <name name-style="western"><surname>Zomorodian</surname><given-names>A.</given-names></name></person-group> (<year>2008</year>). <article-title>On the local behavior of spaces of natural images</article-title>. <source>International Journal of Computer Vision</source>, <volume>76</volume>, <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation>
    </ref>
    <ref id="bib15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Carlsson</surname><given-names>G.</given-names></name>, &amp; <name name-style="western"><surname>Memoli</surname><given-names>F.</given-names></name></person-group> (<year>2008</year>). <article-title>Persistent clustering and a theorem of J. Kleinberg</article-title>. <source>arXiv Preprint arXiv:0808.2241</source>.</mixed-citation>
    </ref>
    <ref id="bib16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Carlsson</surname><given-names>G.</given-names></name>, &amp; <name name-style="western"><surname>Mémoli</surname><given-names>F.</given-names></name></person-group> (<year>2010</year>). <article-title>Characterization, stability and convergence of hierarchical clustering methods</article-title>. <source>Journal of Machine Learning Research</source>, <volume>11</volume>, <fpage>1425</fpage>–<lpage>1470</lpage>.</mixed-citation>
    </ref>
    <ref id="bib17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Carlsson</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Singh</surname><given-names>G.</given-names></name>, &amp; <name name-style="western"><surname>Zomorodian</surname><given-names>A.</given-names></name></person-group> (<year>2009</year>). <article-title>Computing multidimensional persistence</article-title>. In <source>International Symposium on Algorithms and Computation</source> (pp. <fpage>730</fpage>–<lpage>739</lpage>).</mixed-citation>
    </ref>
    <ref id="bib18">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Cassidy</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Rae</surname><given-names>C.</given-names></name>, &amp; <name name-style="western"><surname>Solo</surname><given-names>V.</given-names></name></person-group> (<year>2015</year>). <article-title>Brain activity: Conditional dissimilarity and persistent homology</article-title>. In <source>IEEE 12th International Symposium on Biomedical Imaging (ISBI)</source> (pp. <fpage>1356</fpage>–<lpage>1359</lpage>).</mixed-citation>
    </ref>
    <ref id="bib19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chazal</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Cohen-Steiner</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Guibas</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Mémoli</surname><given-names>F.</given-names></name>, &amp; <name name-style="western"><surname>Oudot</surname><given-names>S.</given-names></name></person-group> (<year>2009</year>). <article-title>Gromov-Hausdorff stable signatures for shapes using persistence</article-title>. In <source>Computer Graphics Forum</source> (<volume>Vol. 28</volume>, pp. <fpage>1393</fpage>–<lpage>1403</lpage>).</mixed-citation>
    </ref>
    <ref id="bib20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chiang</surname><given-names>M.-C.</given-names></name>, <name name-style="western"><surname>McMahon</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>de Zubicaray</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Martin</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Hickie</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Toga</surname><given-names>A.</given-names></name>, … <name name-style="western"><surname>Thompson</surname><given-names>P.</given-names></name></person-group> (<year>2011</year>). <article-title>Genetics of white matter development: A DTI study of 705 twins and their siblings aged 12 to 29</article-title>. <source>NeuroImage</source>, <volume>54</volume>, <fpage>2308</fpage>–<lpage>2317</lpage>.<pub-id pub-id-type="pmid">20950689</pub-id></mixed-citation>
    </ref>
    <ref id="bib21">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chowdhury</surname><given-names>S.</given-names></name>, &amp; <name name-style="western"><surname>Mémoli</surname><given-names>F.</given-names></name></person-group> (<year>2016</year>). <article-title>Persistent homology of directed networks</article-title>. In <source>Signals, Systems and Computers, 2016 50th Asilomar Conference on</source> (pp. <fpage>77</fpage>–<lpage>81</lpage>).</mixed-citation>
    </ref>
    <ref id="bib22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Adluru</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Dalton</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Alexander</surname><given-names>A.</given-names></name>, &amp; <name name-style="western"><surname>Davidson</surname><given-names>R.</given-names></name></person-group> (<year>2011</year>). <article-title>Scalable brain network construction on white matter fibers</article-title>. In <source>Proceedings of SPIE</source> (<volume>Vol. 7962</volume>, p. <fpage>79624G</fpage>).</mixed-citation>
    </ref>
    <ref id="bib23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Adluru</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Lazar</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Lainhart</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Alexander</surname><given-names>A.</given-names></name></person-group> (<year>2010</year>). <article-title>Cosine series representation of 3D curves and its application to white matter fiber bundles in diffusion tensor imaging</article-title>. <source>Statistics and Its Interface</source>, <volume>3</volume>, <fpage>69</fpage>–<lpage>80</lpage>.<pub-id pub-id-type="pmid">23316267</pub-id></mixed-citation>
    </ref>
    <ref id="bib24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Hanson</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Adluru</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Alexander</surname><given-names>A. L.</given-names></name>, <name name-style="western"><surname>Davidson</surname><given-names>R.</given-names></name>, &amp; <name name-style="western"><surname>Pollak</surname><given-names>S.</given-names></name></person-group> (<year>2013</year>). <article-title>Persistent homological sparse network approach to detecting white matter abnormality in maltreated children: MRI and DTI multimodal study</article-title>. <source>MICCAI, Lecture Notes in Computer Science (LNCS)</source>, <volume>8149</volume>, <fpage>300</fpage>–<lpage>307</lpage>.</mixed-citation>
    </ref>
    <ref id="bib25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Hanson</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Ye</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Davidson</surname><given-names>R.</given-names></name>, &amp; <name name-style="western"><surname>Pollak</surname><given-names>S.</given-names></name></person-group> (<year>2015</year>). <article-title>Persistent homology in sparse regression and its application to brain morphometry</article-title>. <source>IEEE Transactions on Medical Imaging</source>, <volume>34</volume>, <fpage>1928</fpage>–<lpage>1939</lpage>.<pub-id pub-id-type="pmid">25823032</pub-id></mixed-citation>
    </ref>
    <ref id="bib26">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Solo</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Davidson</surname><given-names>R.</given-names></name>, &amp; <name name-style="western"><surname>Pollak</surname><given-names>S.</given-names></name></person-group> (<year>2017a</year>). <article-title>Topological distances between brain networks</article-title>. <source>International Workshop on Connectomics in Neuroimaging, Lecture Notes in Computer Science</source>, <fpage>161</fpage>–<lpage>170</lpage>.</mixed-citation>
    </ref>
    <ref id="bib27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Luo</surname><given-names>Z.</given-names></name>, <name name-style="western"><surname>Adluru</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Alexander</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Richard</surname><given-names>D.</given-names></name>, &amp; <name name-style="western"><surname>Goldsmith</surname><given-names>H.</given-names></name></person-group> (<year>2018a</year>). <article-title>Heritability of hierarchical structural brain network</article-title>. <source>International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source>, <fpage>554</fpage>–<lpage>557</lpage>.</mixed-citation>
    </ref>
    <ref id="bib28">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Luo</surname><given-names>Z.</given-names></name>, <name name-style="western"><surname>Leow</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Adluru</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Alexander</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Richard</surname><given-names>D.</given-names></name>, &amp; <name name-style="western"><surname>Goldsmith</surname><given-names>H.</given-names></name></person-group> (<year>2018b</year>). <article-title>Exact combinatorial inference for brain images</article-title>. In <source>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</source> (pp. <fpage>629</fpage>–<lpage>637</lpage>).</mixed-citation>
    </ref>
    <ref id="bib29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Vilalta-Gil</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Rathouz</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Lahey</surname><given-names>B.</given-names></name>, &amp; <name name-style="western"><surname>Zald</surname><given-names>D.</given-names></name></person-group> (<year>2017b</year>). <article-title>Exact topological inference for paired brain networks via persistent homology</article-title>. <source>Information Processing in Medical Imaging (IPMI), Lecture Notes in Computer Science (LNCS)</source>, <volume>10265</volume>, <fpage>299</fpage>–<lpage>310</lpage>.</mixed-citation>
    </ref>
    <ref id="bib30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cohen-Steiner</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Edelsbrunner</surname><given-names>H.</given-names></name>, &amp; <name name-style="western"><surname>Harer</surname><given-names>J.</given-names></name></person-group> (<year>2007</year>). <article-title>Stability of persistence diagrams</article-title>. <source>Discrete and Computational Geometry</source>, <volume>37</volume>, <fpage>103</fpage>–<lpage>120</lpage>.</mixed-citation>
    </ref>
    <ref id="bib31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>de Silva</surname><given-names>V.</given-names></name>, &amp; <name name-style="western"><surname>Ghrist</surname><given-names>R.</given-names></name></person-group> (<year>2007</year>). <article-title>Homological sensor networks</article-title>. <source>Notices of the American Mathematical Society</source>, <volume>54</volume>, <fpage>10</fpage>–<lpage>17</lpage>.</mixed-citation>
    </ref>
    <ref id="bib32">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Di Fabio</surname><given-names>B.</given-names></name>, &amp; <name name-style="western"><surname>Ferri</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>Comparing persistence diagrams through complex vectors</article-title>. In <source>International Conference on Image Analysis and Processing</source> (pp. <fpage>294</fpage>–<lpage>305</lpage>).</mixed-citation>
    </ref>
    <ref id="bib33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Edelsbrunner</surname><given-names>H.</given-names></name>, &amp; <name name-style="western"><surname>Harer</surname><given-names>J.</given-names></name></person-group> (<year>2008</year>). <article-title>Persistent homology—a survey</article-title>. <source>Contemporary Mathematics</source>, <volume>453</volume>, <fpage>257</fpage>–<lpage>282</lpage>.</mixed-citation>
    </ref>
    <ref id="bib34">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Edelsbrunner</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Letscher</surname><given-names>D.</given-names></name>, &amp; <name name-style="western"><surname>Zomorodian</surname><given-names>A.</given-names></name></person-group> (<year>2000</year>). <article-title>Topological persistence and simplification</article-title>. In <source>Foundations of Computer Science, 2000. Proceedings. 41st Annual Symposium on</source> (pp. <fpage>454</fpage>–<lpage>463</lpage>).</mixed-citation>
    </ref>
    <ref id="bib35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Edelsbrunner</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Letscher</surname><given-names>D.</given-names></name>, &amp; <name name-style="western"><surname>Zomorodian</surname><given-names>A.</given-names></name></person-group> (<year>2002</year>). <article-title>Topological persistence and simplification</article-title>. <source>Discrete and Computational Geometry</source>, <volume>28</volume>, <fpage>511</fpage>–<lpage>533</lpage>.</mixed-citation>
    </ref>
    <ref id="bib36">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Efron</surname><given-names>B.</given-names></name></person-group> (<year>1982</year>). <source>The Jackknife, the Bootstrap and Other Resampling Plans</source> (<volume>Vol. 38</volume>). <publisher-name>SIAM</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib37">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Falconer</surname><given-names>D.</given-names></name>, &amp; <name name-style="western"><surname>Mackay</surname><given-names>T.</given-names></name></person-group> (<year>1995</year>). <source>Introduction to Quantitative Genetics</source>, (<edition>4th ed.</edition>). <publisher-name>Longman</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Fossella</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Sommer</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>Y.</given-names></name>, &amp; <name name-style="western"><surname>Posner</surname><given-names>M.</given-names></name></person-group> (<year>2003</year>). <article-title>Mapping the genetic variation of executive attention onto brain activity</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>100</volume>, <fpage>7406</fpage>–<lpage>7411</lpage>.</mixed-citation>
    </ref>
    <ref id="bib39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ghrist</surname><given-names>R.</given-names></name></person-group> (<year>2008</year>). <article-title>Barcodes: The persistent topology of data</article-title>. <source>Bulletin of the American Mathematical Society</source>, <volume>45</volume>, <fpage>61</fpage>–<lpage>75</lpage>.</mixed-citation>
    </ref>
    <ref id="bib40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Girvan</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Newman</surname><given-names>M.</given-names></name></person-group> (<year>2002</year>). <article-title>Community structure in social and biological networks</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>99</volume>, <fpage>7821</fpage>.</mixed-citation>
    </ref>
    <ref id="bib41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Giusti</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Pastalkova</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Curto</surname><given-names>C.</given-names></name>, &amp; <name name-style="western"><surname>Itskov</surname><given-names>V.</given-names></name></person-group> (<year>2015</year>). <article-title>Clique topology reveals intrinsic geometric structure in neural correlations</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>112</volume>, <fpage>13455</fpage>–<lpage>13460</lpage>.</mixed-citation>
    </ref>
    <ref id="bib42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Glahn</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Winkler</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Kochunov</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Almasy</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Duggirala</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Carless</surname><given-names>M.</given-names></name>, … <name name-style="western"><surname>Blangero</surname><given-names>J.</given-names></name></person-group> (<year>2010</year>). <article-title>Genetic control over the resting brain</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>107</volume>, <fpage>1223</fpage>–<lpage>1228</lpage>.</mixed-citation>
    </ref>
    <ref id="bib43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gritsenko</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Lindquist</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Kirk</surname><given-names>G.</given-names></name>, &amp; <name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name></person-group> (<year>2018</year>). <article-title>Hill climbing optimized twin classification using resting-state functional MRI</article-title>. <source>arXiv</source>, <fpage>1807.00244</fpage>
<comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.00244">https://arxiv.org/abs/1807.00244</ext-link></comment></mixed-citation>
    </ref>
    <ref id="bib44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name>, &amp; <name name-style="western"><surname>Evans</surname><given-names>A.</given-names></name></person-group> (<year>2008</year>). <article-title>Structural insights into aberrant topological patterns of large-scale cortical networks in Alzheimer’s disease</article-title>. <source>Journal of Neuroscience</source>, <volume>28</volume>, <fpage>4756</fpage>.<pub-id pub-id-type="pmid">18448652</pub-id></mixed-citation>
    </ref>
    <ref id="bib45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Horak</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Maletić</surname><given-names>S.</given-names></name>, &amp; <name name-style="western"><surname>Rajković</surname><given-names>M.</given-names></name></person-group> (<year>2009</year>). <article-title>Persistent homology of complex networks</article-title>. <source>Journal of Statistical Mechanics: Theory and Experiment</source>, <volume>2009</volume>, <fpage>P03034</fpage>.</mixed-citation>
    </ref>
    <ref id="bib46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ibanez-Marcelo</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Campioni</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Manzoni</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Santarcangelo</surname><given-names>E.</given-names></name>, &amp; <name name-style="western"><surname>Petri</surname><given-names>G.</given-names></name></person-group> (<year>2018a</year>). <article-title>Spectral and topological analysis of the cortical representation of the head position: Does hypnotizability matter?</article-title>
<source>bioRxiv</source>, <fpage>442053</fpage>.</mixed-citation>
    </ref>
    <ref id="bib47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ibanez-Marcelo</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Campioni</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Phinyomark</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Petri</surname><given-names>G.</given-names></name>, &amp; <name name-style="western"><surname>Santarcangelo</surname><given-names>E.</given-names></name></person-group> (<year>2018b</year>). <article-title>Topology highlights mesoscopic functional equivalence between imagery and perception</article-title>. <source>bioRxiv</source>, <fpage>268383</fpage>.</mixed-citation>
    </ref>
    <ref id="bib48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kerber</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Morozov</surname><given-names>D.</given-names></name>, &amp; <name name-style="western"><surname>Nigmetov</surname><given-names>A.</given-names></name></person-group> (<year>2017</year>). <article-title>Geometry helps to compare persistence diagrams</article-title>. <source>Journal of Experimental Algorithmics</source>, <volume>22</volume>.</mixed-citation>
    </ref>
    <ref id="bib49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khalid</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Ye</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Jeon</surname><given-names>D.</given-names></name></person-group> (<year>2014</year>). <article-title>Tracing the evolution of multi-scale functional networks in a mouse model of depression using persistent brain network homology</article-title>. <source>NeuroImage</source>, <volume>101</volume>, <fpage>351</fpage>–<lpage>363</lpage>.<pub-id pub-id-type="pmid">25064667</pub-id></mixed-citation>
    </ref>
    <ref id="bib50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Adluru</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Okonkwo</surname><given-names>O.</given-names></name>, <name name-style="western"><surname>Johnson</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Bendlin</surname><given-names>B.</given-names></name>, &amp; <name name-style="western"><surname>Singh</surname><given-names>V.</given-names></name></person-group> (<year>2015</year>). <article-title>Multi-resolution statistical analysis of brain connectivity graphs in preclinical Alzheimer’s disease</article-title>. <source>NeuroImage</source>, <volume>118</volume>, <fpage>103</fpage>–<lpage>117</lpage>.<pub-id pub-id-type="pmid">26025289</pub-id></mixed-citation>
    </ref>
    <ref id="bib51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kondor</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Howard</surname><given-names>A.</given-names></name>, &amp; <name name-style="western"><surname>Jebara</surname><given-names>T.</given-names></name></person-group> (<year>2007</year>). <article-title>Multi-object tracking with representations of the symmetric group</article-title>. In <source>International Conference on Artificial Intelligence and Statistics (Aistats)</source> (<volume>Vol. 1</volume>, p. <fpage>5</fpage>).</mixed-citation>
    </ref>
    <ref id="bib52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ktena</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Parisot</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Ferrante</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Rajchl</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Glocker</surname><given-names>B.</given-names></name>, &amp; <name name-style="western"><surname>Rueckert</surname><given-names>D.</given-names></name></person-group> (<year>2018</year>). <article-title>Metric learning with spectral graph convolutions on brain connectivity networks</article-title>. <source>NeuroImage</source>, <volume>169</volume>, <fpage>431</fpage>–<lpage>442</lpage>.<pub-id pub-id-type="pmid">29278772</pub-id></mixed-citation>
    </ref>
    <ref id="bib53">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Kusano</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Hiraoka</surname><given-names>Y.</given-names></name>, &amp; <name name-style="western"><surname>Fukumizu</surname><given-names>K.</given-names></name></person-group> (<year>2016</year>). <article-title>Persistence weighted Gaussian kernel for topological data analysis</article-title>. In <source>International Conference on Machine Learning</source> (pp. <fpage>2004</fpage>–<lpage>2013</lpage>).</mixed-citation>
    </ref>
    <ref id="bib54">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Kang</surname><given-names>H.</given-names></name>, &amp; <name name-style="western"><surname>Lee</surname><given-names>D.</given-names></name></person-group> (<year>2014</year>). <article-title>Hole detection in metabolic connectivity of Alzheimer’s disease using k-Laplacian</article-title>. In <source>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), Lecture Notes in Computer Science</source> (pp. <fpage>297</fpage>–<lpage>304</lpage>).</mixed-citation>
    </ref>
    <ref id="bib55">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Kang</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Choi</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>Y.</given-names></name>, &amp; <name name-style="western"><surname>Lee</surname><given-names>D.</given-names></name></person-group> (<year>2018</year>). <article-title>Abnormal hole detection in brain connectivity by kernel density of persistence diagram and Hodge Laplacian</article-title>. In <source>IEEE International Symposium on Biomedical Imaging (ISBI)</source> (pp. <fpage>20</fpage>–<lpage>23</lpage>).</mixed-citation>
    </ref>
    <ref id="bib56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Kang</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>B.-N.</given-names></name>, &amp; <name name-style="western"><surname>Lee</surname><given-names>D.</given-names></name></person-group> (<year>2011a</year>). <article-title>Computing the shape of brain networks using graph filtration and Gromov-Hausdorff metric</article-title>. <source>MICCAI, Lecture Notes in Computer Science</source>, <volume>6892</volume>, <fpage>302</fpage>–<lpage>309</lpage>.</mixed-citation>
    </ref>
    <ref id="bib57">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Kang</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>B.-N.</given-names></name>, &amp; <name name-style="western"><surname>Lee</surname><given-names>D.</given-names></name></person-group> (<year>2011b</year>). <article-title>Discriminative persistent homology of brain networks</article-title>. In <source>IEEE International Symposium on Biomedical Imaging (ISBI)</source> (pp. <fpage>841</fpage>–<lpage>844</lpage>).</mixed-citation>
    </ref>
    <ref id="bib58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Kang</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>B.-N.</given-names></name>, &amp; <name name-style="western"><surname>Lee</surname><given-names>D.</given-names></name></person-group> (<year>2012</year>). <article-title>Persistent brain network homology from the perspective of dendrogram</article-title>. <source>IEEE Transactions on Medical Imaging</source>, <volume>31</volume>, <fpage>2267</fpage>–<lpage>2277</lpage>.<pub-id pub-id-type="pmid">23008247</pub-id></mixed-citation>
    </ref>
    <ref id="bib59">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Kang</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Chung</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Lim</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>B.-N.</given-names></name>, &amp; <name name-style="western"><surname>Lee</surname><given-names>D.</given-names></name></person-group> (<year>2017</year>). <article-title>Integrated multimodal network approach to PET and MRI based on multidimensional persistent homology</article-title>. <source>Human Brain Mapping</source>, <volume>38</volume>, <fpage>1387</fpage>–<lpage>1402</lpage>.<pub-id pub-id-type="pmid">27859919</pub-id></mixed-citation>
    </ref>
    <ref id="bib60">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Qin</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Yu</surname><given-names>C.</given-names></name>, &amp; <name name-style="western"><surname>Jiang</surname><given-names>T.</given-names></name></person-group> (<year>2009</year>). <article-title>Brain anatomical network and intelligence</article-title>. <source>PLoS Computational Biology</source>, <volume>5</volume>(<issue>5</issue>), <fpage>e1000395</fpage>.<pub-id pub-id-type="pmid">19492086</pub-id></mixed-citation>
    </ref>
    <ref id="bib61">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lind</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Gonzalez</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Herrmann</surname><given-names>H.</given-names></name></person-group> (<year>2005</year>). <article-title>Cycles and clustering in bipartite networks</article-title>. <source>Physical Review E</source>, <volume>72</volume>, <fpage>056127</fpage>.</mixed-citation>
    </ref>
    <ref id="bib62">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lowe</surname><given-names>D.</given-names></name></person-group> (<year>1995</year>). <article-title>Similarity metric learning for a variable-kernel classifier</article-title>. <source>Neural Computation</source>, <volume>7</volume>, <fpage>72</fpage>–<lpage>85</lpage>.</mixed-citation>
    </ref>
    <ref id="bib63">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McKay</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Knowles</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Winkler</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Sprooten</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Kochunov</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Olvera</surname><given-names>R.</given-names></name>, … <name name-style="western"><surname>Glahn</surname><given-names>D.</given-names></name></person-group> (<year>2014</year>). <article-title>Influence of age, sex and genetic factors on the human brain</article-title>. <source>Brain Imaging and Behavior</source>, <volume>8</volume>, <fpage>143</fpage>–<lpage>152</lpage>.<pub-id pub-id-type="pmid">24297733</pub-id></mixed-citation>
    </ref>
    <ref id="bib64">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McIntosh</surname><given-names>A.</given-names></name>, &amp; <name name-style="western"><surname>Gonzalez-Lima</surname><given-names>F.</given-names></name></person-group> (<year>1994</year>). <article-title>Structural equation modeling and its application to network analysis in functional brain imaging</article-title>. <source>Human Brain Mapping</source>, <volume>2</volume>, <fpage>2</fpage>–<lpage>22</lpage>.</mixed-citation>
    </ref>
    <ref id="bib65">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Meunier</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Lambiotte</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Fornito</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Ersche</surname><given-names>K.</given-names></name>, &amp; <name name-style="western"><surname>Bullmore</surname><given-names>E.</given-names></name></person-group> (<year>2009</year>). <article-title>Hierarchical modularity in human brain functional networks</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>3:37</volume>.</mixed-citation>
    </ref>
    <ref id="bib66">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Newman</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Barabasi</surname><given-names>A.</given-names></name>, &amp; <name name-style="western"><surname>Watts</surname><given-names>D.</given-names></name></person-group> (<year>2006</year>). <source>The Structure and Dynamics of Networks</source>. <publisher-name>Princeton University Press</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib67">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Newman</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Watts</surname><given-names>D.</given-names></name></person-group> (<year>1999</year>). <article-title>Scaling and percolation in the small-world network model</article-title>. <source>Physical Review E</source>, <volume>60</volume>, <fpage>7332</fpage>–<lpage>7342</lpage>.</mixed-citation>
    </ref>
    <ref id="bib68">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nichols</surname><given-names>T.</given-names></name>, &amp; <name name-style="western"><surname>Holmes</surname><given-names>A.</given-names></name></person-group> (<year>2002</year>). <article-title>Nonparametric permutation tests for functional neuroimaging: A primer with examples</article-title>. <source>Human Brain Mapping</source>, <volume>15</volume>, <fpage>1</fpage>–<lpage>25</lpage>.<pub-id pub-id-type="pmid">11747097</pub-id></mixed-citation>
    </ref>
    <ref id="bib69">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Petri</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Expert</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Turkheimer</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Carhart-Harris</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Nutt</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Hellyer</surname><given-names>P.</given-names></name>, &amp; <name name-style="western"><surname>Vaccarino</surname><given-names>F.</given-names></name></person-group> (<year>2014</year>). <article-title>Homological scaffolds of brain functional networks</article-title>. <source>Journal of The Royal Society Interface</source>, <volume>11</volume>, <fpage>20140873</fpage>.</mixed-citation>
    </ref>
    <ref id="bib70">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Petri</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Scolamiero</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Donato</surname><given-names>I.</given-names></name>, &amp; <name name-style="western"><surname>Vaccarino</surname><given-names>F.</given-names></name></person-group> (<year>2013</year>). <article-title>Topological strata of weighted complex networks</article-title>. <source>PloS One</source>, <volume>8</volume>, <fpage>e66506</fpage>.<pub-id pub-id-type="pmid">23805226</pub-id></mixed-citation>
    </ref>
    <ref id="bib71">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Pinehiro</surname><given-names>J.</given-names></name>, &amp; <name name-style="western"><surname>Bates</surname><given-names>D.</given-names></name></person-group> (<year>2002</year>). <source>Mixed Effects Models in S and S-plus</source> (<edition>3rd ed.</edition>). <publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib72">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pothen</surname><given-names>A.</given-names></name>, &amp; <name name-style="western"><surname>Fan</surname><given-names>C.</given-names></name></person-group> (<year>1990</year>). <article-title>Computing the block triangular form of a sparse matrix</article-title>. <source>ACM Transactions on Mathematical Software (TOMS)</source>, <volume>16</volume>, <fpage>324</fpage>.</mixed-citation>
    </ref>
    <ref id="bib73">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rubinov</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Knock</surname><given-names>S. A.</given-names></name>, <name name-style="western"><surname>Stam</surname><given-names>C. J.</given-names></name>, <name name-style="western"><surname>Micheloyannis</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>A. W.</given-names></name>, <name name-style="western"><surname>Williams</surname><given-names>L. M.</given-names></name>, &amp; <name name-style="western"><surname>Breakspear</surname><given-names>M.</given-names></name></person-group> (<year>2009</year>). <article-title>Small-world properties of nonlinear brain activity in schizophrenia</article-title>. <source>Human Brain Mapping</source>, <volume>30</volume>, <fpage>403</fpage>–<lpage>416</lpage>.<pub-id pub-id-type="pmid">18072237</pub-id></mixed-citation>
    </ref>
    <ref id="bib74">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rubinov</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Sporns</surname><given-names>O.</given-names></name></person-group> (<year>2010</year>). <article-title>Complex network measures of brain connectivity: Uses and interpretations</article-title>. <source>NeuroImage</source>, <volume>52</volume>, <fpage>1059</fpage>–<lpage>1069</lpage>.<pub-id pub-id-type="pmid">19819337</pub-id></mixed-citation>
    </ref>
    <ref id="bib75">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sizemore</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Giusti</surname><given-names>C.</given-names></name>, &amp; <name name-style="western"><surname>Bassett</surname><given-names>D.</given-names></name></person-group> (<year>2016</year>). <article-title>Classification of weighted networks through mesoscale homological features</article-title>. <source>Journal of Complex Networks</source>, <volume>5</volume>, <fpage>245</fpage>–<lpage>273</lpage>.</mixed-citation>
    </ref>
    <ref id="bib76">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sizemore</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Giusti</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Kahn</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Vettel</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Betzel</surname><given-names>R.</given-names></name>, &amp; <name name-style="western"><surname>Bassett</surname><given-names>D.</given-names></name></person-group> (<year>2018</year>). <article-title>Cliques and cavities in the human connectome</article-title>. <source>Journal of Computational Neuroscience</source>, <volume>44</volume>, <fpage>115</fpage>–<lpage>145</lpage>.<pub-id pub-id-type="pmid">29143250</pub-id></mixed-citation>
    </ref>
    <ref id="bib77">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Smit</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Stam</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Posthuma</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Boomsma</surname><given-names>D.</given-names></name>, &amp; <name name-style="western"><surname>De Geus</surname><given-names>E.</given-names></name></person-group> (<year>2008</year>). <article-title>Heritability of small-world networks in the brain: A graph theoretical analysis of resting-state EEG functional connectivity</article-title>. <source>Human Brain Mapping</source>, <volume>29</volume>, <fpage>1368</fpage>–<lpage>1378</lpage>.<pub-id pub-id-type="pmid">18064590</pub-id></mixed-citation>
    </ref>
    <ref id="bib78">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Beckmann</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Andersson</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Auerbach</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Bijsterbosch</surname><given-names>J.</given-names></name>, &amp; <etal/></person-group> (<year>2013</year>). <article-title>Resting-state fMRI in the Human Connectome Project</article-title>. <source>NeuroImage</source>, <volume>80</volume>, <fpage>144</fpage>–<lpage>168</lpage>.<pub-id pub-id-type="pmid">23702415</pub-id></mixed-citation>
    </ref>
    <ref id="bib79">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Snijders</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Spreen</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Zwaagstra</surname><given-names>R.</given-names></name></person-group> (<year>1995</year>). <article-title>The use of multilevel modeling for analysing personal networks: Networks of cocaine users in an urban area</article-title>. <source>Journal of Quantitative Anthropology</source>, <volume>5</volume>, <fpage>85</fpage>–<lpage>105</lpage>.</mixed-citation>
    </ref>
    <ref id="bib80">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Havlin</surname><given-names>S.</given-names></name>, &amp; <name name-style="western"><surname>Makse</surname><given-names>H.</given-names></name></person-group> (<year>2005</year>). <article-title>Self-similarity of complex networks</article-title>. <source>Nature</source>, <volume>433</volume>, <fpage>392</fpage>–<lpage>395</lpage>.<pub-id pub-id-type="pmid">15674285</pub-id></mixed-citation>
    </ref>
    <ref id="bib81">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Sporns</surname><given-names>O.</given-names></name></person-group> (<year>2003</year>). <article-title>Graph theory methods for the analysis of neural connectivity patterns</article-title>. In <person-group person-group-type="editor"><name name-style="western"><surname>Kötter</surname><given-names>R.</given-names></name></person-group> (Ed.), <source>Neuroscience Databases: A Practical Guide</source> (pp. <fpage>171</fpage>–<lpage>185</lpage>). <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Springer US</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib82">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sporns</surname><given-names>O.</given-names></name>, <name name-style="western"><surname>Tononi</surname><given-names>G.</given-names></name>, &amp; <name name-style="western"><surname>Edelman</surname><given-names>G.</given-names></name></person-group> (<year>2000</year>). <article-title>Theoretical neuroanatomy: Relating anatomical and functional connectivity in graphs and cortical connection matrices</article-title>. <source>Cerebral Cortex</source>, <volume>10</volume>, <fpage>127</fpage>.<pub-id pub-id-type="pmid">10667981</pub-id></mixed-citation>
    </ref>
    <ref id="bib83">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stolz</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Harrington</surname><given-names>H.</given-names></name>, &amp; <name name-style="western"><surname>Porter</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>). <article-title>Persistent homology of time-dependent functional networks constructed from coupled time series</article-title>. <source>Chaos: An Interdisciplinary Journal of Nonlinear Science</source>, <volume>27</volume>, <fpage>047410</fpage>.</mixed-citation>
    </ref>
    <ref id="bib84">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Supekar</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Menon</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Rubin</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Musen</surname><given-names>M.</given-names></name>, &amp; <name name-style="western"><surname>Greicius</surname><given-names>M.</given-names></name></person-group> (<year>2008</year>). <article-title>Network analysis of intrinsic functional brain connectivity in Alzheimer’s disease</article-title>. <source>PLoS Computational Biology</source>, <volume>4</volume>(<issue>6</issue>), <fpage>e1000100</fpage>.<pub-id pub-id-type="pmid">18584043</pub-id></mixed-citation>
    </ref>
    <ref id="bib85">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tarjan</surname><given-names>R.</given-names></name></person-group> (<year>1972</year>). <article-title>Depth-first search and linear graph algorithms</article-title>. <source>SIAM Journal on Computing</source>, <volume>1</volume>, <fpage>146</fpage>–<lpage>160</lpage>.</mixed-citation>
    </ref>
    <ref id="bib86">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Thompson</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Cannon</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Narr</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>van Erp</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Poutanen</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Huttunen</surname><given-names>M.</given-names></name>, … <name name-style="western"><surname>Khaledy</surname><given-names>M.</given-names></name></person-group> (<year>2001</year>). <article-title>Genetic influences on brain structure</article-title>. <source>Nature Neuroscience</source>, <volume>4</volume>, <fpage>1253</fpage>–<lpage>1258</lpage>.<pub-id pub-id-type="pmid">11694885</pub-id></mixed-citation>
    </ref>
    <ref id="bib87">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tuzhilin</surname><given-names>A.</given-names></name></person-group> (<year>2016</year>). <article-title>Who invented the Gromov-Hausdorff distance?</article-title>
<source>arXiv preprint arXiv:1612.00728</source>.</mixed-citation>
    </ref>
    <ref id="bib88">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tzourio-Mazoyer</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Landeau</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Papathanassiou</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Crivello</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Etard</surname><given-names>O.</given-names></name>, <name name-style="western"><surname>Delcroix</surname><given-names>N.</given-names></name>, … <name name-style="western"><surname>Joliot</surname><given-names>M.</given-names></name></person-group> (<year>2002</year>). <article-title>Automated anatomical labeling of activations in spm using a macroscopic anatomical parcellations of the MNI MRI single-subject brain</article-title>. <source>NeuroImage</source>, <volume>15</volume>, <fpage>273</fpage>–<lpage>289</lpage>.<pub-id pub-id-type="pmid">11771995</pub-id></mixed-citation>
    </ref>
    <ref id="bib89">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Uddin</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Kelly</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Biswal</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Margulies</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Shehzad</surname><given-names>Z.</given-names></name>, <name name-style="western"><surname>Shaw</surname><given-names>D.</given-names></name>, … <name name-style="western"><surname>Milham</surname><given-names>M.</given-names></name></person-group> (<year>2008</year>). <article-title>Network homogeneity reveals decreased integrity of default-mode network in ADHD</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>169</volume>, <fpage>249</fpage>–<lpage>254</lpage>.<pub-id pub-id-type="pmid">18190970</pub-id></mixed-citation>
    </ref>
    <ref id="bib90">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van Essen</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Ugurbil</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Auerbach</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Barch</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Behrens</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Bucholz</surname><given-names>R.</given-names></name>, … <name name-style="western"><surname>Curtiss</surname><given-names>S.</given-names></name></person-group> (<year>2012</year>). <article-title>The human connectome project: A data acquisition perspective</article-title>. <source>Neuroimage</source>, <volume>62</volume>, <fpage>2222</fpage>–<lpage>2231</lpage>.<pub-id pub-id-type="pmid">22366334</pub-id></mixed-citation>
    </ref>
    <ref id="bib91">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wijk</surname><given-names>B. C. M.</given-names></name>, <name name-style="western"><surname>Stam</surname><given-names>C. J.</given-names></name>, &amp; <name name-style="western"><surname>Daffertshofer</surname><given-names>A.</given-names></name></person-group> (<year>2010</year>). <article-title>Comparing brain networks of different size and connectivity density using graph theory</article-title>. <source>PloS One</source>, <volume>5</volume>, <fpage>e13701</fpage>.<pub-id pub-id-type="pmid">21060892</pub-id></mixed-citation>
    </ref>
    <ref id="bib92">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zalesky</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Fornito</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Harding</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Cocchi</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Yücel</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Pantelis</surname><given-names>C.</given-names></name>, &amp; <name name-style="western"><surname>Bullmore</surname><given-names>E.</given-names></name></person-group> (<year>2010</year>). <article-title>Whole-brain anatomical networks: Does the choice of nodes matter?</article-title>
<source>NeuroImage</source>, <volume>50</volume>, <fpage>970</fpage>–<lpage>983</lpage>.<pub-id pub-id-type="pmid">20035887</pub-id></mixed-citation>
    </ref>
    <ref id="bib93">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zomorodian</surname><given-names>A.</given-names></name>, &amp; <name name-style="western"><surname>Carlsson</surname><given-names>G.</given-names></name></person-group> (<year>2005</year>). <article-title>Computing persistent homology</article-title>. <source>Discrete and Computational Geometry</source>, <volume>33</volume>, <fpage>249</fpage>–<lpage>274</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
