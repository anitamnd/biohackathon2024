<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10017698</article-id>
    <article-id pub-id-type="publisher-id">31288</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-023-31288-2</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>IntelliSleepScorer, a software package with a graphic user interface for automated sleep stage scoring in mice based on a light gradient boosting machine algorithm</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Lei A.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kern</surname>
          <given-names>Ryan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yu</surname>
          <given-names>Eunah</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Choi</surname>
          <given-names>Soonwook</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Pan</surname>
          <given-names>Jen Q.</given-names>
        </name>
        <address>
          <email>jpan@broadinstitute.org</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.66859.34</institution-id><institution-id institution-id-type="ISNI">0000 0004 0546 1623</institution-id><institution>Stanley Center for Psychiatric Research, </institution><institution>Broad Institute of MIT and Harvard, </institution></institution-wrap>75 Ames Street, Cambridge, MA 02142 USA </aff>
      <aff id="Aff2"><label>2</label>Troy High School, 2200 Dorothy Lane, Fullerton, CA 92831 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>3</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>15</day>
      <month>3</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>13</volume>
    <elocation-id>4275</elocation-id>
    <history>
      <date date-type="received">
        <day>11</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>9</day>
        <month>3</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Machine learning has been applied in recent years to categorize sleep stages (NREM, REM, and wake) using electroencephalogram (EEG) recordings; however, a well-validated sleep scoring automatic pipeline in rodent research is still not publicly available. Here, we present IntelliSleepScorer, a software package with a graphic user interface to score sleep stages automatically in mice. IntelliSleepScorer uses the light gradient boosting machine (LightGBM) to score sleep stages for each epoch of recordings. We developed LightGBM models using a large cohort of data, which consisted of 5776 h of sleep EEG and electromyogram (EMG) signals across 519 unique recordings from 124 mice. The LightGBM model achieved an overall accuracy of 95.2% and a Cohen’s kappa of 0.91, which outperforms the baseline models such as the logistic regression model (accuracy = 93.3%, kappa = 0.88) and the random forest model (accuracy = 94.3%, kappa = 0.89). The overall performance of the LightGBM model as well as the performance across different sleep stages are on par with that of the human experts. Most importantly, we validated the generalizability of the LightGBM models: (1) The LightGBM model performed well on two publicly available, independent datasets (kappa &gt;  = 0.80), which have different sampling frequency and epoch lengths; (2) The LightGBM model performed well on data recorded at a lower sampling frequency (kappa = 0.90); (3) The performance of the LightGBM model is not affected by the light/dark cycle; and (4) A modified LightGBM model performed well on data containing only one EEG and one EMG electrode (kappa &gt;  = 0.89). Taken together, the LightGBM models offer state-of-the-art performance for automatic sleep stage scoring in mice. Last, we implemented the IntelliSleepScorer software package based on the validated model to provide an out-of-box solution to sleep researchers (available for download at <ext-link ext-link-type="uri" xlink:href="https://sites.broadinstitute.org/pan-lab/resources">https://sites.broadinstitute.org/pan-lab/resources</ext-link>).</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Machine learning</kwd>
      <kwd>Software</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100013113</institution-id>
            <institution>Stanley Center for Psychiatric Research, Broad Institute</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id>
            <institution>National Institutes of Health</institution>
          </institution-wrap>
        </funding-source>
        <award-id>MH115045</award-id>
        <principal-award-recipient>
          <name>
            <surname>Pan</surname>
            <given-names>Jen Q.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">During sleep, the body cycles through NREM (nonrapid eye movement) sleep, during which the breathing, heart rate and oscillations of the brain waves slow down, and REM (rapid eye movement) sleep, which is when vivid dreams take place. Sleep stage scoring in rodents is the process of identifying the three stages (wake, NREM, and REM) based on electroencephalogram (EEG) and electromyogram (EMG) signals. Sleep in rodents is more fragmented than that in humans and is composed of shorter episodes of NREM and REM interspaced by waking<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Sleep stage scoring is critical for studying sleep stage-specific measures and effects. Traditionally, sleep stage scoring has been performed manually by human experts, which is labor intensive and time consuming; thus, it is the rate-limiting step in many analyses in sleep research. To address this issue, several studies have adopted machine learning-based approaches to develop algorithms to automatically categorize sleep stages<sup><xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR6">6</xref></sup>. However, some models were developed and tested using relatively small datasets (less than 800 h of sleep recordings from 10 to 30 mice/rats)<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>, raising uncertainties regarding the generalizability of the models. Two recent studies utilized large datasets (~ 3500 h and ~ 160,000 h of sleep recordings, respectively) and developed models based on convolutional neural networks<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup>. These models achieved high performance that was comparable to that of human experts. However, software based on these high-performing models has yet to be made publicly available for sleep researchers. In this study, we developed machine learning-based models using a large cohort of data, which consisted of 5776 h of EEG and EMG signals from 519 recordings across 124 mice. The machine learning algorithm was based on the LightGBM. The LightGBM, developed by researchers at Microsoft, is an ensemble model based on the decision tree<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. It uses a gradient boosting approach to construct an ensemble of decision trees. The LightGBM model trained in this study consists of over 8000 decision trees. In this study, we not only demonstrated that a sleep stage scoring model based on LightGBM outperforms two widely used baseline models, logistic regression and random forest, but also showed that its performance is comparable to that of human experts. Most importantly, we confirmed that the LightGBM model is not overfitted to our data and is able to perform well on a publicly available independent set of data. To make the model freely available to academic sleep researchers without coding experience so that they can utilize it immediately, we developed a software tool, named IntelliSleepScorer, with a graphic user interface for easy access.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <p id="Par3">This study used data collected from in vivo experiments in mice. No human experiments were involved in the study. All the experiments with animals were approved by the Institutional Animal Care and Use Committee at the Broad Institute. All experiments were performed in accordance with relevant guidelines and regulations. The ARRIVE guidelines are not applicable to this study because the focus of this study is to develop machine learning models rather than comparing different treatment groups.</p>
    <sec id="Sec3">
      <title>IntelliSleepScorer workflow overview</title>
      <p id="Par4">The IntelliSleepScorer software is designed to enable fully automated sleep scoring in mice. Figure <xref rid="Fig1" ref-type="fig">1</xref> shows the schematic view of the workflow facilitated by IntelliSleepScorer. In principle, the sleep stages generated by IntelliSleepScorer can be directly used in downstream analyses that focus on studying the physiology during Wake or NREM. If the downstream analysis requires accurately scoring REM stage, we recommend having human expert(s) verify the stages generated by the software. IntelliSleepScorer provides interactive visualizations of the signals, the associated hypnogram, and the SHAP (SHapley Additive exPlanations) values to facilitate manual verification of the generated sleep stages. SHAP values is a method based on cooperative game theory to facilitate the interpretability of machine learning models<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. IntelliSleepScorer provides visualizations of both global SHAP values (indicating how different features values affect the scoring decision of the model in general) and epoch-level SHAP values (indicating how the features values from each epoch contribute to the scoring decision for that epoch).<fig id="Fig1"><label>Figure 1</label><caption><p>Schematic diagram of the workflow facilitated by IntelliSleepScorer. The highlighted boxes indicate the functionalities provided by IntelliSleepScorer.</p></caption><graphic xlink:href="41598_2023_31288_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>Datasets</title>
      <p id="Par5">The key properties of all datasets used in the study are listed in Supplemental Table <xref rid="MOESM2" ref-type="media">1</xref>. The in-house dataset consists of 519 EEG and EMG recordings (5776 h long, 301 of them were recorded during the light cycle, 218 of them were recorded during the dark cycle) from 124 C57/B6 mice with different genetic backgrounds. Each recording lasted 6–12 h. We recorded two EEG signals from the surface of the skull (EEG1 was recorded from the parietal region: AP Bregma ~ − 1.3 mm, ML Bregma ~  + 2.3 mm, and EEG2 was recorded from the frontal region: AP Bregma ~  + 1.5 mm, ML Bregma ~  + 1.5 mm) and one EMG signal placed in the nuchal muscle. The sampling frequency was 1000 Hz. The detailed method for in vivo EEG recording has been previously described<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. The EEG and EMG recordings were divided into 10-s epochs. Models were trained and tested using epoch-level features. The processes for model training and evaluation are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. No context information (epochs before or after the target epoch) was used to train the models. The dataset contained a total of 2,079,344 epochs. The training set contained 1,810,746 epochs from 454 recordings (87.5% of all recordings, 267 of them were recorded during the light cycle, 187 of them were recorded during the dark cycle) across 111 mice. When training the LightGBM model, the training set was further randomly split into two sets; one set containing 80% of all epochs was used to fit the model, and the remaining 20% was used to validate the model. The logistic regression model and the random forest model were fitted using the entire training set. The test set contained 268,598 epochs from 65 recordings (12.5% of all recordings, 34 of them were recorded during the light cycle, 31 of them were recorded during the dark cycle) across 13 mice. The test set was never, directly or indirectly, exposed to the model during the training process. One potential route of indirect exposure was splitting multiple recordings from the same mouse into both the training set and the test set. This scenario would compromise the independence of the test dataset and lead to biased evaluations of the trained model because different recordings from the same mouse may contain individually specific patterns that could lead to overfitting. Thus, it needs to be emphasized that the data used in the test set were derived from mice that are different from those in the training set. This strategy ensures an unbiased evaluation of the performance of the trained models. The scores from human experts were used as ground truth to evaluate the performance of the models. Each epoch was associated with a ground truth label of the sleep stage that was generated by a human expert in the lab. Multiple experts were involved in the scoring process, but each recording was scored by one expert.<fig id="Fig2"><label>Figure 2</label><caption><p>Schematic diagram of the training and testing processes for the LightGBM models.</p></caption><graphic xlink:href="41598_2023_31288_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par6">In addition to the in-house data, we utilized the dataset published by Miladinovic and colleagues<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Cohort A and cohort B from this public dataset were used to test the generalizability of the LightGBM model. The scores from these two datasets contained three categories of artifacts: wake, NREM, and REM artifacts. We merged those three categories into the wake, NREM, and REM labels to allow fair comparisons to our in-house test data since our experts did not identify artifacts during the scoring process. Cohort A includes four 24-h recordings from wild-type mice and contains 15.2% artifacts. Cohort B includes four 24-h recordings from mutant mice and contains 19.2% artifacts. Both cohorts were recorded at 128 Hz using a similar electrode setup as ours—one frontal EEG channel, one parietal EEG channel, and one EMG channel. Scores by two human experts from the same laboratories were provided for both cohorts A and B. We used cohort C, which contains eight recordings, to estimate the performance of human experts because this cohort included scores provided by two experts from different laboratories. The agreement rate among experts from different laboratories (cohort C) is a more unbiased measure of human performance compared to that among experts from the same laboratory (cohorts A and B).</p>
    </sec>
    <sec id="Sec5">
      <title>Quality control prior to feature extraction</title>
      <p id="Par7">To maximize the generalizability of the trained models, we tolerated noises and artifacts in the input data and adopted a minimal quality control process. Only two recordings were excluded from the training and validation due to loosened connections or lost signals in one of the EEG or EMG channels. Recordings were excluded when they met one of the two criteria: 1. the amplitude of either one of the EEG signals is less than 1 uV for at least 50% of the time; 2. the amplitude of the EMG signal is less than 1 pV for at least 50% of the time.</p>
    </sec>
    <sec id="Sec6">
      <title>Feature extraction</title>
      <p id="Par8">We extracted 291 features from EEG1 (138 features), EEG2 (138 features), and EMG (15 features) signals for each epoch (10 s) using the MNE-Python package and custom code. All 291 features were used to train the LightGBM model and the baseline models: logistic regression and random forest model. Only 153 features (138 EEG features and 15 EMG features) were used to train the LightGBM-1EEG model. The EEG and EMG signals were bandpass filtered at 1–40 Hz to remove 60 Hz line noise prior to feature extraction. The features extracted from the EEG signals included parameters of both the time and the frequency domains, while those extracted from the EMG signal were derived from only the time domain. There were 45 temporal features, which consisted of five raw and ten normalized attributes, for each of the three EEG/EMG channels. The raw time domain features included the mean value of the absolute amplitude, the median value of the absolute amplitude, the maximum value of the absolute amplitude, the standard deviation of the absolute amplitude, and the root mean squared value of the amplitude. The normalized features included the five raw features normalized to the per-recording mean or median values of the corresponding raw features. Normalization per-recording was performed to mitigate the inter-recording amplitude variability. Two-hundred and forty-six frequency domain features were extracted from each of the two EEG channels (123 features per channel). The frequency domain features can be divided into two sets. The first set consisted of the power from distinct frequency bands. EEG signals were transformed to the time–frequency domain using Fast Fourier Transform (FFT) algorithm implemented in the Python NumPy package. The power bands included delta (1–4 Hz), theta (4–8 Hz), alpha (8–12 Hz), sigma (12–15 Hz), beta (15–30 Hz), and low gamma (30–40 Hz) frequency bands. The ratios of power from frequency bands include the theta-delta ratio, alpha-delta ratio, sigma-delta ratio, beta-delta ratio, and low gamma-delta ratio. The second set of frequency domain features was the same as the time domain features except the data were bandpass filtered using finite impulse response (FIR) filters before feature extraction. The frequency ranges used for the FIR filters were the same as the six frequency bands described above.</p>
    </sec>
    <sec id="Sec7">
      <title>Logistic regression model</title>
      <p id="Par9">The logistic regression model used all 291 features as input to score the sleep stages. L2 penalty was used when fitting the model. The tolerance for stopping criteria was set as 1e-4. The inverse of regularization strength was set as 1. An intercept constant was added to the decision function. Lbfgs algorithm was used as the solver. The maximum number of iterations was set as 10,000.</p>
    </sec>
    <sec id="Sec8">
      <title>Random forest model</title>
      <p id="Par10">The random forest model used all 291 features as input to score the sleep stages. The number of trees was set as 100. Gini impurity was used to measure the quality of a split. The maximum depth of the tree was set as unlimited. The minimum number of samples required to split an internal node was set as 2. The minimum number of samples required to be at a leaf node was set as 1. Bootstrap samples were used when building trees.</p>
    </sec>
    <sec id="Sec9">
      <title>LightGBM model</title>
      <p id="Par11">The LightGBM model used all 291 features as input to score the sleep stages. The number of estimators was set as 100,000 and the early stopping round was set as 500. In the end, the model stopped at the 8045th round. The number of leaves was set as 100. The learning rate was set as 0.01.</p>
    </sec>
    <sec id="Sec10">
      <title>LightGBM-1EEG model</title>
      <p id="Par12">The LightGBM-1EEG model had the same overall structure and hyperparameters as the LightGBM model. The only difference was that the LightGBM-1EEG model only used 153 features (138 EEG features from one EEG channel and 15 EMG features). The LightGBM-1EEG model stopped at the 22,127th round during training.</p>
    </sec>
    <sec id="Sec11">
      <title>Metrics used to evaluate the performance of the models and human experts</title>
      <p id="Par13">The metrics used to evaluate the model performance include recall, precision, f1-score, accuracy/inter-rater agreement rate, and Cohen’s kappa score.</p>
      <p id="Par14">For each stage s ∈ {Wake, NREM, and REM}, the following formulas are used to calculate the recall, precision, and f1-score.<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${recall}_{s}=\frac{TP}{TP+FN},$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">recall</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2023_31288_Article_Equa.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equb"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${precision}_{s}=\frac{TP}{TP+FP},$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">precision</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2023_31288_Article_Equb.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equc"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f1}_{s}=\frac{2}{{{recall}_{s}}^{-1}+{{precision}_{s}}^{-1}},$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">recall</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">precision</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2023_31288_Article_Equc.gif" position="anchor"/></alternatives></disp-formula>where TP indicates true positive—the number of epochs that are scored as stage s by both the model and the human expert, FP indicates false positive– the number of epochs that are scored as stage s by the model but are scored as not being stage s by the human expert, and FN indicates false negative—the number of epochs that are scored as not being stage s by the model but are scored as stage s by the human expert.</p>
      <p id="Par15">Based on the above formula, recall and precision cannot be used to compare two human experts due to the lack of ground truth. By making a random assumption (assumption 1) that one human expert is the ground truth, we can calculate the recall<sub>assumption1</sub> and precision<sub>assumption1</sub>; if we change the assumption and pick the other human expert as the ground truth (assumption 2), we can calculate the recall<sub>assumption2</sub> and precision<sub>assumption2.</sub> One interesting observation is that recall<sub>assumption1</sub> equals precision<sub>assumption2</sub> and recall<sub>assumption2</sub> equals precision<sub>assumption1.</sub> By switching the designations of ground truth between the two human experts, we also switch the recall and precision values. Another interesting observation is that the f1 score is invariant to the switch of the designations of ground truth because it is simply the harmonic mean of the recall and precision. Therefore, we decided to use f1 scores to compare two human experts.<disp-formula id="Equd"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Inter-rater\,aggreement\,rate=\,\frac{Number\,of\,epochs\,agreed\,between\,two\,raters}{Total\,number\,of\,epochs}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>-</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.166667em"/><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.166667em"/><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mspace width="0.166667em"/><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.166667em"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.166667em"/><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.166667em"/><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.166667em"/><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.166667em"/><mml:mi>t</mml:mi><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mspace width="0.166667em"/><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.166667em"/><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.166667em"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.166667em"/><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2023_31288_Article_Equd.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par16">Note that accuracy is equivalent to the inter-rater agreement rate between the model and the human expert.<disp-formula id="Eque"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Cohe{n}^{^{\prime}}s\,kappa=\frac{accuracy-{p}_{e}}{1-{p}_{e}}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msup><mml:mi>s</mml:mi><mml:mspace width="0.166667em"/><mml:mi>k</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2023_31288_Article_Eque.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}_{e}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mi>p</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2023_31288_Article_IEq1.gif"/></alternatives></inline-formula> is the probability of agreement by chance. The kappa value is considered to be a better measurement of interrater reliability than accuracy because it accounts for the agreement rate by chance. Kappa values greater than 0.8 are considered to indicate nearly perfect agreement<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>.</p>
    </sec>
    <sec id="Sec12">
      <title>Software packages used in the study</title>
      <p id="Par17">All data processing, model training, and model testing were performed in the Python 3.8 environment. EEG and EMG recordings were stored in EDF/EDF + format. We used the MNE-Python package to read and process the EDF/EDF + files<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. The scikit-learn package was used to train and test the logistic regression model and the random forest model. We used the LightGBM Python package to train and test the LightGBM model. Figures were generated using the matplotlib<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> and seaborn packages. The Windows executable for IntelliSleepScorer as well as all the .dll files were generated using PyInstaller package.</p>
    </sec>
  </sec>
  <sec id="Sec13">
    <title>Results</title>
    <sec id="Sec14">
      <title>The LightGBM model outperformed the logistic regression and random forest models</title>
      <p id="Par18">We compared the sleep stage scoring performance of the LightGBM model to two baseline models, logistic regression and random forest. Overall, all three models performed well, with accuracy greater than 93% and kappa values greater than 0.88 (Fig. <xref rid="Fig3" ref-type="fig">3</xref>A–C), indicating almost perfect agreement between the models and the human experts. Comparing the three approaches, LightGBM outperformed the logistic regression and the random forest models on almost all the metrics across wake, NREM, and REM stages (Fig. <xref rid="Fig3" ref-type="fig">3</xref>A–C). The overall accuracy was 93.3% for the logistic regression, 94.3% for the random forest, and 95.2% for the LightGBM model. The kappa was 0.88 for the logistic regression, 0.89 for the random forest, and 0.91 for the LightGBM model.<fig id="Fig3"><label>Figure 3</label><caption><p>Comparisons between the baseline models (logistic regression and random forest), the LightGBM model, and human experts. (<bold>A</bold>–<bold>C</bold>): the performance of the logistic regression, the random forest model, and the LightGBM model. The numbers without parentheses in the heatmaps indicate the percentage of the scores from human experts (ground truth) that are in agreement with the scores generated by the model. The numbers inside parentheses indicate the percentage of model-generated scores that are in agreement with the scores of the human experts. Therefore, in the left–right diagonal line, the numbers without parentheses indicate recall, and the numbers inside parentheses indicate precision. (<bold>D</bold>) Comparisons of the accuracy, kappa, wake f1-score, NREM f1-score, and REM f1-score between the LightGBM model and human experts. The performance of human experts was estimated using the scores provided by two human experts from different labs from a publicly available dataset.</p></caption><graphic xlink:href="41598_2023_31288_Fig3_HTML" id="MO3"/></fig></p>
    </sec>
    <sec id="Sec15">
      <title>The LightGBM model achieved human expert level performance in scoring sleep stages</title>
      <p id="Par19">We compared the performance of the LightGBM model to that of human experts. The performance of the LightGBM model was defined as the level of agreement (measured by accuracy, kappa, and f1-score) between the LightGBM model and the human experts in our laboratory. The performance of human experts was defined as the level of agreement between two experts from different laboratories in a previous study<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. We found that the performance metrics for the LightGBM model were similar to or higher than those of the human experts (Fig. <xref rid="Fig3" ref-type="fig">3</xref>D). The LightGBM model had an accuracy of 0.95, a kappa of 0.91, a wake f1-score of 0.96, a NREM f1-score of 0.95, and a REM f1-score of 0.85. The two human experts from different laboratories had an agreement rate of 0.91, a kappa of 0.85, a wake f1-score of 0.92, a NREM f1-score of 0.92, and a REM f1-score of 0.86.</p>
    </sec>
    <sec id="Sec16">
      <title>The LightGBM model was highly generalizable</title>
      <p id="Par20">Using a publicly available independent dataset, we compared the sleep stages scored by the LightGBM model to the ones scored by two human experts across two cohorts of mice (four conditions). The LightGBM model achieved high performance with accuracy values close to or above 90% and kappa values greater than or equal to 0.8 in all four conditions (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). The precision and recall values for the wake stage and the NREM stage were above 83% in all cases (16 total values) and were above 90% in more than half of the cases (9 out of 16 total values). The LightGBM model had lower recall but high precision (&gt; 90% in all cases) when scoring the REM stage. The performance on wild-type mice (Fig. <xref rid="Fig4" ref-type="fig">4</xref>A, B) and mutant mice (Fig. <xref rid="Fig4" ref-type="fig">4</xref>C, D) was similar, suggesting that the model was able to score sleep stages for mice with different genetic backgrounds. Collectively, the high performance in an independent dataset confirmed that the LightGBM model had good generalizability.<fig id="Fig4"><label>Figure 4</label><caption><p>Evaluation of the generalizability of the LightGBM model using a publicly available independent dataset. The performance of the LightGBM model was evaluated across cohorts A (top row) and B (bottom row) against the scores of expert 1 (left column) and expert 2 (right column).</p></caption><graphic xlink:href="41598_2023_31288_Fig4_HTML" id="MO4"/></fig></p>
    </sec>
    <sec id="Sec17">
      <title>The LightGBM model performed consistently well on individual recordings</title>
      <p id="Par21">We measured the consistency of the performances of the LightGBM model across the test set of 65 recordings from 13 mice. The precision and recall values were combined into a single f1-score for easier visualization and evaluation. The results (Fig. <xref rid="Fig5" ref-type="fig">5</xref>A) showed high consistency for the wake f1-score (61 out of 68 or 93.8% recordings &gt; 0.9), NREM f1-score (89.2% recordings &gt; 0.9), accuracy (93.8% recordings &gt; 0.9), and kappa values (92.3% recordings &gt; 0.8). The REM f1-scores were not at consistently high levels (21.5% recordings &gt; 0.9, 73.8% recordings &gt; 0.8). A reduced REM f1-score was also observed when comparing the scores from two human experts (Fig. <xref rid="Fig5" ref-type="fig">5</xref>B). Collectively, these results demonstrate that the consistency of the LightGBM model performance reached a similar level as that of the human experts, indicating that LightGBM is a reliable tool for scoring individual recordings.<fig id="Fig5"><label>Figure 5</label><caption><p>Comparisons of the recording-level performances between the LightGBM model and human experts. (<bold>A</bold>) The performance of the LightGBM model was evaluated individually in 65 recordings. Recordings from the dark cycle are shown in blue; recordings from the light cycle are shown in orange. (<bold>B</bold>) The scores of two human experts from a publicly available dataset were compared to each other across 8 recordings. Blue text indicates the percentage of recordings above the threshold of 0.9, indicated by the blue dashed lines. Red text indicates the percentage of recordings above the threshold of 0.8, indicated by the red dashed lines.</p></caption><graphic xlink:href="41598_2023_31288_Fig5_HTML" id="MO5"/></fig></p>
    </sec>
    <sec id="Sec18">
      <title>The LightGBM model performed well across the recordings collected from light and dark cycles</title>
      <p id="Par22">The average durations of sleep stages during the light cycle are distinct from those during the dark cycle (Fig. S<xref rid="MOESM1" ref-type="media">1</xref>), but the LightGBM model performed well regardless of the light/dark cycle (Figs. <xref rid="Fig5" ref-type="fig">5</xref>A, S<xref rid="MOESM1" ref-type="media">2</xref>).</p>
    </sec>
    <sec id="Sec19">
      <title>The LightGBM model performed well on data down-sampled to 100 Hz</title>
      <p id="Par23">To batch process recordings, sampling signals at a lower frequency is sometimes necessary due to limited computational resources. Here, we tested whether the sampling rate affected the performance of the LightGBM model. We down-sampled the test set to 100 Hz. The sleep stages were scored using the LightGBM model that was trained and validated using 1000 Hz data. The results showed that the performance on 100 Hz data was still high (Fig. <xref rid="Fig6" ref-type="fig">6</xref>). The overall accuracy was 94.7%, and the kappa was 0.90. The recall values were 94.5% for wake, 96.5% for NREM, and 81.5% for REM stages. The precision values were 96.7% for wake, 93.4% for NREM, and 87.6% for REM stages.<fig id="Fig6"><label>Figure 6</label><caption><p>The performance of the LightGBM model when signals were downsampled to 100 Hz. The numbers without parentheses in the heatmaps indicate the percentage of the scores from human experts (ground truth) that are in agreement with the scores generated by the model. The numbers inside parentheses indicate the percentage of model-generated scores that are in agreement with the scores of the human experts. Therefore, in the left–right diagonal line, the numbers without parentheses indicate recall, and the numbers inside parentheses indicate precision.</p></caption><graphic xlink:href="41598_2023_31288_Fig6_HTML" id="MO6"/></fig></p>
    </sec>
    <sec id="Sec20">
      <title>The LightGBM-1EEG model performed well on data containing one EEG and one EMG channel</title>
      <p id="Par24">Our in vivo sleep EEG recordings included one EMG channel and two EEG channels. While this is a widely used setup, we tried to extend the utility of a LightGBM model by accommodating different recording setups. We trained a LightGBM model using only one EEG and one EMG channel (LightGBM-1EEG). The LightGBM-1EEG model still had high overall accuracy (94.1% using parietal EEG; 95.1% using frontal EEG, Fig. <xref rid="Fig7" ref-type="fig">7</xref>) and high kappa values (0.89 using parietal EEG; 0.91 using frontal EEG, Fig. <xref rid="Fig7" ref-type="fig">7</xref>). This performance was still on par with that of the human experts.<fig id="Fig7"><label>Figure 7</label><caption><p>The performance of the LightGBM-1EEG model in the test data containing one EMG channel and one EEG channel in the parietal region (<bold>A</bold>) or in the frontal region (<bold>B</bold>). The numbers without parentheses in the heatmaps indicate the percentage of the scores from human experts (ground truth) that are in agreement with the scores generated by the model. The numbers inside parentheses indicate the percentage of model-generated scores that are in agreement with the scores of the human experts. Therefore, in the left–right diagonal line, the numbers without parentheses indicate recall, and the numbers inside parentheses indicate precision.</p></caption><graphic xlink:href="41598_2023_31288_Fig7_HTML" id="MO7"/></fig></p>
    </sec>
    <sec id="Sec21">
      <title>IntelliSleepScorer implementation</title>
      <p id="Par25">We implemented IntelliSleepScorer with a graphic user interface (GUI) to enable sleep researchers without coding experience to access and utilize the trained models. The GUI can be used to extract EEG and EMG features and score sleep stages by calling the trained models in the backend. Figure <xref rid="Fig8" ref-type="fig">8</xref> shows the GUI of IntelliSleepScorer. The current version of IntelliSleepScorer accepts inputs of EDF/EDF + files. For detailed requirements of the EDF/EDF + files, please refer to <ext-link ext-link-type="uri" xlink:href="https://github.com/broadinstitute/IntelliSleepScorer">https://github.com/broadinstitute/IntelliSleepScorer</ext-link>. The sleep stages in the output txt file are coded as 1: wake, 2: NREM, and 3: REM. On a computer (Processor: Intel(R) Core(TM) i7-8550U CPU @ 1.80 GHz 1.99 GHz; RAM: 24 GB) running on a 64-bit Windows 10 system, it took around 10 min to process 12 h of recordings sampled at 1000 Hz.<fig id="Fig8"><label>Figure 8</label><caption><p>The GUI of IntelliSleepScorer. Please view the detailed documentation and instructions at <ext-link ext-link-type="uri" xlink:href="https://github.com/broadinstitute/IntelliSleepScorer">https://github.com/broadinstitute/IntelliSleepScorer</ext-link>.</p></caption><graphic xlink:href="41598_2023_31288_Fig8_HTML" id="MO8"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec22">
    <title>Discussion</title>
    <p id="Par26">In this study, we demonstrated that the LightGBM model achieved an overall accuracy of 95.2% and a kappa value of 0.91, suggesting excellent agreement between the LightGBM model and human experts. Based on the study from Miladinovic and colleagues, experts from different laboratories had an agreement rate of 90% and a kappa value of 0.85 (we calculated the kappa value using the data published by the authors) in sleep stage scoring, and experts from the same laboratory had an agreement rate of 95–96%<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Considering that experts from the same laboratory may share the same bias, we think that the inter-laboratory agreement rate is a better indication of the performance agreement among human experts. Another study by Rytkönen and colleagues suggested that the agreement rate was 88% among three human experts and 92% between any two human experts<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Therefore, our LightGBM model achieved a similar level of overall performance as human experts.</p>
    <p id="Par27">Across all four models we established, including logistic regression, random forest, LightGBM, and LightGBM-1EEG, we observed a drop of performance when scoring the REM stage. For example, the LightGBM model had lower agreement with human experts in scoring the REM stage (precision, 88.8%; recall, 82.2%; f1-score, 0.85). This reduced performance when scoring the REM stage was reported for previous machine learning models<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup>. The important question is how such performance in the REM stage compares to that of human experts. To estimate the performance of human experts in scoring the REM stage, we calculated the f1-score for the REM stage using the published sleep stage scoring results of human experts from different labs<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. The f1-score for the REM stage was 0.86 for human experts, which is similar to the REM f1-score (0.85) of the LightGBM model. Based on these results, the overall performance of the LightGBM model for the REM stage was similar to that of human experts.</p>
    <p id="Par28">Previous studies developing deep learning models have focused on evaluating the overall performance of the trained models<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup>. A high overall performance does not guarantee usefulness in practice if the performance of the models varies largely across different recordings. In this study, we evaluated the consistency of the performance of the LightGBM model across 65 recordings/mice individually in the test data. The results demonstrated the high consistency of the LightGBM model for scoring the wake and NREM stages during both the light cycle and the dark cycle. When scoring the REM stage, we found that 21.5% (14 out 65) of recordings had a f1-score greater than 0.9, and 73.8% (48 out of 65) of recordings had a f1-score greater than 0.8. However, the REM f1-score when comparing two human experts was not obviously higher than that of the LightGBM; 25.0% (2 out of 8) of recordings had REM f1-scores greater than 0.9, and 87.5% (7 out of 8) had REM f1-scores greater than 0.8. These results suggest that human experts also tend to have a lower agreement rate when determining REM stages. Collectively, the LightGBM model performs at a similar level of consistency as human experts across all three sleep stages. Considering that the LightGBM model performed poorly (REM f1-score &lt; 0.6) on a few recordings, we recommend having human expert(s) verify the model-generated sleep stages if the study requires accurately scoring REM stage. In studies that focus on investigating the physiology during wake or NREM stage (such as most of our research), the LightGBM model still enables a robust and fully automated analytical pipeline.</p>
    <p id="Par29">Considering that the data generated from different laboratories may have different sampling rates, a model needs to accommodate different sampling rates to work well in practice. The performance of the LightGBM model in data sampled at 100 Hz are also on par with that in data sampled at 1000 Hz with an overall accuracy of 94.7% and a kappa of 0.90. Therefore, the performance of the LightGBM model was not affected much by the sampling frequency (as long as it is higher than 100 Hz). Using a low sampling rate has the advantages of reduced computational requirements and reduced run time for batch processing.</p>
    <p id="Par30">We demonstrated that the LightGBM-1EEG model trained using data containing only one EEG and one EMG channel had high performance with an overall accuracy greater than 94.1% and kappa greater than 0.89. The ability to perform well using only one EEG instead of two further expanded the generalizability of our scoring tool since any data containing at least one EEG and one EMG channel can be reconstructed to meet the requirement of the LightGBM-1EEG model.</p>
    <p id="Par31">In conclusion, we developed the LightGBM model, which achieved a similar level of performance in sleep stage scoring compared to human experts. We validated the generalizability of the LightGBM models extensively for practical use in research. In addition, we implemented IntelliSleepScorer, a GUI software based on the models reported in this study. Such automation in sleep scoring has an immediate impact for sleep researchers who would like to quickly score large amounts of EDF/EDF + files without in-depth coding experience.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec23">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41598_2023_31288_MOESM1_ESM.pdf">
            <caption>
              <p>Supplementary Information 1.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="41598_2023_31288_MOESM2_ESM.xlsx">
            <caption>
              <p>Supplementary Information 2.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="41598_2023_31288_MOESM3_ESM.xlsx">
            <caption>
              <p>Supplementary Information 3.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary Information</title>
    <p>The online version contains supplementary material available at 10.1038/s41598-023-31288-2.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank Kerana Yan and Jingwen Hu for manually scoring sleep stages.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>L.A.W. and J.Q.P. conceived the project. L.A.W. designed the algorithm and implemented the approaches together with R.K. S.C. and E.Y. performed the in vivo recordings and most of the manual sleep scoring. L.A.W., R.K., and J.Q.P. wrote the manuscript with feedback from all authors.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>The study was funded by the Stanley Center for Psychiatric Research and Wuxi Mental Health Center. This work was additionally supported by R01 MH115045-01 and R01MH118298 to JQP.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>IntelliSleepScorer software was released under the Creative Commons Attribution-NonCommercial-ShareAlike (CC-BY-NC-SA) license. It is free to academic users at this link (<ext-link ext-link-type="uri" xlink:href="https://sites.broadinstitute.org/pan-lab/resources">https://sites.broadinstitute.org/pan-lab/resources</ext-link>). For commercial use, please contact the authors for licenses. The source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/broadinstitute/IntelliSleepScorer">https://github.com/broadinstitute/IntelliSleepScorer</ext-link>.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par32">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Astori</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wimmer</surname>
            <given-names>RD</given-names>
          </name>
          <name>
            <surname>Lüthi</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Manipulating sleep spindles–expanding views on sleep, memory, and disease</article-title>
        <source>Trends Neurosci.</source>
        <year>2013</year>
        <volume>36</volume>
        <issue>12</issue>
        <fpage>738</fpage>
        <lpage>748</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tins.2013.10.001</pub-id>
        <?supplied-pmid 24210901?>
        <pub-id pub-id-type="pmid">24210901</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Akada</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Yagi</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Miura</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Beuckmann</surname>
            <given-names>CT</given-names>
          </name>
          <name>
            <surname>Koyama</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Aoshima</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>A deep learning algorithm for sleep stage scoring in mice based on a multimodal network with fine-tuning technique</article-title>
        <source>Neurosci. Res.</source>
        <year>2021</year>
        <volume>173</volume>
        <fpage>99</fpage>
        <lpage>105</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neures.2021.07.003</pub-id>
        <?supplied-pmid 34280429?>
        <pub-id pub-id-type="pmid">34280429</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rytkönen</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Zitting</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Porkka-Heiskanen</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Automated sleep scoring in rats and mice using the naive Bayes classifier</article-title>
        <source>J. Neurosci. Methods</source>
        <year>2011</year>
        <volume>202</volume>
        <issue>1</issue>
        <fpage>60</fpage>
        <lpage>64</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jneumeth.2011.08.023</pub-id>
        <?supplied-pmid 21884727?>
        <pub-id pub-id-type="pmid">21884727</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yamabe</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Horie</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Shiokawa</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Funato</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yanagisawa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kitagawa</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>MC-SleepNet: Large-scale sleep stage scoring in mice by deep neural networks</article-title>
        <source>Sci. Rep.</source>
        <year>2019</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>15793</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-019-51269-8</pub-id>
        <?supplied-pmid 31672998?>
        <pub-id pub-id-type="pmid">31672998</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Barger</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Frye</surname>
            <given-names>CG</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Dan</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bouchard</surname>
            <given-names>KE</given-names>
          </name>
        </person-group>
        <article-title>Robust, automated sleep scoring by a compact neural network with distributional shift correction</article-title>
        <source>PLoS ONE</source>
        <year>2019</year>
        <volume>14</volume>
        <issue>12</issue>
        <fpage>e0224642</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0224642</pub-id>
        <?supplied-pmid 31834897?>
        <pub-id pub-id-type="pmid">31834897</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Miladinović</surname>
            <given-names>Đ</given-names>
          </name>
          <name>
            <surname>Muheim</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Bauer</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SPINDLE: End-to-end learning from EEG/EMG to extrapolate animal sleep scoring across experimental settings, labs and species</article-title>
        <source>PLOS Comput. Biol.</source>
        <year>2019</year>
        <volume>15</volume>
        <issue>4</issue>
        <fpage>e1006968</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006968</pub-id>
        <?supplied-pmid 30998681?>
        <pub-id pub-id-type="pmid">30998681</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Ke, G., Meng, Q., Finley, T., et al. LightGBM: A highly efficient gradient boosting decision tree. In: <italic>Advances in neural information processing systems</italic>. Vol 30. Curran Associates, Inc.; 2017. Accessed Jan 7, 2022. <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Lundberg, S.M., Lee, S.I. A unified approach to interpreting model predictions. In: <italic>Advances in neural information processing systems</italic>. Vol 30. Curran Associates, Inc.; 2017. Accessed Feb 10, 2023. <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ghoshal</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Uygun</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Effects of a patient-derived de novo coding alteration of CACNA1I in mice connect a schizophrenia risk gene with sleep spindle deficits</article-title>
        <source>Transl. Psychiatry</source>
        <year>2020</year>
        <volume>10</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1038/s41398-020-0685-1</pub-id>
        <pub-id pub-id-type="pmid">32066695</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McHugh</surname>
            <given-names>ML</given-names>
          </name>
        </person-group>
        <article-title>Interrater reliability: the kappa statistic</article-title>
        <source>Biochem. Medica.</source>
        <year>2012</year>
        <volume>22</volume>
        <issue>3</issue>
        <fpage>276</fpage>
        <lpage>282</lpage>
        <pub-id pub-id-type="doi">10.11613/BM.2012.031</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gramfort</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Luessi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Larson</surname>
            <given-names>E</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MEG and EEG data analysis with MNE-Python</article-title>
        <source>Front. Neurosci.</source>
        <year>2013</year>
        <volume>7</volume>
        <fpage>267</fpage>
        <pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id>
        <?supplied-pmid 24431986?>
        <pub-id pub-id-type="pmid">24431986</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Matplotlib: A 2D graphics environment | IEEE Journals &amp; Magazine | IEEE Xplore. Accessed Jan 6, 2022. <ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/document/4160265">https://ieeexplore.ieee.org/document/4160265</ext-link>.</mixed-citation>
    </ref>
  </ref-list>
</back>
