<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neurosci</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neurosci.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroscience</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1662-4548</issn>
    <issn pub-type="epub">1662-453X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7522368</article-id>
    <article-id pub-id-type="doi">10.3389/fnins.2020.00900</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Technology and Code</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Pyneal: Open Source Real-Time fMRI Software</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>MacInnes</surname>
          <given-names>Jeff J.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Adcock</surname>
          <given-names>R. Alison</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Stocco</surname>
          <given-names>Andrea</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/41792/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Prat</surname>
          <given-names>Chantel S.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rao</surname>
          <given-names>Rajesh P. N.</given-names>
        </name>
        <xref ref-type="aff" rid="aff4">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dickerson</surname>
          <given-names>Kathryn C.</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Institute for Learning and Brain Sciences, University of Washington</institution>, <addr-line>Seattle, WA</addr-line>, <country>United States</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Department of Psychiatry and Behavioral Sciences, Center for Cognitive Neuroscience, Duke Institute for Brain Sciences, Duke University</institution>, <addr-line>Durham, NC</addr-line>, <country>United States</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Department of Psychology, University of Washington</institution>, <addr-line>Seattle, WA</addr-line>, <country>United States</country></aff>
    <aff id="aff4"><sup>4</sup><institution>Department of Computer Science and Engineering, Center for Neurotechnology, University of Washington</institution>, <addr-line>Seattle, WA</addr-line>, <country>United States</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Alard Roebroeck, Maastricht University, Netherlands</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Shijie Zhao, Northwestern Polytechnical University, China; Mark S. Bolding, The University of Alabama at Birmingham, United States</p>
      </fn>
      <corresp id="c001">*Correspondence: Kathryn C. Dickerson, <email>kathryn.dickerson@duke.edu</email></corresp>
      <fn fn-type="other" id="fn004">
        <p>This article was submitted to Brain Imaging Methods, a section of the journal Frontiers in Neuroscience</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>9</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>14</volume>
    <elocation-id>900</elocation-id>
    <history>
      <date date-type="received">
        <day>20</day>
        <month>1</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>03</day>
        <month>8</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2020 MacInnes, Adcock, Stocco, Prat, Rao and Dickerson.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>MacInnes, Adcock, Stocco, Prat, Rao and Dickerson</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Increasingly, neuroimaging researchers are exploring the use of real-time functional magnetic resonance imaging (rt-fMRI) as a way to access a participant’s ongoing brain function throughout a scan. This approach presents novel and exciting experimental applications ranging from monitoring data quality in real time, to delivering neurofeedback from a region of interest, to dynamically controlling experimental flow, or interfacing with remote devices. Yet, for those interested in adopting this method, the existing software options are few and limited in application. This presents a barrier for new users, as well as hinders existing users from refining techniques and methods. Here we introduce a free, open-source rt-fMRI package, the Pyneal toolkit, designed to address this limitation. The Pyneal toolkit is python-based software that offers a flexible and user friendly framework for rt-fMRI, is compatible with all three major scanner manufacturers (GE, Siemens, Phillips), and, critically, allows fully customized analysis pipelines. In this article, we provide a detailed overview of the architecture, describe how to set up and run the Pyneal toolkit during an experimental session, offer tutorials with scan data that demonstrate how data flows through the Pyneal toolkit with example analyses, and highlight the advantages that the Pyneal toolkit offers to the neuroimaging community.</p>
    </abstract>
    <kwd-group>
      <kwd>real-time</kwd>
      <kwd>functional magnetic resonance imaging</kwd>
      <kwd>neurofeedback</kwd>
      <kwd>open source software</kwd>
      <kwd>python (programming language)</kwd>
      <kwd>neuroimaging methods</kwd>
      <kwd>rt-fMRI</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">W. M. Keck Foundation<named-content content-type="fundref-id">10.13039/100000888</named-content></funding-source>
      </award-group>
      <award-group>
        <funding-source id="cn002">National Center for Advancing Translational Sciences<named-content content-type="fundref-id">10.13039/100006108</named-content></funding-source>
        <award-id rid="cn002">1KL2TR002554</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn003">National Institute of Mental Health<named-content content-type="fundref-id">10.13039/100000025</named-content></funding-source>
        <award-id rid="cn003">R01 MH094743</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn004">National Science Foundation<named-content content-type="fundref-id">10.13039/100000001</named-content></funding-source>
        <award-id rid="cn004">EEC-1028725</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn005">Alfred P. Sloan Foundation<named-content content-type="fundref-id">10.13039/100000879</named-content></funding-source>
      </award-group>
      <award-group>
        <funding-source id="cn006">Esther A. and Joseph Klingenstein Fund<named-content content-type="fundref-id">10.13039/100001207</named-content></funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="5"/>
      <table-count count="1"/>
      <equation-count count="0"/>
      <ref-count count="28"/>
      <page-count count="16"/>
      <word-count count="0"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <title>Introduction</title>
    <p>Real-time functional magnetic resonance imaging (rt-fMRI) is an emerging technique that expands the scope of research questions beyond what traditional neuroimaging methods can offer (<xref rid="B25" ref-type="bibr">Sulzer et al., 2013a</xref>; <xref rid="B24" ref-type="bibr">Stoeckel et al., 2014</xref>; <xref rid="B23" ref-type="bibr">Sitaram et al., 2017</xref>; <xref rid="B15" ref-type="bibr">MacInnes and Dickerson, 2018</xref>). With traditional fMRI, brain activation is measured concurrently but independently from the experiment. All analyses (e.g., correlating behavior or cognitive state with brain activations) therefore, take place after the scan<sup><xref ref-type="fn" rid="footnote1">1</xref></sup> is completed, once the brain images and behavioral data have been saved and transferred to a shared location. In contrast, <italic>real-time fMRI</italic> is an approach whereby MRI data is accessed and analyzed throughout an ongoing scan, and can be incorporated directly into the experiment. Technological advances over the last decade have made it feasible to reconfigure an MRI environment to allow researchers to access and analyze incoming data at a rate that matches data acquisition. A few key advantages that rt-fMRI provides over traditional fMRI include the ability to: (1) monitor data quality in real time, thereby saving time and money, (2) provide participants with feedback from a region or network of regions in cognitive training paradigms, and (3) use ongoing brain activation as an independent variable to dynamically control the flow of an experimental task.</p>
    <p>While rt-fMRI has risen in popularity over the past decade (<xref rid="B25" ref-type="bibr">Sulzer et al., 2013a</xref>), the majority of imaging centers around the world remain unequipped to support this technique. In the past, this was primarily due to the computational demands exceeding scanner hardware capabilities [e.g., reconstructing and analyzing datasets composed of &gt;100 k voxels at a rate that matched data acquisition was not feasible (<xref rid="B27" ref-type="bibr">Voyvodic, 1999</xref>)]. Excitingly, modern day scanners available from each of the major MRI manufacturers – GE, Philips, and Siemens – are now outfitted with multicore processors, capable of operating in parallel to reconstruct imaging data and write files to disk while a scan is ongoing.</p>
    <p>The availability of fMRI data in real time presents novel opportunities to design experiments that incorporate information about ongoing brain activation. However, finding the right software tool to read images across multiple data formats, support flexible analyses, and integrate the results into an ongoing experimental presentation is a challenge. To date, the existing software options are limited for one or more reasons, including: cost (requiring a commercial license or dependent upon commercially licensed software such as Matlab) or a constrained choice of analysis options [e.g., region of interest (ROI) analysis only].</p>
    <p>In this article, we describe the Pyneal toolkit, an open source and freely available software package that was developed to address these limitations and support real-time fMRI. It is written entirely in Python, a programming language that offers flexibility and performance, balanced with readability and widespread support among the neuroimaging community. The Pyneal toolkit was built using a modular architecture to support a variety of different data formats, including those used across all three major MRI scanner manufacturers – GE, Philips, and Siemens. It offers built-in routines for basic data quality measures and single ROI summary statistics, as well as a web-based dashboard for monitoring the progress of ongoing scans. Its primary advantage, however, is that it offers an easy-to-use scaffolding on which users can design fully customized analyses to meet their unique experimental needs (e.g., neurofeedback from multiple ROIs, dynamic experimental control, classification of brain states, brain-computer interaction). This flexibility allows researchers full control over which neural regions to include, which analyses to carry out, and how the results of those analyses may be incorporated into the overall experimental flow. Moreover, computational and technological advances have ushered in new and more sensitive approaches to fMRI analyses. As the field continues to evolve, the ability to customize analyses within the Pyneal toolkit will allow researchers to quickly adapt new analytic methods to real-time experiments.</p>
    <p>The Pyneal toolkit was designed to offer a powerful and flexible tool to existing rt-fMRI practitioners as well as to lower the burden of entry for new researchers or imaging centers looking to add this capability to their facilities. Here we provide an overview of the software architecture, describe how it is used, offer tutorial data and analyses demonstrating how to use the Pyneal toolkit, and discuss the advantages of the Pyneal toolkit. We conclude by describing both limitations of and future directions for the Pyneal toolkit.</p>
  </sec>
  <sec id="S2">
    <title>Method</title>
    <p>The Pyneal toolkit is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jeffmacinnes/pyneal">https://github.com/jeffmacinnes/pyneal</ext-link> and full documentation is online at <ext-link ext-link-type="uri" xlink:href="https://jeffmacinnes.github.io/pyneal-docs/">https://jeffmacinnes.github.io/pyneal-docs/</ext-link>.</p>
    <sec id="S2.SS1">
      <title>Overview</title>
      <p>The Pyneal toolkit was created as a flexible and open-source option for researchers interested in pursuing real-time fMRI methods. The entire codebase is written in Python 3<sup><xref ref-type="fn" rid="footnote2">2</xref></sup> and integrates commonly used neuroimaging libraries (e.g., Nipy, NiBabel). For users developing customized real-time analyses, Python has a low burden of entry (compared to languages like Java or C++), while at the same time offers performance measures that meet or exceed the needs of basic research applications, in part due to backend numeric computing libraries (e.g., Numpy, Scipy) that are wrapped on top of a fast, C-based architecture.</p>
      <p>In order to support a wide range of data types and computing environments, the software is divided into two primary components: Pyneal Scanner and Pyneal<sup><xref ref-type="fn" rid="footnote3">3</xref></sup> (see <xref ref-type="fig" rid="F1">Figure 1</xref>). The two components communicate via TCP/IP connections, allowing users the flexibility to run the components on the same or different machines as required by their individual scanning environments<sup><xref ref-type="fn" rid="footnote4">4</xref></sup>. Internally, Pyneal uses ZeroMQ<sup><xref ref-type="fn" rid="footnote5">5</xref></sup>, a performant and reliable messaging framework, for all TCP/IP-based communication among its core processes.</p>
      <fig id="F1" position="float">
        <label>FIGURE 1</label>
        <caption>
          <p>Overview of the Pyneal toolkit. The Pyneal toolkit consists of two modules: Pyneal Scanner and Pyneal. Pyneal Scanner receives the raw data and transforms it into a standardized format for Pyneal to use. Pyneal analyzes the data in real time and makes it available for subsequent use (e.g., by a remote End User for experimental display). Pyneal Scanner and Pyneal can operate on the same computer (e.g., dedicated analysis computer) or separate computers (as required by the specific scanning environment).</p>
        </caption>
        <graphic xlink:href="fnins-14-00900-g001"/>
      </fig>
      <p>During a scan, Pyneal Scanner is responsible for converting data into a standardized format and passing it along to Pyneal (see <xref ref-type="fig" rid="F2">Figure 2</xref>). Pyneal receives incoming data, carries out the specified preprocessing and analysis steps, and stores the results of the analysis on a locally running server. Throughout the scan, any remote End User (e.g., a workstation running the experimental task) can retrieve analysis results from Pyneal at any point. Each of these components is discussed in greater detail below.</p>
      <fig id="F2" position="float">
        <label>FIGURE 2</label>
        <caption>
          <p>Process flow diagram illustrating the multi-threaded nature of the Pyneal toolkit. Pyneal Scanner has two sub-modules: a scan watcher and scan processor. The scan watcher monitors and adds all new raw images to a queue. The scan processor receives all new raw images from the queue, extracts the image data, transforms it to a standardized format, and sends it to Pyneal for analysis. Pyneal operates as an independent, multi-threaded component and has three sub-modules: scan receiver, scan processor, and results server. The scan receiver receives formatted data from Pyneal Scanner and sends it to the scan processor, which completes the specified analyses and sends them to the results server. The results server listens to incoming requests from End Users (e.g., experimental task).</p>
        </caption>
        <graphic xlink:href="fnins-14-00900-g002"/>
      </fig>
    </sec>
    <sec id="S2.SS2">
      <title>Pyneal Scanner</title>
      <p>Given the range of potential input data formats, depending on the scanning environment, we aimed to standardize the incoming data in a way that allows subsequent processing steps to be environment agnostic. Thus we divided the overall Pyneal toolkit architecture into two components that operate independently, enabling one component, Pyneal Scanner, to adapt to the idiosyncrasies of the local scanning environment without affecting the downstream processing and analysis stages of the Pyneal component (see <xref ref-type="fig" rid="F2">Figure 2</xref>).</p>
      <p>Architecturally, Pyneal Scanner uses a multithreaded design with one thread <italic>monitoring</italic> for the appearance of new image data, and a second thread <italic>processing</italic> image data as it appears. This design allows Pyneal Scanner to efficiently process incoming scan data with minimal latency (in practice, under typical scanning conditions, the latency between when new image data arrives and is processed is on the order of tens of milliseconds). Throughout a scan, new images that appear from the scanner are placed into a queue. The processing thread pulls individual files from that queue and converts the data to a standardized format. In addition, header information from the first images to arrive is processed to determine key metadata about the current scan, including total volume dimensions, voxel spacing, total number of expected time points, and the affine transformation needed to reorient the data to RAS+ format (axes increase from left to right, posterior to anterior, inferior to superior).</p>
      <p>Pyneal Scanner is initialized through a simple configuration text file specifying the scanner type and paths to where data files are expected to appear throughout a scan. Users can create this file manually, or follow the command line prompts when first launching; in either case, once Pyneal Scanner is configured at the start of a session, it does not need to be modified, unless the scanning environment itself is modified. In that case, users can update Pyneal Scanner without having to add any additional modifications to downstream processes in Pyneal. Regardless of how and where the data arrives from the scanner, as long as Pyneal Scanner continues to output data in the expected format, subsequent stages in the pipeline will proceed unaffected. This is a significant advantage that provides researchers the necessary latitude to customize the installation to their unique environment.</p>
      <p>Pyneal Scanner has built-in routines for handling common data formats used in GE (e.g., 2D dicom slice files), Siemens (e.g., 3D dicom mosaic files), and Philips scanners (e.g., PAR/REC files), and is easily extensible to incorporate additional formats that may emerge in the future.</p>
      <p>As soon as a complete volume (i.e., 3D array of voxel values from a single time point) has arrived, it is passed along to Pyneal via a dedicated TCP/IP socket interface. This arrangement allows Pyneal Scanner and Pyneal to run on separate machines or as separate processes on the same machine, depending on the particular requirements of the local scanning environment. For instance, if newly arriving images are only accessible from the scanner console itself, Pyneal Scanner can run on that machine, monitoring the local directory where new images appear, and then transferring processed volumes to Pyneal running on a separate dedicated machine. Alternatively, the scanner network configuration may be such that it is possible to remotely mount the directory where new images appear, allowing Pyneal Scanner and Pyneal to run concurrently on the same machine.</p>
      <p>Each transmitted volume from Pyneal Scanner to Pyneal occurs in two waves: First, Pyneal Scanner sends a JSON-formatted header that contains relevant metadata about the current volume, including the time point index and volume dimensions. Second, it sends the numeric array representing the volume data itself. Pyneal uses the information from the header to reconstruct the incoming array, store it as a memory- and computation-efficient Numpy array, and index the volume in a way to facilitate subsequent processing and analysis steps.</p>
    </sec>
    <sec id="S2.SS3">
      <title>Pyneal</title>
      <p>Pyneal is divided up into three distinct submodules that operate efficiently in a multithreaded configuration: submodule 1: the scan receiver, accepts incoming data from Pyneal Scanner; submodule 2: the processing module, oversees the preprocessing and analysis stages on each incoming volume, and submodule 3: the results server fields requests for data from remote End Users throughout the scan (see <xref ref-type="fig" rid="F2">Figure 2</xref>).</p>
      <p>As described above, throughout a scan Pyneal’s submodule 1 (scan receiver) receives re-formatted data from Pyneal Scanner. Each new data point is represented as a 3D matrix of voxel values corresponding to a single sample (i.e., one TR). The JSON header that Pyneal Scanner provides with every transmission allows Pyneal to reconstruct the 3D volume with the correct dimensions, as well as assign it the proper index location in time. Each new volume is passed to the proper location of a preallocated 4D matrix that incrementally fills in throughout the scan.</p>
      <p>Submodule 2 (processing module) accepts each 3D volume and submits it through preprocessing and analysis stages. The preprocessing stage estimates motion using a histogram registration algorithm and yields mean displacement in millimeters relative to a fixed reference volume from the start of the run (absolute motion), as well as relative to the previous time point (relative motion) (<xref rid="B11" ref-type="bibr">Jenkinson, 2000</xref>).</p>
      <p>The analysis stage takes the preprocessed volume and runs the specified analyses or computations on the volume. Users have the option of selecting from built-in analysis routines (including calculating a weighted or unweighted mean signal within a supplied ROI mask), or, importantly, can generate and include their own custom analysis script (written in Python) that will be executed on each volume. The ability to design and execute customized analyses in real-time provides researchers the freedom to measure and use ongoing brain activations however they desire. See <italic>Using Pyneal</italic> below for more details on selecting an analysis or building a custom analysis script.</p>
      <p>The analysis stage is capable of computing and returning multiple results on each volume (e.g., mean signal from multiple distinct ROIs). The computed results are tagged with the corresponding volume index, and passed along to the third submodule: the results server.</p>
      <p>Submodule 3, (the results server), listens for and responds to incoming requests for specific results from an End User throughout the scan. An End User is anything that may wish to access real-time results throughout an on-going scan (e.g., experimental presentation software that will present results as neurofeedback to the participant in the scanner). To request results, the End User sends a specific volume index to the results server via a TCP/IP socket interface. The result server receives the request and checks to see if the requested volume has arrived and been analyzed. Responses are sent as a JSON-formatted reply to the End User. If the requested volume has not been processed yet, the reply message from the Result Server will contain the entry foundResults: False; if the requested volume exists, the Results Server retrieves the requested results for that volume, and sends a reply message to the End User that contains foundResults: True as well as the full set of results for that volume. The End User can then parse and make use of the results as needed (e.g., update a graphical display showing mean percent signal change in an ROI).</p>
      <p>At the completion of each run, Pyneal creates a unique output directory for the current scan. The scan data is written to this directory as a 4D NIFTI image, along with a JSON file containing all computed results as well as log files.</p>
    </sec>
    <sec id="S2.SS4">
      <title>Using Pyneal</title>
      <p>Once installed, users can interact with and customize Pyneal via configuration files and graphical user interfaces (GUIs). At the start of a new scan, the user needs to launch both Pyneal Scanner and Pyneal.</p>
      <p>Launching Pyneal Scanner is done via the command line. Pyneal Scanner uses a configuration text file to obtain parameters specific to the current computing environment, including the scanner make and the directory path where new incoming data is expected to appear (see example in the Full Pipeline tutorial below, section “Pyneal Toolkit <bold><italic>–</italic></bold> Full Pipeline Tutorial”). Users can manually create this configuration file ahead of time, or, if no file exists, the user will be prompted to specify the parameters via the command line when launching. Parameters specified via the command line will be written into the configuration file and saved to disk. Pyneal Scanner will automatically read this configuration file at the start of every scan. Thus, Pyneal Scanner needs to be configured only once at the beginning of each experimental session.</p>
      <p>Launching Pyneal is also done via the command line. Upon launching Pyneal at the start of each scan (run), the user is presented with a setup GUI for configuring Pyneal to the current scan (see <xref ref-type="fig" rid="F3">Figure 3</xref>). The setup GUI includes sections for socket communication parameters (e.g., IP address), selecting an input mask, setting preprocessing parameters, choosing analyses, and specifying an output directory. Some parameters, like the socket communication host address and ports, are unlikely to change from experimental session to session, while other parameters, most notably the input mask and output directory, will be specific to experimental session and/or each individual scan. The GUI is populated with the last used settings to minimize set-up time, however, the GUI must be launched before each scan.</p>
      <fig id="F3" position="float">
        <label>FIGURE 3</label>
        <caption>
          <p>Pyneal Graphical User Interface (GUI). The Pyneal GUI contains the following sections: (1) Communication: allows Pyneal to communicate with Pyneal Scanner and any End Users. This includes the IP address of the computer running Pyneal as well as the port numbers for Pyneal Scanner and End Users to communicate with Pyneal. (2) Mask: users have the option of loading a mask to use during real-time fMRI runs (weighted or unweighted). (3) Preprocessing: users specify the number of timepoints (volumes) in the run. (4) Analysis: users may choose between one of the default options (calculating the average or median of a mask) or importantly can upload a custom analysis script (e.g., correlation between two regions). (5) Output: users specify a location where the output files are saved.</p>
        </caption>
        <graphic xlink:href="fnins-14-00900-g003"/>
      </fig>
      <p>The setup GUI asks users to specify the path to an input mask, which will be used during the analysis stage of a scan. If the user selects one of the built-in analysis options (i.e., calculate an <italic>average</italic> or <italic>median</italic>), the mask will define which voxels are included in the calculation. Alternatively, if the user chooses to use a custom analysis, a reference to this mask will be passed into the custom analysis script, which the user is free to use or ignore as needed. In addition, the mask panel also allows users to specify whether or not to use voxel values from the mask as weights in subsequent analyses.</p>
      <p>All analyses in Pyneal take place in the native functional space of the current scan, and as such, this mask is required to match the dimensions and orientation of the incoming functional data. For cases where the user wishes to use an existing anatomical mask in a different imaging space (e.g., MNI space), the Pyneal toolkit includes a Create Mask tool (<italic>utils/createMask.py</italic>) for transforming masks to the functional space of the current subject [see <xref ref-type="fig" rid="F4">Figure 4</xref>; Note that this functionality requires FSL (<xref rid="B12" ref-type="bibr">Jenkinson et al., 2012</xref>) to be installed].</p>
      <fig id="F4" position="float">
        <label>FIGURE 4</label>
        <caption>
          <p>Create Mask GUI. This GUI assists users in making a mask that can be used in analysis during the real-time fMRI runs. Users can choose between making a whole brain mask or a mask from a pre-specified MNI template (e.g., amygdala ROI). Users must load an example functional data file for both mask types. When creating a mask from an MNI template, users must additionally load an anatomical data file, specify the path to the MNI standard brain file, the MNI mask file, and specify the new file name (output prefix). Note, this tool requires FSL.</p>
        </caption>
        <graphic xlink:href="fnins-14-00900-g004"/>
      </fig>
      <p>Pyneal includes built-in analysis options for calculating the average and median activation levels across all voxels in the supplied mask. For experiments that wish to present neurofeedback from a single ROI, these options may be appropriate. However, one of Pyneal’s primary advantages is the ability to run fully customized analyses. By selecting “custom” in the analysis panel, the user will be prompted to choose a python-based analysis script they have composed. Pyneal requires that a custom script contain certain functions in order to integrate with the rest of the Pyneal pipeline throughout a scan. However, beyond that basic structure, there are few limitations on what users may wish to include. To assist users in designing a custom analysis script, we include a basic template file<sup><xref ref-type="fn" rid="footnote6">6</xref></sup> with the required named functions and input/output variable names that users can expand upon as needed. The benefit of this approach is that it liberates users to design analysis approaches that are best suited to their experimental questions, all while fully integrating into the existing Pyneal pipeline.</p>
      <p>Lastly, users are able to specify an output directory for the current experimental session. During an experimental session, the output from each scan will be saved to its own unique subdirectory within this output directory. The saved output from each scan includes a log file showing all settings and messages recorded throughout the scan, a JSON file containing all of the computed analysis results, and a 4D NIFTI image containing the functional data as received by Pyneal.</p>
      <p>Once the user hits “submit,” Pyneal will establish communication with Pyneal Scanner, launch the results server, and wait for the scan to start and data to appear.</p>
    </sec>
    <sec id="S2.SS5">
      <title>Web-Based Dashboard</title>
      <p>Once the scan begins, users are presented with a web-based dashboard (see <xref ref-type="fig" rid="F5">Figure 5</xref>) viewable in an internet browser. The dashboard updates in real-time allowing users to view the progress of the scan, and monitor the status via four separate components. A plot in the top-left displays on-going head motion estimates expressed in millimeters relative to both a fixed reference volume (absolute displacement) and the previous volume (relative displacement). In the top-right, a separate plot shows the processing time for each volume. By monitoring this plot, users can ensure that all analyses are completing at a rate that keeps pace with data acquisition. At the bottom, two log windows allow users to watch incoming messages from Pyneal Scanner (bottom left) and communication between Pyneal’s results server and any End User (bottom right).</p>
      <fig id="F5" position="float">
        <label>FIGURE 5</label>
        <caption>
          <p>Pyneal Dashboard. This web-based dashboard allows users to monitor analysis and progress during real-time runs. The current volume is displayed along with basic information about the scan (e.g., mask, analysis, etc.). Two plots indicate: (1) head motion (top left) – both relative (compared to previous volume) and absolute (compared to the start of the scan) and (2) processing time for each volume (top right). Two log windows display: (1) messages from Pyneal Scanner (bottom left) and (2) communication between Pyneal and the End User (bottom right).</p>
        </caption>
        <graphic xlink:href="fnins-14-00900-g005"/>
      </fig>
    </sec>
  </sec>
  <sec id="S3">
    <title>Results</title>
    <p>Here we present two complementary tutorials and results using real fMRI data. Section “Pyneal Toolkit – Full Pipeline Tutorial” details how to set up and use the Pyneal toolkit. It demonstrates the full pipeline of data flow throughout the Pyneal Toolkit. Section “Pyneal Analysis Tutorial” describes in more detail how to run two example analyses in Pyneal – one using the default built-in ROI-averaging tool in the toolkit and the second using a custom analysis script. Please see: <ext-link ext-link-type="uri" xlink:href="https://github.com/jeffmacinnes/pyneal-tutorial">https://github.com/jeffmacinnes/pyneal-tutorial</ext-link> for full access to the data and scripts for both tutorials.</p>
    <p>Both tutorials assume the user has downloaded and installed the Pyneal toolkit and the Pyneal Tutorial repositories in their local folder. If so, the following directories should be located in the user’s home directory:</p>
    <list list-type="simple">
      <list-item>
        <p>∼/pyneal</p>
      </list-item>
      <list-item>
        <p>∼/pyneal-tutorial</p>
      </list-item>
    </list>
    <sec id="S3.SS1">
      <title>Pyneal Toolkit – Full Pipeline Tutorial</title>
      <p>The goal of this tutorial is to test the Pyneal toolkit’s complete pipeline using conditions similar to what is available at the three major scanner manufacturers. This tutorial uses the Scanner Simulator command line tool that comes with the Pyneal toolkit. This tool mimics the behavior of an actual scanner by writing image data to an output directory at a steady rate (directory and rate specified by the user). The source data (included) are actual scan images from GE, Philips, and Siemens scanners. These data are meant to simulate the format and directory structure typical of each of these platforms. This tutorial allows users to test the complete Pyneal toolkit’s pipeline on any of these platforms prior to actual data collection.</p>
      <p>Regardless of scanner type, each platform follows the same general steps:</p>
      <list list-type="simple">
        <list-item>
          <label>•</label>
          <p>Set up the Scan Simulator.</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p>Set up Pyneal Scanner.</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p>Set up Pyneal.</p>
        </list-item>
      </list>
      <p>Below we provide a complete example using the Siemens’ scanner setup. Please see <ext-link ext-link-type="uri" xlink:href="https://github.com/jeffmacinnes/pyneal-tutorial">https://github.com/jeffmacinnes/pyneal-tutorial</ext-link> for source data and information for all scanner types, including examples using GE and Philips scanners.</p>
      <p><italic>Siemens Full Pipeline Tutorial</italic>:</p>
      <p>Inside the Siemens_demo folder, there is a directory named scanner. This directory serves as the mock scanner for this tutorial, and follows a structure similar to what is observed on actual Siemens scanners. There’s a single session directory (data) that contains all of the dicom files for two functional series (000013, 000015) and an anatomical series (for more source data detail, see Appendix: Siemens source data within: <ext-link ext-link-type="uri" xlink:href="https://github.com/jeffmacinnes/pyneal-tutorial/blob/master/FullPipelineTutorial.md">https://github.com/jeffmacinnes/pyneal-tutorial/blob/master/FullPipelineTutorial.md</ext-link>).</p>
      <p>We will use the Scanner Simulator tool to simulate a new functional series, using 000013 as our source data. The new series will appear in the session directory alongside the existing series files, and dicom files will contain the series name 000014.</p>
      <p>To perform this tutorial the following steps are required:</p>
      <list list-type="simple">
        <list-item>
          <label>I.</label>
          <p>Launch <monospace>Siemens_sim.py</monospace> with the desired input data</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p>Open a new terminal window and navigate to the Scanner Simulator tool: <monospace>cd ∼ /pyneal/pyneal_</monospace>
<monospace>scanner/simulation/scannerSimulators</monospace></p>
        </list-item>
        <list-item>
          <label>•</label>
          <p>launch <monospace>Siemens_sim.py,</monospace> specifying paths to the source directory (<monospace>∼/pyneal-tutorial/Siemens_ demo/scanner/data</monospace>) and series numbers (<monospace>000013</monospace>). The user can also specify the new series number (<monospace>-n 000014</monospace>), and TR (<monospace>-t 1000</monospace>) if desired.</p>
        </list-item>
        <list-item>
          <p>
            <monospace>python Siemens_sim.py ∼/pyneal- tutorial/Siemens_demo/scanner/data 000013 -t 1000 -n 000014</monospace>
          </p>
        </list-item>
      </list>
      <p>The user should see details about the current scan, and an option to press ENTER to begin the scan:</p>
      <list list-type="simple">
        <list-item>
          <p>————————</p>
        </list-item>
        <list-item>
          <p>
            <monospace>Source dir: ∼/pyneal-tutorial/Siemens_ demo/scanner/data</monospace>
          </p>
        </list-item>
        <list-item>
          <p>
            <monospace>Total Mosaics Found: 60</monospace>
          </p>
        </list-item>
        <list-item>
          <p>
            <monospace>TR: 1000</monospace>
          </p>
        </list-item>
        <list-item>
          <p>
            <monospace>Press ENTER to begin the “scan”</monospace>
          </p>
        </list-item>
      </list>
      <p>Before starting the simulator, first complete the following two steps: setting up Pyneal Scanner and Pyneal.</p>
      <list list-type="simple">
        <list-item>
          <label>II.</label>
          <p>Configure Pyneal Scanner to watch for new scan data in the session directory for the Siemens mock scanner.</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p>open a second terminal window, and navigate to Pyneal Scanner: <monospace>cd ∼/pyneal/pyneal_scanner</monospace></p>
        </list-item>
        <list-item>
          <label>•</label>
          <p>create (or edit the existing) scannerConfig.yaml file in this directory to set the scannerMake to Siemens and the scannerBaseDir to the mock scanner folder. The contents of the scannerConfig.yaml file should be:</p>
        </list-item>
        <list-item>
          <p>
            <monospace>pynealSocketHost: 127.0.0.1</monospace>
          </p>
        </list-item>
        <list-item>
          <p>
            <monospace>pynealSocketPort: ‘5555’</monospace>
          </p>
        </list-item>
        <list-item>
          <p>
            <monospace>scannerBaseDir: ∼/pyneal-tutorial/Siemens_demo/scanner/data</monospace>
          </p>
        </list-item>
        <list-item>
          <p>
            <monospace>scannerMake: Siemens</monospace>
          </p>
        </list-item>
        <list-item>
          <label>•</label>
          <p>launch Pyneal Scanner:</p>
        </list-item>
        <list-item>
          <p>
            <monospace>python pynealScanner.py</monospace>
          </p>
        </list-item>
      </list>
      <p>The user should see details about the current session, and an indication that Pyneal Scanner is attempting to connect to Pyneal:</p>
      <list list-type="simple">
        <list-item>
          <p>================</p>
        </list-item>
        <list-item>
          <p>
            <monospace>Session Dir:</monospace>
          </p>
        </list-item>
        <list-item>
          <p>
            <monospace>∼/pyneal-tutorial/Siemens_demo/scanner/data</monospace>
          </p>
        </list-item>
        <list-item>
          <p>
            <monospace>Unique Series:</monospace>
          </p>
          <list list-type="simple">
            <list-item>
              <p>
                <monospace>000013 60 files 1113170 min, 51 s ago</monospace>
              </p>
            </list-item>
            <list-item>
              <p>
                <monospace>000015 60 files 1113170 min, 51 s ago</monospace>
              </p>
            </list-item>
            <list-item>
              <p>
                <monospace>000017 52 files 1113170 min, 51 s ago</monospace>
              </p>
            </list-item>
          </list>
        </list-item>
        <list-item>
          <p>
            <monospace>MainThread - Connecting to pynealSocket…</monospace>
          </p>
        </list-item>
      </list>
      <p>There is nothing more to do in this terminal window. Once Pyneal is set up and the Scan Simulator tool starts, Pyneal Scanner will begin processing new images as they appear and sending the data to Pyneal. The user can monitor the progress via the log messages that appear in this terminal.</p>
      <list list-type="simple">
        <list-item>
          <label>III.</label>
          <p>Configure and launch Pyneal</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p>Open a third terminal window, navigate to and launch Pyneal.</p>
        </list-item>
        <list-item>
          <p>
            <monospace>cd ∼/pyneal</monospace>
          </p>
        </list-item>
        <list-item>
          <p>
            <monospace>python pyneal.py</monospace>
          </p>
        </list-item>
        <list-item>
          <label>•</label>
          <p>Configure Pyneal for the Siemens tutorial demo:</p>
          <list list-type="simple">
            <list-item>
              <label>∘</label>
              <p>Communication: Set the <monospace>Pyneal Host IP</monospace> to <monospace>127.0.0.1</monospace>, the <monospace>Pyneal-Scanner Port</monospace> to <monospace>5555</monospace>, and the <monospace>Results Server Port</monospace> to <monospace>5558</monospace>.</p>
            </list-item>
            <list-item>
              <label>∘</label>
              <p>Mask: In the <monospace>Siemens_demo</monospace> directory, there is a file named <monospace>dummyMask_64-64-18.nii.gz.</monospace> Set the mask value in Pyneal to use this file. This mask was pre-made to match the volume dimensions of the <monospace>Siemens_demo</monospace> scan data. This mask is simply a rectangle positioned in the middle slice of the 3D volume, and is for demonstration purposes only. The user can unselect the <monospace>Weighted Mask?</monospace> option</p>
            </list-item>
            <list-item>
              <label>∘</label>
              <p>Preprocessing: Set # <monospace>of timepts</monospace> to: <monospace>60.</monospace> The user can keep the <monospace>Estimate Motion?</monospace> option selected if preferred.</p>
            </list-item>
            <list-item>
              <label>∘</label>
              <p>Analysis: Select the <monospace>Average</monospace> option.</p>
            </list-item>
            <list-item>
              <label>∘</label>
              <p>Output: Set the output directory to <monospace>∼/ pyneal-tutorial/Siemens_demo/output.</monospace> Check <monospace>Launch Dashboard?</monospace></p>
            </list-item>
          </list>
        </list-item>
        <list-item>
          <label>•</label>
          <p>Start Pyneal by pressing <monospace>Submit.</monospace></p>
          <list list-type="simple">
            <list-item>
              <label>∘</label>
              <p>In the Pyneal Scanner terminal, the user will see messages indicating that Pyneal Scanner has successfully set up a connection to Pyneal and that it is waiting for a new <monospace>seriesDir</monospace> (which will be created once the scan starts).</p>
            </list-item>
            <list-item>
              <label>∘</label>
              <p>In addition, the user can open a browser window and enter <monospace>127.0.0.1:5558</monospace> in the URL bar to see the Pyneal dashboard.</p>
            </list-item>
          </list>
        </list-item>
      </list>
      <list list-type="simple">
        <list-item>
          <label>IV.</label>
          <p>Start demo</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p>In the first terminal window, where the Scan Simulator tool is running, press <monospace>ENTER</monospace> to begin the scan.</p>
        </list-item>
        <list-item>
          <label>•</label>
          <p>As the scan is progressing, each of the three terminal windows will update with new log messages. In addition, the user can monitor the progress from the dashboard in a web browser at <monospace>127.0.0.1:5558.</monospace></p>
        </list-item>
        <list-item>
          <label>•</label>
          <p>As soon at the scan finishes, the user can find the Pyneal output at <monospace>∼/pyneal-tutorial/Siemens_ demo/output/pyneal_001.</monospace> This directory will have:</p>
          <list list-type="simple">
            <list-item>
              <label>∘</label>
              <p><italic>pynealLog.log:</italic> log file from the current scan.</p>
            </list-item>
            <list-item>
              <label>∘</label>
              <p><italic>receivedFunc.nii.gz:</italic> 4D nifti file of the data, as received by Pyneal <sup>∗</sup>
<italic>results.json:</italic> JSON file containing the analysis results from the current scan.</p>
            </list-item>
          </list>
        </list-item>
      </list>
    </sec>
    <sec id="S3.SS2">
      <title>Pyneal Analysis Tutorial</title>
      <p>The goal of this tutorial is to guide users through two different analyses using Pyneal. We provide real fMRI data (note – this tool also allows for use of randomly generated data). This tutorial uses the <monospace>pynealScanner_sim.py</monospace> command line tool that comes with the Pyneal toolkit. This tool takes real or generated data, breaks it apart, and sends it to Pyneal for analysis. The source data (included) is a nifti file from one run of a hand squeezing task. It alternates between blocks of squeeze and rest (each 20 s, repeated five times). The first analysis demonstrates Pyneal’s built-in ROI neurofeedback tool. The second demonstrates use of a custom analysis script: correlating the activation of two ROIs and using it for neurofeedback.</p>
      <sec id="S3.SS2.SSS1">
        <title>Neurofeedback: Single ROI Averaging Using Built-in Analysis Functions</title>
        <p>Example: A researcher wishes to provide participants with neurofeedback from the primary motor cortex (M1) in a hand-squeezing task. The M1 ROI is defined on the basis of an anatomical mask using the Juelich atlas in FSL.</p>
        <p><italic>Tutorial</italic>: Ordinarily, the first step is to create a unique mask in functional space of the target ROI (M1). For the purposes of this tutorial, we provide the ROI in subject-specific space for users. We used the left M1 ROI from the Juelich atlas freely available in FSL. We thresholded the mask at 10% and binarized it using <monospace>fslmaths.</monospace> Then using <monospace>flirt,</monospace> we converted the left M1 mask (in MNI space) to functional space (subject-specific). The resulting mask, <monospace>L_MotorCortex.nii.gz</monospace> is now ready to use in this tutorial.</p>
        <p>This tutorial uses the Pyneal Scanner simulation script, which is located in:</p>
        <list list-type="simple">
          <list-item>
            <p>
              <monospace>∼/pyneal/utils/simulation/pyneal Scanner_sim.py</monospace>
            </p>
          </list-item>
        </list>
        <p>Usage includes:</p>
        <list list-type="simple">
          <list-item>
            <p>
              <monospace>python pynealScanner_sim.py [–filePath] [–random] [–dims] [–TR] [–sockethost] [–socketport]</monospace>
            </p>
          </list-item>
        </list>
        <p>Input arguments:</p>
        <list list-type="simple">
          <list-item>
            <label>•</label>
            <p>
              <monospace>-f/–filePath: path to 4D nifti image the user wants to use as the “scan” data. Here we are using “func.nii.gz” provided in ∼/pyneal-tutorial/analysis Tutorial as our input data.</monospace>
            </p>
          </list-item>
          <list-item>
            <label>•</label>
            <p>
              <monospace>-r/–random: flag to generate random data instead of using a pre-existing nifti image</monospace>
            </p>
          </list-item>
          <list-item>
            <label>•</label>
            <p>
              <monospace>-d/–dims: desired dimensions of randomly generated dataset [default: 64 64 18 60]</monospace>
            </p>
          </list-item>
          <list-item>
            <label>•</label>
            <p>
              <monospace>-t/–TR: set the TR in ms [default: 1000]</monospace>
            </p>
          </list-item>
          <list-item>
            <label>•</label>
            <p>
              <monospace>-sh/–sockethost: IP address Pyneal host [default: 127.0.0.1]</monospace>
            </p>
          </list-item>
          <list-item>
            <label>•</label>
            <p>
              <monospace>-sp/–socketport: port number to send 3D volumes over to Pyneal [default: 5555]</monospace>
            </p>
          </list-item>
        </list>
        <p>To run the tutorial, the following steps are required:</p>
        <list list-type="simple">
          <list-item>
            <label>I.</label>
            <p>Launch pynealScanner_sim.py script</p>
          </list-item>
        </list>
        <list list-type="simple">
          <list-item>
            <p>
              <monospace>python pynealScanner_sim.py -f ∼/pyneal -tutorial/analysisTutorial/func. nii.gz -t 1000 -sh 127.0.0.1 -sp 5555</monospace>
            </p>
          </list-item>
        </list>
        <p>Here we are setting the TR to 1000 ms, the host socket number to 127.0.0.1 and the port number to 5555. This tool will simulate the behavior of Pyneal Scanner. During a real scan, Pyneal Scanner will send data to Pyneal over a socket connection. Each transmission comes in two phases: (1) a json header with metadata about the volume and (2) the volume itself.</p>
        <p>Once the user hits enter, she should see the following:</p>
        <list list-type="simple">
          <list-item>
            <p>
              <monospace>Prepping dataset: ∼/pyneal-tutorial/analysisTutorial/func.nii.gz</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>Dimensions: (64, 64, 18, 208)</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>TR: 1000</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>Connecting to Pyneal at 127.0.0.1:5555</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>waiting for connection…</monospace>
            </p>
          </list-item>
        </list>
        <list list-type="simple">
          <list-item>
            <label>II.</label>
            <p>Launch Pyneal using the appropriate configurations. In a new terminal window type:</p>
          </list-item>
        </list>
        <list list-type="simple">
          <list-item>
            <p>
              <monospace>cd ∼/pyneal</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>python pyneal.py</monospace>
            </p>
          </list-item>
        </list>
        <p>This will launch the Pyneal GUI. Configure Pyneal with the following:</p>
        <list list-type="simple">
          <list-item>
            <label>∘</label>
            <p>Communication: Set the <monospace>Pyneal Host IP</monospace> to <monospace>127.0.0.1,</monospace> the <monospace>Pyneal-Scanner Port</monospace> to <monospace>5555,</monospace> and the <monospace>Results Server Port</monospace> to <monospace>5558</monospace>.</p>
          </list-item>
          <list-item>
            <label>∘</label>
            <p>Mask: In the <monospace>∼/pyneal-tutorial/analysis Tutorial/masks/</monospace> directory, there is a file named <monospace>L_MotorCortex.nii.gz.</monospace> Set the mask value in Pyneal to use this file. The user can unselect the <monospace>Weighted Mask?</monospace> option. Once the mask is loaded, the GUI should display the volume dimensions of the selected mask (here 64, 64, 18), allowing us to confirm a match with the dimensions of the upcoming scan.</p>
          </list-item>
          <list-item>
            <label>∘</label>
            <p>Preprocessing: Set # <monospace>of timepts</monospace> to: <monospace>208.</monospace> The user can keep the <monospace>Estimate Motion?</monospace> option selected if preferred.</p>
          </list-item>
          <list-item>
            <label>∘</label>
            <p>Analysis: Select the <monospace>Average</monospace> option.</p>
          </list-item>
          <list-item>
            <label>∘</label>
            <p>Output: Set the output directory to <monospace>∼/pyneal-tutorial/analysisTutorial/output.</monospace> Check <monospace>Launch Dashboard?</monospace></p>
          </list-item>
        </list>
        <p>The user can then hit <monospace>Submit</monospace> to start Pyneal.</p>
        <list list-type="simple">
          <list-item>
            <label>III.</label>
            <p>Start the scan</p>
          </list-item>
        </list>
        <list list-type="simple">
          <list-item>
            <p>Back in the Scan Simulator terminal, the user should see a successful connection to Pyneal</p>
          </list-item>
        </list>
        <list list-type="simple">
          <list-item>
            <p>
              <monospace>connected to pyneal</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>Press ENTER to begin the “scan”</monospace>
            </p>
          </list-item>
        </list>
        <list list-type="simple">
          <list-item>
            <label>IV.</label>
            <p>Hit Enter to begin the simulated scan</p>
          </list-item>
        </list>
        <p>As soon as the scan simulation begins, Pyneal Scanner begins processing and transmitting volumes of the provided data (<monospace>func.nii.gz</monospace>) to Pyneal, which calculates the mean activation within the target region on each volume and stores the results on the Pyneal’s Results Server. As the scan is progressing, the user should see information about each volume appear in both the Scan Simulator and Pyneal terminals, indicating the volumes are being successfully transmitted and processed.</p>
        <list list-type="simple">
          <list-item>
            <label>V.</label>
            <p>Results</p>
          </list-item>
          <list-item>
            <label>•</label>
            <p>At the completion of the scan, the user can find the following Pyneal output files in <monospace>∼/pyneal- tutorial/analysisTutorial/output/pyneal_ 001</monospace> (<italic>Note</italic>: the directory names increase in sequence. If this is the first time saving output to this directory, it will be <monospace>_001,</monospace> otherwise it will be a larger number):</p>
            <list list-type="simple">
              <list-item>
                <label>∘</label>
                <p><monospace>pynealLog.log:</monospace> complete log file from the scan.</p>
              </list-item>
              <list-item>
                <label>∘</label>
                <p><monospace>receivedFunc.nii.gz:</monospace> 4D Nifti of the data, as received by Pyneal.</p>
              </list-item>
              <list-item>
                <label>∘</label>
                <p><monospace>results.json:</monospace> JSON-formatted file containing the computed analysis results at each timepoint.</p>
              </list-item>
            </list>
          </list-item>
          <list-item>
            <label>•</label>
            <p>Since the input data here came from a simple hand squeezing task where we computed the average signal within the Left Motor Cortex, we expect to see a fairly robust signal in the results, following the alternating blocks design of the task.</p>
            <list list-type="simple">
              <list-item>
                <label>∘</label>
                <p>To confirm, the user can open the results.json file and plot the results at each timepoint using the user’s preferred tools (e.g., Python, Matlab).</p>
              </list-item>
            </list>
          </list-item>
        </list>
        <p>Note – it is also possible to use this setup to test communication with an End User (e.g., experimental presentation script) if desired. See <ext-link ext-link-type="uri" xlink:href="https://jeffmacinnes.github.io/pyneal-docs/simulations/">https://jeffmacinnes.github.io/pyneal-docs/simulations/</ext-link> for more details.</p>
        <p>See <italic><xref rid="B16" ref-type="bibr">MacInnes et al. (2016)</xref> for an example of a single ROI analysis using built-in tools in the Pyneal toolkit. For additional examples of rt-fMRI single-ROI neurofeedback studies, see <xref rid="B4" ref-type="bibr">deCharms et al. (2005)</xref>, <xref rid="B1" ref-type="bibr">Caria et al. (2010)</xref>, <xref rid="B26" ref-type="bibr">Sulzer et al. (2013b)</xref>, <xref rid="B8" ref-type="bibr">Greer et al. (2014)</xref></italic>, <italic><xref rid="B28" ref-type="bibr">Young et al. (2017)</xref></italic>.</p>
      </sec>
      <sec id="S3.SS2.SSS2">
        <title>Neurofeedback: Correlation Between Two ROIs Using a Custom Analysis Script</title>
        <p><italic>Example</italic>: Using a custom analysis script to calculate the correlation between two ROIs and use the correlation as feedback during a task. <italic>E.g., A researcher wishes to calculate the correlation between the primary motor cortex and the caudate nucleus and use that correlated signal as neurofeedback in a hand squeezing task.</italic></p>
        <p>This tutorial uses the Pyneal Scanner simulation script, which is located in:</p>
        <list list-type="simple">
          <list-item>
            <p>
              <monospace>∼/pyneal/utils/simulation/pyneal Scanner_sim.py</monospace>
            </p>
          </list-item>
        </list>
        <p>To perform this tutorial the following steps are required:</p>
        <list list-type="simple">
          <list-item>
            <label>I.</label>
            <p>Setup Scan Simulator</p>
          </list-item>
        </list>
        <p>Like in the example in “Neurofeedback: Single ROI Averaging Using Built-in Analysis Functions,” the first step is to set up Pyneal Scanner Simulator, which will send our sample dataset to Pyneal for analysis.</p>
        <list list-type="simple">
          <list-item>
            <p>Open a new terminal and navigate to the Simulation Tools directory: <monospace>cd ∼/pyneal/utils/simulation</monospace></p>
          </list-item>
        </list>
        <list list-type="simple">
          <list-item>
            <p>Run <monospace>pynealScanner_sim.py</monospace> and pass in the path to our sample dataset.</p>
          </list-item>
        </list>
        <list list-type="simple">
          <list-item>
            <p>Type:</p>
          </list-item>
          <list-item>
            <p>
              <monospace>python pynealScanner_sim.py -f ∼/ pyneal-tutorial/analysisTutorial/func. nii.gz -t 1000 -sh 127.0.0.1 -sp 5555</monospace>
            </p>
          </list-item>
        </list>
        <list list-type="simple">
          <list-item>
            <p>Hit enter. The user should see the simulator prepare the data and wait for a connection to Pyneal:</p>
          </list-item>
          <list-item>
            <p>
              <monospace>Prepping dataset:</monospace>
              <monospace>∼/pyneal-tutorial/</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>analysisTutorial/func.nii.gz</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>Dimensions: (64, 64, 18, 208)</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>TR: 1000</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>Connecting to Pyneal at 127.0.0.1:5555</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>waiting for connection…</monospace>
            </p>
          </list-item>
        </list>
        <list list-type="simple">
          <list-item>
            <label>II.</label>
            <p>Setup Custom Analysis Script</p>
          </list-item>
        </list>
        <p>This tutorial includes a custom analysis script that the user will load into pyneal. This script can be found at: <monospace>∼/pyneal-tutorial/analysisTutorial/custom Analysis_ROI_corr.py.</monospace> Open this file to follow along below. This script is adapted from the <monospace>customAnalysis Template.py</monospace> that is included in the Pyneal toolkit.</p>
        <p>There are two relevant sections to this script:</p>
        <p>
          <bold>initialize</bold>
        </p>
        <p>The analysis script includes an <monospace>__init__</monospace> method that runs once Pyneal is launched. This section should be used to load any required files and initialize any variables needed once the scan begins.</p>
        <p>In the <monospace>__init__</monospace> method in the tutorial script, the user will find the following code block:</p>
        <list list-type="simple">
          <list-item>
            <p>
              <monospace>## Load the mask files for the 2 ROIs we will compute the correlation between</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace># Note: we will be ignoring the mask that is passed in from the Pyneal GUI</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>mask1_path = join(self.customAnalysis Dir, ‘masks/L_Caudate.nii.gz’)</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>mask2_path = join(self.customAnalysis Dir, ‘masks/L_MotorCortex.nii.gz’)</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>mask1_img = nib.load(mask1_path)</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>mask2_img = nib.load(mask2_path)</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>self.masks = {</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>‘mask1’: {</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>‘mask’: mask1_img.get_data() &gt; 0,</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace># creat boolean mask</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>‘vals’: np.zeros(self.numTimepts)</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace># init array to store mean signal</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>on each timept</monospace>
            </p>
          </list-item>
          <list-item>
            <p>},</p>
          </list-item>
          <list-item>
            <p>
              <monospace>‘mask2’: {</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>‘mask’: mask2_img.get_data() &gt; 0,</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>‘vals’: np.zeros(self.numTimepts)</monospace>
            </p>
          </list-item>
          <list-item>
            <p>}</p>
          </list-item>
          <list-item>
            <p>}</p>
          </list-item>
          <list-item>
            <p>
              <monospace>## Correlation config</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>self.corr_window = 10 # number of timepts to calculate correlation over</monospace>
            </p>
          </list-item>
        </list>
        <p>The above block of code does the following:</p>
        <list list-type="simple">
          <list-item>
            <label>•</label>
            <p>Loads each mask file. Note that while the template provides a reference to the mask file loaded via the Pyneal GUI, we are ignoring that mask and instead loading each mask manually.</p>
          </list-item>
          <list-item>
            <label>•</label>
            <p>Pre-allocates an array for each mask where we will store the mean signal within that mask on each timepoint.</p>
          </list-item>
          <list-item>
            <label>•</label>
            <p>Sets the correlation window to 10 timepoints, meaning that, with each new volume that arrives, the correlation between the two ROIs will be computed over the previous 10 timepoints.</p>
          </list-item>
        </list>
        <p>
          <bold>compute</bold>
        </p>
        <p>The <monospace>compute</monospace> method will be executed on each incoming volume throughout the scan, and provides the image data (<monospace>vol</monospace>) and volume index (<monospace>volIdx</monospace>) as inputs. This method should be used to define analysis steps.</p>
        <p>In the compute method in the tutorial script, the user will find the following code block:</p>
        <list list-type="simple">
          <list-item>
            <p>
              <monospace>## Get the mean signal within each mask at this timept</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>for roi in self.masks:</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>mask = self.masks[roi][’mask’]</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>meanSignal = np.mean(vol[mask])</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>self.masks[roi][’vals’][volIdx] =</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>meanSignal</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>## Once enough timepts have accumulated, start calculating rolling correlation</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>if volIdx &gt; self.corr_window:</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace># get the timeseries from each ROI</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>over the correlation window</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>roi1_ts = self.masks[’mask1’][’vals’]</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>[volIdx-self.corr_window:volIdx]</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>roi2_ts = self.masks[’mask2’][’vals’]</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>[volIdx-self.corr_window:volIdx]</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace># compute correlation,</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>return r-value only</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>Corr = stats.pearsonr(roi1_ts,</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>roi2_ts)[0]</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>else:</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>corr = None</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>return {’corr’: corr }</monospace>
            </p>
          </list-item>
        </list>
        <p>The above block of code does the following:</p>
        <list list-type="simple">
          <list-item>
            <label>•</label>
            <p>Computes the mean signal within each mask at the current timepoint.</p>
          </list-item>
          <list-item>
            <label>•</label>
            <p>Once enough volumes have arrived, computes the correlation between the two ROIs over the specified correlation window.</p>
          </list-item>
          <list-item>
            <label>•</label>
            <p>Returns the result of the correlation as a dictionary.</p>
          </list-item>
        </list>
        <p>The results of any custom script need to be returned as a dictionary. The Pyneal will integrate these results into the existing pipeline and the results will be available via the Pyneal Results Server (for requests from an End User if desired) in the same manner as with the built-in analysis options.</p>
        <list list-type="simple">
          <list-item>
            <label>III.</label>
            <p>Set up Pyneal</p>
          </list-item>
        </list>
        <p>Next, configure Pyneal to use the custom analysis script developed above.</p>
        <p>In a new terminal, launch Pyneal:</p>
        <list list-type="simple">
          <list-item>
            <p>
              <monospace>cd ∼/pyneal</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>python pyneal.py</monospace>
            </p>
          </list-item>
        </list>
        <list list-type="simple">
          <list-item>
            <label>•</label>
            <p>Configure Pyneal:</p>
            <list list-type="simple">
              <list-item>
                <label>∘</label>
                <p>Communication: Set the <monospace>Pyneal Host IP</monospace> to <monospace>127.0.0.1, Pyneal-Scanner Port</monospace> to <monospace>5555,</monospace> and the <monospace>Results Server Port</monospace> to <monospace>5558</monospace>.</p>
              </list-item>
              <list-item>
                <label>∘</label>
                <p>Mask: In the <monospace>∼/pyneal-tutorial/analysis Tutorial/masks/</monospace> directory, select <monospace>L_Motor Cortext.nii.gz.</monospace> Note that although the custom analysis script overrides the mask supplied here, a valid mask file is required nonetheless.</p>
              </list-item>
              <list-item>
                <label>∘</label>
                <p>Preprocessing: Set # <monospace>of timepts</monospace> to: <monospace>208.</monospace> The user can keep the Estimate Motion? option selected if preferred.</p>
              </list-item>
              <list-item>
                <label>∘</label>
                <p>Analysis: Select the <monospace>Custom</monospace> option. The user will be presented with a file dialog. Select the custom analysis script at <monospace>∼/pyneal-</monospace></p>
              </list-item>
              <list-item>
                <p>
                  <monospace>tutorial/analysisTutorial/custom Analysis_ROI_corr.py.</monospace>
                </p>
              </list-item>
              <list-item>
                <label>∘</label>
                <p>Output: Set the output directory to <monospace>∼/pyneal-tutorial/analysisTutorial/output.</monospace> Check <monospace>Launch Dashboard?</monospace></p>
              </list-item>
            </list>
          </list-item>
          <list-item>
            <label>•</label>
            <p>Hit <monospace>Submit</monospace> to start Pyneal.</p>
          </list-item>
        </list>
        <list list-type="simple">
          <list-item>
            <label>IV.</label>
            <p>Start the scan</p>
          </list-item>
        </list>
        <p>Back in the Scan Simulator terminal, the user should see a successful connection to Pyneal</p>
        <list list-type="simple">
          <list-item>
            <p>
              <monospace>connected to pyneal</monospace>
            </p>
          </list-item>
          <list-item>
            <p>
              <monospace>Press ENTER to begin the “scan”</monospace>
            </p>
          </list-item>
        </list>
        <list list-type="simple">
          <list-item>
            <label>•</label>
            <p>Hit <monospace>Enter</monospace> to begin the simulated scan</p>
          </list-item>
        </list>
        <p>As the scan is progressing, the user should see information about each volume appear in both the Scan Simulator and the Pyneal terminals, indicating the volumes are being successfully transmitted and processed.</p>
        <list list-type="simple">
          <list-item>
            <label>V.</label>
            <p>Results</p>
          </list-item>
        </list>
        <list list-type="simple">
          <list-item>
            <label>•</label>
            <p>At the completion of the scan, the user can find the following Pyneal output files in <monospace>∼/pyneal- tutorial/analysisTutorial/output/pyneal_ 002</monospace> (Note: the directory names increase in sequence. If the user completed the single ROI NF tutorial first, it’ll be <monospace>_002,</monospace> otherwise it’ll be a different number):</p>
            <list list-type="simple">
              <list-item>
                <label>∘</label>
                <p><monospace>pynealLog.log:</monospace> complete log file from the scan.</p>
              </list-item>
              <list-item>
                <label>∘</label>
                <p><monospace>receivedFunc.nii.gz:</monospace> 4D Nifti of the data, as received by the Pyneal.</p>
              </list-item>
              <list-item>
                <label>∘</label>
                <p><monospace>results.json:</monospace> JSON-formatted file containing the computed analysis results at each timepoint.</p>
              </list-item>
            </list>
          </list-item>
          <list-item>
            <label>•</label>
            <p>The custom analysis script computed a sliding window correlation between the Left Motor Cortex and the Left Caudate throughout the task.</p>
            <list list-type="simple">
              <list-item>
                <label>∘</label>
                <p>To visualize these results, the user can open the <monospace>results.json</monospace> file and plot the results at each timepoint using their preferred tools (e.g., Python, Matlab).</p>
              </list-item>
            </list>
          </list-item>
        </list>
      </sec>
    </sec>
  </sec>
  <sec id="S4">
    <title>Discussion</title>
    <sec id="S4.SS1">
      <title>Advantages of the Pyneal Toolkit rt-fMRI Software</title>
      <p>A variety of tools currently exist that support real-time fMRI to varying degrees, including AFNI (<xref rid="B3" ref-type="bibr">Cox and Jesmanowicz, 1995</xref>), FIRE (<xref rid="B5" ref-type="bibr">Gembris et al., 2000</xref>), scanSTAT (<xref rid="B2" ref-type="bibr">Cohen, 2001</xref>), STAR (<xref rid="B17" ref-type="bibr">Magland et al., 2011</xref>), FieldTrip toolbox extension (<xref rid="B19" ref-type="bibr">Oostenveld et al., 2011</xref>), Turbo-BrainVoyager (<xref rid="B7" ref-type="bibr">Goebel, 2012</xref>), FRIEND (<xref rid="B22" ref-type="bibr">Sato et al., 2013</xref>), BART (<xref rid="B9" ref-type="bibr">Hellrung et al., 2015</xref>), OpenNFT (<xref rid="B13" ref-type="bibr">Koush et al., 2017</xref>), and Neu3CA-RT (<xref rid="B10" ref-type="bibr">Heunis et al., 2018</xref>). At a time when implementing real-time fMRI meant researchers had to develop custom in-house software solutions, these tools presented a valuable alternative, catalyzing new experiments, and supporting pioneering early research with real-time fMRI. Nevertheless, the existing software options are limited in one or more ways that fundamentally restricts who can use them and where, and what types of experiments they support. Please see <xref rid="T1" ref-type="table">Table 1</xref> for a comparison of the Pyneal toolkit to the other main rt-fMRI software packages currently available. For example, some of these tools require users to purchase licensing agreements for the package itself (e.g., Turbo-BrainVoyager), or are designed to work inside of commercial software packages like Matlab<sup><xref ref-type="fn" rid="footnote7">7</xref></sup>. In addition, a number of these tools are designed to only support a particular usage of real-time fMRI, like neurofeedback, while not supporting other uses of rt-fMRI. And lastly, even in cases where the underlying code <italic>is</italic> customizable, it often requires proficiency with advanced computer languages like C++. We built the Pyneal toolkit to directly address these limitations.</p>
      <table-wrap id="T1" position="float">
        <label>TABLE 1</label>
        <caption>
          <p>Comparison of common, currently available real-time fMRI software packages.</p>
        </caption>
        <table frame="hsides" rules="groups" cellspacing="5" cellpadding="5">
          <thead>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Software</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Commercial license required?</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Cost</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Source code available?</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Base software language</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Network requirements</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Main types of built in analyses</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Limitations</td>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Pyneal</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Free</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Python</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TCP/IP socket communication</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Out of the box ROI neurofeedback analyses; <italic>allows fully custom analysis scripts written by users in Python</italic></td>
              <td valign="top" align="left" rowspan="1" colspan="1">Not compatible with multiband data yet; no online data preprocessing yet; has not been tested on Windows environment (no known incompatibilities)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>AFNI plug_realtime</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">No (GNU GPL v2.0 license)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Free</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">C</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TCP/IP socket communication</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Data quality assessment (e.g., motion) and neurofeedback</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Limited documentation online; limited built-in functions</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Turbo-Brain Voyager</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Price varies across countries; see <ext-link ext-link-type="uri" xlink:href="https://www.brainvoyager.com/products/purchase.html">https://www.brainvoyager.com/products/purchase.html</ext-link> for current pricing</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Not for main software; may be available for plugins and remote extensions</td>
              <td valign="top" align="left" rowspan="1" colspan="1">C++</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Depends on the MRI manufacturer. Siemens: TCP/IP connection to the imager and default dicom export to the TBV analysis computer. GE/Philips: supports reading the exported files</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Incremental: GLM, ERA, motion correction, spatial smoothing, and drift removal; multi-voxel pattern classification; ICA; ROI NF</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Commercial license required. Requires sophisticated software knowledge: custom analyses are allowed via user development of plugins; must be in C++. (Note – TBV provides a network interface build on top of the plugin interface. The network interface is available for all programming languages.)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>FRIEND</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Free</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">C++</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TCP/IP socket communication for communicating with End Users via Friend Engine</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Image preprocessing, NF, ROI analysis (PSC), and multivoxel pattern decoding</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Requires sophisticated software knowledge: custom plugins must be written in C++. Frontend is fully customizable, but users must be comfortable writing socket protocols.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>OpenNFT</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">No (GNU GPL v3.0 license)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Free</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Python and Matlab</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TCP/IP socket communication</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Built-in key volume and time series preprocessing and processing procedures (e.g., spatial smoothing, high pass filtering, despiking); rt QA; incremental and cummulative GLM; the following types of NF: continuous and intermittent activation-based NF; intermittent effective connectivity NF, continuous classification-based NF.</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Requires Matlab license (commercial); tested with Siemens and Philips, but should be compatible with other scanner types. Has not been tested on GE to our knowledge.</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <attrib>
            <italic>ROI, region of analysis; GLM, general linear model; ERA, event related averaging; ICA, independent component analysis; NF, neurofeedback; IP, internet protocol; TCP, transmission control protocol. Note, in this table we exclude packages that are exclusively quality control (for example see rtQC: <ext-link ext-link-type="uri" xlink:href="https://github.com/rtQC-group/rtQC">https://github.com/rtQC-group/rtQC</ext-link>).</italic>
          </attrib>
        </table-wrap-foot>
      </table-wrap>
      <sec id="S4.SS1.SSS1">
        <title>Free and Open-Source</title>
        <p>The Pyneal toolkit offers a number of key features that make it an appealing package for existing real-time fMRI practitioners as well as those new to the field. First, in support of the growing movement toward open-science, the Pyneal toolkit is free and open source. It is written entirely in Python (see text footnote 2), and all required dependencies are similarly cost-free and open. We chose to use Python specifically because it is sufficiently powerful to handle the computational demands of fMRI analysis in real-time and the language is comparatively easy for users to read and write, an important consideration when designing a package that encourages customization by researchers. Furthermore, the number of libraries designed to aid scientific computing (e.g., Numpy, Scipy, Scikit-learn), and the large user support community worldwide, have lead Python to surge in popularity among the sciences (see <xref rid="B20" ref-type="bibr">Perez et al., 2011</xref>), and neuroscience in particular (see <xref rid="B6" ref-type="bibr">Gleeson et al., 2017</xref> and <xref rid="B18" ref-type="bibr">Muller et al., 2015</xref>). The Pyneal toolkit follows style and documentation guidelines of scientific python libraries, and when possible uses the same data formats and image orientation conventions as popular neuroimaging libraries (e.g., NiBabel). Moreover, the source code for the Pyneal toolkit is hosted via a GitHub repository, which ensures users can access the most up-to-date code releases, as well as track modifications and revisions to the codebase across time (<xref rid="B21" ref-type="bibr">Perkel, 2016</xref>).</p>
      </sec>
      <sec id="S4.SS1.SSS2">
        <title>Flexibility in Handling Multiple Data Formats and Local Computing Configurations</title>
        <p>A second advantage the Pyneal toolkit offers is flexibility in handling multiple different data formats and directory structures. MRI data can be represented via a number of different file formats, depending in part on the particular scanner manufacturer and/or automated processing pipelines that modify data before it gets written to disk. For instance, the scanner may store images using a universal medical imaging standard like DICOM, a more specific neuroimaging standard like Nifti, or a proprietary format like the PAR/REC file convention currently seen with Philips scanners. Moreover, even within a given file format, there is considerable variation in <italic>how</italic> data are represented. For instance, a single DICOM image file may represent a 2D slice (GE scanners) or a 3D volume arranged as a 2D mosaic grid (Siemens scanners). Lastly, even when two imaging centers have the same scanners and use the same data formats, there can be differences in how the local computing networks are configured. This affects where data is saved, and how the Pyneal toolkit can access existing pipelines. The Pyneal toolkit was designed to be robust to these differences across scanning environments.</p>
        <p>Relatedly, a third advantage is the ability of the Pyneal toolkit to accommodate multiple different environmental variations. Importantly, the Pyneal toolkit splits data handling from real-time analysis tasks into modular components that run via independent processes. Pyneal Scanner is responsible for reading incoming MRI data in whatever form it takes, accessing the raw data, and reformatting to a standardized form that is compatible with subsequent analysis stages of Pyneal. The re-formatted data is then passed to the preprocessing and analysis stage of Pyneal via TCP/IP based interprocess communications. The modular nature of this configuration offers important advantages. For one, Pyneal Scanner and Pyneal are able (though not required) to run on separate workstations. This is important as researchers may lack the administrative permissions needed to significantly modify the computing environment of a shared scanning suite. For example, in a situation where the scanner console does not export images to a shared network directory, Pyneal Scanner can run on the scanner console and pass data to a remote workstation running Pyneal, minimizing the risk of interfering with normal scanner operations. In other situations where the scanner <italic>does</italic> export images to a shared network directory, Pyneal Scanner and Pyneal can run on the same workstation.</p>
        <p>The modular nature of the Pyneal toolkit’s design means that it can be modified to support new data formats in the future without having to drastically alter the core codebase. Importantly, if the Pyneal toolkit does not currently support a desired data format, researchers can modify Pyneal Scanner to accommodate their needs without having to modify the rest of the Pyneal toolkit core utilities. As the entire toolkit is free and open-source, users and welcome and encouraged to do so.</p>
      </sec>
      <sec id="S4.SS1.SSS3">
        <title>Fully Customizable Analyses</title>
        <p>A fourth, and chief, advantage that the Pyneal toolkit offers is flexibility of analyses. The ability to design and implement uniquely tailored analysis routines via custom analysis scripts means that users can adapt the method to their research question rather than having to constrain their research questions based on the methodology. This flexibility means that the Pyneal toolkit can be used to accommodate a broader and more diverse spectrum of research and experimental goals, offering numerous benefits to the real-time neuroimaging community and general scientific advancement. Importantly, in the Pyneal toolkit, the entire incoming data stream is made available, and by using custom analysis scripts, researchers can extract, manipulate and interrogate whichever portions of that data are most relevant to their question. In addition, researchers are able to use these results in real-time for whatever purpose they choose, including neurofeedback, experimental control, quality-assurance monitoring, etc.</p>
        <p>The ability to design and test one’s own analyses will expedite the growth and maturation of real-time neuroimaging more broadly. It is worth highlighting that real-time fMRI is still a comparatively new approach, with many open questions regarding imaging parameters, experimental design, effect sizes, subject populations, long-term outcomes, and general best practices (<xref rid="B25" ref-type="bibr">Sulzer et al., 2013a</xref>). Determining satisfactory answers to these questions has been slow, in part due to the limitations of existing software and a small community of users. Customizing analyses in the Pyneal toolkit allows researchers to work in a rapid and iterative way to explore new methods, addressing these questions, and establishing a framework for future studies. It also means that researchers can keep up with the latest analytic advances in their domain without having to rely on external software developers to release new updates for their real-time tools.</p>
        <p>In short, the Pyneal toolkit is powerful precisely because it does not presuppose how researchers intend to use it; our conviction is that advances in real-time neuroimaging are best achieved by empowering the community to develop those advances itself.</p>
      </sec>
    </sec>
    <sec id="S4.SS2">
      <title>Limitations</title>
      <p>While the Pyneal toolkit offers a convenient and flexible infrastructure for accessing and using fMRI data in real-time, there are a few limitations with the software presently. First, the Pyneal toolkit does not currently include built-in online denoising of the raw fMRI data. Depending on the application, a user may find that simple denoising steps prior to analysis, such as slow-wave drift removal or head motion correction, may increase the signal-to-noise ratio and improve the statistical power of the analysis. We plan to include built-in options for basic denoising in forthcoming software releases. In the meantime, the current version of the Pyneal toolkit allows users to implement their own denoising steps as part of a customized processing pipeline via a custom analysis script.</p>
      <p>Second, the Pyneal toolkit offers built-in support for standard data formats found across the three main scanner manufacturers, but does not currently support multiband acquisitions. As imaging technology advances, multiband acquisitions are becoming increasingly common as a way to increase coverage while maintaining short TRs. As such, we plan to offer built-in multiband support in an upcoming software update. Due to the modular nature of the Pyneal toolkit, multiband support can be integrated as a component of Pyneal Scanner without requiring significant changes to the bulk of the code base.</p>
      <p>Third, the Pyneal toolkit was built and tested using Python 3 on Linux and macOS environments. While there are no obvious incompatibilities with a Windows environment, we have not had the resources to thoroughly test the Pyneal toolkit across multiple platforms. We encourage Windows users to run the Pyneal toolkit via a virtual machine configured as a Linux operating system. In future versions of the Pyneal toolkit we hope to offer broader support across platforms, or containerize the application using a tool like Docker<sup><xref ref-type="fn" rid="footnote7">8</xref></sup> in order to be platform agnostic.</p>
      <p>While our team is working to improve the aforementioned limitations, we would also like to extend an invitation to the neuroimaging community to contribute directly to the Pyneal toolkit. The Pyneal toolkit was developed with the open source ethos of sharing and collaboration. It lives in the GitHub ecosystem, which facilitates collaborative work across multiple teams and/or individuals, and offers an easy way for users to submit new features, discuss code modifications in detail, and log bugs as they are discovered. Working collaboratively in this manner ensures efficiency in expanding the software’s capabilities and improving stability. Anyone interested in working on the Pyneal toolkit can find information in the <italic>Contributor Guidelines</italic> and <italic>Contributor Code of Conduct</italic> outlined in the documentation at the Pyneal toolkit GitHub repository at: <ext-link ext-link-type="uri" xlink:href="https://github.com/jeffmacinnes/pyneal">https://github.com/jeffmacinnes/pyneal</ext-link>.</p>
    </sec>
  </sec>
  <sec id="S5">
    <title>Conclusion</title>
    <p>In this article we describe the Pyneal toolkit, a free and open-source software platform for rt-fMRI. The Pyneal toolkit provides seamless access to incoming MRI data across a variety of formats, a flexible basis to carry out preprocessing and analysis in real-time, a mechanism to communicate results in real-time with remote devices, and interactive tools to monitor the quality and status of an on-going real-time fMRI experimental session. In addition to a number of basic built-in analysis options, the Pyneal toolkit offers users the flexibility to design and implement fully customized processing pipelines, allowing real-time fMRI analyses to be tailored to the experimental question instead of the other way around [for two examples using the Pyneal toolkit with different experimental approaches see (<xref rid="B16" ref-type="bibr">MacInnes et al., 2016</xref>) and (<xref rid="B14" ref-type="bibr">MacDuffie et al., 2018</xref>)]. As the rt-fMRI community grows worldwide, new tools are needed that allow researchers to flexibly adapt to suit their unique needs, be that neurofeedback from a single or multiple regions, triggering task flow, or online multivariate classification. The Pyneal toolkit offers researchers a powerful way to address the current open questions in the field, and the flexibility necessary to adapt to answer future questions.</p>
  </sec>
  <sec sec-type="data-availability" id="S6">
    <title>Data Availability Statement</title>
    <p>Pyneal is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jeffmacinnes/pyneal">https://github.com/jeffmacinnes/pyneal</ext-link> and full documentation is online at <ext-link ext-link-type="uri" xlink:href="https://jeffmacinnes.github.io/pyneal-docs/">https://jeffmacinnes.github.io/pyneal-docs/</ext-link>.</p>
  </sec>
  <sec id="S7">
    <title>Author Contributions</title>
    <p>JM designed and developed the software and documentation, and co-wrote the manuscript. KD consulted on software design, provided material support, and co-wrote the manuscript. RA and AS consulted on software design and implementation, provided material support, and revised the manuscript. RR and CP provided material support and revised the manuscript. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> Funding support came from W.M. Keck Foundation (AS, CP, and RR), NCATS 1KL2TR002554 (KD), NIMH R01 MH094743 (RA), NSF EEC-1028725 (RR), as well as the Alfred P. Sloan Foundation (RA), Klingenstein Fellowship Award in the Neurosciences (RA), and the Dana Foundation Brain and Immuno-Imaging Program (RA). The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH or the NSF.</p>
    </fn>
  </fn-group>
  <ack>
    <p>The authors would like to thank Ian Ballard for helpful comments on the manuscript.</p>
  </ack>
  <fn-group>
    <fn id="footnote1">
      <label>1</label>
      <p>Throughout this article we use the term <italic>scan</italic> or <italic>run</italic> to refer to a single, discrete 4D acquisition, and the terms <italic>experimental session</italic> to refer to a collection of scans that are administered to a particular participant in a continuous time window.</p>
    </fn>
    <fn id="footnote2">
      <label>2</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="https://docs.python.org/3.6">https://docs.python.org/3.6</ext-link>
      </p>
    </fn>
    <fn id="footnote3">
      <label>3</label>
      <p>Throughout this article we will use the phrase “Pyneal toolkit” when referring to the overall toolkit and “Pyneal” when referencing the specific software component comprising Pyneal (as opposed to Pyneal Scanner for example).</p>
    </fn>
    <fn id="footnote4">
      <label>4</label>
      <p>The Pyneal toolkit documentation, and the figures shown throughout this section, use the terms “Scanner Computer” and “Analysis Computer” to refer to the machines that are running Pyneal Scanner and Pyneal, respectively. However, it is important to note that these terms refer to functional roles: there is no conflict in having the Pyneal Scanner and Pyneal running on the same physical machine.</p>
    </fn>
    <fn id="footnote5">
      <label>5</label>
      <p>ZeroMQ, a powerful open source library for messaging, is available at <ext-link ext-link-type="uri" xlink:href="https://zeromq.org/">https://zeromq.org/</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/zeromq">https://github.com/zeromq</ext-link>.</p>
    </fn>
    <fn id="footnote6">
      <label>6</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/jeffmacinnes/pyneal/blob/master/utils/customAnalyses/customAnalysisTemplate.py">https://github.com/jeffmacinnes/pyneal/blob/master/utils/customAnalyses/customAnalysisTemplate.py</ext-link>
      </p>
    </fn>
    <fn id="footnote7">
      <label>7</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/help/matlab/">https://www.mathworks.com/help/matlab/</ext-link>
      </p>
    </fn>
    <fn id="footnote8">
      <label>8</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="https://www.docker.com/">https://www.docker.com/</ext-link>
      </p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caria</surname><given-names>A.</given-names></name><name><surname>Ranganatha</surname><given-names>S.</given-names></name><name><surname>Ralf</surname><given-names>V.</given-names></name><name><surname>Chiara</surname><given-names>B.</given-names></name><name><surname>Niels</surname><given-names>B.</given-names></name></person-group> (<year>2010</year>). <article-title>Volitional control of anterior insula activity modulates the response to aversive stimuli. a real-time functional magnetic resonance imaging study.</article-title>
<source><italic>Biol. Psychiatry</italic></source>
<volume>68</volume>
<fpage>425</fpage>–<lpage>432</lpage>. <pub-id pub-id-type="doi">10.1016/j.biopsych.2010.04.020</pub-id>
<?supplied-pmid 20570245?><pub-id pub-id-type="pmid">20570245</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>M. S.</given-names></name></person-group> (<year>2001</year>). <article-title>Real-time functional magnetic resonance imaging.</article-title>
<source><italic>Methods</italic></source>
<volume>25</volume>
<fpage>201</fpage>–<lpage>220</lpage>. <pub-id pub-id-type="doi">10.1006/meth.2001.1235</pub-id>
<?supplied-pmid 11812206?><pub-id pub-id-type="pmid">11812206</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>R. W.</given-names></name><name><surname>Jesmanowicz</surname><given-names>A.</given-names></name></person-group> (<year>1995</year>). <article-title>Real-time functional magnetic resonance imaging.</article-title>
<source><italic>Magn. Reson. Insights</italic></source>
<volume>33</volume>
<fpage>230</fpage>–<lpage>236</lpage>. <pub-id pub-id-type="doi">10.1002/mrm.1910330213</pub-id>
<?supplied-pmid 7707914?><pub-id pub-id-type="pmid">7707914</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>deCharms</surname><given-names>R. C.</given-names></name><name><surname>Fumiko</surname><given-names>M.</given-names></name><name><surname>Gary</surname><given-names>H. G.</given-names></name><name><surname>David</surname><given-names>L.</given-names></name><name><surname>John</surname><given-names>M. P.</given-names></name><name><surname>Deepak</surname><given-names>S.</given-names></name><etal/></person-group> (<year>2005</year>). <article-title>Control over brain activation and pain learned by using real-time functional MRI.</article-title>
<source><italic>Proc. Natl. Acad. Sci. U.S.A.</italic></source>
<volume>102</volume>
<fpage>18626</fpage>–<lpage>18631</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0505210102</pub-id>
<?supplied-pmid 16352728?><pub-id pub-id-type="pmid">16352728</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gembris</surname><given-names>D.</given-names></name><name><surname>John</surname><given-names>G. T.</given-names></name><name><surname>Stefan</surname><given-names>S.</given-names></name><name><surname>Wolfgang</surname><given-names>F.</given-names></name><name><surname>Dieter</surname><given-names>S.</given-names></name><name><surname>Stefan</surname><given-names>P.</given-names></name></person-group> (<year>2000</year>). <article-title>Functional magnetic resonance imaging in real time (FIRE): sliding-window correlation analysis and reference-vector optimization.</article-title>
<source><italic>Magn. Reson. Med.</italic></source>
<volume>43</volume>
<fpage>259</fpage>–<lpage>268</lpage>. <pub-id pub-id-type="doi">10.1002/(sici)1522-2594(200002)43:2&lt;259::aid-mrm13&gt;3.0.co;2-p</pub-id><pub-id pub-id-type="pmid">10680690</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gleeson</surname><given-names>P.</given-names></name><name><surname>Andrew</surname><given-names>P. D.</given-names></name><name><surname>Silver</surname><given-names>R. A.</given-names></name><name><surname>Giorgio</surname><given-names>A. A.</given-names></name></person-group> (<year>2017</year>). <article-title>A commitment to open source in neuroscience.</article-title>
<source><italic>Neuron</italic></source>
<volume>96</volume>
<fpage>964</fpage>–<lpage>965</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2017.10.013</pub-id>
<?supplied-pmid 29216458?><pub-id pub-id-type="pmid">29216458</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goebel</surname><given-names>R.</given-names></name></person-group> (<year>2012</year>). <article-title>BrainVoyager–past, present, future.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>62</volume>
<fpage>748</fpage>–<lpage>756</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.083</pub-id>
<?supplied-pmid 22289803?><pub-id pub-id-type="pmid">22289803</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greer</surname><given-names>S. M.</given-names></name><name><surname>Andrew</surname><given-names>J. T.</given-names></name><name><surname>Gary</surname><given-names>H. G.</given-names></name><name><surname>Brian</surname><given-names>K.</given-names></name></person-group> (<year>2014</year>). <article-title>Control of nucleus accumbens activity with neurofeedback.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>96</volume>
<fpage>237</fpage>–<lpage>244</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.03.073</pub-id>
<?supplied-pmid 24705203?><pub-id pub-id-type="pmid">24705203</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hellrung</surname><given-names>L.</given-names></name><name><surname>Maurice</surname><given-names>H.</given-names></name><name><surname>Oliver</surname><given-names>Z.</given-names></name><name><surname>Torsten</surname><given-names>S.</given-names></name><name><surname>Christian</surname><given-names>K.</given-names></name><name><surname>Elisabeth</surname><given-names>R.</given-names></name></person-group> (<year>2015</year>). <article-title>Flexible adaptive paradigms for fMRI using a novel software package ‘Brain Analysis in Real-Time’ (BART).</article-title>
<source><italic>PLoS One</italic></source>
<volume>10</volume>:<issue>e0118890</issue>. <pub-id pub-id-type="doi">10.1371/journal.pone.0118890</pub-id>
<?supplied-pmid 25837719?><pub-id pub-id-type="pmid">25837719</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heunis</surname><given-names>S.</given-names></name><name><surname>René</surname><given-names>B.</given-names></name><name><surname>Rolf</surname><given-names>L.</given-names></name><name><surname>Anton</surname><given-names>D. L.</given-names></name><name><surname>Marcel</surname><given-names>B.</given-names></name><name><surname>Bert</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Neu3CA-RT: a framework for real-time fMRI analysis.</article-title>
<source><italic>Psychiatry Res.</italic></source>
<volume>282</volume>
<fpage>90</fpage>–<lpage>102</lpage>. <pub-id pub-id-type="doi">10.1016/j.pscychresns.2018.09.008</pub-id>
<?supplied-pmid 30293911?><pub-id pub-id-type="pmid">30293911</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M.</given-names></name></person-group> (<year>2000</year>). <source><italic>Measuring Transformation Error by RMS Deviation. TR99MJ1.</italic></source>
<publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M.</given-names></name><name><surname>Christian</surname><given-names>F. B.</given-names></name><name><surname>Timothy</surname><given-names>E. J. B.</given-names></name><name><surname>Mark</surname><given-names>W. W.</given-names></name><name><surname>Stephen</surname><given-names>M. S.</given-names></name></person-group> (<year>2012</year>). <article-title>FSL.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>62</volume>
<fpage>782</fpage>–<lpage>790</lpage>.<pub-id pub-id-type="pmid">21979382</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koush</surname><given-names>Y.</given-names></name><name><surname>John</surname><given-names>A.</given-names></name><name><surname>Evgeny</surname><given-names>P.</given-names></name><name><surname>Ronald</surname><given-names>S.</given-names></name><name><surname>Peter</surname><given-names>Z.</given-names></name><name><surname>Sergei</surname><given-names>B.</given-names></name></person-group> (<year>2017</year>). <article-title>OpenNFT: an open-source python/matlab framework for real-time fMRI neurofeedback training based on activity, connectivity and multivariate pattern analysis.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>156</volume>
<fpage>489</fpage>–<lpage>503</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.039</pub-id>
<?supplied-pmid 28645842?><pub-id pub-id-type="pmid">28645842</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacDuffie</surname><given-names>K. E.</given-names></name><name><surname>Jeff</surname><given-names>M.</given-names></name><name><surname>Kathryn</surname><given-names>C. D.</given-names></name><name><surname>Kari</surname><given-names>M. E.</given-names></name><name><surname>Timothy</surname><given-names>J. S.</given-names></name><name><surname>Alison</surname><given-names>R. A.</given-names></name></person-group> (<year>2018</year>). <article-title>Single session real-time fMRI neurofeedback has a lasting impact on cognitive behavioral therapy strategies.</article-title>
<source><italic>NeuroImage Clin.</italic></source>
<volume>19</volume>
<fpage>868</fpage>–<lpage>875</lpage>. <pub-id pub-id-type="doi">10.1016/j.nicl.2018.06.009</pub-id>
<?supplied-pmid 29922575?><pub-id pub-id-type="pmid">29922575</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>MacInnes</surname><given-names>J. J.</given-names></name><name><surname>Dickerson</surname><given-names>K. C.</given-names></name></person-group> (<year>2018</year>). <source><italic>Real-Time Functional Magnetic Resonance Imaging. eLS.</italic></source>
<publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley &amp; Sons, Ltd</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacInnes</surname><given-names>J. J.</given-names></name><name><surname>Dickerson</surname><given-names>K. C.</given-names></name><name><surname>Nan-Kuei</surname><given-names>C.</given-names></name><name><surname>Adcock</surname><given-names>R. A.</given-names></name></person-group> (<year>2016</year>). <article-title>Cognitive neurostimulation: learning to volitionally sustain ventral tegmental area activation.</article-title>
<source><italic>Neuron</italic></source>
<volume>89</volume>
<fpage>1331</fpage>–<lpage>1342</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.002</pub-id>
<?supplied-pmid 26948894?><pub-id pub-id-type="pmid">26948894</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magland</surname><given-names>J. F.</given-names></name><name><surname>Christopher</surname><given-names>W. T.</given-names></name><name><surname>Anna</surname><given-names>R. C.</given-names></name></person-group> (<year>2011</year>). <article-title>Spatio-temporal activity in real time (STAR): optimization of regional fMRI feedback.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>55</volume>
<fpage>1044</fpage>–<lpage>1053</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.12.085</pub-id>
<?supplied-pmid 21232612?><pub-id pub-id-type="pmid">21232612</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname><given-names>E.</given-names></name><name><surname>James</surname><given-names>A. B.</given-names></name><name><surname>Markus</surname><given-names>D.</given-names></name><name><surname>Marc-Oliver</surname><given-names>G.</given-names></name><name><surname>Michael</surname><given-names>H.</given-names></name><name><surname>Davison</surname><given-names>A. P.</given-names></name></person-group> (<year>2015</year>). <article-title>Python in neuroscience.</article-title>
<source><italic>Front, Neuroinform.</italic></source>
<volume>9</volume>:<issue>11</issue>. <pub-id pub-id-type="doi">10.3389/fninf.2015.00011</pub-id>
<?supplied-pmid 25926788?><pub-id pub-id-type="pmid">25926788</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R.</given-names></name><name><surname>Pascal</surname><given-names>F.</given-names></name><name><surname>Eric</surname><given-names>M.</given-names></name><name><surname>Jan-Mathijs</surname><given-names>S.</given-names></name></person-group> (<year>2011</year>). <article-title>FieldTrip: open source software for advanced analysis of meg, eeg, and invasive electrophysiological data.</article-title>
<source><italic>Comput. Intell. Neurosci.</italic></source>
<volume>2011</volume>:<issue>156869</issue>.</mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perez</surname><given-names>F.</given-names></name><name><surname>Granger</surname><given-names>B. E.</given-names></name><name><surname>Hunter</surname><given-names>J. D.</given-names></name></person-group> (<year>2011</year>). <article-title>Python: an ecosystem for scientific computing.</article-title>
<source><italic>Comput. Sci. Eng.</italic></source>
<volume>13</volume>
<fpage>13</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1109/mcse.2010.119</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perkel</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>Democratic databases: science on GitHub.</article-title>
<source><italic>Nature</italic></source>
<volume>538</volume>
<fpage>127</fpage>–<lpage>128</lpage>. <pub-id pub-id-type="doi">10.1038/538127a</pub-id>
<?supplied-pmid 27708327?><pub-id pub-id-type="pmid">27708327</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>J. R.</given-names></name><name><surname>Rodrigo</surname><given-names>B.</given-names></name><name><surname>Fernando</surname><given-names>F. P.</given-names></name><name><surname>Griselda</surname><given-names>J. G.</given-names></name><name><surname>Ivanei</surname><given-names>E. B.</given-names></name><name><surname>Patricia</surname><given-names>B.</given-names></name></person-group> (<year>2013</year>). <article-title>Real-Time fMRI pattern decoding and neurofeedback using FRIEND: an FSL-Integrated BCI toolbox.</article-title>
<source><italic>PLoS One</italic></source>
<volume>8</volume>:<issue>e81658</issue>. <pub-id pub-id-type="doi">10.1371/journal.pone.0081658</pub-id>
<?supplied-pmid 24312569?><pub-id pub-id-type="pmid">24312569</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sitaram</surname><given-names>R.</given-names></name><name><surname>Ros</surname><given-names>T.</given-names></name><name><surname>Stoeckel</surname><given-names>L.</given-names></name><name><surname>Haller</surname><given-names>S.</given-names></name><name><surname>Scharnowski</surname><given-names>F.</given-names></name><name><surname>Lewis-Peacock</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Closed-loop brain training: the science of neurofeedback.</article-title>
<source><italic>Nat. Rev. Neurosci.</italic></source>
<volume>18</volume>
<fpage>86</fpage>–<lpage>100</lpage>. <pub-id pub-id-type="doi">10.1038/nrn.2016.164</pub-id>
<?supplied-pmid 28003656?><pub-id pub-id-type="pmid">28003656</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoeckel</surname><given-names>L. E.</given-names></name><name><surname>Garrison</surname><given-names>K. A.</given-names></name><name><surname>Ghosh</surname><given-names>S.</given-names></name><name><surname>Wighton</surname><given-names>P.</given-names></name><name><surname>Hanlon</surname><given-names>C. A.</given-names></name><name><surname>Gilman</surname><given-names>J. M.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>Optimizing real time fMRI neurofeedback for therapeutic discovery and development.</article-title>
<source><italic>NeuroImage Clin.</italic></source>
<volume>5</volume>
<fpage>245</fpage>–<lpage>255</lpage>. <pub-id pub-id-type="doi">10.1016/j.nicl.2014.07.002</pub-id>
<?supplied-pmid 25161891?><pub-id pub-id-type="pmid">25161891</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sulzer</surname><given-names>J.</given-names></name><name><surname>Haller</surname><given-names>S.</given-names></name><name><surname>Scharnowski</surname><given-names>F.</given-names></name><name><surname>Weiskopf</surname><given-names>N.</given-names></name><name><surname>Birbaumer</surname><given-names>N.</given-names></name><name><surname>Blefari</surname><given-names>M. L.</given-names></name><etal/></person-group> (<year>2013a</year>). <article-title>Real-Time fMRI neurofeedback: progress and challenges.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>76</volume>
<fpage>386</fpage>–<lpage>399</lpage>.<pub-id pub-id-type="pmid">23541800</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sulzer</surname><given-names>J.</given-names></name><name><surname>Ranganatha</surname><given-names>S.</given-names></name><name><surname>Maria</surname><given-names>L. B.</given-names></name><name><surname>Spyros</surname><given-names>K.</given-names></name><name><surname>Niels</surname><given-names>B.</given-names></name><name><surname>Klaas</surname><given-names>E. S.</given-names></name></person-group> (<year>2013b</year>). <article-title>Neurofeedback-mediated self-regulation of the dopaminergic midbrain.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>83</volume>
<fpage>817</fpage>–<lpage>825</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.115</pub-id>
<?supplied-pmid 23791838?><pub-id pub-id-type="pmid">23791838</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voyvodic</surname><given-names>J. T.</given-names></name></person-group> (<year>1999</year>). <article-title>Real-Time fMRI paradigm control, physiology, and behavior combined with near real-time statistical analysis.</article-title>
<source><italic>NeuroImage</italic></source>
<volume>10</volume>
<fpage>91</fpage>–<lpage>106</lpage>. <pub-id pub-id-type="doi">10.1006/nimg.1999.0457</pub-id>
<?supplied-pmid 10417244?><pub-id pub-id-type="pmid">10417244</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Young</surname><given-names>K. D.</given-names></name><name><surname>Greg</surname><given-names>J. S.</given-names></name><name><surname>Vadim</surname><given-names>Z.</given-names></name><name><surname>Raquel</surname><given-names>P.</given-names></name><name><surname>Masaya</surname><given-names>M.</given-names></name><name><surname>Han</surname><given-names>Y.</given-names></name></person-group> (<year>2017</year>). <article-title>Randomized clinical trial of real-time fMRI amygdala neurofeedback for major depressive disorder: effects on symptoms and autobiographical memory recall.</article-title>
<source><italic>Am. J. Psychiatry</italic></source>
<volume>174</volume>
<fpage>748</fpage>–<lpage>755</lpage>. <pub-id pub-id-type="doi">10.1176/appi.ajp.2017.16060637</pub-id>
<?supplied-pmid 28407727?><pub-id pub-id-type="pmid">28407727</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
