<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7703771</article-id>
    <article-id pub-id-type="pmid">31584634</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz718</article-id>
    <article-id pub-id-type="publisher-id">btz718</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Graph embedding on biomedical networks: methods, applications and evaluations</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Yue</surname>
          <given-names>Xiang</given-names>
        </name>
        <xref ref-type="aff" rid="btz718-aff1">1</xref>
        <xref ref-type="corresp" rid="btz718-cor1"/>
        <!--<email>yue.149@osu.edu</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Zhen</given-names>
        </name>
        <xref ref-type="aff" rid="btz718-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Huang</surname>
          <given-names>Jingong</given-names>
        </name>
        <xref ref-type="aff" rid="btz718-aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Parthasarathy</surname>
          <given-names>Srinivasan</given-names>
        </name>
        <xref ref-type="aff" rid="btz718-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Moosavinasab</surname>
          <given-names>Soheil</given-names>
        </name>
        <xref ref-type="aff" rid="btz718-aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Huang</surname>
          <given-names>Yungui</given-names>
        </name>
        <xref ref-type="aff" rid="btz718-aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lin</surname>
          <given-names>Simon M</given-names>
        </name>
        <xref ref-type="aff" rid="btz718-aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Wen</given-names>
        </name>
        <xref ref-type="aff" rid="btz718-aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Ping</given-names>
        </name>
        <xref ref-type="aff" rid="btz718-aff1">1</xref>
        <xref ref-type="aff" rid="btz718-aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sun</surname>
          <given-names>Huan</given-names>
        </name>
        <xref ref-type="aff" rid="btz718-aff1">1</xref>
        <xref ref-type="corresp" rid="btz718-cor1"/>
        <!--<email>sun.397@osu.edu</email>-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Cowen</surname>
          <given-names>Lenore</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <aff id="btz718-aff1"><label>1</label><institution>Department of Computer Science and Engineering</institution>, OH, <country country="US">USA</country></aff>
    <aff id="btz718-aff2"><label>2</label><institution>Department of Electrical and Computer Engineering, The Ohio State University</institution>, Columbus, OH, <country country="US">USA</country></aff>
    <aff id="btz718-aff3"><label>3</label><institution>Research Information Solutions and Innovation, The Research Institute at Nationwide Children’s Hospital</institution>, Columbus, OH, <country country="US">USA</country></aff>
    <aff id="btz718-aff4"><label>4</label><institution>College of Informatics, Huazhong Agricultural University</institution>, Wuhan, Hubei, <country country="CN">China</country></aff>
    <aff id="btz718-aff5"><label>5</label><institution>Department of Biomedical Informatics, The Ohio State University</institution>, Columbus, OH, <country country="US">USA</country></aff>
    <author-notes>
      <corresp id="btz718-cor1">To whom correspondence should be addressed. <email>yue.149@osu.edu</email> or <email>sun.397@osu.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>15</day>
      <month>2</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-10-04">
      <day>04</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>04</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>36</volume>
    <issue>4</issue>
    <fpage>1241</fpage>
    <lpage>1251</lpage>
    <history>
      <date date-type="received">
        <day>09</day>
        <month>4</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>25</day>
        <month>8</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>26</day>
        <month>9</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz718.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Graph embedding learning that aims to automatically learn low-dimensional node representations, has drawn increasing attention in recent years. To date, most recent graph embedding methods are evaluated on social and information networks and are not comprehensively studied on biomedical networks under systematic experiments and analyses. On the other hand, for a variety of biomedical network analysis tasks, traditional techniques such as matrix factorization (which can be seen as a type of graph embedding methods) have shown promising results, and hence there is a need to systematically evaluate the more recent graph embedding methods (e.g. random walk-based and neural network-based) in terms of their usability and potential to further the state-of-the-art.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We select 11 representative graph embedding methods and conduct a systematic comparison on 3 important biomedical <italic>link prediction</italic> tasks: drug-disease association (DDA) prediction, drug–drug interaction (DDI) prediction, protein–protein interaction (PPI) prediction; and 2 <italic>node classification</italic> tasks: medical term semantic type classification, protein function prediction. Our experimental results demonstrate that the recent graph embedding methods achieve promising results and deserve more attention in the future biomedical graph analysis. Compared with three state-of-the-art methods for DDAs, DDIs and protein function predictions, the recent graph embedding methods achieve competitive performance without using any biological features and the learned embeddings can be treated as complementary representations for the biological features. By summarizing the experimental results, we provide general guidelines for properly selecting graph embedding methods and setting their hyper-parameters for different biomedical tasks.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>As part of our contributions in the paper, we develop an easy-to-use Python package with detailed instructions, BioNEV, available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/xiangyue9607/BioNEV">https://github.com/xiangyue9607/BioNEV</ext-link>, including all source code and datasets, to facilitate studying various graph embedding methods on biomedical tasks.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">Patient-Centered Outcomes Research Institute</named-content>
          <named-content content-type="funder-identifier">10.13039/100006093</named-content>
        </funding-source>
        <award-id>ME-2017C1-6413</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="11"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Graphs (a.k.a. networks) have been widely used to represent biomedical entities (as nodes) and their relations (as edges). Analyzing biomedical graphs can greatly benefit various important biomedical tasks, such as predicting potential drug indications (a.k.a. drug repositioning) based on drug-disease association (DDA) graphs (<xref rid="btz718-B16" ref-type="bibr">Gottlieb <italic>et al.</italic>, 2011</xref>), detecting long non-coding RNA (lncRNA) functions based on lncRNA–protein interaction networks (<xref rid="btz718-B60" ref-type="bibr">Zhang <italic>et al.</italic>, 2018d</xref>), and assisting clinical decision making via disease-symptom graphs (<xref rid="btz718-B40" ref-type="bibr">Rotmensch <italic>et al.</italic>, 2017</xref>). </p>
    <p>In order to analyze the graph data, a surge of graph embedding (a.k.a. network embedding or graph representation learning) methods (<xref rid="btz718-B17" ref-type="bibr">Grover and Leskovec, 2016</xref>; <xref rid="btz718-B38" ref-type="bibr">Perozzi <italic>et al.</italic>, 2014</xref>; Ribeiro <italic>et al.</italic>, <xref rid="btz718-B38" ref-type="bibr">2017</xref>; <xref rid="btz718-B47" ref-type="bibr">Tang <italic>et al.</italic>, 2015</xref>) have been proposed, where their goal is to automatically learn a low-dimensional feature representation for each node in the graph. Intuitively, the low-dimensional representations are learned to preserve the structural information of graphs, and thus can be used as features in building machine learning models for various downstream tasks, such as link prediction, community detection, node classification and clustering. However, to date, these advanced approaches are mainly evaluated on non-biomedical networks such as social networks, citation networks, and user-item networks, and only a few studies provide evaluations and analyses on biomedical networks. For example, <xref rid="btz718-B34" ref-type="bibr">Nelson <italic>et al.</italic> (2019)</xref> review the application of embedding methods on three representative biomedical prediction tasks. For each task, they select two biomedical embedding methods for comparison. But some of the selected methods are biomedical task-driven and may not be generalized to other tasks. And there exist more graph embedding methods in open-domain that need comprehensive comparison. Some recent studies (<xref rid="btz718-B18" ref-type="bibr">Hamilton <italic>et al.</italic>, 2017</xref>; <xref rid="btz718-B44" ref-type="bibr">Su <italic>et al.</italic>, 2018</xref>; <xref rid="btz718-B57" ref-type="bibr">Zhang <italic>et al.</italic>, 2018a</xref>) review the technical details of graph embedding methods, but few of them have systematically compared the performance of each method on biomedical datasets.</p>
    <p>On the other hand, traditional embedding techniques such as Laplacian eigenmap (LE) (<xref rid="btz718-B4" ref-type="bibr">Belkin and Niyogi, 2003</xref>) and matrix factorization (MF) have shown promising results for a variety biomedical graph analysis tasks (<xref rid="btz718-B13" ref-type="bibr">Ezzat <italic>et al.</italic>, 2017</xref>; <xref rid="btz718-B56" ref-type="bibr">You <italic>et al.</italic>, 2017</xref>). Given that the recent graph embedding methods have been demonstrated to be more effective than the traditional methods in a wide range of non-biomedical tasks (<xref rid="btz718-B17" ref-type="bibr">Grover and Leskovec, 2016</xref>; <xref rid="btz718-B38" ref-type="bibr">Perozzi <italic>et al.</italic>, 2014</xref>; <xref rid="btz718-B47" ref-type="bibr">Tang <italic>et al.</italic>, 2015</xref>), we conduct this work to <italic>investigate the effectiveness and potential of advanced graph embedding methods on biomedical tasks</italic>. <xref ref-type="fig" rid="btz718-F1">Figure 1</xref> summarizes the pipeline for applying various graph embedding methods to downstream prediction tasks. 
</p>
    <fig id="btz718-F1" orientation="portrait" position="float">
      <label>Fig. 1.</label>
      <caption>
        <p>Pipeline for applying graph embedding methods to biomedical tasks. Low-dimensional node representations are first learned from biomedical networks by graph embedding methods and then used as features to build specific classifiers for different tasks. For (<bold>a</bold>) matrix factorization-based methods, they use a data matrix (e.g. adjacency matrix) as the input to learn embeddings through matrix factorization. For (<bold>b</bold>) random walk-based methods, they first generate sequences of nodes through random walks and then feed the sequences into the word2vec model (<xref rid="btz718-B32" ref-type="bibr">Mikolov <italic>et al.</italic>, 2013</xref>) to learn node representations. For (<bold>c</bold>) neural network-based methods, their architectures and inputs vary from different models (see Section 2 for details)</p>
      </caption>
      <graphic xlink:href="btz718f1"/>
    </fig>
    <p>In this paper, we first provide an overview of existing graph embedding methods and their applications on three important biomedical <italic>link prediction</italic> tasks: DDA prediction (<xref rid="btz718-B16" ref-type="bibr">Gottlieb <italic>et al.</italic>, 2011</xref>), drug–drug interaction (DDI) prediction (<xref rid="btz718-B58" ref-type="bibr">Zhang <italic>et al.</italic>, 2018b</xref>), protein–protein interaction (PPI) prediction (<xref rid="btz718-B53" ref-type="bibr">Wang <italic>et al.</italic>, 2017</xref>) and one popular node classification task, protein function prediction (<xref rid="btz718-B8" ref-type="bibr">Cho <italic>et al.</italic>, 2016</xref>). In addition, we formulate a relatively less-studied but meaningful node classification task, medical term semantic type classification and apply graph embedding methods to solve it. For the above 5 tasks, we compile 7 datasets from commonly used biomedical databases or previous studies and select 11 graph embedding methods (including both traditional and more recent methods) for comprehensive comparisons. By benchmarking them, we demonstrate that the recent graph embedding methods can achieve promising results in various biomedical tasks and should deserve more attention in the future biomedical graph analysis. Additionally, we compare the graph embedding methods with three recent computational methods that are specially designed for DDAs, DDIs and protein function prediction. The results indicate that the graph embedding methods can achieve very competitive or better performance while being general (i.e. applied on different graphs and tasks). The learned embedding can also be treated as a complementary representation for the biological features. By summarizing the experimental results, we provide insightful observations as well as suggestions for selecting proper graph embedding methods and setting their hyper-parameters for biomedical prediction tasks. For instance, for MF-based methods, modeling high-order proximity (<xref rid="btz718-B10" ref-type="bibr">Cowen <italic>et al.</italic>, 2017</xref>) [e.g. HOPE (<xref rid="btz718-B36" ref-type="bibr">Ou <italic>et al.</italic>, 2016</xref>) and GraRep (<xref rid="btz718-B6" ref-type="bibr">Cao <italic>et al.</italic>, 2015</xref>)] is more useful in biomedical link prediction tasks compared with node classification tasks. For random walk-based methods, DeepWalk (<xref rid="btz718-B38" ref-type="bibr">Perozzi <italic>et al.</italic>, 2014</xref>) and node2vec (<xref rid="btz718-B17" ref-type="bibr">Grover and Leskovec, 2016</xref>) perform better in node classification tasks while struc2vec achieves better results in biomedical link prediction tasks. We also discuss the connections between embedding methods and the recent network propagation and diffusion methods in biomedical graph analysis (<xref rid="btz718-B10" ref-type="bibr">Cowen <italic>et al.</italic>, 2017</xref>). Additionally, we illustrate a few new trends and directions (e.g. transfer learning in biomedical graph embedding) to encourage future work.</p>
    <p>To summarize, our contributions are 3-fold:
<list list-type="bullet"><list-item><p>We provide an overview of different types of graph embedding methods, and discuss how they can be used in three important biomedical <italic>link prediction</italic> tasks: DDAs, DDIs and PPIs prediction; and two <italic>node classification</italic> tasks, protein function prediction and medical term semantic type classification.</p></list-item><list-item><p>We compile 7 benchmark datasets for all the above prediction tasks and use them to systematically evaluate 11 representative graph embedding methods selected from different categories (i.e. 5 MF-based, 3 random walk-based, 3 neural network-based). We discuss our observations from extensive experiments and provide some insights and guidelines for how to choose embedding methods (including their hyper-parameter settings).</p></list-item><list-item><p>We develop an easy-to-use Python package with detailed instructions, BioNEV (<underline>Bio</underline>medical <underline>N</underline>etwork <underline>E</underline>mbedding <underline>Ev</underline>aluation), available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/xiangyue9607/BioNEV">https://github.com/xiangyue9607/BioNEV</ext-link>, including all source code and datasets, to facilitate studying various graph embedding methods on biomedical tasks.</p></list-item></list></p>
  </sec>
  <sec>
    <title>2 Overview of graph embedding methods</title>
    <p>In this section, we provide a brief overview of different graph embedding methods that are categorized into three groups: MF-based, random walk-based and neural network-based (<xref ref-type="fig" rid="btz718-F1">Fig. 1</xref> provides a high-level illustration).</p>
    <sec>
      <title>2.1 MF-based methods</title>
      <p>MF has been widely adopted for data analyses. Essentially, it aims to factorize a data matrix into lower dimensional matrices and still keep the manifold structure and topological properties hidden in the original data matrix. Pioneer work in this category dates back to the early 2000s, such as Isomap (<xref rid="btz718-B48" ref-type="bibr">Tenenbaum <italic>et al.</italic>, 2000</xref>), Locally Linear Embedding (<xref rid="btz718-B41" ref-type="bibr">Roweis and Saul, 2000</xref>) and LEs (<xref rid="btz718-B4" ref-type="bibr">Belkin and Niyogi, 2003</xref>). Traditional MF has many variants, such as singular value decomposition (SVD) and graph factorization (GF) (<xref rid="btz718-B1" ref-type="bibr">Ahmed <italic>et al.</italic>, 2013</xref>). And they often focus on factorizing the first-order data matrix (e.g. adjacency matrix).</p>
      <p>More recently, researchers focus on designing various high-order data proximity matrices to preserve the graph structure and propose various MF-based graph embedding learning methods. For example, GraRep (<xref rid="btz718-B6" ref-type="bibr">Cao <italic>et al.</italic>, 2015</xref>) considers the high-order proximity of the network and designs <italic>k</italic>-step transition probability matrices for factorization. HOPE (<xref rid="btz718-B36" ref-type="bibr">Ou <italic>et al.</italic>, 2016</xref>) also considers the high-order proximity. But different from GraRep, it adopts some well-known network similarity measures such as Katz Index and Common Neighbors to preserve network structures.</p>
    </sec>
    <sec>
      <title>2.2 Random walk-based methods</title>
      <p>Inspired by the word2vec (<xref rid="btz718-B32" ref-type="bibr">Mikolov <italic>et al.</italic>, 2013</xref>) model, a popular word embedding technique from Natural Language Processing (NLP), which tries to learn word representations from sentences, random walk-based methods are developed to learn node representations by generating ‘node sequences’ through random walks in graphs. Specifically, given a graph and a starting node, random walk-based methods first select one of the node’s neighbors randomly and then move to this neighbor. This procedure is repeated to obtain node sequences. Then the word2vec model is adopted to learn embeddings based on the generated sequences of nodes. In this way, structural and topological information can be preserved into latent features.</p>
      <p>One of the initial works in this category is DeepWalk (<xref rid="btz718-B38" ref-type="bibr">Perozzi <italic>et al.</italic>, 2014</xref>), which performs truncated random walks on a graph. Compared with DeepWalk, node2vec (<xref rid="btz718-B17" ref-type="bibr">Grover and Leskovec, 2016</xref>) adopts a flexible biased random walk procedure that smoothly combines breadth-first sampling and depth-first sampling to generate node sequences. Furthermore, struc2vec (<xref rid="btz718-B39" ref-type="bibr">Ribeiro <italic>et al.</italic>, 2017</xref>) is proposed for better modeling the structural identity (e.g. nodes in the network may perform similar functions). Particularly, struct2vec first constructs a multi-layer weighted graph that encodes the structural similarity between nodes where each layer <italic>k</italic> is defined by using the <italic>k</italic>-hop neighborhoods of the nodes. DeepWalk is then performed on the multilayer graph to learn node representations in which nodes with high structural similarity are close to each other in the embedding space.</p>
    </sec>
    <sec>
      <title>2.3 Neural network-based methods</title>
      <p>Recent years have witnessed the success of neural network models in many fields. Various neural networks also have been introduced into graph embedding areas, such as multilayer perceptron (MLP) (<xref rid="btz718-B47" ref-type="bibr">Tang <italic>et al.</italic>, 2015</xref>), autoencoder (<xref rid="btz718-B7" ref-type="bibr">Cao <italic>et al.</italic>, 2016</xref>; <xref rid="btz718-B20" ref-type="bibr">Kipf and Welling, 2016</xref>; <xref rid="btz718-B50" ref-type="bibr">Wang <italic>et al.</italic>, 2016</xref>), generative adversarial network (GAN) (<xref rid="btz718-B52" ref-type="bibr">Wang <italic>et al.</italic>, 2018</xref>) and graph convolutional network (GCN) (<xref rid="btz718-B20" ref-type="bibr">Kipf and Welling, 2016</xref>, <xref rid="btz718-B21" ref-type="bibr">2017</xref>). Different methods adopt different neural architectures and use different kinds of graph information as input. For example, LINE (<xref rid="btz718-B47" ref-type="bibr">Tang <italic>et al.</italic>, 2015</xref>) directly models node embedding vectors by approximating the first-order proximity and second-order proximity of nodes, which can be seen as a single-layer MLP model. DNGR (<xref rid="btz718-B7" ref-type="bibr">Cao <italic>et al.</italic>, 2016</xref>) applies the stacked denoising autoencoders on the positive pointwise mutual information (PPMI) matrix to learn deep low-dimensional node embeddings. SDNE (<xref rid="btz718-B50" ref-type="bibr">Wang <italic>et al.</italic>, 2016</xref>) adopts a deep autoencoder to preserve the second-order proximity by reconstructing the neighborhood structure of each node; meanwhile, it also incorporates LEs proximity measure into the learning framework to exploit the first-order proximity. GAE (<xref rid="btz718-B20" ref-type="bibr">Kipf and Welling, 2016</xref>) utilizes a GCN encoder and an inner product decoder to learn node embeddings. GraphGAN (<xref rid="btz718-B52" ref-type="bibr">Wang <italic>et al.</italic>, 2018</xref>) adopts GANs to model the connectivity of nodes. The GAN framework includes a generator and a discriminator where the generator approximates the true connectivity distribution over all other nodes and generates fake samples, while the discriminator model detects whether the sampled nodes are from ground truth or generated by the generator.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Applications of graph embedding on biomedical networks</title>
    <p>In this section, we select 11 representative graph embedding methods (5 MF-based, 3 random walk-based, 3 neural network-based), and review how they are used on 3 popular biomedical <italic>link prediction</italic> applications: DDA prediction, DDI prediction, PPI prediction; and 2 biomedical <italic>node classification</italic> applications: protein function prediction and medical term semantic type classification.</p>
    <sec>
      <title>3.1 Link prediction</title>
      <p>Discovering new interactions (links) is one of the most important tasks in the biomedical area. A considerable amount of efforts has been devoted to developing computational methods to predict potential interactions in various biomedical networks, such as the DDA network (<xref rid="btz718-B26" ref-type="bibr">Liang <italic>et al.</italic>, 2017</xref>), DDI network (<xref rid="btz718-B58" ref-type="bibr">Zhang <italic>et al.</italic>, 2018b</xref>) and PPI network (<xref rid="btz718-B51" ref-type="bibr">Wang <italic>et al.</italic>, 2014</xref>). Developing such computational methods can help generate hypotheses of potential associations or interactions in biological networks.</p>
      <p>The link prediction task can be formulated as: <italic>given</italic> <italic>a set of biomedical entities and their known interactions, we aim to predict other potential interactions between entities</italic> (<xref rid="btz718-B29" ref-type="bibr">Lü and Zhou, 2011</xref>). Traditional methods in the biomedical field put much effort on feature engineering to develop biological features [e.g. chemical substructures (<xref rid="btz718-B26" ref-type="bibr">Liang <italic>et al.</italic>, 2017</xref>), gene ontology (<xref rid="btz718-B16" ref-type="bibr">Gottlieb <italic>et al.</italic>, 2011</xref>)] or graph properties [e.g. topological similarities (<xref rid="btz718-B18" ref-type="bibr">Hamilton <italic>et al.</italic>, 2017</xref>)]. After that, supervised learning methods [e.g. support vector machine (SVM), Random Forest] (<xref rid="btz718-B18" ref-type="bibr">Hamilton <italic>et al.</italic>, 2017</xref>) or semi-supervised graph inference model [e.g. label propagation (<xref rid="btz718-B10" ref-type="bibr">Cowen <italic>et al.</italic>, 2017</xref>)] are utilized to predict potential interactions. The assumption behind these methods is that entities sharing similar biological features or graph features could have similar connections.</p>
      <p>However, deploying methods based on biological features typically faces two problems: (i) biological features may not always be available and can be hard and costly to obtain. One popular approach to solve this problem is to remove those biological entities without features via pre-processing, which usually results in small-scale pruned datasets and thus is not pragmatic and useful in the real setting. (ii) Biological features, as well as hand-crafted graph features (e.g. node degrees), may not be precise enough to represent or characterize biomedical entities, and may fail to help build a robust and accurate model for many applications (<xref rid="btz718-B18" ref-type="bibr">Hamilton <italic>et al.</italic>, 2017</xref>).</p>
      <p>Graph embedding methods that seek to learn node representations automatically are promising to solve the two problems mentioned above. Embedding ideas have also been employed in some recently proposed computational methods in the biomedical field. For example, MF-based techniques (<xref rid="btz718-B11" ref-type="bibr">Dai <italic>et al.</italic>, 2015</xref>; <xref rid="btz718-B55" ref-type="bibr">Yang <italic>et al.</italic>, 2014</xref>; <xref rid="btz718-B59" ref-type="bibr">Zhang <italic>et al.</italic>, 2018c</xref>) are used for predictions of DDAs. Essentially, a DDA matrix is factorized to learn low-dimensional representations for drugs and diseases in the latent space. During factorization, regularization terms or constraints can be added to further improve the quality of latent representations. For predictions of DDIs, <xref rid="btz718-B58" ref-type="bibr">Zhang <italic>et al.</italic> (2018b</xref>) propose manifold regularized MF in which Laplacian regularization is incorporated to learn a better drug representation. Besides, graph neural network is introduced for DDIs prediction (<xref rid="btz718-B30" ref-type="bibr">Ma <italic>et al.</italic>, 2018</xref>; <xref rid="btz718-B64" ref-type="bibr">Zitnik <italic>et al.</italic>, 2018</xref>) and the intuitions are similar to the GAE (<xref rid="btz718-B20" ref-type="bibr">Kipf and Welling, 2016</xref>). PPIs are commonly predicted using Laplacian and SVD techniques (You <italic>et al.</italic>, <xref rid="btz718-B55" ref-type="bibr">2017</xref>; <xref rid="btz718-B61" ref-type="bibr">Zhu <italic>et al.</italic>, 2013</xref>). More recently, <xref rid="btz718-B53" ref-type="bibr">Wang <italic>et al.</italic> (2017</xref>) propose an autoencoder-based model to learn embeddings of proteins, which has a similar design to SDNE (<xref rid="btz718-B50" ref-type="bibr">Wang <italic>et al.</italic>, 2016</xref>).</p>
    </sec>
    <sec>
      <title>3.2 Node classification</title>
      <p>In addition to the link prediction task, <italic>node classification</italic> which aims to predict the class of unlabeled nodes given a partially labeled graph, is also one of the most important applications in graph analyses. Here, we mainly focus on two node classification applications: protein function prediction and medical term semantic type classification.</p>
      <p><italic>Protein function prediction.</italic> The large-scale experimental functional annotation of proteins is often expensive (<xref rid="btz718-B15" ref-type="bibr">Gligorijević <italic>et al.</italic>, 2018</xref>; <xref rid="btz718-B22" ref-type="bibr">Kulmanov <italic>et al.</italic>, 2018</xref>), hence graph-based computational methods which widely incorporate the idea of graph embedding, have been proposed in recent years. For example, <xref rid="btz718-B27" ref-type="bibr">Lim <italic>et al.</italic> (2018)</xref> propose a regularized Laplacian kernel-based method to learn low-dimensional embeddings of proteins. <xref rid="btz718-B8" ref-type="bibr">Cho <italic>et al.</italic> (2016)</xref> develop Mashup, which first performs random walks with restart (RWR) on PPI networks and then learns embeddings for each protein via a low rank matrix approximation method (can be optimized by SVD). The feature vectors are then fed into classifiers to derive functional insights about genes or proteins. <xref rid="btz718-B22" ref-type="bibr">Kulmanov <italic>et al.</italic> (2018)</xref> propose DeepGO that learns joint representations of proteins based on protein sequences as well as PPI network via convolutional neural nets and a graph embedding method (<xref rid="btz718-B2" ref-type="bibr">Alshahrani <italic>et al.</italic>, 2017</xref>) [similar to DeepWalk (<xref rid="btz718-B38" ref-type="bibr">Perozzi <italic>et al.</italic>, 2014</xref>)]. In node2vec, <xref rid="btz718-B17" ref-type="bibr">Grover and Leskovec (2016)</xref> test the effectiveness of the proposed embedding method on a PPI network. Furthermore, <xref rid="btz718-B62" ref-type="bibr">Zitnik and Leskovec (2017)</xref> develop OhmNet, which optimizes hierarchical dependency objectives based on node2vec to learn feature representations in multi-layer tissue networks for function prediction. <xref rid="btz718-B15" ref-type="bibr">Gligorijević <italic>et al.</italic> (2018)</xref> develop deepNF, which learns embeddings of proteins via a deep autoencoder [similar to SDNE (<xref rid="btz718-B50" ref-type="bibr">Wang <italic>et al.</italic>, 2016</xref>)].</p>
      <p><italic>Medical term semantic type classification.</italic> In the past few years, the increase of clinical texts have been encouraging data-driven models for improving the patient personal care and help clinical decision (<xref rid="btz718-B33" ref-type="bibr">Mullenbach <italic>et al.</italic>, 2018</xref>). However, due to the privacy and security concerns, the access to <italic>raw clinical texts</italic> is often limited (<xref rid="btz718-B3" ref-type="bibr">Beam <italic>et al.</italic>, 2018</xref>; <xref rid="btz718-B14" ref-type="bibr">Finlayson <italic>et al.</italic>, 2014</xref>; <xref rid="btz718-B46" ref-type="bibr">Ta <italic>et al.</italic>, 2018</xref>). To facilitate research on clinical texts, a popular substitute strategy for releasing raw clinical texts is to extract medical terms and their aggregated co-occurrence counts from the clinical texts (<xref rid="btz718-B14" ref-type="bibr">Finlayson <italic>et al.</italic>, 2014</xref>; <xref rid="btz718-B46" ref-type="bibr">Ta <italic>et al.</italic>, 2018</xref>). However, such released privacy-aware datasets only contain medical terms (words or phrases) extracted from clinical texts and do not reveal the semantic information (e.g. semantic types or categories). By referring to some medical knowledge bases, e.g. unified medical language system (UMLS) (<xref rid="btz718-B5" ref-type="bibr">Bodenreider, 2004</xref>), we can obtain semantic types (labels) medical terms. But due to mismatch and incomplete knowledge in UMLS, the semantic types of some medical terms remain unknown. Hence, we formulate a less-investigated but meaningful node classification task (<xref ref-type="fig" rid="btz718-F2">Fig. 2</xref>): g<italic>iven a medical term co-occurrence graph where terms and co-occurrence statistics have been extracted from clinical texts, classify the semantic types of medical terms.</italic> In this work, we assume the clinical texts have been converted into a medical term–term co-occurrence graph as in <xref rid="btz718-B14" ref-type="bibr">Finlayson <italic>et al.</italic> (2014)</xref>, where each node is an extracted medical term and each edge is the co-occurrence count of two terms in a context window. We apply graph embedding methods to the co-occurrence graph to learn representations of medical terms. Afterward, a multi-label classifier can be trained based on the learned embeddings to classify the semantic types of medical terms.
</p>
      <fig id="btz718-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>Illustration of (<bold>a</bold>) how medical term–term co-occurrence graph is constructed and (<bold>b</bold>) node type classification in the graph. Our work assumes that the graph is given as in <xref rid="btz718-B14" ref-type="bibr">Finlayson <italic>et al.</italic> (2014)</xref> and mainly focuses on (b), i.e. testing various embedding methods on the classification performance</p>
        </caption>
        <graphic xlink:href="btz718f2"/>
      </fig>
    </sec>
    <sec>
      <title>3.3 Summary</title>
      <p>In order to show the current research status of evaluated graph embedding methods on the above biomedical applications, we summarize 11 graph embedding techniques by 3 categories and the existing works which have applied these techniques on certain tasks in <xref rid="btz718-T1" ref-type="table">Table 1</xref>. As can be seen, existing methods for the five representative biomedical applications primarily adopt the traditional techniques, e.g. LEs, MF. On the other hand, more recent advanced graph embedding methods have been demonstrated to outperform traditional techniques in social/information networks (<xref rid="btz718-B6" ref-type="bibr">Cao <italic>et al.</italic>, 2015</xref>; <xref rid="btz718-B17" ref-type="bibr">Grover and Leskovec, 2016</xref>; <xref rid="btz718-B47" ref-type="bibr">Tang <italic>et al.</italic>, 2015</xref>), but their performance in biomedical networks is not unknown. In addition, the comparison between these general graph embedding methods and state-of-the-arts in the individual prediction task should be explored to encourage future research. Hence, we conduct comprehensive experiments to evaluate those 11 graph embedding methods selected from 3 different categories on 5 representative biomedical tasks and compare them against the state-of-the-arts in each biomedical prediction task.
</p>
      <table-wrap id="btz718-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>A summary of 11 representative graph embedding methods and existing work (if any) using them for a certain task</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="×" span="1"/>
            <col valign="top" align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th colspan="2" rowspan="1">Method category</th>
              <th align="left" rowspan="1" colspan="1">Method name</th>
              <th align="left" colspan="3" rowspan="1">Link prediction tasks<hr/></th>
              <th align="left" colspan="2" rowspan="1">Node classification tasks<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">Drug-disease association prediction</th>
              <th align="left" rowspan="1" colspan="1">Drug–drug interaction prediction</th>
              <th align="left" rowspan="1" colspan="1">Protein–protein interaction prediction</th>
              <th align="left" rowspan="1" colspan="1">Medical term type classification</th>
              <th align="left" rowspan="1" colspan="1">Protein function prediction</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Traditional</td>
              <td rowspan="1" colspan="1">Matrix factorization- based</td>
              <td rowspan="1" colspan="1">Laplacian (<xref rid="btz718-B4" ref-type="bibr">Belkin and Niyogi, 2003</xref>)</td>
              <td rowspan="1" colspan="1"><xref rid="btz718-B59" ref-type="bibr">Zhang <italic>et al.</italic> (2018c</xref>)</td>
              <td rowspan="1" colspan="1">(<xref rid="btz718-B58" ref-type="bibr">Zhang <italic>et al.</italic>, 2018b</xref>)</td>
              <td rowspan="1" colspan="1">
                <xref rid="btz718-B61" ref-type="bibr">Zhu <italic>et al.</italic> (2013)</xref>
              </td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td rowspan="1" colspan="1">
                <xref rid="btz718-B27" ref-type="bibr">Lim <italic>et al.</italic> (2018)</xref>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">SVD</td>
              <td rowspan="1" colspan="1">
                <xref rid="btz718-B11" ref-type="bibr">Dai <italic>et al.</italic> (2015)</xref>
              </td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td rowspan="1" colspan="1">
                <xref rid="btz718-B56" ref-type="bibr">You <italic>et al.</italic> (2017)</xref>
              </td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td rowspan="1" colspan="1">
                <xref rid="btz718-B8" ref-type="bibr">Cho <italic>et al.</italic> (2016)</xref>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GF (<xref rid="btz718-B1" ref-type="bibr">Ahmed <italic>et al.</italic>, 2013</xref>)</td>
              <td rowspan="1" colspan="1"><xref rid="btz718-B55" ref-type="bibr">Yang <italic>et al.</italic> (2014)</xref> and <xref rid="btz718-B59" ref-type="bibr">Zhang <italic>et al.</italic> (2018c</xref>)</td>
              <td rowspan="1" colspan="1">(<xref rid="btz718-B58" ref-type="bibr">Zhang <italic>et al.</italic>, 2018b</xref>)</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Recently Proposed</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">HOPE (<xref rid="btz718-B36" ref-type="bibr">Ou <italic>et al.</italic>, 2016</xref>)</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GraRep (<xref rid="btz718-B6" ref-type="bibr">Cao <italic>et al.</italic>, 2015</xref>)</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">Random walk-based</td>
              <td rowspan="1" colspan="1">DeepWalk (<xref rid="btz718-B38" ref-type="bibr">Perozzi <italic>et al.</italic>, 2014</xref>)</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td rowspan="1" colspan="1"><xref rid="btz718-B8" ref-type="bibr">Cho <italic>et al.</italic> (2016)</xref> and <xref rid="btz718-B22" ref-type="bibr"> Kulmanov <italic>et al.</italic> (2018)</xref></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">node2vec (<xref rid="btz718-B17" ref-type="bibr">Grover and Leskovec, 2016</xref>)</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td rowspan="1" colspan="1"><xref rid="btz718-B17" ref-type="bibr">Grover and Leskovec (2016)</xref> and <xref rid="btz718-B62" ref-type="bibr">Zitnik and Leskovec (2017)</xref></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">struc2vec (<xref rid="btz718-B39" ref-type="bibr">Ribeiro <italic>et al.</italic>, 2017</xref>)</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">Neural network-based</td>
              <td rowspan="1" colspan="1">LINE (<xref rid="btz718-B47" ref-type="bibr">Tang <italic>et al.</italic>, 2015</xref>)</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">SDNE (<xref rid="btz718-B50" ref-type="bibr">Wang <italic>et al.</italic>, 2016</xref>)</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td rowspan="1" colspan="1"><xref rid="btz718-B53" ref-type="bibr">Wang <italic>et al.</italic> (2017</xref>)</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td rowspan="1" colspan="1">
                <xref rid="btz718-B15" ref-type="bibr">Gligorijević <italic>et al.</italic> (2018)</xref>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GAE (<xref rid="btz718-B20" ref-type="bibr">Kipf and Welling, 2016</xref>)</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td rowspan="1" colspan="1"><xref rid="btz718-B64" ref-type="bibr">Zitnik <italic>et al.</italic> (2018)</xref> and <xref rid="btz718-B30" ref-type="bibr">Ma <italic>et al.</italic> (2018)</xref></td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
              <td align="center" rowspan="1" colspan="1">✗</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic>Note</italic>: ✗ means that a method (row) has not been applied for a task (column).</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>We follow the pipeline (shown in <xref ref-type="fig" rid="btz718-F1">Fig. 1</xref>) of the widely adopted link prediction and node classification methods in general domains (<xref rid="btz718-B17" ref-type="bibr">Grover and Leskovec, 2016</xref>; <xref rid="btz718-B47" ref-type="bibr">Tang <italic>et al.</italic>, 2015</xref>): graph embeddings are first learned and then used as feature inputs to build a binary classifier or multi-label classifier (e.g. Logistic Regression, SVM, MLP) to predict the unobserved links or the node labels.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Experiments</title>
    <p>In this section, we introduce the details of seven compiled datasets, including two DDA graphs, a DDI graph, a PPI graph for <italic>link prediction</italic> and a medical term–term co-occurrence graph as well as two PPI graphs for <italic>node classification</italic>. Then, we conduct comprehensive comparisons of 11 selected graph embedding methods on these compiled datasets.</p>
    <sec>
      <title>4.1 Datasets</title>
      <p>We use the following datasets for <italic>Link Prediction</italic>:
<list list-type="order"><list-item><p><italic>DDA</italic> <italic>graph</italic>. We extract chemical-disease associations from the Comparative Toxicogenomics Database (CTD) (<xref rid="btz718-B12" ref-type="bibr">Davis <italic>et al.</italic>, 2018</xref>). CTD offers two kinds of associations: curated (verified) and inferred. Since our task is to infer potential chemical-disease associations, we only use curated ones as our golden instances. Finally, we obtain 92 813 edges between 12 765 nodes (9580 chemicals and 3185 diseases) in this graph (named as ‘CTD DDA’). Also, we construct another DDA network from National Drug File Reference Terminology (NDF-RT) in UMLS (<xref rid="btz718-B5" ref-type="bibr">Bodenreider, 2004</xref>). NDF-RT is produced by the US Department of Veterans Affairs, and models drug characteristics including ingredients, physiologic effect and related diseases. We extract drug-disease treatment associations using the <italic>may treat</italic> and <italic>may be treated by</italic> relationships in NDF-RT. This graph (named ‘NDFRT DDA’) contains 13 545 nodes (12 337 drugs and 1208 diseases) and 56 515 edges.</p></list-item><list-item><p><italic>DDI</italic> <italic>graph.</italic> We collect verified DDIs from DrugBank (<xref rid="btz718-B54" ref-type="bibr">Wishart <italic>et al.</italic>, 2018</xref> ), a comprehensive and freely accessible online database that contains detailed information about drugs and drug targets. We obtain 242 027 DDIs between 2191 drugs and refer to this dataset as ‘DrugBank DDI’.</p></list-item><list-item><p><italic>PPI</italic> <italic>graph.</italic> We extract <italic>Homo sapiens</italic> PPIs from STRING database (<xref rid="btz718-B45" ref-type="bibr">Szklarczyk <italic>et al.</italic>, 2015</xref>). Each PPI is associated with a confidence score that indicates its possibility to be a true positive interaction. To reduce noise, we only collect PPI whose confidence score is larger than 0.7 according to the guidelines of STRING database. Finally, we obtain 359 776 interactions among 15 131 proteins and name this dataset as ‘STRING PPI’.</p></list-item></list></p>
      <p>We use the following datasets for <italic>Node Classification</italic>:
<list list-type="order"><list-item><p><italic>Medical term</italic>–<italic>term co-occurrence graph.</italic> We adopt a publicly available set of medical terms with their co-occurrence statistics which are extracted by <xref rid="btz718-B14" ref-type="bibr">Finlayson <italic>et al.</italic> (2014)</xref> from 20 million clinical notes collected from Stanford Hospitals and Clinics (<xref rid="btz718-B28" ref-type="bibr">Lowe <italic>et al.</italic>, 2009</xref>) since 1995. Medical terms are extracted from raw clinical notes using an existing phrase mining tool (<xref rid="btz718-B23" ref-type="bibr">LePendu <italic>et al.</italic>, 2012</xref>) by matching with 22 clinically relevant ontologies such as SNOMED-CT and MedDRA. Co-occurrence frequencies between two terms are counted based on how many times they co-occur in the same temporal <italic>bin</italic> (i.e. a certain time-frame; see <xref rid="btz718-B14" ref-type="bibr">Finlayson <italic>et al.</italic>, 2014</xref> for more details). We select <italic>perBin 1-day</italic> dataset since it contains more medical terms compared with other bins. To filter very common medical terms (e.g. ‘medical history’, ‘medication dose’) that may influence the quality of embeddings, we convert the co-occurrence counts to the PPMI value (<xref rid="btz718-B24" ref-type="bibr">Levy and Goldberg, 2014</xref>) and remove the edges whose PPMI value is &lt;2. We also adopt a subsampling (<xref rid="btz718-B32" ref-type="bibr">Mikolov <italic>et al.</italic>, 2013</xref>) strategy to further filter common terms and construct a medical term–term co-occurrence graph that contains 48 651 medical terms and 1 659 249 edges. We keep the medical terms that can be mapped to the UMLS Concept Unique Identifiers (CUI) and collect their corresponding semantic types (e.g. clinical drug, disease or syndrome) from UMLS. We select 31 different semantic types, with each having more than 20 samples. Finally, we obtain 25 120 nodes with label information. This dataset is called ‘Clin Term COOC’.</p></list-item><list-item><p><italic>PPI graphs with functional annotations.</italic> We also compile two PPI graphs with functional annotations from previous studies. One is from node2vec (<xref rid="btz718-B17" ref-type="bibr">Grover and Leskovec, 2016</xref>), which contains 3890 proteins, 76 584 interactions and 50 different function annotations (labels). This dataset is named as ‘node2vec PPI’. The other one is from Mashup (<xref rid="btz718-B8" ref-type="bibr">Cho <italic>et al.</italic>, 2016</xref>), which is designed for integrating different information from multiple networks. The Mashup dataset contains six individual PPI networks (e.g. experimental, coexpression). Given that our selected graph embedding methods can only work on a single network, we select the <italic>experimental</italic> PPI network (there are six individual PPI networks in Mashup dataset, we select the <italic>experimental</italic> PPI network since Mashup achieves the best performance on it under single-network circumstance) to learn embeddings. The <italic>experimental</italic> PPI network contains 300 181 interactions between 16 143 proteins. Same to Mashup, we use the 3 grouped distinct levels of functional categories of varying specificity, each containing 28 100 and 262 different annotations, respectively. We only adopt the first level (28 labels) for the main comparison experiment for simplicity. Other label information is used in comparing the recent embedding methods with Mashup in Section 4.4. This dataset is called ‘Mashup PPI’. </p></list-item></list></p>
      <p>The details of all datasets are summarized in <xref rid="btz718-T2" ref-type="table">Table 2</xref>.
</p>
      <table-wrap id="btz718-T2" orientation="portrait" position="float">
        <label>Table 2.</label>
        <caption>
          <p>Statistics of the datasets, where the <italic>Density</italic> is defined as <inline-formula id="IE1"><mml:math id="IM1"><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mtext>no</mml:mtext><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mtext>edges</mml:mtext><mml:mo>/</mml:mo><mml:mtext>no</mml:mtext><mml:mo>.</mml:mo><mml:mo> </mml:mo><mml:mtext>node</mml:mtext><mml:msup><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Task type</th>
              <th align="left" rowspan="1" colspan="1">Dataset</th>
              <th align="left" rowspan="1" colspan="1">No. nodes</th>
              <th align="left" rowspan="1" colspan="1">No. edges</th>
              <th align="left" rowspan="1" colspan="1">Density</th>
              <th align="left" rowspan="1" colspan="1">No. node labels</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Link prediction</td>
              <td rowspan="1" colspan="1">CTD DDA</td>
              <td rowspan="1" colspan="1">12 765</td>
              <td rowspan="1" colspan="1">92 813</td>
              <td rowspan="1" colspan="1">0.11%</td>
              <td rowspan="1" colspan="1">—</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">NDFRT DDA</td>
              <td rowspan="1" colspan="1">13 545</td>
              <td rowspan="1" colspan="1">56 515</td>
              <td rowspan="1" colspan="1">0.06%</td>
              <td rowspan="1" colspan="1">—</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">DrugBank DDI</td>
              <td rowspan="1" colspan="1">2191</td>
              <td rowspan="1" colspan="1">242 027</td>
              <td rowspan="1" colspan="1">10.08%</td>
              <td rowspan="1" colspan="1">—</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">STRING PPI</td>
              <td rowspan="1" colspan="1">15 131</td>
              <td rowspan="1" colspan="1">359 776</td>
              <td rowspan="1" colspan="1">0.31%</td>
              <td rowspan="1" colspan="1">—</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Node classification</td>
              <td rowspan="1" colspan="1">Clin Term COOC</td>
              <td rowspan="1" colspan="1">48 651</td>
              <td rowspan="1" colspan="1">1 659 249</td>
              <td rowspan="1" colspan="1">0.14%</td>
              <td rowspan="1" colspan="1">31</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">node2vec PPI</td>
              <td rowspan="1" colspan="1">3890</td>
              <td rowspan="1" colspan="1">76 584</td>
              <td rowspan="1" colspan="1">1.01%</td>
              <td rowspan="1" colspan="1">50</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">MashUp PPI</td>
              <td rowspan="1" colspan="1">16 143</td>
              <td rowspan="1" colspan="1">300 181</td>
              <td rowspan="1" colspan="1">0.23%</td>
              <td rowspan="1" colspan="1">28</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>4.2 Experimental set-up</title>
      <p>We use OpenNE (<ext-link ext-link-type="uri" xlink:href="https://github.com/thunlp/OpenNE">https://github.com/thunlp/OpenNE</ext-link>), an open-source Python package for network embedding, to learn node embeddings for LEs (<xref rid="btz718-B4" ref-type="bibr">Belkin and Niyogi, 2003</xref>), HOPE (<xref rid="btz718-B36" ref-type="bibr">Ou <italic>et al.</italic>, 2016</xref>), GF (<xref rid="btz718-B1" ref-type="bibr">Ahmed <italic>et al.</italic>, 2013</xref>), DeepWalk (<xref rid="btz718-B38" ref-type="bibr">Perozzi <italic>et al.</italic>, 2014</xref>), LINE (<xref rid="btz718-B47" ref-type="bibr">Tang <italic>et al.</italic>, 2015</xref>) and SDNE (<xref rid="btz718-B50" ref-type="bibr">Wang <italic>et al.</italic>, 2016</xref>). We run SVD using Numpy (<ext-link ext-link-type="uri" xlink:href="http://www.numpy.org/">http://www.numpy.org/</ext-link>) and obtain struc2vec (<ext-link ext-link-type="uri" xlink:href="https://github.com/leoribeiro/struc2vec">https://github.com/leoribeiro/struc2vec</ext-link>) (<xref rid="btz718-B39" ref-type="bibr">Ribeiro <italic>et al.</italic>, 2017</xref>) and GAE (<ext-link ext-link-type="uri" xlink:href="https://github.com/tkipf/gae">https://github.com/tkipf/gae</ext-link>) (<xref rid="btz718-B20" ref-type="bibr">Kipf and Welling, 2016</xref>) embeddings using the source code provided by their authors. More implementation details can be found in <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>.</p>
      <p>For the link prediction tasks (Section 4.3), all the known interactions are positive samples and are split into the training set (80%) and testing set (20%). Since unknown interactions are far more than known ones, we randomly select disconnected edges as negative samples with an equal number of positive samples in both training and testing phase. For each node pair, we concatenate the embeddings of two nodes as the edge feature and then build a Logistic Regression binary classifier based on it using scikit-learn package (<xref rid="btz718-B37" ref-type="bibr">Pedregosa <italic>et al.</italic>, 2011</xref>). Area under ROC curve (<italic>AUC</italic>), <italic>accuracy</italic> and <italic>F1</italic> score are used to evaluate the performance of the classifiers, so as to evaluate different embedding methods.</p>
      <p>For the node classification task (Section 4.4), we use the entire graph information to train the embeddings. Nodes with label information are then split into the training set (80%) and the testing set (20%). The embedding vectors of nodes are directly treated as feature vectors and used to train <italic>One-vs-Rest</italic> Logistic Regression classifiers using the scikit-learn package. We assign top <italic>α<sub>i</sub></italic> predictions to the node <italic>i</italic> as its predicted labels, where <italic>α<sub>i</sub></italic> is the number of golden labels of the node <italic>i</italic> in the testing set. <italic>Accuracy</italic>, <italic>Macro-F1</italic> and <italic>Micro-F1</italic> are used to evaluate the performance of different embedding methods on the testing set. Accuracy is defined as the percentage of samples that have all their labels classified correctly. <italic>F1</italic> score is the harmonic mean of precision and recall. We adopt two weighted strategies of <italic>F1</italic> score: micro (calculate metrics globally by counting the total true positives, false negatives and false positives) and macro (calculate metrics for each label, and find their unweighted mean).</p>
      <p>For all embedding methods, the dimensionality of the learned embedding is set to 100 unless otherwise stated (we also discuss its impact on the performance in Section 4.5). Moreover, we tune 1–2 significant hyper-parameters for some embedding methods via grid-search (see Section 4.5 for details). Other hyper-parameters for each method are set at their default values recommended by the corresponding papers.</p>
    </sec>
    <sec>
      <title>4.3 Link prediction results</title>
      <p>We conduct the link prediction task on the 4 compiled biomedical networks: CTD DDA, NDFRT DDA, DrugBank DDI and STRING PPI. <xref rid="btz718-T3" ref-type="table">Table 3</xref> shows the overall performance of different embedding methods on the four datasets.
</p>
      <table-wrap id="btz718-T3" orientation="portrait" position="float">
        <label>Table 3.</label>
        <caption>
          <p>Overall link prediction performance on the four compiled biomedical datasets</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th colspan="2" rowspan="1">Method category</th>
              <th align="left" rowspan="1" colspan="1">Method name</th>
              <th align="left" rowspan="1" colspan="1">CTD DDA</th>
              <th align="left" rowspan="1" colspan="1">NDFRT DDA</th>
              <th align="left" rowspan="1" colspan="1">DrugBank DDI</th>
              <th align="left" rowspan="1" colspan="1">STRING PPI</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Traditional</td>
              <td rowspan="1" colspan="1">Matrix factorization-based</td>
              <td rowspan="1" colspan="1">Laplacian (<xref rid="btz718-B4" ref-type="bibr">Belkin and Niyogi, 2003</xref>)</td>
              <td rowspan="1" colspan="1">0.856±0.004</td>
              <td rowspan="1" colspan="1">0.930±0.003</td>
              <td rowspan="1" colspan="1">0.796±0.002</td>
              <td rowspan="1" colspan="1">0.639±0.021</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">SVD</td>
              <td rowspan="1" colspan="1">0.936±0.002</td>
              <td rowspan="1" colspan="1">0.779±0.003</td>
              <td rowspan="1" colspan="1">0.919±0.001</td>
              <td rowspan="1" colspan="1">0.867±0.001</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GF (<xref rid="btz718-B1" ref-type="bibr">Ahmed <italic>et al.</italic>, 2013</xref>)</td>
              <td rowspan="1" colspan="1">0.884±0.004</td>
              <td rowspan="1" colspan="1">0.720±0.006</td>
              <td rowspan="1" colspan="1">0.882±0.003</td>
              <td rowspan="1" colspan="1">0.817±0.005</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Recently proposed</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">HOPE (<xref rid="btz718-B36" ref-type="bibr">Ou <italic>et al.</italic>, 2016</xref>)</td>
              <td rowspan="1" colspan="1">0.951±0.001</td>
              <td rowspan="1" colspan="1">0.949±0.001</td>
              <td rowspan="1" colspan="1">0.923±0.001</td>
              <td rowspan="1" colspan="1">0.839±0.001</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GraRep (<xref rid="btz718-B6" ref-type="bibr">Cao <italic>et al.</italic>, 2015</xref>)</td>
              <td rowspan="1" colspan="1">
                <bold>0.960±0.001</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.963±0.001</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.925±0.001</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.894±0.001</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td colspan="6" rowspan="1">
                <hr/>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">Random walk-based</td>
              <td rowspan="1" colspan="1">DeepWalk (<xref rid="btz718-B38" ref-type="bibr">Perozzi <italic>et al.</italic>, 2014</xref>)</td>
              <td rowspan="1" colspan="1">0.929±0.002</td>
              <td rowspan="1" colspan="1">0.783±0.004</td>
              <td rowspan="1" colspan="1">
                <bold>0.921±0.001</bold>
              </td>
              <td rowspan="1" colspan="1">0.884±0.001</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">node2vec (<xref rid="btz718-B17" ref-type="bibr">Grover and Leskovec, 2016</xref>)</td>
              <td rowspan="1" colspan="1">0.911±0.002</td>
              <td rowspan="1" colspan="1">0.819±0.005</td>
              <td rowspan="1" colspan="1">0.902±0.001</td>
              <td rowspan="1" colspan="1">0.828±0.003</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">struc2vec (<xref rid="btz718-B39" ref-type="bibr">Ribeiro <italic>et al.</italic>, 2017</xref>)</td>
              <td rowspan="1" colspan="1">
                <bold>0.965±0.001</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.958±0.001</bold>
              </td>
              <td rowspan="1" colspan="1">0.904±0.001</td>
              <td rowspan="1" colspan="1">
                <bold>0.909±0.001</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td colspan="6" rowspan="1">
                <hr/>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">Neural network-based</td>
              <td rowspan="1" colspan="1">LINE (<xref rid="btz718-B47" ref-type="bibr">Tang <italic>et al.</italic>, 2015</xref>)</td>
              <td rowspan="1" colspan="1">
                <bold>0.965±0.001</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.962±0.002</bold>
              </td>
              <td rowspan="1" colspan="1">0.905±0.002</td>
              <td rowspan="1" colspan="1">0.859±0.003</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">SDNE (<xref rid="btz718-B50" ref-type="bibr">Wang <italic>et al.</italic>, 2016</xref>)</td>
              <td rowspan="1" colspan="1">0.935±0.010</td>
              <td rowspan="1" colspan="1">0.944±0.004</td>
              <td rowspan="1" colspan="1">0.911±0.006</td>
              <td rowspan="1" colspan="1">0.884±0.008</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GAE (<xref rid="btz718-B20" ref-type="bibr">Kipf and Welling, 2016</xref>)</td>
              <td rowspan="1" colspan="1">0.937±0.001</td>
              <td rowspan="1" colspan="1">0.813±0.007</td>
              <td rowspan="1" colspan="1">
                <bold>0.917±0.001</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.900±0.001</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p><italic>Note</italic>: Due to the limited space, we only show the <italic>AUC</italic> value. Other evaluation metrics can be found in <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>. The best performing method in each category is in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Generally, compared with traditional techniques (e.g. LEs, SVD, GF), the recently proposed embedding methods have largely improved the link prediction performance. For example, LINE achieves 3–23% improvement in terms of <italic>AUC</italic> value on the four datasets compared with LEs. Struc2vec obtains 3–15% increment in the <italic>accuracy</italic> on the four datasets, respectively, when compared with GF. These results demonstrate that the recently proposed graph embedding methods are more effective and could be used on various biological link prediction tasks to improve the prediction performance.</p>
      <p>Furthermore, we have the following key observations and analyses:
<list list-type="bullet"><list-item><p><italic>For the</italic> <italic>MF</italic><italic>-based methods</italic>, since HOPE and GraRep are designed to capture the high-order proximity of graphs, they are usually more effective than traditional MF methods that only preserve the first-order of networks.</p></list-item><list-item><p><italic>For the random walk-based methods</italic>, generally, struc2vec performs better than DeepWalk and node2vec. This is because compared with DeepWalk and node2vec, struc2vec constructs a hierarchy weighted graph to measure the structural identity. Such hierarchy structure design incorporates both node degree distributions from the bottom as well as the entire network on the top, which can better capture the graph structure information and hence obtain better performance.</p></list-item><list-item><p><italic>For the neural network-based methods</italic>, LINE achieves competitive prediction performance consistently when compared with the best performing method on each dataset. It indicates that directly modeling edge information by a single-layer MLP is an effective way to learn node embeddings. SDNE and GAE also obtain satisfying prediction performance, which demonstrates that autoencoders and GCNs can also be useful for capturing graph structural information.</p></list-item></list></p>
      <p><italic>Comparison with state-of-the-art studies.</italic> To further demonstrate the effectiveness of graph embedding methods, we compare them with the state-of-the-art methods for two link prediction: DDA prediction and DDI prediction.</p>
      <p>For the DDAs prediction, we select LRSSL (<xref rid="btz718-B26" ref-type="bibr">Liang <italic>et al.</italic>, 2017</xref>) as our baseline. LRSSL is a Laplacian regularized sparse subspace learning framework which aims to project different drug features into a common subspace. Three drug feature profiles (i.e. chemical substructure, target domain and target annotation) are used in the training process. To be fair, we adopt the code and dataset used in the LRSSL. To learn graph embeddings without modeling biological features, we run four representative graph embedding methods: GraRep, DeepWalk, LINE and struc2vec on LRSSL’s DDA graph. Following the same train/test split, training and evaluation process of link prediction in Section 4.2, we plot the ROC Curves to illustrate the performance of different methods better. As seen in <xref ref-type="fig" rid="btz718-F3">Figure 3a</xref>, graph embedding methods achieve competitive performance compared with LRSSL. Furthermore, we use the learned DeepWalk embedding vectors as the fourth feature for the LRSSL method and improve the LRSSL performance, which indicates that the learned node embedding can be used as a <italic>complementary representation</italic> for biological features.
</p>
      <fig id="btz718-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>(<bold>a</bold>) Comparison with the state-of-the-arts for drug-disease association prediction (LRSSL) (<xref rid="btz718-B26" ref-type="bibr">Liang <italic>et al.</italic>, 2017</xref>); (<bold>b</bold>) drug–drug interaction prediction (DeepDDI) (<xref rid="btz718-B42" ref-type="bibr">Ryu <italic>et al.</italic>, 2018</xref>) and (<bold>c</bold>) gene (protein) function prediction (Mashup) (<xref rid="btz718-B8" ref-type="bibr">Cho <italic>et al.</italic>, 2016</xref>). Same as Mashup, we evaluate their performance on three-level human Biological Process (BP) gene annotations (each containing GO terms with 101–300, 31–100 and 11–30 genes, respectively). As can be seen, in each task, general graph embedding methods achieve competitive performance against them </p>
        </caption>
        <graphic xlink:href="btz718f3"/>
      </fig>
      <p>For the DDIs prediction, we compare the embedding methods with a recent method DeepDDI (<xref rid="btz718-B42" ref-type="bibr">Ryu <italic>et al.</italic>, 2018</xref>). DeepDDI first adopts principal component analysis to reduce the dimension of the drug features (i.e. drug substructure) and then feeds these into a deep neural network (DNN) classifier. For a fair comparison with graph embedding methods and to reduce the bias caused by different classifiers, we compare these methods under four classifiers, Naive Bayes, Linear SVM, Logistic Regression and eight-layer DNN (the same as the original paper). More implement details can be found in <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>. As seen in <xref ref-type="fig" rid="btz718-F3">Figure 3b</xref>, graph embedding methods outperform the drug features-based model or obtain very competitive performance under each classifier, which demonstrates the power of graph embedding methods.</p>
    </sec>
    <sec>
      <title>4.4 Node classification results</title>
      <p><xref rid="btz718-T4" ref-type="table">Table 4</xref> shows the performance of different embedding methods on medical term semantic type classification and protein function prediction. We make the following key observations:
</p>
      <table-wrap id="btz718-T4" orientation="portrait" position="float">
        <label>Table 4.</label>
        <caption>
          <p>Overall node classification performance on the three compiled datasets</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method category</th>
              <th align="left" rowspan="1" colspan="1">Method name</th>
              <th align="left" colspan="2" rowspan="1">Clini COOC<hr/></th>
              <th align="left" colspan="2" rowspan="1">node2vec PPI<hr/></th>
              <th align="left" colspan="2" rowspan="1">Mashup PPI<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">Micro-F1</th>
              <th align="left" rowspan="1" colspan="1">Macro-F1</th>
              <th align="left" rowspan="1" colspan="1">Micro-F1</th>
              <th align="left" rowspan="1" colspan="1">Macro-F1</th>
              <th align="left" rowspan="1" colspan="1">Micro-F1</th>
              <th align="left" rowspan="1" colspan="1">Macro-F1</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Matrix factorization-based</td>
              <td rowspan="1" colspan="1">Laplacian (<xref rid="btz718-B4" ref-type="bibr">Belkin and Niyogi, 2003</xref>)</td>
              <td rowspan="1" colspan="1">0.313±0.005</td>
              <td rowspan="1" colspan="1">0.073±0.002</td>
              <td rowspan="1" colspan="1">0.101±0.008</td>
              <td rowspan="1" colspan="1">0.070±0.007</td>
              <td rowspan="1" colspan="1">0.132±0.009</td>
              <td rowspan="1" colspan="1">0.107±0.008</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">SVD</td>
              <td rowspan="1" colspan="1">0.420±0.005</td>
              <td rowspan="1" colspan="1">0.186±0.007</td>
              <td rowspan="1" colspan="1">0.228±0.011</td>
              <td rowspan="1" colspan="1">0.179±0.011</td>
              <td rowspan="1" colspan="1">0.347±0.014</td>
              <td rowspan="1" colspan="1">0.297±0.014</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GF (<xref rid="btz718-B1" ref-type="bibr">Ahmed <italic>et al.</italic>, 2013</xref>)</td>
              <td rowspan="1" colspan="1">0.352±0.007</td>
              <td rowspan="1" colspan="1">0.143±0.009</td>
              <td rowspan="1" colspan="1">0.168±0.011</td>
              <td rowspan="1" colspan="1">0.121±0.011</td>
              <td rowspan="1" colspan="1">0.290±0.015</td>
              <td rowspan="1" colspan="1">0.237±0.016</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">HOPE (<xref rid="btz718-B36" ref-type="bibr">Ou <italic>et al.</italic>, 2016</xref>)</td>
              <td rowspan="1" colspan="1">0.395±0.005</td>
              <td rowspan="1" colspan="1">0.163±0.006</td>
              <td rowspan="1" colspan="1">0.208±0.011</td>
              <td rowspan="1" colspan="1">0.152±0.011</td>
              <td rowspan="1" colspan="1">0.322±0.013</td>
              <td rowspan="1" colspan="1">0.266±0.013</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GraRep (<xref rid="btz718-B6" ref-type="bibr">Cao <italic>et al.</italic>, 2015</xref>)</td>
              <td rowspan="1" colspan="1">
                <bold>0.424±0.006</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.177±0.005</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.238±0.010</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.193±0.013</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.334±0.011</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.283±0.011</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Random walk-based</td>
              <td rowspan="1" colspan="1">DeepWalk (<xref rid="btz718-B38" ref-type="bibr">Perozzi <italic>et al.</italic>, 2014</xref>)</td>
              <td rowspan="1" colspan="1">0.472±0.005</td>
              <td rowspan="1" colspan="1">0.227±0.007</td>
              <td rowspan="1" colspan="1">
                <bold>0.243±0.001</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.194±0.011</bold>
              </td>
              <td rowspan="1" colspan="1">0.357±0.011</td>
              <td rowspan="1" colspan="1">0.311±0.012</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">node2vec (<xref rid="btz718-B17" ref-type="bibr">Grover and Leskovec, 2016</xref>)</td>
              <td rowspan="1" colspan="1">
                <bold>0.479±0.005</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.231±0.010</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.243±0.009</bold>
              </td>
              <td rowspan="1" colspan="1">0.190±0.011</td>
              <td rowspan="1" colspan="1">
                <bold>0.367±0.012</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.313±0.013</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">struc2vec (<xref rid="btz718-B39" ref-type="bibr">Ribeiro <italic>et al.</italic>, 2017</xref>)</td>
              <td rowspan="1" colspan="1">0.253±0.006</td>
              <td rowspan="1" colspan="1">0.038±0.001</td>
              <td rowspan="1" colspan="1">0.094±0.006</td>
              <td rowspan="1" colspan="1">0.061±0.004</td>
              <td rowspan="1" colspan="1">0.120±0.010</td>
              <td rowspan="1" colspan="1">0.087±0.008</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Neural network-based</td>
              <td rowspan="1" colspan="1">LINE (<xref rid="btz718-B47" ref-type="bibr">Tang <italic>et al.</italic>, 2015</xref>)</td>
              <td rowspan="1" colspan="1">
                <bold>0.453±0.006</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.205±0.008</bold>
              </td>
              <td rowspan="1" colspan="1">0.236±0.011</td>
              <td rowspan="1" colspan="1">0.176±0.012</td>
              <td rowspan="1" colspan="1">0.352±0.017</td>
              <td rowspan="1" colspan="1">0.296±0.017</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">SDNE (<xref rid="btz718-B50" ref-type="bibr">Wang <italic>et al.</italic>, 2016</xref>)</td>
              <td rowspan="1" colspan="1">0.271±0.016</td>
              <td rowspan="1" colspan="1">0.042±0.007</td>
              <td rowspan="1" colspan="1">0.098±0.010</td>
              <td rowspan="1" colspan="1">0.047±0.007</td>
              <td rowspan="1" colspan="1">0.178±0.013</td>
              <td rowspan="1" colspan="1">0.109±0.012</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GAE (<xref rid="btz718-B20" ref-type="bibr">Kipf and Welling, 2016</xref>)<xref ref-type="table-fn" rid="tblfn4"><sup>a</sup></xref></td>
              <td rowspan="1" colspan="1">—</td>
              <td rowspan="1" colspan="1">—</td>
              <td rowspan="1" colspan="1">
                <bold>0.237±0.014</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.186±0.014</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.358±0.013</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.307±0.014</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn3">
            <p><italic>Note</italic>: The best performing method in each category is in bold.</p>
          </fn>
          <fn id="tblfn4">
            <label>a</label>
            <p>The source code of GAE provided by the authors does not support a large-scale graph (nodes&gt;40k). We omit its performance on ‘Clini COOC’ here.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <list list-type="bullet">
        <list-item>
          <p><italic>For the</italic> <italic>MF</italic><italic>-based methods</italic>, it is a little surprising that the traditional method SVD achieves better performance, even surpassing HOPE and GraRep. This may indicate that directly modeling the first-order proximity would be good enough to classify the nodes.</p>
        </list-item>
        <list-item>
          <p><italic>For the random walk-based methods</italic>, node2vec performs better since it aims to capture different functions of nodes (i.e. homophily and structural equivalence) via a more flexible biased random walk. Struc2vec performs not good as DeepWalk and node2vec as it mainly focuses on modeling the structural identity of nodes; however, a clear structural role may not exist in these biomedical graphs and struc2vec is not suitable on such graphs.</p>
        </list-item>
        <list-item>
          <p><italic>For the neural network-based methods</italic>, LINE achieves better performance than SDNE, which demonstrates that directly modeling edge information is an effective way to learn the embedding for the node classification task. And GAE also achieve promising performance, which demonstrates the power of the graph neural networks.</p>
        </list-item>
      </list>
      <p><italic>Comparison with state-of-the-art study.</italic> To better illustrate the effectiveness of the recent graph embedding methods in biomedical node classification tasks, we select protein function prediction as our representative node classification task and compare the graph embedding methods with a popular state-of-the-art: Mashup (<xref rid="btz718-B8" ref-type="bibr">Cho <italic>et al.</italic>, 2016</xref>).</p>
      <p>Mashup is also one of embedding learning methods. But different from other embedding methods which learn node embedding in a single network, Mashup is carefully designed to diffuse the information from multi-networks. Specifically, RWR is firstly used to compute the diffusion state for each node in each individual network. Low-dimensional embeddings are then obtained by jointly minimizing the difference between the observed diffusion states and the parameterized-multinomial logistic distributions across all networks. To make a fair comparison with Mashup, we construct a diffusion PPI network by doing a simple unweighted sum of each interaction score in the individual networks and then run different embedding methods on this simple diffusion network. As seen in <xref ref-type="fig" rid="btz718-F3">Figure 3c</xref>, the three representative graph embedding methods: DeepWalk, node2vec and LINE achieve very competitive or better performance compared with Mashup on three-level protein function prediction.</p>
      <p>Mashup is specially designed for protein/gene-related prediction tasks and has an advanced network diffusion strategy (e.g. jointly optimizing the embedding based on information from each individual network), but the recent embedding methods can still achieve competitive performance. This may give some inspirations for future study (e.g. considering to replace the current embedding optimization process of Mashup with DeepWalk, node2vec or LINE).</p>
    </sec>
    <sec>
      <title>4.5 Influence of hyper-parameters</title>
      <p>Hyper-parameters play essential roles in machine learning models. However, selecting proper hyper-parameters is often time-consuming. We investigate the influence of some important hyper-parameters in various embedding methods. By running grid-search of these important hyper-parameters of each method, we expect to summarize some general guidelines for helping researchers better set the hyper-parameters, so as to save their time and efforts.</p>
      <p>We first evaluate how different embedding dimensions can affect the prediction performance and time efficiency. <xref ref-type="fig" rid="btz718-F4">Figure 4</xref> shows the impact of embedding dimensionality on the prediction performance and time efficiency for ‘CTD DDA’ dataset. Generally, the prediction performance becomes better when the embedding dimensionality increases, which is intuitive since higher dimensionality can encode more useful information. Then, the performance tends to saturate when the dimension reaches to a threshold (e.g. 100). As for the time cost, it first increases gradually below 100 but tends to boost sharply (the <italic>y</italic>-axis is log-based) if the dimensionality continues to increase. So we would not suggest to set the dimensionality to be too large (e.g. around 100 is a good option) for the practitioners when considering both performance and time efficiency. The results of dimensionality’s influence on other datasets can be found in <xref ref-type="supplementary-material" rid="sup1">Supplementary Figures S1 and S2</xref>.
</p>
      <fig id="btz718-F4" orientation="portrait" position="float">
        <label>Fig. 4.</label>
        <caption>
          <p>The influence of dimensionality on the performance and training time of different embedding methods based on ‘CTD DDA’ dataset</p>
        </caption>
        <graphic xlink:href="btz718f4"/>
      </fig>
      <p>Furthermore, we choose sensitive hyper-parameters for 7 embedding methods, which have been pointed out to be important by their authors. <xref rid="btz718-T5" ref-type="table">Table 5</xref> shows the selected hyper-parameters in different embedding methods as well as their meanings. We spend a lot of efforts on carefully tuning these hyper-parameters by grid search. The influence of the hyper-parameters on each embedding method is shown in <xref ref-type="supplementary-material" rid="sup1">Supplementary Figures S3–S9</xref>, respectively. By summarizing these results, we provide some high-level guidelines on setting hyper-parameters for practitioners in <xref rid="btz718-T5" ref-type="table">Table 5</xref>.
</p>
      <table-wrap id="btz718-T5" orientation="portrait" position="float">
        <label>Table 5.</label>
        <caption>
          <p>Meanings of main hyper-parameters in different embedding methods and general guidelines for setting hyper-parameters of these embedding methods</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Methods</th>
              <th align="left" rowspan="1" colspan="1">Hyper-parameters</th>
              <th align="left" rowspan="1" colspan="1">General guidelines</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">GraRep (<xref rid="btz718-B6" ref-type="bibr">Cao <italic>et al.</italic>, 2015</xref>)</td>
              <td rowspan="1" colspan="1"><italic>Ksteps</italic>: <italic>k</italic>-step relational information (<italic>k</italic>-step transition probability matrix)</td>
              <td rowspan="1" colspan="1">A large value for link prediction tasks (e.g. 3, 4); a small value for node classification tasks (e.g. 1, 2)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeepWalk (<xref rid="btz718-B38" ref-type="bibr">Perozzi <italic>et al.</italic>, 2014</xref>)</td>
              <td rowspan="1" colspan="1"><italic>Number of walks</italic>: the number of walks at each node; <italic>walk length</italic>: the length of each walk;</td>
              <td rowspan="1" colspan="1">Large values for both (e.g. 64 128 256)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">node2vec (<xref rid="btz718-B17" ref-type="bibr">Grover and Leskovec, 2016</xref>)</td>
              <td rowspan="1" colspan="1"><italic>p</italic>, <italic>q</italic>: two parameters that control how fast the walk explores and leaves the neighborhood of starting node</td>
              <td rowspan="1" colspan="1">Vary from graphs to graphs, may tune at small values for both (e.g. 0.25)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">struc2vec (<xref rid="btz718-B39" ref-type="bibr">Ribeiro <italic>et al.</italic>, 2017</xref>)</td>
              <td rowspan="1" colspan="1"><italic>Number of walks</italic>: the number of walks at each node; <italic>walk length</italic>: the length of each walk</td>
              <td rowspan="1" colspan="1">Large values for both (e.g. 64 128 256)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LINE (<xref rid="btz718-B47" ref-type="bibr">Tang <italic>et al.</italic>, 2015</xref>)</td>
              <td rowspan="1" colspan="1"><italic>epochs</italic>: number of training epochs</td>
              <td rowspan="1" colspan="1">Small training epochs for small-scale graphs (e.g. 5); and large value for large-scale graph (e.g. 20)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SDNE (<xref rid="btz718-B50" ref-type="bibr">Wang <italic>et al.</italic>, 2016</xref>)</td>
              <td rowspan="1" colspan="1"><italic>α</italic>: balances the weight of first-order and second-order proximities; <italic>β</italic>: controls the reconstruction weight of the non-zero elements in the training graph</td>
              <td rowspan="1" colspan="1">Vary from graphs to graphs, may tune at small values for both (e.g. <italic>a</italic>=0.1, <italic>b</italic>=0)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GAE (<xref rid="btz718-B20" ref-type="bibr">Kipf and Welling, 2016</xref>)</td>
              <td rowspan="1" colspan="1"><italic>Hidden units</italic>: number of units in hidden layer</td>
              <td rowspan="1" colspan="1">A large value (e.g. 128)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>4.6 Summary of experimental results</title>
      <p>To better help the practitioners select proper embedding methods for their biomedical prediction task, we summarize the experimental results and discuss our observations:
<list list-type="bullet"><list-item><p>Generally, the recently proposed graph embedding methods achieve very promising results in various biomedical prediction tasks. They deserve more attention for future biomedical graph analysis.</p></list-item><list-item><p>By simply applying the recent graph embedding methods on biomedical graphs and then feeding into a classifier, we can achieve very competitive or better performance compared with state-of-the-arts. Future model design for biomedical prediction tasks may begin at these embedding methods or integrate them as one module into the proposed method, which is expected to gain better results.</p></list-item><list-item><p>In particular, for MF-based methods, we observe that modeling high-order proximity (e.g. HOPE, GraRep) is generally useful for link prediction tasks on medical graphs but may be less meaningful for the node classification tasks. For random walk-based methods, struc2vec is more suitable for link prediction tasks (when there is a lack of structural identity in graphs) while node2vec and DeepWalk are more suitable for node classification tasks. For neural network-based methods, LINE usually achieves competitive performance against the best performing method on each dataset. SDNE can achieve good performance on link prediction tasks but less satisfying performance on node classification. GAE performs well in relatively large-scale network but may not perform well on small-scale datasets.</p></list-item></list></p>
      <p>More details of the datasets, implementation, experiment results, guidelines can be found in <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>5 Discussions and future directions</title>
    <p><italic>Connections of network embedding and network propagation.</italic> In the recent biomedical network analyses, a very popular paradigm is network propagation (<xref rid="btz718-B10" ref-type="bibr">Cowen <italic>et al.</italic>, 2017</xref>), which amplifies a biological signal (e.g. label, association) based on the assumption that nodes with similar neighbors (e.g. genes underlying similar phenotypes) tend to interact with one another (<xref rid="btz718-B31" ref-type="bibr">Menche <italic>et al.</italic>, 2015</xref>). Specifically, the information of one node is propagated through the edges to their neighbors in an iterative manner for a fixed number of steps or until convergence (<xref rid="btz718-B10" ref-type="bibr">Cowen <italic>et al.</italic>, 2017</xref>). The core of these propagation methods is random walk, which is also adopted in many embedding methods (e.g. Deepwalk, node2vec and struc2vec). But different from network diffusion, which propagates the ‘signal’ in the network directly, the random walk-based embedding methods treat the ‘walk’ as a kind of node similarity or proximity characterizing method. They expect to preserve the network structural information as much as possible through a fixed number of random walks. These ‘walking histories’ (i.e. node sequences) are then fed into word2vec (<xref rid="btz718-B32" ref-type="bibr">Mikolov <italic>et al.</italic>, 2013</xref>) to learn low-dimensional embeddings. Though the pipeline of the random walk-based embedding methods and network propagation methods is different, their idea and assumption are similar. They both assume that nodes with similar neighbors have similar functions and tend to interact with each other. Besides random walk-based embedding methods, this assumption is also widely adopted in other embedding methods (e.g. LINE, SDNE).</p>
    <p>Additionally, there are some variants of random walk, e.g. random walk with restart (RWR), personalized PageRank and diffusion kernel. They also involve the embedding ideas, e.g. using Laplacian normalized matrix, factorizing inverse Laplacian matrix. These variants can also be incorporated into current random walk-based embedding framework.</p>
    <p><italic>Modeling external information in graphs.</italic> In addition to the graph structure, external information can also help build computational models for biomedical networks. For example, <xref rid="btz718-B59" ref-type="bibr">Zhang <italic>et al.</italic> (2018c</xref>) incorporate drug and disease features into MF to learn better representations. <xref rid="btz718-B63" ref-type="bibr">Žitnik and Zupan (2014)</xref> incorporate prior information (e.g. gene network) as a vector or a matrix to further improve the gene-related prediction tasks. There may also exist partial label information on graphs (e.g. semantic types are partly available for nodes in a medical term co-occurrence graph). Incorporating those features and labels into advanced graph embedding models can potentially further improve the performance. There have been a surge of <italic>attributed graph embedding</italic> methods that explore this direction. For example, DDRW (<xref rid="btz718-B25" ref-type="bibr">Li <italic>et al.</italic>, 2016</xref>) and MMDW (<xref rid="btz718-B49" ref-type="bibr">Tu <italic>et al.</italic>, 2016</xref>) jointly optimize the objective of DeepWalk with an SVM classification loss to incorporate label information. We leave benchmarking such <italic>attributed network embedding</italic> methods on biomedical graphs as our future work.</p>
    <p><italic>Transfer learning for graph embedding.</italic> Recent studies in Computer Vision and NLP show that <italic>transfer learning</italic> helps improve model performance on different tasks (<xref rid="btz718-B19" ref-type="bibr">Howard and Ruder, 2018</xref>; <xref rid="btz718-B43" ref-type="bibr">Shin <italic>et al.</italic>, 2016</xref>). General patterns are captured during pre-trained processes and can be ‘transferred’ into new prediction tasks. There also exist some pre-trained embeddings of biomedical entities (Beam <italic>et al.</italic>, 2018; <xref rid="btz718-B9" ref-type="bibr">Choi <italic>et al.</italic>, 2016</xref>) which allow us to adopt similar ideas of ‘transfer learning’ to learn graph embeddings. We can initialize the embedding vector for each node on a graph with its pre-trained embedding (e.g. by looking for the corresponding entity in <xref rid="btz718-B9" ref-type="bibr">Choi <italic>et al.</italic>, 2016</xref>; <xref rid="btz718-B3" ref-type="bibr">Beam <italic>et al.</italic>, 2018</xref>) rather than by random initialization, and then continue training various graph embedding methods as before (which is often referred to as ‘fine-tuning’). The pre-trained embeddings can be seen as ‘coarse embeddings’ since they are usually pre-trained on a large general corpus and have not been optimized for downstream tasks yet. Nevertheless, they contain some additional semantic information that may not be able to be learned from a downstream task graph (e.g. due to its small scale). By fine-tuning, such additional semantic information can be ‘transferred’ into the finally learned embeddings. We conduct experiment with this transfer learning idea on the ‘CTD DDA’ graph. As seen in <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S3</xref>, the link prediction performance has been improved using the pre-trained embeddings from <xref rid="btz718-B3" ref-type="bibr">Beam <italic>et al.</italic> (2018)</xref>. Currently, the number of released biomedical entities with pre-trained embeddings is still limited and entities without pre-trained embeddings have to be initialized randomly. However, with the increasing volume of biomedical data, more and more entities can have pre-trained embeddings, and the idea of <italic>pre-training</italic><italic>—</italic><italic>then</italic><italic>—</italic><italic>fine-tuning</italic> can be more promising.</p>
  </sec>
  <sec>
    <title>6 Conclusion</title>
    <p>This paper provides an overview of various graph embedding techniques and evaluates their performance on two important biomedical tasks, link prediction and node classification. Specifically, we compile 7 datasets from public database or previous studies and use them to benchmark 11 representative graph embedding methods. Through extensive experiments, we find that generally the recent graph embedding methods can perform well in various biomedical prediction tasks and can also achieve very competitive or better performance compared with state-of-the-arts. Hence, these recent graph embedding methods can be considered as a starting point when designing advanced models for future biomedical prediction tasks. Additionally, we tune some important hyper-parameters of graph embedding methods and provide general guidelines for setting hyper-parameters for practitioners. We also discuss the connections between the recent network propagation (diffusion) methods and the graph embedding methods as well as potential directions (e.g. transfer learning for graph embedding) to inspire the future work.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btz718_Supplementary_Data</label>
      <media xlink:href="btz718_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We thank Dr Deborah A. Petrone, Kaushik Mani and anonymous reviewers for their helpful comments and suggestions on our work, and Ohio Supercomputer Center (OSC) (<xref rid="btz718-B35" ref-type="bibr">Ohio Supercomputer Center, 1987</xref>) for providing us computing resources.</p>
    <sec>
      <title>Funding</title>
      <p>This work has been supported by Patient-Centered Outcomes Research Institute (PCORI) under grant ME-2017C1-6413. </p>
      <p><italic>Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz718-B1">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Ahmed</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) <chapter-title>Distributed large-scale natural graph factorization</chapter-title> In: <source>22nd International World Wide Web Conference, WWW ′13, Rio de Janeiro, Brazil</source>, pp. <fpage>37</fpage>–<lpage>48</lpage>. 
<publisher-name>ACM</publisher-name>. </mixed-citation>
    </ref>
    <ref id="btz718-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alshahrani</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Neuro-symbolic representation learning on biological knowledge graphs</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>2723</fpage>–<lpage>2730</lpage>.<pub-id pub-id-type="pmid">28449114</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Beam</surname><given-names>A.L.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Clinical concept embeddings learned from massive sources of medical data. arXiv: 1804.01486 [arXiv preprint]. </mixed-citation>
    </ref>
    <ref id="btz718-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Belkin</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Niyogi</surname><given-names>P.</given-names></name></person-group> (<year>2003</year>) 
<article-title>Laplacian eigenmaps for dimensionality reduction and data representation</article-title>. <source>Neural Comput</source>., <volume>15</volume>, <fpage>1373</fpage>–<lpage>1396</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bodenreider</surname><given-names>O.</given-names></name></person-group> (<year>2004</year>) 
<article-title>The unified medical language system (UMLS): integrating biomedical terminology</article-title>. <source>Nucleic Acids Res</source>., <volume>32</volume>(<issue>Suppl 1</issue>), <fpage>D267</fpage>–<lpage>D270</lpage>.<pub-id pub-id-type="pmid">14681409</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) <chapter-title>Grarep: learning graph representations with global structural information</chapter-title> In: <source>Proceedings of the 24th ACM International Conference on Information and Knowledge Management, Melbourne, VIC, Australia</source>, pp. <fpage>891</fpage>–<lpage>900</lpage>. 
<publisher-name>ACM</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btz718-B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) <chapter-title>Deep neural networks for learning graph representations</chapter-title> In: <source>Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, Phoenix, Arizona</source>, pp. <fpage>1145</fpage>–<lpage>1152</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cho</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Compact integration of multi-network topology for functional analysis of genes</article-title>. <source>Cell Syst</source>., <volume>3</volume>, <fpage>540</fpage>–<lpage>548</lpage>.<pub-id pub-id-type="pmid">27889536</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Choi</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Learning low-dimensional representations of medical concepts</article-title>. In: <italic>Summit on Clinical Research Informatics, San Francisco, CA</italic>.</mixed-citation>
    </ref>
    <ref id="btz718-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cowen</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Network propagation: a universal amplifier of genetic associations</article-title>. <source>Nat. Rev. Genet</source>., <volume>18</volume>, <fpage>551.</fpage><pub-id pub-id-type="pmid">28607512</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dai</surname><given-names>W.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Matrix factorization-based prediction of novel drug indications by integrating genomic space</article-title>. <source>Comput. Math. Methods Med</source>., <volume>2015</volume>, <fpage>1</fpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Davis</surname><given-names>A.P.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>The comparative toxicogenomics database: update 2019</article-title>. <source>Nucleic Acids Res</source>., <volume>47</volume>, <fpage>D948</fpage>–<lpage>D954</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ezzat</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Drug-target interaction prediction using ensemble learning and dimensionality reduction</article-title>. <source>Methods</source>, <volume>129</volume>, <fpage>81</fpage>–<lpage>88</lpage>.<pub-id pub-id-type="pmid">28549952</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Finlayson</surname><given-names>S.G.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Building the graph of medicine from millions of clinical narratives</article-title>. <source>Sci. Data</source>, <volume>1</volume>, <fpage>140032.</fpage><pub-id pub-id-type="pmid">25977789</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gligorijević</surname><given-names>V.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>deepnf: deep network fusion for protein function prediction</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>3873</fpage>–<lpage>3881</lpage>.<pub-id pub-id-type="pmid">29868758</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gottlieb</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Predict: a method for inferring novel drug indications with application to personalized medicine</article-title>. <source>Mol. Syst. Biol</source>., <volume>7</volume>, <fpage>496.</fpage><pub-id pub-id-type="pmid">21654673</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Grover</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Leskovec</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>) <chapter-title>node2vec: scalable feature learning for networks</chapter-title> In: <source>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA</source>, pp. <fpage>855</fpage>–<lpage>864</lpage>. 
<publisher-name>ACM</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btz718-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hamilton</surname><given-names>W.L.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Representation learning on graphs: Methods and applications. <source>IEEE Data Eng. Bull.</source>, <volume>40</volume>, <fpage>52</fpage>–<lpage>74</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Howard</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Ruder</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>) 
<article-title>Universal language model fine-tuning for text classification</article-title>. In: <source>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Melbourne, Australia</source>, vol. <volume>1</volume>, pp. <fpage>328</fpage>–<lpage>339</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kipf</surname><given-names>T.N.</given-names></name>, <name name-style="western"><surname>Welling</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>) Variational graph auto-encoders. <source>NIPS Workshop on Bayesian Deep Learning</source>.</mixed-citation>
    </ref>
    <ref id="btz718-B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kipf</surname><given-names>T.N.</given-names></name>, <name name-style="western"><surname>Welling</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>) Semi-supervised classification with graph convolutional networks. In: <source>5th International Conference on Learning Representations, Toulon, France.</source></mixed-citation>
    </ref>
    <ref id="btz718-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kulmanov</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Deepgo: predicting protein functions from sequence and interactions using a deep ontology-aware classifier</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>660</fpage>–<lpage>668</lpage>.<pub-id pub-id-type="pmid">29028931</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LePendu</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Annotation analysis for testing drug safety signals using unstructured clinical notes</article-title>. <source>J. Biomed. Semantics</source>, <volume>3</volume>, <fpage>S5</fpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Levy</surname><given-names>O.</given-names></name>, <name name-style="western"><surname>Goldberg</surname><given-names>Y.</given-names></name></person-group> (<year>2014</year>) 
<article-title>Linguistic regularities in sparse and explicit word representations</article-title>. In: <source>Proceedings of the Eighteenth Conference on Computational Natural Language Learning, Baltimore, Maryland</source>, pp. <fpage>171</fpage>–<lpage>180</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Discriminative deep random walk for network classification</article-title>. In: <source>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Berlin, Germany</source>, vol. <volume>1</volume>, pp. <fpage>1004</fpage>–<lpage>1013</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>X.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Lrssl: predict and interpret drug-disease associations based on data integration using sparse subspace learning</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>1187</fpage>–<lpage>1196</lpage>.<pub-id pub-id-type="pmid">28096083</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lim</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>A multi-species functional embedding integrating sequence and network structure</article-title>. In: <source>Research in Computational Molecular Biology–22nd Annual International Conference</source>, RECOMB 2018, Paris, France, pp. 263–265.</mixed-citation>
    </ref>
    <ref id="btz718-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lowe</surname><given-names>H.J.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>Stride—an integrated standards-based translational research informatics platform</article-title>. In: <source> American Medical Informatics Association Annual Symposium, San Francisco, CA</source>, pp. <fpage>391</fpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lü</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Zhou</surname><given-names>T.</given-names></name></person-group> (<year>2011</year>) 
<article-title>Link prediction in complex networks: a survey</article-title>. <source>Phys. A: Stat. Mech. Its Appl</source>., <volume>390</volume>, <fpage>1150</fpage>–<lpage>1170</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B30">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Drug similarity integration through attentive multi-view graph auto-encoders. In: <italic>Proceedings of the 27th International Joint Conference on Artificial Intelligence</italic>. AAAI Press, Stockholm, Sweden, pp. 3477–3483.</mixed-citation>
    </ref>
    <ref id="btz718-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Menche</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Uncovering disease-disease relationships through the incomplete interactome</article-title>. <source>Science</source>, <volume>347</volume>, <fpage>1257601.</fpage><pub-id pub-id-type="pmid">25700523</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B32">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Mikolov</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) Distributed representations of words and phrases and their compositionality. In: <italic>Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems, Lake Tahoe, Nevada</italic>, pp. 3111–3119.</mixed-citation>
    </ref>
    <ref id="btz718-B33">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Mullenbach</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Explainable prediction of medical codes from clinical text. In: <italic>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, New Orleans, Louisiana, Vol. 1 (Long Papers)</italic>, pp. <fpage>1101</fpage>–<lpage>1111</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nelson</surname><given-names>W.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>To embed or not: network embedding as a paradigm in computational biology</article-title>. <source>Front. Genet</source>., <volume>10</volume>, 381. </mixed-citation>
    </ref>
    <ref id="btz718-B35">
      <mixed-citation publication-type="book">Ohio Supercomputer Center (<year>1987</year>) <source>Ohio Supercomputer Center</source>. 
<publisher-name>Ohio Supercomputer Center</publisher-name>, 
<publisher-loc>Columbus, OH</publisher-loc>
<ext-link ext-link-type="uri" xlink:href="http://osc.edu/ark:/19495/f5s1ph73">http://osc.edu/ark:/19495/f5s1ph73</ext-link> (9 April 2019, date last accessed).</mixed-citation>
    </ref>
    <ref id="btz718-B36">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Ou</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) <chapter-title>Asymmetric transitivity preserving graph embedding</chapter-title> In: <source>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA</source>, pp. <fpage>1105</fpage>–<lpage>1114</lpage>. 
<publisher-name>ACM</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btz718-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pedregosa</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Scikit-learn: machine learning in Python</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B38">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Perozzi</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) <chapter-title>Deepwalk: online learning of social representations</chapter-title> In <source>The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York, NY</source>, pp. <fpage>701</fpage>–<lpage>710</lpage>. 
<publisher-name>ACM</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btz718-B39">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Ribeiro</surname><given-names>L.F.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) <chapter-title>struc2vec: learning node representations from structural identity</chapter-title> In: <source>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada</source>, pp. <fpage>385</fpage>–<lpage>394</lpage>. 
<publisher-name>ACM</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btz718-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rotmensch</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Learning a health knowledge graph from electronic medical records</article-title>. <source>Sci. Rep</source>., <volume>7</volume>, <fpage>5994.</fpage><pub-id pub-id-type="pmid">28729710</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Roweis</surname><given-names>S.T.</given-names></name>, <name name-style="western"><surname>Saul</surname><given-names>L.K.</given-names></name></person-group> (<year>2000</year>) 
<article-title>Nonlinear dimensionality reduction by locally linear embedding</article-title>. <source>Science</source>, <volume>290</volume>, <fpage>2323</fpage>–<lpage>2326</lpage>.<pub-id pub-id-type="pmid">11125150</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ryu</surname><given-names>J.Y.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Deep learning improves prediction of drug–drug and drug–food interactions</article-title>. <source>PNAS</source>, <volume>115</volume>, <fpage>E4304</fpage>–<lpage>E4311</lpage>.<pub-id pub-id-type="pmid">29666228</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shin</surname><given-names>H.-C.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning</article-title>. <source>IEEE Trans. Med. Imaging</source>, <volume>35</volume>, <fpage>1285</fpage>–<lpage>1298</lpage>.<pub-id pub-id-type="pmid">26886976</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Su</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Network embedding in biomedical data science</article-title>. <source>Brief. Bioinformatics</source>. doi: 10.1093/bib/bby117. </mixed-citation>
    </ref>
    <ref id="btz718-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Szklarczyk</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>String v10: protein–protein interaction networks, integrated over the tree of life</article-title>. <source>Nucleic Acids Res</source>., <volume>43</volume>, <fpage>D447</fpage>–<lpage>D452</lpage>.<pub-id pub-id-type="pmid">25352553</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ta</surname><given-names>C.N.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Columbia open health data, clinical concept prevalence and co-occurrence from electronic health records</article-title>. <source>Sci. Data</source>, <volume>5</volume>, <fpage>180273.</fpage><pub-id pub-id-type="pmid">30480666</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B47">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Tang</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) <chapter-title>Line: large-scale information network embedding</chapter-title> In: <source>Proceedings of the 24th International Conference on World Wide Web, Florence, Italy</source>, pp. <fpage>1067</fpage>–<lpage>1077</lpage>. 
<publisher-name>ACM</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btz718-B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tenenbaum</surname><given-names>J.B.</given-names></name></person-group><etal>et al</etal> (<year>2000</year>) 
<article-title>A global geometric framework for nonlinear dimensionality reduction</article-title>. <source>Science</source>, <volume>290</volume>, <fpage>2319</fpage>–<lpage>2323</lpage>.<pub-id pub-id-type="pmid">11125149</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tu</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Max-Margin Deepwalk: discriminative learning of network representation</article-title>. In: <source>Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, New York, NY</source>, pp. <fpage>3889</fpage>–<lpage>3895</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B50">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) <chapter-title>Structural deep network embedding</chapter-title> In: <source>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA</source>, pp. <fpage>1225</fpage>–<lpage>1234</lpage>. 
<publisher-name>ACM</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btz718-B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>D.D.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Fast prediction of protein–protein interaction sites based on extreme learning machines</article-title>. <source>Neurocomputing</source>, <volume>128</volume>, <fpage>258</fpage>–<lpage>266</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B52">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Graphgan: graph representation learning with generative adversarial nets. In: <source>Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</source>, AAAI-18, New Orleans, Louisiana, pp. 2508–251.</mixed-citation>
    </ref>
    <ref id="btz718-B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.-B.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Predicting protein–protein interactions from protein sequences by a stacked sparse autoencoder deep neural network</article-title>. <source>Mol. Biosyst</source>., <volume>13</volume>, <fpage>1336</fpage>–<lpage>1344</lpage>.<pub-id pub-id-type="pmid">28604872</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wishart</surname><given-names>D.S.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Drugbank 5.0: a major update to the drugbank database for 2018</article-title>. <source>Nucleic Acids Res</source>., <volume>46</volume>, <fpage>D1074</fpage>–<lpage>D1082</lpage>.<pub-id pub-id-type="pmid">29126136</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Drug-disease association and drug-repositioning predictions in complex diseases using causal inference—probabilistic matrix factorization</article-title>. <source>JCIM</source>, <volume>54</volume>, <fpage>2562</fpage>–<lpage>2569</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>You</surname><given-names>Z.-H.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>An improved sequence-based prediction protocol for protein-protein interactions using amino acids substitution matrix and rotation forest ensemble classifiers</article-title>. <source>Neurocomputing</source>, <volume>228</volume>, <fpage>277</fpage>–<lpage>282</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B57">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2018a</year>) 
<article-title>Network representation learning: a survey</article-title>. <source>IEEE Trans. Big Data</source>. doi: 10.1109/TBDATA.2018.2850013. </mixed-citation>
    </ref>
    <ref id="btz718-B58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name></person-group><etal>et al</etal> (<year>2018b</year>) 
<article-title>Manifold regularized matrix factorization for drug-drug interaction prediction</article-title>. <source>J. Biomed. Inform</source>., <volume>88</volume>, <fpage>90</fpage>–<lpage>97</lpage>.<pub-id pub-id-type="pmid">30445219</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B59">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name></person-group><etal>et al</etal> (<year>2018c</year>) 
<article-title>Predicting drug-disease associations by using similarity constrained matrix factorization</article-title>. <source>BMC Bioinformatics</source>, <volume>19</volume>, <fpage>233.</fpage><pub-id pub-id-type="pmid">29914348</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B60">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>W.</given-names></name></person-group><etal>et al</etal> (<year>2018d</year>) 
<article-title>Sfpel-lpi: sequence-based feature projection ensemble learning for predicting lncRNA-protein interactions</article-title>. <source>PLoS Comput. Biol</source>., <volume>14</volume>, <fpage>e1006616.</fpage><pub-id pub-id-type="pmid">30533006</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B61">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Increasing the reliability of protein–protein interaction networks via non-convex semantic embedding</article-title>. <source>Neurocomputing</source>, <volume>121</volume>, <fpage>99</fpage>–<lpage>107</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B62">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zitnik</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Leskovec</surname><given-names>J.</given-names></name></person-group> (<year>2017</year>) 
<article-title>Predicting multicellular function through multi-layer tissue networks</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>i190</fpage>–<lpage>i198</lpage>.<pub-id pub-id-type="pmid">28881986</pub-id></mixed-citation>
    </ref>
    <ref id="btz718-B63">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Žitnik</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Zupan</surname><given-names>B.</given-names></name></person-group> (<year>2014</year>) 
<article-title>Data fusion by matrix factorization</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>., <volume>37</volume>, <fpage>41</fpage>–<lpage>53</lpage>.</mixed-citation>
    </ref>
    <ref id="btz718-B64">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zitnik</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Modeling polypharmacy side effects with graph convolutional networks</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>i457</fpage>–<lpage>i466</lpage>.<pub-id pub-id-type="pmid">29949996</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
