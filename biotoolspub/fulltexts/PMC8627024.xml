<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Cheminform</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Cheminform</journal-id>
    <journal-title-group>
      <journal-title>Journal of Cheminformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1758-2946</issn>
    <publisher>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8627024</article-id>
    <article-id pub-id-type="publisher-id">570</article-id>
    <article-id pub-id-type="doi">10.1186/s13321-021-00570-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Chemical toxicity prediction based on semi-supervised learning and graph convolutional neural network</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Jiarui</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Si</surname>
          <given-names>Yain-Whar</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Un</surname>
          <given-names>Chon-Wai</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3695-7758</contrib-id>
        <name>
          <surname>Siu</surname>
          <given-names>Shirley W. I.</given-names>
        </name>
        <address>
          <email>shirleysiu@um.edu.mo</email>
          <email>shirley.siu@usj.edu.mo</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.437123.0</institution-id><institution-id institution-id-type="ISNI">0000 0004 1794 8068</institution-id><institution>Department of Computer and Information Science, </institution><institution>University of Macau, </institution></institution-wrap>Avenida da Universidade, Taipa, 999078 Macau China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.445022.5</institution-id><institution-id institution-id-type="ISNI">0000 0004 0632 6909</institution-id><institution>Present Address: Institute of Science and Environment, </institution><institution>University of Saint Joseph, </institution></institution-wrap>Rua de Londres 106, 999078 Macau, China </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.11875.3a</institution-id><institution-id institution-id-type="ISNI">0000 0001 2294 3534</institution-id><institution>School of Pharmaceutical Sciences, </institution><institution>Universiti Sains Malaysia, </institution></institution-wrap>USM, 11800 Penang, Malaysia </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>27</day>
      <month>11</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>27</day>
      <month>11</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>13</volume>
    <elocation-id>93</elocation-id>
    <history>
      <date date-type="received">
        <day>20</day>
        <month>7</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>11</day>
        <month>11</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">As safety is one of the most important properties of drugs, chemical toxicology prediction has received increasing attentions in the drug discovery research. Traditionally, researchers rely on in vitro and in vivo experiments to test the toxicity of chemical compounds. However, not only are these experiments time consuming and costly, but experiments that involve animal testing are increasingly subject to ethical concerns. While traditional machine learning (ML) methods have been used in the field with some success, the limited availability of annotated toxicity data is the major hurdle for further improving model performance. Inspired by the success of semi-supervised learning (SSL) algorithms, we propose a Graph Convolution Neural Network (GCN) to predict chemical toxicity and trained the network by the Mean Teacher (MT) SSL algorithm. Using the Tox21 data, our optimal SSL-GCN models for predicting the twelve toxicological endpoints achieve an average ROC-AUC score of 0.757 in the test set, which is a 6% improvement over GCN models trained by supervised learning and conventional ML methods. Our SSL-GCN models also exhibit superior performance when compared to models constructed using the built-in DeepChem ML methods. This study demonstrates that SSL can increase the prediction power of models by learning from unannotated data. The optimal unannotated to annotated data ratio ranges between 1:1 and 4:1. This study demonstrates the success of SSL in chemical toxicity prediction; the same technique is expected to be beneficial to other chemical property prediction tasks by utilizing existing large chemical databases. Our optimal model SSL-GCN is hosted on an online server accessible through: <ext-link ext-link-type="uri" xlink:href="https://app.cbbio.online/ssl-gcn/home">https://app.cbbio.online/ssl-gcn/home</ext-link>.</p>
      <sec>
        <title>Supplementary information</title>
        <p><bold>Supplementary information</bold> accompanies this paper at 10.1186/s13321-021-00570-8.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Chemical toxicity</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Graph convolutional neural network</kwd>
      <kwd>Semi-supervised learning</kwd>
      <kwd>Mean teacher</kwd>
      <kwd>Tox21</kwd>
      <kwd>ADMET</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004733</institution-id>
            <institution>universidade de macau</institution>
          </institution-wrap>
        </funding-source>
        <award-id>MYRG2019-00098-FST</award-id>
        <principal-award-recipient>
          <name>
            <surname>Siu</surname>
            <given-names>Shirley W. I.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">The fundamental strategy in modern drug discovery and development is to identify chemical compounds that potently and selectively modulate the functions of the target molecules to elicit a desired biological response. How to quickly locate these compounds from the vast chemical space and then determine their drug-like properties remains a major challenge [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. Traditionally, chemists and biologists perform in vitro and in vivo experiments to test the pharmacodynamics and pharmacokinetic (PD/PK) properties of selected candidates obtained from initial screening results [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>]. However, these experiments are not only very costly in terms of time and money, the experiments that involve animal testings are increasingly questionable from ethical perspectives [<xref ref-type="bibr" rid="CR6">6</xref>]. Previous studies show that it typically takes 6 to 12 years and more than 2.6 billion dollars to develop a new drug. Of this cost, about 1.1 billion dollars is for the drug development phases prior to human testing [<xref ref-type="bibr" rid="CR7">7</xref>].</p>
    <p id="Par3">Toxicity is one of the five pharmacokinetic properties (ADMET) that must be strictly ascertained before a new drug candidate is approved for clinical trials [<xref ref-type="bibr" rid="CR8">8</xref>]. On the premise that “<italic>the structure of a chemical substance implicitly determines its physical and chemical properties and reactivity, and these properties interact with biological systems to determine its biological/toxicological properties</italic>” [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>], efforts have been made to develop computational methods, often machine learning (ML) based, that attempt to relate the toxicological properties of compounds to their chemical structures. For a comprehensive review of ML-based toxicity prediction methods, the readers are referred to refs [<xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR13">13</xref>].</p>
    <p id="Par4">Graph Convolutional Neural Networks (GCN) are commonly used for tasks such as social network analysis and knowledge graph mining. Since biomolecular structures can also be represented as graphs, a variety of GCN-based biomolecular property prediction models have been developed in recent years. For example, the Weave model was proposed by Kearnes et al. in 2016 [<xref ref-type="bibr" rid="CR14">14</xref>], which was a deep learning system based on molecular graph convolutions. This model uses only the simple descriptions of atoms, bonds, and atom pairs as input data. In addition, a learnable module called Weave module, extracts and combines the features of atom and distance relationship with learnable parameters. These modules can be stacked to an arbitrary depth to allow fine-tuning of the architecture for the needs of different learning tasks. In 2017, Li et al. proposed the GraphConv-SuperNode model [<xref ref-type="bibr" rid="CR15">15</xref>]. By adding a dummy fully connected node (the super node) in each graph, this model captures and extracts graph-level representations from chemical structures, allowing it to focus on graph-level classification and regression tasks. In 2020, Wang et al. proposed a graph attention convolutional neural network (GACNN) that classified poisonous chemicals to honey bees [<xref ref-type="bibr" rid="CR16">16</xref>], which is a Graph Convolution Neural Network with undirected graph and attention mechanism. They demonstrated that the performance of their GACNN model was better than all previous models, and they also summarised important structural features that might lead to poisoning.</p>
    <p id="Par5">All of these previous studies have highlighted the advantages of using GCN-based models to predict biomolecular properties. First, the suitability of different traditional molecular descriptors for different tasks significantly affects the performance of the models [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>]. Graph-based molecular representations can circumvent this problem by preserving the structural and physicochemical information of the molecules. Second, the majority of models using graph-based techniques perform better on biomolecular property prediction tasks than conventional ML models using traditional molecular descriptors [<xref ref-type="bibr" rid="CR14">14</xref>–<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. Third, since GCN-based models can directly manipulate graph-based molecular representations, they can retain molecular structural information during prediction. This characteristics of GCN makes the interpretability of GCN-based models superior to other traditional ML models.</p>
    <p id="Par6">Based on the different training strategies, ML algorithms can be broadly classified into 4 types, namely supervised learning (SL), semi-supervised learning (SSL), unsupervised learning and reinforcement learning [<xref ref-type="bibr" rid="CR19">19</xref>]. All the prediction models we mentioned above are based on the SL algorithms which learn only from annotated datasets. However, despite enormous efforts in data curation and data sharing, the amount of labeled data falls far short of the amount of known compounds. Strategies to make use of the unannotated data such as those of SSL are expected to enhance the generalizability of prediction models.</p>
    <p id="Par7">Therefore, inspired by the success of GCN and the needs for improving chemical toxicity prediction confronted with limited data, we designed a learning system that hybridizes graph convolutional neural network (GCN) and SSL to predict the toxicity of chemical compounds. Here, we used chemical data from the Tox21 dataset as annotated data and collected compounds from other datasets as unannotated data. First, the molecular features encoded in GCN were defined, then experiments were performed to investigate the influence of SSL on the predictivity of the models. Moreover, the performances of the SSL models with varying unannotated data ratios were compared, which showed that SSL has a positive influence on the prediction performance of GCN models.</p>
    <p id="Par8">This paper is organized as follows. The theoretical foundation of GCN and the mean teacher SSL algorithm are presented in the Material and Method section. The dataset, model, and validation technique are then described. The Results section contains comparative study of the traditional ML, SL-GCN, and SSL-GCN models performances. The impact of various unannotated data ratios was also investigated. Finally, SSL-GCN was compared to existing DeepChem methods for toxicity prediction.</p>
  </sec>
  <sec id="Sec2">
    <title>Material and method</title>
    <sec id="Sec3">
      <title>Graph convolutional neural network (GCN)</title>
      <p id="Par9">Traditional convolutional neural networks (CNN) can extract features from Euclidean or grid structure data, such as images and text. But for non-Euclidean data like social networks, knowledge graphs, or chemical structures, due to its irregular data topology, CNN cannot directly operate on them [<xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR21">21</xref>]. A solution for machine learning on non-Euclidean data is Graph Convolutional Neural Network (GCN) [<xref ref-type="bibr" rid="CR22">22</xref>]. GCN has been widely used in solving computer science problems such as social network analysis [<xref ref-type="bibr" rid="CR23">23</xref>], natural language processing [<xref ref-type="bibr" rid="CR24">24</xref>, <xref ref-type="bibr" rid="CR25">25</xref>], and recommendation system [<xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR27">27</xref>], and also chemistry problems such as molecular properties prediction [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR20">20</xref>]. For the latter, each molecule is described as an undirected graph where atoms are represented as nodes and covalent chemical bonds are represented as edges. The basic idea of graph convolution is to apply a learnable function on each node and its neighbors, gradually merging information from distant atoms through the connecting edges, and ultimately extracting the atom-type and connectivity patterns in the molecule. In this work, we used off-the-shelf GCN method that was proposed by Kipf et al. in 2017 [<xref ref-type="bibr" rid="CR28">28</xref>]. The layer-wise propagation function of this approach is defined in the following equations in terms of matrix calculation:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned}&amp;{\tilde{A}} = A + I \end{aligned}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13321_2021_570_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned}&amp;H^{(l+1)}=\sigma ({\tilde{D}}^{-\frac{1}{2}}{\tilde{A}}{\tilde{D}}^{-\frac{1}{2}}H^{(l)}W^{(l)}) \end{aligned}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13321_2021_570_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>These equations can be denoted as <inline-formula id="IEq1"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f(H^{(l)}, A)$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq1.gif"/></alternatives></inline-formula>. <inline-formula id="IEq2"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{A}}$$\end{document}</tex-math><mml:math id="M8"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq2.gif"/></alternatives></inline-formula> represents the adjacency matrix <italic>A</italic> of an undirected graph <inline-formula id="IEq3"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal {G}}$$\end{document}</tex-math><mml:math id="M10"><mml:mi mathvariant="script">G</mml:mi></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq3.gif"/></alternatives></inline-formula> with added self-connections <italic>I</italic>. <inline-formula id="IEq4"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{D}}$$\end{document}</tex-math><mml:math id="M12"><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq4.gif"/></alternatives></inline-formula> is the degree matrix of <inline-formula id="IEq5"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{A}}$$\end{document}</tex-math><mml:math id="M14"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq5.gif"/></alternatives></inline-formula>. <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{(l)}\in {\mathbb {R}}^{N\times D}$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq6.gif"/></alternatives></inline-formula> represents the nodes signal matrix (features) generated by the <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l{\rm th}$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mi>l</mml:mi><mml:mi mathvariant="normal">th</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq7.gif"/></alternatives></inline-formula> layer, where <italic>N</italic> and <italic>D</italic> denote the number of nodes in this graph and the dimension of each node’s signal matrix respectively. <inline-formula id="IEq8"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W^{(l)}$$\end{document}</tex-math><mml:math id="M20"><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq8.gif"/></alternatives></inline-formula> is the layer-specific learnable weight matrix of the <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l^{th}$$\end{document}</tex-math><mml:math id="M22"><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="italic">th</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq9.gif"/></alternatives></inline-formula> layer. <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M24"><mml:mi>σ</mml:mi></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq10.gif"/></alternatives></inline-formula> denotes a non-linear activation function [<xref ref-type="bibr" rid="CR28">28</xref>].</p>
      <p id="Par10">To facilitate implementation, the previous equations can be represented as the following:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} h_{i}^{(l+1)} = ReLU\left( b^{(l)}+\sum _{j\in {\mathcal {N}}(i)}\frac{1}{\sqrt{\left| {\mathcal {N}}(i) \right| } \sqrt{\left| {\mathcal {N}}(j) \right| }}h_{j}^{(l)}W^{(l)} \right) \end{aligned}$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced close=")" open="("><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mfenced close="|" open="|"><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mfenced></mml:msqrt><mml:msqrt><mml:mfenced close="|" open="|"><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mfenced></mml:msqrt></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13321_2021_570_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq11"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal {N}}(i)$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq11.gif"/></alternatives></inline-formula> is the set of neighbors of the node <italic>i</italic>. <inline-formula id="IEq12"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W^{(l)}$$\end{document}</tex-math><mml:math id="M30"><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq12.gif"/></alternatives></inline-formula> represents the layer-specific learnable weight matrix of the <inline-formula id="IEq13"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l{th}$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="italic">th</mml:mi></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq13.gif"/></alternatives></inline-formula> layer, <inline-formula id="IEq14"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{j}^{(l)}$$\end{document}</tex-math><mml:math id="M34"><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq14.gif"/></alternatives></inline-formula> is the signal matrix (features) of each neighbor node <italic>j</italic> around <italic>i</italic>, and <inline-formula id="IEq15"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b^{(l)}$$\end{document}</tex-math><mml:math id="M36"><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq15.gif"/></alternatives></inline-formula> is the bias value of the <inline-formula id="IEq16"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l^{th}$$\end{document}</tex-math><mml:math id="M38"><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mi mathvariant="italic">th</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq16.gif"/></alternatives></inline-formula> layer. Therefore, the signal of each node in the next layer is determined by the weighted sum of signals in each node of the current layer and the signals of its adjacent nodes of the same layer. All signals are nonlinearly transformed using the Rectified Linear Unit (ReLU) function, <inline-formula id="IEq17"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ReLU(x)=max(0,x)$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq17.gif"/></alternatives></inline-formula>.</p>
    </sec>
    <sec id="Sec4">
      <title>Semi-supervised learning (SSL)</title>
      <p id="Par11">The basic idea of machine learning (ML) is to reproduce the human learning process by computer algorithms. Most ML algorithms can be classified into four types [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR29">29</xref>]: supervised learning, unsupervised learning, semi-supervised learning and reinforcement learning. The most commonly used method is supervised learning. It derives knowledge from training data with fully annotated labels [<xref ref-type="bibr" rid="CR30">30</xref>]. However, acquiring accurate annotated data is sometimes difficult for certain tasks such as chemical compound properties prediction. On one hand, there are tens of thousands known chemical compounds that exist in nature, and even more artificial chemical compounds are being produced every year. On the other hand, each annotation requires labor-intensive and expensive procedure from compound synthesis to measurement. Consequently, a significant amount of molecules are not properly labelled while some labels may subject to experimental errors. To learn from incompletely annotated data, semi-supervised learning method is more suitable [<xref ref-type="bibr" rid="CR31">31</xref>].</p>
      <p id="Par12">In SSL, it is assumed that the label function is smooth in high-density areas, so data points located in the same area should share the same label. Based on this smoothness assumption, even unlabelled data can be exploited in the learning process. Here, the main idea is to build classification models that are robust to local perturbations in the input data. When the input data is perturbed with a small amount of noise, the prediction results for the perturbed data and original data should be similar [<xref ref-type="bibr" rid="CR32">32</xref>]. Since this consistency in predictions does not depend on the data labels, therefore unlabelled data can be exploited in the training process to enhance the prediction consistency of the model.</p>
      <p id="Par13">Earlier SSL models that used this consistency regularization, such as the <inline-formula id="IEq18"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Gamma$$\end{document}</tex-math><mml:math id="M42"><mml:mi mathvariant="normal">Γ</mml:mi></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq18.gif"/></alternatives></inline-formula>-model [<xref ref-type="bibr" rid="CR33">33</xref>], assigned two roles (teacher and student) to the same model. With the role of student, the model learns based on labeled data. With the teacher role, the model generates targets for unlabeled data, which are then used by itself as a student for consistency learning. However, at the beginning of training, the generated targets for unlabeled data are most likely incorrect. The consistency cost for unlabeled data outweighs the classification cost for labeled data at the beginning of training, so the model cannot learn any new information from the training process [<xref ref-type="bibr" rid="CR34">34</xref>]. One way to solve this problem is to carefully select or update the teacher model instead of sharing the same model with the student model. Following this idea, the <inline-formula id="IEq19"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Pi$$\end{document}</tex-math><mml:math id="M44"><mml:mi mathvariant="normal">Π</mml:mi></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq19.gif"/></alternatives></inline-formula>-model and Temporal Ensembling model were proposed in 2017 [<xref ref-type="bibr" rid="CR35">35</xref>].</p>
      <p id="Par14">In each training epoch of the <inline-formula id="IEq20"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Pi$$\end{document}</tex-math><mml:math id="M46"><mml:mi mathvariant="normal">Π</mml:mi></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq20.gif"/></alternatives></inline-formula>-model, the same unlabeled data are predicted twice with different roles (student and teacher). Since data perturbations and dropout methods are implemented in each prediction process, two prediction processes will give slightly different predictions for the same data. The goal of the <inline-formula id="IEq21"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Pi$$\end{document}</tex-math><mml:math id="M48"><mml:mi mathvariant="normal">Π</mml:mi></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq21.gif"/></alternatives></inline-formula>-model during the training process is to make two predictions for the same unlabeled data as consistent as possible. Their experiments show that this method can eventually make the teacher model make accurate targets for unlabeled data [<xref ref-type="bibr" rid="CR35">35</xref>]. However, the computational cost of this model is too high. The Temporal Ensembling model improves on the <inline-formula id="IEq22"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Pi$$\end{document}</tex-math><mml:math id="M50"><mml:mi mathvariant="normal">Π</mml:mi></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq22.gif"/></alternatives></inline-formula>-model by making predictions only once per training epoch for unlabeled data, reducing the number of predictions by half and nearly doubling the speed. To calculate the consistency cost in the Temporal Ensembling model, the target of unlabeled data is generated by the exponential moving average (EMA) of the predictions for unlabeled data in previous training epochs. However, since each target is updated only once per epoch, the updating speed is too slow, which still limits the training speed of Temporal Ensembling model [<xref ref-type="bibr" rid="CR34">34</xref>].</p>
      <p id="Par15">In this study, we implemented the SSL algorithm proposed by Tarvainen and Valpola, called Mean Teacher (MT) [<xref ref-type="bibr" rid="CR34">34</xref>]. To circumvent the limitations of the Temporal Ensembling model, the MT algorithm updates the internal weights of the model through the EMA strategy at each training step to produce a more accurate model, rather than updating the targets of the unlabeled data at each training epoch. During training process, this algorithm requires two models with the same architecture, namely the student model and the teacher model. In each training step, the student model updates its internal weights based on the classification loss on the labeled data and the consistency loss between the two models on the unlabeled data. After the student model is updated, the teacher model is also updated using EMA strategy defined in Equation <xref rid="Equ4" ref-type="">4</xref> [<xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR34">34</xref>]. Previous studies have demonstrated that this kind of self-ensembling framework could bring improvements to classification models [<xref ref-type="bibr" rid="CR34">34</xref>, <xref ref-type="bibr" rid="CR35">35</xref>]. The pseudo code of this algorithm is shown below:</p>
      <graphic position="anchor" xlink:href="13321_2021_570_Figa_HTML" id="MO4"/>
      <p id="Par16"><inline-formula id="IEq23"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g(\cdot )$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq23.gif"/></alternatives></inline-formula> denotes the data perturbation function, <inline-formula id="IEq24"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m_{s}(\cdot )$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq24.gif"/></alternatives></inline-formula> and <inline-formula id="IEq25"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m_{t}(\cdot )$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq25.gif"/></alternatives></inline-formula> represent the student and teacher models respectively, <inline-formula id="IEq26"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta _s^i$$\end{document}</tex-math><mml:math id="M58"><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq26.gif"/></alternatives></inline-formula> and <inline-formula id="IEq27"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta _t^i$$\end{document}</tex-math><mml:math id="M60"><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq27.gif"/></alternatives></inline-formula> represent the internal weights in the training step <italic>i</italic>, <italic>z</italic> and <inline-formula id="IEq28"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{z}}$$\end{document}</tex-math><mml:math id="M62"><mml:mover accent="true"><mml:mi>z</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq28.gif"/></alternatives></inline-formula> are the generated classification probabilities. <inline-formula id="IEq29"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Loss_{cls}$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="italic">cls</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq29.gif"/></alternatives></inline-formula> and <inline-formula id="IEq30"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Loss_{con}$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="italic">con</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq30.gif"/></alternatives></inline-formula> represent classification loss and consistency loss. <inline-formula id="IEq31"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_{i}$$\end{document}</tex-math><mml:math id="M68"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq31.gif"/></alternatives></inline-formula> denotes the consistency loss coefficient in the training step <italic>i</italic>. This consistency loss coefficient varies with the training steps. It is defined as the function <inline-formula id="IEq32"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e^{-5(1-t)^2}$$\end{document}</tex-math><mml:math id="M70"><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq32.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq33"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t\in \{0,1\}$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq33.gif"/></alternatives></inline-formula>, represents scaled number of training step [<xref ref-type="bibr" rid="CR34">34</xref>]. <inline-formula id="IEq34"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Update(\cdot )$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:mi>U</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq34.gif"/></alternatives></inline-formula> is the process of updating the internal weights of the model through backpropagation.</p>
      <p id="Par17"><inline-formula id="IEq35"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$EMA(\cdot )$$\end{document}</tex-math><mml:math id="M76"><mml:mrow><mml:mi>E</mml:mi><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq35.gif"/></alternatives></inline-formula> is the process of updating the weights in <inline-formula id="IEq36"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m_{t}$$\end{document}</tex-math><mml:math id="M78"><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq36.gif"/></alternatives></inline-formula> by applying the Exponential Moving Average (EMA) of weights in <inline-formula id="IEq37"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m_{s}$$\end{document}</tex-math><mml:math id="M80"><mml:msub><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq37.gif"/></alternatives></inline-formula> where <inline-formula id="IEq38"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha _{i}$$\end{document}</tex-math><mml:math id="M82"><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq38.gif"/></alternatives></inline-formula> is the smoothing coefficient. The following equation defines this process mathematically:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \theta _{t}^{i} = \alpha _{i}\theta _{t}^{i-1} +(1-\alpha _{i})\theta _{s}^{i} \end{aligned}$$\end{document}</tex-math><mml:math id="M84" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13321_2021_570_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>In our implementation, we applied the Gaussian noise <italic>g</italic>(<italic>x</italic>) as the data perturbation method using the same distribution for both <inline-formula id="IEq39"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m_{s}(\cdot )$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq39.gif"/></alternatives></inline-formula> and <inline-formula id="IEq40"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m_{t}(\cdot )$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq40.gif"/></alternatives></inline-formula>. The cross entropy loss function and Mean Squared Error (MSE) are used to compute the classification loss and consistency loss, respectively. The GCN network is optimized using the Adam optimizer [<xref ref-type="bibr" rid="CR36">36</xref>], which is the optimizer chosen in the original implementation of MT [<xref ref-type="bibr" rid="CR34">34</xref>]. Although both the well-trained teacher model and the student model can be used for prediction, previous studies have demonstrated that the teacher model is more accurate than the student model [<xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR34">34</xref>]. Therefore, the teacher model is used as the final classification model.
</p>
    </sec>
    <sec id="Sec5">
      <title>Datasets</title>
      <p id="Par18">For semi-supervised learning, both labeled (compounds with toxicity information) and unlabeled (compounds without toxicity information) data are required. In this study, the Tox21 dataset from MoleculeNet [<xref ref-type="bibr" rid="CR37">37</xref>] is used as the labeled data. The Tox21 challenge is a community-wide compound toxicity prediction competition in 2014. Since then, the Tox21 dataset has been widely used as the benchmark dataset for evaluating toxicity prediction models. It consists of 12 endpoints, including 7 nuclear receptor signals (NR-AR, NR-AhR, NR-AR-LBD, NR-ER, NR-ER-LBD, NR-Aromatase, NR-PPAR-gamma) and 5 stress response indicators (SR-ARE, SR-ATAD5, SR-HSE, SR-MMP, SR-p53). In this dataset, each compound is expressed in Simplified Molecular Input Line Entry Specification (SMILES) format and the binary labels indicate whether the compound is toxic to a specific toxicological endpoint. In total, the Tox21 dataset include 7831 compounds and 12 different endpoints. It should be noted that not all compounds have all endpoint labels; the missing endpoint label means that the toxicology effect toward this endpoint is unknown. For unlabeled data, other chemical compound datasets were sought from the MoleculeNet website, including ClinTox, SIDER, ToxCast, and HIV datasets [<xref ref-type="bibr" rid="CR37">37</xref>]. All the label information in these datasets have been removed. In addition, duplicate molecules between these datasets and the Tox21 dataset have also been removed. In total, 50527 compounds were used as unlabeled data. Table <xref rid="Tab1" ref-type="table">1</xref> shows the details of the datasets used in this study.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The labeled compound toxicity datasets for 12 toxicological endpoints and the unlabeled dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Endpoint</th><th align="left">Compounds(labeled)</th><th align="left">Training set</th><th align="left">Validation set</th><th align="left">Test set</th></tr></thead><tbody><tr><td align="left">NR-AhR</td><td align="left">6549</td><td align="left">5239</td><td align="left">655</td><td align="left">655</td></tr><tr><td align="left">NR-AR-LBD</td><td align="left">6758</td><td align="left">5406</td><td align="left">676</td><td align="left">676</td></tr><tr><td align="left">NR-AR</td><td align="left">7265</td><td align="left">5812</td><td align="left">726</td><td align="left">727</td></tr><tr><td align="left">NR-Aromatase</td><td align="left">5821</td><td align="left">4656</td><td align="left">582</td><td align="left">583</td></tr><tr><td align="left">NR-ER-LBD</td><td align="left">6955</td><td align="left">5564</td><td align="left">695</td><td align="left">696</td></tr><tr><td align="left">NR-ER</td><td align="left">6193</td><td align="left">4954</td><td align="left">619</td><td align="left">620</td></tr><tr><td align="left">NR-PPAR-gamma</td><td align="left">6450</td><td align="left">5160</td><td align="left">645</td><td align="left">645</td></tr><tr><td align="left">SR-ARE</td><td align="left">5832</td><td align="left">4665</td><td align="left">583</td><td align="left">584</td></tr><tr><td align="left">SR-ATAD5</td><td align="left">7072</td><td align="left">5657</td><td align="left">707</td><td align="left">708</td></tr><tr><td align="left">SR-HSE</td><td align="left">6467</td><td align="left">5173</td><td align="left">647</td><td align="left">647</td></tr><tr><td align="left">SR-MMP</td><td align="left">5810</td><td align="left">4648</td><td align="left">581</td><td align="left">581</td></tr><tr><td align="left">SR-p53</td><td align="left">6774</td><td align="left">5419</td><td align="left">677</td><td align="left">678</td></tr><tr><td align="left">Unlabeled data</td><td align="left">50527</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr></tbody></table></table-wrap></p>
      <p id="Par19">For each labeled dataset, we follow the conventional dataset splitting rule with the splitting ratios of 0.8:0.1:0.1 to divide the dataset into training, validation and test sets. Training set is used for the training process, validation set for the hyperparameter tuning process and the test set is to measure the generalization performance. The most commonly used splitting method is random splitting. However, it is not always suitable for molecular data because random splitting cannot guarantee that the training and test sets contain diverse and representative data samples [<xref ref-type="bibr" rid="CR37">37</xref>, <xref ref-type="bibr" rid="CR38">38</xref>]. In order to overcome the problem of data bias, we adopted a scaffold splitting method. It splits the dataset according to the two-dimensional structural framework of the molecule [<xref ref-type="bibr" rid="CR39">39</xref>, <xref ref-type="bibr" rid="CR40">40</xref>] and then assign structurally different molecules into different subsets [<xref ref-type="bibr" rid="CR37">37</xref>]. In this way, both the training set and the test set contain a good proportion of data samples scattered in the molecular space of the dataset, and we can expect that the performance of the model measured on this test set is closer to its actual performance on new data.</p>
      <p id="Par20">As mentioned above, an undirected graph can be described by two matrices, namely the signal (feature) matrix <italic>H</italic> and the adjacency matrix <italic>A</italic>. In this study, we used the molecule-graph conversion tool from Deep Graph Library (DGL) [<xref ref-type="bibr" rid="CR41">41</xref>] to convert molecules from SMILES to graphs. For each molecule, the connectivity of atoms is stored in the adjacency matrix and the physicochemical properties of each atom (node features) are encoded into a feature matrix in binary or numerical form. Since the DGL conversion tool provides eight default atom features, as listed in Table <xref rid="Tab2" ref-type="table">2</xref>, the dimension of each node feature matrix is <inline-formula id="IEq41"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1\times 74$$\end{document}</tex-math><mml:math id="M90"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>74</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq41.gif"/></alternatives></inline-formula>. Therefore, for a molecule with <italic>N</italic> atoms, the conversion will generate one adjacency matrix of dimension <inline-formula id="IEq42"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N\times N$$\end{document}</tex-math><mml:math id="M92"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq42.gif"/></alternatives></inline-formula> and one feature matrix of dimension <inline-formula id="IEq43"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N\times 74$$\end{document}</tex-math><mml:math id="M94"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>74</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq43.gif"/></alternatives></inline-formula>. This graph conversion process is depicted in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. After this step, the graph-based molecular data can be learned by the graph convolutional neural network.<fig id="Fig1"><label>Fig. 1</label><caption><p>The SSL-GCN model for compound toxicity prediction. Molecular compounds are converted into graphs of nodes and connections. The GCN model architecture is composed of two stacked layers of graph convolutional layer, dropout, and batch normalization layer. All signals are summarized by the max pooling layer and fed into the multilayer perceptron network to generate the final output. The teacher and student GCN models are updated using the MT algorithm</p></caption><graphic xlink:href="13321_2021_570_Fig1_HTML" id="MO6"/></fig><table-wrap id="Tab2"><label>Table 2</label><caption><p>Atom features provided by the molecule-graph conversion tool from Deep Graph Library</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">No.</th><th align="left">Description</th><th align="left">No. of bits</th><th align="left">Form</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">One hot encoding of the atom type</td><td align="left">1-43</td><td align="left">Binary</td></tr><tr><td align="left">2</td><td align="left">One hot encoding of the atom degree</td><td align="left">44-54</td><td align="left">Binary</td></tr><tr><td align="left">3</td><td align="left">One hot encoding of the number of implicit Hs on the atom</td><td align="left">55-61</td><td align="left">Binary</td></tr><tr><td align="left">4</td><td align="left">Formal charge of the atom</td><td align="left">62</td><td align="left">Numerical</td></tr><tr><td align="left">5</td><td align="left">Number of radical electrons of the atom</td><td align="left">63</td><td align="left">Numerical</td></tr><tr><td align="left">6</td><td align="left">One hot encoding of the atom hybridization</td><td align="left">64-68</td><td align="left">Binary</td></tr><tr><td align="left">7</td><td align="left">Whether the atom is aromatic</td><td align="left">69</td><td align="left">Numerical</td></tr><tr><td align="left">8</td><td align="left">One hot encoding of the number of total Hs on the atom</td><td align="left">70-74</td><td align="left">Binary</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec6">
      <title>Model architecture and hyperparameters selection</title>
      <p id="Par21">The architecture of our GCN model consists of two parts, an encoder and a classifier. The encoder extracts and updates node representations through several graph convolutional layers (Graph Conv). In addition, there is a dropout layer after each Graph Conv layer to provide additional noise to the molecular representations [<xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR34">34</xref>]. The last layer of the encoder merges all nodes features into a tensor by using max-pooling and weighted sum operations. This tensor is the learned representation of the input molecule. The classifier is to compute the final prediction. We used the classifier provided in DGL [<xref ref-type="bibr" rid="CR41">41</xref>] which contains two layers perceptron (MLP) with a dropout layer and a batch normalization layer.</p>
      <p id="Par22">In order to select the best hyperparameters for these models, Bayesian optimization algorithm [<xref ref-type="bibr" rid="CR42">42</xref>] is used to search the hyperparameter space, and the maximum number of trials is 32. In each trial, the algorithm selects a set of candidate hyperparameters and initializes the model. Then, model training and validation are carried out iteratively until the early stopping condition of 30 epochs is met. After all trials are completed, a set of candidate hyperparameters with the best validation metric (ROC-AUC) is selected as the default hyperparameters for the following experiments.</p>
      <p id="Par23">Since the toxicity dataset is highly imbalanced, with an average toxic/non-toxic data ratio of about 1:17, the area under the Receiver Characteristic Operator curve (ROC-AUC) is used as the main metric in the hyperparameter selection process (practically, to decide for early stopping) and the final model evaluation. The hyperparameters with the best validation performance are selected to construct the optimal toxicity prediction models. Finally, the generalization performance of these models are estimated using the test set.</p>
    </sec>
    <sec id="Sec7">
      <title>Implementation detail</title>
      <p id="Par24">In this study, all implementations and experiments are carried out in an environment with following libraries/software: Python 3.7.9, Anaconda 4.7.10, Scikit-learn 0.23.2, RDKit v2018.09.3.0. We used Pytorch 1.7.0 with CUDA 10.0 as the basic machine learning framework. The GCN model is implemented using DGL 0.5.6 and its supplementary package DGL-LifeSci 0.2.6 [<xref ref-type="bibr" rid="CR41">41</xref>] (available on GitHub, DGL [<xref ref-type="bibr" rid="CR43">43</xref>], DGL-LifeSci [<xref ref-type="bibr" rid="CR44">44</xref>]). The Bayesian Optimization process for hyperparameter selection is implemented using Hyperopt 0.2.5 [<xref ref-type="bibr" rid="CR42">42</xref>] (available on GitHub [<xref ref-type="bibr" rid="CR45">45</xref>]). We also used DeepChem 2.5.0 [<xref ref-type="bibr" rid="CR46">46</xref>] to generate the benchmark scores of other state-of-the-art models on the Tox21 dataset (available on GitHub [<xref ref-type="bibr" rid="CR47">47</xref>]). The original source code for the Mean Teacher(MT) algorithm [<xref ref-type="bibr" rid="CR34">34</xref>] can be accessed via its GitHub repository [<xref ref-type="bibr" rid="CR48">48</xref>].</p>
    </sec>
  </sec>
  <sec id="Sec8">
    <title>Results</title>
    <p id="Par25">All experiments were repeated five times to observe the variability of the results and obtain an accurate measure of model performance through the average ROC-AUC score. The complete record of all experiments can be found in the Additional file <xref rid="MOESM1" ref-type="media">1</xref>.</p>
    <sec id="Sec9">
      <title>Performance of conventional machine learning (ML) methods</title>
      <p id="Par26">To establish the baseline performance, several commonly used ML algorithms, namely K-Nearest Neighbor (KNN), Neural Network (NN), Random Forest (RF), Support Vector Machine (SVM) and eXtreme Gradient Boosting (XGBoost) were tested. The compounds were encoded using the Extended Connectivity Fingerprints (ECFP4), which is a circular topological fingerprint designed for molecular characterization, similarity searching, and structure-activity modeling [<xref ref-type="bibr" rid="CR49">49</xref>]. The encoding was generated using the RDKit library. In total, 60 different ML models (12 prediction tasks <inline-formula id="IEq44"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M96"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq44.gif"/></alternatives></inline-formula> 5 types of ML algorithms) were trained and optimized using the training and validation sets. Subsequently, the optimal models were tested on the test set. The test performance of these conventional models on the 12 toxicity prediction tasks are presented in Table <xref rid="Tab3" ref-type="table">3</xref>. Each experiment was repeated 5 times; the average ROC-AUC score and the standard deviation (std) were reported. In all prediction tasks, the ROC-AUC scores range between 0.5127 and 0.8287. In certain cases (KNN, SVM, and XGBoost), we observed that the same optimal models were obtained in all replicate experiments such that the ROC-AUC scores are the same (std = 0). Overall, RF, XGBoost, and SVM generated the best models for 5, 4, 3 of the prediction tasks, respectively. The average ROC-AUC score of the best performing conventional ML models of all tasks is 0.71.<table-wrap id="Tab3"><label>Table 3</label><caption><p>The average test performance of conventional ML models on the 12 prediction tasks in 5 repeated experiments</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Tasks</th><th align="left" colspan="2">KNN</th><th align="left" colspan="2">NN</th><th align="left" colspan="2">RF</th><th align="left" colspan="2">SVM</th><th align="left" colspan="2">XGBoost</th></tr><tr><th align="left">AUC</th><th align="left">Std.</th><th align="left">AUC</th><th align="left">Std.</th><th align="left">AUC</th><th align="left">Std.</th><th align="left">AUC</th><th align="left">Std.</th><th align="left">AUC</th><th align="left">Std.</th></tr></thead><tbody><tr><td align="left">NR-AR-LBD</td><td align="left">0.6955</td><td align="left">–</td><td align="left">0.6671</td><td align="left">0.0244</td><td align="left"><bold>0.7323</bold></td><td align="left">0.0267</td><td align="left">0.6795</td><td align="left">–</td><td align="left">0.6784</td><td align="left">–</td></tr><tr><td align="left">NR-AR</td><td align="left">0.6527</td><td align="left">–</td><td align="left">0.6806</td><td align="left">0.0088</td><td align="left">0.6836</td><td align="left">0.0266</td><td align="left"><bold>0.7193</bold></td><td align="left">–</td><td align="left">0.6818</td><td align="left">–</td></tr><tr><td align="left">NR-AhR</td><td align="left">0.7639</td><td align="left">–</td><td align="left">0.7628</td><td align="left">0.0177</td><td align="left">0.8243</td><td align="left">0.0074</td><td align="left">0.7794</td><td align="left">–</td><td align="left"><bold>0.8287</bold></td><td align="left">–</td></tr><tr><td align="left">NR-Aromatase</td><td align="left">0.5576</td><td align="left">–</td><td align="left">0.5127</td><td align="left">0.0772</td><td align="left">0.6900</td><td align="left">0.0092</td><td align="left">0.6873</td><td align="left">–</td><td align="left"><bold>0.7106</bold></td><td align="left">–</td></tr><tr><td align="left">NR-ER-LBD</td><td align="left">0.6191</td><td align="left">–</td><td align="left">0.5387</td><td align="left">0.1171</td><td align="left">0.6169</td><td align="left">0.0300</td><td align="left">0.6078</td><td align="left">–</td><td align="left"><bold>0.6250</bold></td><td align="left">–</td></tr><tr><td align="left">NR-ER</td><td align="left">0.6597</td><td align="left">–</td><td align="left">0.6549</td><td align="left">0.0162</td><td align="left">0.6316</td><td align="left">0.0080</td><td align="left">0.6126</td><td align="left">–</td><td align="left"><bold>0.6745</bold></td><td align="left">–</td></tr><tr><td align="left">NR-PPAR-gamma</td><td align="left">0.6182</td><td align="left">–</td><td align="left">0.5558</td><td align="left">0.0736</td><td align="left"><bold>0.7135</bold></td><td align="left">0.0258</td><td align="left">0.6454</td><td align="left">–</td><td align="left">0.6414</td><td align="left">–</td></tr><tr><td align="left">SR-ARE</td><td align="left">0.6366</td><td align="left">–</td><td align="left">0.5656</td><td align="left">0.0251</td><td align="left">0.6603</td><td align="left">0.0018</td><td align="left"><bold>0.6843</bold></td><td align="left">–</td><td align="left">0.6640</td><td align="left">–</td></tr><tr><td align="left">SR-ATAD5</td><td align="left">0.5866</td><td align="left">–</td><td align="left">0.6240</td><td align="left">0.0537</td><td align="left"><bold>0.6928</bold></td><td align="left">0.0189</td><td align="left">0.6546</td><td align="left">–</td><td align="left">0.6841</td><td align="left">–</td></tr><tr><td align="left">SR-HSE</td><td align="left">0.6574</td><td align="left">–</td><td align="left">0.6143</td><td align="left">0.0222</td><td align="left">0.6852</td><td align="left">0.0131</td><td align="left"><bold>0.6858</bold></td><td align="left">–</td><td align="left">0.6647</td><td align="left">–</td></tr><tr><td align="left">SR-MMP</td><td align="left">0.7057</td><td align="left">–</td><td align="left">0.6551</td><td align="left">0.0612</td><td align="left"><bold>0.7818</bold></td><td align="left">0.0065</td><td align="left">0.7794</td><td align="left">–</td><td align="left">0.7656</td><td align="left">–</td></tr><tr><td align="left">SR-p53</td><td align="left">0.6778</td><td align="left">–</td><td align="left">0.5963</td><td align="left">0.0075</td><td align="left"><bold>0.7263</bold></td><td align="left">0.0130</td><td align="left">0.7051</td><td align="left">–</td><td align="left">0.6942</td><td align="left">–</td></tr></tbody></table><table-wrap-foot><p>The bold number denotes the best result among all conventional ML models in the corresponding task</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec10">
      <title>Performance of supervised learning GCN (SL-GCN)</title>
      <p id="Par27">Having established the baseline performance of the traditional ML models in toxicity prediction, we went on to test the GCN models for the 12 prediction tasks. Similar to other ML models above, the GCN models were trained using supervised learning and optimized by the Bayesian optimization algorithm, hence the name SL-GCN. In Fig. <xref rid="Fig2" ref-type="fig">2</xref>, the ROC curves of the SL-GCN models on the test set prediction are plotted against other ML models, and the 5-repeated average of the ROC-AUC scores are tabulated in Table <xref rid="Tab4" ref-type="table">4</xref>. The results show that, while the SL-GCN models perform similarly to the best conventional ML models in the majority of the twelve toxicity prediction tasks, they improve in four of the tasks, including NR-ER, SR-ARE, SR-HSE, and SR-MMP, while they perform worse in three of the tasks, including NR-AR-LBD, NR-PPAR-gamma, SR-p53. <fig id="Fig2"><label>Fig. 2</label><caption><p>ROC curves of conventional ML models and SL-GCN models. The comparison of ROC curves between conventional ML models (black line) and SL-GCN models (red line) on 12 toxicity prediction tasks. Additional information of the ROC curves are provided in the Additional file <xref rid="MOESM1" ref-type="media">1</xref></p></caption><graphic xlink:href="13321_2021_570_Fig2_HTML" id="MO7"/></fig><table-wrap id="Tab4"><label>Table 4</label><caption><p>The average test performance of SSL-GCN models with various unlabeled data ratio (<inline-formula id="IEq45"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_{u}$$\end{document}</tex-math><mml:math id="M98"><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq45.gif"/></alternatives></inline-formula> in brackets) on the 12 prediction tasks in 5 repeated experiments. For comparison, the results of the SL-GCN models are shown</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Tasks</th><th align="left" colspan="2">SL-GCN</th><th align="left" colspan="2">SSL-GCN (0.5)</th><th align="left" colspan="2">SSL-GCN (1.0)</th></tr><tr><th align="left">AUC</th><th align="left">Std.</th><th align="left">AUC</th><th align="left">Std.</th><th align="left">AUC</th><th align="left">Std.</th></tr></thead><tbody><tr><td align="left">NR-AR-LBD</td><td align="left">0.6783</td><td align="left">0.0269</td><td align="left">0.7417</td><td align="left">0.0105</td><td align="left">0.7333</td><td align="left">0.0401</td></tr><tr><td align="left">NR-AR</td><td align="left">0.7157</td><td align="left">0.0367</td><td align="left">0.7550</td><td align="left">0.0483</td><td align="left">0.7858</td><td align="left">0.0357</td></tr><tr><td align="left">NR-AhR</td><td align="left">0.8260</td><td align="left">0.0055</td><td align="left">0.8161</td><td align="left">0.0121</td><td align="left">0.8295</td><td align="left">0.0129</td></tr><tr><td align="left">NR-Aromatase</td><td align="left">0.7092</td><td align="left">0.0167</td><td align="left">0.7202</td><td align="left">0.0057</td><td align="left">0.7306</td><td align="left">0.0156</td></tr><tr><td align="left">NR-ER-LBD</td><td align="left">0.6340</td><td align="left">0.0161</td><td align="left">0.6623</td><td align="left">0.0330</td><td align="left">0.6794</td><td align="left">0.0411</td></tr><tr><td align="left">NR-ER</td><td align="left">0.6899</td><td align="left">0.0160</td><td align="left"><bold>0.7188</bold></td><td align="left">0.0196</td><td align="left">0.7114</td><td align="left">0.0179</td></tr><tr><td align="left">NR-PPAR-gamma</td><td align="left">0.6753</td><td align="left">0.0278</td><td align="left">0.7267</td><td align="left">0.0210</td><td align="left"><bold>0.7614</bold></td><td align="left">0.0212</td></tr><tr><td align="left">SR-ARE</td><td align="left">0.7134</td><td align="left">0.0137</td><td align="left">0.7241</td><td align="left">0.0065</td><td align="left">0.7288</td><td align="left">0.0063</td></tr><tr><td align="left">SR-ATAD5</td><td align="left">0.6850</td><td align="left">0.0223</td><td align="left">0.7119</td><td align="left">0.0080</td><td align="left">0.7061</td><td align="left">0.0245</td></tr><tr><td align="left">SR-HSE</td><td align="left">0.7644</td><td align="left">0.0096</td><td align="left">0.7636</td><td align="left">0.0239</td><td align="left">0.7678</td><td align="left">0.0080</td></tr><tr><td align="left">SR-MMP</td><td align="left">0.7988</td><td align="left">0.0066</td><td align="left"><bold>0.8120</bold></td><td align="left">0.0075</td><td align="left">0.8035</td><td align="left">0.0061</td></tr><tr><td align="left">SR-p53</td><td align="left">0.6970</td><td align="left">0.0253</td><td align="left">0.7291</td><td align="left">0.0114</td><td align="left">0.7401</td><td align="left">0.0203</td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Tasks</th><th align="left" colspan="2">SSL-GCN (2.0)</th><th align="left" colspan="2">SSL-GCN (3.0)</th><th align="left" colspan="2">SSL-GCN (4.0)</th></tr><tr><th align="left">AUC</th><th align="left">Std.</th><th align="left">AUC</th><th align="left">Std.</th><th align="left">AUC</th><th align="left">Std.</th></tr></thead><tbody><tr><td align="left">NR-AR-LBD</td><td align="left"><bold>0.7647</bold></td><td align="left">0.0279</td><td align="left">0.7377</td><td align="left">0.0145</td><td align="left">0.7477</td><td align="left">0.0135</td></tr><tr><td align="left">NR-AR</td><td align="left">0.7512</td><td align="left">0.0358</td><td align="left">0.7412</td><td align="left">0.0659</td><td align="left"><bold>0.7967</bold></td><td align="left">0.0251</td></tr><tr><td align="left">NR-AhR</td><td align="left">0.8287</td><td align="left">0.0072</td><td align="left"><bold>0.8303</bold></td><td align="left">0.0055</td><td align="left">0.8224</td><td align="left">0.0090</td></tr><tr><td align="left">NR-Aromatase</td><td align="left">0.7232</td><td align="left">0.0040</td><td align="left">0.7287</td><td align="left">0.0082</td><td align="left"><bold>0.7337</bold></td><td align="left">0.0057</td></tr><tr><td align="left">NR-ER-LBD</td><td align="left">0.6772</td><td align="left">0.0161</td><td align="left">0.6662</td><td align="left">0.0250</td><td align="left"><bold>0.6870</bold></td><td align="left">0.0282</td></tr><tr><td align="left">NR-ER</td><td align="left">0.7039</td><td align="left">0.0124</td><td align="left">0.7113</td><td align="left">0.0083</td><td align="left">0.7166</td><td align="left">0.0137</td></tr><tr><td align="left">NR-PPAR-gamma</td><td align="left">0.7491</td><td align="left">0.0201</td><td align="left">0.7429</td><td align="left">0.0177</td><td align="left">0.7456</td><td align="left">0.0223</td></tr><tr><td align="left">SR-ARE</td><td align="left"><bold>0.7297</bold></td><td align="left">0.0080</td><td align="left">0.7277</td><td align="left">0.0067</td><td align="left">0.7243</td><td align="left">0.0114</td></tr><tr><td align="left">SR-ATAD5</td><td align="left">0.7096</td><td align="left">0.0139</td><td align="left"><bold>0.7175</bold></td><td align="left">0.0143</td><td align="left">0.7077</td><td align="left">0.0162</td></tr><tr><td align="left">SR-HSE</td><td align="left"><bold>0.7822</bold></td><td align="left">0.0097</td><td align="left">0.7731</td><td align="left">0.0098</td><td align="left">0.7700</td><td align="left">0.0066</td></tr><tr><td align="left">SR-MMP</td><td align="left">0.8100</td><td align="left">0.0033</td><td align="left">0.8031</td><td align="left">0.0088</td><td align="left">0.8081</td><td align="left">0.0078</td></tr><tr><td align="left">SR-p53</td><td align="left"><bold>0.7518</bold></td><td align="left">0.0198</td><td align="left">0.7359</td><td align="left">0.0147</td><td align="left">0.7434</td><td align="left">0.0126</td></tr></tbody></table><table-wrap-foot><p>The bold number denotes the best result among all SSL-GCN models with various unlabeled data ratio in the corresponding task</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec11">
      <title>Performance of semi-supervised learning GCN (SSL-GCN)</title>
      <p id="Par28">The MT technique employed in this study necessitates the use of two models with the same architecture, one for <inline-formula id="IEq46"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m_t$$\end{document}</tex-math><mml:math id="M100"><mml:msub><mml:mi>m</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq46.gif"/></alternatives></inline-formula> and one for <inline-formula id="IEq47"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m_s$$\end{document}</tex-math><mml:math id="M102"><mml:msub><mml:mi>m</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq47.gif"/></alternatives></inline-formula>. Therefore, we used the hyperparameters obtained from the SL-GCN models as the initial parameters to train SSL-GCN. As shown in the previous study [<xref ref-type="bibr" rid="CR34">34</xref>], the amount of unlabeled data in the training process can affect the final model performance. To investigate this impact on the performance of the SSL-GCN models, we ran numerous trials with varying amounts of unlabeled data. We define the unlabeled-to-labeled data ratio as <inline-formula id="IEq48"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_u \in \{0.5, 1.0, 2.0, 3.0, 4.0\}$$\end{document}</tex-math><mml:math id="M104"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1.0</mml:mn><mml:mo>,</mml:mo><mml:mn>2.0</mml:mn><mml:mo>,</mml:mo><mml:mn>3.0</mml:mn><mml:mo>,</mml:mo><mml:mn>4.0</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq48.gif"/></alternatives></inline-formula>. So, when <inline-formula id="IEq49"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_u=0.5$$\end{document}</tex-math><mml:math id="M106"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq49.gif"/></alternatives></inline-formula>, we randomly select a portion of unlabeled data from the entire unlabeled data set to participate in the semi-supervised learning process, and the amount of this portion of unlabeled data is only half of the labeled data. Due to significant increase in training time, a large <inline-formula id="IEq50"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_u$$\end{document}</tex-math><mml:math id="M108"><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq50.gif"/></alternatives></inline-formula>, such as <inline-formula id="IEq51"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&gt; 4.0$$\end{document}</tex-math><mml:math id="M110"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>4.0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq51.gif"/></alternatives></inline-formula>, were not considered. Table <xref rid="Tab4" ref-type="table">4</xref> shows the test results of the optimized SSL-GCN models for the 12 toxicity prediction tasks, as well as a comparison of the ROC curves in Fig. <xref rid="Fig3" ref-type="fig">3</xref>.<fig id="Fig3"><label>Fig. 3</label><caption><p>ROC curves of best SSL-GCN, SL-GCN, and CM models. The comparison of ROC curves between the best conventional ML models (CM, black line), SL-GCN models (blue line), and SSL-GCN models with the best <inline-formula id="IEq52"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_{u}$$\end{document}</tex-math><mml:math id="M112"><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq52.gif"/></alternatives></inline-formula> (red line) on 12 toxicity prediction tasks. Additional information on the ROC curves can be found in the Additional file <xref rid="MOESM1" ref-type="media">1</xref></p></caption><graphic xlink:href="13321_2021_570_Fig3_HTML" id="MO8"/></fig></p>
      <p id="Par29">As shown in Table <xref rid="Tab4" ref-type="table">4</xref>, SSL improves the predictive power of the GCN models when sufficient amount of unlabeled data is included in the training. SSL-GCN with <inline-formula id="IEq53"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_u$$\end{document}</tex-math><mml:math id="M114"><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq53.gif"/></alternatives></inline-formula> of 0.5 improves the ROC-AUC score in 10 of the 12 prediction tasks, while only the ROC-AUC scores of two tasks are somewhat reduced. When the SSL-GCN models are trained with additional unlabeled data (<inline-formula id="IEq54"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_u$$\end{document}</tex-math><mml:math id="M116"><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq54.gif"/></alternatives></inline-formula>= 1.0 to 4.0) , they always outperform their SL-GCN counterparts in terms of AUC score. Nonetheless, the best <inline-formula id="IEq55"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_u$$\end{document}</tex-math><mml:math id="M118"><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq55.gif"/></alternatives></inline-formula> for each prediction task is different. SSL-GCN produces 4 optimal models when <inline-formula id="IEq56"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_u=2.0$$\end{document}</tex-math><mml:math id="M120"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>2.0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq56.gif"/></alternatives></inline-formula>; 3 optimal models when <inline-formula id="IEq57"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_u=4.0$$\end{document}</tex-math><mml:math id="M122"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>4.0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq57.gif"/></alternatives></inline-formula>; 2 optimal models when <inline-formula id="IEq58"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_u=0.5$$\end{document}</tex-math><mml:math id="M124"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq58.gif"/></alternatives></inline-formula>, and 1 optimal model when <inline-formula id="IEq59"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_u=1.0$$\end{document}</tex-math><mml:math id="M126"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq59.gif"/></alternatives></inline-formula>. As a result, the best <inline-formula id="IEq60"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_u$$\end{document}</tex-math><mml:math id="M128"><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq60.gif"/></alternatives></inline-formula> varies depending on the prediction task at hand. The rates of performance improvement in terms of ROC-AUC for different task range from 1% to 13%. Finally, Fig. <xref rid="Fig4" ref-type="fig">4</xref> compares the best CM, SL-GCN and SSL-GCN models. As can be clearly seen, SSL-GCN can produce models with greater predictive potential than CM and SL-GCN in all toxicity prediction tasks.<fig id="Fig4"><label>Fig. 4</label><caption><p>Comparison of AUC scores between SL-GCN, SSL-GCN and CM models Comparison of the best models from conventional methods (CM), SL-GCN, and the SSL-GCN on twelve toxicity prediction tasks. The mean and standard deviation are obtained from the 5-repeat experiments</p></caption><graphic xlink:href="13321_2021_570_Fig4_HTML" id="MO9"/></fig></p>
      <p id="Par30">As a summary, the comparative study of the SSL-GCN models with varying <inline-formula id="IEq61"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_u$$\end{document}</tex-math><mml:math id="M130"><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq61.gif"/></alternatives></inline-formula> values suggests that when training with unlabeled data, the ratio of unlabeled and labeled data should be treated as a hyperparameter in order to obtain the optimal model.</p>
    </sec>
    <sec id="Sec12">
      <title>Case study: how the similarity between unlabeled and labeled data affects the semi-supervised learning process?</title>
      <p id="Par31">In the previous section, we showed that semi-supervised learning algorithms can improve the performance of our GCN models compared to models trained with purely supervised algorithm. However, we only studied the effect of unlabeled data ration <inline-formula id="IEq62"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_{u}$$\end{document}</tex-math><mml:math id="M132"><mml:msub><mml:mi>R</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq62.gif"/></alternatives></inline-formula> on the SSL algorithm. Here, we will further investigate how the similarity between unlabeled and labeled data affects the performance of SSL-GCN model.</p>
      <p id="Par32">To define the similarity between unlabeled data and labeled dataset, we used the k-nearest neighbors (KNN) method proposed by Tropsha et al. [<xref ref-type="bibr" rid="CR50">50</xref>, <xref ref-type="bibr" rid="CR51">51</xref>] This method are been widely used to measure the similarity between known and unknown chemical compounds using different similarity cutoff, <inline-formula id="IEq63"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_s$$\end{document}</tex-math><mml:math id="M134"><mml:msub><mml:mi>C</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq63.gif"/></alternatives></inline-formula>, which is defined by following equation <xref rid="Equ5" ref-type="">5</xref>.<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} C_s(Z) = &lt;d&gt; + Z\sigma \end{aligned}$$\end{document}</tex-math><mml:math id="M136" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo><mml:mo>+</mml:mo><mml:mi>Z</mml:mi><mml:mi>σ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13321_2021_570_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq64"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$&lt;d&gt;$$\end{document}</tex-math><mml:math id="M138"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>d</mml:mi><mml:mo>&gt;</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq64.gif"/></alternatives></inline-formula> denotes the average of similarity scores of all instances in labeled data set, <inline-formula id="IEq65"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M140"><mml:mi>σ</mml:mi></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq65.gif"/></alternatives></inline-formula> denotes the standard deviation of these similarity scores. <italic>Z</italic> is a self-defined parameter to control the similarity cutoff <inline-formula id="IEq66"><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_s$$\end{document}</tex-math><mml:math id="M142"><mml:msub><mml:mi>C</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq66.gif"/></alternatives></inline-formula>, which can help us determine the level of similarity. Next, we used the average similarity score <inline-formula id="IEq67"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$SS_i$$\end{document}</tex-math><mml:math id="M144"><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq67.gif"/></alternatives></inline-formula> between each unlabeled instance <italic>i</italic> and its k nearest neighbors in the labeled dataset to evaluate how similar each unlabeled instance is to the labeled dataset. In this study, <inline-formula id="IEq68"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k=5$$\end{document}</tex-math><mml:math id="M146"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq68.gif"/></alternatives></inline-formula> and we used RDKit to calculate the most commonly used Tanimoto (Jaccard) distance as similarity score. To properly define the level of similarity, we first counted the distribution of <inline-formula id="IEq69"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$SS_i$$\end{document}</tex-math><mml:math id="M148"><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq69.gif"/></alternatives></inline-formula> in 12 similarity domains defined by different cutoff values <inline-formula id="IEq70"><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_s$$\end{document}</tex-math><mml:math id="M150"><mml:msub><mml:mi>C</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq70.gif"/></alternatives></inline-formula>. The <italic>Z</italic> of these cutoff values range from − 2 to 3.5 with a step size of 0.5. The detail of the distribution can be found in the Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S4.</p>
      <p id="Par33">To shorten the experiment time and to ensure that there is enough unlabeled data at each similarity level to support the semi-supervised learning process, we reorganized the above 12 similarity domains into 3 similarity domains based on the distribution, namely close, normal, and far. For one unlabeled instance <italic>i</italic> with similarity score <inline-formula id="IEq71"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$SS_i$$\end{document}</tex-math><mml:math id="M152"><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq71.gif"/></alternatives></inline-formula>, <inline-formula id="IEq72"><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$SS_i \le C_s(Z=0)$$\end{document}</tex-math><mml:math id="M154"><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq72.gif"/></alternatives></inline-formula> means <italic>i</italic> belongs to close domain; <inline-formula id="IEq73"><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_s(Z=0) &lt; SS_i \le C_s(Z=1)$$\end{document}</tex-math><mml:math id="M156"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq73.gif"/></alternatives></inline-formula> means it belongs to normal domain; <inline-formula id="IEq74"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_s(Z=1) &lt; SS_i$$\end{document}</tex-math><mml:math id="M158"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13321_2021_570_Article_IEq74.gif"/></alternatives></inline-formula> represents <italic>i</italic> belongs to far domain. Based on three similarity domains, we divided the entire unlabeled dataset into three subsets with corresponding similarity level. The following Table <xref rid="Tab5" ref-type="table">5</xref> presents the detail of these unlabeled subsets.<table-wrap id="Tab5"><label>Table 5</label><caption><p>The subsets of unlabeled toxicity compounds for 12 toxicological endpoints with varying levels of similarity to the corresponding labeled dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Endpoint</th><th align="left">Compounds(close)</th><th align="left">Compounds(normal)</th><th align="left">Compounds(far)</th><th align="left">Total</th></tr></thead><tbody><tr><td align="left">NR-AhR</td><td align="left">12116</td><td align="left">20416</td><td align="left">17995</td><td align="left">50527</td></tr><tr><td align="left">NR-AR-LBD</td><td align="left">11765</td><td align="left">20379</td><td align="left">18383</td><td align="left">50527</td></tr><tr><td align="left">NR-AR</td><td align="left">12471</td><td align="left">19857</td><td align="left">18199</td><td align="left">50527</td></tr><tr><td align="left">NR-Aromatase</td><td align="left">11658</td><td align="left">20764</td><td align="left">18105</td><td align="left">50527</td></tr><tr><td align="left">NR-ER-LBD</td><td align="left">11930</td><td align="left">20140</td><td align="left">18457</td><td align="left">50527</td></tr><tr><td align="left">NR-ER</td><td align="left">11868</td><td align="left">20527</td><td align="left">18132</td><td align="left">50527</td></tr><tr><td align="left">NR-PPAR-gamma</td><td align="left">11527</td><td align="left">20797</td><td align="left">18203</td><td align="left">50527</td></tr><tr><td align="left">SR-ARE</td><td align="left">11659</td><td align="left">21301</td><td align="left">17567</td><td align="left">50527</td></tr><tr><td align="left">SR-ATAD5</td><td align="left">12309</td><td align="left">19875</td><td align="left">18343</td><td align="left">50527</td></tr><tr><td align="left">SR-HSE</td><td align="left">12534</td><td align="left">20640</td><td align="left">17353</td><td align="left">50527</td></tr><tr><td align="left">SR-MMP</td><td align="left">11552</td><td align="left">21325</td><td align="left">17650</td><td align="left">50527</td></tr><tr><td align="left">SR-p53</td><td align="left">12239</td><td align="left">19991</td><td align="left">18297</td><td align="left">50527</td></tr></tbody></table></table-wrap></p>
      <p id="Par34">Here, we used these newly generated subsets to train several SSL-GCN models for comparison. We adopted the same experimental procedure (repeated 5 times) and optimal hyperparameter settings as in the previous section to facilitate performance comparison. The average ROC-AUC scores of these SSL-GCN models on the 12 test sets can be found in Table <xref rid="Tab6" ref-type="table">6</xref>. The bold number denotes the best result among all models (all, close, normal, far) in the corresponding task, the underlined number represents only the best result among models using different similarity levels of unlabeled subsets (close, normal, far).<table-wrap id="Tab6"><label>Table 6</label><caption><p>The average test performance of the SSL-GCN models with different similarity levels of unlabeled subsets (close, normal, far) on the 12 prediction tasks in 5 repeated experiments</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Tasks</th><th align="left" colspan="2">SSL-GCN (all)</th><th align="left" colspan="2">SSL-GCN (close)</th><th align="left" colspan="2">SSL-GCN (normal)</th><th align="left" colspan="2">SSL-GCN (far)</th></tr><tr><th align="left">AUC</th><th align="left">Std</th><th align="left">AUC</th><th align="left">Std</th><th align="left">AUC</th><th align="left">Std</th><th align="left">AUC</th><th align="left">Std</th></tr></thead><tbody><tr><td align="left">NR-AR-LBD</td><td align="left">0.7647</td><td align="left">0.0279</td><td align="left">0.7353</td><td align="left">0.0353</td><td align="left">0.7410</td><td align="left">0.0210</td><td align="left"><bold><underline>0.7726</underline></bold></td><td align="left">0.0242</td></tr><tr><td align="left">NR-AR</td><td align="left"><bold>0.7967</bold></td><td align="left">0.0251</td><td align="left"><underline>0.7398</underline></td><td align="left">0.0594</td><td align="left">0.7389</td><td align="left">0.0401</td><td align="left">0.7351</td><td align="left">0.0357</td></tr><tr><td align="left">NR-AhR</td><td align="left"><bold>0.8303</bold></td><td align="left">0.0055</td><td align="left">0.8261</td><td align="left">0.0076</td><td align="left"><underline>0.8292</underline></td><td align="left">0.0080</td><td align="left">0.8278</td><td align="left">0.0055</td></tr><tr><td align="left">NR-Aromatase</td><td align="left">0.7337</td><td align="left">0.0057</td><td align="left">0.7318</td><td align="left">0.0082</td><td align="left">0.7222</td><td align="left">0.0131</td><td align="left"><bold><underline>0.7382</underline></bold></td><td align="left">0.0145</td></tr><tr><td align="left">NR-ER-LBD</td><td align="left"><bold>0.6870</bold></td><td align="left">0.0282</td><td align="left"><underline>0.6731</underline></td><td align="left">0.0261</td><td align="left">0.6532</td><td align="left">0.0207</td><td align="left">0.6609</td><td align="left">0.0253</td></tr><tr><td align="left">NR-ER</td><td align="left">0.7188</td><td align="left">0.0196</td><td align="left"><bold><underline>0.7214</underline></bold></td><td align="left">0.0087</td><td align="left">0.7108</td><td align="left">0.0133</td><td align="left">0.7190</td><td align="left">0.0107</td></tr><tr><td align="left">NR-PPAR-gamma</td><td align="left"><bold>0.7614</bold></td><td align="left">0.0212</td><td align="left">0.7435</td><td align="left">0.0493</td><td align="left"><underline>0.7538</underline></td><td align="left">0.0164</td><td align="left">0.7493</td><td align="left">0.0171</td></tr><tr><td align="left">SR-ARE</td><td align="left">0.7297</td><td align="left">0.0080</td><td align="left"><bold><underline>0.7308</underline></bold></td><td align="left">0.0066</td><td align="left">0.7099</td><td align="left">0.0118</td><td align="left">0.7172</td><td align="left">0.0081</td></tr><tr><td align="left">SR-ATAD5</td><td align="left"><bold>0.7175</bold></td><td align="left">0.0143</td><td align="left">0.6896</td><td align="left">0.0261</td><td align="left">0.6855</td><td align="left">0.0295</td><td align="left"><underline>0.7095</underline></td><td align="left">0.0113</td></tr><tr><td align="left">SR-HSE</td><td align="left">0.7822</td><td align="left">0.0097</td><td align="left"><bold><underline>0.7833</underline></bold></td><td align="left">0.0116</td><td align="left">0.7700</td><td align="left">0.0071</td><td align="left">0.7745</td><td align="left">0.0075</td></tr><tr><td align="left">SR-MMP</td><td align="left"><bold>0.8120</bold></td><td align="left">0.0075</td><td align="left">0.8096</td><td align="left">0.0097</td><td align="left"><underline>0.8099</underline></td><td align="left">0.0091</td><td align="left">0.8080</td><td align="left">0.0091</td></tr><tr><td align="left">SR-p53</td><td align="left"><bold>0.7518</bold></td><td align="left">0.0198</td><td align="left">0.7159</td><td align="left">0.0208</td><td align="left"><underline>0.7417</underline></td><td align="left">0.0129</td><td align="left">0.7279</td><td align="left">0.0113</td></tr></tbody></table><table-wrap-foot><p>For comparison, the best results of the SSL-GCN models trained with the entire unlabeled dataset (all) are shown. The complete test performance can be found in the Additional file <xref rid="MOESM1" ref-type="media">1</xref></p><p>The bold number denotes the best result among all models (all, close, normal, far) in the corresponding task, the underlined number represents only the best result among models using different similarity levels of unlabeled subsets (close, normal, far)</p></table-wrap-foot></table-wrap></p>
      <p id="Par35">As shown in Table <xref rid="Tab6" ref-type="table">6</xref>, the optimal model for 7 tasks still belongs to the model trained on the entire unlabeled dataset, SSL-GCN(all). For the remaining 5 tasks, the optimal model for 3 tasks (NR-ER, SR-ARE, SR-HSE) was trained with the close subset, and only for 2 tasks (NR-AR-LBD, NR-Aromatase) the optimal model was trained with the far subset. However, the performance improvement of the SSL-GCN model on these 5 tasks is slight, ranging from 0.0011 to 0.0080, suggesting that the use of close subset and far subset in the SSL process had a limited impact on these models. On the other hand, the use of these similarity-based subsets leads to performance degradation in 7 tasks, with the largest degradation occurring in the NR-AR task, where the average AUC value decreased by 0.0616.</p>
      <p id="Par36">From the perspective of similarity between labeled and unlabeled data, models trained with the close subset tend to perform better than models trained with normal and far subsets. After excluding the performance of SSL-GCN(all) models, 5 SSL-GCN(close) models, 3 SSL-GCN(normal) models, and 3 SSL-GCN(far) models achieved optimal performance on the corresponding task. In addition, the model SSL-GCN(close) outperformed the SSL-GCN(all) model on 3 tasks (NR-ER, SR-ARE, SR-HSE), while this number is 0 for SSL-GCN(normal) model and 2 for SSL-GCN(far) model. Therefore, the performance of SSL-GCN(normal) is the worst among these three types of models; the overall scores of SSL-GCN(near), SSL-GCN(normal), and SSL-GCN(far) on 12 tasks are 0.7417, 0.7388, and 0.7450 respectively, which also indicates this fact.</p>
      <p id="Par37">There are several reasons that lead to this result. First, using unlabeled data in the close subset that is similar to the labeled data allows the semi-supervised learning model to make more accurate predictions about unlabeled data in the early training phase, allowing the model to more accurately generate and update the loss in the early training phase. This enriches the information learned by the model and results in the SSL process generating a better model. Second, using unlabeled data that is dissimilar to the labeled data (far subset) provides additional information for the SSL-GCN model during the semi-supervised learning process. This may improve the generalization ability of the model, which could increase the performance of the model on unseen data. In summary, we believe that using the entire unlabeled dataset and labeled data to train the SSL-GCN model is still the best way to generate the optimal model since the whole unlabeled dataset mixes unlabeled data with different similarities to labeled data.</p>
    </sec>
    <sec id="Sec13">
      <title>Performance comparison of SSL-GCN to the built-in DeepChem methods</title>
      <p id="Par38">The DeepChem package [<xref ref-type="bibr" rid="CR46">46</xref>] provides some built-in ML methods that can be readily used to generate predictive models for different computational chemistry challenges. Making use of the DeepChem-integrated MoleculeNet datasets [<xref ref-type="bibr" rid="CR37">37</xref>], we performed experiments to evaluate the performances of the DeepChem models on the Tox21 dataset. The dataset was splitted by scaffold splitting method and all models were initialized with the hyperparameters provided by the DeepChem package. Following the previous experimental procedure, we conducted the training, validation and test processes, and repeated them five times for each model. Here, we benchmark our method by comparing the performance of the SL-GCN and SSL-GCN models in the test set to these DeepChem models in terms of the average ROC-AUC score.
</p>
      <p id="Par39">As shown in Table <xref rid="Tab7" ref-type="table">7</xref>, among the 8 DeepChem models, the best one is <italic>kernelsvm</italic>, with an overall score of 0.7, whereas both our models SL-GCN and SSL-GCN beat the best DeepChem model with overall scores of 0.7156 (2% improvement) and 0.7571 (8% improvement), respectively. It should be mentioned that while the <italic>graphconv</italic> model utilizes similar graph convolution technique to our method but its use of different model architecture and molecular feature rendering their model less effective.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Comparison of our GCN models (SL-GCN and SSL-GCN) and the models constructed using the DeepChem built-in ML methods</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">Description</th><th align="left">Overall score</th><th align="left">Std.</th><th align="left">Refs.</th></tr></thead><tbody><tr><td align="left">logreg</td><td align="left">Logistic regression model</td><td align="left">0.6397</td><td align="left">–</td><td align="left">[<xref ref-type="bibr" rid="CR52">52</xref>]</td></tr><tr><td align="left">tf</td><td align="left">Deep neural network</td><td align="left">0.6582</td><td align="left">0.0097</td><td align="left">[<xref ref-type="bibr" rid="CR37">37</xref>]</td></tr><tr><td align="left">tf-robust</td><td align="left">Deep neural network (with bypass layers)</td><td align="left">0.6825</td><td align="left">0.0056</td><td align="left">[<xref ref-type="bibr" rid="CR53">53</xref>]</td></tr><tr><td align="left">rf</td><td align="left">Random forest model</td><td align="left">0.6618</td><td align="left">0.0066</td><td align="left">[<xref ref-type="bibr" rid="CR52">52</xref>]</td></tr><tr><td align="left">kernelsvm</td><td align="left">Kernel SVM model</td><td align="left">0.7000</td><td align="left">–</td><td align="left">[<xref ref-type="bibr" rid="CR52">52</xref>]</td></tr><tr><td align="left">graphconv</td><td align="left">Graph convolutional model</td><td align="left">0.6943</td><td align="left">0.0043</td><td align="left">[<xref ref-type="bibr" rid="CR54">54</xref>]</td></tr><tr><td align="left">irv</td><td align="left">Influence relevance voting (IRV) classifier</td><td align="left">0.6853</td><td align="left">–</td><td align="left">[<xref ref-type="bibr" rid="CR55">55</xref>]</td></tr><tr><td align="left">xgb</td><td align="left">Xgboost classification model</td><td align="left">0.6908</td><td align="left">0.0039</td><td align="left">[<xref ref-type="bibr" rid="CR56">56</xref>]</td></tr><tr><td align="left">SL-GCN</td><td align="left">Supervised GCN model</td><td align="left">0.7156</td><td align="left">0.0068</td><td align="left">This study</td></tr><tr><td align="left">SSL-GCN</td><td align="left">Semi-supervised GCN model</td><td align="left"><bold>0.7571</bold></td><td align="left">0.0084</td><td align="left">This study</td></tr></tbody></table><table-wrap-foot><p>The overall score is the average ROC-AUC score in predicting the 12 prediction tasks in the test set. The experiments were repeated 5 times</p><p>The bold number denotes the best overall score among all models</p></table-wrap-foot></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Discussion and conclusions</title>
    <p id="Par40">In this work, we attempt to improve compound toxicity prediction using graph convolutional neural network (GCN) and semi-supervised learning (SSL). We choose Mean Teacher [<xref ref-type="bibr" rid="CR34">34</xref>] as the SSL algorithm to improve the prediction performance of GCN on 12 toxicity prediction tasks from the Tox21 dataset. Meanwhile, we hope to answer two questions about predictive modeling in this research. First, is GCN superior to other more commonly used ML methods? Second, is unlabeled data advantageous for model training?</p>
    <p id="Par41">To this end, we have designed and implemented a GCN model for chemical compounds based on simple physicochemical properties of atoms. Unlike other commonly used chemical fingerprints that represent an entire compound in a one-dimensional feature vector for learning, GCN encodes it into a network of features, where the network resembles bond connectivity in the molecule. Given that structural diversity of a dataset is one of the elements that affect the prediction performance and generalizability of a model, we have used the scaffold splitting approach to divide the dataset into training, validation, and test sets for each prediction task. The Bayesian optimization technique has been used to speed up the process of tuning hyperparameters.</p>
    <p id="Par42">Now, with the GCN model in place, we have trained and optimized the supervised learning SL-GCN models and the semi-supervised learning SSL-GCN models on 12 toxicity prediction tasks. To answer the first question, is GCN superior to other commonly used ML methods? We have trained and optimized toxicity prediction models using 5 conventional ML methods in the supervised learning setting. Our comparative study has revealed that out of the 12 prediction tasks, 5 tasks are better predicted by SL-GCN, 2 tasks are similarly predicted, and 5 tasks are worse by SL-GCN; and the “better” models are not improved by a large margin. Therefore, our experimental result suggests that in the same supervised learning setting, GCN is not superior to conventional ML methods. The answer to this question is a bit disappointing though, as a GCN model is much more complex and expensive to train than the conventional models.</p>
    <p id="Par43">We believe that the bottleneck to improvement is the limitation of available data. Instead of adding more annotated data, which is not always possible or easy, we turn our attention to unlabeled data. Here, we have applied the SSL algorithm, called Mean Teacher (MT), to enhance the performance of the GCN model. Encouragingly, SSL-GCN models consistently outperform their SL-GCN counterparts, with the ROC-AUC scores improving between 1 and 13%. Nonetheless, the amount of unlabeled data required to boost performance has to be determined on a case-by-case basis. We have found that for the prediction of various toxicological endpoints, the appropriate ratios of unlabel-to-label data range from 1 to 4. Larger ratios may improve further, but were not investigated in this study due to limited computational resources. Finally, a comparative analysis of our models with the models from the DeepChem library was done. The findings are that the SL-GCN models are 2 to 12% better than the DeepChem models in terms of ROC-AUC, while the SSL-GCN models are 8 to 18% better. Based on the above results, our answer to the second question, “Is unlabeled data advantageous for model training?”, is therefore yes, and the amount of unlabeled data required to optimize the model is subject to each study.</p>
    <p id="Par44">In many bioinformatics tasks, the size of an annotated dataset is often limited, which complicates the implementation and limits the performance of many ML algorithms. The result of this study suggests that SSL could be applied to other property prediction tasks such as adsorption/distribution/metabolism/excretion (ADME), solubility, binding activity, etc., to improve the predictive ability of model by using unannotated data.</p>
    <p id="Par45">This study does, however, have some limitations that we must point out.</p>
    <p id="Par46">First, the toxicity of a compound is determined by several factors such as chirality and the nature of functional groups. This information requires a more delicate coding approach to avoid information loss during graph conversion. Although there are various well-designed molecular fingerprints or descriptors for conventional ML algorithms that can be used, there is no specific one that is suitable for GCN. Therefore, we have to use the molecule-graph conversion tool from Deep Graph Library (DGL) to convert molecules from SMILES to graphs. However, the graphs converted by this tool only include few basic molecular physicochemical properties. Due to the limited computational power, the running time of the graph convolution layers using the current feature matrix was already very high and adding additional features will certainly cost more time during the model development process. In our future study, it becomes particularly important to increase the diversity of molecular information contained in the feature matrix while limiting the size of the matrix.</p>
    <p id="Par47">Second, the interpretability of our graph convolution model has not been explored. Most researchers consider ML methods with neural networks as a black box. The only factor that can be confirmed during the training or prediction process is the input data, and the prediction results produced by these ML models are unexplainable. Specifically for biomedical ML applications, this limitation has been amplified. Without knowing which part of the compound led to the prediction result, researchers cannot modify the original compounds or select the compounds with better structure to conduct further studies. Therefore, in the next step of our study, we will focus on the interpretability of the graph convolutional neural network.</p>
    <p id="Par48">Third, the activity cliffs problem has not yet been solved in this study. Activity cliffs refer to those chemical compounds that have highly similar structure but different or opposite chemical properties. Although the semi-supervised learning algorithm can use unlabeled data to improve the performance of our GCN model. But nothing comes for free, the basic assumption of the SSL algorithm we implemented is the smoothing assumption, i.e., it assumes that the label function is smooth in high-density areas, so data points located in the same area of the feature space should share the same label. This fundamental assumption makes our model very unreliable in predicting molecules distributed at the edges of high density areas (decision boundary), where most of the molecules with “activity cliffs” are located. Moreover, there is currently no good way for QSAR models to solve the “activity cliff” problem, since the primary assumption of the QSAR model is that similar molecular structure should lead to similar properties [<xref ref-type="bibr" rid="CR57">57</xref>, <xref ref-type="bibr" rid="CR58">58</xref>]. We have already noted that there are some studies [<xref ref-type="bibr" rid="CR58">58</xref>–<xref ref-type="bibr" rid="CR62">62</xref>] that attempt to address this problem, and we will follow these studies in our future work.</p>
    <p id="Par49">Finally, our study has exploited the SSL algorithm that is based on the self-ensembling framework. There are other recently proposed SSL algorithms, such as Mixup [<xref ref-type="bibr" rid="CR63">63</xref>], Interpolation Consistency Training [<xref ref-type="bibr" rid="CR64">64</xref>], ReMixMatch [<xref ref-type="bibr" rid="CR65">65</xref>], FixMatch [<xref ref-type="bibr" rid="CR66">66</xref>], etc. The impact of different SSL algorithms on the toxicity prediction needs further research.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec26">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="13321_2021_570_MOESM1_ESM.pdf">
            <caption>
              <p><bold>Additional file 1: Table S1.</bold> Full test performances of conventional machine learning models on the 12prediction tasks in 5 repeated experiments. <bold>Table S2.</bold> Full test performances of SL-GCN on the 12 prediction tasks in 5 repeatedexperiments. <bold>Table S3.</bold> Full test performances of SSL-GCN on the 12 prediction tasks in 5 repeatedexperiments. The values in parentheses represent unlabeled data ratios (R<sub>u</sub>). <bold>Table S4.</bold> Full test performance of the SSL-GCN models with different similarity levels of unlabeled subsets (close, normal, far) on the 12 prediction tasks in 5 repeated experiments. <bold>Figure S1.</bold> Details of the ROC curves of the conventional machine learning models in 5repeated experiments on the 12 prediction tasks. <bold>Figure S2.</bold> Details of the ROC curves of the SL-GCN models in 5 repeated experimentson the 12 prediction tasks. <bold>Figure S3.</bold> Details of the ROC curves of the SL-GCN models with different unlabeledratios (R<sub>u</sub>) in 5 repeated experiments on the 12 prediction tasks. <bold>Figure S4.</bold> The distribution of <italic>SS</italic><sub><italic>i</italic></sub> in 12 similarity domains defined by different cutoff values <italic>C</italic><sub><italic>s</italic></sub>. The <italic>Z</italic> value of these cutoff values range from − 2 to 3.5 with a step size of 0.5.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank the Faculty of Science and Technology at University of Macau for providing the needed computing facilities.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors' contributions</title>
    <p>SWIS and YWS conceived the study. JC designed the solution, conducted the experiments, analyzed the results, and drafted the manuscript. CWU developed the web server. SWIS and YWS finalized the manuscript. All the authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This project was supported by University of Macau (Grant No. MYRG2019-00098-FST).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>All data used in this study comes from MoleculeNet (<ext-link ext-link-type="uri" xlink:href="https://moleculenet.org/">https://moleculenet.org/</ext-link>). The data and script files for reproducing the experiments can be downloaded from <ext-link ext-link-type="uri" xlink:href="https://github.com/chen709847237/SSL-GCN">https://github.com/chen709847237/SSL-GCN</ext-link>. The final prediction models we trained in this study are also available.</p>
  </notes>
  <notes>
    <title>Declaration</title>
    <notes id="FPar2" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par55">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Llanos</surname>
            <given-names>EJ</given-names>
          </name>
          <name>
            <surname>Leal</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Luu</surname>
            <given-names>DH</given-names>
          </name>
          <name>
            <surname>Jost</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Stadler</surname>
            <given-names>PF</given-names>
          </name>
          <name>
            <surname>Restrepo</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Exploration of the chemical space and its three historical regimes</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>2019</year>
        <volume>116</volume>
        <issue>26</issue>
        <fpage>12660</fpage>
        <lpage>12665</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1816039116</pub-id>
        <pub-id pub-id-type="pmid">31186353</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McInnes</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Virtual screening strategies in drug discovery</article-title>
        <source>Curr Opin Chem Biol</source>
        <year>2007</year>
        <volume>11</volume>
        <issue>5</issue>
        <fpage>494</fpage>
        <lpage>502</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cbpa.2007.08.033</pub-id>
        <pub-id pub-id-type="pmid">17936059</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Kubinyi</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Mannhold</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Timmerman</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <source>Virtual screening for bioactive molecules</source>
        <year>2008</year>
        <publisher-loc>Weinheim</publisher-loc>
        <publisher-name>Wiley</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Dean</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lewis</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <source>Screening: methods for experimentation in industry, drug discovery, and genetics</source>
        <year>2006</year>
        <publisher-loc>Berlin</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Oprea</surname>
            <given-names>TI</given-names>
          </name>
          <name>
            <surname>Matter</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Integrating virtual screening in lead discovery</article-title>
        <source>Curr Opin Chem Biol</source>
        <year>2004</year>
        <volume>8</volume>
        <issue>4</issue>
        <fpage>349</fpage>
        <lpage>358</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cbpa.2004.06.008</pub-id>
        <pub-id pub-id-type="pmid">15288243</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bailey</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Balls</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Recent efforts to elucidate the scientific validity of animal-based drug tests by the pharmaceutical industry, pro-testing lobby groups, and animal welfare organisations</article-title>
        <source>BMC Med Ethics</source>
        <year>2019</year>
        <volume>20</volume>
        <fpage>16</fpage>
        <pub-id pub-id-type="doi">10.1186/s12910-019-0352-3</pub-id>
        <pub-id pub-id-type="pmid">30823899</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Naderi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>H-C</given-names>
          </name>
          <name>
            <surname>Mukhopadhyay</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Brylinski</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>e toxpred: a machine learning-based approach to estimate the toxicity of drug candidates</article-title>
        <source>BMC Pharmacol Toxicol</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>2</fpage>
        <pub-id pub-id-type="doi">10.1186/s40360-018-0282-6</pub-id>
        <pub-id pub-id-type="pmid">30621790</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Raies</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Bajic</surname>
            <given-names>VB</given-names>
          </name>
        </person-group>
        <article-title>In silico toxicology: computational methods for the prediction of chemical toxicity</article-title>
        <source>Wiley Interdiscipl Rev Comput Mol Sci</source>
        <year>2016</year>
        <volume>6</volume>
        <issue>2</issue>
        <fpage>147</fpage>
        <lpage>172</lpage>
        <pub-id pub-id-type="doi">10.1002/wcms.1240</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McKinney</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Richard</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Waller</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Newman</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Gerberick</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>The practice of structure activity relationships (SAR) in toxicology</article-title>
        <source>Toxicol Sci</source>
        <year>2000</year>
        <volume>56</volume>
        <issue>1</issue>
        <fpage>8</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1093/toxsci/56.1.8</pub-id>
        <pub-id pub-id-type="pmid">10869449</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Roy</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kar</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Roy</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kar</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>RN</given-names>
          </name>
        </person-group>
        <article-title>Chapter 7—validation of qsar models</article-title>
        <source>Understanding the basics of QSAR for applications in pharmaceutical sciences and risk assessment</source>
        <year>2015</year>
        <publisher-loc>Cambridge</publisher-loc>
        <publisher-name>Academic press</publisher-name>
        <fpage>231</fpage>
        <lpage>289</lpage>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Machine learning based toxicity prediction: from chemical structural description to transcriptome analysis</article-title>
        <source>Int J Mol Sci</source>
        <year>2018</year>
        <volume>19</volume>
        <issue>8</issue>
        <fpage>2358</fpage>
        <pub-id pub-id-type="doi">10.3390/ijms19082358</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Idakwo</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Luttrell</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>A review on machine learning methods for in silico toxicity prediction</article-title>
        <source>J Environ Sci Health Part C</source>
        <year>2018</year>
        <volume>36</volume>
        <issue>4</issue>
        <fpage>169</fpage>
        <lpage>191</lpage>
        <pub-id pub-id-type="doi">10.1080/10590501.2018.1537118</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>In silico prediction of chemical toxicity for drug design using machine learning methods and structural alerts</article-title>
        <source>Front Chem</source>
        <year>2018</year>
        <volume>6</volume>
        <fpage>30</fpage>
        <pub-id pub-id-type="doi">10.3389/fchem.2018.00030</pub-id>
        <?supplied-pmid 29515993?>
        <pub-id pub-id-type="pmid">29515993</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kearnes</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>McCloskey</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Berndl</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pande</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Riley</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Molecular graph convolutions: moving beyond fingerprints</article-title>
        <source>J Comput Aided Mol Design</source>
        <year>2016</year>
        <volume>30</volume>
        <issue>8</issue>
        <fpage>595</fpage>
        <lpage>608</lpage>
        <pub-id pub-id-type="doi">10.1007/s10822-016-9938-8</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Li J, Cai D, He X (2017) Learning graph-level representation for drug discovery. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1709.03741">arXiv:1709.03741</ext-link></mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>JF</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>MY</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>CY</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>XX</given-names>
          </name>
          <name>
            <surname>Hao</surname>
            <given-names>GF</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>GF</given-names>
          </name>
        </person-group>
        <article-title>Graph attention convolutional neural network model for chemical poisoning of honey bees’ prediction</article-title>
        <source>Sci Bull</source>
        <year>2020</year>
        <volume>65</volume>
        <issue>14</issue>
        <fpage>1184</fpage>
        <lpage>1191</lpage>
        <pub-id pub-id-type="doi">10.1016/j.scib.2020.04.006</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lusci</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pollastri</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules</article-title>
        <source>J Chem Inform Model</source>
        <year>2013</year>
        <volume>53</volume>
        <issue>7</issue>
        <fpage>1563</fpage>
        <lpage>1575</lpage>
        <pub-id pub-id-type="doi">10.1021/ci400187y</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Feinberg</surname>
            <given-names>EN</given-names>
          </name>
          <name>
            <surname>Sur</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Husic</surname>
            <given-names>BE</given-names>
          </name>
          <name>
            <surname>Mai</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ramsundar</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Pande</surname>
            <given-names>VS</given-names>
          </name>
        </person-group>
        <article-title>Potentialnet for molecular property prediction</article-title>
        <source>ACS Central Sci</source>
        <year>2018</year>
        <volume>4</volume>
        <issue>11</issue>
        <fpage>1520</fpage>
        <lpage>1530</lpage>
        <pub-id pub-id-type="doi">10.1021/acscentsci.8b00507</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Portugal</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Alencar</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Cowan</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>The use of machine learning algorithms in recommender systems: a systematic review</article-title>
        <source>Expert Syst Appl</source>
        <year>2018</year>
        <volume>97</volume>
        <fpage>205</fpage>
        <lpage>227</lpage>
        <pub-id pub-id-type="doi">10.1016/j.eswa.2017.12.020</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Altae-Tran</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ramsundar</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Pappu</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Pande</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Low data drug discovery with one-shot learning</article-title>
        <source>ACS Central Sci</source>
        <year>2017</year>
        <volume>3</volume>
        <issue>4</issue>
        <fpage>283</fpage>
        <lpage>293</lpage>
        <pub-id pub-id-type="doi">10.1021/acscentsci.6b00367</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rao</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Acp-gcn: the identification of anticancer peptides based on graph convolution networks</article-title>
        <source>IEEE Access</source>
        <year>2020</year>
        <volume>8</volume>
        <fpage>176005</fpage>
        <lpage>176011</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3023800</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Li G, Muller M, Thabet A, Ghanem B (2019) Deepgcns: can gcns go as deep as cnns? In: Proceedings of the IEEE/CVF international conference on computer vision, pp 9267–9276</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Tang L, Liu H (2009) Relational learning via latent social dimensions. In: Proceedings of the 15th ACM SIGKDD international conference on knowledge discovery and data mining, pp 817–826</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Marcheggiani D, Titov I (2017) Encoding sentences with graph convolutional networks for semantic role labeling. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1703.04826">arXiv:1703.04826</ext-link></mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Bastings J, Titov I, Aziz W, Marcheggiani D, Sima’an K (2017) Graph convolutional encoders for syntax-aware neural machine translation. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1704.04675">arXiv:1704.04675</ext-link></mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Ying R, He R, Chen K, Eksombatchai P, Hamilton WL, Leskovec J (2018) Graph convolutional neural networks for web-scale recommender systems. In: Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining, pp 974–983</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Monti F, Bronstein MM, Bresson X (2017) Geometric matrix completion with recurrent multi-graph neural networks. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1704.06803">arXiv:1704.06803</ext-link></mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Kipf TN, Welling M (2016) Semi-supervised classification with graph convolutional networks. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1609.02907">arXiv:1609.02907</ext-link></mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Siu</surname>
            <given-names>SW</given-names>
          </name>
        </person-group>
        <article-title>Machine learning approaches for quality assessment of protein structures</article-title>
        <source>Biomolecules</source>
        <year>2020</year>
        <volume>10</volume>
        <issue>4</issue>
        <fpage>626</fpage>
        <pub-id pub-id-type="doi">10.3390/biom10040626</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kotsiantis</surname>
            <given-names>SB</given-names>
          </name>
          <name>
            <surname>Zaharakis</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Pintelas</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Supervised machine learning: a review of classification techniques</article-title>
        <source>Emerg Artif Intell Appl Comput Eng</source>
        <year>2007</year>
        <volume>160</volume>
        <fpage>3</fpage>
        <lpage>24</lpage>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Cui W, Liu Y, Li Y, Guo M, Li Y, Li X, Wang T, Zeng X, Ye, C (2019) Semi-supervised brain lesion segmentation with an adapted mean teacher model. In: International conference on information processing in medical imaging. Springer, pp 554–565</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van Engelen</surname>
            <given-names>JE</given-names>
          </name>
          <name>
            <surname>Hoos</surname>
            <given-names>HH</given-names>
          </name>
        </person-group>
        <article-title>A survey on semi-supervised learning</article-title>
        <source>Mach Learn</source>
        <year>2020</year>
        <volume>109</volume>
        <issue>2</issue>
        <fpage>373</fpage>
        <lpage>440</lpage>
        <pub-id pub-id-type="doi">10.1007/s10994-019-05855-6</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Rasmus A, Valpola H, Honkala M, Berglund M, Raiko T (2015) Semi-supervised learning with ladder networks. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1507.02672">arXiv:1507.02672</ext-link></mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Tarvainen A, Valpola H (2017) Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1703.01780">arXiv:1703.01780</ext-link></mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Laine S, Aila T (2016) Temporal ensembling for semi-supervised learning. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1610.02242">arXiv:1610.02242</ext-link></mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J (2014) Adam: a method for stochastic optimization. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">arXiv:1412.6980</ext-link></mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Ramsundar</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Feinberg</surname>
            <given-names>EN</given-names>
          </name>
          <name>
            <surname>Gomes</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Geniesse</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Pappu</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Leswing</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Pande</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Moleculenet: a benchmark for molecular machine learning</article-title>
        <source>Chem Sci</source>
        <year>2018</year>
        <volume>9</volume>
        <issue>2</issue>
        <fpage>513</fpage>
        <lpage>530</lpage>
        <pub-id pub-id-type="doi">10.1039/C7SC02664A</pub-id>
        <pub-id pub-id-type="pmid">29629118</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sheridan</surname>
            <given-names>RP</given-names>
          </name>
        </person-group>
        <article-title>Time-split cross-validation as a method for estimating the goodness of prospective prediction</article-title>
        <source>J Chem Inform Model</source>
        <year>2013</year>
        <volume>53</volume>
        <issue>4</issue>
        <fpage>783</fpage>
        <lpage>790</lpage>
        <pub-id pub-id-type="doi">10.1021/ci400084k</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bemis</surname>
            <given-names>GW</given-names>
          </name>
          <name>
            <surname>Murcko</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>The properties of known drugs. 1. molecular frameworks</article-title>
        <source>J Med Chem</source>
        <year>1996</year>
        <volume>39</volume>
        <issue>15</issue>
        <fpage>2887</fpage>
        <lpage>2893</lpage>
        <pub-id pub-id-type="doi">10.1021/jm9602928</pub-id>
        <pub-id pub-id-type="pmid">8709122</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">RDKit: Open-Source Cheminformatics Software (2006). <ext-link ext-link-type="uri" xlink:href="https://www.rdkit.org/">https://www.rdkit.org/</ext-link> Accessed 14 July 2021</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Wang M, Yu L, Zheng D, Gan Q, Gai Y, Ye Z, Li M, Zhou J, Huang Q, Ma C et al. (2019) Deep graph library: towards efficient and scalable deep learning on graphs</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Bergstra J, Yamins D, Cox D (2013) Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In: International conference on machine learning, pp 115–123. PMLR</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">DGL: Deep Graph Library (2018). <ext-link ext-link-type="uri" xlink:href="https://github.com/dmlc/dgl">https://github.com/dmlc/dgl</ext-link>. Accessed 14 July 2021</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">DGL-LifeSci (2020). <ext-link ext-link-type="uri" xlink:href="https://github.com/awslabs/dgl-lifesci">https://github.com/awslabs/dgl-lifesci</ext-link>. Accessed 14 July 2021</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Hyperopt: Distributed Hyperparameter Optimization (2018). <ext-link ext-link-type="uri" xlink:href="https://github.com/hyperopt/hyperopt">https://github.com/hyperopt/hyperopt</ext-link>. Accessed 14 July 2021</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Ramsundar B, Eastman P, Walters P, Pande V, Leswing K, Wu Z (2019) Deep learning for the life sciences. O’Reilly Media, 1005 Gravenstein Highway North, Sebastopol, CA 95472, USA</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">DeepChem (2015). <ext-link ext-link-type="uri" xlink:href="https://github.com/deepchem/deepchem">https://github.com/deepchem/deepchem</ext-link>. Accessed 14 July 2021</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Mean teachers are better role models (2018). <ext-link ext-link-type="uri" xlink:href="https://github.com/CuriousAI/mean-teacher">https://github.com/CuriousAI/mean-teacher</ext-link>. Accessed 17 Oct 2021</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rogers</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hahn</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Extended-connectivity fingerprints</article-title>
        <source>J Chem Inform Model</source>
        <year>2010</year>
        <volume>50</volume>
        <issue>5</issue>
        <fpage>742</fpage>
        <lpage>754</lpage>
        <pub-id pub-id-type="doi">10.1021/ci100050t</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tropsha</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gramatica</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Gombar</surname>
            <given-names>VK</given-names>
          </name>
        </person-group>
        <article-title>The importance of being earnest: validation is the absolute essential for successful application and interpretation of qspr models</article-title>
        <source>QSAR Combinatorial Sci</source>
        <year>2003</year>
        <volume>22</volume>
        <issue>1</issue>
        <fpage>69</fpage>
        <lpage>77</lpage>
        <pub-id pub-id-type="doi">10.1002/qsar.200390007</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>LeTiran</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Golbraikh</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kohn</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Tropsha</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Quantitative structure-activity relationship analysis of functionalized amino acid anticonvulsant agents using k nearest neighbor and simulated annealing pls methods</article-title>
        <source>J Med Chem</source>
        <year>2002</year>
        <volume>45</volume>
        <issue>13</issue>
        <fpage>2811</fpage>
        <lpage>2823</lpage>
        <pub-id pub-id-type="doi">10.1021/jm010488u</pub-id>
        <pub-id pub-id-type="pmid">12061883</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Michel</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Grisel</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Blondel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Prettenhofer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Dubourg</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Vanderplas</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Passos</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cournapeau</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Brucher</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Perrot</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Duchesnay</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Scikit-learn: machine learning in Python</article-title>
        <source>J Mach Learning Res</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ramsundar</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Verras</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tudor</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sheridan</surname>
            <given-names>RP</given-names>
          </name>
          <name>
            <surname>Pande</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Is multitask deep learning practical for pharma?</article-title>
        <source>J Chem Inform Model</source>
        <year>2017</year>
        <volume>57</volume>
        <issue>8</issue>
        <fpage>2068</fpage>
        <lpage>2076</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jcim.7b00146</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <mixed-citation publication-type="other">Duvenaud D, Maclaurin D, Aguilera-Iparraguirre J, Gómez-Bombarelli R, Hirzel T, Aspuru-Guzik A, Adams RP (2015) Convolutional networks on graphs for learning molecular fingerprints. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1509.09292">arXiv:1509.09292</ext-link></mixed-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Swamidass</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Azencott</surname>
            <given-names>C-A</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>T-W</given-names>
          </name>
          <name>
            <surname>Gramajo</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Tsai</surname>
            <given-names>S-C</given-names>
          </name>
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Influence relevance voting: an accurate and interpretable virtual high throughput screening method</article-title>
        <source>J Chem Inform Model</source>
        <year>2009</year>
        <volume>49</volume>
        <issue>4</issue>
        <fpage>756</fpage>
        <lpage>766</lpage>
        <pub-id pub-id-type="doi">10.1021/ci8004379</pub-id>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <mixed-citation publication-type="other">Chen T, Guestrin C (2016) Xgboost: A scalable tree boosting system. In: Proceedings of the 22nd Acm Sigkdd international conference on knowledge discovery and data mining, pp 785–794</mixed-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Maggiora</surname>
            <given-names>GM</given-names>
          </name>
        </person-group>
        <source>On outliers and activity cliffs why QSAR often disappoints</source>
        <year>2006</year>
        <publisher-loc>Washington, D.C.</publisher-loc>
        <publisher-name>ACS Publications</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Bae</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nam</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Artificial intelligence in drug discovery: a comprehensive review of data-driven and machine learning approaches</article-title>
        <source>Biotechnol Bioprocess Eng</source>
        <year>2020</year>
        <volume>25</volume>
        <issue>6</issue>
        <fpage>895</fpage>
        <lpage>930</lpage>
        <pub-id pub-id-type="doi">10.1007/s12257-020-0049-y</pub-id>
        <pub-id pub-id-type="pmid">33437151</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kohonen</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Parkkinen</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Willighagen</surname>
            <given-names>EL</given-names>
          </name>
          <name>
            <surname>Ceder</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wennerberg</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kaski</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Grafström</surname>
            <given-names>RC</given-names>
          </name>
        </person-group>
        <article-title>A transcriptomics data-driven gene space accurately predicts liver cytopathology and drug-induced liver injury</article-title>
        <source>Nat Commun</source>
        <year>2017</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="doi">10.1038/ncomms15932</pub-id>
        <pub-id pub-id-type="pmid">28232747</pub-id>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rueda-Zárate</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Imaz-Rosshandler</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Cárdenas-Ovando</surname>
            <given-names>RA</given-names>
          </name>
          <name>
            <surname>Castillo-Fernández</surname>
            <given-names>JE</given-names>
          </name>
          <name>
            <surname>Noguez-Monroy</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rangel-Escareño</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>A computational toxicogenomics approach identifies a list of highly hepatotoxic compounds from a large microarray database</article-title>
        <source>PLoS ONE</source>
        <year>2017</year>
        <volume>12</volume>
        <issue>4</issue>
        <fpage>0176284</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0176284</pub-id>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Su</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Developing a multi-dose computational model for drug-induced hepatotoxicity prediction based on toxicogenomics data</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinformatics</source>
        <year>2018</year>
        <volume>16</volume>
        <issue>4</issue>
        <fpage>1231</fpage>
        <lpage>1239</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2018.2858756</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blaschke</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Feldmann</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Bajorath</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Prediction of promiscuity cliffs using machine learning</article-title>
        <source>Mol Inform</source>
        <year>2021</year>
        <volume>40</volume>
        <issue>1</issue>
        <fpage>2000196</fpage>
        <pub-id pub-id-type="doi">10.1002/minf.202000196</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <mixed-citation publication-type="other">Zhang H, Cisse M, Dauphin YN, Lopez-Paz D (2017) mixup: beyond empirical risk minimization. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1710.09412">arXiv:1710.09412</ext-link></mixed-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <mixed-citation publication-type="other">Verma V, Kawaguchi K, Lamb A, Kannala J, Bengio Y, Lopez-Paz D (2019) Interpolation consistency training for semi-supervised learning. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1903.03825">arXiv:1903.03825</ext-link></mixed-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <mixed-citation publication-type="other">Berthelot D, Carlini N, Cubuk ED, Kurakin A, Sohn K, Zhang H, Raffel C (2019) Remixmatch: semi-supervised learning with distribution alignment and augmentation anchoring. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1911.09785">arXiv:1911.09785</ext-link></mixed-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <mixed-citation publication-type="other">Sohn K, Berthelot D, Li C-L, Zhang Z, Carlini N, Cubuk ED, Kurakin A, Zhang H, Raffel C (2020) Fixmatch: simplifying semi-supervised learning with consistency and confidence. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2001.07685">arXiv:2001.07685</ext-link></mixed-citation>
    </ref>
  </ref-list>
</back>
