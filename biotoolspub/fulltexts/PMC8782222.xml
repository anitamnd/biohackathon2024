<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Digit Imaging</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Digit Imaging</journal-id>
    <journal-title-group>
      <journal-title>Journal of Digital Imaging</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0897-1889</issn>
    <issn pub-type="epub">1618-727X</issn>
    <publisher>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8782222</article-id>
    <article-id pub-id-type="pmid">35064372</article-id>
    <article-id pub-id-type="publisher-id">574</article-id>
    <article-id pub-id-type="doi">10.1007/s10278-021-00574-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Studierfenster: an Open Science Cloud-Based Medical Imaging Analysis Platform</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Egger</surname>
          <given-names>Jan</given-names>
        </name>
        <address>
          <email>egger@tugraz.at</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wild</surname>
          <given-names>Daniel</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Weber</surname>
          <given-names>Maximilian</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bedoya</surname>
          <given-names>Christopher A. Ramirez</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Karner</surname>
          <given-names>Florian</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Prutsch</surname>
          <given-names>Alexander</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schmied</surname>
          <given-names>Michael</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dionysio</surname>
          <given-names>Christina</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Krobath</surname>
          <given-names>Dominik</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jin</surname>
          <given-names>Yuan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gsaxner</surname>
          <given-names>Christina</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Jianning</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pepe</surname>
          <given-names>Antonio</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.117476.2</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 7611</institution-id><institution>Institute of Computer Graphics and Vision, Faculty of Computer Science and Biomedical Engineering, </institution><institution>Graz University of Technology, </institution></institution-wrap>Inffeldgasse 16, 8010 Graz, Australia </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.512301.4</institution-id><institution>Computer Algorithms for Medicine Laboratory, </institution></institution-wrap>Graz, Austria </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.410718.b</institution-id><institution-id institution-id-type="ISNI">0000 0001 0262 7331</institution-id><institution>Institute for Artificial Intelligence in Medicine, AI-guided Therapies, </institution><institution>University Hospital Essen, </institution></institution-wrap>Girardetstraße 2, 45131 Essen, Germany </aff>
      <aff id="Aff4"><label>4</label>Research Center for Connected Healthcare Big Data, ZhejiangLab, 311121, Hangzhou, Zhejiang, China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>21</day>
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <volume>35</volume>
    <issue>2</issue>
    <fpage>340</fpage>
    <lpage>355</lpage>
    <history>
      <date date-type="received">
        <day>10</day>
        <month>8</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>14</day>
        <month>12</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>16</day>
        <month>12</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine 2022</copyright-statement>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Imaging modalities such as computed tomography (CT) and magnetic resonance imaging (MRI) are widely used in diagnostics, clinical studies, and treatment planning. Automatic algorithms for image analysis have thus become an invaluable tool in medicine. Examples of this are two- and three-dimensional visualizations, image segmentation, and the registration of all anatomical structure and pathology types. In this context, we introduce Studierfenster (<ext-link ext-link-type="uri" xlink:href="http://www.studierfenster.at">www.studierfenster.at</ext-link>): a free, non-commercial open science client-server framework for (bio-)medical image analysis. Studierfenster offers a wide range of capabilities, including the visualization of medical data (CT, MRI, etc.) in two-dimensional (2D) and three-dimensional (3D) space in common web browsers, such as Google Chrome, Mozilla Firefox, Safari, or Microsoft Edge. Other functionalities are the calculation of medical metrics (dice score and Hausdorff distance), manual slice-by-slice outlining of structures in medical images, manual placing of (anatomical) landmarks in medical imaging data, visualization of medical data in virtual reality (VR), and a facial reconstruction and registration of medical data for augmented reality (AR). More sophisticated features include the automatic cranial implant design with a convolutional neural network (CNN), the inpainting of aortic dissections with a generative adversarial network, and a CNN for automatic aortic landmark detection in CT angiography images. A user study with medical and non-medical experts in medical image analysis was performed, to evaluate the usability and the manual functionalities of Studierfenster. When participants were asked about their overall impression of Studierfenster in an ISO standard (ISO-Norm) questionnaire, a mean of 6.3 out of 7.0 possible points were achieved. The evaluation also provided insights into the results achievable with Studierfenster in practice, by comparing these with two ground truth segmentations performed by a physician of the Medical University of Graz in Austria. In this contribution, we presented an online environment for (bio-)medical image analysis. In doing so, we established a client-server-based architecture, which is able to process medical data, especially 3D volumes. Our online environment is not limited to medical applications for humans. Rather, its underlying concept could be interesting for researchers from other fields, in applying the already existing functionalities or future additional implementations of further image processing applications. An example could be the processing of medical acquisitions like CT or MRI from animals [Clinical Pharmacology &amp; Therapeutics, 84(4):448–456, <xref ref-type="bibr" rid="CR68">68</xref>], which get more and more common, as veterinary clinics and centers get more and more equipped with such imaging devices. Furthermore, applications in entirely non-medical research in which images/volumes need to be processed are also thinkable, such as those in optical measuring techniques, astronomy, or archaeology.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Medical image analysis</kwd>
      <kwd>Cloud</kwd>
      <kwd>Client/server</kwd>
      <kwd>Python</kwd>
      <kwd>ITK</kwd>
      <kwd>VTK</kwd>
      <kwd>Deep learning</kwd>
      <kwd>CNN</kwd>
      <kwd>GAN</kwd>
      <kwd>Virtual reality</kwd>
      <kwd>Augmented reality</kwd>
      <kwd>Whitepaper</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">In the past few decades, image-based analysis of radiological datasets has gone through a remarkable period of rapid technological innovation. Since imaging modalities, such a computed tomography (CT) and magnetic resonance imaging (MRI), are widely used in diagnostics, clinical studies, and treatment planning, automatic algorithms for (bio-)medical image processing and analysis have become an invaluable tool in medicine.</p>
    <p id="Par3">Computers today assist medical decisions in all treatment phases throughout diagnosis, monitoring, therapy planning, execution, and follow-up examinations [<xref ref-type="bibr" rid="CR1">1</xref>]. Examples are intra-operative navigation in brain tumor surgery [<xref ref-type="bibr" rid="CR2">2</xref>], radiation therapy planning for cervical cancer [<xref ref-type="bibr" rid="CR3">3</xref>], or skin tumor segmentation [<xref ref-type="bibr" rid="CR4">4</xref>]. Segmentation, for example, is typically the first step in a (bio-)medical image analysis pipeline. An incorrect segmentation thus affects any subsequent step. However, automatic medical image segmentation is known to be one of the most complex problems in image analysis and is still an object of active research. Zhang already estimated in 2006 that there are over 4000 image segmentation algorithms [<xref ref-type="bibr" rid="CR5">5</xref>] and this was well before the advent of the deep learning “era” [<xref ref-type="bibr" rid="CR6">6</xref>]. Nevertheless, the majority of such algorithms are only available locally and to the research groups that developed them, and an own usage would need both a reimplementation and new training data [<xref ref-type="bibr" rid="CR7">7</xref>]. As a result several medical image processing platforms such as MeVisLab (<ext-link ext-link-type="uri" xlink:href="https://www.mevislab.de/">https://www.mevislab.de/</ext-link>) [<xref ref-type="bibr" rid="CR8">8</xref>], (3D) Slicer (<ext-link ext-link-type="uri" xlink:href="https://www.slicer.org/">https://www.slicer.org/</ext-link>) [<xref ref-type="bibr" rid="CR9">9</xref>], MITK (<ext-link ext-link-type="uri" xlink:href="http://www.mitk.org/">www.mitk.org/</ext-link> and <ext-link ext-link-type="uri" xlink:href="http://www.radiomics.net.cn/">www.radiomics.net.cn/</ext-link>) [<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR11">11</xref>], OsiriX (<ext-link ext-link-type="uri" xlink:href="https://www.osirix-viewer.com/">https://www.osirix-viewer.com/</ext-link>) [<xref ref-type="bibr" rid="CR12">12</xref>], ITK-SNAP (<ext-link ext-link-type="uri" xlink:href="http://www.itksnap.org/">http://www.itksnap.org/</ext-link>) [<xref ref-type="bibr" rid="CR13">13</xref>], RadBuilder [<xref ref-type="bibr" rid="CR14">14</xref>], XIP-Builder [<xref ref-type="bibr" rid="CR15">15</xref>], MedAlyVis [<xref ref-type="bibr" rid="CR16">16</xref>], IBIS (<ext-link ext-link-type="uri" xlink:href="http://ibisneuronav.org/">http://ibisneuronav.org/</ext-link>) [<xref ref-type="bibr" rid="CR17">17</xref>], and MeDaS [<xref ref-type="bibr" rid="CR18">18</xref>] have been established during recent years or even decades, offering common algorithms to the entire community. In doing so, they mostly provide graphical user interfaces to the common and open-source software libraries Insight Segmentation and Registration Toolkit (ITK; <ext-link ext-link-type="uri" xlink:href="https://itk.org/">https://itk.org/</ext-link>) and the Visualization Toolkit (VTK; <ext-link ext-link-type="uri" xlink:href="https://www.vtk.org/">https://www.vtk.org/</ext-link>). Unfortunately, this implicitly means that if a bug is discovered or new library versions are available, the user needs to download and install a new version of the platform. In addition, computer science is currently an incredibly fast evolving field, resulting in recurring updates and new versions, sometimes on a daily basis. To handle this, Slicer provides a so-called nightly build to provide the newest features and bug fixes. The semi-commercial platform MeVisLab is in the meantime now offering new installers that are already much larger than one gigabyte and a problem all platforms have in common is that a user must download a newer version, uninstall the “<italic>old</italic>” version, and install the newer version from time to time.</p>
    <p id="Par4">An online, cloud-based environment for medical applications, by contrast, would make this process obsolete for end users, as new versions would be deployed and executed on a remote server. In addition, users would not need to worry about the operating system, whereas OsiriX is only offered for macOS/iOS (Apple Inc.). Furthermore, new developments and algorithms from research groups around the world can also be integrated into this cloud application and would thus be instantly available for use, with rapid feedback and refinement. This would result in an elegant solution for integrating worldwide research findings instantly in a single environment, which is not possible for the existing desktop solutions; or where there is anything whatsoever in this direction, then only as suboptimal extension plug-ins. Such an online environment can also offer an uncomplicated usage of recently arising, portable augmented reality (AR) [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>] and virtual reality (VR) devices [<xref ref-type="bibr" rid="CR21">21</xref>], by accessing the online environment from a VR/AR-ready web browser, such as Mozilla, Firefox, or Google Chrome, thus also removing the need for downloading, installing, and configuring a comprehensive platform package. In summary, an online environment would make the use of AR and VR devices more widespread in the medical domain, especially for medical teaching, training, and web conferencing. The first steps in this direction have already been taken, for example, with an online platform for interactive feedback in biomedical machine learning (<ext-link ext-link-type="uri" xlink:href="https://www.gradiohub.com">https://www.gradiohub.com</ext-link>, currently not reachable anymore, last accessed from the authors around July 2021) [<xref ref-type="bibr" rid="CR22">22</xref>] and Biomedisa, an open-source online platform for biomedical image segmentation (<ext-link ext-link-type="uri" xlink:href="https://biomedisa.de/">https://biomedisa.de/</ext-link>) [<xref ref-type="bibr" rid="CR23">23</xref>].</p>
    <p id="Par5">Another topic that is currently being actively researched — also for medical applications — is (medical) deep learning [<xref ref-type="bibr" rid="CR24">24</xref>–<xref ref-type="bibr" rid="CR26">26</xref>]. For a reliable deep neural network, a massive quantity of training data is needed. This data can be collected globally over a cloud environment and thus be used to build immense training datasets. A single medical dataset can easily reach a few dozen megabytes of storage size, which is not an issue for a cable network connection, but also wireless internet is now able to process these massive volumes, especially with the recently introduced 5G networks.</p>
  </sec>
  <sec id="Sec2">
    <title>Material and Methods</title>
    <p id="Par6">In this contribution, we introduce Studierfenster or StudierFenster (<ext-link ext-link-type="uri" xlink:href="http://www.studierfenster.at">www.studierfenster.at</ext-link>), which is a free, non-commercial open science client-server framework for (bio-)medical image analysis. Studierfenster offers a wide range of capabilities, including the visualization of medical data in common web browsers, such as Google Chrome, Mozilla Firefox, Safari, or Microsoft Edge. Other functionalities are the calculation of common medical scores, manual slice-by-slice outlining of structures in medical images, manual placing of landmarks in medical imaging data, visualization of medical data in VR, and a facial reconstruction and registration of medical data for AR. More sophisticated features include automatic cranial implant design with a convolutional neural network (CNN), the inpainting of aortic dissections with a generative adversarial network (GAN), and a CNN for automatic aortic landmark detection in CT angiography (CTA) images. The following sections will describe the datasets formats, the overall platform architecture, and the single module of Studierfenster in detail.</p>
    <sec id="Sec3">
      <title>Datasets and Preprocessing</title>
      <p id="Par7">Currently, our online platform mainly supports NRRD (nearly raw raster data) image files. These files consist of only the image values (voxel gray values) plus minimal header information and do not include any further information such as the name of the patient or the medical institution where the scan has been performed, which is typically stored in the DICOM (Digital Imaging and Communications in Medicine; <ext-link ext-link-type="uri" xlink:href="http://dicom.nema.org">http://dicom.nema.org</ext-link>) tags. In addition, NRRD files are more convenient to handle, given that they store the whole 3D volume in one single file, in contrast to DICOM, where every scanned slice of the volume is usually stored in a separate file. In summary, NRRD is a file format for the representation and processing of N-dimensional raster data. It is intended to support scientific visualization and (medical) image processing applications [<xref ref-type="bibr" rid="CR27">27</xref>].</p>
      <p id="Par8">The conversion of the original DICOM files into NRRD files can be easily achieved with tools like MeVisLab, MITK, or Slicer. In general, there are two types of NRRD files: ASCII and binary, which reduces the online traffic, and our web platform only works with compressed binary files. Studierfenster, however, also offers a module to convert non-compressed NRRD files to the corresponding compressed binary versions on its landing page. After conversion, the compressed NRRD file can be downloaded and used, for example, in the <italic>Medical 3D Viewer</italic> of Studierfenster for 2D and 3D visualization, and further image processing.</p>
    </sec>
    <sec id="Sec4">
      <title>Overall Platform Architecture</title>
      <p id="Par9">The overall platform architecture and its communication is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Studierfenster is setup as a distributed application via a client-server model. The client side (front end) has been developed using Hypertext Markup Language (HTML) and JavaScript. The front end also uses the Web Graphics Library (WebGL), a JavaScript Application Programming Interface (API) descending from the Open Graphics Library (OpenGL) ES 2.0 specification, which it still closely resembles. In contrast to OpenGL, WebGL allows the rendering of 2D and 3D graphics in web browsers. This enables the use of graphics features known from stand-alone programs directly in web applications, supported by the processing power of a client-sided graphics processing unit (GPU).<fig id="Fig1"><label>Fig. 1</label><caption><p>Overall platform architecture of Studierfenster with its modules and communications</p></caption><graphic xlink:href="10278_2021_574_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par10">The server side (back end) has been mostly developed in C, C++, and Python (<ext-link ext-link-type="uri" xlink:href="https://www.python.org/">https://www.python.org/</ext-link>). It interfaces with common open-source libraries and software tools such as ITK, VTK, the X Toolkit (XTK; <ext-link ext-link-type="uri" xlink:href="https://github.com/xtk/X">https://github.com/xtk/X</ext-link>), and Slice:Drop (<ext-link ext-link-type="uri" xlink:href="https://slicedrop.com/">https://slicedrop.com/</ext-link>). The server communication is handled through AJAX requests (<ext-link ext-link-type="uri" xlink:href="https://www.w3schools.com/xml/ajax_intro.asp">https://www.w3schools.com/xml/ajax_intro.asp</ext-link>) [<xref ref-type="bibr" rid="CR28">28</xref>]. The requests are managed by a Flask server (<ext-link ext-link-type="uri" xlink:href="https://flask.palletsprojects.com">https://flask.palletsprojects.com</ext-link>) that handles the exchange of image data, abstract data structures, like seed point positions or segmentation contours, and takes care of partial progress reporting when algorithms are executed on the server. The Flask server connects to the various algorithms, implemented in C++, ITK, VTK, PyTorch (<ext-link ext-link-type="uri" xlink:href="https://pytorch.org/">https://pytorch.org/</ext-link>), or TensorFlow (<ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/">https://www.tensorflow.org/</ext-link>), and also communicates with the file system on the server.</p>
      <p id="Par11">For a distributed multi-user development, a Vagrant box (<ext-link ext-link-type="uri" xlink:href="http://www.vagrantup.com">www.vagrantup.com</ext-link>) in combination with a GitLab repository (<ext-link ext-link-type="uri" xlink:href="http://www.gitlab.com">www.gitlab.com</ext-link>) was set up. The Vagrant box is platform-independent and thus allows contributors to use their preferred and installed operating systems, such as Windows, macOS, or Linux. In addition, users work on local developer branches and <italic>push</italic> their final implementations to the main branch once fully implemented and tested to go online. The online environment, which is currently hosted at the Graz University of Technology in Austria (<ext-link ext-link-type="uri" xlink:href="http://studierfenster.tugraz.at/">http://studierfenster.tugraz.at/</ext-link>), can already be used to perform tasks like medical visualizations and manual segmentations directly in the web browser. The manual segmentation workflow, for example, consists of dragging and dropping the input file into the browser window and outlining the object under consideration in a slice-by-slice fashion. The final segmentation can then be exported as a file, storing its contours or a binary segmentation mask. The following sections will introduce several single Studierfenster modules in more detail and the current landing page of Studierfenster is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.<fig id="Fig2"><label>Fig. 2</label><caption><p>The current Studierfenster landing page</p></caption><graphic xlink:href="10278_2021_574_Fig2_HTML" id="MO2"/></fig></p>
    </sec>
    <sec id="Sec5">
      <title>DICOM Browser and Converter</title>
      <p id="Par12">As stated above, Studierfenster works mainly with an NRRD image. However, we also offer a purely client-sided DICOM browser and converter [<xref ref-type="bibr" rid="CR29">29</xref>] (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). The DICOM browser allows client-sided parsing of a zipped local folder with DICOM files. Subsequently the whole folder can be converted to compressed NRRD files and downloaded as a single .zip file (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). These resulting NRRD files contain no patient tags, like name and age, and because the DICOM browser and converter is purely client-sided (in fact, it is loaded completely into the cache of the web browser of the user when accessing the Studierfenster website), the DICOM files and patient tags are never transferred to our server during the conversion process.<fig id="Fig3"><label>Fig. 3</label><caption><p>Client-sided DICOM browser and converter from Studierfenster [<xref ref-type="bibr" rid="CR30">30</xref>]</p></caption><graphic xlink:href="10278_2021_574_Fig3_HTML" id="MO3"/></fig><fig id="Fig4"><label>Fig. 4</label><caption><p>After the selection and conversion of specific (or all) studies or series to compressed .nrrd (nearly raw raster data) files, theses can be downloaded as a single .zip file</p></caption><graphic xlink:href="10278_2021_574_Fig4_HTML" id="MO4"/></fig></p>
      <p id="Par13">Using the DICOM browser of Studierfenster, it is also possible to select specific studies or series exclusively and to convert only these. The DICOM module has been developed in JavaScript and when the Studierfenster website is accessed, it is loaded into the cache of the web browser. This avoids further server communications and ensures that no DICOM data is transferred to the Studierfenster server, also during the conversion process.</p>
    </sec>
    <sec id="Sec6">
      <title>2D and 3D Data Visualization</title>
      <p id="Par14">The core of Studierfenster is currently the so-called <italic>Medical 3D Viewer</italic> module. It enables the visualization of 2D and 3D data in a standard web browser without further server communication. It is based on XTK and offers for (medical) volume data the classical 2D views in axial, coronal, and sagittal directions (Fig. <xref rid="Fig5" ref-type="fig">5</xref>). In addition, XTK already comes with options such as thresholding and cross-sectional slicing of 3D image data, and volume rendering, which is achieved via WebGL. For Studierfenster these functionalities have been extended and the viewer has been connected to several own client- and server-sided modules, for example, by manual contouring and landmarking capabilities of anatomical structures or pathologies in (bio-)medical datasets or even the fully automatic processing of (bio-)medical datasets.<fig id="Fig5"><label>Fig. 5</label><caption><p>Medical 3D Viewer of Studierfenster with the classical 2D views in axial, coronal, and sagittal directions (right) and volume rendering (middle) [<xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR32">32</xref>]</p></caption><graphic xlink:href="10278_2021_574_Fig5_HTML" id="MO5"/></fig></p>
    </sec>
    <sec id="Sec7">
      <title>Manual Annotation</title>
      <p id="Par15">Within the <italic>Medical 3D Viewer</italic>, Studierfenster offers a functionality for client-sided manual annotation of the imaging datasets [<xref ref-type="bibr" rid="CR33">33</xref>]. On the one hand, this can be the manual outlining (contouring) of anatomical or pathological structures (Fig. <xref rid="Fig6" ref-type="fig">6</xref>); on the other hand, this can be the manual placement of landmarks within the image or volume. The main usage for these functionalities is to generate ground truth annotations within medical datasets. These can be, for example, used afterward to evaluate the automatic annotation results produced by automatic algorithms. The annotations are purely client-sided, which means no further server communications are needed and the medical data does not need to be transferred to the Studierfenster server. This is particularly important for all cases where local legislation does not allow a data upload to third-party servers.<fig id="Fig6"><label>Fig. 6</label><caption><p>Manual segmentation of a brain tumor (glioblastoma multiforme (GBM), blue) in a magnetic resonance imaging (MRI) scan of a patient [<xref ref-type="bibr" rid="CR37">37</xref>]</p></caption><graphic xlink:href="10278_2021_574_Fig6_HTML" id="MO6"/></fig></p>
      <p id="Par16">The annotations can be saved locally as CSV files. However, the CSV files can later be re-loaded in Studierfenster to continue with the annotation or for visualization. Because these functionalities work in a standard web browser, they can be used, for example, by physicians to generate ground truths within a very restricted hospital environment, which, in general, does not permit the installation of (external) segmentation tools or software, like Slicer. In addition, Studierfenster offers a brush-based manual segmentation module, through which segmentation results can be saved locally (and re-loaded) as NRRD masks.</p>
    </sec>
    <sec id="Sec8">
      <title>Automatic Aortic Landmark Detection</title>
      <p id="Par17">In addition to the manual placement of landmarks in medical datasets, Studierfenster also offers a fully automatic detection of aortic landmarks in CTA datasets of the thorax, which we initially developed in the course of the TU Graz Lead Project on Aortic Dissections (<ext-link ext-link-type="uri" xlink:href="https://www.tugraz.at/projekte/aortic-dissection/home/">https://www.tugraz.at/projekte/aortic-dissection/home/</ext-link>) [<xref ref-type="bibr" rid="CR38">38</xref>]. The landmarks were used in this project as a starting point for the tracking of the aortic centerlines. The automatic landmark detection utilizes a patch-based CNN [<xref ref-type="bibr" rid="CR39">39</xref>] that runs on the Studierfenster server and that can predict different landmarks simultaneously. In more detail, Studierfenster uses a patch-based iterative network (PIN-algorithm) [<xref ref-type="bibr" rid="CR40">40</xref>], which combines a classification and a regression problem in one joint network. Thereby, random initial landmarks move towards a “<italic>true</italic>” landmark by multi-task learning, predicting the magnitude and direction of movement. The functionality is accessible via the <italic>Medical 3D Viewer</italic> of Studierfenster, but needs the transfer of the medical dataset (as anonymized NRRD) to the Studierfenster server for processing.</p>
    </sec>
    <sec id="Sec9">
      <title>Aortic Dissection Inpainting</title>
      <p id="Par18">Studierfenster also offers an inpainting functionality for aortic dissections [<xref ref-type="bibr" rid="CR41">41</xref>] based on a GAN [<xref ref-type="bibr" rid="CR42">42</xref>] within the <italic>Medical 3D Viewer</italic> [<xref ref-type="bibr" rid="CR43">43</xref>]. In more detail, it is a semi-supervised virtual regression of aortic dissections, which removes the so-called false lumen and reconstructs an approximation of the healthy aorta [<xref ref-type="bibr" rid="CR44">44</xref>]. In doing so, a two-stage GAN is used. The first stage reconstructs the aortic wall and depends on a network that is trained on the edge information of the healthy aortas. The second stage reconstructs the entire dataset including the texture by inferring the image information of the aorta. The inpainting can be performed by a user on specific axial slices of a volume loaded with the 3D viewer in Studierfenster showing an aortic dissection. Afterward, the inpainted volume can be downloaded by the user as NRRD. Corresponding dissected and healthy cases (for the same patient), which in general do not exist in clinical practice, can be used for further analysis of aortic dissections and their causes and disease development.</p>
    </sec>
    <sec id="Sec10">
      <title>Centerline Tracking</title>
      <p id="Par19">The centerline functionality of Studierfenster allows the automatic calculation of a vessel’s centerline in a 3D volume [<xref ref-type="bibr" rid="CR45">45</xref>]. A vessel’s centerline can be a practical method to analyze blood vessels, like the aorta, and, for example, evaluate its shape in a CT or CTA scan. The cloud-based centerline tool of Studierfenster has mainly been developed and tested for the aorta, but can also be tried out on other vascular structures. It has been implemented within the <italic>Medical 3D Viewer</italic> of Studierfenster and requires the user to place two seed points (as start and end points of the centerline) within the vessel. Following on from this an initial centerline of the vessel, e.g., the aorta, is computed (Fig. <xref rid="Fig7" ref-type="fig">7</xref>, left). In a subsequent step, the user has the additional option to further and automatically smooth this initial centerline (Fig. <xref rid="Fig7" ref-type="fig">7</xref>, right). Finally, the user can locally download the centerline as a CSV file, but also import an existing centerline into Studierfenster for visualization or further processing.<fig id="Fig7"><label>Fig. 7</label><caption><p>An initial centerline (left, red) and the corresponding smoothed centerline (right, red) calculated and visualized with Studierfenster [<xref ref-type="bibr" rid="CR45">45</xref>]</p></caption><graphic xlink:href="10278_2021_574_Fig7_HTML" id="MO7"/></fig></p>
    </sec>
    <sec id="Sec11">
      <title>3D Skull Reconstruction</title>
      <p id="Par20">Studierfenster also offers a module for the automatic reconstruction of skull defects, on which we work currently within the CAMed project (<ext-link ext-link-type="uri" xlink:href="https://www.medunigraz.at/camed">https://www.medunigraz.at/camed</ext-link>): a skull dataset of a patient with a cranial defect/hole can be uploaded to the Studierfenster server and the hole will be automatically filled to restore a healthy skull (i.e., skull shape completion) [<xref ref-type="bibr" rid="CR46">46</xref>]. In addition to this, Studierfenster can also then subtract the defective skull from the restored/completed skull. The result (i.e., the difference) can be downloaded in the STereoLithography (STL) format for further processing and, for example, additive manufacturing/3D printing (Fig. <xref rid="Fig8" ref-type="fig">8</xref>). Integrated into clinical practice, this would enable a very rapid, patient-specific in-house cranial implant design, a process which is currently still outsourced to external products in clinical routine with all the drawbacks that this involves [<xref ref-type="bibr" rid="CR48">48</xref>]. The fully automatic deep learning-based algorithm has been self-supervised during training by injecting artificial defects in healthy skulls [<xref ref-type="bibr" rid="CR49">49</xref>, <xref ref-type="bibr" rid="CR50">50</xref>]. An in-depth review of algorithms for an automatic cranial implant design can be found in <italic>AutoImplant 2020</italic> summary paper [<xref ref-type="bibr" rid="CR51">51</xref>] and challenge proceedings [<xref ref-type="bibr" rid="CR52">52</xref>].<fig id="Fig8"><label>Fig. 8</label><caption><p>Skull reconstruction under Studierfenster: defected skull (left window), reconstructed skull (window in the middle), and subtraction (right window) [<xref ref-type="bibr" rid="CR47">47</xref>]</p></caption><graphic xlink:href="10278_2021_574_Fig8_HTML" id="MO8"/></fig></p>
    </sec>
    <sec id="Sec12">
      <title>3D Face Reconstruction and Registration</title>
      <p id="Par21">Studierfenster offers also a reconstruction and registration module for medical applications. In more detail, the module enables the reconstruction of a 3D model from a single 2D photo of a person’s face [<xref ref-type="bibr" rid="CR53">53</xref>], which can be automatically registered to the medical head/face scan of this person (Fig. <xref rid="Fig9" ref-type="fig">9</xref>). This approach was later used in a non-real-time mobile medical augmented reality application for the facial area [<xref ref-type="bibr" rid="CR54">54</xref>]. This allows an augmented visualization of anatomical and pathological information in a video see-through fashion on the mobile screen, accurately registered and overlaid with the live patient. In summary, this permits a cost-effective, marker-less, and easy-to-use approach, in contrast to other systems that rely on additional markers, external devices, or depth sensors [<xref ref-type="bibr" rid="CR55">55</xref>].<fig id="Fig9"><label>Fig. 9</label><caption><p>3D Face Reconstruction and Registration module of Studierfenster: extracted surface of a medical head/face CT scan (left window), reconstructed 3D model from a single photo from a person’s face (window in the middle), and registration of both 3D models (right window)</p></caption><graphic xlink:href="10278_2021_574_Fig9_HTML" id="MO9"/></fig></p>
    </sec>
    <sec id="Sec13">
      <title>Medical Virtual Reality Viewer</title>
      <p id="Par22">The <italic>VR Viewer</italic> (or <italic>Medical VR Viewer</italic>) module of Studierfenster enables viewing (medical) data in VR with devices like the Google Cardboard or the HTC Vive (via the WebVR App). For viewing the data in VR, it needs to be converted to the VTI (.vti) format, which can be done within ParaView (<ext-link ext-link-type="uri" xlink:href="https://www.paraview.org">https://www.paraview.org</ext-link>). In contrast to other options, like viewing medical data under Unity (<ext-link ext-link-type="uri" xlink:href="https://unity.com/">https://unity.com/</ext-link>) [<xref ref-type="bibr" rid="CR56">56</xref>], which needs the installation of Unity or an App itself and dealing with specific plug-ins and also a file conversion, the Studierfenster option is quite lightweight. The same applies for other software tools that need to be installed, like MeVisLab [<xref ref-type="bibr" rid="CR57">57</xref>].</p>
    </sec>
    <sec id="Sec14">
      <title>Dice Coefficient and Hausdorff Distance Calculation</title>
      <p id="Par23">Studierfenster also provides a functionality to calculate dice similarity coefficient (DSC) [<xref ref-type="bibr" rid="CR58">58</xref>] and directed and undirected Hausdorff distance (HD) [<xref ref-type="bibr" rid="CR59">59</xref>] scores for two uploaded volumes (Fig. <xref rid="Fig10" ref-type="fig">10</xref>) [<xref ref-type="bibr" rid="CR60">60</xref>]. We decided for the DSC and HD, because these two metrics are the most popular and widely used ones in the (bio-)medical community. However, based on our implementation, other scores can easily be added in the future. An overview of medical scores can be found in the publication of Taha and Hanbury [<xref ref-type="bibr" rid="CR61">61</xref>]. The Dice coefficient, also known as the Sorensen dice coefficient, is actually the most used metric for validating (bio-)medical image segmentations. It is an overlap-based metric, and for a ground truth segmentation <italic>S</italic><sub><italic>g</italic></sub> and a predicted segmentation <italic>S</italic><sub><italic>p</italic></sub>, the DSC can be calculated as<fig id="Fig10"><label>Fig. 10</label><caption><p>Studierfenster functionality of calculating dice similarity coefficient (DSC) and directed and undirected Hausdorff distance (HD) scores for two uploaded volumes</p></caption><graphic xlink:href="10278_2021_574_Fig10_HTML" id="MO10"/></fig><disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{DSC}=\frac{2|{S}_{g}\cap {S}_{p}|}{\left|{S}_{g}\right|+|{S}_{p}|}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mi mathvariant="normal">DSC</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfenced close="|" open="|"><mml:msub><mml:mi>S</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="10278_2021_574_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par24">where 2|<italic>S</italic><sub><italic>g</italic></sub>∩<italic>S</italic><sub><italic>p</italic></sub>| is the intersection between ground truth segmentation and predicted segmentation, which corresponds to the true positives (TP). |<italic>S</italic><sub><italic>g</italic></sub>| and |<italic>S</italic><sub><italic>p</italic></sub>| denote the total amount of pixels/voxels classified as foreground in the ground truth segmentation and the predicted segmentation, respectively. The DSC takes values between zero and one, whereby one equals a perfect match (100% overlap between <italic>S</italic><sub><italic>g</italic></sub> and <italic>S</italic><sub><italic>p</italic></sub>).</p>
      <p id="Par25">The HD, on the other hand, is a spatial distance-based similarity measure, which means that the spatial positions of the pixels/voxels are taken into account, and the HD between point set A and point set B is defined as<disp-formula id="Equb"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{HD}\left(A,B\right)=\mathrm{max}\;(h\left(A,B\right), h\left(B,A\right))$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mi mathvariant="normal">HD</mml:mi><mml:mfenced close=")" open="("><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">max</mml:mi><mml:mspace width="0.277778em"/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mfenced close=")" open="("><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mfenced><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mfenced close=")" open="("><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mfenced><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="10278_2021_574_Article_Equb.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par26">where <italic>h</italic>(<italic>A</italic>,<italic>B</italic>) is the directed HD that describes the maximal distance of the point set A to the closest point in the point set B:<disp-formula id="Equc"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h\left(A,B\right)+\max_{a\in A}\underset{b\in B}{\;\min}\vert\left|a-b\right|\vert$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mi>h</mml:mi><mml:mfenced close=")" open="("><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:munder><mml:mo movablelimits="true">max</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mspace width="0.277778em"/><mml:mo movablelimits="true">min</mml:mo></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mfenced close="|" open="|"><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:mi>b</mml:mi></mml:mfenced><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="10278_2021_574_Article_Equc.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par27">where <italic>a</italic> and <italic>b</italic> are points of point set A and point set B, respectively, and ||…|| is a norm, in example a L2 norm to calculate the Euclidian distance between the two points, e.g.,<disp-formula id="Equd"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\left|\left|a-b\right|\right|}_{2}=\sqrt{\sum_{i}{{(a}_{i}-{b}_{i})}^{2}}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:msub><mml:mfenced close="|" open="|"><mml:mfenced close="|" open="|"><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:mi>b</mml:mi></mml:mfenced></mml:mfenced><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math><graphic xlink:href="10278_2021_574_Article_Equd.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par28"><italic>h</italic>(<italic>A</italic>,<italic>B</italic>) is the distance between the most distant point of point set A from the closest point of point set B and vice versa for <italic>h</italic>(<italic>B</italic>,<italic>A</italic>). Finally, HD represents the maximum between <italic>h</italic>(<italic>A</italic>,<italic>B</italic>) and <italic>h</italic>(<italic>B</italic>,<italic>A</italic>) [<xref ref-type="bibr" rid="CR62">62</xref>].</p>
      <p id="Par29">After the calculation of DSCs and HDs for several volumes, Studierfenster offers the options to filter or search for specific values in the calculated metrics, and these filtered metric lists can be exported in different file formats, like CSV, Excel, and PDF (Portable Document Format) (Fig. <xref rid="Fig11" ref-type="fig">11</xref>).<fig id="Fig11"><label>Fig. 11</label><caption><p>After the calculation of DSCs and HDs for several volumes, our tool provides the options filtering for specific values, searching for specific values in the calculated metrics, and exporting filtered metric lists in different file formats, such as CSV, Excel, and PDF</p></caption><graphic xlink:href="10278_2021_574_Fig11_HTML" id="MO11"/></fig></p>
    </sec>
    <sec id="Sec15" sec-type="data-availability">
      <title>Data Availability</title>
      <p id="Par30">The datasets generated during and/or analyzed during the current study are available in the Figshare repository (<ext-link ext-link-type="uri" xlink:href="https://figshare.com/authors/_/3752557">https://figshare.com/authors/_/3752557</ext-link>).</p>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Results</title>
    <p id="Par31">The single modules and functionalities have been evaluated with various medical datasets from different modalities, anatomies, and pathologies. The DICOM browser and converter have been tested with internal and external publicly available DICOM data collections, like [<xref ref-type="bibr" rid="CR63">63</xref>]. Average computational times have been measured, but also a breakdown of sub-processes in the conversion pipeline for an ablation study has been performed [<xref ref-type="bibr" rid="CR29">29</xref>]. The resulting and converted NRRD files have been compared to the conversion results of other offline platforms mentioned in the introduction. We currently have two configurations as server hardware, one VPN (virtual private network) machine (Ubuntu) with no GPU access running on <ext-link ext-link-type="uri" xlink:href="http://studierfenster.tugraz.at/">http://studierfenster.tugraz.at/</ext-link> and a second server with an Intel Core i7-7700K at 4.2 GHz and 64GB DDR4-RAM and GPU access to a GeForce 2070 with 8GB RAM running on <ext-link ext-link-type="uri" xlink:href="http://studierfenster.icg.tugraz.at/">http://studierfenster.icg.tugraz.at/</ext-link> (<ext-link ext-link-type="uri" xlink:href="http://www.studierfenster.at">www.studierfenster.at</ext-link> currently forwards to the server with the GPU by default).</p>
    <p id="Par32">For an automatic landmark detection, the PIN-algorithm has been evaluated via the localization mean error and standard deviation for 1000 independent predictions of four landmarks: one landmark is located close to the entrance of the heart’s left ventricle in the ascending part of the aorta; another one was placed in the center of the aortic arch; the third landmark was on the height of the first landmark, but positioned on the descending branch of the aorta (proximal descending aorta); and the fourth and final landmark was placed near the bottom of the scan in <italic>z</italic>-direction (distal descending aorta), about six slices from the lower end of the scan. For generating the ground truth, all four landmarks were individually placed by hand for all 75 CTPA (Computed Tomography Pulmonary Angiography) scans (<ext-link ext-link-type="uri" xlink:href="http://www.cad-pe.org/?page_id=14.Online">http://www.cad-pe.org/?page_id=14.Online</ext-link>) [<xref ref-type="bibr" rid="CR66">66</xref>] with the manual landmark annotation functionality of Studierfenster. The network’s prediction accuracy was quantitatively evaluated and the best setup achieved an average landmark localization error of 16.04 mm. Thereby, 99.0% of all predictions had a mean error smaller than 30 mm, while the best prediction during evaluation had a mean error of 3.53 mm over all landmarks [<xref ref-type="bibr" rid="CR38">38</xref>].</p>
    <p id="Par33">The <italic>Inpainting</italic> module of Studierfenster has been evaluated with different browsers with a specific focus on the code execution on the client side, which took only between 40 and 60 ms. An in-depth ablation study showed that it takes between 3.7 and 3.9 s to fulfill the HTTP request to the Studierfenster server. Further time measurements showed that the execution of the Flask Server code produces only a small impact of 3% of the whole request duration and the share of the data transfer network was also relatively small with 5%. With 92%, executing <italic>EdgeConnect</italic> needed the vast majority of the time to fulfill the HTTP request and an overall inpainting task took around 4 s [<xref ref-type="bibr" rid="CR43">43</xref>]. Furthermore, we trained our two-stage GAN with 3D patches from 55 non-dissected aortic datasets and evaluated it on 20 more non-dissected datasets, which we plan to add to Studierfenster in the near future [<xref ref-type="bibr" rid="CR44">44</xref>].</p>
    <p id="Par34">In order to evaluate the <italic>Centerline</italic> module of Studierfenster, we tested the smoothed centerlines computed within Studierfenster against 40 baseline centerlines from the public available CAD-PE Challenge dataset that we used also for evaluating the automatic landmark detection module [<xref ref-type="bibr" rid="CR45">45</xref>]. The baseline centerlines were extracted by us in a semi-manual process using a customized MeVisLab network. For the comparison between a ground truth and a Studierfenster-generated centerline, we computed the minimum, maximum, and mean distances between them in millimeters, which resulted in the smallest distance of 0.59 mm, an overall maximum distance of 14.18 mm, and a mean distance for all cases of 3.86 mm with a standard deviation of 0.99 mm. The centerline computation times for Studierfenster varied between 18 and 55 s on our current hardware setup, depending on the input file size and the location of the seed points inside the aorta. The centerline smoothing in the second step took between 3 and 5 s and depended on the number of initial centerline points. The initial centerlines were smoothed with a weight parameter of 1, a tolerance value of 0.00001, and a maximum iteration number of 1000.</p>
    <p id="Par35">The Studierfenster <italic>3D Skull Reconstruction</italic> module has been qualitatively and quantitatively evaluated on the publicly available CQ500 dataset (<ext-link ext-link-type="uri" xlink:href="http://headctstudy.qure.ai/dataset/">http://headctstudy.qure.ai/dataset/</ext-link>) [<xref ref-type="bibr" rid="CR64">64</xref>]. In this process large, artificial defects have been injected into the (healthy/complete) skulls to create corresponding defective skulls [<xref ref-type="bibr" rid="CR46">46</xref>]. Following on from this the <italic>Skull Reconstruction</italic> module of Studierfenster has been utilized to reconstruct the defected skulls again, or in other words, to fill the artificial injected holes again with “<italic>bone</italic>.” The evaluation between the ground truth skulls (original CQ500 skulls) and the reconstructed skulls from Studierfenster yielded a mean Hausdorff distance between 0.5328 and 1.4917 voxels, and a mean RMS (root mean square) between 0.7998 and 2.0154.</p>
    <p id="Par36">The datasets from [<xref ref-type="bibr" rid="CR65">65</xref>] were used to verify the calculation results of the <italic>Metrics</italic> module (<italic>segmentation score calculator</italic>) achieved with Studierfenster. This dataset collection consists of manual segmentations of mandibles in CT scans from ten patients. Thereby, every mandible has been segmented twice by two facial surgeons and the dice similarity coefficients and Hausdorff distances between two corresponding manual segmentation masks have been calculated with MeVisLab [<xref ref-type="bibr" rid="CR60">60</xref>]. The resulting scores have been compared with the results from Studierfenster, where the dice similarity coefficients and Hausdorff distances between the same two corresponding manual segmentation masks have also been calculated. As a result, Studierfenster delivered exactly the same metric results as the widely used desktop application MeVisLab.</p>
    <p id="Par37">The <italic>Virtual Reality</italic> module of Studierfenster has been tested with a Google Cardboard, the HTC Vive, and an Oculus Rift, for technical functionality. As datasets, we mainly used cases from public collections, such as [<xref ref-type="bibr" rid="CR65">65</xref>]. Moreover, we linked three confirmed CT scans of COVID-19 cases into Studierfenster, which can be opened with just one click in the <italic>3D Viewer</italic> by an interested user. After this, it is possible to explore these cases in 2D (in axial, sagittal, and coronal directions) and 3D. The COVID-19 cases come from a public dataset collection and more details can be found under the following website: <ext-link ext-link-type="uri" xlink:href="http://medicalsegmentation.com/covid19/">http://medicalsegmentation.com/covid19/</ext-link>.</p>
    <p id="Par38">In order to evaluate the usability and the manual functionalities of Studierfenster, a user study with medical and non-medical experts in medical image analysis was performed. The user study involved ten users in total and participant’s informed consent was obtained. Five of these users were familiar with other current existing software solutions and the other five users had no prior experience in medical image analysis. When asked about their overall impression of Studierfenster in an ISO standard (ISO-Norm) questionnaire, a mean of 6.3 out of 7.0 possible points was achieved [<xref ref-type="bibr" rid="CR33">33</xref>]. Answers were given on a Likert scale ranging from one to seven, where one is the worst rating and seven the best. The evaluation also provided insights into the results achievable with the Studierfenster in practice, by comparing them with two ground truth segmentations performed by a physician of the Medical University of Graz in Austria. Figure <xref rid="Fig12" ref-type="fig">12</xref> presents the mean ratings given in the questionnaire and the corresponding standard error as a bar chart for the following questions:<list list-type="order"><list-item><p id="Par39">The software does not need much training time.</p></list-item><list-item><p id="Par40">The software is well adjusted for the achievement of a satisfying result.</p></list-item><list-item><p id="Par41">The software provides all the necessary functions for achieving the goal.</p></list-item><list-item><p id="Par42">The software is not complicated to use.</p></list-item><list-item><p id="Par43">How satisfied are you with the UI surface?</p></list-item><list-item><p id="Par44">How satisfied are you with the presented result?</p></list-item><list-item><p id="Par45">How satisfied have you been with the time required?</p></list-item><list-item><p id="Par46">What is your overall impression?</p></list-item></list><fig id="Fig12"><label>Fig. 12</label><caption><p>User study results visualized as a bar chart, presenting the mean of the ratings of all users grouped per question [<xref ref-type="bibr" rid="CR33">33</xref>]</p></caption><graphic xlink:href="10278_2021_574_Fig12_HTML" id="MO12"/></fig></p>
    <p id="Par47">The user study consisted of two parts: (1) a short introduction to the tool, where we gave the participants the opportunity to freely explore the tool. This initial training and familiarizing took about 5 min on average. (2) The actual segmentation task, which took the participants between 3 and 15 min to finish. Here, most of the variance comes from the medical group, which took on average of 8 min and 47 s, with participants well distributed between the 3- and 15-min mark. In contrast, all but one participant from the non-medical group took between 3 and 4 min to complete the segmentation task. The one “outlier” took 14 min and 32 s. The main reason for the time difference between the two groups is that participants of the medical group edited and refined their segmentation contours more frequently [<xref ref-type="bibr" rid="CR33">33</xref>].</p>
  </sec>
  <sec id="Sec17">
    <title>Conclusions</title>
    <p id="Par48">In this contribution, we presented an online environment for (bio-)medical image analysis. In doing so, we established a client-server-based architecture, which is able to process medical data, especially 3D volumes. For elementary capabilities, like the visualization and annotation of medical imaging data, no server connection and interchange are needed, everything take place in a standard web browser on the client side. For more sophisticated features and algorithms, such as statistical calculations, centerline tracking, cranial implant generation, inpainting, and deep learning tasks, data is transferred and processed via Studierfenster. After this, the processed results, which can be an image, volume, landmarks, or a centerline, are send back to the client for visualization and further processing, and downloading. Our online environment works mainly with data in the anonymized and compressed NRRD format, to reduce the client/server network traffic. By anonymized we mean that no patient DICOM tags, such as name and age, are transferred to our server and an automatic routine ensures that all uploaded data is deleted on a daily basis. The user is still responsible, however, to ensure that only data is processed, which is already publicly available, like [<xref ref-type="bibr" rid="CR65">65</xref>–<xref ref-type="bibr" rid="CR68">68</xref>] and that the user complies with all the local regulations, such as an approved institutional review board (IRB), and is allowed to process the (bio-)medical data with our online environment.</p>
    <p id="Par49">Our online environment is not limited to medical applications for humans. Its underlying concept is rather something that could be interesting for researchers from other fields, in applying the already existing functionalities or future additional implementations of further image processing applications. An example could be the processing of medical acquisitions like CT or MRI from animals [<xref ref-type="bibr" rid="CR69">69</xref>], which are becoming more and more common, as veterinary clinics and centers are becoming ever more widely equipped with imaging devices of the kind. Furthermore, use in entirely non-medical research in which images/volumes need to be processed is also thinkable, as for example in optical measuring techniques [<xref ref-type="bibr" rid="CR70">70</xref>], astronomy [<xref ref-type="bibr" rid="CR71">71</xref>], or archaeology [<xref ref-type="bibr" rid="CR72">72</xref>].</p>
    <p id="Par50">Further work in the near future envisions the adding of more functionalities to our online environment, for example, in the online design of cranial or facial implants with task-specific workflows [<xref ref-type="bibr" rid="CR73">73</xref>–<xref ref-type="bibr" rid="CR75">75</xref>]. In addition, we need to address a software testing approach, for instance, an automated method for regression testing. In parallel, we invite other researchers and research groups to collaborate and join our research playground. Note that there is currently no public API for Studierfenster available, so interested external researchers or developers need to get in contact with us and we will provide them an API or integrate their algorithms.</p>
    <p id="Par51">Finally, we started monitoring the worldwide access and usage of our Studierfenster server with Google Analytics. In this regards, Fig. <xref rid="Fig13" ref-type="fig">13</xref> shows a map of the worldwide distribution of the very first 1000 users, accessing and interaction with Studierfenster. Interestingly, most of the users were from Germany, which may be attributed to the German origin of the chosen name for our framework: Studierfenster. In addition, we noticed that Studierfenster has been used, for example, in the Dice coefficient calculations to compare manual anatomical segmentations of brain tumors [<xref ref-type="bibr" rid="CR34">34</xref>], listed under softwares and techniques in a book about data analytics [<xref ref-type="bibr" rid="CR35">35</xref>] and stated in a review about published literature on systems and algorithms for the classification, identification, and detection of white matter hyperintensities of brain MR images [<xref ref-type="bibr" rid="CR36">36</xref>]. In the future and when more users are involved, we also plan to monitor more “access details,” such as user access to different Studierfenster transactions/tasks, which will provide more insights and indirect feedback of mainly used and lesser-used functionalities.<fig id="Fig13"><label>Fig. 13</label><caption><p>Map of the worldwide distribution of the first 1000 users, accessing and interaction with Studierfenster (monitored with Google Analytics)</p></caption><graphic xlink:href="10278_2021_574_Fig13_HTML" id="MO13"/></fig></p>
  </sec>
  <sec id="Sec18">
    <title>Contributions to the Literature</title>
    <p id="Par52">The contribution of our paper to the literature is manifold. We</p>
    <p id="Par53">• established Studierfenster (<ext-link ext-link-type="uri" xlink:href="http://studierfenster.icg.tugraz.at/">www.studierfenster.at</ext-link>): as a free, non-commercial open science client-server framework for (bio-)medical image analysis in the research community;</p>
    <p id="Par54">• provide a wide range of capabilities with our framework, like the visualization of medical data (CT, MRI, etc.) in two-dimensional (2D) and three-dimensional (3D) space in common web browsers, like Google Chrome, Mozilla Firefox, Safari or Microsoft Edge;</p>
    <p id="Par55">• conducted a usability user study with Studierfenster including medical and non-medical experts in (bio-)medical image analysis;</p>
    <p id="Par56">• report user study results in an ISO standard (ISO-Norm) questionnaire.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <notes notes-type="author-contribution">
    <title>Author Contribution</title>
    <p>Conceived and designed the experiments: JE, DW, MW, CARB, FK, AP, MS, CD, DK, YJ, CG, JL, AP. Performed the experiments: JE, DW, MW, CARB, FK, AP, MS, CD, DK, YJ, CG, JL, AP. Analyzed the data: JE, DW, MW, CARB, FK, AP, MS, CD, DK, YJ, CG, JL, AP. Contributed reagents/materials/analysis tools: JE, DW, MW, CARB, FK, AP, MS, CD, DK, YJ, CG, JL, AP. Wrote the paper: JE.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work received funding from the Austrian Science Fund (FWF) KLI 678-B31: “enFaced: Virtual and Augmented Reality Training and Navigation Module for 3D-Printed Facial Defect Reconstructions” and the TU Graz Lead Project (“Mechanics, Modeling, and Simulation of Aortic Dissection”). Moreover, this work was supported by “CAMed” (COMET K-Project 871132), which is funded by the Austrian Federal Ministry of Transport, Innovation, and Technology (BMVIT); the Austrian Federal Ministry for Digital and Economic Affairs (BMDW); and the Styrian Business Promotion Agency (SFG). Finally, the REACT-EU project KITE (Plattform für KI-Translation Essen) also provided support.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of Data and Materials/Code Availability</title>
    <p>All data and materials are available under Studierfenster (<ext-link ext-link-type="uri" xlink:href="http://www.studierfenster.at">www.studierfenster.at</ext-link>) and Figshare (<ext-link ext-link-type="uri" xlink:href="https://figshare.com/authors/_/3752557">https://figshare.com/authors/_/3752557</ext-link>). All the software and source code of Studierfenster are licensed under the GNU General Public License (GPL) version 3. The rights of third-party components are hold by their producers, with their respective license. Text and multimedia material, such as images and videos, are released under the Creative Common Attribution-Sharealike 3.0 Unported license (CC-BY-SA) and the GNU Free Documentation License (GFDL) (unversioned, with no invariant sections, front-cover texts, or back-cover texts). Text, multimedia, and source code can be re-used and modified, given that the original sources are cited, namely, (1.) Studierfenster/Studierfenster.at and (2.) this whitepaper.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar2">
      <title>Ethics Approval</title>
      <p id="Par57">Not applicable.</p>
    </notes>
    <notes id="FPar3">
      <title>Consent to Participate</title>
      <p id="Par58">Not applicable.</p>
    </notes>
    <notes id="FPar4">
      <title>Consent for Publication</title>
      <p id="Par59">Not applicable.</p>
    </notes>
    <notes id="FPar5" notes-type="COI-statement">
      <title>Conflict of Interest</title>
      <p id="Par60">The authors declare no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Egger J. Pre- and Postoperative Segmentation and Virtual Stenting of Aneurysms and Stenosis. Dissertation in Computer Science (Dr. rer. nat.), Philipps-University of Marburg, Department of Mathematics and Computer Science. 215, 2009.</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Akamatsu</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Intraoperative neuronavigation system without rigid pin fixation</article-title>
        <source>No Shinkei Geka.</source>
        <year>2009</year>
        <volume>37</volume>
        <issue>12</issue>
        <fpage>1193</fpage>
        <lpage>9</lpage>
        <?supplied-pmid 19999551?>
        <pub-id pub-id-type="pmid">19999551</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ghose</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A review of segmentation and deformable registration methods applied to adaptive cervical cancer radiation therapy treatment planning</article-title>
        <source>Artif Intell Med.</source>
        <year>2015</year>
        <volume>64</volume>
        <issue>2</issue>
        <fpage>75</fpage>
        <lpage>87</lpage>
        <pub-id pub-id-type="doi">10.1016/j.artmed.2015.04.006</pub-id>
        <pub-id pub-id-type="pmid">26025124</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abbas</surname>
            <given-names>Q</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Skin tumor area extraction using an improved dynamic programming approach</article-title>
        <source>Skin Res Technol.</source>
        <year>2012</year>
        <volume>18</volume>
        <issue>2</issue>
        <fpage>133</fpage>
        <lpage>42</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1600-0846.2011.00544.x</pub-id>
        <pub-id pub-id-type="pmid">21507072</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Zhang Y-J. Advances in Image and Video Segmentation. Hershey, PA: IRM Press. 457, 2006.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goldenberg</surname>
            <given-names>SL</given-names>
          </name>
          <name>
            <surname>Nir</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Salcudean</surname>
            <given-names>SE</given-names>
          </name>
        </person-group>
        <article-title>A new era: artificial intelligence and machine learning in prostate cancer</article-title>
        <source>Nature Reviews Urology.</source>
        <year>2019</year>
        <volume>16</volume>
        <issue>7</issue>
        <fpage>391</fpage>
        <lpage>403</lpage>
        <pub-id pub-id-type="doi">10.1038/s41585-019-0193-3</pub-id>
        <pub-id pub-id-type="pmid">31092914</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">McDermott MB, Wang S, Marinsek N, Ranganath R, Foschini L, Ghassemi M. Reproducibility in machine learning for health research: Still a ways to go. Science Translational Medicine. 2021 Mar 24;13(586).</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Egger</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Tokuda</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chauvin</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Freisleben</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Nimsky</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kapur</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Wells</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Integration of the OpenIGTlink network protocol for image guided therapy with the medical platform MeVisLab</article-title>
        <source>The international Journal of medical Robotics and Computer assisted Surgery</source>
        <year>2012</year>
        <volume>8</volume>
        <issue>3</issue>
        <fpage>282</fpage>
        <lpage>390</lpage>
        <pub-id pub-id-type="doi">10.1002/rcs.1415</pub-id>
        <pub-id pub-id-type="pmid">22374845</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Egger</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kapur</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Fedorov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pieper</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>JV</given-names>
          </name>
          <name>
            <surname>Veeraraghavan</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Freisleben</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Golby</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Nimsky</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kikinis</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>GBM Volumetry using the 3D Slicer Medical Image Computing Platform</article-title>
        <source>Sci. Rep.</source>
        <year>2013</year>
        <volume>3</volume>
        <fpage>1364</fpage>
        <pub-id pub-id-type="doi">10.1038/srep01364</pub-id>
        <pub-id pub-id-type="pmid">23455483</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Wolf I, et al. The medical imaging interaction toolkit (MITK): a toolkit facilitating the creation of interactive software by extending VTK and ITK. Proceedings Volume 5367, Medical Imaging 2004: Visualization, Image-Guided Procedures, and Display, 2004.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tian</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A Novel Software Platform for Medical Image Processing and Analyzing</article-title>
        <source>IEEE Transactions on Information Technology in Biomedicine</source>
        <year>2008</year>
        <volume>12</volume>
        <issue>6</issue>
        <fpage>800</fpage>
        <lpage>812</lpage>
        <pub-id pub-id-type="doi">10.1109/TITB.2008.926395</pub-id>
        <pub-id pub-id-type="pmid">19000961</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rosset</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>OsiriX: An Open-Source Software for Navigating in Multidimensional DICOM Images</article-title>
        <source>Journal of Digital Imaging</source>
        <year>2004</year>
        <volume>17</volume>
        <issue>3</issue>
        <fpage>205</fpage>
        <lpage>216</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-004-1014-6</pub-id>
        <pub-id pub-id-type="pmid">15534753</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yushkevich</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>Piven</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hazlett</surname>
            <given-names>HC</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>RG</given-names>
          </name>
          <name>
            <surname>Ho</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gee</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Gerig</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability</article-title>
        <source>Neuroimage.</source>
        <year>2006</year>
        <volume>31</volume>
        <issue>3</issue>
        <fpage>1116</fpage>
        <lpage>28</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.015</pub-id>
        <pub-id pub-id-type="pmid">16545965</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Egger J, Grosskopf S, O'Donnell T, Freisleben B. A software system for stent planning, stent simulation and follow-up examinations in the vascular domain. In 2009 22nd IEEE International Symposium on Computer-Based Medical Systems 2009 Aug 2 (pp. 1-7). IEEE.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Paladini G, Azar FS. An extensible imaging platform for optical imaging applications. In Multimodal Biomedical Imaging IV 2009 Feb 20 (Vol. 7171, p. 717108). International Society for Optics and Photonics.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Kuhnt D, Bauer MH, Egger J, Richter M, Kapur T, Sommer J, Merhof D, Nimsky C. Fiber tractography based on diffusion tensor imaging compared with high-angular-resolution diffusion imaging with compressed sensing: initial experience. Neurosurgery. 2013 Jan;72(0 1):165.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Drouin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kochanowska</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kersten-Oertel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gerard</surname>
            <given-names>IJ</given-names>
          </name>
          <name>
            <surname>Zelmann</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>De Nigris</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bériault</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Arbel</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sirhan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Sadikot</surname>
            <given-names>AF</given-names>
          </name>
          <name>
            <surname>Hall</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <article-title>IBIS: an OR ready open-source platform for image-guided neurosurgery</article-title>
        <source>International journal of computer assisted radiology and surgery.</source>
        <year>2017</year>
        <volume>12</volume>
        <issue>3</issue>
        <fpage>363</fpage>
        <lpage>78</lpage>
        <pub-id pub-id-type="doi">10.1007/s11548-016-1478-0</pub-id>
        <pub-id pub-id-type="pmid">27581336</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Zhang L, Li J, Li P, Lu X, Shen P, Zhu G, Shah SA, Bennarmoun M, Qian K, Schuller BW. MeDaS: An open-source platform as service to help break the walls between medicine and informatics. arXiv preprint arXiv:2007.06013. 2020 Jul 12.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Gsaxner C, Pepe A, Wallner J, Schmalstieg D, Egger J. Markerless Image-to-Face Registration for Untethered Augmented Reality in Head and Neck Surgery. MICCAI, pp. 1–9, 2019.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Development of a surgical navigation system based on augmented reality using an optical see-through head-mounted display</article-title>
        <source>J Biomed Inform</source>
        <year>2015</year>
        <volume>55</volume>
        <fpage>124</fpage>
        <lpage>31</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2015.04.003</pub-id>
        <pub-id pub-id-type="pmid">25882923</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Egger J, et al. HTC Vive MeVisLab integration via OpenVR for medical applications. PloS One 12, e0173972, 2017.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abid</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Abdalla</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Abid</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Alfozan</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>An online platform for interactive feedback in biomedical machine learning</article-title>
        <source>Nature Machine Intelligence.</source>
        <year>2020</year>
        <volume>2</volume>
        <issue>2</issue>
        <fpage>86</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-020-0147-8</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lösel</surname>
            <given-names>PD</given-names>
          </name>
          <name>
            <surname>van de Kamp</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Jayme</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ershov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Faragó</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Pichler</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Jerome</surname>
            <given-names>NT</given-names>
          </name>
          <name>
            <surname>Aadepu</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Bremer</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Chilingaryan</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Heethoff</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Introducing Biomedisa as an open-source online platform for biomedical image segmentation</article-title>
        <source>Nature communications.</source>
        <year>2020</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>4</lpage>
        <pub-id pub-id-type="doi">10.1038/s41467-020-19303-w</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Egger J, Gsaxner C, Pepe A, Li J. Medical Deep Learning--A systematic Meta-Review. arXiv preprint arXiv:2010.14881. 2020 Oct 28.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Gsaxner C, Roth P, Wallner J, Egger J. Exploit fully automatic low-level segmented PET Data for training high-level Deep Learning Algorithms for the corresponding CT Data. Plos One 14(3): e0212550, 2019.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Egger</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pepe</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gsaxner</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kern</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Deep Learning–A first meta-survey of selected reviews across scientific disciplines, their commonalities, challenges and research impact</article-title>
        <source>PeerJ Comput. Sci.</source>
        <year>2021</year>
        <volume>7</volume>
        <issue>e773</issue>
        <fpage>1</fpage>
        <lpage>83</lpage>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Aja-Fernández S, de Luis Garcia R, Tao D, Li X, editors. Tensors in image processing and computer vision. Springer Science &amp; Business Media; 2009 May 21.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Eichorn J. Understanding AJAX: Using JavaScript to create rich internet applications. Prentice Hall; 2006 Aug 1.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Bedoya CA, Wild D, Pepe A, Gsaxner C, Li J, Egger J. A web client-based online DICOM browser and NRRD converter for Studierfenster. In Medical Imaging 2021: Imaging Informatics for Healthcare, Research, and Applications 2021 Feb 15 (Vol. 11601, p. 116010P). International Society for Optics and Photonics.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Egger J. Non-contrast and contrast-enhanced t1-weighted MRI scans from a healthy subject. ResearchGate, 10.13140/RG.2.2.29916.08326, 2018.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Lindner L, Kolodziej M, Egger J. Skull-stripped Contrast-Enhanced MRI Datasets. Figshare, 10.6084/m9.figshare.7472, 2018.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Lindner L, Narnhofer D, Weber M, Gsaxner C, Kolodziej M, Egger J. Using synthetic training data for deep learning-based GBM segmentation. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) 2019 Jul 23 (pp. 6724–9). IEEE.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Wild D, Weber M, Egger J. Client/server based online environment for manual segmentation of medical images. In The 23rd Central European Seminar on Computer Graphics (CESCG), pp. 1–8, Apr. 2019.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bhandari</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Koppen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Agzarian</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Convolutional neural networks for brain tumour segmentation</article-title>
        <source>Insights into Imaging.</source>
        <year>2020</year>
        <volume>11</volume>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1186/s13244-020-00869-4</pub-id>
        <pub-id pub-id-type="pmid">31901171</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Malik H, Fatema N, Iqbal A. Intelligent Data-Analytics for Condition Monitoring: Smart Grid Applications. Elsevier; 2021 Mar 26.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Castillo</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lakshminarayanan</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Rodríguez-Álvarez</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>MR Images, Brain Lesions, and Deep Learning</article-title>
        <source>Applied Sciences.</source>
        <year>2021</year>
        <volume>11</volume>
        <issue>4</issue>
        <fpage>1675</fpage>
        <pub-id pub-id-type="doi">10.3390/app11041675</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Egger J. GBM Datasets. ResearchGate, 10.13140/RG.2.2.33331.73760, 2017.</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Schmied M, Pepe A, Egger J. A patch-based-approach for aortic landmarking. In Medical Imaging 2021: Biomedical Applications in Molecular, Structural, and Functional Imaging 2021 Feb 15 (Vol. 11600, p. 1160010). International Society for Optics and Photonics.</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>GE</given-names>
          </name>
        </person-group>
        <article-title>Imagenet classification with deep convolutional neural networks</article-title>
        <source>Advances in neural information processing systems.</source>
        <year>2012</year>
        <volume>25</volume>
        <fpage>1097</fpage>
        <lpage>105</lpage>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Li Y, Alansary A, Cerrolaza JJ, Khanal B, Sinclair M, Matthew J, Gupta C, Knight C, Kainz B, Rueckert D. Fast multiple landmark localisation using a patch-based iterative network. InInternational Conference on Medical Image Computing and Computer-Assisted Intervention 2018 Sep 16 (pp. 563-571). Springer, Cham.</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Pepe A, Li J, Rolf-Pissarczyk M, Gsaxner C, Chen X, Holzapfel GA, Egger J. Detection, segmentation, simulation and visualization of aortic dissections: A review. Medical image analysis. 2020 Oct 1;65:101773.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, Bengio Y. Generative adversarial networks. Proceedings of the International Conference on Neural Information Processing Systems (NIPS 2014). pp. 2672–2680.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Prutsch A, Pepe A, Egger J. Design and development of a web-based tool for inpainting of dissected aortae in angiography images. In The 24th Central European Seminar on Computer Graphics (CESCG), pp. 1–8, May 2020.</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <collab>Pepe A, Mistelbauer G, Gsaxner C, Li J, Fleischmann D, Schmalstieg D, Egger J. Semi-supervised Virtual Regression of Aortic Dissections Using 3D Generative Inpainting. InInternational Workshop on Thoracic Image Analysis</collab>
        </person-group>
        <source>8</source>
        <year>2020</year>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>130</fpage>
        <lpage>140</lpage>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Dionysio C, Wild D, Pepe A, Gsaxner C, Li J, Alvarez L, Egger J. A cloud-based centerline algorithm for Studierfenster. In Medical Imaging 2021: Imaging Informatics for Healthcare, Research, and Applications 2021 Feb 15 (Vol. 11601, p. 1160115). International Society for Optics and Photonics.</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Li J, Pepe A, Gsaxner C, Egger J. An online platform for automatic skull defect restoration and cranial implant design. In Medical Imaging 2021: Image-Guided Procedures, Robotic Interventions, and Modeling 2021 Feb 15 (Vol. 11598, p. 115981Q). International Society for Optics and Photonics.</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Kodym O, Li J, Pepe A, Gsaxner C, Chilamkurthy S, Egger J, Španěl M. SkullBreak/SkullFix–Dataset for automatic cranial implant design and a benchmark for volumetric shape learning tasks. Data in Brief. 2021 Apr 1;35:106902.</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Campe G, von Pistracher K. Patient Specific Implants (PSI). InCranial Implant Design Challenge, 2020 8 Springer Cham 1 9</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">Li J, Pepe A, Gsaxner C, Campe G, von JA. Egger baseline approach for autoimplant: the MICCAI 2020 cranial implant design challenge. In Multimodal Learning for Clinical Decision Support and Clinical Image-Based Procedures, 2020 Oct 4 Springer Cham 75 84</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Morais R, Egger J, Alves V. Automated Computer-aided Design of Cranial Implants using a Deep Volumetric Convolutional Denoising Autoencoder. WorldCist'19, pp. 151–60, Mar. 2019.</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Li J, Pimentel P, Szengel A, Ehlke M, Lamecker H, Zachow S, Estacio L, Doenitz C, Ramm H, Shi H, Chen X, et al. AutoImplant 2020-First MICCAI Challenge on Automatic Cranial Implant Design. IEEE Transactions on Medical Imaging. 2021 May 3.</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Egger</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Towards the Automatization of Cranial Implant Design in Cranioplasty</source>
        <year>2020</year>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <mixed-citation publication-type="other">Jackson AS, Bulat A, Argyriou V, Tzimiropoulos G. Large pose 3D face reconstruction from a single image via direct volumetric CNN regression. In Proceedings of the IEEE international conference on computer vision 2017 (pp. 1031–9).</mixed-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Karner</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Gsaxner</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Pepe</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Fleck</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Arth</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Wallner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Single-Shot</surname>
            <given-names>Egger J</given-names>
          </name>
          <name>
            <surname>Regression</surname>
            <given-names>Deep Volumetric</given-names>
          </name>
          <name>
            <surname>for Mobile Medical Augmented Reality. In Multimodal Learning for Clinical Decision Support and Clinical Image-Based Procedures, </surname>
          </name>
        </person-group>
        <source>4</source>
        <year>2020</year>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>64</fpage>
        <lpage>74</lpage>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">Gsaxner C, Pepe A, Li J, Ibrahimpasic U, Wallner J, Schmalstieg D, Egger J. Augmented Reality for Head and Neck Carcinoma Imaging: Description and Feasibility of an Instant Calibration, Markerless Approach. Computer Methods and Programs in Biomedicine. 2021 Mar 1;200:105854.</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wheeler</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Toussaint</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Pushparajah</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Schnabel</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Simpson</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Gomez</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Virtual interaction and visualisation of 3D medical imaging data with VTK and Unity</article-title>
        <source>Healthcare technology letters.</source>
        <year>2018</year>
        <volume>5</volume>
        <issue>5</issue>
        <fpage>148</fpage>
        <lpage>53</lpage>
        <pub-id pub-id-type="doi">10.1049/htl.2018.5064</pub-id>
        <pub-id pub-id-type="pmid">30800321</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <mixed-citation publication-type="other">Egger J, Gall M, Wallner J, Boechat PD, Hann A, Li X, Chen X, Schmalstieg D. Integration of the HTC Vive into the medical platform MeVisLab. In Medical Imaging 2017: Imaging Informatics for Healthcare, Research, and Applications 2017 Mar 13 (Vol. 10138, p. 1013817). International Society for Optics and Photonics.</mixed-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dice</surname>
            <given-names>LR</given-names>
          </name>
        </person-group>
        <article-title>Measures of the amount of ecologic association between species</article-title>
        <source>Ecology</source>
        <year>1945</year>
        <volume>26</volume>
        <issue>3</issue>
        <fpage>297</fpage>
        <lpage>302</lpage>
        <pub-id pub-id-type="doi">10.2307/1932409</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <mixed-citation publication-type="other">Rockafellar RT, Wets RJ-B. Variational analysis, volume 317. Springer Science &amp; Business Media, 2009.</mixed-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <mixed-citation publication-type="other">Weber M, Wild D, Wallner J, Egger J. A client/server based online environment for the calculation of medical segmentation scores. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) 2019 Jul 23 (pp. 3463–7). IEEE.</mixed-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Taha</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Hanbury</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool</article-title>
        <source>BMC Med Imaging.</source>
        <year>2015</year>
        <volume>12</volume>
        <issue>15</issue>
        <fpage>29</fpage>
        <pub-id pub-id-type="doi">10.1186/s12880-015-0068-x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Molinari</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Suri</surname>
            <given-names>JS</given-names>
          </name>
        </person-group>
        <article-title>A state of the art review on intima-media thickness (imt) measurement and wall segmentation techniques for carotid ultrasound</article-title>
        <source>Computer methods and programs in biomedicine</source>
        <year>2010</year>
        <volume>100</volume>
        <issue>3</issue>
        <fpage>201</fpage>
        <lpage>221</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2010.04.007</pub-id>
        <pub-id pub-id-type="pmid">20478640</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vallieres</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kay-Rivest</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Perrin</surname>
            <given-names>LJ</given-names>
          </name>
          <name>
            <surname>Liem</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Furstoss</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Aerts</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Khaouam</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Nguyen-Tan</surname>
            <given-names>PF</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>CS</given-names>
          </name>
          <name>
            <surname>Sultanem</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Seuntjens</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer</article-title>
        <source>Scientific reports.</source>
        <year>2017</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>4</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-017-10371-5</pub-id>
        <pub-id pub-id-type="pmid">28127051</pub-id>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <mixed-citation publication-type="other">Chilamkurthy S, Ghosh R, Tanamala S, Biviji M, Campeau NG, Venugopal VK, Mahajan V, Rao P, Warier P. Development and validation of deep learning algorithms for detection of critical findings in head CT scans. arXiv preprint arXiv:1803.05854. 2018 Mar 13.</mixed-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <mixed-citation publication-type="other">Wallner J, Mischak I, Egger J. Computed tomography data collection of the complete human mandible and valid clinical ground truth models. Scientific Data volume 6, Article number: 190003, 2019.</mixed-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <mixed-citation publication-type="other">Masoudi M, et al. A new dataset of computed-tomography angiography images for computer-aided detection of pulmonary embolism. Scientific Data, 5:180180, 09 2018.</mixed-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Menze</surname>
            <given-names>BH</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2015</year>
        <volume>34</volume>
        <issue>10</issue>
        <fpage>1993</fpage>
        <lpage>2024</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2014.2377694</pub-id>
        <pub-id pub-id-type="pmid">25494501</pub-id>
      </element-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Armato</surname>
            <given-names>SG</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The reference image database to evaluate response to therapy in lung cancer (rider) project: A resource for the development of change-analysis software</article-title>
        <source>Clinical Pharmacology &amp; Therapeutics</source>
        <year>2008</year>
        <volume>84</volume>
        <issue>4</issue>
        <fpage>448</fpage>
        <lpage>456</lpage>
        <pub-id pub-id-type="doi">10.1038/clpt.2008.161</pub-id>
        <pub-id pub-id-type="pmid">18754000</pub-id>
      </element-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nakamae</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Displacement of the large colon in a horse with enterolithiasis due to changed positions observed by computed tomography</article-title>
        <source>J Equine Sci.</source>
        <year>2018</year>
        <volume>29</volume>
        <issue>1</issue>
        <fpage>9</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1294/jes.29.9</pub-id>
        <pub-id pub-id-type="pmid">29593443</pub-id>
      </element-citation>
    </ref>
    <ref id="CR70">
      <label>70.</label>
      <mixed-citation publication-type="other">Vega F, et al. Image processing applied to measurement of particle size. J. Phys.: Conf. Ser. 582 012061, 2015.</mixed-citation>
    </ref>
    <ref id="CR71">
      <label>71.</label>
      <mixed-citation publication-type="other">Sedmak G. Image processing for astronomy. Meeting on Advanced Image Processing and Planetological Application, Vulcano, Italy, Sept. 16-18, 1985 Società Astronomica Italiana, Memorie (ISSN 0037-8720), vol. 57, no. 2, p. 149–71, 1986.</mixed-citation>
    </ref>
    <ref id="CR72">
      <label>72.</label>
      <mixed-citation publication-type="other">Forte M. Image Processing Applications in Archaeology: Classification Systems of Archaeological Sites in the Landscape, in: Andresen, J., T. Madsen and I. Scollar (eds.), Computing the Past. Computer Applications and Quantitative Methods in Archaeology. CAA92. Aarhus University Press, Aarhus, pp. 53–62, 1993.</mixed-citation>
    </ref>
    <ref id="CR73">
      <label>73.</label>
      <mixed-citation publication-type="other">Egger J, Gall M, Tax A, Ücal M, Zefferer U, Li X, von Campe G, Schäfer U, Schmalstieg D, Chen X. Interactive reconstructions of cranial 3D implants under MeVisLab as an alternative to commercial planning software. PLoS ONE, 12(3): e0172694, 2017.</mixed-citation>
    </ref>
    <ref id="CR74">
      <label>74.</label>
      <mixed-citation publication-type="other">Egger J, Wallner J, Gall M, Chen X, Schwenzer-Zimmerer K, Reinbacher K, Schmalstieg D. Computer-aided position planning of miniplates to treat facial bone defects. PLoS One. 2017 Aug 17;12(8):e0182839.</mixed-citation>
    </ref>
    <ref id="CR75">
      <label>75.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Memon</surname>
            <given-names>AR</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Egger</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>A review on computer-aided design and manufacturing of patient-specific maxillofacial implants</article-title>
        <source>Expert review of medical devices.</source>
        <year>2020</year>
        <volume>17</volume>
        <issue>4</issue>
        <fpage>345</fpage>
        <lpage>56</lpage>
        <pub-id pub-id-type="doi">10.1080/17434440.2020.1736040</pub-id>
        <pub-id pub-id-type="pmid">32105159</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
