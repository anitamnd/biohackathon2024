<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nat Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Methods</journal-id>
    <journal-title-group>
      <journal-title>Nature Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1548-7091</issn>
    <issn pub-type="epub">1548-7105</issn>
    <publisher>
      <publisher-name>Nature Publishing Group US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9349041</article-id>
    <article-id pub-id-type="pmid">35879608</article-id>
    <article-id pub-id-type="publisher-id">1541</article-id>
    <article-id pub-id-type="doi">10.1038/s41592-022-01541-z</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Self-supervised deep learning encodes high-resolution features of protein subcellular localization</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4505-2061</contrib-id>
        <name>
          <surname>Kobayashi</surname>
          <given-names>Hirofumi</given-names>
        </name>
        <address>
          <email>hirofumi.kobayashi@czbiohub.org</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cheveralls</surname>
          <given-names>Keith C.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4881-405X</contrib-id>
        <name>
          <surname>Leonetti</surname>
          <given-names>Manuel D.</given-names>
        </name>
        <address>
          <email>manuel.leonetti@czbiohub.org</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9991-9724</contrib-id>
        <name>
          <surname>Royer</surname>
          <given-names>Loic A.</given-names>
        </name>
        <address>
          <email>loic.royer@czbiohub.org</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.499295.a</institution-id><institution-id institution-id-type="ISNI">0000 0004 9234 0175</institution-id><institution>Chan Zuckerberg Biohub, </institution></institution-wrap>San Francisco, CA USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>25</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>25</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2022</year>
    </pub-date>
    <volume>19</volume>
    <issue>8</issue>
    <fpage>995</fpage>
    <lpage>1003</lpage>
    <history>
      <date date-type="received">
        <day>2</day>
        <month>7</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>26</day>
        <month>5</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Explaining the diversity and complexity of protein localization is essential to fully understand cellular architecture. Here we present cytoself, a deep-learning approach for fully self-supervised protein localization profiling and clustering. Cytoself leverages a self-supervised training scheme that does not require preexisting knowledge, categories or annotations. Training cytoself on images of 1,311 endogenously labeled proteins from the OpenCell database reveals a highly resolved protein localization atlas that recapitulates major scales of cellular organization, from coarse classes, such as nuclear and cytoplasmic, to the subtle localization signatures of individual protein complexes. We quantitatively validate cytoself’s ability to cluster proteins into organelles and protein complexes, showing that cytoself outperforms previous self-supervised approaches. Moreover, to better understand the inner workings of our model, we dissect the emergent features from which our clustering is derived, interpret them in the context of the fluorescence images, and analyze the performance contributions of each component of our approach.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">Cytoself is a self-supervised deep learning-based approach for profiling and clustering protein localization from fluorescence images. Cytoself outperforms established approaches and can accurately predict protein subcellular localization.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Machine learning</kwd>
      <kwd>Data mining</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100001691</institution-id>
            <institution>MEXT | Japan Society for the Promotion of Science (JSPS)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>Overseas Research Fellowships</award-id>
        <principal-award-recipient>
          <name>
            <surname>Kobayashi</surname>
            <given-names>Hirofumi</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s), under exclusive licence to Springer Nature America, Inc. 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Main</title>
    <p id="Par3">Systematic and large-scale microscopy-based cell assays are becoming an increasingly important tool for biological discovery<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>, playing a key role in drug screening<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup>, drug profiling<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup> and for mapping the subcellular localization of the proteome<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. In particular, large-scale datasets based on immuno-fluorescence or endogenous fluorescent tagging comprehensively capture localization patterns across the human<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup> and yeast proteome<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. Together with recent advances in computer vision and deep learning<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, such datasets are poised to help systematically map the cell’s spatial architecture. This situation is reminiscent of the early days of genomics, when the advent of high-throughput and -fidelity sequencing technologies was accompanied by the development of new algorithms to analyze, compare and categorize these sequences, and the genes therein. However, images pose unique obstacles to analysis. While sequences can be compared against a frame of reference (that is, genomes), there are no such references for microscopy images. Indeed, cells exhibit a wide variety of shapes and appearances that reflect a plurality of states. This rich diversity is much harder to model and analyze than, for example, sequence variability. Moreover, much of this diversity is stochastic, posing the additional challenge of separating information of biological relevance from irrelevant variance. The fundamental computational challenge posed by image-based screens is therefore to extract well-referenced vectorial representations that faithfully capture only the relevant biological information and allow for quantitative comparison, categorization and biological interpretation of protein localization patterns.</p>
    <p id="Par4">Previous approaches to classify and compare images have relied on engineered features that quantify different aspects of image content, such as cell size, shape and texture<sup><xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR16">16</xref></sup>. While these features are, by design, relevant and interpretable, the underlying assumption is that all the relevant features needed to analyze an image can be identified and appropriately quantified. This assumption has been challenged by deep learning’s recent successes<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. On a wide range of computer vision tasks such as image classification, hand-designed features cannot compete against learned features that are automatically discovered from the data themselves<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup>. Assuming features are available, the typical approach consists of boot-strapping the annotation process by either (1) unsupervised clustering techniques<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup>, or (2) manual curation and supervised learning<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>. In the case of supervised approaches, human annotators examine images and assign annotations, and once sufficient data are garnered, a machine learning model is trained in a supervised manner and later applied to unannotated data<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>. Another approach consists of reusing models trained on natural images to learn generic features on which supervised training can be bootstrapped<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>. While successful, these approaches suffer from potential biases, as manual annotation imposes our own preconceptions. Overall, the ideal algorithm should not rely on human knowledge or judgments, but instead automatically synthesize features and analyze images without a priori assumptions, that is, solely on the basis of the images themselves.</p>
    <p id="Par5">Recent advances in computer vision and machine learning have shown that forgoing manual labeling is possible and nears the performance of supervised approaches<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>. Instead of annotating datasets, which is inherently nonscalable and labor-intensive, self-supervised models can be trained from large uncurated datasets<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR29">29</xref>–<xref ref-type="bibr" rid="CR32">32</xref></sup>. Self-supervised models are trained by formulating an auxiliary pretext task, typically one that withholds parts of the data and instructs the model to predict them<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. This works because the task-relevant information within a piece of data is often distributed over multiple observed dimensions<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. For example, given the picture of a car, we can recognize the presence of a vehicle even if many pixels are hidden, perhaps even when half of the image is occluded. Now, consider a large dataset of pictures of real-world objects (for example, ImageNet<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>). Training a model to predict hidden parts from these images forces it to identify their important features<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. Once trained, the vectorial representations that emerge from pretext tasks capture the important features of the images, and can be used for comparison and categorization.</p>
    <p id="Par6">Here, we present the development, validation and use of cytoself, a deep learning-based approach for fully self-supervised protein localization profiling and clustering. The key innovation is a pretext task that ensures that the localization features that emerge from different images of the same protein are helpful to distinguish the microscopy images of that protein from the images of other proteins in the dataset. We demonstrate the ability of cytoself to reduce images to feature profiles characteristic of protein localization, validate their use to predict protein assignment to organelles and protein complexes, and compare the performance of cytoself with previous image featurization approaches.</p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>A robust and comprehensive image dataset</title>
      <p id="Par7">A prerequisite to our deep-learning approach is a collection of high-quality images of fluorescently tagged proteins obtained under uniform conditions. Our OpenCell<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> dataset of live-cell confocal images of 1,311 endogenously tagged proteins (<ext-link ext-link-type="uri" xlink:href="http://opencell.czbiohub.org">http://opencell.czbiohub.org</ext-link>) meets this purpose. We reasoned that providing a fiducial channel could provide a useful reference frame for our model to capture protein localization. Hence, in addition to imaging the endogenous tag (split mNeonGreen2), we also imaged a nuclear fiducial marker (Hoechst 33342) and converted it into a distance map (<xref rid="Sec15" ref-type="sec">Methods</xref>). On average, we imaged the localization of a given protein in 18.59 fields of view (FOV). Approximately 45 cropped images from each FOV containing 1–3 cells were then extracted for a total of 800 cropped images per protein. This scale, as well as the uniform conditions under which the images were collected, were important because our model must learn to ignore irrelevant image variance and instead focus on protein localization. Finally, in our approach all images that represent the same protein were labeled by the same unique identifier (we used the corresponding synthetic cell line identifier, but the identifier may be arbitrary). This identifier does not carry any explicit localization information, nor is it linked to any metadata or annotations, but rather is used to link together all the different images of the same protein.</p>
    </sec>
    <sec id="Sec4">
      <title>A deep-learning model to generate image representations</title>
      <p id="Par8">Our deep-learning model is based on the vector quantized variational autoencoder architecture (VQ-VAE<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR36">36</xref></sup>). In a classical VQ-VAE, images are encoded into a quantized latent representation, a vector, and then decoded to reconstruct the input image (Fig. <xref rid="Fig1" ref-type="fig">1</xref> and Supplementary File <xref rid="MOESM1" ref-type="media">1</xref>). The encoder and decoder are trained so as to minimize distortion between input and output images. The representation produced by the encoder is assembled by arraying a finite number of symbols (indices) that stand for vectors in a codebook (Fig. <xref rid="Fig1" ref-type="fig">1b</xref> and Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>). The codebook vectors themselves evolve during training so as to be most effective for the encoding–decoding task<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. The latest incarnation of this architecture (VQ-VAE-2, ref. <sup><xref ref-type="bibr" rid="CR37">37</xref></sup>) introduces a hierarchy of representations that operate at multiple spatial scales (termed VQ1 and VQ2 in the original VQ-VAE-2 study). We chose this architecture as a starting point because of the large body of evidence that suggests that quantized architectures currently learn the best image representations<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR36">36</xref></sup>. As shown in Fig. <xref rid="Fig1" ref-type="fig">1b</xref>, we developed a variant that uses a split vector quantization scheme to improve quantization at large spatial scales (<xref rid="Sec15" ref-type="sec">Methods</xref> and Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>). This new approach to vector quantization achieves better perplexity as shown in Fig. <xref rid="Fig1" ref-type="fig">1c</xref>, which means better codebook use.<fig id="Fig1"><label>Fig. 1</label><caption><title>Self-supervised deep learning of protein subcellular localization with cytoself.</title><p><bold>a</bold>, Workflow of the learning process. Only images and the proteins identifiers are required as input. We trained our model with a second fiducial channel for the cell nuclei, but its presence is optional as its performance contribution is negligible (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). The protein identification pretext task ensures that images corresponding to the same or similar proteins have similar representations. <bold>b</bold>, Architecture of our VQ-VAE-2 (ref. <sup><xref ref-type="bibr" rid="CR37">37</xref></sup>) -based deep-learning model featuring our two innovations: split-quantization and protein identification pretext task. Numbers in the encoders and decoders indicate encoder1, encoder2, decoder1 or decoder2 (Supplementary File <xref rid="MOESM1" ref-type="media">1</xref>). Global representation and local representation use different codebooks. <bold>c</bold>, The level of use of the codebook (that is, perplexity) increases and then saturates during training and is enhanced by applying split quantization.</p></caption><graphic xlink:href="41592_2022_1541_Fig1_HTML" id="d32e505"/></fig></p>
    </sec>
    <sec id="Sec5">
      <title>Protein localization encoding via self-supervision</title>
      <p id="Par9">Our model consists of two pretext tasks applied to each individual cropped image: first, it is tasked to encode and then decode the image as in the original VQ-VAE model. Second, it is tasked to predict the protein identifier associated with the image solely on the basis of the encoded representation. In other words, that second task aims to predict, for each single cropped image, which one of the 1,311 proteins in our library the image corresponds to. The first task forces our model to distill lower-dimensional representations of the images, while the second task forces these representations to be strong predictors of protein identity. This second task assumes that protein localization is the primary image information that is correlated to protein identity. Therefore, predicting the identifier associated with each image is key to encouraging our model to learn localization-specific representations. It is acceptable, and in some cases perfectly reasonable, for these tasks to fail. For example, when two proteins have identical localization, it is impossible to resolve the identity of the tagged proteins from images alone. Moreover, the autoencoder might be unable to perfectly reconstruct an image from the intermediate representation, when constrained to make that representation maximally predictive of protein identity. It follows that the real output of our model is not the reconstructed image, nor the predicted identity of the tagged protein, but instead the distilled image representations, which we refer to as ‘localization encodings’, that are obtained as a necessary byproduct of satisfying both pretext tasks. Specifically, our model encodes two representations for each image that correspond to two different spatial scales, the local and global representations, that correspond to VQ1 and VQ2, respectively. The global representation captures large-scale image structure scaled-down to a 4 × 4 pixel image with 576 features (values) per pixel. The local representation captures finer spatially resolved details (25 × 25 pixel image with 64 features per pixel). We use the global representations to perform localization clustering, and the local representations to provide a finer and spatially resolved decomposition of protein localization.</p>
    </sec>
    <sec id="Sec6">
      <title>Mapping the protein localization landscape with cytoself</title>
      <p id="Par10">Obtaining image representations that are highly correlated with protein localization and invariant to other sources of heterogeneity (that is, cell state, density and shape) is only the first step for biological interpretation. Indeed, while these representations are lower dimensional than the images themselves, they still have too many dimensions for direct inspection and visualization. Therefore, we performed dimensionality reduction using the uniform manifold approximation and projection (UMAP) algorithm on the set of global localization encodings (that is, global representation in the Fig. <xref rid="Fig1" ref-type="fig">1</xref>) obtained from all images (<xref rid="Sec15" ref-type="sec">Methods</xref>). In the resulting UMAP (Fig. <xref rid="Fig2" ref-type="fig">2</xref>) each point represents a single (cropped) image in our test dataset (that is, 10% of entire dataset, <xref rid="Sec15" ref-type="sec">Methods</xref>), which collectively form a highly detailed map representing the full diversity of protein subcellular localizations. This protein localization atlas reveals an organization of clusters and subclusters reflective of eukaryotic subcellular architecture. We can evaluate and explore this map by labeling each protein according to its known subcellular localization obtained from independent manual annotations of our image dataset (Supplementary File <xref rid="MOESM1" ref-type="media">2</xref>). The most pronounced delineation corresponds to nuclear (top right) versus nonnuclear (bottom left) localizations (encircled and expanded in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, top right and bottom left, respectively). Within the nuclear cluster, subclusters are resolved that correspond to nucleoplasm, chromatin, nuclear membrane and the nucleolus. Within each region, tight clusters that correspond to specific cellular functions can be resolved (dashed outlines). For example, subunits involved in splicing (SF3 splicesome), transcription (core RNA polymerase) or nuclear import (nuclear pore) cluster tightly together (outlined in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, dashed outlines). Similarly, subdomains emerge within the nonnuclear cluster, the largest corresponding to cytoplasmic and vesicular localizations. Within these domains are several well delineated clusters corresponding to mitochondria, endoplasmic reticulum (ER) exit sites (COPII), ribosomes and clathrin coated vesicles (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). The large set of unlabeled points in Fig. <xref rid="Fig2" ref-type="fig">2</xref> (gray dots) correspond mainly to proteins that exhibit mixed localization patterns. Prominent among these is a band of proteins interspersed between the nuclear and nonnuclear regions (expanded in Fig. <xref rid="Fig3" ref-type="fig">3a</xref>). Representative proteins chosen along that path show a continuous gradation from mostly cytoplasmic to mostly nuclear localization.<fig id="Fig2"><label>Fig. 2</label><caption><title>High-resolution protein localization atlas.</title><p>Each point corresponds to a single image from our test dataset of 109,751 images. To reveal the underlying structure of our map, each point in the central UMAP is colored according to 11 distinct protein localization categories (mitochondria, vesicles, nucleoplasm, cytoplasm, nuclear membrane, ER, nucleolus, Golgi, chromatin domain). These categories are expanded in the surrounding circles. Tight clusters corresponding to functionally defined protein complexes can be identified within each localization category. Only proteins with a clear and exclusive localization pattern are colored, gray points correspond to proteins with other or mixed localizations. Within each localization category, the resolution of cytoself representations is further illustrated by labeling the images corresponding to individual proteins in different colors (dashed circular inserts). Note that while the colors in the central UMAP represent different cellular territories, colors in the inserts are only used to delineate individual proteins, and do not correspond to the colors used in the main UMAP. The list of annotated proteins and the subunits of each complex are indicated in Supplementary Files <xref rid="MOESM1" ref-type="media">2</xref> and <xref rid="MOESM1" ref-type="media">5</xref>, respectively.</p></caption><graphic xlink:href="41592_2022_1541_Fig2_HTML" id="d32e563"/></fig><fig id="Fig3"><label>Fig. 3</label><caption><title>Exploring the protein localization atlas.</title><p><bold>a</bold>, Representative images of proteins localized along an exemplary path across the nuclear-cytoplasmic transition and over the ‘gray’ space of mixed localizations. <bold>b</bold>, The subunits of well-known and stable protein complexes tightly cluster together. Moreover, the complexes themselves are placed in their correct cellular contexts. Different proteins have different expression levels, hence we adjusted the brightness of each panel so as to make all localizations present in each image more visible (only minimum–maximum intensities are adjusted, no gamma adjustment used). All representative images were randomly selected. Protein localization is displayed in grayscale in both panels, the nuclei in <bold>b</bold> are displayed in blue. The list of the subunits of each complex are indicated in Supplementary File <xref rid="MOESM1" ref-type="media">5</xref>. Scale bars, 10 μm.</p></caption><graphic xlink:href="41592_2022_1541_Fig3_HTML" id="d32e586"/></fig></p>
    </sec>
    <sec id="Sec7">
      <title>Quantifying cytoself’s clustering performance</title>
      <p id="Par11">To validate our results, clustering scores were computed (<xref rid="Sec15" ref-type="sec">Methods</xref>, Fig. <xref rid="Fig4" ref-type="fig">4</xref> and Table <xref rid="Tab1" ref-type="table">1</xref>) using two ground-truth annotation datasets to capture known protein localization at two different scales: the first is a manually curated list of proteins with unique organelle-level localizations (Supplementary File <xref rid="MOESM1" ref-type="media">2</xref>), whereas the second is a list of proteins participating in stable protein complexes derived from the CORUM database<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> (Supplementary File <xref rid="MOESM1" ref-type="media">3</xref>). While the first ground-truth dataset helps us assess how well our encodings cluster together proteins belonging to the same organelles, the second helps us assess whether proteins interacting within the same complex—and thus functionally related—are in proximity. We compared cytoself to other previously developed unsupervised (CellProfiler<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>) or self-supervised (Cell inpainting<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>) approaches for image featurization. We applied these methods to the OpenCell image dataset and then compared the results to that obtained by cytoself. UMAPs were calculated for each model (<xref rid="Sec15" ref-type="sec">Methods</xref>) and compared with our set of ground-truth organelles and protein complexes. As can be seen in Extended Data Figs. <xref rid="Fig1" ref-type="fig">1</xref> and <xref rid="Fig1" ref-type="fig">2</xref> and Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>, the resolution obtained by cytoself exceeded that of both previous approaches. This was also apparent in our calculations of clustering scores (Fig. <xref rid="Fig4" ref-type="fig">4</xref> and Table <xref rid="Tab1" ref-type="table">1</xref>).<fig id="Fig4"><label>Fig. 4</label><caption><title>Clustering performance comparison.</title><p>For each model variation, we trained five model instances, compute UMAPs for ten random seeds, compute clustering scores using organelle- and protein-complex-level ground truth and then report the mean and standard error of the mean.</p></caption><graphic xlink:href="41592_2022_1541_Fig4_HTML" id="d32e649"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption><p>Clustering performance comparison</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Approach</th><th>Organelle level</th><th>Complex level</th></tr></thead><tbody><tr><td>Cytoself full model</td><td>3.41 ± 0.18</td><td>5.96 ± 0.25</td></tr><tr><td> Without nuclear channel</td><td>3.35 ± 0.23</td><td>5.38 ± 0.19</td></tr><tr><td> Without distance transform</td><td>3.17 ± 0.18</td><td>4.90 ± 0.13</td></tr><tr><td> Without vector quantization</td><td>2.98 ± 0.14</td><td>4.46 ± 0.15</td></tr><tr><td> Without id. pretext task</td><td>1.13 ± 0.094</td><td>1.26 ± 0.062</td></tr><tr><td> Without split quantization</td><td>2.85 ± 0.20</td><td>5.04 ± 0.16</td></tr><tr><td> Without decoder</td><td>2.98 ± 0.17</td><td>4.48 ± 0.12</td></tr><tr><td>Lu et al. (conv3_1)</td><td>2.19 ± 0.097</td><td>2.67 ± 0.045</td></tr><tr><td>Lu et al. (conv4_1)</td><td>2.33 ± 0.11</td><td>2.88 ± 0.10</td></tr><tr><td>Lu et al. (conv5_1)</td><td>2.91 ± 0.18</td><td>3.06 ± 0.084</td></tr><tr><td>CellProfiler</td><td>0.129 ± 0.013</td><td>0.124 ± 0.0074</td></tr></tbody></table><table-wrap-foot><p>Our full model surpasses all variants considered, the previously reported cell-inpainting model<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> and CellProfiler derived representations<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. We trained the models five times, computed ten different UMAPs, computed clustering scores using organelle- and protein-complex-level ground truth, and then report the mean and standard error of the mean (mean ± s.e.m.). For the latent representations in the inpainting model, we examined the three network layers discussed in Lu et al. to produce image representations for UMAP. Note that our approach works with a single fluorescence channel whereas the approach by Lu et al. needs at least two channels.</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec8">
      <title>Identifying cytoself’s essential components</title>
      <p id="Par12">To evaluate the impact of different aspects of our model on its clustering performance, we conducted an ablation study. We retrained our model and recomputed protein localization UMAPs after individually removing each component or input of our model (Extended Data Figs. <xref rid="Fig3" ref-type="fig">3</xref> and <xref rid="Fig4" ref-type="fig">4</xref>), including: (1) the nuclear fiducial channel, (2) the distance transform applied to nuclear fiducial channel, (3) the split vector quantization and (4) the identification pretext task. We also quantitatively evaluated the effects of their ablation by computing clustering scores for different variants (Fig. <xref rid="Fig4" ref-type="fig">4</xref> and Table <xref rid="Tab1" ref-type="table">1</xref>). The UMAP results and scores from both sets of ground-truth labels make it clear that the single most important component of cytoself, in terms of clustering performance, is the protein identification pretext task. The remaining components—the nuclear channel, split quantization, vector quantization and so on—are important but not crucial. Forgoing the fiducial nuclear channel entirely led to the smallest decrease in clustering score, suggesting that our approach works well even in the absence of any fiducial marker—a notable advantage that widens the applicability of our approach and greatly simplifies the experimental design<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. Overall, our data show a robust fit with ground truth. In conclusion, although all features contribute to the overall performance of our model, the identification pretext task is the key and necessary ingredient.</p>
    </sec>
    <sec id="Sec9">
      <title>Revealing unannotated protein localization</title>
      <p id="Par13">The key advantage of self-supervised approaches is that they are not limited by the quality, completeness or granularity of human annotations. To demonstrate this, we asked whether cytoself could resolve subtle localization differences that are not present in image-derived manual annotations: focusing on proteins localized to intracellular vesicles. Even though several known subcategories of vesicles exist (for example, lysosomes versus endosomes), in both OpenCell and Human Protein Atlas annotations, these groups are annotated simply as ‘vesicles’. This reflects the difficulty for human curators to accurately distinguish and classify localization subcategories that present similarly in the images. To test whether our self-supervised approach manages to capture these subcategories, we focused on a curated list of endosomal as well as lysosomal proteins identified by an objective criterion. Specifically, we selected proteins annotated as lysosomal (GO 000576500) or endosomal (GO 0031901) in Uniprot<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> (excluding targets annotated to reside in both compartments), and for which localization in each compartment has been confirmed independently by mass spectrometry<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup>. As shown in Extended Data Fig. <xref rid="Fig5" ref-type="fig">5</xref>, the representation of the lysosomal versus endosomal images derived from cytoself form two distinct, well-separated clusters (<italic>P</italic> &lt; 10<sup>−3</sup>, Mann–Whitney <italic>U-</italic>test). This demonstrates that self-supervised approaches are not limited by ground-truth annotations and can reveal subtle differences in protein localization not explicitly present in existing databases.</p>
    </sec>
    <sec id="Sec10">
      <title>Extracting feature spectra for quantitative analysis</title>
      <p id="Par14">Cytoself can generate a highly resolved map of protein localization on the basis of distilled image representations. Can we dissect and understand the features that make up these representations and interpret their meaning? To identify and better define the features that make up these representations, we created a feature spectrum of the main components contributing to each protein’s localization encoding. The spectra were constructed by calculating the histogram of codebook feature indices from the local representations in Fig. <xref rid="Fig1" ref-type="fig">1</xref> (see Extended Data Fig. <xref rid="Fig6" ref-type="fig">6</xref> and <xref rid="Sec15" ref-type="sec">Methods</xref> for details). To group related and possibly redundant features together, we performed hierarchical biclustering<sup><xref ref-type="bibr" rid="CR43">43</xref></sup> (Fig. <xref rid="Fig5" ref-type="fig">5a</xref>), and thus obtained a meaningful linear ordering of features by which the spectra can be sorted. This analysis reveals feature clusters of which we manually selected 11 from the top levels of the feature hierarchy (Fig. <xref rid="Fig5" ref-type="fig">5a</xref>, bottom and Supplementary Fig. <xref rid="MOESM1" ref-type="media">3</xref>).<fig id="Fig5"><label>Fig. 5</label><caption><title>Feature spectral analysis.</title><p><bold>a</bold>, Features in the local representation are reordered by hierarchical clustering to form a feature spectra (Extended Data Fig. <xref rid="Fig6" ref-type="fig">6</xref>). The color bar indicates the strength of correlation. Negative values indicate anti-correlation. On the basis of the feature clustering, we manually identified 11 primary top-level clusters, which are illustrated with representative images (Supplementary Fig. <xref rid="MOESM1" ref-type="media">3</xref>). Those images have the highest occurrence of the corresponding features. <bold>b</bold>, Average feature spectrum for each unique localization family. Occurrence indicates how many times a quantized vector is found in the local representation of an image. All spectra, as well as the heatmap, are vertically aligned. <bold>c</bold>, The feature spectrum of FAM241A, a poorly characterized orphan protein. <bold>d</bold>, Correlation between FAM241A and other unique localization categories. The highest correlation is 0.777 with ER, next is 0.08 with cytoplasm. <bold>e</bold>, Experimental confirmation of the ER localization of FAM241A. The localization of FAM241A to the ER is experimentally confirmed by coexpression of a classical ER marker (mCherry fused to the SEC61B transmembrane domain, left) in FAM241A-mNeonGreen endogenously tagged cells (right). The ER marker is expressed using transient transfection. As a consequence, not all cells are transfected and levels of expression may vary. Scale bars, 10 μm.</p></caption><graphic xlink:href="41592_2022_1541_Fig5_HTML" id="d32e903"/></fig></p>
      <p id="Par15">Representative images from each cluster illustrate the variety of distinctive localization patterns that are present at different levels across all proteins. For example, the features in the first clusters (i, ii, iii and iv) correspond to a wide range of diffuse cytoplasmic localizations. Cluster v features are unique to nucleolar proteins. Features making up cluster vi correspond to very small and bright punctate structures that are often characteristic of centrosomes, vesicles or cytoplasmic condensates. Clusters vii, viii and x correspond to different types of nuclear localization pattern. Cluster ix are dark features corresponding to nonfluorescent background regions. Finally, cluster xi corresponds to a large variety of more abundant, punctate structures occurring throughout the cells, primarily vesicular, but also Golgi, mitochondria, cytoskeleton and subdomains of the ER. For a quantitative evaluation, we computed the average feature spectrum for all proteins belonging to each localization category present in our reference set of manual annotations (for example, Golgi, nucleolus and so on; Fig. <xref rid="Fig5" ref-type="fig">5b</xref> and Supplementary File <xref rid="MOESM1" ref-type="media">4</xref>). This analysis confirms that certain spectral clusters are specific to certain localization categories and thus correspond to characteristic textures and patterns in the images. For example, the highly specific chromatin and mitochondrial localizations both appear to elicit very narrow responses in their feature spectra.</p>
    </sec>
    <sec id="Sec11">
      <title>Predicting protein organelle localization with cytoself</title>
      <p id="Par16">We next asked whether feature spectra could be used to predict the localizations of proteins not present in our training data. For this purpose, we computed the feature spectrum of FAM241A: a protein of unknown function that was not present in the training dataset. Its spectrum is most correlated to the consensus spectrum of proteins belonging to the ER (Fig. <xref rid="Fig5" ref-type="fig">5b–d</xref> and Supplementary Fig. <xref rid="MOESM1" ref-type="media">4</xref>). Indeed, FAM241A’s localization to the ER was validated experimentally by coexpression experiments showing that endogenously tagged FAM241A colocalizes with an ER marker (Fig. <xref rid="Fig5" ref-type="fig">5e</xref>). In a companion study<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, we further validated by mass spectrometry that FAM241A is in fact a new subunit of the oligosaccharyltransferase complex, responsible for cotranslational glycosylation at the ER membrane. Our successful prediction of the localization of FAM241A suggests that cytoself encodings can be used more generally to predict organelle-level localization categories. To demonstrate this, we focused on proteins annotated to localize to a single organelle (that is, not multi-localizing, Supplementary File <xref rid="MOESM1" ref-type="media">4</xref>). For each of these proteins, we recomputed the representative spectra for each of their known localization categories (that is, ER, mitochondria, Golgi and so on), but leaving out that protein, and then applied the same spectral correlation as described for FAM241A. This allows us to predict the protein’s localization by identifying the organelle with which its spectrum correlates best. Extended Data Fig. <xref rid="Fig7" ref-type="fig">7</xref> shows the accuracy of the predictions derived from this approach: for 88% of proteins, the spectra correlate best with the correctly annotated organelle. For 96% of proteins, the correct annotation is within the top two predictions and for 99% it is within the top three predictions. Overall, this form of cross-validation verifies the discriminating power of our spectra and shows that the information encoded in each protein’s spectrum can be interpreted to predict subcellular localization.</p>
    </sec>
    <sec id="Sec12">
      <title>Cytoself applicability beyond OpenCell data</title>
      <p id="Par17">Can cytoself make reasonable protein localization predictions for images from datasets other than OpenCell? To answer this question, we chose data from the Allen Institute Cell collection<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>, which also uses endogenous tagging and live-cell imaging, making their image data directly comparable to ours. The Allen collection uses a cell line (WTC11, induced pluripotent stem cell) whose overall morphology is very different from the cell line used for OpenCell (human embryonic kidney 293T: HEK293T). We reasoned that if cytoself manages to capture true features of protein localization, a compelling validation would be that its performance would be cell-type agnostic. Indeed, localization encodings for images from the Allen dataset generated by a cytoself model trained only on OpenCell images revealed a strong concordance between the embeddings of the same (or closely related) protein that were imaged in both cell datasets (Extended Data Fig. <xref rid="Fig8" ref-type="fig">8a</xref>). This shows that our model manages to predict protein localization even under conditions that were not directly included for training. To facilitate comparison, we focused on the intersection set of nine proteins found in both the OpenCell and Allen datasets (Extended Data Fig. <xref rid="Fig8" ref-type="fig">8b</xref>). We ran the same organelle localization prediction task and observed that in 88% (eight out of nine) of cases the correct localization is among the top three predictions (Supplementary Fig. <xref rid="MOESM1" ref-type="media">5</xref>).</p>
    </sec>
    <sec id="Sec13">
      <title>Hypothesizing protein-complex membership from images</title>
      <p id="Par18">The resolving power of our approach is further illustrated by examining known stable protein complexes, which are found to form well delineated clusters in our localization UMAP (see examples highlighted in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, dashed line). Fluorescent images of 11 representative subunits from these complexes illustrate these discrete localization patterns (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>). To substantiate these observations quantitatively, we computed the correlation of feature spectra between any two pairs of proteins in our dataset. This showed a significantly higher correlation for protein pairs annotated to belong to the same complex in CORUM compared to pairs that are not (<italic>P</italic> &lt; 10<sup>−10</sup>, Mann–Whitney <italic>U</italic>-test; Supplementary Fig. <xref rid="MOESM1" ref-type="media">6a</xref>). To further evaluate the relationship between proximity in feature space and protein-complex membership, we examine the proportion of proteins in OpenCell that share complex membership with their most-correlated neighboring protein (Supplementary Fig. <xref rid="MOESM1" ref-type="media">6b</xref>). We find that 83% of highly correlated (&gt;0.95) neighbor proteins are in the same complex, and even more weakly correlated (&gt;0.8) proteins are localized to complexes 60% of the time. These results confirm that close proximity in feature space is highly indicative of protein-complex membership and suggests that the features derived by cytoself contain fine-grained information related to very specific functional relationships.</p>
    </sec>
  </sec>
  <sec id="Sec14" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par19">We have shown that a self-supervised training scheme can produce image representations that capture the organization of protein subcellular localization (Fig. <xref rid="Fig2" ref-type="fig">2</xref>), solely on the basis of a large high-quality dataset of fluorescence images. Our model generates a high-resolution localization atlas capable of delineating not only organelles, but also protein complexes. Moreover, we can represent each image with a feature spectrum to better analyze the repertoire of localization patterns present in our data. Since a protein’s localization is highly correlated with its cellular function, cytoself will be an invaluable tool to make preliminary functional predictions for unknown or poorly studied proteins, and for quantitatively studying the effect of cellular perturbations and cell state changes on protein subcellular localization.</p>
    <p id="Par20">Our method makes few assumptions, but imposes two pretext tasks (that is, image and protein identity). Of these, requiring the model to identify proteins based solely on their localization encodings was essential. We also included Hoechst DNA-staining as a fiducial marker, assuming that this would provide a spatial reference frame against which to interpret localization. However, this added little to the performance of our model in terms of clustering score. By comparison, the self-supervised approach by Lu et al.<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> applied a pretext task that predicts the fluorescence signal of a labeled protein in one cell from its fiducial markers and from the fluorescence signal in a second, different cell from the same FOV. This assumes that fiducial channels are available, and that protein fluorescence is always well-correlated to these fiducials. In contrast, our approach only requires a single fluorescence channel and yields better clustering performance (Fig. <xref rid="Fig4" ref-type="fig">4</xref> and Table <xref rid="Tab1" ref-type="table">1</xref>).</p>
    <p id="Par21">The main difference between our work and the problem addressed by the Human Protein Atlas Image Classification competition<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> is that we do not aim to predict localization patterns on the basis of manual annotations. Instead, we aim to discover de novo the landscape of possible protein localizations. This frees us from the limitations of these annotations that include: lack of uniform coverage, uneven annotation granularity, human perceptive biases and existing preconceptions on the architecture of the cell. This also circumvents the time-intensive efforts required to manually annotate images.</p>
    <p id="Par22">While powerful, there remain a few avenues for further development of cytoself. For example, we trained our model using two-dimensional (2D) maximum-intensity <italic>z</italic>-projections and have not yet leveraged the full three-dimensional (3D) confocal images available in the OpenCell<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> dataset. The third dimension might confer an advantage for specific protein localization patterns that are characterized by specific variations along the basal-apical cell axis. Other important topics to explore are the automatic suppression of residual batch effects, improved cell segmentation via additional fiducial channels, use of label-free imaging modalities, as well as automatic rejection of anomalous or uncharacteristic cells from our training dataset. More fundamentally, notable conceptual improvements will require an improved self-supervised model that explicitly disentangles cellular heterogeneity from localization diversity<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>.</p>
    <p id="Par23">More generally, our ability to generate data is outpacing the human ability to manually annotate it. Moreover, there is already ample evidence that abundance of image data has a quality all its own: Increasing the size of an image dataset often has a higher impact on performance than improving the algorithm itself<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. We envision that self-supervision will be a powerful tool to handle the large amount of data produced by new instruments, end-to-end automation and high-throughput image-based assays.</p>
  </sec>
  <sec id="Sec15">
    <title>Methods</title>
    <sec id="Sec16">
      <title>Fluorescence image dataset</title>
      <p id="Par24">All experimental and imaging details can be found in our companion study<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. Briefly, HEK293T cells were genetically tagged with split-fluorescent proteins using CRISPR-based techniques<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. After nuclear staining with Hoechst 33342, live cells were imaged with a spinning-disk confocal microscope (Andor Dragonfly). Typically, 18 FOV were acquired for each one of the 1,311 tagged proteins, for a total of 24,382 three-dimensional images of dimension 1,024 × 1,024 × 22 voxels.</p>
    </sec>
    <sec id="Sec17">
      <title>Image data preprocessing</title>
      <p id="Par25">Each 3D confocal image was first reduced to two dimensions using a maximum-intensity projection along the <italic>z</italic> axis followed by downsampling in the <italic>xy</italic> dimensions by a factor of two to obtain a single 2D image per FOV (512 × 512 pixels). To help our model make use of the nuclear fiducial label, we applied a distance transform to a nucleus segmentation mask (below). The distance transform is constructed so that pixels within the nucleus were assigned a positive value that represents the shortest distance from the pixel to the nuclear boundary, and pixel values outside the nucleus were assigned a negative value that represents the shortest distance to the nuclear boundary (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>). For each dual-channel and full FOV image, multiple regions of dimension 100 × 100 pixels were computationally chosen so that at least one cell is present and centered, resulting in a total of 1,100,253 cropped images. Cells (and their nuclei) that are too close to image edges are ignored. The raw pixel intensities in the fluorescence channel are normalized between 0 and 1, and the nuclear distance channel is normalized between −1 and 1.</p>
    </sec>
    <sec id="Sec18">
      <title>Nucleus segmentation</title>
      <p id="Par26">Nuclei are segmented by first thresholding the nucleus channel (Hoechst staining) and then applying a custom algorithm to segment any under-segmented nuclei. In the thresholding step, a background mask is generated by applying a low-pass Gaussian filter to the raw image, then thresholding it using a threshold value calculated by the iterative Minimum Cross Entropy method<sup><xref ref-type="bibr" rid="CR48">48</xref>,<xref ref-type="bibr" rid="CR49">49</xref></sup>. Under-segmented nuclei in the resulting mask are then segmented by applying the following steps: (1) we generate a second mask by applying a Laplacian of the Gaussian (LoG) filter to the original image, thresholding it at zero, and multiplying it by the background mask from the intensity thresholding step, (2) we morphologically close this second mask and fill holes to eliminate intra-nuclear holes or gaps (empirically, this requires a closing disk of radius at least 4 pixels), (3) we multiply the second mask again by the background mask to restore any true morphological holes that were present in the background mask, (4) we generate a mask of the local minima in the original LoG-filtered image using an empirically selected percentile threshold, and finally (5) we iterate over regions in this local-minima mask and remove them from the second mask if they partially overlap with the background of the refined mask. The second mask is then the final nucleus segmentation mask.</p>
    </sec>
    <sec id="Sec19">
      <title>Detailed model architecture</title>
      <p id="Par27">All details of our model architecture are given in Supplementary File <xref rid="MOESM1" ref-type="media">1</xref> and a diagram is shown in Fig. <xref rid="Fig1" ref-type="fig">1b</xref>. First, the input image (100 × 100 × 2 pixels) is fed to encoder1 to produce a set of latent vectors that have two destinations: encoder2 and VQ1 VectorQuantizer layer. In the encoder2, higher level representations are distilled from these latent vectors and passed to the output. The output of encoder2 is quantized in the VQ2 VectorQuantizer layer to form what we call ‘global representation’. The global representation is then passed to the fc2 classifier for purposes of the classification pretext task. It is also passed on to decoder2 to reconstruct the input data of encoder2. In this way, encoder2 and decoder2 form an independent autoencoder. The function of layer mselyr1 is to adapt the output of decoder2 to match the dimensions of the output of encoder1, which is identical to the dimensions of the input of encoder2. In the case of the VQ1 VectorQuantizer layer, vectors are quantized to form what we call the local representations. The local representation is then passed to the fc1 classifier for purposes of the classification pretext task, as well as concatenated to the global representation that is resized to match the local representations’ dimensions. The concatenated result is then passed to the decoder1 to reconstruct the input image. Here, encoder1 and decoder1 form another autoencoder.</p>
    </sec>
    <sec id="Sec20">
      <title>Split quantization</title>
      <p id="Par28">In the case of our global representation, we observed that the high level of spatial pooling required (4 × 4 pixels) led to codebook under-use because the quantized vectors are too few and each one of them has too many dimensions (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>). To solve this challenge, we introduced the concept of split quantization. Instead of quantizing all the dimensions of a vector at once, we first split the vectors into subvectors of equal length and then quantize each subvectors using a shared codebook. The main advantage of split quantization when applied to the VQ-VAE architecture is that one may vary the degree of spatial pooling without changing the total number of quantized vectors per representation. In practice, to maintain the number of quantized vectors while increasing spatial pooling, we simply split along the channel dimension. We observed that the global representations’ perplexity, which indicates the level of use of the codebook, substantially increases when split quantization is used compared to standard quantization (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>). As shown in Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>, split quantization is performed along the channel dimension by splitting each channel-wise vector into nine parts, and quantizing each of the resulting ‘subvectors’ against the same codebook. Split quantization is only needed for the global representation.</p>
    </sec>
    <sec id="Sec21">
      <title>Global and local representations</title>
      <p id="Par29">The dimensions of the global and local representations are 4 × 4 × 576 and 25 × 25 × 64 voxels, respectively. These two representations are quantized with two separate codebooks consisting of 2,048 64-dimensional features (or codes).</p>
    </sec>
    <sec id="Sec22">
      <title>Identification pretext task</title>
      <p id="Par30">The part of our model that is tasked with identifying a held-back protein is implemented as a two-layer perceptron built by alternatively stacking fully connected layers with 1,000 hidden units and nonlinear ReLU layers. The output of the classifier is a one-hot encoded vector for which each coordinate corresponds to one of the 1,311 proteins. We use categorical cross entropy as classification loss during training.</p>
    </sec>
    <sec id="Sec23">
      <title>Computational efficiency</title>
      <p id="Par31">Due to the large size of our image data (1,100,253 cropped images of dimensions 100 × 100 × 2 pixels) we recognized the need to make our architecture more efficient and thus allow for more design iterations. We opted to implement the encoder using principles from the EfficientNet architecture to increase computational efficiency without losing learning capacity<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. Specifically, we split the model of EfficientNetB0 into two parts to make the two encoders in our model (Supplementary File <xref rid="MOESM1" ref-type="media">1</xref>). While we did not notice a loss of performance for the encoder, EfficientNet did not perform as well for decoding. Therefore, we opted to keep a standard architecture based on a stack of residual blocks for the decoder<sup><xref ref-type="bibr" rid="CR51">51</xref></sup></p>
    </sec>
    <sec id="Sec24">
      <title>Training protocol</title>
      <p id="Par32">The whole dataset (1,100,253 cropped images) was split into 8:1:1 into training, validation and testing data, respectively. All results shown in the figures are from testing data. We used the Adam optimizer with the initial learning rate of 0.0004. The learning rate was multiplied by 0.1 every time the validation loss did not improve for four epochs, and the training was terminated when the validation loss did not improve for more than 12 consecutive epochs. Images were augmented by random rotation and flipping in the training phase.</p>
    </sec>
    <sec id="Sec25">
      <title>Dimensionality reduction and clustering</title>
      <p id="Par33">Dimensionality reduction is performed using the UMAP<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> algorithm. We used the reference open-source python package umap-learn (v.0.5.0) with default values for all parameters (that is, the Euclidean distance metric, 15 nearest neighbors and a minimal distance of 0.1). We used AlignedUMAP for the clustering performance evaluation to facilitate the comparison of the different projections derived from three variants of the previously described cell-inpainting model<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> (Extended Data Figs. <xref rid="Fig1" ref-type="fig">1</xref> and <xref rid="Fig2" ref-type="fig">2</xref>) or all seven variants of our model (Extended Data Figs. <xref rid="Fig3" ref-type="fig">3</xref> and <xref rid="Fig4" ref-type="fig">4)</xref>. Hierarchical biclustering was performed using seaborn (v.0.11.1) with its default settings.</p>
    </sec>
    <sec id="Sec26">
      <title>Ground-truth labels for localization</title>
      <p id="Par34">To evaluate the clustering performance, we used two sets of ground-truth labels at two different cellular scales: a manually curated list of proteins with exclusive organelle-level localization patterns (Supplementary File <xref rid="MOESM1" ref-type="media">2</xref>) and 38 protein complexes collected from the CORUM database<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> (Supplementary File <xref rid="MOESM1" ref-type="media">3</xref>). The 38 protein complexes were collected based on the following conditions: (1) all subunits are present in the OpenCell data, (2) no overlapping subunit across the complexes and (3) each protein complex consists of more than one distinct subunit.</p>
      <p id="Par35">For the evaluation of feature spectra, we simply extracted the proteins with single-localization annotation based on the localization annotation given by the OpenCell database (Supplementary File <xref rid="MOESM1" ref-type="media">4</xref>).</p>
    </sec>
    <sec id="Sec27">
      <title>Clustering score</title>
      <p id="Par36">To calculate a clustering score, we assume a collection of <italic>n</italic> points (vectors) in <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathbb{R}}}^{m}$$\end{document}</tex-math><mml:math id="M2"><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41592_2022_1541_Article_IEq1.gif"/></alternatives></inline-formula>, <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S=\{{\bf x}_{i}\in {{\mathbb{R}}}^{m}| 0\le i\le n\}$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>∣</mml:mo><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41592_2022_1541_Article_IEq2.gif"/></alternatives></inline-formula>, and that we have a (ground truth) assignment of each point <bold>x</bold><sub><italic>i</italic></sub> to a class <italic>C</italic><sub><italic>j</italic></sub>, and these classes form a partition of <italic>S</italic>:<disp-formula id="Equa"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S=\mathop{\bigcup}\limits_{j}{C}_{j}$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>⋃</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><graphic xlink:href="41592_2022_1541_Article_Equa.gif" position="anchor"/></alternatives></disp-formula>Ideally, the vectors <bold>x</bold><sub><italic>i</italic></sub> are such that all points in a class are tightly grouped together, and that the centroids of each class are as far apart from each other as possible. This intuition is captured in the following definition of our clustering score:<disp-formula id="Equb"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\Gamma }}({C}_{i})=\frac{{\sigma }^{* }({\{{\mu }^{* }({C}_{j})\}}_{j})}{{\mu }^{* }({\{{\sigma }^{* }({C}_{j})\}}_{j})}$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41592_2022_1541_Article_Equb.gif" position="anchor"/></alternatives></disp-formula>where {.}<sub><italic>k</italic></sub> denotes the set of values obtained by evaluating the expression for each value of parameter <italic>k</italic>, and where <italic>μ</italic><sup>*</sup> and <italic>σ</italic><sup>*</sup> stand for the robust mean (median) and robust standard deviation (computed using medians). Variance statistics were obtained by training the model variant five times followed by computing the UMAP ten times per trained model.</p>
    </sec>
    <sec id="Sec28">
      <title>Feature spectrum</title>
      <p id="Par37">Extended Data Fig. <xref rid="Fig6" ref-type="fig">6a</xref> illustrates the workflow for constructing the feature spectra. Specifically, we first obtain the indices of quantized vectors in the latent representation for each image crop, and then calculate the histogram of indices in all images of each protein. As a result, we obtain a matrix of histograms in which rows correspond to protein identification (ID) and columns to the feature indices (Extended Data Fig. <xref rid="Fig6" ref-type="fig">6b</xref>). At this point, the order of the columns (that is, the feature indices) is arbitrary. Yet, different features might be highly correlated and thus either related or even redundant (depending on how ‘saturated’ the codebook is). To meaningfully order the feature indices, we compute the Pearson correlation coefficient between the feature index ‘profiles’ (the columns of the matrix) for each pair of feature indices to obtain a 2,048 × 2,048 pairwise correlation matrix (Extended Data Fig. <xref rid="Fig6" ref-type="fig">6c</xref>). Next, we perform hierarchical biclustering in which the feature indices with the most similar profiles are iteratively merged<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. The result is that features that have similar profiles are grouped together (Extended Data Fig. <xref rid="Fig6" ref-type="fig">6d</xref>). This ordering yields a more meaningful and interpretable view of the whole spectrum of feature indices. We identified several clusters from the top levels of the feature hierarchy and manually segment them into 11 major feature clusters (ordered i to xi). Finally, for a given protein, we can produce a interpretable feature spectrum by ordering the horizontal axis of the quantized vectors histogram in the same way.</p>
    </sec>
    <sec id="Sec29">
      <title>Training cell-inpainting model on OpenCell data</title>
      <p id="Par38">The cell-inpainting model was constructed using the code provided by its original authors (<ext-link ext-link-type="uri" xlink:href="https://github.com/alexxijielu/paired_cell_inpainting">https://github.com/alexxijielu/paired_cell_inpainting</ext-link>). The whole dataset was split into training, validation and testing sets (8:1:1). All results shown in the figures are computed on the basis of the test set. We used the Adam optimizer with the initial learning rate of 0.0004. The learning rate was multiplied by 0.1 every time the validation loss did not improve for four epochs, and the training was terminated when the validation loss did not improve for more than 12 consecutive epochs. The features to generate UMAP were extracted from layers denoted as ‘conv3_1’, ‘conv4_1’ and ‘conv5_1’ by the authors.</p>
    </sec>
    <sec id="Sec30">
      <title>Applying cytoself on the Allen Institute dataset</title>
      <p id="Par39">Image data from the Allen Institute were downloaded from <ext-link ext-link-type="uri" xlink:href="https://www.allencell.org/data-downloading.html#DownloadImageData">https://www.allencell.org/data-downloading.html#DownloadImageData</ext-link>. Patches were made following the same procedure as for the OpenCell dataset including max-intensity projection and downsampling to match pixel resolutions. Nuclear centers were determined using the nuclear label included in the Allen Institute dataset. We randomly selected 80 patches per protein and used these for analysis.</p>
    </sec>
    <sec id="Sec31">
      <title>Feature extraction with CellProfiler</title>
      <p id="Par40">CellProfiler v.4.2.1 was used to extract features from nuclear images (without distance transform) and fluorescence protein images. In the case of cytoself, we computed all features compatible to the data including texture features up to scale 15, for a total of 1,397 features that required 2 days of computation. Only features that did not require object detection were used, including granularity, texture and the correlations between the two channels. Each feature was standardized by subtracting its mean followed by dividing by its standard deviation.</p>
    </sec>
    <sec id="Sec32">
      <title>Evaluation of feature correlation against protein complex</title>
      <p id="Par41">The Pearson correlation between any two proteins found in both the OpenCell and CORUM databases were computed with their feature spectra as the proximity metrics in the feature space. For each protein, we found the ‘nearest protein’ with which it had the highest correlation, and incremented the number if the correlation was higher than a given threshold, and if both of them shared at least one complex in the CORUM database. To take into account the strength of correlation, we varied the minimal correlation threshold thus obtaining the curve shown in Supplementary Fig. <xref rid="MOESM1" ref-type="media">6b</xref>.</p>
    </sec>
    <sec id="Sec33">
      <title>Statistics and reproducibility</title>
      <p id="Par42">All box plots were generated using matplotlib (v.3.4.2). Each box indicates the extent from the first to the third quartile of the data, with a line representing the median. The whiskers indicate 1.5 times the interquartile range. Scipy (v.1.8.0) was used to compute <italic>P</italic> values and Pearson’s correlations.</p>
    </sec>
    <sec id="Sec34">
      <title>Software and hardware</title>
      <p id="Par43">All deep-learning architectures were implemented in TensorFlow v.1.15 (ref. <sup><xref ref-type="bibr" rid="CR54">54</xref></sup>) on Python v.3.7. Training was performed on NVIDIA V100-32GB GPUs.</p>
    </sec>
    <sec id="Sec35">
      <title>Reporting summary</title>
      <p id="Par44">Further information on research design is available in the <xref rid="MOESM2" ref-type="media">Nature Research Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec id="Sec36" sec-type="materials|methods">
    <title>Online content</title>
    <p id="Par45">Any methods, additional references, Nature Research reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at 10.1038/s41592-022-01541-z.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <p>
      <supplementary-material content-type="local-data" id="MOESM1">
        <media xlink:href="41592_2022_1541_MOESM1_ESM.pdf">
          <label>Supplementary Information</label>
          <caption>
            <p>Supplementary Text, Figs. 1–9 and Files 1–5.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM2">
        <media xlink:href="41592_2022_1541_MOESM2_ESM.pdf">
          <caption>
            <p>Reporting Summary.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM3">
        <media xlink:href="41592_2022_1541_MOESM3_ESM.zip">
          <label>Supplementary Data</label>
          <caption>
            <p><bold>a</bold>–<bold>e</bold>, Detailed structure of VQ-VAE model, including the whole model structure (<bold>a</bold>), the structure of encoder1 (<bold>b</bold>), the structure of encoder2 (<bold>c</bold>), the structure of decoder1 (<bold>d</bold>) and the structure of decoder2 (<bold>e</bold>).</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM4">
        <media xlink:href="41592_2022_1541_MOESM4_ESM.csv">
          <label>Supplementary Table</label>
          <caption>
            <p>The ground truth used for evaluating clustering performance at organelle level.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM5">
        <media xlink:href="41592_2022_1541_MOESM5_ESM.csv">
          <label>Supplementary Table</label>
          <caption>
            <p>A list of protein subunits collected from CORUM as a ground truth to compute clustering scores. See Methods for how they were selected.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM6">
        <media xlink:href="41592_2022_1541_MOESM6_ESM.csv">
          <label>Supplementary Table</label>
          <caption>
            <p>The ground truth used for evaluating feature spectra.</p>
          </caption>
        </media>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="MOESM7">
        <media xlink:href="41592_2022_1541_MOESM7_ESM.csv">
          <label>Supplementary Table</label>
          <caption>
            <p>A list of protein subunits for protein complexes mentioned in Figs. 2 and 3b.</p>
          </caption>
        </media>
      </supplementary-material>
    </p>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec37">
        <title>Extended data</title>
        <p id="Par50">
          <fig id="Fig6">
            <label>Extended Data Fig. 1</label>
            <caption>
              <title>Comparing the UMAP representations between <italic>c</italic><italic>y</italic><italic>t</italic><italic>o</italic><italic>s</italic><italic>e</italic><italic>l</italic><italic>f</italic> and cell-inpainting annotated with organelle-level ground truth.</title>
              <p>Aligned UMAPs are given to aid visual comparison.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1541_Fig6_ESM" id="d32e1633"/>
          </fig>
        </p>
        <p id="Par51">
          <fig id="Fig7">
            <label>Extended Data Fig. 2</label>
            <caption>
              <title>Comparing the UMAP representations between <italic>c</italic><italic>y</italic><italic>t</italic><italic>o</italic><italic>s</italic><italic>e</italic><italic>l</italic><italic>f</italic> and cell-inpainting annotated with protein-complex-level ground truth.</title>
              <p>Aligned UMAPs are given to aid visual comparison.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1541_Fig7_ESM" id="d32e1664"/>
          </fig>
        </p>
        <p id="Par52">
          <fig id="Fig8">
            <label>Extended Data Fig. 3</label>
            <caption>
              <title>Identifying the essential components of our model with organelle-level ground truth.</title>
              <p>Protein localization UMAPs are derived after removing each components of our model separately. Aligned UMAPs are given to aid visual comparison.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1541_Fig8_ESM" id="d32e1678"/>
          </fig>
        </p>
        <p id="Par53">
          <fig id="Fig9">
            <label>Extended Data Fig. 4</label>
            <caption>
              <title>Identifying the essential components of our model with protein-complex-level ground truth.</title>
              <p>Protein localization UMAPs are derived after removing each components of our model separately. Aligned UMAPs are given to aid visual comparison.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1541_Fig9_ESM" id="d32e1692"/>
          </fig>
        </p>
        <p id="Par54">
          <fig id="Fig10">
            <label>Extended Data Fig. 5</label>
            <caption>
              <title>cytoself discriminates between lysosomal and endosomal proteins.</title>
              <p><bold>(a)</bold> We selected 11 proteins in OpenCell annotated in Uniprot as lysosomal or endosomal that are independently confirmed as such by mass spectrometry<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup>. We show that <italic>c</italic><italic>y</italic><italic>t</italic><italic>o</italic><italic>s</italic><italic>e</italic><italic>l</italic><italic>f</italic> is able to distinguish the lysosomal from endosomal proteins solely on the basis of the fluorescence images. All of these proteins are annotated on the basis of the images as ‘vesicles’ in both HPA and OpenCell. The min and max intensities of each image are adjusted to ensure comparable visibility. All representative images were randomly selected from each protein. Scale bar: 10 <italic>μ</italic><italic>m</italic>. <bold>(b)</bold> Clustering of these proteins on the basis of the feature spectra. <bold>(c)</bold> Feature spectra correlations for pairs of lysosomal and endosomal proteins, and for mixed lysosomal-endosomal pairs. Each box indicates the extent from the first to the third quartile of the data, with a line representing the median. The whiskers indicates 1.5 times the inter-quartile range. The p-values are computed using two-sided Mann–Whitney U test.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1541_Fig10_ESM" id="d32e1744"/>
          </fig>
        </p>
        <p id="Par55">
          <fig id="Fig11">
            <label>Extended Data Fig. 6</label>
            <caption>
              <title>Process of constructing feature spectra.</title>
              <p><bold>(a)</bold> First, the quantized vectors in the local representation were extracted and converted to a histogram by counting the occurrence of each quantized vector. <bold>(b)</bold> Next, taking the average of the histograms per protein ID over all the data to create a 2D histogram. <bold>(c)</bold> Pearson’s correlations between any two representation indices were calculated and plotted as a 2D matrix. <bold>(d)</bold> Finally, hierarchical clustering was performed on the correlation map so that similar features are clustered together, revealing the structure inside the local representation. The whole process corresponds to the Spectrum Conversion in Fig. <xref rid="Fig1" ref-type="fig">1a</xref>.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1541_Fig11_ESM" id="d32e1772"/>
          </fig>
        </p>
        <p id="Par56">
          <fig id="Fig12">
            <label>Extended Data Fig. 7</label>
            <caption>
              <title>Predicting the localization category of mono-localized OpenCell proteins by correlating the <italic>cytoself</italic> spectra of each protein with the representative spectra of each category – in a leave-one-out fashion.</title>
              <p>Result: 88% of proteins are correctly classified. For 96% of proteins the correct annotation is within the top 2 predictions, and for 99% it is within the top 3 predictions.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1541_Fig12_ESM" id="d32e1789"/>
          </fig>
        </p>
        <p id="Par57">
          <fig id="Fig13">
            <label>Extended Data Fig. 8</label>
            <caption>
              <title>Visualizing the predicted localization categories of proteins present both in OpenCell and the Allen Institute dataset.</title>
              <p>The <italic>c</italic><italic>y</italic><italic>t</italic><italic>o</italic><italic>s</italic><italic>e</italic><italic>l</italic><italic>f</italic> model is trained only on OpenCell data which is the same <italic>f</italic><italic>u</italic><italic>l</italic><italic>l</italic> model used throughout this work. UMAPs <bold>(a)</bold> and example images <bold>(b)</bold> from OpenCell and Allen Institute datasets for the same or related proteins. The min and max intensities of each image are adjusted to ensure comparable visibility. All representative images were randomly selected. Scale bar 10 <italic>μ</italic><italic>m</italic>. Protein names and contour lines in orange color are from OpenCell dataset, and those in green color are from Allen Institute dataset.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1541_Fig13_ESM" id="d32e1841"/>
          </fig>
        </p>
        <p id="Par58">
          <fig id="Fig14">
            <label>Extended Data Fig. 9</label>
            <caption>
              <title>Interpreting image spectral features. Feature spectra were computed for each example proteins <bold>(a)</bold> POLR2E, <bold>(b)</bold> SEC22B, and <bold>(c)</bold> RPS18.</title>
              <p>Subsequently, information derived from the indicated major peaks of their feature spectra was removed by zeroing them out before passing the features again through the decoder. Highlighted in red are the differences between the resulting output images for the corresponding features and reconstructed image with full features on. The feature classes outlined in Fig. <xref rid="Fig5" ref-type="fig">5</xref> are shown as background color for reference. The pixel intensities are rescaled to the minimum and maximum of each image. Scale bars: 10<italic>μ</italic><italic>m</italic>.</p>
            </caption>
            <graphic position="anchor" xlink:href="41592_2022_1541_Fig14_ESM" id="d32e1872"/>
          </fig>
        </p>
      </sec>
    </app>
  </app-group>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <sec id="FPar1">
      <title>Extended data</title>
      <p id="Par46">are available for this paper at 10.1038/s41592-022-01541-z.</p>
    </sec>
    <sec id="FPar2" sec-type="supplementary-material">
      <title>Supplementary information</title>
      <p id="Par47">The online version contains supplementary material available at 10.1038/s41592-022-01541-z.</p>
    </sec>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank our colleagues at the Chan Zuckerberg Biohub, S. Schmid, M. Bucci, A.C. Solak, B. Yang, M. Lange, S. Vijaykumar, L. Hyman and M. Hein, for insightful discussions, feedback and for reviewing the manuscript. We thank K. Kim for assistance with data analysis. We thank our colleagues M. Wu and J. Zou from Stanford University for advice. We thank A. Lakoduk, J. Bragantini and S. Schmid for reviewing the manuscript and A. C. Solak for helping with coding. Finally, we thank the Japan Society for the Promotion of Science and its overseas research fellowships and the Chan Zuckerberg Biohub and its donors for funding this work.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>H.K., M.D.L. and L.A.R. conceived the piece. H.K. and K.C.C. performed data analysis. All the authors wrote the manuscript and designed the figures.</p>
  </notes>
  <notes notes-type="peer-review">
    <title>Peer review</title>
    <sec id="FPar3">
      <title>Peer review information</title>
      <p id="Par48"><italic>Nature Methods</italic> thanks Assaf Zaritsky and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Primary Handling Editor: Rita Strack, in collaboration with the <italic>Nature Methods</italic> team.</p>
    </sec>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>The image data used in this work are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/royerlab/cytoself">https://github.com/royerlab/cytoself</ext-link>. The CORUM database is available at <ext-link ext-link-type="uri" xlink:href="http://mips.helmholtz-muenchen.de/corum/">http://mips.helmholtz-muenchen.de/corum/</ext-link>. Image data from the Allen Institute are available at <ext-link ext-link-type="uri" xlink:href="https://www.allencell.org/data-downloading.html#DownloadImageData">https://www.allencell.org/data-downloading.html#DownloadImageData.</ext-link></p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>Source code for the models used in this work is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/royerlab/cytoself">https://github.com/royerlab/cytoself.</ext-link></p>
  </notes>
  <notes id="FPar4" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par49">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pepperkok</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ellenberg</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>High-throughput fluorescence microscopy for systems biology</article-title>
        <source>Nat. Rev. Mol. Cell Biol.</source>
        <year>2006</year>
        <volume>7</volume>
        <fpage>690</fpage>
        <lpage>696</lpage>
        <pub-id pub-id-type="doi">10.1038/nrm1979</pub-id>
        <pub-id pub-id-type="pmid">16850035</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chandrasekaran</surname>
            <given-names>SN</given-names>
          </name>
          <name>
            <surname>Ceulemans</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Boyd</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Carpenter</surname>
            <given-names>AE</given-names>
          </name>
        </person-group>
        <article-title>Image-based profiling for drug discovery: due for a machine-learning upgrade?</article-title>
        <source>Nat. Rev. Drug Discov.</source>
        <year>2020</year>
        <volume>20</volume>
        <fpage>145</fpage>
        <lpage>159</lpage>
        <pub-id pub-id-type="doi">10.1038/s41573-020-00117-w</pub-id>
        <pub-id pub-id-type="pmid">33353986</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boutros</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Heigwer</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Laufer</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Microscopy-based high-content screening</article-title>
        <source>Cell</source>
        <year>2015</year>
        <volume>163</volume>
        <fpage>1314</fpage>
        <lpage>1325</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2015.11.007</pub-id>
        <pub-id pub-id-type="pmid">26638068</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abraham</surname>
            <given-names>VC</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Haskins</surname>
            <given-names>JR</given-names>
          </name>
        </person-group>
        <article-title>High content screening applied to large-scale cell biology</article-title>
        <source>Trends Biotechnol.</source>
        <year>2004</year>
        <volume>22</volume>
        <fpage>15</fpage>
        <lpage>22</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tibtech.2003.10.012</pub-id>
        <pub-id pub-id-type="pmid">14690618</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Scheeder</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Heigwer</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Boutros</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Machine learning and image-based profiling in drug discovery</article-title>
        <source>Curr. Opin. Syst. Biol.</source>
        <year>2018</year>
        <volume>10</volume>
        <fpage>43</fpage>
        <lpage>52</lpage>
        <pub-id pub-id-type="doi">10.1016/j.coisb.2018.05.004</pub-id>
        <pub-id pub-id-type="pmid">30159406</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Loo</surname>
            <given-names>L-H</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>LF</given-names>
          </name>
          <name>
            <surname>Altschuler</surname>
            <given-names>SJ</given-names>
          </name>
        </person-group>
        <article-title>Image-based multivariate profiling of drug responses from single cells</article-title>
        <source>Nat. Methods</source>
        <year>2007</year>
        <volume>4</volume>
        <fpage>445</fpage>
        <lpage>453</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth1032</pub-id>
        <pub-id pub-id-type="pmid">17401369</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huh</surname>
            <given-names>W-K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Global analysis of protein localization in budding yeast</article-title>
        <source>Nature</source>
        <year>2003</year>
        <volume>425</volume>
        <fpage>686</fpage>
        <lpage>691</lpage>
        <pub-id pub-id-type="doi">10.1038/nature02026</pub-id>
        <pub-id pub-id-type="pmid">14562095</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cai</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Experimental and computational framework for a dynamic protein atlas of human cell division</article-title>
        <source>Nature</source>
        <year>2018</year>
        <volume>561</volume>
        <fpage>411</fpage>
        <lpage>415</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-018-0518-z</pub-id>
        <pub-id pub-id-type="pmid">30202089</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Thul, P. J. et al. A subcellular map of the human proteome. <italic>Science</italic><bold>356</bold>, aal3321 (2017).</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cho</surname>
            <given-names>NH</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Opencell: endogenous tagging for the cartography of human cellular organization</article-title>
        <source>Science</source>
        <year>2022</year>
        <volume>375</volume>
        <fpage>eabi6983</fpage>
        <pub-id pub-id-type="doi">10.1126/science.abi6983</pub-id>
        <pub-id pub-id-type="pmid">35271311</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>AX</given-names>
          </name>
          <name>
            <surname>Kraus</surname>
            <given-names>OZ</given-names>
          </name>
          <name>
            <surname>Cooper</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Moses</surname>
            <given-names>AM</given-names>
          </name>
        </person-group>
        <article-title>Learning unsupervised feature representations for single cell microscopy images with paired cell inpainting</article-title>
        <source>PLoS Computat. Biol.</source>
        <year>2019</year>
        <volume>15</volume>
        <fpage>e1007348</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1007348</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Perlman</surname>
            <given-names>ZE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Multidimensional drug profiling by automated microscopy</article-title>
        <source>Science</source>
        <year>2004</year>
        <volume>306</volume>
        <fpage>1194</fpage>
        <lpage>1198</lpage>
        <pub-id pub-id-type="doi">10.1126/science.1100709</pub-id>
        <pub-id pub-id-type="pmid">15539606</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carpenter</surname>
            <given-names>AE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cellprofiler: image analysis software for identifying and quantifying cell phenotypes</article-title>
        <source>Genome Biol.</source>
        <year>2006</year>
        <volume>7</volume>
        <fpage>R100</fpage>
        <pub-id pub-id-type="doi">10.1186/gb-2006-7-10-r100</pub-id>
        <pub-id pub-id-type="pmid">17076895</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yin</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A screen for morphological complexity identifies regulators of switch-like transitions between discrete cell shapes</article-title>
        <source>Nat. Cell Biol.</source>
        <year>2013</year>
        <volume>15</volume>
        <fpage>860</fpage>
        <lpage>871</lpage>
        <pub-id pub-id-type="doi">10.1038/ncb2764</pub-id>
        <pub-id pub-id-type="pmid">23748611</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bray</surname>
            <given-names>M-A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cell painting, a high-content image-based assay for morphological profiling using multiplexed fluorescent dyes</article-title>
        <source>Nat. Protoc.</source>
        <year>2016</year>
        <volume>11</volume>
        <fpage>1757</fpage>
        <pub-id pub-id-type="doi">10.1038/nprot.2016.105</pub-id>
        <pub-id pub-id-type="pmid">27560178</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kraus</surname>
            <given-names>OZ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated analysis of high-content microscopy data with deep learning</article-title>
        <source>Mol. Syst. Biol.</source>
        <year>2017</year>
        <volume>13</volume>
        <fpage>924</fpage>
        <pub-id pub-id-type="doi">10.15252/msb.20177551</pub-id>
        <pub-id pub-id-type="pmid">28420678</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eulenberg</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Reconstructing cell cycle and disease progression using deep learning</article-title>
        <source>Nat. Commun.</source>
        <year>2017</year>
        <volume>8</volume>
        <fpage>463</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-017-00623-3</pub-id>
        <pub-id pub-id-type="pmid">28878212</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Caicedo</surname>
            <given-names>JC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Data-analysis strategies for image-based cell profiling</article-title>
        <source>Nat. Methods</source>
        <year>2017</year>
        <volume>14</volume>
        <fpage>849</fpage>
        <lpage>863</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.4397</pub-id>
        <pub-id pub-id-type="pmid">28858338</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sailem</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Bousgouni</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Cooper</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Bakal</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Cross-talk between rho and RAC GTPases drives deterministic exploration of cellular shape space and morphological heterogeneity</article-title>
        <source>Open Biol.</source>
        <year>2014</year>
        <volume>4</volume>
        <fpage>130132</fpage>
        <pub-id pub-id-type="doi">10.1098/rsob.130132</pub-id>
        <pub-id pub-id-type="pmid">24451547</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Traag</surname>
            <given-names>VA</given-names>
          </name>
          <name>
            <surname>Waltman</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Van Eck</surname>
            <given-names>NJ</given-names>
          </name>
        </person-group>
        <article-title>From Louvain to Leiden: guaranteeing well-connected communities</article-title>
        <source>Sci. Rep.</source>
        <year>2019</year>
        <volume>9</volume>
        <fpage>5233</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-019-41695-z</pub-id>
        <pub-id pub-id-type="pmid">30914743</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>TR</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scoring diverse cellular morphologies in image-based screens with iterative feedback and machine learning</article-title>
        <source>Proc. Natl Acad. Sci. USA</source>
        <year>2009</year>
        <volume>106</volume>
        <fpage>1826</fpage>
        <lpage>1831</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0808843106</pub-id>
        <pub-id pub-id-type="pmid">19188593</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ouyang</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Analysis of the human protein atlas image classification competition</article-title>
        <source>Nat. Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>1254</fpage>
        <lpage>1261</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0658-6</pub-id>
        <pub-id pub-id-type="pmid">31780840</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blasi</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Label-free cell cycle analysis for high-throughput imaging flow cytometry</article-title>
        <source>Nat. Commun.</source>
        <year>2016</year>
        <volume>7</volume>
        <fpage>10256</fpage>
        <pub-id pub-id-type="doi">10.1038/ncomms10256</pub-id>
        <pub-id pub-id-type="pmid">26739115</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Pawlowski, N., Caicedo, J. C., Singh, S., Carpenter, A. E. &amp; Storkey, A. Automating morphological profiling with generic deep convolutional networks. Preprint at <italic>bioRxiv</italic> 085118 (2016).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Doan</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deepometry, a framework for applying supervised and weakly supervised deep learning to imaging cytometry</article-title>
        <source>Nat. Protoc.</source>
        <year>2021</year>
        <volume>16</volume>
        <fpage>3572</fpage>
        <lpage>3595</lpage>
        <pub-id pub-id-type="doi">10.1038/s41596-021-00549-7</pub-id>
        <pub-id pub-id-type="pmid">34145434</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Goyal, P. et al. Self-supervised pretraining of visual features in the wild. Preprint at arXiv:2103.01988 (2021).</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Holmberg</surname>
            <given-names>OG</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Self-supervised retinal thickness prediction enables deep learning from unlabelled data to boost classification of diabetic retinopathy</article-title>
        <source>Nat. Mach. Intell.</source>
        <year>2020</year>
        <volume>2</volume>
        <fpage>719</fpage>
        <lpage>726</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-020-00247-1</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hadsell</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Learning long-range vision for autonomous off-road driving</article-title>
        <source>J. Field Robotics</source>
        <year>2009</year>
        <volume>26</volume>
        <fpage>120</fpage>
        <lpage>144</lpage>
        <pub-id pub-id-type="doi">10.1002/rob.20276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Batson, J. &amp; Royer, L. Noise2self: blind denoising by self-supervision. In <italic>Proc. International Conference on Machine Learning</italic> (eds Chaudhuri, K. &amp; Salakhutdinov, R.) 524–533 (PMLR, 2019).</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kobayashi</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Intelligent whole-blood imaging flow cytometry for simple, rapid, and cost-effective drug-susceptibility testing of leukemia</article-title>
        <source>Lab. Chip</source>
        <year>2019</year>
        <volume>19</volume>
        <fpage>2688</fpage>
        <lpage>2698</lpage>
        <pub-id pub-id-type="doi">10.1039/C8LC01370E</pub-id>
        <pub-id pub-id-type="pmid">31287108</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Chen, T., Kornblith, S., Norouzi, M. &amp; Hinton, G. A simple framework for contrastive learning of visual representations. In <italic>Proc. International Conference on Machine Learning</italic> (eds III Hal, D. &amp; Singh, A.) 1597–1607 (PMLR, 2020).</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Kolesnikov, A., Zhai, X. &amp; Beyer, L. Revisiting self-supervised visual representation learning. In <italic>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 1920–1929 (IEEE, 2019).</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Deng, J. et al. Imagenet: a large-scale hierarchical image database. In <italic>Proc. 2009 IEEE Conference on Computer Vision and Pattern Recognition</italic> 248–255 (IEEE, 2009).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Van Den Oord, A., Vinyals, O. et al. Neural discrete representation learning. In <italic>Proc. Advances in Neural Information Processing Systems</italic> (eds Guyon, I. et al.) 6306–6315 (2017).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Razavi, A., van den Oord, A. &amp; Vinyals, O. Generating diverse high-fidelity images with VQ-VAE-2. In <italic>Proc. Advances in Neural Information Processing Systems</italic> (eds Wallach, H. et al.) 14866–14876 (2019).</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Wu, H. &amp; Flierl, M. Vector quantization-based regularization for autoencoders. In <italic>Proc. AAAI Conference on Artificial Intelligence</italic> vol. <bold>34</bold>, 6380–6387 (AAAI, 2020).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Giurgiu</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Corum: the comprehensive resource of mammalian protein complexes-2019</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2019</year>
        <volume>47</volume>
        <fpage>D559</fpage>
        <lpage>D563</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gky973</pub-id>
        <pub-id pub-id-type="pmid">30357367</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Donovan-Maiye</surname>
            <given-names>RM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A deep generative model of 3D single-cell organization</article-title>
        <source>PLoS Comput. Biol.</source>
        <year>2022</year>
        <volume>18</volume>
        <fpage>e1009155</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1009155</pub-id>
        <pub-id pub-id-type="pmid">35041651</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Consortium</surname>
            <given-names>TU</given-names>
          </name>
        </person-group>
        <article-title>Uniprot: the universal protein knowledgebase in 2021</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2021</year>
        <volume>49</volume>
        <fpage>D480</fpage>
        <lpage>D489</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkaa1100</pub-id>
        <pub-id pub-id-type="pmid">33237286</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schröder</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Wrocklage</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hasilik</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Saftig</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>The proteome of lysosomes</article-title>
        <source>Proteomics</source>
        <year>2010</year>
        <volume>10</volume>
        <fpage>4053</fpage>
        <lpage>4076</lpage>
        <pub-id pub-id-type="doi">10.1002/pmic.201000196</pub-id>
        <pub-id pub-id-type="pmid">20957757</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gosney</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Wilkey</surname>
            <given-names>DW</given-names>
          </name>
          <name>
            <surname>Merchant</surname>
            <given-names>ML</given-names>
          </name>
          <name>
            <surname>Ceresa</surname>
            <given-names>BP</given-names>
          </name>
        </person-group>
        <article-title>Proteomics reveals novel protein associations with early endosomes in an epidermal growth factor–dependent manner</article-title>
        <source>J. Biol. Chem.</source>
        <year>2018</year>
        <volume>293</volume>
        <fpage>5895</fpage>
        <lpage>5908</lpage>
        <pub-id pub-id-type="doi">10.1074/jbc.RA117.000632</pub-id>
        <pub-id pub-id-type="pmid">29523688</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Cheng, Y. &amp; Church, G. M. Biclustering of expression data. In <italic>Proc. International Conference on Intelligent Systems for Molecular Biology</italic> Vol. 8, 93–103 (AAAI Press, 2000).</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gerbin</surname>
            <given-names>KA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cell states beyond transcriptomics: integrating structural organization and gene expression in hIPSC-derived cardiomyocytes</article-title>
        <source>Cell Syst.</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>670</fpage>
        <lpage>687</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cels.2021.05.001</pub-id>
        <pub-id pub-id-type="pmid">34043964</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Viana, M. P. et al. Robust integrated intracellular organization of the human IPS cell: where, how much, and how variable. Preprint at <italic>bioRxiv</italic> 2020-12 (2021).</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Halevy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Norvig</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Pereira</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>The unreasonable effectiveness of data</article-title>
        <source>IEEE Intell. Syst.</source>
        <year>2009</year>
        <volume>24</volume>
        <fpage>8</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1109/MIS.2009.36</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leonetti</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>Sekine</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kamiyama</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Weissman</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>A scalable strategy for high-throughput GFP tagging of endogenous human proteins</article-title>
        <source>Proc. Natl Acad. Sci. USA</source>
        <year>2016</year>
        <volume>113</volume>
        <fpage>E3501</fpage>
        <lpage>E3508</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1606731113</pub-id>
        <pub-id pub-id-type="pmid">27274053</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>CH</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Minimum cross entropy thresholding</article-title>
        <source>Pattern Recog.</source>
        <year>1993</year>
        <volume>26</volume>
        <fpage>617</fpage>
        <lpage>625</lpage>
        <pub-id pub-id-type="doi">10.1016/0031-3203(93)90115-D</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Tam</surname>
            <given-names>PK-S</given-names>
          </name>
        </person-group>
        <article-title>An iterative algorithm for minimum cross entropy thresholding</article-title>
        <source>Pattern Recog. Lett.</source>
        <year>1998</year>
        <volume>19</volume>
        <fpage>771</fpage>
        <lpage>776</lpage>
        <pub-id pub-id-type="doi">10.1016/S0167-8655(98)00057-9</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Tan, M. &amp; Le, Q. Efficientnet: rethinking model scaling for convolutional neural networks. In <italic>Proc. International Conference on Machine Learning</italic> (eds Chaudhuri, K. &amp; Salakhutdinov, R.) 6105–6114 (PMLR 2019).</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep residual learning for image recognition. In <italic>Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 770–778 (IEEE, 2016).</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">McInnes, L., Healy, J. &amp; Melville, J. UMAP: uniform manifold approximation and projection for dimension reduction. Preprint at <italic>arXiv</italic> arXiv:1802.03426 (2018).</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <mixed-citation publication-type="other">Rokach, L. &amp; Maimon, O. (eds) <italic>Data Mining and Knowledge Discovery Handbook</italic> 321–352 (Springer, 2005).</mixed-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <mixed-citation publication-type="other">Abadi, M. et al. TensorFlow: large-scale machine learning on heterogeneous systems. <italic>tensorflow.org</italic><ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/">https://www.tensorflow.org/</ext-link> (2015).</mixed-citation>
    </ref>
  </ref-list>
</back>
