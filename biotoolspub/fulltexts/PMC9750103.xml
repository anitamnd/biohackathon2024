<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9750103</article-id>
    <article-id pub-id-type="pmid">36271850</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac682</article-id>
    <article-id pub-id-type="publisher-id">btac682</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>LaGAT: link-aware graph attention network for drug–drug interaction prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-6601-278X</contrib-id>
        <name>
          <surname>Hong</surname>
          <given-names>Yue</given-names>
        </name>
        <aff><institution>School of Informatics, Xiamen University</institution>, Xiamen 361005, <country country="CN">China</country></aff>
        <xref rid="btac682-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2300-6355</contrib-id>
        <name>
          <surname>Luo</surname>
          <given-names>Pengyu</given-names>
        </name>
        <aff><institution>School of Informatics, Xiamen University</institution>, Xiamen 361005, <country country="CN">China</country></aff>
        <xref rid="btac682-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-8113-9367</contrib-id>
        <name>
          <surname>Jin</surname>
          <given-names>Shuting</given-names>
        </name>
        <aff><institution>School of Informatics, Xiamen University</institution>, Xiamen 361005, <country country="CN">China</country></aff>
        <aff><institution>National Institute for Data Science in Health and Medicine, Xiamen University</institution>, Xiamen 361005, <country country="CN">China</country></aff>
        <aff><institution>MindRank AI Ltd.</institution>, Hangzhou 310000, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-9885-1978</contrib-id>
        <name>
          <surname>Liu</surname>
          <given-names>Xiangrong</given-names>
        </name>
        <aff><institution>School of Informatics, Xiamen University</institution>, Xiamen 361005, <country country="CN">China</country></aff>
        <aff><institution>National Institute for Data Science in Health and Medicine, Xiamen University</institution>, Xiamen 361005, <country country="CN">China</country></aff>
        <xref rid="btac682-cor1" ref-type="corresp"/>
        <!--xrliu@xmu.edu.cn-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Wren</surname>
          <given-names>Jonathan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <fn id="btac682-FM1">
        <p>The authors wish it to be known that, in their opinion, Yue Hong and Pengyu Luo should be regarded as Joint First Authors.</p>
      </fn>
      <corresp id="btac682-cor1">To whom correspondence should be addressed. Email: <email>xrliu@xmu.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>15</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-10-22">
      <day>22</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>22</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <volume>38</volume>
    <issue>24</issue>
    <fpage>5406</fpage>
    <lpage>5412</lpage>
    <history>
      <date date-type="received">
        <day>06</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>30</day>
        <month>8</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>27</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>08</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac682.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Drug–drug interaction (DDI) prediction is a challenging problem in pharmacology and clinical applications. With the increasing availability of large biomedical databases, large-scale biological knowledge graphs containing drug information have been widely used for DDI prediction. However, large knowledge graphs inevitably suffer from data noise problems, which limit the performance and interpretability of models based on the knowledge graph. Recent studies attempt to improve models by introducing inductive bias through an attention mechanism. However, they all only depend on the topology of entity nodes independently to generate fixed attention pathways, without considering the semantic diversity of entity nodes in different drug pair links. This makes it difficult for models to select more meaningful nodes to overcome data quality limitations and make more interpretable predictions.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>To address this issue, we propose a Link-aware Graph Attention method for DDI prediction, called LaGAT, which is able to generate different attention pathways for drug entities based on different drug pair links. For a drug pair link, the LaGAT uses the embedding representation of one of the drugs as a query vector to calculate the attention weights, thereby selecting the appropriate topological neighbor nodes to obtain the semantic information of the other drug. We separately conduct experiments on binary and multi-class classification and visualize the attention pathways generated by the model. The results prove that LaGAT can better capture semantic relationships and achieves remarkably superior performance over both the classical and state-of-the-art models on DDI prediction.</p>
      </sec>
      <sec id="s3">
        <title>Availabilityand implementation</title>
        <p>The source code and data are available at <ext-link xlink:href="https://github.com/Azra3lzz/LaGAT" ext-link-type="uri">https://github.com/Azra3lzz/LaGAT</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>62072384</award-id>
        <award-id>61872309</award-id>
        <award-id>62072385</award-id>
        <award-id>61772441</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Zhijiang Lab</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2022RD0AB02</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="7"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Drug–drug interactions (DDI) mean that when two or more drugs are taken by the patient at the same time, the efficacy of the drugs may be enhanced or weakened, and even side effects may occur. The DDI is a very common situation for patients with complex conditions, it may accelerate the treatment process of the condition, may also affect the health of the patient, and even lead to the death of the patient (<xref rid="btac682-B24" ref-type="bibr">Vilar <italic toggle="yes">et al.</italic>, 2014</xref>). To reduce medical risks and improve treatment effects, accurate prediction of DDI has become an important clinical task.</p>
    <p>Since deep learning is suitable for processing large-scale data and can obtain features with high generalization ability (<xref rid="btac682-B19" ref-type="bibr">Ryu <italic toggle="yes">et al.</italic>, 2018</xref>), it has been widely used to predict DDI. Some researchers focus on directly exploiting drug-related features such as side effects (<xref rid="btac682-B13" ref-type="bibr">Jin <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac682-B17" ref-type="bibr">Ma <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac682-B33" ref-type="bibr">Zitnik <italic toggle="yes">et al.</italic>, 2018</xref>), molecular structure (<xref rid="btac682-B2" ref-type="bibr">Asada <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac682-B18" ref-type="bibr">Nyamabo <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac682-B21" ref-type="bibr">Takeda <italic toggle="yes">et al.</italic>, 2017</xref>) and SMILES sequences (<xref rid="btac682-B12" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2020</xref>) for DDI prediction. These DDI prediction models treat drugs as an independent data sample without considering the topological information of the interactions among drugs. Therefore, many researchers further combine large drug databases with biomedical textual information to construct a biological knowledge graph (KG) to obtain semantic information of drug entities (<xref rid="btac682-B7" ref-type="bibr">Gao <italic toggle="yes">et al.</italic>, 2022</xref>; <xref rid="btac682-B31" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2021</xref>), and to perform DDI prediction combined with drug molecular features.</p>
    <p>Existing KG-based models usually use node embedding methods to derive drug node features (<xref rid="btac682-B4" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2021</xref>), or learn higher-order semantics of nodes through graph neural network (<xref rid="btac682-B16" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac682-B29" ref-type="bibr">Yu <italic toggle="yes">et al.</italic>, 2021</xref>). Although these methods have contributed for the DDI prediction, they share the common problem that each drug obtains the same embedding representation when making predictions with different drugs. That is, they ignore the semantic diversity of entity nodes in different drug pair links, which limits the model’s ability to further mine knowledge graph data information and make more interpretable predictions.</p>
    <p>To address the above limitations, we propose a Link-aware Graph Attention method, named LaGAT, which enables the model to capture different semantic information of drugs from neighborhood nodes in the knowledge graph based on the drug pair link. Specifically, we construct a link-aware graph attention layer as the core of the model. For drug <italic toggle="yes">v</italic> and drug <italic toggle="yes">u</italic> in a drug pair link, the embedded representation of drug <italic toggle="yes">u</italic> is used as a query vector to compute the attention weights of the neighbors of drug <italic toggle="yes">v</italic> to generate different attention paths and update the embedding of each node in the subgraph with the embedding representation of the neighbors. We conduct experiments on two real datasets, binary and multi-class, and selected typical DDI examples to visualize the generated attention pathways of our model. Experimental results show that our model has certain interpretability and outperforms existing state-of-the-art models on both datasets. Our contributions are summarized as follows:
</p>
    <list list-type="bullet">
      <list-item>
        <p>We propose a novel graph attention method that improves on the limitations of DDI prediction on knowledge graphs. It allows the model to generate different attention pathways for the drug entity by different drug pair links to aggregate neighbor node information.</p>
      </list-item>
      <list-item>
        <p>Our propose method can make the model interpretable. The LaGAT can make the model pay attention to more meaningful entity nodes, which is beneficial for the model to predict different types of DDIs.</p>
      </list-item>
      <list-item>
        <p>We evaluate LaGAT on two public datasets and demonstrate its superiority over the state-of-the-art DDI prediction models.</p>
      </list-item>
    </list>
  </sec>
  <sec>
    <title>2 Related work</title>
    <p><italic toggle="yes">Attention mechanism on graph</italic>. Attention mechanism can be used to improve the prediction performance of neural network models (<xref rid="btac682-B3" ref-type="bibr">Bahdanau <italic toggle="yes">et al.</italic>, 2014</xref>) and provide a certain degree of interpretability for neural network structures (<xref rid="btac682-B22" ref-type="bibr">Vashishth <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac682-B26" ref-type="bibr">Wiegreffe and Pinter, 2019</xref>), which has become a research hotspot in recent years. There have been some studies trying to promote the attention mechanism on graph data structure. The Graph Attention Network (GAT) (<xref rid="btac682-B23" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic>, 2018</xref>) incorporates the attention mechanism into the propagation step and the multi-head attention mechanism used by to stabilize the learning process. The Gated Attention Network (GaAN) (<xref rid="btac682-B30" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic>, 2018</xref>) also uses a multi-head attention mechanism. It uses a gated attention mechanism to collect information from different heads instead of GAT’s average operation. However, these methods all only rely on adjacent node pairs to calculate the attention weight, without considering that entity nodes may have different weights in different predicted links to contribute semantic information. In contrast, our method can give different attention weights to its neighbor nodes for the same drug entity according to the different prediction targets, so as to obtain its different semantic information. Similarly, the TAGNN (<xref rid="btac682-B28" ref-type="bibr">Ying <italic toggle="yes">et al.</italic>, 2021</xref>) uses a gated graph neural network to obtain node semantic information and uses a target-based attention layer based to calculate weights for full-graph nodes for recommendation. It does not combine the attention method with the topological structure of the graph, which makes it likely to overfit to discrete special nodes and thus not interpretable. Correspondingly, we construct an attention layer to generate attention pathways through the topology of each node to obtain node semantic information, which not only focus on different nodes according to different predicted links but also maintain interpretability.</p>
    <p><italic toggle="yes">Knowledge graph-based models</italic>. Knowledge graph (KG) is essentially described as a network with rich information, which can provide structural relationships among multiple entities, and unstructured semantic relationships related to each entity. Nowadays, many biomedical knowledge bases are published in the form of knowledge graphs (<xref rid="btac682-B25" ref-type="bibr">Whirl-Carrillo <italic toggle="yes">et al.</italic>, 2012</xref>), and some researchers have combined public datasets and biomedical corpora to build more complete biomedical knowledge graphs (<xref rid="btac682-B5" ref-type="bibr">Cong <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac682-B11" ref-type="bibr">Harnoune <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac682-B31" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2021</xref>). The DDI prediction model developed based on these knowledge graphs has also achieved certain results. <xref rid="btac682-B14" ref-type="bibr">Karim <italic toggle="yes">et al.</italic> (2019)</xref> adopted the method of graph embedding for DDI prediction using KG. <xref rid="btac682-B16" ref-type="bibr">Lin <italic toggle="yes">et al.</italic> (2020)</xref> adopted an end-to-end and fully knowledge graph-based framework for DDI prediction. <xref rid="btac682-B29" ref-type="bibr">Yu <italic toggle="yes">et al.</italic> (2021)</xref> improves model performance and interpretability through a layer-independent graph attention mechanism. <xref rid="btac682-B4" ref-type="bibr">Chen <italic toggle="yes">et al.</italic> (2021)</xref> comprehensively consider the molecular features of drugs and the semantic information of knowledge graphs and fuse the drug representations of the two modalities in a novel way. Whether it is a graph embedding or a graph attention method, each drug can only obtain one embedding representation, which cannot reflect the semantic diversity of drugs in different drug pair links. Correspondingly, our method can generate different attention pathways according to different drug pairs, and generate different embedding representations for drug nodes, so as to obtain the potential different semantics of drugs.</p>
  </sec>
  <sec>
    <title>3 Materials and methods</title>
    <sec>
      <title>3.1 Overview</title>
      <p>As shown in <xref rid="btac682-F1" ref-type="fig">Figure 1</xref>, our model can be decomposed into three parts: the subgraph sampling module, link-aware graph attention layer and the layer-wise aggregation layer. First, the subgraph sampling module extracts drug pairs from the dataset, forming a DDI matrix. At the same time, it obtains the <italic toggle="yes">H</italic>-hop neighbor nodes of each drug, and randomly selects <italic toggle="yes">K</italic> of each hop neighbor nodes to form a drug’s subgraph and embeds all the nodes contained in the drug subgraph. Then, the link-aware attention layer computes attention weights for the neighbor nodes of each node in the subgraph, making each node update its own embedding representation by aggregating the embedding representations of its neighbor nodes according to the attention weight. Finally, the layer-wise aggregation layer connects the embedding representation of each update of the drug node to obtain the final feature vector of the drug node and then predicts the label of the DDI.</p>
      <fig position="float" id="btac682-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>The Overview of LaGAT. (<bold>a</bold>) The Subgraph Sampling Module extracts the nodes in the KG according to the drug pair to form the subgraph corresponding to the drug. (<bold>b</bold>) After obtains the subgraph, LaGAT uses drug <italic toggle="yes">v</italic> as the query vector to calculate the attention weight (a11 to a14) of each node in the drug <italic toggle="yes">u</italic> subgraph, and uses drug <italic toggle="yes">u</italic> as the query vector to calculate the attention weight (a24 to a27) of each node in the drug <italic toggle="yes">v</italic> subgraph. Then, each node (<italic toggle="yes">U<sub>i</sub></italic>, <italic toggle="yes">V <sub>i</sub></italic>) of the two subgraphs aggregates the embedding representations of neighbor nodes according to the attention weights of its neighbor nodes, updating its own embedding representation. Then, LaGAT can get the updated embedding representation of all nodes including nodes <italic toggle="yes">u</italic>, <italic toggle="yes">v</italic>. (<bold>c</bold>) After iterating again <italic toggle="yes">H</italic> times, the layer-wise aggregation layer obtain multiple representations of drug <italic toggle="yes">u</italic>, <italic toggle="yes">v</italic> and concatenate the representations into a single vector to calculate the final DDI prediction score</p>
        </caption>
        <graphic xlink:href="btac682f1" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.2 Notation and problem formulation</title>
      <p>Given a set of various biomedical entities <italic toggle="yes">N<sub>e</sub></italic> and the biomedical relation <italic toggle="yes">R</italic> among the entities, the external biomedical KG is definedas <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>|</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>e</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Each entity-relation-entity triples <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> describes a relation <italic toggle="yes">r</italic> between biomedical entity <italic toggle="yes">h</italic> and biomedical entity <italic toggle="yes">t</italic>. We present drug set as <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⊆</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and define the DDI matrix <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> with each element <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> means the types of DDI relations, where <italic toggle="yes">C</italic> is the total number of types of DDI pairs. Note that we consider all types of DDI pairs and <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> indicates the absence of evidence for interaction for the binary-class prediction task.</p>
      <p>Our main task is to learn a prediction function <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>ρ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mo>θ</mml:mo><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the probability that drug <italic toggle="yes">j</italic> will interact with drug <italic toggle="yes">i</italic>, and <italic toggle="yes">θ</italic> denotes the model parameters of function <italic toggle="yes">ρ</italic>.</p>
    </sec>
    <sec>
      <title>3.3 The subgraph sampling module</title>
      <p>Inspired by the GNN method (<xref rid="btac682-B10" ref-type="bibr">Hamilton <italic toggle="yes">et al.</italic>, 2017</xref>), we only select nodes with the <italic toggle="yes">H</italic>-hop distance from a drug to form a subgraph (the same definition as the GNN method), where <italic toggle="yes">H</italic> is a hyper-parameter. Usually the value of <italic toggle="yes">H</italic> will be greater than or equal to 1 to extract higher-order structures and semantic relations, so that we do not need to put the entire KG into training and still obtain the effective information from KG.</p>
      <p>At the same time, in the real-world knowledge graph, the size of the neighbor node set <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>ε</mml:mo><mml:mrow><mml:mtext>neigh</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> of each entity node <italic toggle="yes">ε</italic> may be very different. Inspired by the KGNN method (<xref rid="btac682-B16" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2020</xref>), we uniformly sample the sets <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>ε</mml:mo><mml:mrow><mml:mtext>neigh</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and get the set <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo>ε</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (here <italic toggle="yes">k</italic> is also a hyper-parameter, which determines the size of the receptive field of a single-layer entity). For any sample drug pair (<italic toggle="yes">u</italic>, <italic toggle="yes">v</italic>), sampling can get their 1-hop neighbors, sampling 1-hop neighbor nodes to get 2-hop neighbors from them, and so on. Then we can get a set of node <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>u</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and get the desired subgraph <italic toggle="yes">G<sub>u</sub></italic>, <italic toggle="yes">G<sub>v</sub></italic> as input to the attention layer.</p>
    </sec>
    <sec>
      <title>3.4 The link-aware graph attention layer</title>
      <p>In this subsection, we describe the link-aware graph attention layer and compare it with the origin the GAT layer.</p>
      <p>Generally, the GAT layer use the embedding representation of the node itself as the Query vector, combined with the embedding representation of its neighboring nodes to learn the attention weight. This strategy successfully makes each node assign different weights to each of its neighbors, but its limitation is that as long as the topology of a node is deterministic, the attention weights it eventually learns are deterministic. That is, GAT assumes that the attention pathway inside each drug subgraph is fixed, resulting in learned node features that can only capture a single semantic information. However, the link prediction task uses the label of the drug pair as a supervision signal. In different drug pairs, the nodes that each node needs to pay attention to are not fixed, that is, attention pathways in drug subgraphs should be variable based on drug pairs, and the semantic information of nodes may be diverse. In this case, the GAT layer may only learn the semantic information of nodes under most link labels or tend to evenly distribute attention weights, which may fail to learn a suitable attention weight for each neighbor node and hurt the interpretability of the model.<boxed-text id="btac682-BOX1" position="float"><caption><p>Algorithm 1 LaGAT algorithm</p></caption><p><bold>Input</bold>: Drug pair (<italic toggle="yes">u</italic>, <italic toggle="yes">v</italic>); subgraph <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi>u</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>e</mml:mi><mml:mi>u</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>u</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>; neighborhood funtion <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi><mml:mo>:</mml:mo><mml:mo>ε</mml:mo><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>ε</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mo>∀</mml:mo><mml:mo>ε</mml:mo><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>e</mml:mi><mml:mi>u</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>; node embedding function <italic toggle="yes">E</italic>; depth <italic toggle="yes">H</italic>;</p><p><bold>Output</bold>: Node vector representations <italic toggle="yes">Z<sub>u</sub></italic>, <italic toggle="yes">Z<sub>v</sub></italic></p><p>1: <bold>for</bold><inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula><bold>do</bold></p><p>2:  <bold>if</bold> <italic toggle="yes">d</italic> is equal to <italic toggle="yes">u</italic> <bold>then</bold></p><p>3:   <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:mi>q</mml:mi><mml:mo>←</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p><p>4:  <bold>else</bold></p><p>5:   <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:mi>q</mml:mi><mml:mo>←</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p><p>6:  <bold>end if</bold></p><p>7:  <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>ε</mml:mo><mml:mn>0</mml:mn></mml:msubsup><mml:mo>←</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>ε</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>∀</mml:mo><mml:mo>ε</mml:mo><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></p><p>8:  <bold>for</bold><inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>⋯</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:math></inline-formula><bold>do</bold></p><p>9:   <bold>for</bold><inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:mo>ε</mml:mo><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula><bold>do</bold></p><p>10:    <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:mi>q</mml:mi><mml:mo>·</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>j</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>∀</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>ε</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p><p>11:    <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>ε</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>j</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula></p><p>12:    <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>ε</mml:mo><mml:mi>l</mml:mi></mml:msubsup><mml:mo>←</mml:mo><mml:mi mathvariant="italic">Aggregator</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>ε</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p><p>13:   <bold>end for</bold></p><p>14:  <bold>end for</bold></p><p>15:  <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></p><p>16: <bold>end for</bold></p><p>17: <bold>return</bold> <italic toggle="yes">Z<sub>u</sub></italic>, <italic toggle="yes">Z<sub>v</sub></italic></p></boxed-text>So, we design link-aware graph attention layer to replace GAT layer. Algorithm 1 shows the pseudo-codes of applying LaGAT between given drug pairs. Specifically, for any sample drug pair(<italic toggle="yes">u</italic>, <italic toggle="yes">v</italic>), after obtain the subgraph <italic toggle="yes">G<sub>u</sub></italic>, <italic toggle="yes">G<sub>v</sub></italic>, link-aware graph attention layer produces a new set of node vector representations <italic toggle="yes">Z<sub>u</sub></italic>, <italic toggle="yes">Z<sub>v</sub></italic> as its output. When the node it computes belongs to <italic toggle="yes">G<sub>u</sub></italic>, it uses the embedding representation of drug node <italic toggle="yes">v</italic> as the query vector (Lines 2–6). At the same time, it still uses the neighbor nodes of each node as the Key vector, and obtains the attention weight by calculating the inner product of the Query vector and the Key vector, which makes the neighbor nodes concerned by each node change according to the label. Among them, <italic toggle="yes">α<sub>j</sub></italic> is the attention weight of the node embedding <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>j</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, and query vector is the embedding representation of the drug node <italic toggle="yes">ε</italic> (Line 10). Then, it aggregates the embedding to obtain a vector <italic toggle="yes">z<sub>N</sub></italic>, which is input to the Multilayer Perceptron (MLP) layer named aggregator to obtain the next embedding representation of node <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>ε</mml:mo><mml:mi>h</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> (Line 11).</p>
      <p>Aggregator is an important MLP layer in the link-aware graph attention layer. We implement similar multiple types of aggregators (<xref rid="btac682-B16" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2020</xref>) as follows: <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mtext>Concat</mml:mtext><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo>·</mml:mo><mml:mtext>concat</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mo>ε</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:mtext>Neighbor</mml:mtext><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We use LeakyReLU (<inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:mo>α</mml:mo><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>) as the activation function in the above method. After evaluating the results of the three methods, we finally chose the neighbor form of aggregation. We will show the evaluation results of the two methods in more experimental results (Section 4.5).</p>
    </sec>
    <sec>
      <title>3.5 The layer-wise aggregation layer</title>
      <p>After iterating again <italic toggle="yes">H</italic> times, we obtain multiple representations <italic toggle="yes">Z<sub>u</sub></italic> for drug node <italic toggle="yes">u</italic>, namely <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>u</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>u</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>; analogous to another drug node <italic toggle="yes">v</italic>, <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> are obtained. As we mentioned earlier and shown in <xref rid="btac682-F1" ref-type="fig">Figure 1</xref>, <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>u</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> represents different depths of high-level structure and semantic information. We hence adopt the layer-aggregation layer (<xref rid="btac682-B27" ref-type="bibr">Xu <italic toggle="yes">et al.</italic>, 2018</xref>), to concatenate the representations at each step into a single vector, as follows:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>…</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>…</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>In this way, we try our best to aggregate high-level structure information together, which contains more information than just relying on the embedding representation of the last iteration.</p>
      <p>Finally, we calculate the inner product of the two drug representations and obtain the final DDI prediction score through the activation function:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mtext>score</mml:mtext></mml:mrow></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>u</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mtext>score</mml:mtext></mml:mrow></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>δ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">MLP</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>u</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>For binary classification tasks, we use score<sub><italic toggle="yes">b</italic></sub>, where <italic toggle="yes">σ</italic> is the sigmoid activation function; for multi-classification tasks, we use score<sub><italic toggle="yes">m</italic></sub>, where <italic toggle="yes">δ</italic> denotes softmax. Both scores represent the possibility of drug pair interaction.</p>
    </sec>
    <sec>
      <title>3.6 Training</title>
      <p>Through the representation learned in the previous part, we can integrate all the information from the drug and the topological neighborhood to predict the interaction value between the drug–drug pair. We use Xavier (<xref rid="btac682-B9" ref-type="bibr">Glorot and Bengio, 2010</xref>) to initialize the training parameters in all layers (including the embedding layer).</p>
      <p>For a given set of drug–drug pairs and the true interaction value in the training dataset, we use binary cross-entropy as the loss function cross, the formula is as follows:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mtext>Loss</mml:mtext><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:munder><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the predicted value, <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the ground-truth value, and <italic toggle="yes">Y</italic> represents the set of drug–drug pairs. During training, we learn the model parameter by minimizing the loss using stochastic gradient optimizers such as Adam (<xref rid="btac682-B9" ref-type="bibr">Glorot and Bengio, 2010</xref>)</p>
    </sec>
  </sec>
  <sec>
    <title>4 Experiment</title>
    <sec>
      <title>4.1 Datasets and settings</title>
      <p><italic toggle="yes">Datasets</italic>. We use two real-world datasets to evaluate the KGSAT we proposed. (i) <bold>Binary-class DDIs</bold>: We use the data from the KEGG-drug provided by (<xref rid="btac682-B16" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2020</xref>) to construct KG including DDI matrix, which contains 1925 approved drugs and 56 983 approved interactions. We randomly generated negative samples at a ratio of 1:1, where negative samples refer to drug pairs that have not appeared in the positive samples. (ii) <bold>Multi-class DDIs</bold>: We use the data provided by (<xref rid="btac682-B16" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2020</xref>) from the Drugbank to construct KG and the data provided by (<xref rid="btac682-B29" ref-type="bibr">Yu <italic toggle="yes">et al.</italic>, 2021</xref>), which contains 1709 drugs mapped to DrugBank identifiers (IDs) and 136351 drug pairs involving 86 types of pharmacological relationships between drugs.</p>
      <p><italic toggle="yes">Experimental settings</italic>. For the binary-class, experimental results are presented in following metrics: the accuracy (ACC), the area under the receiver operating characteristic (AUC), the area under the Precision–Recall curve (AUPR), the F1 score. And we randomly divide the dataset KEGG into 5 folds, 4 of which are training sets, and the remaining one is equally divided into validation set and test set, so that 5-fold cross-validation is performed. For the multi-classification tasks, we use ACC and Macro-F1 as performance metrics. Macro-F1 is the average of the F1 scores for each category, and the samples of the Drugbank dataset are the same as <xref rid="btac682-B29" ref-type="bibr">Yu <italic toggle="yes">et al.</italic> (2021)</xref>.</p>
      <p>We use Adam algorithm for parameter training, and perform a grid search on our model on two important hyperparameters, node dimension and number of neighbor samples. We finally use a node embedding dimension of 64 and a neighborhood sampling number of 64 as parameters on the KEGG dataset. On the Drugbank dataset, we finally use a combination of a node embedding dimension of 32 and a neighborhood sampling number of 128. The number of training epochs is set to 50, and other hyper-parameter settings are shown in the <xref rid="btac682-T1" ref-type="table">Table 1</xref>, which are optimized on the validation set by AUCROC on the binary classification task and by ACC on the validation set on the multi-classification task.</p>
      <table-wrap position="float" id="btac682-T1">
        <label>Table 1.</label>
        <caption>
          <p>Hyper-parameter settings</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Parameters</th>
              <th rowspan="1" colspan="1">Setting</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Batch size</td>
              <td align="char" char="." rowspan="1" colspan="1">1024</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Learning rate</td>
              <td align="char" char="." rowspan="1" colspan="1">1e–2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">L2 weight</td>
              <td align="char" char="." rowspan="1" colspan="1">1e–7</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">H-hop</td>
              <td align="char" char="." rowspan="1" colspan="1">1</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>4.2 Baselines</title>
      <p>We constructed several strong baseline models according to the original paper: KGNN, GAT and GAT-const, and performed grid search on each model on two important hyperparameters, node dimension and neighbor sampling number, and found keeping the rest of the parameters consistent with our method allows a relatively fair discussion of the effectiveness of our proposed attention method.
</p>
      <list list-type="order">
        <list-item>
          <p>KGNN: (<xref rid="btac682-B16" ref-type="bibr">Lin <italic toggle="yes">et al.</italic>, 2020</xref>) through GNN and external KG, the neighborhood information of each node is sampled and aggregated from the local receiver of each node for DDI prediction. On KEGG, we end up using a node embedding dimension of 16 and a neighborhood sampling number of 64 as parameters for GAT and GAT-const. On Drugbank, we end up using a node embedding dimension of 32 and neighborhood sampling number 32 as the GAT and GAT-const parameters. Parameters of GAT-const.</p>
        </list-item>
        <list-item>
          <p>GAT: (<xref rid="btac682-B23" ref-type="bibr">Veličković <italic toggle="yes">et al.</italic>, 2018</xref>) A multi-head attention mechanism is adopted to assign multiple attention weights to each neighbor node through multiple attention heads. At the same time, they also proposed a model GAT-const that assigns the same weight to each node to compare the effectiveness of GAT. On KEGG, we finally set the number of attention heads for GAT to 8, and use the node embedding dimension to be 64 and the number of neighborhood samples to be 64 as the parameters of GAT and GAT-const. On Drugbank, we finally set the attention of GAT to The number of heads is 2, and the node embedding dimension is 32 and the number of neighborhood samples is 128 as parameters for GAT and GAT-const.</p>
        </list-item>
        <list-item>
          <p>SumGNN: (<xref rid="btac682-B29" ref-type="bibr">Yu <italic toggle="yes">et al.</italic>, 2021</xref>) propose a GNN based method, which is enabled by a subgraph extraction module, a self-attention based subgraph summarization scheme to generate reasoning pathway within the subgraph, and a multichannel knowledge and data integration module. Unlike other models, we directly use the results of the original SumGNN paper for discussion.</p>
        </list-item>
      </list>
    </sec>
    <sec>
      <title>4.3 Results and analysis</title>
      <p>We compare the performance of the proposed method with the baseline we mentioned. <xref rid="btac682-T2" ref-type="table">Table 2</xref> reports AUC, ACC, F1 scores, AUPR on the KEGG-drug dataset and ACC, F1 scores on the Drugbank dataset. Compared to all baselines, our model performs the best. More specifically, on KEGG-drug, GAT and GAT-const are not significantly different while outperform KGNN, and our model improves by 2.54% on AUC-ROC, 3.02% on ACC, 3.09% on F1 and 1.98% on AUPR compared to the GAT. On Drugbank, GAT and GAT-const are equally competitive with SumGNN, and our model improves by 4.3% on ACC and 4.14% on F1 compared to the GAT. These findings demonstrate the effectiveness of our modele.</p>
      <table-wrap position="float" id="btac682-T2">
        <label>Table 2.</label>
        <caption>
          <p>The performance of baselines, LaGAT-base and LaGAT on two datasets, where LaGAT-base removes layer aggregation layers on the basis of LaGAT</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Methods</th>
              <th colspan="4" align="center" rowspan="1">Binary_class: KEGG-drug<hr/></th>
              <th colspan="2" align="center" rowspan="1">Multi_class: DrugBank<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th align="center" rowspan="1" colspan="1">AUC</th>
              <th align="center" rowspan="1" colspan="1">ACC</th>
              <th align="center" rowspan="1" colspan="1">F1</th>
              <th align="center" rowspan="1" colspan="1">AUPR</th>
              <th align="center" rowspan="1" colspan="1">ACC</th>
              <th align="center" rowspan="1" colspan="1">Marco-F1</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">KGNN</td>
              <td rowspan="1" colspan="1">95.63 ± 0.18</td>
              <td rowspan="1" colspan="1">90.08 ± 0.27</td>
              <td rowspan="1" colspan="1">90.32 ± 0.22</td>
              <td rowspan="1" colspan="1">94.20 ± 0.32</td>
              <td rowspan="1" colspan="1">93.03 ± 0.07</td>
              <td rowspan="1" colspan="1">84.56 ± 1.36</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GAT</td>
              <td rowspan="1" colspan="1">97.42 ± 0.14</td>
              <td rowspan="1" colspan="1">92.88 ± 0.21</td>
              <td rowspan="1" colspan="1">92.97 ± 0.18</td>
              <td rowspan="1" colspan="1">96.67 ± 0.30</td>
              <td rowspan="1" colspan="1">91.74 ± 0.15</td>
              <td rowspan="1" colspan="1">88.75 ± 1.89</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GAT-const</td>
              <td rowspan="1" colspan="1">97.68 ± 0.22</td>
              <td rowspan="1" colspan="1">93.22 ± 0.51</td>
              <td rowspan="1" colspan="1">93.37 ± 0.56</td>
              <td rowspan="1" colspan="1">97.10 ± 0.27</td>
              <td rowspan="1" colspan="1">91.71 ± 0.04</td>
              <td rowspan="1" colspan="1">87.66 ± 0.86</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SumGNN<xref rid="tblfn2" ref-type="table-fn"><sup>a</sup></xref></td>
              <td align="center" rowspan="1" colspan="1">–</td>
              <td align="center" rowspan="1" colspan="1">–</td>
              <td align="center" rowspan="1" colspan="1">–</td>
              <td align="center" rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">92.66 ± 0.14</td>
              <td rowspan="1" colspan="1">86.85 ± 0.44</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LaGAT-base</td>
              <td rowspan="1" colspan="1">98.33 ± 0.08</td>
              <td rowspan="1" colspan="1">94.94 ± 0.10</td>
              <td rowspan="1" colspan="1">95.04 ± 0.11</td>
              <td rowspan="1" colspan="1">97.70 ± 0.19</td>
              <td rowspan="1" colspan="1">95.71 ± 0.23</td>
              <td rowspan="1" colspan="1">89.25 ± 1.03</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>LaGAT</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>98.96 ± 0.10</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>95.90 ± 0.21</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>95.96 ± 0.20</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>98.95 ± 0.21</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>96.04 ± 0.08</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>92.89 ± 0.91</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic toggle="yes">Note</italic>: We used a 5-fold cross-validation with random split, and reported the average and standard deviation. Bold numbers signify the best performance for each metric column.</p>
          </fn>
          <fn id="tblfn2">
            <label>a</label>
            <p>The experimental results of SumGNN are from <xref rid="btac682-B29" ref-type="bibr">Yu <italic toggle="yes">et al.</italic> (2021)</xref>.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>In order to further explore the effectiveness of our designed attention method, we remove layer aggregation layers to get LaGAT-base. As shown in <xref rid="btac682-T2" ref-type="table">Table 2</xref>, LaGAT-base still significantly outperforms all baseline models. Meanwhile, we add layer aggregation layer to GAT, GAT-const and KGNN, respectively, and then conduct experiments on two datasets with different neighborhood sampling numbers <italic toggle="yes">k</italic>. The result is shown in <xref rid="btac682-F2" ref-type="fig">Figure 2</xref>.</p>
      <fig position="float" id="btac682-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>The performance of all models using layer aggregation layers. LaGAT still performs best. (<bold>a</bold>) Midazolam and Cyclosporine. (<bold>b</bold>) Midazolam and Amobarbital</p>
        </caption>
        <graphic xlink:href="btac682f2" position="float"/>
      </fig>
      <p>Combining the results in <xref rid="btac682-T2" ref-type="table">Table 2</xref> and <xref rid="btac682-F2" ref-type="fig">Figure 2</xref>, it can be seen that the performance of all models has been improved after adding the layer aggregation layer, and LaGAT is still significantly better than other models on both datasets, which shows that our proposed attention method is effective and does not depend on the layer aggregation layer. Among them, the performance of GAT and GAT-const still did not show a statistically significant difference under each parameter. This supports our hypothesis that when nodes need to notice different neighbors based on labels, GAT may degenerate into assigning the same weight to each node. At the same time, the way KGNN uses the inner product of the embedded representation of nodes and edges as the attention score is not better than GAT-const, which is why we do not use a similar way to introduce edge features.</p>
    </sec>
    <sec>
      <title>4.4 Case study</title>
      <p>We visualize the predict results of the model on the drugbank test set, and choose a common drug, Midazolam, as an example to show how our proposed attention method improves the accuracy of model predictions. Midazolam is a short-acting injectable benzodiazepine with rapid onset that is commonly used in seizures, anesthesia and anxiety disorders. We visualized the attention pathways generated by the model for two different types of DDI pairs picked from a drug pair collection containing Midazolam, as <xref rid="btac682-F3" ref-type="fig">Figure 3</xref> shown. We see that the model only assigns high attention weights to a small number of nodes, so we only label the top three nodes for each drug attention weight.</p>
      <fig position="float" id="btac682-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>LaGAT generates attention pathways to show why models predict DDI types. For each drug node, we annotate the top-3 neighbor nodes with attention weights and bold the connected edges. For the common neighbor nodes in the two drugs, we take the maximum value of its weight, and then use the node size to indicate the attention weight of each node</p>
        </caption>
        <graphic xlink:href="btac682f3" position="float"/>
      </fig>
      <p>For Midazolam and Cyclosporine, LaGAT assigns highest probability for the DDI type ’The serum concentration of drug one can be increased when it is combined with drug two’. Among the nodes common to both drugs, the model assigns the highest attention weight to the enzyme node, CYP 3A4. Studies have shown that cyclosporine is metabolized in the gut and liver by CYP450 enzymes, mainly contributed by CYP 3A4 and CYP 3A5, which competitively inhibits CYP3A4 in human liver microsomes (<xref rid="btac682-B1" ref-type="bibr">Amundsen <italic toggle="yes">et al.</italic>, 2012</xref>; <xref rid="btac682-B20" ref-type="bibr">Saiz-Rodríguez <italic toggle="yes">et al.</italic>, 2020</xref>). Meanwhile, midazolam biotransformation is mediated by CYP 3A4 with highly variable activity (<xref rid="btac682-B6" ref-type="bibr">Denisov <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac682-B8" ref-type="bibr">Gascon, 1991</xref>), which supports the predictions of our model.</p>
      <p>For Midazolam and Amobarbital, LaGAT assigns highest probability for the DDI type ’The risk or severity of adverse effects can be increased when Midazolam is combined with Amobarbital’. We note that among nodes common to both drugs, the model assigns the highest attention weight to the drug category node, Psycholeptics. In addition, the high attention nodes that the model pays attention to are the target node of Amobarbital, GABA(A) receptor subunit alpha-1 and the drug categorie node of Midazolam, GABA Agents. Studies have shown that amobarbital (like all barbiturates) works by binding to GABAA receptors at the alpha or beta subunit (<xref rid="btac682-B15" ref-type="bibr">Kim and Mathers, 2004</xref>; <xref rid="btac682-B32" ref-type="bibr">Zhu <italic toggle="yes">et al.</italic>, 2018</xref>). Like benzodiazepines, barbiturates potentiate the action of GABA on this receptor, but at a site that is different from the binding site for GABA itself and from the benzodiazepine binding site, which supports the predictions of our model.</p>
      <p>Noting that the model correctly predicts the DDI types of Midazolam and Amobarbital both before and after using our attention method. But for Midazolam and Cyclosporine, the model predicts correctly only after using our method. Meanwhile, the top-3 nodes with attention weights computed by LaGAT for Midazolam are completely different in the two DDI predictions. This supports our hypothesis that generating different attention pathways for the same drug according to different prediction targets can capture its different semantic information and help the model make correct DDI inferences.</p>
    </sec>
    <sec>
      <title>4.5 Ablation experiment</title>
      <p>To verify the effect of layer aggregation layer and aggregation method on performance, we conduct a series of ablation studies on our proposed model on two datasets. As mentioned above, we implemented two types of aggregation as aggregation methods, corresponding to Neigh-LA and Concat-LA in the following figures, respectively. We then remove the layer aggregation layer for these two models to obtain Neigh and Concat, and the final result is shown in <xref rid="btac682-F4" ref-type="fig">Figure 4</xref>.</p>
      <fig position="float" id="btac682-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>The performance of different variants of the model under different neighbor sampling size <italic toggle="yes">K</italic></p>
        </caption>
        <graphic xlink:href="btac682f4" position="float"/>
      </fig>
      <p>The results show that the neigh method performs better than the concat method on KEGG and the opposite on Drugbank. However, the layer aggregation layer can significantly improve the model using the neigh method, which may be because the concat method and the layer aggregation layer reuse the initial embedding representation of the node under the Hop parameter of 1. Note that although using the concat method without the layer aggregation layer can have better results on Drugbank, we still choose the neigh method as the aggregation method for LaGAT-base and show the results in Section 4.3, because the other models compared in Section 4.3 all use the neigh method as the aggregation layer.</p>
    </sec>
    <sec>
      <title>4.6 Parameter studies</title>
      <p>In this section, we investigate the effect of node embedding dimension and the number of neighborhood samples on our model. On KEGG, we set the model’s node embedding dimensions nd = 32, 64, 128, 256, 512, and explore the performance of the model in the neighborhood sampling number <italic toggle="yes">k</italic> = 8, 16, 32, 64, 128, 256 under each dimension; on Drugbank, we set the model separately The node embedding dimensions of nd = 16, 32, 64, 128, 256, explore the performance of the model in the neighborhood sampling number <italic toggle="yes">k</italic> = 8, 16, 32, 64, 128, 256 under each dimension; the remaining parameters are the same as those in Section 4.1. <xref rid="btac682-F5" ref-type="fig">Figures 5</xref> report the various metrics of our model on both datasets.</p>
      <fig position="float" id="btac682-F5">
        <label>Fig. 5.</label>
        <caption>
          <p>Model performance under different neighbor sample sizes <italic toggle="yes">K</italic> and node embedding dimension settings</p>
        </caption>
        <graphic xlink:href="btac682f5" position="float"/>
      </fig>
      <p><italic toggle="yes">Influence of neighborhood sampling size</italic>. We observe that appropriately increasing <italic toggle="yes">K</italic> can improve the performance of the model when the number of neighborhood samples <italic toggle="yes">k</italic> is small. This may be caused by insufficient capacity to contain semantic information when <italic toggle="yes">k</italic> is too small. With the further increase of <italic toggle="yes">k</italic>, the performance of the model on KEGG stabilizes, and the performance of the model on drugbank decreases greatly. We believe the reason is that the number of neighbors for each drug node in the drugbank dataset is about 100–300, while in the kegg dataset it is 10–100. When <italic toggle="yes">K</italic> exceeds the number of neighbors, the model will resample, so the model performance tends to be stable on kegg. However, on drugbank, considering too many different neighbors at the same time may make the model more likely to overfit some special neighbor nodes, which leads to a sharp drop in model performance when <italic toggle="yes">K</italic> is too large.</p>
      <p><italic toggle="yes">Influence of dimension of node embedding</italic>. The results show that an appropriate node dimension can make the model achieve the best performance. A too large node dimension will lead to overfitting of the model, and when <italic toggle="yes">k</italic> is large, the model performance is prone to a large decline. Too small node dimension is not enough to express enough semantic information. For the consideration of parameters and model stability, we finally chose 32 as the node embedding dimension on the drugbank data.</p>
    </sec>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>In this article, we propose a link-aware graph attention network for binary and multi-class drug interaction prediction tasks. It is able to generate different attention pathways for the same drug entity in different DDIs, providing interpretability for the results predicted by the model on KG. Experimental results on two real-world datasets demonstrate the effectiveness of our proposed method compared to several state-of-art works using KG. In addition, the visualization results of case study shows that our attention method can learn more accurate semantics, which is beneficial for the model to predict different types of DDIs. In our work, we mainly focus on extracting features from KGs via a plug-and-play link-aware graph attention layer. However, our method still suffers from the cold start problem. In the future, we will try to use the molecular features of drugs to perform feature fusion to solve this problem and further improve the model performance.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btac682_Supplementary_Data</label>
      <media xlink:href="btac682_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>The authors want to express gratitude to the anonymous reviewers for their hard work and kind comments, which will further improve our work in the future.</p>
    <sec>
      <title>Funding</title>
      <p>This work was supported by the National Natural Science Foundation of China [62072384, 61872309, 62072385 and 61772441], the Zhijiang Lab [2022RD0AB02].</p>
      <p><italic toggle="yes">Conflict of Interest</italic>: The authors declare that there is no conflict of interest.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btac682-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Amundsen</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2012</year>) <article-title>Cyclosporine a- and tacrolimus-mediated inhibition of cyp3a4 and cyp3a5 in vitro</article-title>. <source>Drug Metab. Dispos</source>., <volume>40</volume>, <fpage>655</fpage>–<lpage>661</lpage>.<pub-id pub-id-type="pmid">22205779</pub-id></mixed-citation>
    </ref>
    <ref id="btac682-B2">
      <mixed-citation publication-type="other">Asada, M. <etal>et al</etal> (2018). Enhancing drug-drug interaction extractionfrom texts by molecular structure information. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics(Volume 2: Short Papers), pages 680–685, Melbourne, Australia.Association for Computational Linguistics.</mixed-citation>
    </ref>
    <ref id="btac682-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bahdanau</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2014</year>) Neural machine translation by jointly learning to align and translate. arXiv, 0473</mixed-citation>
    </ref>
    <ref id="btac682-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Muffin: multi-scale feature fusion for drug–drug interaction prediction</article-title>. <source>Bioinformatics</source>, <volume>37</volume>, <fpage>2651</fpage>–<lpage>2658</lpage>.</mixed-citation>
    </ref>
    <ref id="btac682-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Cong</surname><given-names>Q.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Constructing biomedical knowledge graph based on semmeddb and linked open data. In: <italic toggle="yes">2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), Madrid, Spain</italic>. IEEE, pp. <fpage>1628</fpage>–<lpage>1631</lpage>.</mixed-citation>
    </ref>
    <ref id="btac682-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Denisov</surname><given-names>I.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Midazolam as a probe for drug–drug interactions mediated by cyp3a4: homotropic allosteric mechanism of site-specific hydroxylation</article-title>. <source>Biochemistry</source>, <volume>60</volume>, <fpage>1670</fpage>–<lpage>1681</lpage>.<pub-id pub-id-type="pmid">34015213</pub-id></mixed-citation>
    </ref>
    <ref id="btac682-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname><given-names>Z.</given-names></string-name></person-group><etal>et al</etal> (<year>2022</year>) <article-title>Kg-predict: a knowledge graph computational framework for drug repurposing</article-title>. <source>J. Biomed. Inf</source>., <volume>132</volume>, <fpage>104133</fpage>.</mixed-citation>
    </ref>
    <ref id="btac682-B8">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Gascon</surname><given-names>M.P.</given-names></string-name></person-group> (<year>1991</year>) In vitro forecasting of drugs which may interfere with the biotransformation of midazolam. <italic toggle="yes">Eur J Clin Pharmacol.</italic>, 41(6) <fpage>573</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="btac682-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Glorot</surname><given-names>X.</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group> (<year>2010</year>) Understanding the difficulty of training deep feedforward neural networks. In: Teh,Y.W. and Titterington,M. (eds.) <italic toggle="yes">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</italic>, volume 9 of <italic toggle="yes">Proceedings of Machine Learning Research</italic>, Chia Laguna Resort, Sardinia, Italy. PMLR, pp. <fpage>249</fpage>–<lpage>256</lpage>.</mixed-citation>
    </ref>
    <ref id="btac682-B10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hamilton</surname><given-names>W.L.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <part-title>Inductive representation learning on large graphs</part-title>. In: <source>Proceedings of the 31st International Conference on Neural Information Processing Systems</source>, NIPS’17. <publisher-name>Curran Associates Inc</publisher-name>., <publisher-loc>Red Hook, NY, USA</publisher-loc>, pp. <fpage>1025</fpage>–<lpage>1035</lpage></mixed-citation>
    </ref>
    <ref id="btac682-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harnoune</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Bert based clinical knowledge extraction for biomedical knowledge graph construction and analysis</article-title>. <source>Comput. Methods Programs Biomed. Update</source>, <volume>1</volume>, <fpage>100042</fpage>.</mixed-citation>
    </ref>
    <ref id="btac682-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>K.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Caster: predicting drug interactions with chemical substructure representation</article-title>. <source>AAAI</source>, <volume>34</volume>, <fpage>702</fpage>–<lpage>709</lpage>.</mixed-citation>
    </ref>
    <ref id="btac682-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jin</surname><given-names>B.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>Multitask dyadic prediction and its application in prediction of adverse drug–drug interaction</article-title>. <source>AAAI</source>, <volume>31</volume>,1367–373.</mixed-citation>
    </ref>
    <ref id="btac682-B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Karim</surname><given-names>M.R.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) Drug–drug interaction prediction based on knowledge graph embeddings and convolutional-LSTM network. In: <italic toggle="yes">Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics</italic>, BCB ’19. Association for Computing Machinery, New York, NY, USA, pp. <fpage>113</fpage>–<lpage>123</lpage>.</mixed-citation>
    </ref>
    <ref id="btac682-B15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kim</surname><given-names>H.S.</given-names></string-name>, <string-name><surname>Mathers</surname><given-names>D.P.E.</given-names></string-name></person-group> (<year>2004</year>) Selective GABA-receptor actions of amobarbital on thalamic neurons. <italic toggle="yes">Br J Pharmacol</italic>., 2004 Oct, <bold>143</bold>(4), <fpage>485</fpage>–<lpage>94</lpage>.</mixed-citation>
    </ref>
    <ref id="btac682-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>X.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) KGNN: knowledge graph neural network for drug–drug interaction prediction. In: <person-group person-group-type="editor"><string-name><surname>Bessiere</surname><given-names>C.</given-names></string-name></person-group> (ed.) <italic toggle="yes">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</italic>. International Joint Conferences on Artificial Intelligence Organization. Main track, pp. <fpage>2739</fpage>–<lpage>2745</lpage>.</mixed-citation>
    </ref>
    <ref id="btac682-B17">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ma</surname><given-names>T.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Drug similarity integration through attentive multi-view graph auto-encoders. In: <italic toggle="yes">Proceedings of the 27th International Joint Conference on Artificial Intelligence, Stockholm, Sweden</italic>, pp. <fpage>3477</fpage>–<lpage>3483</lpage>.</mixed-citation>
    </ref>
    <ref id="btac682-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nyamabo</surname><given-names>A.K.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Drug–drug interaction prediction with learnable size-adaptive molecular substructures</article-title>. <source>Brief. Bioinformatics</source>, <volume>22</volume>, <fpage>bbab441</fpage>.</mixed-citation>
    </ref>
    <ref id="btac682-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ryu</surname><given-names>J.Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Deep learning improves prediction of drug–drug and drug–food interactions</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>115</volume>, <fpage>E4304</fpage>–<lpage>E4311</lpage>.<pub-id pub-id-type="pmid">29666228</pub-id></mixed-citation>
    </ref>
    <ref id="btac682-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saiz-Rodríguez</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Effect of the most relevant cyp3a4 and cyp3a5 polymorphisms on the pharmacokinetic parameters of 10 cyp3a substrates</article-title>. <source>Biomedicines</source>, <volume>8</volume>, <fpage>94</fpage>.<pub-id pub-id-type="pmid">32331352</pub-id></mixed-citation>
    </ref>
    <ref id="btac682-B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Takeda</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) Predicting drug–drug interactions through drug structural similarities and interaction networks incorporating pharmacokinetics and pharmacodynamics knowledge. <italic toggle="yes">J. Cheminform., </italic>9, 16. https://doi.org/10.1186/s13321-017-0200-8.</mixed-citation>
    </ref>
    <ref id="btac682-B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Vashishth</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) Attention interpretability across NLP tasks. arXiv, 11218</mixed-citation>
    </ref>
    <ref id="btac682-B23">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Veličković</surname><given-names>P.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Graph attention networks. In: <italic toggle="yes">International Conference on Learning Representations, vancouver, Canada</italic>.</mixed-citation>
    </ref>
    <ref id="btac682-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vilar</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2014</year>) Similarity-based modeling in large-scale prediction of drug–drug interactions. <source>Nat Protoc</source>., <year>2014</year> Sep, <volume>9</volume>(9), <fpage>2147</fpage>–<lpage>63</lpage>.<pub-id pub-id-type="pmid">25122524</pub-id></mixed-citation>
    </ref>
    <ref id="btac682-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Whirl-Carrillo</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2012</year>) <article-title>Pharmacogenomics knowledge for personalized medicine</article-title>. <source>Clin. Pharmacol. Ther</source>., <volume>92</volume>, <fpage>414</fpage>–<lpage>417</lpage>.<pub-id pub-id-type="pmid">22992668</pub-id></mixed-citation>
    </ref>
    <ref id="btac682-B26">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wiegreffe</surname><given-names>S.</given-names></string-name>, <string-name><surname>Pinter</surname><given-names>Y.</given-names></string-name></person-group> (<year>2019</year>) Attention is not explanation. arXiv, 10186</mixed-citation>
    </ref>
    <ref id="btac682-B27">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Xu</surname><given-names>K.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) Representation learning on graphs with jumping knowledge networks. arXiv, 03536</mixed-citation>
    </ref>
    <ref id="btac682-B28">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ying</surname><given-names>C.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) Do transformers really perform bad for graph representation? arXiv, 05234</mixed-citation>
    </ref>
    <ref id="btac682-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>Y.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>SumGNN: multi-typed drug interaction prediction via efficient knowledge graph summarization</article-title>. <source>Bioinformatics</source>, <volume>37</volume>, <fpage>2988</fpage>–<lpage>2995</lpage>.</mixed-citation>
    </ref>
    <ref id="btac682-B30">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>J.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) GaAN: gated attention networks for learning on large and spatiotemporal graphs. arXiv,07294</mixed-citation>
    </ref>
    <ref id="btac682-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zheng</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>PharmKG: a dedicated knowledge graph benchmark for biomedical data mining</article-title>. <source>Brief. Bioinformatics</source>, <volume>22</volume>, <fpage>bbaa344</fpage>.<pub-id pub-id-type="pmid">33341877</pub-id></mixed-citation>
    </ref>
    <ref id="btac682-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhu</surname><given-names>S.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Structure of a human synaptic GABAA receptor</article-title>. <source>Nature</source>, <volume>559</volume>, <fpage>67</fpage>–<lpage>72</lpage>.<pub-id pub-id-type="pmid">29950725</pub-id></mixed-citation>
    </ref>
    <ref id="btac682-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zitnik</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) <article-title>Modeling polypharmacy side effects with graph convolutional networks</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>i457</fpage>–<lpage>i466</lpage>.<pub-id pub-id-type="pmid">29949996</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
