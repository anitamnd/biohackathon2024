<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nat Commun</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id>
    <journal-title-group>
      <journal-title>Nature Communications</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2041-1723</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10185530</article-id>
    <article-id pub-id-type="pmid">37188731</article-id>
    <article-id pub-id-type="publisher-id">38347</article-id>
    <article-id pub-id-type="doi">10.1038/s41467-023-38347-2</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>A general model to predict small molecule substrates of enzymes based on machine and deep learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Kroll</surname>
          <given-names>Alexander</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ranjan</surname>
          <given-names>Sahasra</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Engqvist</surname>
          <given-names>Martin K. M.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3940-1621</contrib-id>
        <name>
          <surname>Lercher</surname>
          <given-names>Martin J.</given-names>
        </name>
        <address>
          <email>Martin.Lercher@hhu.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.411327.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 2176 9917</institution-id><institution>Institute for Computer Science and Department of Biology, </institution><institution>Heinrich Heine University, </institution></institution-wrap>D-40225 Düsseldorf, Germany </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.417971.d</institution-id><institution-id institution-id-type="ISNI">0000 0001 2198 7527</institution-id><institution>Department of Computer Science and Engineering, </institution><institution>Indian Institute of Technology Bombay, </institution></institution-wrap>Powai, Mumbai, 400076 India </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.5371.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 0775 6028</institution-id><institution>Department of Biology and Bioengineering, </institution><institution>Chalmers University of Technology, </institution></institution-wrap>SE-412 96 Gothenburg, Sweden </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.502456.0</institution-id><institution>Present Address: EnginZyme AB, </institution></institution-wrap>Tomtebodevägen 6, 17165 Stockholm, Sweden </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>5</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>15</day>
      <month>5</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>14</volume>
    <elocation-id>2787</elocation-id>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>6</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>21</day>
        <month>4</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">For most proteins annotated as enzymes, it is unknown which primary and/or secondary reactions they catalyze. Experimental characterizations of potential substrates are time-consuming and costly. Machine learning predictions could provide an efficient alternative, but are hampered by a lack of information regarding enzyme non-substrates, as available training data comprises mainly positive examples. Here, we present ESP, a general machine-learning model for the prediction of enzyme-substrate pairs with an accuracy of over 91% on independent and diverse test data. ESP can be applied successfully across widely different enzymes and a broad range of metabolites included in the training data, outperforming models designed for individual, well-studied enzyme families. ESP represents enzymes through a modified transformer model, and is trained on data augmented with randomly sampled small molecules assigned as non-substrates. By facilitating easy in silico testing of potential substrates, the ESP web server may support both basic and applied science.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">For many enzymes, it is unknown which primary and/or secondary reactions they catalyze. Here, the authors use machine and deep learning to develop a general model for the prediction of enzyme-small molecule substrate pairs and make the resulting model available through a webserver.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Machine learning</kwd>
      <kwd>Enzymes</kwd>
      <kwd>Protein function predictions</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100001659</institution-id>
            <institution>Deutsche Forschungsgemeinschaft (German Research Foundation)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>CRC 1310 + 390686111</award-id>
        <principal-award-recipient>
          <name>
            <surname>Lercher</surname>
            <given-names>Martin J.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100001663</institution-id>
            <institution>Volkswagen Foundation (VolkswagenStiftung)</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Springer Nature Limited 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par3">Enzymes evolved to efficiently catalyze one or more specific chemical reactions, increasing reaction rates up to over a million-fold over the spontaneous rates<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. In addition, most enzymes are promiscuous, i.e., they catalyze further, physiologically irrelevant or even harmful reactions<sup><xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR4">4</xref></sup>. Accordingly, a comprehensive mapping of enzyme-substrate relationships plays a crucial role in pharmaceutical research and bio-engineering, e.g., for the production of drugs, chemicals, food, and biofuels<sup><xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR7">7</xref></sup>.</p>
    <p id="Par4">Unfortunately, it is both expensive and time-consuming to determine experimentally which reactions are catalyzed by a given enzyme. There is thus a huge imbalance between the number of proteins predicted to be enzymes and the experimental knowledge about their substrate scopes. While the UniProt database<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> contains entries for over 36 million different enzymes, more than 99% of these lack high-quality annotations of the catalyzed reactions. Efforts are underway to develop high-throughput methods for the experimental determination of enzyme-substrate relationships, but these are still in their infancy<sup><xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR11">11</xref></sup>. Furthermore, even high-throughput methods cannot deal with the vast search space of all possible small molecule substrates, but require the experimenter to choose a small subset for testing.</p>
    <p id="Par5">Our goal in this study was to develop a single machine learning model capable of predicting enzyme-substrate relationships across all proteins, thereby providing a tool that helps to focus experimental efforts on enzyme-small molecule pairs likely to be biologically relevant. Developing such a model faces two major challenges. First, a numerical representation of each enzyme that is maximally informative for the downstream prediction task must be obtained<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. To be as broadly applicable as possible, these representations should be based solely on the enzymes’ primary sequence and not require additional features, such as binding site characteristics. Second, public enzyme databases only list positive instances, i.e., molecules with which enzymes display measurable activity (substrates)<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. For training a prediction model, an automated strategy for obtaining suitable negative, non-binding enzyme-small molecule instances must thus be devised.</p>
    <p id="Par6">Existing machine learning approaches for predicting enzyme-substrate pairs were either developed specifically for small enzyme families for which unusually comprehensive training datasets are available<sup><xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR18">18</xref></sup>, or they are only capable of connecting substrates with EC classes but not with specific enzymes. For example, Mou et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> developed models to predict the substrates of bacterial nitrilases, using input features based on the 3D-structures and active sites of the enzymes. They trained various machine learning models based on experimental evidence for all possible enzyme-small molecule combinations within the models’ prediction scope (<italic>N</italic> = 240). Yang et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> followed a similar approach, predicting the substrate scope of plant glycosyltransferases among a pre-defined set of small molecules. They trained a decision tree-based model with a dataset covering almost all possible combinations of enzymes and relevant small molecules. Pertusi et al.<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> trained four different support vectors machines (SVMs), each for a specific enzyme. As input features, their models only use information about the (potential) substrates, as well as non-substrates manually extracted from the literature; no explicit information about the enzymes was used. Roettig et al.<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> and Chevrette et al.<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> predicted the substrate scopes of small enzyme families, training machine-learning models with structural information relating to the enzymes’ active sites. Finally, Visani et al.<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> implemented a general machine-learning model for predicting suitable EC classes for a given substrate. To train this model, all EC classes that are not associated with a certain substrate were used as negative data points, which resulted in a low average positive to negative ratio of 0.0032. Visani et al. did not use any enzyme information beyond the EC class as model input, and therefore the model cannot distinguish between different enzymes assigned to the same EC class.</p>
    <p id="Par7">All these previous models can either not be applied to individual enzymes, or they aim to predict substrates for only a single enzyme or enzyme family. Those models that make predictions for specific enzymes rely on very dense experimental training data, i.e., experimental results for all or almost all potential enzyme-substrate pairs. However, for the vast majority of enzyme families, such extensive training data is not available. As yet, there have been no published attempts to formulate and train a general model that can be applied to predict substrates for specific enzymes across widely different enzyme families. Deep learning models have been used to predict enzyme functions by either predicting their assignment to EC classes<sup><xref ref-type="bibr" rid="CR20">20</xref>–<xref ref-type="bibr" rid="CR22">22</xref></sup>, or by predicting functional domains within the protein sequence<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. However, different enzymes sharing the same domain architecture or assigned to the same EC class can have highly diverse substrate scopes<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Directly predicting specific substrates for enzymes goes an important step beyond those previous methods and can help to predict enzyme function more specifically and more precisely.</p>
    <p id="Par8">Prior work related to the prediction of enzyme-substrate pairs are the prediction of drug-target binding affinities (DTBAs) and of Michaelis-Menten constants, <italic>K</italic><sub>M</sub> and <italic>k</italic><sub>cat</sub>. State-of-the-art approaches in this domain are feature-based, i.e., numerical representations of the protein and the substrate molecule are used as input to machine learning models<sup><xref ref-type="bibr" rid="CR25">25</xref>–<xref ref-type="bibr" rid="CR29">29</xref></sup>. As numerical descriptions of the substrate molecule, these approaches use SMILES representations<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, expert-crafted fingerprints<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, or fingerprints created with graph neural networks<sup><xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>. Proteins are usually encoded numerically through deep learning-based representations of the amino acid sequences<sup><xref ref-type="bibr" rid="CR34">34</xref>–<xref ref-type="bibr" rid="CR36">36</xref></sup>. However, these approaches cannot be transferred one-to-one to the problem of predicting enzyme-substrate pairs. The <italic>K</italic><sub>M</sub> and <italic>k</italic><sub>cat</sub> prediction models are exclusively trained with positive enzyme-substrate pairs and therefore cannot classify molecules as substrates or non-substrates<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>. Many of the proteins used to train the DTBA prediction models have no enzymatic functions; even if they do, the molecules used for training are mostly not naturally occurring potential substrates, and thus there has been no natural selection for or against binding. In contrast, the binding between enzymes and substrates evolved under natural selection. It appears likely that this evolutionary relationship influences our ability to predict enzyme-substrate pairs, and DTBA models are thus not expected to perform well at this task.</p>
    <p id="Par9">In this work, we go beyond the current state-of-the-art by creating maximally informative protein representations, using a customized, task-specific version of the ESM-1b transformer model<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. The model contains an extra 1280-dimensional token, which was trained end-to-end to store enzyme-related information salient to the downstream prediction task. This general approach was first introduced for natural language processing<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, but has not yet been applied to protein feature prediction. We created negative training examples using data augmentation, by randomly sampling small molecules similar to the substrates in experimentally confirmed enzyme-substrate pairs. Importantly, we sampled all negative data points from a limited set of metabolites, the set of ~ 1400 substrates that occur among all experimentally confirmed enzyme-substrate pairs of our dataset. Thus, we do not sample from the space of all possible alternative reactants similar to the true substrates, but only consider small molecules likely to occur in at least some biological cells. While many enzymes are rather promiscuous<sup><xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR4">4</xref></sup>, it is likely that most of the potential secondary substrates are not contained in this restricted set for any given enzyme, and hence the chance of sampling false negative data points was likely small. We numerically represented all small molecules with task-specific fingerprints that we created with graph neural networks (GNNs)<sup><xref ref-type="bibr" rid="CR38">38</xref>–<xref ref-type="bibr" rid="CR40">40</xref></sup>. A gradient-boosted decision tree model was trained on the combined protein and small molecule representations for a high-quality dataset with ~18,000 very diverse, experimentally confirmed positive enzyme-substrate pairs (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). The resulting Enzyme Substrate Prediction model – ESP – achieves high prediction accuracy for those ~1400 substrates that have been part of our training set and outperforms previously published enzyme family-specific prediction models.<fig id="Fig1"><label>Fig. 1</label><caption><title>Model overview.</title><p>Experimentally validated enzyme-substrate pairs and sampled negative enzyme-small metabolite pairs are numerically represented with task-specific enzyme and small molecule representations. Concatenated enzyme-small molecule representations are used to train a gradient-boosting model. After training, the fitted model can be used to predict promising candidate substrates for enzymes.</p></caption><graphic xlink:href="41467_2023_38347_Fig1_HTML" id="d32e468"/></fig></p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>Obtaining training and test data</title>
      <p id="Par10">We created a dataset with experimentally confirmed enzyme-substrate pairs using the GO annotation database for UniProt IDs<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> (Methods, “Creating a database with enzyme-substrate pairs”). For training our machine learning models, we extracted 18,351 enzyme-substrate pairs with experimental evidence for binding, comprised of 12,156 unique enzymes and 1379 unique metabolites. We also extracted 274,030 enzyme-substrate pairs with phylogenetically inferred evidence, i.e., these enzymes are evolutionarily closely related to enzymes associated with the same reactions. These “guilt by association” assignments are much less reliable than direct experimental evidence, and we only used them during pre-training to create task-specific enzyme representations – numerical vectors aimed at capturing information relevant to the prediction task from the enzyme amino acid sequences. Our validations demonstrate that using phylogenetically inferred functions for the construction of appropriate enzyme representations has a positive effect on the prediction of experimentally confirmed enzyme-substrate pairs (see below, “Fine-tuning of a state-of-the-art protein embedding model”).</p>
      <p id="Par11">There is no systematic information on negative enzyme-small molecule pairs, i.e., pairs where the molecule is not a substrate of the enzyme. We hypothesized that such negative data points could be created artificially through random sampling, which is a common strategy in classification tasks that lack negative training data<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. To challenge our model to learn to distinguish similar binding and non-binding reactants, we sampled negative training data only from enzyme-small molecule pairs where the small molecule is structurally similar to a known true substrate. However, we only considered small molecules included among the experimentally confirmed enzyme-substrate pairs in our dataset. Among such a limited and biased subset, enzymes are quite specific catalysts, and therefore most of the potential secondary substrates are not included for the majority of enzymes. Thus, we assumed that the frequency of incorrectly created negative labels is sufficiently low to not adversely affect model performance. This assumption was confirmed by the high model accuracy on independent test data, as detailed below.</p>
      <p id="Par12">To select putatively non-binding small molecules that are structurally similar to the known substrates, we used a similarity score based on molecular fingerprints, with values ranging from 0 (no similarity) to 1 (identity; see Methods, “Sampling negative data points”). For every positive enzyme-substrate pair, we sampled three molecules with similarity scores between 0.75 and 0.95 to the actual substrate of the enzyme, and used them to construct negative enzyme-molecule pairs. We opted for creating more negative data points than we have positive data points, as this not only provided us with more data, but it also more closely reflects the true distribution of positive and negative data points compared to a balanced distribution.</p>
      <p id="Par13">Our final dataset comprises 69,365 entries. We split this data into a training set (80%) and a test set (20%). In many machine learning domains, it is standard practice to split the data into training and test set completely at random. However, when dealing with protein sequences, this strategy often leads to test sets with amino acid sequences that are almost identical to those of proteins in the training set. Such close homologs often share the same function<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, and the assessment of model performance could thus be overly optimistic. It is therefore common practice to split such datasets into training, validation, and test sets based on protein sequences similarities<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. Here, we made sure that no enzyme in the test set has a sequence identity higher than 80% compared to any enzyme in the training set. To show that despite this sequence-based partitioning, enzymes from the training and test sets follow the same distribution, we used dimensionality reduction to map all enzymes to a two-dimensional subspace and plotted the corresponding data points (Supplementary Fig. <xref rid="MOESM1" ref-type="media">1)</xref>. To evaluate how well our final model performs for different levels of enzyme similarities, we divided the test set further into three subsets with maximal sequence identities between 0–40%, 40–60%, and 60–80% compared to all enzymes in the training set.</p>
    </sec>
    <sec id="Sec4">
      <title>Representing small molecules as numerical vectors</title>
      <p id="Par14">Extended-connectivity fingerprints (ECFPs) are expert-crafted binary representations for small molecules. The molecules are represented as graphs, with atoms interpreted as nodes and chemical bonds as edges. For the numerical encoding, one classifies bond types and calculates feature vectors with information about every atom (types, masses, valences, atomic numbers, atom charges, and number of attached hydrogen atoms)<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. Afterwards, these identifiers are updated for a fixed number of steps by iteratively applying predefined functions to summarize aspects of neighboring atoms and bonds. After the iteration process, all identifiers are converted into a single binary vector with structural information about the molecule. The number of iterations and the dimension of the fingerprint can be chosen freely. We set them to the default values of 3 and 1024, respectively. For comparison, we also created 512- and 2048-dimensional ECFPs, but these led to slightly inferior predictions (Supplementary Fig. <xref rid="MOESM1" ref-type="media">2)</xref>. Using ECFPs can lead to identical representations for structurally very similar molecules, e.g., for some molecules that differ only by the length of a chain of carbon atoms. In our dataset, 182 out of 1379 different molecules shared an identical fingerprint with a structurally similar molecule.</p>
      <p id="Par15">As an alternative to expert-crafted fingerprints such as ECFPs, neural networks can be used to learn how to map graph representations of small molecules to numerical vectors. Such networks are referred to as graph neural networks (GNNs)<sup><xref ref-type="bibr" rid="CR38">38</xref>–<xref ref-type="bibr" rid="CR40">40</xref></sup>. We trained a GNN for the binary task of predicting if a small molecule is a substrate for a given enzyme. While training for this task, the GNN is challenged to store all information about the small molecule that is relevant for solving the prediction task in a single numerical vector. After training, we extracted these 100-dimensional task-specific vectors for all small molecules in our dataset. It has been observed that pre-training GNNs for a related task can significantly improve model performance<sup><xref ref-type="bibr" rid="CR45">45</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>. Thus, we first pre-trained a GNN for the related task of predicting the Michaelis constants <italic>K</italic><sub>M</sub> of enzyme-substrate pairs (see Methods, “Calculating task-specific fingerprints for small molecules”). As shown below (see “Successful prediction of enzyme-substrate pairs”), pre-training indeed improved prediction performance significantly. In contrast to ECFPs, GNN-generated fingerprints lead to much fewer cases of identical representations for different molecules. In our dataset, identical fingerprints occurred for 42 out of 1379 molecules.</p>
    </sec>
    <sec id="Sec5">
      <title>Fine-tuning of a state-of-the-art protein embedding model</title>
      <p id="Par16">The ESM-1b model is a state-of-the-art transformer network<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, trained with ~27 million proteins from the UniRef50 dataset<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> in a self-supervised fashion<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. This model takes an amino acid sequence as its input and puts out a numerical representation of the sequence; these representations are often referred to as protein embeddings. During training of ESM-1b, ~15% of the amino acids in a protein’s sequence are randomly masked and the model is trained to predict the identity of the masked amino acids (Fig. <xref rid="Fig2" ref-type="fig">2</xref>a). This training procedure forces the model to store both local and global information about the protein sequence in one 1280-dimensional representation vector for each individual amino acid. In order to create a single fixed-length numerical representation of the whole protein, one typically calculates the element-wise mean across all amino acid representations<sup><xref ref-type="bibr" rid="CR34">34</xref>,<xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR49">49</xref></sup>. We refer to these protein representations as ESM-1b vectors.<fig id="Fig2"><label>Fig. 2</label><caption><title>A task-specific enzyme representation developed from the ESM-1b model.</title><p><bold>a</bold> ESM-1b model. Amino acids of a protein sequence are represented with numerical vectors and passed through a transformer network. Some amino acid representations are masked. All representations are iteratively updated 33 times, using information about neighboring and distant amino acids. The ESM-1b model is trained to predict the masked amino acids. ESM-1b vectors are calculated by taking the element-wise mean of all representations in the last layer. <bold>b</bold> Modified ESM-1b model. An additional representation for the whole enzyme is added to the amino acid representations. After updating all representations 33 times, the enzyme representation is concatenated with a small molecule representation. The network is trained to predict whether the small molecule is a substrate for the given enzyme. After training, the ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vector is extracted as the enzyme representation before adding the small molecule representation.</p></caption><graphic xlink:href="41467_2023_38347_Fig2_HTML" id="d32e587"/></fig></p>
      <p id="Par17">However, simply taking the element-wise mean results in information loss and does not consider the task for which the representations shall be used, which can lead to subpar performance<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. To overcome these issues, we created task-specific enzyme representations optimized for the prediction of enzyme-substrate pairs. We slightly modified the architecture of the ESM-1b model, adding one additional 1280-dimensional token to represent the complete enzyme, intended to capture information salient to the downstream prediction task (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b). The extra token is not adding input information to the model, but it allows an easier extraction of enzyme information from the trained model. This whole-enzyme representation was updated in the same way as the regular ESM-1b amino acid representations.</p>
      <p id="Par18">After a predefined number of update steps, the enzyme representation was concatenated with the small molecule ECFP-vector. The combined vector was used as the input for a fully connected neural network (FCNN), which was then trained end-to-end to predict whether the small molecule is a substrate for the enzyme. This approach facilitates the construction of a single, optimized, task-specific representation. The ESM-1b model contains many parameters and thus requires substantial training data. Therefore, in the pre-training that produces the task-specific enzyme representations, we added phylogenetically inferred evidence to our training set; this resulted in a total of ~287,000 data points used for training the task-specific enzyme representation. After training, we used the network to extract the 1280-dimensional task-specific representations for all enzymes in our dataset. In the following, these representations are called ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors.</p>
    </sec>
    <sec id="Sec6">
      <title>Successful prediction of enzyme-substrate pairs</title>
      <p id="Par19">To compare the performances of the different enzyme representations (ESM-1b and ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors) and of the two small molecule representations (ECFPs and GNN-generated fingerprints), we estimated prediction quality on our test set when using machine learning models with each of the four combinations of enzyme and small molecule representations. In each case, we concatenated one of the two 1280-dimensional enzyme representations with one of the two small molecule representations to create a single input vector for every enzyme-small molecule pair. We used these inputs to train gradient-boosted decision tree models<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> for the binary classification task of predicting whether the small molecule is a substrate for the enzyme.</p>
      <p id="Par20">We performed hyperparameter optimizations for all four models, including the parameters learning rate, depth of trees, number of iterations, and regularization coefficients. For this, we performed a random grid search with a 5-fold cross-validation (CV) on the training set. To challenge the model to learn to predict the substrate scope of enzymes not included in the training data, we made sure that each enzyme occurred in only one of the five subsets used for cross-validation (Methods, “Hyperparameter optimization of the gradient boosting models”). To account for the higher number of negative compared to positive training data, we also included a weight parameter that lowered the influence of the negative data points. The results of the cross-validations are displayed as boxplots in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a. The best sets of hyperparameters are listed in Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>. After hyperparameter optimization, the models were trained with the best set of hyperparameters on the whole training set and were validated on our independent test set, which had not been used for model training or hyperparameter selection. It is noteworthy that for some input combinations, the accuracies on the test set are higher than the accuracies achieved during cross-validation (Fig. <xref rid="Fig3" ref-type="fig">3</xref>a). This improved performance on the test set may result from the fact that before validation on the test set, models are trained with approximately 11,000 more samples than before each cross-validation; the number of training samples has a substantial influence on model performance (see below, “Model performance increases with increased training set size”).<fig id="Fig3"><label>Fig. 3</label><caption><title>Optimized models provide accurate predictions of enzyme-substrate pairs.</title><p><bold>a</bold> Accuracies. Boxplots summarize the results of the CV with <italic>n</italic> = 5 folds on the training set with the best sets of hyperparameters. We used a 2 × interquartile range for the whiskers, the boxes extend from the lower to upper quartile values, and the red horizontal lines are displaying the median of the data points. Blue dots display the accuracies on the test set, using the optimized models trained on the whole training set. <bold>b</bold> ROC curves for the test set. The dotted line displays the ROC curve expected for a completely random model. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_38347_Fig3_HTML" id="d32e649"/></fig></p>
      <p id="Par21">Commonly used metrics to measure the performance of binary classification models are accuracy, ROC-AUC score, and Matthews correlation coefficient (MCC). Accuracy is simply the fraction of correctly predicted data points among the test data. The ROC-AUC score is a value between 0 and 1 that summarizes how well a classifier is able to distinguish between the positive and negative classes, where a value of 0.5 would result from a model that randomly assigns class labels, and a value of 1 corresponds to perfect predictions. The MCC is a correlation coefficient for binary data, comparable to the Pearson correlation coefficient for continuous data; it takes values between -1 and +1, where 0 would result from a model that randomly assigns class labels, and +1 indicates perfect agreement.</p>
      <p id="Par22">As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref> and Table <xref rid="Tab1" ref-type="table">1</xref>, models with task-specific enzyme and/or small molecule representations performed better than those with generic representations. The best-performing model combined the fine-tuned ESM-1b<sub><italic>t</italic><italic>s</italic></sub> enzyme representations with the GNN-generated small molecule fingerprints, achieving an accuracy of 91.5%, a ROC-AUC score of 0.956, and an MCC of 0.78. The difference between the two best models (ESM-1b<sub><italic>t</italic><italic>s</italic></sub> + GNN vs. ESM-1b<sub><italic>t</italic><italic>s</italic></sub> + ECFP) is statistically highly significant (McNemar’s test: <italic>p</italic> &lt; 10<sup>−5</sup>). For the final ESP model, we thus chose to represent enzymes with ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors and small molecules with GNN-generated, task-specific fingerprints.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Prediction performance on the test set for all four combinations of enzyme and small molecule representations</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>ROC-AUC score</th><th>Accuracy</th><th>MCC</th></tr></thead><tbody><tr><td>ESM-1b+ECFP</td><td>0.937</td><td>87.2%</td><td>0.69</td></tr><tr><td>ESM-1b<sub><italic>t</italic><italic>s</italic></sub>+ECFP</td><td>0.950</td><td>90.5%</td><td>0.75</td></tr><tr><td>ESM-1b+GNN</td><td>0.940</td><td>88.8%</td><td>0.72</td></tr><tr><td>ESM-1b<sub><italic>t</italic><italic>s</italic></sub>+GNN</td><td>0.956</td><td>91.5%</td><td>0.78</td></tr></tbody></table></table-wrap></p>
      <p id="Par23">To compare the gradient boosting model to alternative machine learning models, we also trained a logistic regression model and a random forest model for the task of predicting enzyme-substrate pairs from the combined ESM-1b<sub><italic>t</italic><italic>s</italic></sub> and GNN vectors. However, these models performed worse compared to the gradient boosting model (Supplementary Table <xref rid="MOESM1" ref-type="media">2)</xref>.</p>
      <p id="Par24">The GNN used to represent small molecules in the best-performing model was pre-trained for the task of predicting the Michaelis constants <italic>K</italic><sub>M</sub> of enzyme-substrate pairs. To test if this pre-training improved the predictions, we also tested model performance for fingerprints that were created with a GNN that was not pre-trained. Using a pre-trained GNN indeed led to better model performance (Supplementary Table <xref rid="MOESM1" ref-type="media">3</xref>; <italic>p</italic> &lt; 10<sup>−7</sup> from McNemar’s test).</p>
      <p id="Par25">The results summarized in Table <xref rid="Tab1" ref-type="table">1</xref> demonstrate that re-training and fine-tuning the ESM-1b model can significantly improve model performance. This finding contrasts previous observations that fine-tuning protein representations can negatively influence model performance and can lead to worse results compared to using the original ESM-1b model<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR51">51</xref></sup>. To achieve the improved enzyme representations, we added an extra token for the whole enzyme, and we trained the model to store all relevant information for the prediction task in this token. To investigate the importance of the added token for the observed superior performance, we alternatively re-trained the ESM-1b without such an extra token. Our results show that using the extra token indeed improves model performance (Supplementary Table <xref rid="MOESM1" ref-type="media">4</xref>; <italic>p</italic> = 0.040 from McNemar’s test).</p>
    </sec>
    <sec id="Sec7">
      <title>Good predictions even for unseen enzymes</title>
      <p id="Par26">It appears likely that prediction quality is best for enzymes that are highly similar to enzymes in the training set, and decreases for enzymes that are increasingly dissimilar to the enzymes used for training. How strong is that dependence? To answer this question, we first calculated the maximal enzyme sequence identity compared to the enzymes in the training set for all 2291 enzymes in the test set. Next, we split the test set into three subgroups: data points with enzymes with a maximal sequence identity to training data between 0 and 40%, between 40% and 60%, and between 60% and 80%.</p>
      <p id="Par27">For data points with high sequence identity levels (60-80%), the ESP model is highly accurate, with an accuracy of 95%, ROC-AUC score of 0.99, and MCC of 0.88 (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). ESP still performs very well for data points with intermediate sequence identity levels (40–60%), achieving an accuracy of 93%, ROC-AUC score 0.97, and MCC 0.83. Even for enzymes with low sequence identity to training data (0−40%), the ESP model achieves good results and classifies 89% of the data points correctly, with ROC-AUC score 0.93 and MCC 0.72. Thus, while using more similar enzymes during training improves the prediction quality, very good prediction accuracy can still be achieved for enzymes that are only distantly related to those in the training set. The observed differences were statistically significant for sequence identities 0–40% versus 40–60% (Mann-Whitney <italic>U</italic> test: <italic>p</italic> &lt; 10<sup>−23</sup>), but not for 40–60% versus 60–80% (<italic>p</italic> = 0.14).<fig id="Fig4"><label>Fig. 4</label><caption><title>Accurate predictions even for enzymes with distinct sequence similarity compared to enzymes in the training data.</title><p>We divided the test set into subsets with different levels of enzyme sequence identity compared to enzymes in the training set. <bold>a</bold> ESP accuracies, calculated separately for enzyme-small molecule pairs where the small molecule occurred in the training set and where it did not occur in the training set. <bold>b</bold> ESP ROC curves. The dotted line displays the ROC curve expected for a completely random model. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_38347_Fig4_HTML" id="d32e864"/></fig></p>
    </sec>
    <sec id="Sec8">
      <title>Low model performance for unseen small molecules</title>
      <p id="Par28">In the previous subsection, we showed that model performance is highest for enzymes that are similar to proteins in the training set. Similarly, it appears likely that the model performs better when making predictions for small molecules that are also in the training set. To test this hypothesis, we divided the test set into data points with small molecules that occurred in the training set (<italic>N</italic> = 13,459) and those with small molecules that did not occur in the training set (<italic>N</italic> = 530).</p>
      <p id="Par29">The ESP model does not perform well for data points with small molecules not present in the training set. When considering only enzyme-small molecules pairs with small molecules not represented in the training set and an enzyme sequence identity level of 0–40% compared to the training data, ESP achieves an accuracy of 71%, ROC-AUC score 0.59, and MCC 0.15. At an enzyme sequence identity level of 40–60%, accuracy improves to 83%, with ROC-AUC score 0.78, and MCC 0.25 for unseen small molecules. At high enzyme sequence identity levels of 60–80%, the accuracy reaches 90%, with ROC-AUC score 0.71, and MCC 0.27. Thus, for unseen small molecules, even a very moderate model performance requires that proteins similar to the enzyme ( &gt; 40% identity) are present in the training set. We again found the differences to be statistically significant for 0–40% versus 40–60% (Mann-Whitney <italic>U</italic> test: <italic>p</italic> &lt; 10<sup>−20</sup>), but not for 40–60% versus 60–80% (<italic>p</italic> = 0.226).</p>
      <p id="Par30">For those test data points with small molecules not present in the training set, we wondered if a high similarity of the small molecule compared to at least one substrate in the training set leads to improved predictions, analogous to what we observed for enzymes with higher sequence identities. For each small molecules not present in the training set, we calculated the maximal pairwise similarity score compared to all substrates in the training set. We could not find any evidence that a higher maximal similarity score leads to better model performance (Supplementary Fig. <xref rid="MOESM1" ref-type="media">3)</xref>. Hence, we conclude that ESP only achieves high accuracies for new enzyme-small molecule pairs if the small molecule was present among the ~1 400 substrates of our training set.</p>
      <p id="Par31">How many training data points with identical substrates are needed to achieve good model performance? For every small molecule in the test set, we counted how many times the same molecule occurs as an experimentally confirmed substrate in the training set. Supplementary Fig. <xref rid="MOESM1" ref-type="media">4</xref> shows that having as few as two positive training data points for a given small molecule leads to good accuracy when pairing the same small molecule with other enzymes.</p>
    </sec>
    <sec id="Sec9">
      <title>Model performance increases with increased training set size</title>
      <p id="Par32">The previous subsections suggest that a bigger training set with a more diverse set of enzymes and small molecules should lead to improved performance. However, using more data does not guarantee an improved model performance. For example, there could be a limitation in the model architecture that prevents the model from better fitting the data. To test how our model performs with different amounts of training data and to analyze if more data is expected to lead to higher generalizability, we trained the gradient boosting model with different training set sizes, ranging from 30% to 100% of the available training data. Figure <xref rid="Fig5" ref-type="fig">5</xref> shows that accuracy and ROC-AUC score indeed increase with increasing training set size (Spearman rank correlations, accuracy: <italic>ρ</italic><sup>2</sup> = 0.95, <italic>p</italic> &lt; 10<sup>−4</sup>; ROC-AUC score: <italic>ρ</italic><sup>2</sup> = 1.0, <italic>p</italic> &lt; 10<sup>−15</sup>). Thus, collecting more and more diverse data – for example, through targeted additional experiments – will likely lead to further model improvements.<fig id="Fig5"><label>Fig. 5</label><caption><title>Model performance increases with training set size.</title><p>Points show accuracies and ROC-AUC scores for the test set versus the fraction of the available training data used for training the gradient-boosting model. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_38347_Fig5_HTML" id="d32e941"/></fig></p>
    </sec>
    <sec id="Sec10">
      <title>ESP can express uncertainty</title>
      <p id="Par33">Internally, our trained classification model does not simply output the positive or negative class as a prediction. Instead, it outputs a prediction score between 0 and 1, which can be interpreted as a measurement of the probability for a data point to belong to the positive class. So far, we assigned all predictions with a score ≥0.5 to the positive class, and all predictions below 0.5 to the negative class. To provide a more detailed view of prediction accuracies, Fig. <xref rid="Fig6" ref-type="fig">6</xref> displays the distributions of the true (blue) and false (red) predictions for our test set across prediction scores.<fig id="Fig6"><label>Fig. 6</label><caption><title>Prediction scores around 0.5 indicate model uncertainty.</title><p>Stacked histogram bars display the prediction score distributions of true predictions (blue) and false predictions (red). The inset shows a blow-up of the interval [0.2, 0.8]. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_38347_Fig6_HTML" id="d32e960"/></fig></p>
      <p id="Par34">Most true predictions have a score either close to 0 or close to 1, i.e., the ESP model is very confident about these predictions. In contrast, false predictions are distributed much more evenly across prediction scores. Approximately 4% of prediction scores for our test data fall between 0.4 and 0.6. The model seems to be uncertain for these data points: for this subset, predictions are only barely better than random guesses, with an accuracy of 59%, ROC-AUC score 0.60, and MCC 0.17 (Fig. <xref rid="Fig6" ref-type="fig">6</xref>, inset). Thus, when applied in practice, prediction scores between 0.4 and 0.6 should be considered uncertain and should not be assigned to one of the two classes.</p>
    </sec>
    <sec id="Sec11">
      <title>ESP outperforms two recently published models</title>
      <p id="Par35">We compared ESP with two recently published models for predicting the substrate scopes of specific enzyme families. ESP has been trained with much more data points compared to the previously published models; conversely, these previous models used much more detailed input information. Thus, a fair, direct comparison of model architectures is impossible. Instead, we analyzed if our model, which is capable of making use of large amounts of freely available data, can lead to better prediction accuracies than much more targeted approaches that necessarily work on smaller datasets.</p>
      <p id="Par36">Mou et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> trained four different machine learning models (logistic regression, random forest, gradient-boosted decision trees, and support vector machines) to predict substrates of bacterial nitrilases. For model training and validation, they used a dataset with all possible combinations of 12 enzymes and 20 small molecules (<italic>N</italic> = 240), randomly split into 80% training data and 20% test data. We added all training data from Ref. <sup><xref ref-type="bibr" rid="CR14">14</xref></sup> to our training set and validated the updated ESP model on the corresponding test data, which had no overlap with our training data. Mou et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> achieved an accuracy of 82% and a ROC-AUC score of 0.90 on the test set. ESP achieves better results, with an accuracy of 87.5%, ROC-AUC score 0.94, and MCC 0.75. This improvement is particularly striking given that Mou et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> used knowledge about the enzymes’ 3D structures and binding sites, while we only use a representation of the linear amino acid sequences.</p>
      <p id="Par37">Yang et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> published a decision tree-based model, GT-Predict, for predicting the substrate scope of glycosyltransferases of plants. As a training set, they used 2847 data points with 59 different small molecules and 53 different enzymes from <italic>Arabidopsis thaliana</italic>, i.e., the data covered 90.7% of all possible enzyme-small molecule combinations. These authors used two independent test sets to validate the model, one dataset with 266 data points with enzymes from <italic>Avena strigosa</italic> and another dataset with 380 data points with enzymes from <italic>Lycium barbarum</italic>. On those two test sets, GT-Predict achieves accuracies of 79.0% and 78.8%, respectively, and MCCs of 0.338 and 0.319, respectively. We added the training set from Ref. <sup><xref ref-type="bibr" rid="CR15">15</xref></sup> to our training set. The test sets from <italic>Avena strigosa</italic> and <italic>Lycium barbarum</italic> had no overlap with our training data. For these two sets, we achieved similar accuracies as Yang et al. (78.2% in both cases), but substantially improved MCCs: 0.484 for <italic>Avena strigosa</italic> and 0.517 for <italic>Lycium barbarum</italic> (ROC-AUC scores were 0.80 and 0.84, respectively). As the test datasets used by Yang et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> are imbalanced, with a proportion of 18–31% of positive data points, the MCC is a more meaningful score compared to the accuracy<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>; we hence conclude that ESP outperforms GT-Predict. Beyond benchmarking the performance of ESP, the above comparisons of our model predictions to two (almost) complete experimental datasets also indicate that ESP is indeed capable of predicting the full substrate scope of enzymes.</p>
      <p id="Par38">We also tested model performances for the test sets by Mou et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> and Yang et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> without adding any new training data to ESP. Only ~5% and ~8% of the small molecules in these test sets did already occur in our training set. As we showed above that performance drops massively if the model is applied to unseen small molecules (Fig. <xref rid="Fig4" ref-type="fig">4</xref>a), we did not expect good model performances. Indeed, for all three test sets, accuracies are below 68%, ROC-AUC scores are below 0.59, and MCCs are below 0.12 (Supplementary Table <xref rid="MOESM1" ref-type="media">5)</xref>.</p>
    </sec>
    <sec id="Sec12">
      <title>Web server facilitates easy use of ESP</title>
      <p id="Par39">We implemented a web server that allows an easy use of ESP without requiring programming skills or the installation of specialized software. It is available at <ext-link ext-link-type="uri" xlink:href="https://esp.cs.hhu.de">https://esp.cs.hhu.de</ext-link>. As input, the web server requires an enzyme amino acid sequence and a representation of a small molecule (either as a SMILES string, KEGG Compound ID, or InChI string). Users can either enter a single enzyme-small molecule pair into an online form, or upload a CSV file with multiple such pairs. In addition to the prediction score, the ESP web server reports how often the entered metabolite was present as a true substrate in our training set. Since we have shown that model performance drops substantially when the model is applied to small molecules not used during training, we recommend to use the prediction tool only for those small molecules represented in our training dataset. We uploaded a full list with all small molecules from the training set to the web server homepage, listing how often each one is present among the positive data points.</p>
    </sec>
  </sec>
  <sec id="Sec13" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par40">Here, we present a general approach for predicting the substrate scope of enzymes; ESP achieves an accuracy of over 91% on an independent test set with enzymes that share at most 80% sequence identity with any enzyme used for training. Notably, the model performs with an accuracy of 89% even for enzymes with very low sequence identity (&lt;40%) to proteins in the training set. This performance seems remarkable, as it is believed that enzymes often evolve different substrate specificities or even different functions if sequence identity falls below 40%<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>.</p>
    <p id="Par41">To achieve these results, we use very general input features: a task-specific fingerprint of the small molecule, constructed with a graph neural network (GNN) from a graph representing structural information, and a numerical representation of the enzyme calculated from its amino acid sequence. We show that creating task-specific enzyme representations leads to significant improvements compared to non-task-specific enzyme representations (Fig. <xref rid="Fig3" ref-type="fig">3)</xref>. One of the major challenges in predicting enzyme-substrate relationships is a lack of experimentally confirmed non-binding enzyme-substrate pairs. To overcome this challenge, we developed a carefully devised strategy of randomly sampling negative enzyme-molecule pairs. Although this data augmentation can potentially lead to false-negative data points, such false negatives are expected to be rare, an expectation that is confirmed by the good results on independent test data sets. Future refinements of this approach might boost model performance further. For example, when creating negative data points for confirmed enzyme-substrate pairs, a tighter decision boundary might result from preferentially choosing structurally similar substrates of highly different enzymes. On the other hand, the sets of true substrates of highly similar enzymes often overlap, and excluding known substrates of highly similar enzymes could avoid creating some false negative data points.</p>
    <p id="Par42">An additional avenue towards potential model improvements could be to test new model architectures. In this study, we trained two separate models for creating task-specific enzyme and small molecule representations. Future work could investigate if the pre-training of the enzyme representation and the small molecule representation could be performed jointly in a single model, thereby creating matched, task-specific enzyme and small molecule representations simultaneously.</p>
    <p id="Par43">Despite the structural similarities of ESP to state-of-the-art models for predicting drug-target binding affinities (DTBAs) and for predicting Michaelis-Menten constants of enzyme-substrate pairs<sup><xref ref-type="bibr" rid="CR25">25</xref>–<xref ref-type="bibr" rid="CR29">29</xref></sup>, the performances of these models are not comparable, as we trained ESP for a binary classification task, whereas the other models address regression tasks. Instead, we compare our approach to two recently published models for predicting enzyme-substrate pairs<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup>. These two models used very specific input features, such as an enzyme’s active site properties and physicochemical properties of the metabolite, and were designed and trained for only a single enzyme family. Our general ESP model – which can be trained on much larger datasets – achieves superior results, despite learning and extracting all relevant information for this task from much less detailed, general input representations. The application of ESP to the dataset from Mou et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> also demonstrate that our model can successfully distinguish between similar potential substrates for the same enzyme, as it achieved good results when it was applied to different nitriles for bacterial nitrilases.</p>
    <p id="Par44">One limitation of ESP is that model performance drops substantially for small molecules that did not occur in the training set. However, the current version of ESP can still be applied successfully to a broad range of almost 1400 different small molecules present in our dataset. Once more training data becomes available, model performance will very likely improve further (Fig. <xref rid="Fig5" ref-type="fig">5)</xref>. Mining other biochemical databases–such as BRENDA<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>, Sabio-RK<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>, and UniProt<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> – for new and non-overlapping data might be a low-cost way to expand the number of different small molecules in the dataset. Adding as few as two additional positive training data points for new molecules will typically lead to accurate predictions (Supplementary Figure <xref rid="MOESM1" ref-type="media">4)</xref>.</p>
    <p id="Par45">The recent development of AlphaFold<sup><xref ref-type="bibr" rid="CR55">55</xref></sup> and RoseTTAFold<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> facilitates predictions of the 3D structure for any protein with known amino acid sequence. Future work may also include input features extracted from such predicted enzyme structures. Our high-quality dataset with many positive and negative enzyme-small metabolite pairs, which is available on GitHub, might be a promising starting point to explore the utility of such features.</p>
    <p id="Par46">A main use case for the ESP model will be the prediction of possible substrate candidates for single enzymes. In contrast, ESP will likely not lead to satisfactory results when used to predict all enzyme-substrate pairs in a genome-scale metabolic model. This problem results from the trade-off between the True Positive Rate (TPR) and the False Postive Rate (FPR) for different classification thresholds (Fig. <xref rid="Fig3" ref-type="fig">3</xref>b). For example, choosing a classification threshold with a TPR of ~80% leads to a FPR of ~5%. If we consider a genome-scale model with approximately 2000 enzymes and 2000 metabolites, then there exist ~4 × 10<sup>6</sup> possible enzyme-small molecule pairs, of which only about 6000 will be true enzyme-substrate pairs. A TPR of 80% would lead to the successful detection of 4800 true pairs. At the same time, an FPR of 5% would lead to an additional ~200,000 false predictions.</p>
    <p id="Par47">If, on the other hand, ESP is applied to a set of pre-selected candidate substrates for a single enzyme, a false positive rate of 5% can be acceptable. If we choose 200 molecules as substrate candidates, where one of these 200 is a true substrate for the enzyme, an FPR of 5 % means that the model predicts only ~ 10 molecules falsely as a substrate, and there is an 80% chance that the true substrate is labeled correctly. This could help to bring down the experimental burden – and associated costs – of biochemical assays to levels where laboratory tests become tractable.</p>
  </sec>
  <sec id="Sec14">
    <title>Methods</title>
    <sec id="Sec15">
      <title>Software</title>
      <p id="Par48">All software was coded in Python<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. We implemented and trained the neural networks using the deep learning library PyTorch<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. We fitted the gradient boosting models using the library XGBoost<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>.</p>
    </sec>
    <sec id="Sec16">
      <title>Creating a database with enzyme-substrate pairs</title>
      <p id="Par49">To create a database with positive enzyme-substrate pairs, we searched the Gene Ontology (GO) annotation database for UniProt IDs<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> for experimentally confirmed annotations of the catalytic activity of enzymes. A GO annotation consists of a GO Term that is assigned to a UniProt ID, which is an identifier for proteins. GO Terms can contain information about the biological processes, molecular functions, and cellular components in which proteins are involved<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. We first created a list with all 6587 catalytic GO Terms containing information about enzyme-catalyzed reactions. For each of these GO Terms, we extracted identifiers for the substrates involved in the reaction. If the GO Term definition stated that the reaction is reversible, we treated all reactants (including products) as substrates; if a reaction was labeled as irreversible, we only extracted the reactants annotated as substrates. For this purpose, we used a RHEA reaction ID<sup><xref ref-type="bibr" rid="CR60">60</xref></sup> from the GO Term, which was available for 4086 out of 6587 GO Terms. If no RHEA reaction ID was listed for the GO Term, we extracted the substrate names via text mining from the GO Term definition. Substrate names were then mapped to KEGG and ChEBI identifiers via the synonym database from KEGG<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>, or, if no entry in KEGG was found, the PubChem synonym database<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>. We discarded all 824 catalytic GO Terms for which we could not map at least one substrate to an identifier.</p>
      <p id="Par50">Entries in the GO annotation database have different levels of evidence: experimental, phylogenetically-inferred, computational analysis, author statement, curator statement, and electronic evidence. For training our final model, we were interested only in entries with catalytic GO Terms based on experimental evidence. From these, we removed 6219 enzyme-substrate pairs with water, oxygen, and ions, as these small substrates did not lead to unique representations (see below). We extracted protein and substrate IDs for the remaining 18,351 enzyme-substrate pairs with experimental evidence. 15051 of these pairs resulted from a GO Term that was associated with a RHEA reaction ID, the rest were created via text mining of GO Term definitions. These data points are combinations of 12,156 unique enzymes and 1379 unique substrates.</p>
      <p id="Par51">Before training our models for predicting enzyme-substrate pairs, we pre-trained the ESM-1b protein representations to capture information relevant to enzyme-substrate binding. Due to the high dimensionality of the protein representations, much more data than the 18,351 enzyme-substrate pairs with experimental evidence was required for this task. Only for this pre-training, we thus additionally extracted protein and substrate IDs for 274,030 entries with catalytic GO Terms and phylogenetically inferred evidence (this set excludes 98 384 entries with water, oxygen, and ions as substrates). 200,634 of these enzyme-substrate pairs resulted from a GO Term associated with a RHEA reaction ID, the rest were constructed via text mining of GO Term definitions. These additional data points based on phylogenetic evidence are combinations of 198,259 unique enzymes and 661 unique substrates.</p>
      <p id="Par52">It might be surprising that although we found many more enzyme-substrate pairs with phylogenetically inferred evidence compared to data points with experimental evidence, the number of unique substrates is much smaller. To investigate if we can see a systematic difference between both groups, we plotted the distribution of the first digit of EC classes among the enzymes of both classes. However, no substantial difference was evident except for an over-representation of EC6 (ligases) in the data with phylogenetic evidence (Supplementary Fig. <xref rid="MOESM1" ref-type="media">5)</xref>. Hence, we assume that the data structure of phylogenetically inferred data points is not an important issue for the calculation of enzyme representations.</p>
      <p id="Par53">We downloaded all enzyme amino acid sequences via the UniProt mapping service<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>.</p>
    </sec>
    <sec id="Sec17">
      <title>Sampling negative data points</title>
      <p id="Par54">For every positive enzyme-substrate pair in our dataset, we created three negative data points for the same enzyme by randomly sampling small molecules. The distinction between true and false substrates is harder for small molecules that are similar to the true, known substrates. To challenge our model to learn this distinction, we restricted our sampling of negative data points to small molecules similar to the true substrate. For this purpose, we first calculated the pairwise similarity of all small molecules in our dataset with the function FingerprintSimilarity from the RDKit package DataStructs<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>. This function uses molecular fingerprints of the molecules as its input and computes values between zero (no similarity) and one (high similarity). If possible, we sampled small molecules with a similarity score between 0.7 and 0.95. If we did not find such molecules, we reduced the lower bound in steps of 0.2 until enough small molecules could be sampled. We had to reduce the lower bound in ~ 19% of enzyme-substrate pairs. We did not simply choose the three most similar compounds as negative data points, because if a substrate appears multiple times in our dataset, this would have led to selecting always the same three small molecules as non-substrates. Instead, we randomly picked three molecules from within the selected similarity range. During this sampling process, we took the distribution of the small molecules among the positive data points into account, i.e., molecules that occur more frequently as substrates among the positive data points also appear more frequently among the negative data points. To achieve this, we excluded small molecules from the sampling process if these molecules were already sampled enough times (i.e., three times their total occurrence in the set of positive enzyme-substrate pairs).</p>
    </sec>
    <sec id="Sec18">
      <title>Splitting the dataset into training and test sets</title>
      <p id="Par55">Before we split the dataset into training and test sets, we clustered all sequences by amino acid sequence identity using the CD-HIT algorithm<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>. The clusters were created in such a way that two sequences from different clusters do not have a pairwise sequence identity higher than 80%. We used these clusters to split the dataset randomly into 80% training data and 20% test data using a sequence identity cutoff of 80%, i.e., every enzyme in the test set has a maximal sequence identity of 80% compared to any enzyme in the training set. This was achieved by placing all sequences from one cluster either into the training or the test set. To analyze the ESP performance for different sequence identity levels, we further split the test set into subsets with maximal sequence identity to enzymes in the training set of 0–40%, 40–60%, and 60–80% using the CD-HIT algorithm<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>.</p>
    </sec>
    <sec id="Sec19">
      <title>Calculating extended-connectivity fingerprints</title>
      <p id="Par56">All small molecules in our final datasets were either assigned to a KEGG ID or ChEBI (Chemical Entities of Biological Interest) ID. For all small molecules with a KEGG ID, we downloaded an MDL Molfile with 2D projections of its atoms and bonds from KEGG<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>. If no MDL Molfile could be obtained in this way, we instead downloaded the International Chemical Idenitifier (InChI) string via the mapping service of MetaCyc<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>, if a ChEBI ID was available. We then used the package Chem from RDKit<sup><xref ref-type="bibr" rid="CR63">63</xref></sup> with the MDL Molfiles or InChI strings as the input to calculate the 1024-dimensional binary ECFPs<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> with a radius (number of iterations) of 3. We also calculated 512 and 2048-dimensional ECFPs to investigate if these lead to better model performance than 1024-dimensional ECFPs.</p>
    </sec>
    <sec id="Sec20">
      <title>Calculating task-specific fingerprints for small molecules</title>
      <p id="Par57">In addition to the pre-defined ECFPs, we also used a graph neural network (GNN) to calculate task-specific numerical representations for the small molecules. GNNs are neural networks that can take graphs as their input<sup><xref ref-type="bibr" rid="CR38">38</xref>–<xref ref-type="bibr" rid="CR40">40</xref></sup>. A molecule can be represented as a graph by interpreting the atoms and bonds of the molecule as nodes and edges, respectively.</p>
      <p id="Par58">We trained and implemented a variant of GNNs called Directed Message Passing Neural Network (D-MPNN)<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, using the Python package PyTorch<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. To provide the GNN with information about the small molecules, we calculated feature vectors for every bond and every atom in all molecules<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. For every atom, these features comprise the atomic number, number of bonds, charge, number of hydrogen bonds, mass, aromaticity, hybridization type, and chirality; for every bond, these features comprise bond type, part of ring, stereo configuration, and aromaticity. To input this information into a GNN, the graphs and the feature vectors are encoded with tensors and matrices. While a graph is processed by a GNN, all atom feature vectors are iteratively updated for a pre-defined number of steps by using information of neighboring bond and atom feature vectors. Afterwards, all atom feature vectors are pooled together by applying the element-wise mean to obtain a single graph representation. The dimension <italic>D</italic> of the updated atom feature vectors and of the final graph representation can be freely chosen; we chose <italic>D</italic> = 100.</p>
      <p id="Par59">This small molecule representation was then concatenated with a small representation of an enzyme; we chose to use a small enzyme representation instead of the full ESM-1b vector to keep the input dimension of the machine learning model used for learning the task-specific small molecule representation low. To compute the small enzyme representation, we performed principal component analysis (PCA)<sup><xref ref-type="bibr" rid="CR66">66</xref></sup> on the ESM-1b vectors (see below) and selected the first 50 principal components. The concatenated enzyme-small molecule vector was used as the input for a fully connected neural network (FCNN) with two hidden layers of size 100 and 32, which was trained for predicting whether the small molecule is a substrate for the enzyme. We trained the whole model (the GNN including the FCNN) end-to-end. Thereby, the model was challenged to store task-specific and meaningful information in the graph representations. After training, we extracted a graph representation for every small molecule in our training set, which was then used as input for the complete enzyme-substrate pair prediction model.</p>
      <p id="Par60">We performed a pre-training of the described GNN by training it for the related task of predicting the Michaelis constants <italic>K</italic><sub>M</sub> of enzyme-substrate pairs. As for the task of identifying potential enzyme-substrate pairs, the prediction of <italic>K</italic><sub>M</sub> is dependent on the interaction between enzymes and small molecules, and hence, this pre-training task challenged the GNN to learn interactions between an enzyme and a substrate. To train the model for the <italic>K</italic><sub>M</sub> prediction, we used a dataset that was previously constructed for a <italic>K</italic><sub>M</sub> prediction model<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. After pre-training, we fine-tuned the GNN by training it for the task of predicting enzyme-substrate pairs, i.e., we used all parameters that were learned during the pre-training task as initial parameters for the GNN that was fine-tuned.</p>
    </sec>
    <sec id="Sec21">
      <title>Calculating enzyme representations</title>
      <p id="Par61">We used the ESM-1b model<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> to calculate 1280-dimensional numerical representations of the enzymes. The ESM-1b model is a transformer network<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> that takes amino acid sequences as its input and produces numerical representations of the sequences. First, every amino acid in a sequence is converted into a 1280-dimensional representation, which encodes the type of the amino acid and its position in the sequence. Afterwards, every representation is updated iteratively for 33 update steps by using information about the representation itself as well as about all other representations of the sequence using the attention mechanism<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>. The attention mechanism allows the model to selectively focus only on relevant amino acid representations to make updates<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>. During training, ~15% of the amino acids in a sequence are masked at random, and the model is trained to predict the type of the masked amino acids. The ESM-1b model has been trained with ~27 million proteins from the UniRef50 dataset<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. To create a single representation for the whole enzyme, ESM-1b calculates the element-wise mean of all updated amino acids representations in a sequence<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. We created these representations for all enzymes in our dataset using the code and the trained ESM-1b model provided by the Facebook AI Research team on GitHub.</p>
    </sec>
    <sec id="Sec22">
      <title>Task-specific fine-tuning of the ESM-1b model</title>
      <p id="Par62">To create task-specific enzyme representations for our task of predicting enzyme-substrate pairs, we modified the ESM-1b model. For every input sequence, in addition to the representations of all individual amino acids, we added a token that represents the whole enzyme. This enzyme representation is updated in the same way as the amino acid representations. The parameters of this modified model are initialized with the parameters of the trained ESM-1b model, setting the additional enzyme token initially to the element-wise mean of the amino acid representations. After the last update layer of the model, i.e., after 33 update steps, we take the 1280-dimensional representation of the whole enzyme and concatenate it with a representation for a metabolite, the 1024-dimensional ECFP vector (see above).</p>
      <p id="Par63">This concatenated vector is then used as the input for a fully-connected neural network (FCNN) with two hidden layers of size 256 and 32. The whole model was trained end-to-end for the binary classification task of predicting whether the added metabolite is a substrate for the given enzyme. This training procedure challenged the model to store all necessary enzyme information for the prediction task in the enzyme representation. After training the modified model, we extracted the updated and task-specific representations, the ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors, for all enzymes in our dataset.</p>
      <p id="Par64">We implemented and trained this model using the Python package PyTorch<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. We trained the model with the extended dataset of 287,386 enzyme-substrate pairs with phylogenetically inferred or experimental evidence for 2 epochs on 6 NVIDA DGX A100s, each with 40GB RAM. Training the model for more epochs did not lead to improved results. Because of the immense computational power and long training times, it was not possible to perform a systematic hyperparameter optimization. We chose hyperparameters after trying a few selected hyperparameter settings with values similar to the ones that were used for training the original ESM-1b model.</p>
    </sec>
    <sec id="Sec23">
      <title>Fine-tuning the ESM-1b model without an additional token</title>
      <p id="Par65">To investigate the effect on model performance of adding a token for the whole enzyme to the ESM-1b model, we also re-trained the model without such an extra token. Instead, we calculated the element-wise mean of all amino acid representations after the last update layer of the model, as is done in the original ESM-1b model. We concatenated the resulting 1280-dimensional vector with a representation for a metabolite, the 1024-dimensional ECFP vector. As for the model described above, this concatenated vector is then used as the input for a fully-connected neural network (FCNN) with two hidden layers of size 256 and 32. The whole model was trained end-to-end for the binary classification task of predicting whether the added metabolite is a substrate for the given enzyme. The training procedure of this model was identical to the model with an additional token for the whole enzyme (see above).</p>
    </sec>
    <sec id="Sec24">
      <title>Hyperparameter optimization of the gradient-boosting models</title>
      <p id="Par66">To find the best hyperparameters for the gradient boosting models, we performed 5-fold cross-validations (CVs). To ensure a high diversity between all folds, we created the five folds in such a way that the same enzyme would not occur in two different folds. We used the Python package hyperopt<sup><xref ref-type="bibr" rid="CR68">68</xref></sup> to perform a random grid search for the following hyperparameters: learning rate, maximum tree depth, lambda and alpha coefficients for regularization, maximum delta step, minimum child weight, number of training epochs, and weight for negative data points. The last hyperparameter was added because our dataset is imbalanced; this parameter allows the model to assign a lower weight to the negative data points during training. To ensure that our model is indeed not assigning too many samples to the over-represented negative class, we used a custom loss function that contains the False Negative Rate, FNR, and the False Positive Rate, FPR. Our loss function, 2 × FNR<sup>2</sup> + FPR<sup>1.3</sup>, penalizes data points that are mistakenly assigned to the negative class stronger than data points that are mistakenly assigned to the positive class. After hyperparameter optimization, we chose the set of hyperparameters with the lowest mean loss during CV. We used the python package xgboost<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> for training the gradient boosting models.</p>
    </sec>
    <sec id="Sec25">
      <title>Displaying the results of cross-validations with boxplots</title>
      <p id="Par67">We used boxplots to display the results of the 5-fold cross-validations, which we performed to find the best set of hyperparameters. We used a 2×interquartile range for the whiskers, the boxes extend from the lower to upper quartile values, and the red horizontal lines are displaying the median of the data points.</p>
    </sec>
    <sec id="Sec26">
      <title>Training of additional machine learning models</title>
      <p id="Par68">To compare the performance of the gradient boosting model to additional machine learning models, we also trained a logistic regression model and a random forest model for the same prediction task. To find the best hyperparameters for the models, we again performed 5-fold CVs on the training set. For the random forest model, the hyperparameter optimized was the number of estimators, and for the logistic regression model we searched for the best penalty function and coefficient of regularization strength. We used the python package scikit-learn<sup><xref ref-type="bibr" rid="CR69">69</xref></sup> for training both models.</p>
    </sec>
    <sec id="Sec27">
      <title>Validating our model on two additional test sets</title>
      <p id="Par69">We compared the performance of ESP with two published models for predicting the substrate scope of single enzyme families. One of these models is a machine learning model developed by Mou et al. to predict the substrates of 12 different bacterial nitrilases<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. Their dataset consists of 240 data points, where each of the 12 nitrilases was tested with the same 20 small molecules. This dataset was randomly split by Mou et al. into 80% training data and 20% test data<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. We added all training data to our training set. After re-training, we validated our model performance on the test set from Ref. <sup><xref ref-type="bibr" rid="CR14">14</xref></sup>.</p>
      <p id="Par70">The second model that we compared to ESP is a decision tree-based model, called GT-predict, for predicting the substrate scope of glycosyltransferases of plants<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. As a training set, Yang et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> used 2847 data points with 59 different small molecules and 53 different enzymes from <italic>Arabidopsis thaliana</italic>. They used two independent test sets to validate model performance: one dataset with 266 data points comprising 7 enzymes from <italic>Avena strigose</italic> and 38 different small molecules, and a second dataset with 380 data points comprising 10 enzymes from <italic>Lycium barbarum</italic> and 38 different small molecules. We added all training data to our training set. After re-training, we validated ESP model performance on both test sets from Ref. <sup><xref ref-type="bibr" rid="CR15">15</xref></sup>.</p>
    </sec>
    <sec id="Sec28">
      <title>Analyzing the effect of training set size</title>
      <p id="Par71">To analyze the effect of different training set sizes, we created eight different subsets of our training set, with sizes ranging from 30% to 100% of the original training set size. To create these subsets, we first generated an enzyme list containing all enzymes of the training set in random order. To create the subsets, we extracted all training data points with enzymes that occur in the first 30%, 40%, …, 100% of the generated enzyme list. Afterwards, we re-trained our model on all different subsets of the training set and validated each version on our full test set.</p>
    </sec>
    <sec id="Sec29">
      <title>Statistical tests for model comparison</title>
      <p id="Par72">We tested if the difference in model performance between the two models with ESM-1b<sub><italic>t</italic><italic>s</italic></sub> and ECFP vectors compared to the model with ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors and GNN-generated fingerprints is statistically significant. For this purpose, we used McNemar’s test<sup><xref ref-type="bibr" rid="CR70">70</xref></sup> (implemented in the Python package Statsmodels<sup><xref ref-type="bibr" rid="CR71">71</xref></sup>), testing the null hypothesis that both models have a similar proportion of errors on our test set. We could reject the null hypothesis (<italic>p</italic> &lt; 10<sup>−9</sup>), concluding that combining ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors with GNN-generated fingerprints leads to a statistically significant improvement over a combination with ECFP vectors. We performed the same test to show that a model with fingerprints created with a pre-trained GNN achieves improved results compared to a model with fingerprints created with a not pre-trained GNN (<italic>p</italic> &lt; 10<sup>−7</sup>). Moreover, we used McNemar’s test to show that the model with ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors and GNN-generated fingerprints achieves significantly improved performance compared to the model with ESM-1b and ECFP vectors as the input (<italic>p</italic> &lt; 10<sup>−37</sup>) and also compared to the model with ESM-1b and GNN-generated fingerprints (<italic>p</italic> &lt; 10<sup>−19</sup>). Furthermore, we used the same test to show that the task-specific enzyme representations, the ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors, that were created by fine-tuning the ESM-1b model with an extra token for the whole enzyme achieved improved performance compared to task-specific enzyme representations that resulted from fine-tuning the ESM-1b model without such an extra token (<italic>p</italic> = 0.040).</p>
      <p id="Par73">We also tested if the differences in model performance between the three different splits of our test set with different enzyme sequence identity levels (0–40%, 40–60%, and 60–80%) are statistically significant. Here, we used the non-parametric two-sided Mann–Whitney <italic>U</italic> test implemented in the Python package SciPy<sup><xref ref-type="bibr" rid="CR72">72</xref></sup> to test the null hypothesis that the prediction errors for the different splits are equally distributed.</p>
    </sec>
    <sec id="Sec30">
      <title>Reporting summary</title>
      <p id="Par74">Further information on research design is available in the <xref rid="MOESM3" ref-type="media">Nature Portfolio Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec31">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41467_2023_38347_MOESM1_ESM.pdf">
            <caption>
              <p>Supplementary Information</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="41467_2023_38347_MOESM2_ESM.pdf">
            <caption>
              <p>Peer Review File</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="41467_2023_38347_MOESM3_ESM.pdf">
            <caption>
              <p>Reporting Summary</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec32">
        <title>Source data</title>
        <p id="Par78">
          <media position="anchor" xlink:href="41467_2023_38347_MOESM4_ESM.xlsx" id="MOESM4">
            <caption>
              <p>Source Data</p>
            </caption>
          </media>
        </p>
      </sec>
    </app>
  </app-group>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>The online version contains supplementary material available at 10.1038/s41467-023-38347-2.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank Veronica Maurino for insightful discussions. Computational support and infrastructure was provided by the “Centre for Information and Media Technology” (ZIM) at the University of Düsseldorf (Germany). We acknowledge financial support to M.J.L. by the German Research Foundation (DFG) through CRC 1310, and, under Germany’s Excellence Strategy, through EXC2048/1 (Project ID:390686111), as well as through a grant by the Volkswagen Foundation under the “Life” initiative.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>S.R. implemented and trained the task-specific ESM-1b<sub><italic>t</italic><italic>s</italic></sub> model. A.K. designed the dataset and models and performed all other analyses. M.K.M.E. conceived of the study. M.J.L. supervised the study and acquired funding. A.K., M.K.M.E., and M.J.L. interpreted the results and wrote the manuscript.</p>
  </notes>
  <notes notes-type="peer-review">
    <title>Peer review</title>
    <sec id="FPar1">
      <title>Peer review information</title>
      <p id="Par75"><italic>Nature Communications</italic> thanks the anonymous reviewers for their contribution to the peer review of this work. A <xref rid="MOESM2" ref-type="media">peer reviewer file</xref> is available.</p>
    </sec>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open Access funding enabled and organized by Projekt DEAL.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>All data generated in this study and all processed data used to produce the results of this study have been deposited in the GitHub repository available at <ext-link ext-link-type="uri" xlink:href="https://github.com/AlexanderKroll/ESP">https://github.com/AlexanderKroll/ESP</ext-link><sup><xref ref-type="bibr" rid="CR73">73</xref></sup>. Source data for all figures are provided with this paper. <xref rid="Sec32" ref-type="sec">Source data</xref> are provided with this paper.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>The Python code used to generate all results is publicly available only at <ext-link ext-link-type="uri" xlink:href="https://github.com/AlexanderKroll/ESP">https://github.com/AlexanderKroll/ESP</ext-link><sup><xref ref-type="bibr" rid="CR73">73</xref></sup>.</p>
  </notes>
  <notes id="FPar2" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par76">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Cooper, G. M., Hausman, R. E. &amp; Hausman, R. E.<italic>The Cell: A Molecular Approach</italic>, vol. 4 (ASM press, Washington DC, 2007).</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Copley</surname>
            <given-names>SD</given-names>
          </name>
        </person-group>
        <article-title>Shining a light on enzyme promiscuity</article-title>
        <source>Curr. Opin. Struct. Biol.</source>
        <year>2017</year>
        <volume>47</volume>
        <fpage>167</fpage>
        <lpage>175</lpage>
        <pub-id pub-id-type="doi">10.1016/j.sbi.2017.11.001</pub-id>
        <?supplied-pmid 29169066?>
        <pub-id pub-id-type="pmid">29169066</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Khersonsky</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Tawfik</surname>
            <given-names>DS</given-names>
          </name>
        </person-group>
        <article-title>Enzyme promiscuity: a mechanistic and evolutionary perspective</article-title>
        <source>Annu. Rev. Biochem.</source>
        <year>2010</year>
        <volume>79</volume>
        <fpage>471</fpage>
        <lpage>505</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev-biochem-030409-143718</pub-id>
        <?supplied-pmid 20235827?>
        <pub-id pub-id-type="pmid">20235827</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nobeli</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Favia</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Thornton</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>Protein promiscuity and its implications for biotechnology</article-title>
        <source>Nat. Biotechnol.</source>
        <year>2009</year>
        <volume>27</volume>
        <fpage>157</fpage>
        <lpage>167</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt1519</pub-id>
        <?supplied-pmid 19204698?>
        <pub-id pub-id-type="pmid">19204698</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Adrio</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Demain</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>Microbial enzymes: tools for biotechnological processes</article-title>
        <source>Biomolecules</source>
        <year>2014</year>
        <volume>4</volume>
        <fpage>117</fpage>
        <lpage>139</lpage>
        <pub-id pub-id-type="doi">10.3390/biom4010117</pub-id>
        <?supplied-pmid 24970208?>
        <pub-id pub-id-type="pmid">24970208</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Engineering a synthetic pathway for gentisate in pseudomonas chlororaphis p3</article-title>
        <source>Front. Bioeng. Biotechnol.</source>
        <year>2021</year>
        <volume>8</volume>
        <fpage>1588</fpage>
        <pub-id pub-id-type="doi">10.3389/fbioe.2020.622226</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>M-C</given-names>
          </name>
          <name>
            <surname>Law</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Wilkinson</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Micklefield</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Bioengineering natural product biosynthetic pathways for therapeutic applications</article-title>
        <source>Curr. Opin. Biotechnol.</source>
        <year>2012</year>
        <volume>23</volume>
        <fpage>931</fpage>
        <lpage>940</lpage>
        <pub-id pub-id-type="doi">10.1016/j.copbio.2012.03.008</pub-id>
        <?supplied-pmid 22487048?>
        <pub-id pub-id-type="pmid">22487048</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">UniProt Consortium. Uniprot: the universal protein knowledgebase in 2021. <italic>Nucl. Acids Res</italic>. <bold>49</bold>, D480–D489 (2021).</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rembeza</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Boverio</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Fraaije</surname>
            <given-names>MW</given-names>
          </name>
          <name>
            <surname>Engqvist</surname>
            <given-names>MK</given-names>
          </name>
        </person-group>
        <article-title>Discovery of two novel oxidases using a high-throughput activity screen</article-title>
        <source>ChemBioChem</source>
        <year>2022</year>
        <volume>23</volume>
        <fpage>e202100510</fpage>
        <pub-id pub-id-type="doi">10.1002/cbic.202100510</pub-id>
        <?supplied-pmid 34709726?>
        <pub-id pub-id-type="pmid">34709726</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Longwell</surname>
            <given-names>CK</given-names>
          </name>
          <name>
            <surname>Labanieh</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Cochran</surname>
            <given-names>JR</given-names>
          </name>
        </person-group>
        <article-title>High-throughput screening technologies for enzyme engineering</article-title>
        <source>Curr. Opin. Biotechnol.</source>
        <year>2017</year>
        <volume>48</volume>
        <fpage>196</fpage>
        <lpage>202</lpage>
        <pub-id pub-id-type="doi">10.1016/j.copbio.2017.05.012</pub-id>
        <?supplied-pmid 28624724?>
        <pub-id pub-id-type="pmid">28624724</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Black</surname>
            <given-names>GW</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A high-throughput screening method for determining the substrate scope of nitrilases</article-title>
        <source>Chem. Commun.</source>
        <year>2015</year>
        <volume>51</volume>
        <fpage>2660</fpage>
        <lpage>2662</lpage>
        <pub-id pub-id-type="doi">10.1039/C4CC06021K</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Detlefsen</surname>
            <given-names>NS</given-names>
          </name>
          <name>
            <surname>Hauberg</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Boomsma</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Learning meaningful representations of protein sequences</article-title>
        <source>Nat. Commun.</source>
        <year>2022</year>
        <volume>13</volume>
        <fpage>1914</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-022-29443-w</pub-id>
        <?supplied-pmid 35395843?>
        <pub-id pub-id-type="pmid">35395843</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pertusi</surname>
            <given-names>DA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting novel substrates for enzymes with minimal experimental effort with active learning</article-title>
        <source>Metab. Eng.</source>
        <year>2017</year>
        <volume>44</volume>
        <fpage>171</fpage>
        <lpage>181</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ymben.2017.09.016</pub-id>
        <?supplied-pmid 29030274?>
        <pub-id pub-id-type="pmid">29030274</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mou</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Machine learning-based prediction of enzyme substrate scope: Application to bacterial nitrilases</article-title>
        <source>Proteins Struct. Funct. Bioinf.</source>
        <year>2021</year>
        <volume>89</volume>
        <fpage>336</fpage>
        <lpage>347</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.26019</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Functional and informatics analysis enables glycosyltransferase activity prediction</article-title>
        <source>Nat. Chem. Biol.</source>
        <year>2018</year>
        <volume>14</volume>
        <fpage>1109</fpage>
        <lpage>1117</lpage>
        <pub-id pub-id-type="doi">10.1038/s41589-018-0154-9</pub-id>
        <?supplied-pmid 30420693?>
        <pub-id pub-id-type="pmid">30420693</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Röttig</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rausch</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kohlbacher</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Combining structure and sequence information allows automated prediction of substrate specificities within enzyme families</article-title>
        <source>PLoS Comput. Biol.</source>
        <year>2010</year>
        <volume>6</volume>
        <fpage>e1000636</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000636</pub-id>
        <?supplied-pmid 20072606?>
        <pub-id pub-id-type="pmid">20072606</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chevrette</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Aicheler</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Kohlbacher</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Currie</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>Medema</surname>
            <given-names>MH</given-names>
          </name>
        </person-group>
        <article-title>Sandpuma: ensemble predictions of nonribosomal peptide chemistry reveal biosynthetic diversity across actinobacteria</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <fpage>3202</fpage>
        <lpage>3210</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx400</pub-id>
        <?supplied-pmid 28633438?>
        <pub-id pub-id-type="pmid">28633438</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goldman</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>KK</given-names>
          </name>
          <name>
            <surname>Coley</surname>
            <given-names>CW</given-names>
          </name>
        </person-group>
        <article-title>Machine learning modeling of family wide enzyme-substrate specificity screens</article-title>
        <source>PLoS Comput. Biol.</source>
        <year>2022</year>
        <volume>18</volume>
        <fpage>e1009853</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1009853</pub-id>
        <?supplied-pmid 35143485?>
        <pub-id pub-id-type="pmid">35143485</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Visani</surname>
            <given-names>GM</given-names>
          </name>
          <name>
            <surname>Hughes</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Hassoun</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Enzyme promiscuity prediction using hierarchy-informed multi-label classification</article-title>
        <source>Bioinformatics</source>
        <year>2021</year>
        <volume>37</volume>
        <fpage>2017</fpage>
        <lpage>2024</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btab054</pub-id>
        <?supplied-pmid 33515234?>
        <pub-id pub-id-type="pmid">33515234</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ryu</surname>
            <given-names>JY</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>HU</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>SY</given-names>
          </name>
        </person-group>
        <article-title>Deep learning enables high-quality and high-throughput prediction of enzyme commission numbers</article-title>
        <source>PNAS</source>
        <year>2019</year>
        <volume>116</volume>
        <fpage>13996</fpage>
        <lpage>14001</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1821905116</pub-id>
        <?supplied-pmid 31221760?>
        <pub-id pub-id-type="pmid">31221760</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DEEPre: sequence-based enzyme EC number prediction by deep learning</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>34</volume>
        <fpage>760</fpage>
        <lpage>769</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx680</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sanderson</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Bileschi</surname>
            <given-names>ML</given-names>
          </name>
          <name>
            <surname>Belanger</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Colwell</surname>
            <given-names>LJ</given-names>
          </name>
        </person-group>
        <article-title>Proteinfer, deep neural networks for protein functional inference</article-title>
        <source>eLife</source>
        <year>2023</year>
        <volume>12</volume>
        <fpage>e80942</fpage>
        <pub-id pub-id-type="doi">10.7554/eLife.80942</pub-id>
        <?supplied-pmid 36847334?>
        <pub-id pub-id-type="pmid">36847334</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Bileschi, M. L. et al. Using deep learning to annotate the protein universe. Nat. Biotechnol.10.1038/s41587-021-01179-w (2022).</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rembeza</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Engqvist</surname>
            <given-names>MK</given-names>
          </name>
        </person-group>
        <article-title>Experimental and computational investigation of enzyme functional annotations uncovers misannotation in the ec 1.1. 3.15 enzyme class</article-title>
        <source>PLoS Comput. Biol.</source>
        <year>2021</year>
        <volume>17</volume>
        <fpage>e1009446</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1009446</pub-id>
        <?supplied-pmid 34555022?>
        <pub-id pub-id-type="pmid">34555022</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Öztürk</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Özgür</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ozkirimli</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Deepdta: deep drug–target binding affinity prediction</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <fpage>i821</fpage>
        <lpage>i829</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty593</pub-id>
        <?supplied-pmid 30423097?>
        <pub-id pub-id-type="pmid">30423097</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Feng, Q., Dueva, E., Cherkasov, A. &amp; Ester, M. Padme: A deep learning-based framework for drug-target interaction prediction. <italic>Preprint at</italic>10.48550/arXiv.1807.09741 (2018).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Karimi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Deepaffinity: interpretable deep learning of compound–protein affinity through unified recurrent and convolutional neural networks</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <fpage>3329</fpage>
        <lpage>3338</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz111</pub-id>
        <?supplied-pmid 30768156?>
        <pub-id pub-id-type="pmid">30768156</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kroll</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Engqvist</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Heckmann</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lercher</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>Deep learning allows genome-scale prediction of michaelis constants from structural features</article-title>
        <source>PLoS Biol.</source>
        <year>2021</year>
        <volume>19</volume>
        <fpage>e3001402</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pbio.3001402</pub-id>
        <?supplied-pmid 34665809?>
        <pub-id pub-id-type="pmid">34665809</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning-based k cat prediction enables improved enzyme-constrained model reconstruction</article-title>
        <source>Nat. Catal.</source>
        <year>2022</year>
        <volume>5</volume>
        <fpage>662</fpage>
        <lpage>672</lpage>
        <pub-id pub-id-type="doi">10.1038/s41929-022-00798-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Weininger</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>SMILES, a chemical language and information system. 1. introduction to methodology and encoding rules</article-title>
        <source>J. Chem. Inf. Comput. Sci.</source>
        <year>1988</year>
        <volume>28</volume>
        <fpage>31</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="doi">10.1021/ci00057a005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rogers</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hahn</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Extended-connectivity fingerprints</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2010</year>
        <volume>50</volume>
        <fpage>742</fpage>
        <lpage>754</lpage>
        <pub-id pub-id-type="doi">10.1021/ci100050t</pub-id>
        <?supplied-pmid 20426451?>
        <pub-id pub-id-type="pmid">20426451</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Graph neural networks: A review of methods and applications</article-title>
        <source>AI Open</source>
        <year>2020</year>
        <volume>1</volume>
        <fpage>57</fpage>
        <lpage>81</lpage>
        <pub-id pub-id-type="doi">10.1016/j.aiopen.2021.01.001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Analyzing learned molecular representations for property prediction</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2019</year>
        <volume>59</volume>
        <fpage>3370</fpage>
        <lpage>3388</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jcim.9b00237</pub-id>
        <?supplied-pmid 31361484?>
        <pub-id pub-id-type="pmid">31361484</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rives</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>
        <source>PNAS</source>
        <year>2021</year>
        <volume>118</volume>
        <fpage>e2016239118</fpage>
        <pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id>
        <?supplied-pmid 33876751?>
        <pub-id pub-id-type="pmid">33876751</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alley</surname>
            <given-names>EC</given-names>
          </name>
          <name>
            <surname>Khimulya</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Biswas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>AlQuraishi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Church</surname>
            <given-names>GM</given-names>
          </name>
        </person-group>
        <article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title>
        <source>Nat. Methods.</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>1315</fpage>
        <lpage>1322</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0598-1</pub-id>
        <?supplied-pmid 31636460?>
        <pub-id pub-id-type="pmid">31636460</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep dive into machine learning models for protein engineering</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2020</year>
        <volume>60</volume>
        <fpage>2773</fpage>
        <lpage>2790</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jcim.0c00073</pub-id>
        <?supplied-pmid 32250622?>
        <pub-id pub-id-type="pmid">32250622</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. <italic>Preprint at</italic>10.48550/arXiv.1810.04805 (2018).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kearnes</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>McCloskey</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Berndl</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pande</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Riley</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Molecular graph convolutions: moving beyond fingerprints</article-title>
        <source>J. Comput. -Aided Mol. Des.</source>
        <year>2016</year>
        <volume>30</volume>
        <fpage>595</fpage>
        <lpage>608</lpage>
        <pub-id pub-id-type="doi">10.1007/s10822-016-9938-8</pub-id>
        <?supplied-pmid 27558503?>
        <pub-id pub-id-type="pmid">27558503</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Duvenaud, D. K. et al. Convolutional networks on graphs for learning molecular fingerprints. In <italic>Advances in Neural Information Processing Systems</italic>, 2224–2232 (2015).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Graph neural networks: A review of methods and applications</article-title>
        <source>AI Open</source>
        <year>2020</year>
        <volume>1</volume>
        <fpage>57</fpage>
        <lpage>81</lpage>
        <pub-id pub-id-type="doi">10.1016/j.aiopen.2021.01.001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dimmer</surname>
            <given-names>EC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The uniprot-go annotation database in 2011</article-title>
        <source>Nucl. Acids Res.</source>
        <year>2012</year>
        <volume>40</volume>
        <fpage>D565</fpage>
        <lpage>D570</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkr1048</pub-id>
        <?supplied-pmid 22123736?>
        <pub-id pub-id-type="pmid">22123736</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bekker</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Learning from positive and unlabeled data: A survey</article-title>
        <source>Mach. Learn.</source>
        <year>2020</year>
        <volume>109</volume>
        <fpage>719</fpage>
        <lpage>760</lpage>
        <pub-id pub-id-type="doi">10.1007/s10994-020-05877-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tian</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Skolnick</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>How well is enzyme function conserved as a function of pairwise sequence identity?</article-title>
        <source>J. Mol. Biol.</source>
        <year>2003</year>
        <volume>333</volume>
        <fpage>863</fpage>
        <lpage>882</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmb.2003.08.057</pub-id>
        <?supplied-pmid 14568541?>
        <pub-id pub-id-type="pmid">14568541</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>AlQuraishi</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Proteinnet: a standardized data set for machine learning of protein structure</article-title>
        <source>BMC Bioinforma.</source>
        <year>2019</year>
        <volume>20</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-2932-0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Hu, W. et al. Strategies for pre-training graph neural networks. <italic>Preprint at</italic>10.48550/arXiv.1905.12265 (2019).</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Capela, F., Nouchi, V., Van Deursen, R., Tetko, I. V. &amp; Godin, G. Multitask learning on graph neural networks applied to molecular property predictions. <italic>Preprint at</italic>10.48550/arXiv.1910.13124 (2019).</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Vaswani, A. et al. Attention is all you need. In <italic>Advances in neural information processing systems</italic>, 5998–6008 (2017).</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Suzek</surname>
            <given-names>BE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>31</volume>
        <fpage>926</fpage>
        <lpage>932</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu739</pub-id>
        <?supplied-pmid 25398609?>
        <pub-id pub-id-type="pmid">25398609</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">Elnaggar, A. et al. Prottrans: Towards cracking the language of lifes code through self-supervised deep learning and high performance computing. <italic>IEEE Trans. Pattern Anal. Mach. Intell</italic>. <bold>PP</bold>10.1109/TPAMI.2021.3095381 (2021).</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. In <italic>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic>, 785–794 (2016).</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Hsu, C., Nisonoff, H., Fannjiang, C. &amp; Listgarten, J. Learning protein fitness models from evolutionary and assay-labeled data. <italic>Nat. Biotechnol</italic>. <bold>40</bold>, 1–9 (2022).</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chicco</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Jurman</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>The advantages of the matthews correlation coefficient (mcc) over f1 score and accuracy in binary classification evaluation</article-title>
        <source>BMC Genomics.</source>
        <year>2020</year>
        <volume>21</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1186/s12864-019-6413-7</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chang</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>BRENDA, the ELIXIR core data resource in 2021: new developments and updates</article-title>
        <source>Nucl. Acids Res.</source>
        <year>2021</year>
        <volume>49</volume>
        <fpage>D498</fpage>
        <lpage>D508</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkaa1025</pub-id>
        <?supplied-pmid 33211880?>
        <pub-id pub-id-type="pmid">33211880</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wittig</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Rey</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Weidemann</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kania</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Sabio-rk: an updated resource for manually curated biochemical reaction kinetics</article-title>
        <source>Nucl. Acids Res.</source>
        <year>2018</year>
        <volume>46</volume>
        <fpage>D656</fpage>
        <lpage>D660</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkx1065</pub-id>
        <?supplied-pmid 29092055?>
        <pub-id pub-id-type="pmid">29092055</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jumper</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Highly accurate protein structure prediction with alphafold</article-title>
        <source>Nature</source>
        <year>2021</year>
        <volume>596</volume>
        <fpage>583</fpage>
        <lpage>589</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id>
        <?supplied-pmid 34265844?>
        <pub-id pub-id-type="pmid">34265844</pub-id>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Baek</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Accurate prediction of protein structures and interactions using a three-track neural network</article-title>
        <source>Science</source>
        <year>2021</year>
        <volume>373</volume>
        <fpage>871</fpage>
        <lpage>876</lpage>
        <pub-id pub-id-type="doi">10.1126/science.abj8754</pub-id>
        <?supplied-pmid 34282049?>
        <pub-id pub-id-type="pmid">34282049</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Van Rossum</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Drake</surname>
            <given-names>FL</given-names>
          </name>
        </person-group>
        <source>Python 3 Reference Manual</source>
        <year>2009</year>
        <publisher-loc>Scotts Valley</publisher-loc>
        <publisher-name>CreateSpace</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paszke</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>
        <source>Adv. Neur.</source>
        <year>2019</year>
        <volume>32</volume>
        <fpage>8026</fpage>
        <lpage>8037</lpage>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <mixed-citation publication-type="other">The gene ontology resource: enriching a gold mine. <italic>Nucl. Acids Res</italic>. <bold>49</bold>, D325–D334 (2021).</mixed-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <mixed-citation publication-type="other">Bansal, P. et al. Rhea, the reaction knowledgebase in 2022. <italic>Nucl. Acids Res</italic>. <bold>50</bold>, D693–D700 (2021).</mixed-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kanehisa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Goto</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Kegg: kyoto encyclopedia of genes and genomes</article-title>
        <source>Nucl. Acids Res.</source>
        <year>2000</year>
        <volume>28</volume>
        <fpage>27</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/28.1.27</pub-id>
        <?supplied-pmid 10592173?>
        <pub-id pub-id-type="pmid">10592173</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Pubchem in 2021: new data content and improved web interfaces</article-title>
        <source>Nucl. Acids Res.</source>
        <year>2021</year>
        <volume>49</volume>
        <fpage>D1388</fpage>
        <lpage>D1395</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkaa971</pub-id>
        <?supplied-pmid 33151290?>
        <pub-id pub-id-type="pmid">33151290</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <mixed-citation publication-type="other">Landrum, G. et al. RDKit: Open-source cheminformatics. <ext-link ext-link-type="uri" xlink:href="http://www.rdkit.org">http://www.rdkit.org</ext-link> (2006).</mixed-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Cd-hit: accelerated for clustering the next-generation sequencing data</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <fpage>3150</fpage>
        <lpage>3152</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts565</pub-id>
        <?supplied-pmid 23060610?>
        <pub-id pub-id-type="pmid">23060610</pub-id>
      </element-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Caspi</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The metacyc database of metabolic pathways and enzymes-a 2019 update</article-title>
        <source>Nucl. Acids Res.</source>
        <year>2020</year>
        <volume>48</volume>
        <fpage>D445</fpage>
        <lpage>D453</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkz862</pub-id>
        <?supplied-pmid 31586394?>
        <pub-id pub-id-type="pmid">31586394</pub-id>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <mixed-citation publication-type="other">Jolliffe, I. Principal component analysis. <italic>Encyclopedia of Statistics in Behavioral Science</italic> (2005).</mixed-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <mixed-citation publication-type="other">Bahdanau, D., Cho, K. &amp; Bengio, Y. Neural machine translation by jointly learning to align and translate. <italic>Preprint at</italic>10.48550/arXiv.1409.0473 (2014).</mixed-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <mixed-citation publication-type="other">Bergstra, J., Yamins, D. &amp; Cox, D. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In <italic>International conference on machine learning</italic>, 115-123 (PMLR, 2013).</mixed-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>J. Mach. Learn. Res.</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="CR70">
      <label>70.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dietterich</surname>
            <given-names>TG</given-names>
          </name>
        </person-group>
        <article-title>Approximate statistical tests for comparing supervised classification learning algorithms</article-title>
        <source>Neural Comput.</source>
        <year>1998</year>
        <volume>10</volume>
        <fpage>1895</fpage>
        <lpage>1923</lpage>
        <pub-id pub-id-type="doi">10.1162/089976698300017197</pub-id>
        <?supplied-pmid 9744903?>
        <pub-id pub-id-type="pmid">9744903</pub-id>
      </element-citation>
    </ref>
    <ref id="CR71">
      <label>71.</label>
      <mixed-citation publication-type="other">Seabold, S. &amp; Perktold, J. Statsmodels: Econometric and statistical modeling with python. In <italic>Proceedings of the 9th Python in Science Conference</italic>, vol. 57, 61 (Austin, TX, 2010).</mixed-citation>
    </ref>
    <ref id="CR72">
      <label>72.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Virtanen</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scipy 1.0: fundamental algorithms for scientific computing in python</article-title>
        <source>Nat. Methods.</source>
        <year>2020</year>
        <volume>17</volume>
        <fpage>261</fpage>
        <lpage>272</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>
        <?supplied-pmid 32015543?>
        <pub-id pub-id-type="pmid">32015543</pub-id>
      </element-citation>
    </ref>
    <ref id="CR73">
      <label>73.</label>
      <mixed-citation publication-type="other">Kroll, A., Ranjan, S., Engqvist, M. K. &amp; Lercher, M. J. A general model to predict small molecule substrates of enzymes based on machine and deep learning. <italic>GitHub</italic>10.5281/zenodo.7763142 (2023).</mixed-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nat Commun</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id>
    <journal-title-group>
      <journal-title>Nature Communications</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2041-1723</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10185530</article-id>
    <article-id pub-id-type="pmid">37188731</article-id>
    <article-id pub-id-type="publisher-id">38347</article-id>
    <article-id pub-id-type="doi">10.1038/s41467-023-38347-2</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>A general model to predict small molecule substrates of enzymes based on machine and deep learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Kroll</surname>
          <given-names>Alexander</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ranjan</surname>
          <given-names>Sahasra</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Engqvist</surname>
          <given-names>Martin K. M.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3940-1621</contrib-id>
        <name>
          <surname>Lercher</surname>
          <given-names>Martin J.</given-names>
        </name>
        <address>
          <email>Martin.Lercher@hhu.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.411327.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 2176 9917</institution-id><institution>Institute for Computer Science and Department of Biology, </institution><institution>Heinrich Heine University, </institution></institution-wrap>D-40225 Düsseldorf, Germany </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.417971.d</institution-id><institution-id institution-id-type="ISNI">0000 0001 2198 7527</institution-id><institution>Department of Computer Science and Engineering, </institution><institution>Indian Institute of Technology Bombay, </institution></institution-wrap>Powai, Mumbai, 400076 India </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.5371.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 0775 6028</institution-id><institution>Department of Biology and Bioengineering, </institution><institution>Chalmers University of Technology, </institution></institution-wrap>SE-412 96 Gothenburg, Sweden </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.502456.0</institution-id><institution>Present Address: EnginZyme AB, </institution></institution-wrap>Tomtebodevägen 6, 17165 Stockholm, Sweden </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>5</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>15</day>
      <month>5</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>14</volume>
    <elocation-id>2787</elocation-id>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>6</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>21</day>
        <month>4</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">For most proteins annotated as enzymes, it is unknown which primary and/or secondary reactions they catalyze. Experimental characterizations of potential substrates are time-consuming and costly. Machine learning predictions could provide an efficient alternative, but are hampered by a lack of information regarding enzyme non-substrates, as available training data comprises mainly positive examples. Here, we present ESP, a general machine-learning model for the prediction of enzyme-substrate pairs with an accuracy of over 91% on independent and diverse test data. ESP can be applied successfully across widely different enzymes and a broad range of metabolites included in the training data, outperforming models designed for individual, well-studied enzyme families. ESP represents enzymes through a modified transformer model, and is trained on data augmented with randomly sampled small molecules assigned as non-substrates. By facilitating easy in silico testing of potential substrates, the ESP web server may support both basic and applied science.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">For many enzymes, it is unknown which primary and/or secondary reactions they catalyze. Here, the authors use machine and deep learning to develop a general model for the prediction of enzyme-small molecule substrate pairs and make the resulting model available through a webserver.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Machine learning</kwd>
      <kwd>Enzymes</kwd>
      <kwd>Protein function predictions</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100001659</institution-id>
            <institution>Deutsche Forschungsgemeinschaft (German Research Foundation)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>CRC 1310 + 390686111</award-id>
        <principal-award-recipient>
          <name>
            <surname>Lercher</surname>
            <given-names>Martin J.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100001663</institution-id>
            <institution>Volkswagen Foundation (VolkswagenStiftung)</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Springer Nature Limited 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par3">Enzymes evolved to efficiently catalyze one or more specific chemical reactions, increasing reaction rates up to over a million-fold over the spontaneous rates<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. In addition, most enzymes are promiscuous, i.e., they catalyze further, physiologically irrelevant or even harmful reactions<sup><xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR4">4</xref></sup>. Accordingly, a comprehensive mapping of enzyme-substrate relationships plays a crucial role in pharmaceutical research and bio-engineering, e.g., for the production of drugs, chemicals, food, and biofuels<sup><xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR7">7</xref></sup>.</p>
    <p id="Par4">Unfortunately, it is both expensive and time-consuming to determine experimentally which reactions are catalyzed by a given enzyme. There is thus a huge imbalance between the number of proteins predicted to be enzymes and the experimental knowledge about their substrate scopes. While the UniProt database<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> contains entries for over 36 million different enzymes, more than 99% of these lack high-quality annotations of the catalyzed reactions. Efforts are underway to develop high-throughput methods for the experimental determination of enzyme-substrate relationships, but these are still in their infancy<sup><xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR11">11</xref></sup>. Furthermore, even high-throughput methods cannot deal with the vast search space of all possible small molecule substrates, but require the experimenter to choose a small subset for testing.</p>
    <p id="Par5">Our goal in this study was to develop a single machine learning model capable of predicting enzyme-substrate relationships across all proteins, thereby providing a tool that helps to focus experimental efforts on enzyme-small molecule pairs likely to be biologically relevant. Developing such a model faces two major challenges. First, a numerical representation of each enzyme that is maximally informative for the downstream prediction task must be obtained<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. To be as broadly applicable as possible, these representations should be based solely on the enzymes’ primary sequence and not require additional features, such as binding site characteristics. Second, public enzyme databases only list positive instances, i.e., molecules with which enzymes display measurable activity (substrates)<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. For training a prediction model, an automated strategy for obtaining suitable negative, non-binding enzyme-small molecule instances must thus be devised.</p>
    <p id="Par6">Existing machine learning approaches for predicting enzyme-substrate pairs were either developed specifically for small enzyme families for which unusually comprehensive training datasets are available<sup><xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR18">18</xref></sup>, or they are only capable of connecting substrates with EC classes but not with specific enzymes. For example, Mou et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> developed models to predict the substrates of bacterial nitrilases, using input features based on the 3D-structures and active sites of the enzymes. They trained various machine learning models based on experimental evidence for all possible enzyme-small molecule combinations within the models’ prediction scope (<italic>N</italic> = 240). Yang et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> followed a similar approach, predicting the substrate scope of plant glycosyltransferases among a pre-defined set of small molecules. They trained a decision tree-based model with a dataset covering almost all possible combinations of enzymes and relevant small molecules. Pertusi et al.<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> trained four different support vectors machines (SVMs), each for a specific enzyme. As input features, their models only use information about the (potential) substrates, as well as non-substrates manually extracted from the literature; no explicit information about the enzymes was used. Roettig et al.<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> and Chevrette et al.<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> predicted the substrate scopes of small enzyme families, training machine-learning models with structural information relating to the enzymes’ active sites. Finally, Visani et al.<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> implemented a general machine-learning model for predicting suitable EC classes for a given substrate. To train this model, all EC classes that are not associated with a certain substrate were used as negative data points, which resulted in a low average positive to negative ratio of 0.0032. Visani et al. did not use any enzyme information beyond the EC class as model input, and therefore the model cannot distinguish between different enzymes assigned to the same EC class.</p>
    <p id="Par7">All these previous models can either not be applied to individual enzymes, or they aim to predict substrates for only a single enzyme or enzyme family. Those models that make predictions for specific enzymes rely on very dense experimental training data, i.e., experimental results for all or almost all potential enzyme-substrate pairs. However, for the vast majority of enzyme families, such extensive training data is not available. As yet, there have been no published attempts to formulate and train a general model that can be applied to predict substrates for specific enzymes across widely different enzyme families. Deep learning models have been used to predict enzyme functions by either predicting their assignment to EC classes<sup><xref ref-type="bibr" rid="CR20">20</xref>–<xref ref-type="bibr" rid="CR22">22</xref></sup>, or by predicting functional domains within the protein sequence<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. However, different enzymes sharing the same domain architecture or assigned to the same EC class can have highly diverse substrate scopes<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Directly predicting specific substrates for enzymes goes an important step beyond those previous methods and can help to predict enzyme function more specifically and more precisely.</p>
    <p id="Par8">Prior work related to the prediction of enzyme-substrate pairs are the prediction of drug-target binding affinities (DTBAs) and of Michaelis-Menten constants, <italic>K</italic><sub>M</sub> and <italic>k</italic><sub>cat</sub>. State-of-the-art approaches in this domain are feature-based, i.e., numerical representations of the protein and the substrate molecule are used as input to machine learning models<sup><xref ref-type="bibr" rid="CR25">25</xref>–<xref ref-type="bibr" rid="CR29">29</xref></sup>. As numerical descriptions of the substrate molecule, these approaches use SMILES representations<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, expert-crafted fingerprints<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, or fingerprints created with graph neural networks<sup><xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>. Proteins are usually encoded numerically through deep learning-based representations of the amino acid sequences<sup><xref ref-type="bibr" rid="CR34">34</xref>–<xref ref-type="bibr" rid="CR36">36</xref></sup>. However, these approaches cannot be transferred one-to-one to the problem of predicting enzyme-substrate pairs. The <italic>K</italic><sub>M</sub> and <italic>k</italic><sub>cat</sub> prediction models are exclusively trained with positive enzyme-substrate pairs and therefore cannot classify molecules as substrates or non-substrates<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>. Many of the proteins used to train the DTBA prediction models have no enzymatic functions; even if they do, the molecules used for training are mostly not naturally occurring potential substrates, and thus there has been no natural selection for or against binding. In contrast, the binding between enzymes and substrates evolved under natural selection. It appears likely that this evolutionary relationship influences our ability to predict enzyme-substrate pairs, and DTBA models are thus not expected to perform well at this task.</p>
    <p id="Par9">In this work, we go beyond the current state-of-the-art by creating maximally informative protein representations, using a customized, task-specific version of the ESM-1b transformer model<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. The model contains an extra 1280-dimensional token, which was trained end-to-end to store enzyme-related information salient to the downstream prediction task. This general approach was first introduced for natural language processing<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, but has not yet been applied to protein feature prediction. We created negative training examples using data augmentation, by randomly sampling small molecules similar to the substrates in experimentally confirmed enzyme-substrate pairs. Importantly, we sampled all negative data points from a limited set of metabolites, the set of ~ 1400 substrates that occur among all experimentally confirmed enzyme-substrate pairs of our dataset. Thus, we do not sample from the space of all possible alternative reactants similar to the true substrates, but only consider small molecules likely to occur in at least some biological cells. While many enzymes are rather promiscuous<sup><xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR4">4</xref></sup>, it is likely that most of the potential secondary substrates are not contained in this restricted set for any given enzyme, and hence the chance of sampling false negative data points was likely small. We numerically represented all small molecules with task-specific fingerprints that we created with graph neural networks (GNNs)<sup><xref ref-type="bibr" rid="CR38">38</xref>–<xref ref-type="bibr" rid="CR40">40</xref></sup>. A gradient-boosted decision tree model was trained on the combined protein and small molecule representations for a high-quality dataset with ~18,000 very diverse, experimentally confirmed positive enzyme-substrate pairs (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). The resulting Enzyme Substrate Prediction model – ESP – achieves high prediction accuracy for those ~1400 substrates that have been part of our training set and outperforms previously published enzyme family-specific prediction models.<fig id="Fig1"><label>Fig. 1</label><caption><title>Model overview.</title><p>Experimentally validated enzyme-substrate pairs and sampled negative enzyme-small metabolite pairs are numerically represented with task-specific enzyme and small molecule representations. Concatenated enzyme-small molecule representations are used to train a gradient-boosting model. After training, the fitted model can be used to predict promising candidate substrates for enzymes.</p></caption><graphic xlink:href="41467_2023_38347_Fig1_HTML" id="d32e468"/></fig></p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>Obtaining training and test data</title>
      <p id="Par10">We created a dataset with experimentally confirmed enzyme-substrate pairs using the GO annotation database for UniProt IDs<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> (Methods, “Creating a database with enzyme-substrate pairs”). For training our machine learning models, we extracted 18,351 enzyme-substrate pairs with experimental evidence for binding, comprised of 12,156 unique enzymes and 1379 unique metabolites. We also extracted 274,030 enzyme-substrate pairs with phylogenetically inferred evidence, i.e., these enzymes are evolutionarily closely related to enzymes associated with the same reactions. These “guilt by association” assignments are much less reliable than direct experimental evidence, and we only used them during pre-training to create task-specific enzyme representations – numerical vectors aimed at capturing information relevant to the prediction task from the enzyme amino acid sequences. Our validations demonstrate that using phylogenetically inferred functions for the construction of appropriate enzyme representations has a positive effect on the prediction of experimentally confirmed enzyme-substrate pairs (see below, “Fine-tuning of a state-of-the-art protein embedding model”).</p>
      <p id="Par11">There is no systematic information on negative enzyme-small molecule pairs, i.e., pairs where the molecule is not a substrate of the enzyme. We hypothesized that such negative data points could be created artificially through random sampling, which is a common strategy in classification tasks that lack negative training data<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. To challenge our model to learn to distinguish similar binding and non-binding reactants, we sampled negative training data only from enzyme-small molecule pairs where the small molecule is structurally similar to a known true substrate. However, we only considered small molecules included among the experimentally confirmed enzyme-substrate pairs in our dataset. Among such a limited and biased subset, enzymes are quite specific catalysts, and therefore most of the potential secondary substrates are not included for the majority of enzymes. Thus, we assumed that the frequency of incorrectly created negative labels is sufficiently low to not adversely affect model performance. This assumption was confirmed by the high model accuracy on independent test data, as detailed below.</p>
      <p id="Par12">To select putatively non-binding small molecules that are structurally similar to the known substrates, we used a similarity score based on molecular fingerprints, with values ranging from 0 (no similarity) to 1 (identity; see Methods, “Sampling negative data points”). For every positive enzyme-substrate pair, we sampled three molecules with similarity scores between 0.75 and 0.95 to the actual substrate of the enzyme, and used them to construct negative enzyme-molecule pairs. We opted for creating more negative data points than we have positive data points, as this not only provided us with more data, but it also more closely reflects the true distribution of positive and negative data points compared to a balanced distribution.</p>
      <p id="Par13">Our final dataset comprises 69,365 entries. We split this data into a training set (80%) and a test set (20%). In many machine learning domains, it is standard practice to split the data into training and test set completely at random. However, when dealing with protein sequences, this strategy often leads to test sets with amino acid sequences that are almost identical to those of proteins in the training set. Such close homologs often share the same function<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, and the assessment of model performance could thus be overly optimistic. It is therefore common practice to split such datasets into training, validation, and test sets based on protein sequences similarities<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. Here, we made sure that no enzyme in the test set has a sequence identity higher than 80% compared to any enzyme in the training set. To show that despite this sequence-based partitioning, enzymes from the training and test sets follow the same distribution, we used dimensionality reduction to map all enzymes to a two-dimensional subspace and plotted the corresponding data points (Supplementary Fig. <xref rid="MOESM1" ref-type="media">1)</xref>. To evaluate how well our final model performs for different levels of enzyme similarities, we divided the test set further into three subsets with maximal sequence identities between 0–40%, 40–60%, and 60–80% compared to all enzymes in the training set.</p>
    </sec>
    <sec id="Sec4">
      <title>Representing small molecules as numerical vectors</title>
      <p id="Par14">Extended-connectivity fingerprints (ECFPs) are expert-crafted binary representations for small molecules. The molecules are represented as graphs, with atoms interpreted as nodes and chemical bonds as edges. For the numerical encoding, one classifies bond types and calculates feature vectors with information about every atom (types, masses, valences, atomic numbers, atom charges, and number of attached hydrogen atoms)<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. Afterwards, these identifiers are updated for a fixed number of steps by iteratively applying predefined functions to summarize aspects of neighboring atoms and bonds. After the iteration process, all identifiers are converted into a single binary vector with structural information about the molecule. The number of iterations and the dimension of the fingerprint can be chosen freely. We set them to the default values of 3 and 1024, respectively. For comparison, we also created 512- and 2048-dimensional ECFPs, but these led to slightly inferior predictions (Supplementary Fig. <xref rid="MOESM1" ref-type="media">2)</xref>. Using ECFPs can lead to identical representations for structurally very similar molecules, e.g., for some molecules that differ only by the length of a chain of carbon atoms. In our dataset, 182 out of 1379 different molecules shared an identical fingerprint with a structurally similar molecule.</p>
      <p id="Par15">As an alternative to expert-crafted fingerprints such as ECFPs, neural networks can be used to learn how to map graph representations of small molecules to numerical vectors. Such networks are referred to as graph neural networks (GNNs)<sup><xref ref-type="bibr" rid="CR38">38</xref>–<xref ref-type="bibr" rid="CR40">40</xref></sup>. We trained a GNN for the binary task of predicting if a small molecule is a substrate for a given enzyme. While training for this task, the GNN is challenged to store all information about the small molecule that is relevant for solving the prediction task in a single numerical vector. After training, we extracted these 100-dimensional task-specific vectors for all small molecules in our dataset. It has been observed that pre-training GNNs for a related task can significantly improve model performance<sup><xref ref-type="bibr" rid="CR45">45</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>. Thus, we first pre-trained a GNN for the related task of predicting the Michaelis constants <italic>K</italic><sub>M</sub> of enzyme-substrate pairs (see Methods, “Calculating task-specific fingerprints for small molecules”). As shown below (see “Successful prediction of enzyme-substrate pairs”), pre-training indeed improved prediction performance significantly. In contrast to ECFPs, GNN-generated fingerprints lead to much fewer cases of identical representations for different molecules. In our dataset, identical fingerprints occurred for 42 out of 1379 molecules.</p>
    </sec>
    <sec id="Sec5">
      <title>Fine-tuning of a state-of-the-art protein embedding model</title>
      <p id="Par16">The ESM-1b model is a state-of-the-art transformer network<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, trained with ~27 million proteins from the UniRef50 dataset<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> in a self-supervised fashion<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. This model takes an amino acid sequence as its input and puts out a numerical representation of the sequence; these representations are often referred to as protein embeddings. During training of ESM-1b, ~15% of the amino acids in a protein’s sequence are randomly masked and the model is trained to predict the identity of the masked amino acids (Fig. <xref rid="Fig2" ref-type="fig">2</xref>a). This training procedure forces the model to store both local and global information about the protein sequence in one 1280-dimensional representation vector for each individual amino acid. In order to create a single fixed-length numerical representation of the whole protein, one typically calculates the element-wise mean across all amino acid representations<sup><xref ref-type="bibr" rid="CR34">34</xref>,<xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR49">49</xref></sup>. We refer to these protein representations as ESM-1b vectors.<fig id="Fig2"><label>Fig. 2</label><caption><title>A task-specific enzyme representation developed from the ESM-1b model.</title><p><bold>a</bold> ESM-1b model. Amino acids of a protein sequence are represented with numerical vectors and passed through a transformer network. Some amino acid representations are masked. All representations are iteratively updated 33 times, using information about neighboring and distant amino acids. The ESM-1b model is trained to predict the masked amino acids. ESM-1b vectors are calculated by taking the element-wise mean of all representations in the last layer. <bold>b</bold> Modified ESM-1b model. An additional representation for the whole enzyme is added to the amino acid representations. After updating all representations 33 times, the enzyme representation is concatenated with a small molecule representation. The network is trained to predict whether the small molecule is a substrate for the given enzyme. After training, the ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vector is extracted as the enzyme representation before adding the small molecule representation.</p></caption><graphic xlink:href="41467_2023_38347_Fig2_HTML" id="d32e587"/></fig></p>
      <p id="Par17">However, simply taking the element-wise mean results in information loss and does not consider the task for which the representations shall be used, which can lead to subpar performance<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. To overcome these issues, we created task-specific enzyme representations optimized for the prediction of enzyme-substrate pairs. We slightly modified the architecture of the ESM-1b model, adding one additional 1280-dimensional token to represent the complete enzyme, intended to capture information salient to the downstream prediction task (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b). The extra token is not adding input information to the model, but it allows an easier extraction of enzyme information from the trained model. This whole-enzyme representation was updated in the same way as the regular ESM-1b amino acid representations.</p>
      <p id="Par18">After a predefined number of update steps, the enzyme representation was concatenated with the small molecule ECFP-vector. The combined vector was used as the input for a fully connected neural network (FCNN), which was then trained end-to-end to predict whether the small molecule is a substrate for the enzyme. This approach facilitates the construction of a single, optimized, task-specific representation. The ESM-1b model contains many parameters and thus requires substantial training data. Therefore, in the pre-training that produces the task-specific enzyme representations, we added phylogenetically inferred evidence to our training set; this resulted in a total of ~287,000 data points used for training the task-specific enzyme representation. After training, we used the network to extract the 1280-dimensional task-specific representations for all enzymes in our dataset. In the following, these representations are called ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors.</p>
    </sec>
    <sec id="Sec6">
      <title>Successful prediction of enzyme-substrate pairs</title>
      <p id="Par19">To compare the performances of the different enzyme representations (ESM-1b and ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors) and of the two small molecule representations (ECFPs and GNN-generated fingerprints), we estimated prediction quality on our test set when using machine learning models with each of the four combinations of enzyme and small molecule representations. In each case, we concatenated one of the two 1280-dimensional enzyme representations with one of the two small molecule representations to create a single input vector for every enzyme-small molecule pair. We used these inputs to train gradient-boosted decision tree models<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> for the binary classification task of predicting whether the small molecule is a substrate for the enzyme.</p>
      <p id="Par20">We performed hyperparameter optimizations for all four models, including the parameters learning rate, depth of trees, number of iterations, and regularization coefficients. For this, we performed a random grid search with a 5-fold cross-validation (CV) on the training set. To challenge the model to learn to predict the substrate scope of enzymes not included in the training data, we made sure that each enzyme occurred in only one of the five subsets used for cross-validation (Methods, “Hyperparameter optimization of the gradient boosting models”). To account for the higher number of negative compared to positive training data, we also included a weight parameter that lowered the influence of the negative data points. The results of the cross-validations are displayed as boxplots in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a. The best sets of hyperparameters are listed in Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>. After hyperparameter optimization, the models were trained with the best set of hyperparameters on the whole training set and were validated on our independent test set, which had not been used for model training or hyperparameter selection. It is noteworthy that for some input combinations, the accuracies on the test set are higher than the accuracies achieved during cross-validation (Fig. <xref rid="Fig3" ref-type="fig">3</xref>a). This improved performance on the test set may result from the fact that before validation on the test set, models are trained with approximately 11,000 more samples than before each cross-validation; the number of training samples has a substantial influence on model performance (see below, “Model performance increases with increased training set size”).<fig id="Fig3"><label>Fig. 3</label><caption><title>Optimized models provide accurate predictions of enzyme-substrate pairs.</title><p><bold>a</bold> Accuracies. Boxplots summarize the results of the CV with <italic>n</italic> = 5 folds on the training set with the best sets of hyperparameters. We used a 2 × interquartile range for the whiskers, the boxes extend from the lower to upper quartile values, and the red horizontal lines are displaying the median of the data points. Blue dots display the accuracies on the test set, using the optimized models trained on the whole training set. <bold>b</bold> ROC curves for the test set. The dotted line displays the ROC curve expected for a completely random model. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_38347_Fig3_HTML" id="d32e649"/></fig></p>
      <p id="Par21">Commonly used metrics to measure the performance of binary classification models are accuracy, ROC-AUC score, and Matthews correlation coefficient (MCC). Accuracy is simply the fraction of correctly predicted data points among the test data. The ROC-AUC score is a value between 0 and 1 that summarizes how well a classifier is able to distinguish between the positive and negative classes, where a value of 0.5 would result from a model that randomly assigns class labels, and a value of 1 corresponds to perfect predictions. The MCC is a correlation coefficient for binary data, comparable to the Pearson correlation coefficient for continuous data; it takes values between -1 and +1, where 0 would result from a model that randomly assigns class labels, and +1 indicates perfect agreement.</p>
      <p id="Par22">As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref> and Table <xref rid="Tab1" ref-type="table">1</xref>, models with task-specific enzyme and/or small molecule representations performed better than those with generic representations. The best-performing model combined the fine-tuned ESM-1b<sub><italic>t</italic><italic>s</italic></sub> enzyme representations with the GNN-generated small molecule fingerprints, achieving an accuracy of 91.5%, a ROC-AUC score of 0.956, and an MCC of 0.78. The difference between the two best models (ESM-1b<sub><italic>t</italic><italic>s</italic></sub> + GNN vs. ESM-1b<sub><italic>t</italic><italic>s</italic></sub> + ECFP) is statistically highly significant (McNemar’s test: <italic>p</italic> &lt; 10<sup>−5</sup>). For the final ESP model, we thus chose to represent enzymes with ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors and small molecules with GNN-generated, task-specific fingerprints.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Prediction performance on the test set for all four combinations of enzyme and small molecule representations</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>ROC-AUC score</th><th>Accuracy</th><th>MCC</th></tr></thead><tbody><tr><td>ESM-1b+ECFP</td><td>0.937</td><td>87.2%</td><td>0.69</td></tr><tr><td>ESM-1b<sub><italic>t</italic><italic>s</italic></sub>+ECFP</td><td>0.950</td><td>90.5%</td><td>0.75</td></tr><tr><td>ESM-1b+GNN</td><td>0.940</td><td>88.8%</td><td>0.72</td></tr><tr><td>ESM-1b<sub><italic>t</italic><italic>s</italic></sub>+GNN</td><td>0.956</td><td>91.5%</td><td>0.78</td></tr></tbody></table></table-wrap></p>
      <p id="Par23">To compare the gradient boosting model to alternative machine learning models, we also trained a logistic regression model and a random forest model for the task of predicting enzyme-substrate pairs from the combined ESM-1b<sub><italic>t</italic><italic>s</italic></sub> and GNN vectors. However, these models performed worse compared to the gradient boosting model (Supplementary Table <xref rid="MOESM1" ref-type="media">2)</xref>.</p>
      <p id="Par24">The GNN used to represent small molecules in the best-performing model was pre-trained for the task of predicting the Michaelis constants <italic>K</italic><sub>M</sub> of enzyme-substrate pairs. To test if this pre-training improved the predictions, we also tested model performance for fingerprints that were created with a GNN that was not pre-trained. Using a pre-trained GNN indeed led to better model performance (Supplementary Table <xref rid="MOESM1" ref-type="media">3</xref>; <italic>p</italic> &lt; 10<sup>−7</sup> from McNemar’s test).</p>
      <p id="Par25">The results summarized in Table <xref rid="Tab1" ref-type="table">1</xref> demonstrate that re-training and fine-tuning the ESM-1b model can significantly improve model performance. This finding contrasts previous observations that fine-tuning protein representations can negatively influence model performance and can lead to worse results compared to using the original ESM-1b model<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR51">51</xref></sup>. To achieve the improved enzyme representations, we added an extra token for the whole enzyme, and we trained the model to store all relevant information for the prediction task in this token. To investigate the importance of the added token for the observed superior performance, we alternatively re-trained the ESM-1b without such an extra token. Our results show that using the extra token indeed improves model performance (Supplementary Table <xref rid="MOESM1" ref-type="media">4</xref>; <italic>p</italic> = 0.040 from McNemar’s test).</p>
    </sec>
    <sec id="Sec7">
      <title>Good predictions even for unseen enzymes</title>
      <p id="Par26">It appears likely that prediction quality is best for enzymes that are highly similar to enzymes in the training set, and decreases for enzymes that are increasingly dissimilar to the enzymes used for training. How strong is that dependence? To answer this question, we first calculated the maximal enzyme sequence identity compared to the enzymes in the training set for all 2291 enzymes in the test set. Next, we split the test set into three subgroups: data points with enzymes with a maximal sequence identity to training data between 0 and 40%, between 40% and 60%, and between 60% and 80%.</p>
      <p id="Par27">For data points with high sequence identity levels (60-80%), the ESP model is highly accurate, with an accuracy of 95%, ROC-AUC score of 0.99, and MCC of 0.88 (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). ESP still performs very well for data points with intermediate sequence identity levels (40–60%), achieving an accuracy of 93%, ROC-AUC score 0.97, and MCC 0.83. Even for enzymes with low sequence identity to training data (0−40%), the ESP model achieves good results and classifies 89% of the data points correctly, with ROC-AUC score 0.93 and MCC 0.72. Thus, while using more similar enzymes during training improves the prediction quality, very good prediction accuracy can still be achieved for enzymes that are only distantly related to those in the training set. The observed differences were statistically significant for sequence identities 0–40% versus 40–60% (Mann-Whitney <italic>U</italic> test: <italic>p</italic> &lt; 10<sup>−23</sup>), but not for 40–60% versus 60–80% (<italic>p</italic> = 0.14).<fig id="Fig4"><label>Fig. 4</label><caption><title>Accurate predictions even for enzymes with distinct sequence similarity compared to enzymes in the training data.</title><p>We divided the test set into subsets with different levels of enzyme sequence identity compared to enzymes in the training set. <bold>a</bold> ESP accuracies, calculated separately for enzyme-small molecule pairs where the small molecule occurred in the training set and where it did not occur in the training set. <bold>b</bold> ESP ROC curves. The dotted line displays the ROC curve expected for a completely random model. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_38347_Fig4_HTML" id="d32e864"/></fig></p>
    </sec>
    <sec id="Sec8">
      <title>Low model performance for unseen small molecules</title>
      <p id="Par28">In the previous subsection, we showed that model performance is highest for enzymes that are similar to proteins in the training set. Similarly, it appears likely that the model performs better when making predictions for small molecules that are also in the training set. To test this hypothesis, we divided the test set into data points with small molecules that occurred in the training set (<italic>N</italic> = 13,459) and those with small molecules that did not occur in the training set (<italic>N</italic> = 530).</p>
      <p id="Par29">The ESP model does not perform well for data points with small molecules not present in the training set. When considering only enzyme-small molecules pairs with small molecules not represented in the training set and an enzyme sequence identity level of 0–40% compared to the training data, ESP achieves an accuracy of 71%, ROC-AUC score 0.59, and MCC 0.15. At an enzyme sequence identity level of 40–60%, accuracy improves to 83%, with ROC-AUC score 0.78, and MCC 0.25 for unseen small molecules. At high enzyme sequence identity levels of 60–80%, the accuracy reaches 90%, with ROC-AUC score 0.71, and MCC 0.27. Thus, for unseen small molecules, even a very moderate model performance requires that proteins similar to the enzyme ( &gt; 40% identity) are present in the training set. We again found the differences to be statistically significant for 0–40% versus 40–60% (Mann-Whitney <italic>U</italic> test: <italic>p</italic> &lt; 10<sup>−20</sup>), but not for 40–60% versus 60–80% (<italic>p</italic> = 0.226).</p>
      <p id="Par30">For those test data points with small molecules not present in the training set, we wondered if a high similarity of the small molecule compared to at least one substrate in the training set leads to improved predictions, analogous to what we observed for enzymes with higher sequence identities. For each small molecules not present in the training set, we calculated the maximal pairwise similarity score compared to all substrates in the training set. We could not find any evidence that a higher maximal similarity score leads to better model performance (Supplementary Fig. <xref rid="MOESM1" ref-type="media">3)</xref>. Hence, we conclude that ESP only achieves high accuracies for new enzyme-small molecule pairs if the small molecule was present among the ~1 400 substrates of our training set.</p>
      <p id="Par31">How many training data points with identical substrates are needed to achieve good model performance? For every small molecule in the test set, we counted how many times the same molecule occurs as an experimentally confirmed substrate in the training set. Supplementary Fig. <xref rid="MOESM1" ref-type="media">4</xref> shows that having as few as two positive training data points for a given small molecule leads to good accuracy when pairing the same small molecule with other enzymes.</p>
    </sec>
    <sec id="Sec9">
      <title>Model performance increases with increased training set size</title>
      <p id="Par32">The previous subsections suggest that a bigger training set with a more diverse set of enzymes and small molecules should lead to improved performance. However, using more data does not guarantee an improved model performance. For example, there could be a limitation in the model architecture that prevents the model from better fitting the data. To test how our model performs with different amounts of training data and to analyze if more data is expected to lead to higher generalizability, we trained the gradient boosting model with different training set sizes, ranging from 30% to 100% of the available training data. Figure <xref rid="Fig5" ref-type="fig">5</xref> shows that accuracy and ROC-AUC score indeed increase with increasing training set size (Spearman rank correlations, accuracy: <italic>ρ</italic><sup>2</sup> = 0.95, <italic>p</italic> &lt; 10<sup>−4</sup>; ROC-AUC score: <italic>ρ</italic><sup>2</sup> = 1.0, <italic>p</italic> &lt; 10<sup>−15</sup>). Thus, collecting more and more diverse data – for example, through targeted additional experiments – will likely lead to further model improvements.<fig id="Fig5"><label>Fig. 5</label><caption><title>Model performance increases with training set size.</title><p>Points show accuracies and ROC-AUC scores for the test set versus the fraction of the available training data used for training the gradient-boosting model. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_38347_Fig5_HTML" id="d32e941"/></fig></p>
    </sec>
    <sec id="Sec10">
      <title>ESP can express uncertainty</title>
      <p id="Par33">Internally, our trained classification model does not simply output the positive or negative class as a prediction. Instead, it outputs a prediction score between 0 and 1, which can be interpreted as a measurement of the probability for a data point to belong to the positive class. So far, we assigned all predictions with a score ≥0.5 to the positive class, and all predictions below 0.5 to the negative class. To provide a more detailed view of prediction accuracies, Fig. <xref rid="Fig6" ref-type="fig">6</xref> displays the distributions of the true (blue) and false (red) predictions for our test set across prediction scores.<fig id="Fig6"><label>Fig. 6</label><caption><title>Prediction scores around 0.5 indicate model uncertainty.</title><p>Stacked histogram bars display the prediction score distributions of true predictions (blue) and false predictions (red). The inset shows a blow-up of the interval [0.2, 0.8]. Source data are provided as a Source Data file.</p></caption><graphic xlink:href="41467_2023_38347_Fig6_HTML" id="d32e960"/></fig></p>
      <p id="Par34">Most true predictions have a score either close to 0 or close to 1, i.e., the ESP model is very confident about these predictions. In contrast, false predictions are distributed much more evenly across prediction scores. Approximately 4% of prediction scores for our test data fall between 0.4 and 0.6. The model seems to be uncertain for these data points: for this subset, predictions are only barely better than random guesses, with an accuracy of 59%, ROC-AUC score 0.60, and MCC 0.17 (Fig. <xref rid="Fig6" ref-type="fig">6</xref>, inset). Thus, when applied in practice, prediction scores between 0.4 and 0.6 should be considered uncertain and should not be assigned to one of the two classes.</p>
    </sec>
    <sec id="Sec11">
      <title>ESP outperforms two recently published models</title>
      <p id="Par35">We compared ESP with two recently published models for predicting the substrate scopes of specific enzyme families. ESP has been trained with much more data points compared to the previously published models; conversely, these previous models used much more detailed input information. Thus, a fair, direct comparison of model architectures is impossible. Instead, we analyzed if our model, which is capable of making use of large amounts of freely available data, can lead to better prediction accuracies than much more targeted approaches that necessarily work on smaller datasets.</p>
      <p id="Par36">Mou et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> trained four different machine learning models (logistic regression, random forest, gradient-boosted decision trees, and support vector machines) to predict substrates of bacterial nitrilases. For model training and validation, they used a dataset with all possible combinations of 12 enzymes and 20 small molecules (<italic>N</italic> = 240), randomly split into 80% training data and 20% test data. We added all training data from Ref. <sup><xref ref-type="bibr" rid="CR14">14</xref></sup> to our training set and validated the updated ESP model on the corresponding test data, which had no overlap with our training data. Mou et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> achieved an accuracy of 82% and a ROC-AUC score of 0.90 on the test set. ESP achieves better results, with an accuracy of 87.5%, ROC-AUC score 0.94, and MCC 0.75. This improvement is particularly striking given that Mou et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> used knowledge about the enzymes’ 3D structures and binding sites, while we only use a representation of the linear amino acid sequences.</p>
      <p id="Par37">Yang et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> published a decision tree-based model, GT-Predict, for predicting the substrate scope of glycosyltransferases of plants. As a training set, they used 2847 data points with 59 different small molecules and 53 different enzymes from <italic>Arabidopsis thaliana</italic>, i.e., the data covered 90.7% of all possible enzyme-small molecule combinations. These authors used two independent test sets to validate the model, one dataset with 266 data points with enzymes from <italic>Avena strigosa</italic> and another dataset with 380 data points with enzymes from <italic>Lycium barbarum</italic>. On those two test sets, GT-Predict achieves accuracies of 79.0% and 78.8%, respectively, and MCCs of 0.338 and 0.319, respectively. We added the training set from Ref. <sup><xref ref-type="bibr" rid="CR15">15</xref></sup> to our training set. The test sets from <italic>Avena strigosa</italic> and <italic>Lycium barbarum</italic> had no overlap with our training data. For these two sets, we achieved similar accuracies as Yang et al. (78.2% in both cases), but substantially improved MCCs: 0.484 for <italic>Avena strigosa</italic> and 0.517 for <italic>Lycium barbarum</italic> (ROC-AUC scores were 0.80 and 0.84, respectively). As the test datasets used by Yang et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> are imbalanced, with a proportion of 18–31% of positive data points, the MCC is a more meaningful score compared to the accuracy<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>; we hence conclude that ESP outperforms GT-Predict. Beyond benchmarking the performance of ESP, the above comparisons of our model predictions to two (almost) complete experimental datasets also indicate that ESP is indeed capable of predicting the full substrate scope of enzymes.</p>
      <p id="Par38">We also tested model performances for the test sets by Mou et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> and Yang et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> without adding any new training data to ESP. Only ~5% and ~8% of the small molecules in these test sets did already occur in our training set. As we showed above that performance drops massively if the model is applied to unseen small molecules (Fig. <xref rid="Fig4" ref-type="fig">4</xref>a), we did not expect good model performances. Indeed, for all three test sets, accuracies are below 68%, ROC-AUC scores are below 0.59, and MCCs are below 0.12 (Supplementary Table <xref rid="MOESM1" ref-type="media">5)</xref>.</p>
    </sec>
    <sec id="Sec12">
      <title>Web server facilitates easy use of ESP</title>
      <p id="Par39">We implemented a web server that allows an easy use of ESP without requiring programming skills or the installation of specialized software. It is available at <ext-link ext-link-type="uri" xlink:href="https://esp.cs.hhu.de">https://esp.cs.hhu.de</ext-link>. As input, the web server requires an enzyme amino acid sequence and a representation of a small molecule (either as a SMILES string, KEGG Compound ID, or InChI string). Users can either enter a single enzyme-small molecule pair into an online form, or upload a CSV file with multiple such pairs. In addition to the prediction score, the ESP web server reports how often the entered metabolite was present as a true substrate in our training set. Since we have shown that model performance drops substantially when the model is applied to small molecules not used during training, we recommend to use the prediction tool only for those small molecules represented in our training dataset. We uploaded a full list with all small molecules from the training set to the web server homepage, listing how often each one is present among the positive data points.</p>
    </sec>
  </sec>
  <sec id="Sec13" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par40">Here, we present a general approach for predicting the substrate scope of enzymes; ESP achieves an accuracy of over 91% on an independent test set with enzymes that share at most 80% sequence identity with any enzyme used for training. Notably, the model performs with an accuracy of 89% even for enzymes with very low sequence identity (&lt;40%) to proteins in the training set. This performance seems remarkable, as it is believed that enzymes often evolve different substrate specificities or even different functions if sequence identity falls below 40%<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>.</p>
    <p id="Par41">To achieve these results, we use very general input features: a task-specific fingerprint of the small molecule, constructed with a graph neural network (GNN) from a graph representing structural information, and a numerical representation of the enzyme calculated from its amino acid sequence. We show that creating task-specific enzyme representations leads to significant improvements compared to non-task-specific enzyme representations (Fig. <xref rid="Fig3" ref-type="fig">3)</xref>. One of the major challenges in predicting enzyme-substrate relationships is a lack of experimentally confirmed non-binding enzyme-substrate pairs. To overcome this challenge, we developed a carefully devised strategy of randomly sampling negative enzyme-molecule pairs. Although this data augmentation can potentially lead to false-negative data points, such false negatives are expected to be rare, an expectation that is confirmed by the good results on independent test data sets. Future refinements of this approach might boost model performance further. For example, when creating negative data points for confirmed enzyme-substrate pairs, a tighter decision boundary might result from preferentially choosing structurally similar substrates of highly different enzymes. On the other hand, the sets of true substrates of highly similar enzymes often overlap, and excluding known substrates of highly similar enzymes could avoid creating some false negative data points.</p>
    <p id="Par42">An additional avenue towards potential model improvements could be to test new model architectures. In this study, we trained two separate models for creating task-specific enzyme and small molecule representations. Future work could investigate if the pre-training of the enzyme representation and the small molecule representation could be performed jointly in a single model, thereby creating matched, task-specific enzyme and small molecule representations simultaneously.</p>
    <p id="Par43">Despite the structural similarities of ESP to state-of-the-art models for predicting drug-target binding affinities (DTBAs) and for predicting Michaelis-Menten constants of enzyme-substrate pairs<sup><xref ref-type="bibr" rid="CR25">25</xref>–<xref ref-type="bibr" rid="CR29">29</xref></sup>, the performances of these models are not comparable, as we trained ESP for a binary classification task, whereas the other models address regression tasks. Instead, we compare our approach to two recently published models for predicting enzyme-substrate pairs<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup>. These two models used very specific input features, such as an enzyme’s active site properties and physicochemical properties of the metabolite, and were designed and trained for only a single enzyme family. Our general ESP model – which can be trained on much larger datasets – achieves superior results, despite learning and extracting all relevant information for this task from much less detailed, general input representations. The application of ESP to the dataset from Mou et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> also demonstrate that our model can successfully distinguish between similar potential substrates for the same enzyme, as it achieved good results when it was applied to different nitriles for bacterial nitrilases.</p>
    <p id="Par44">One limitation of ESP is that model performance drops substantially for small molecules that did not occur in the training set. However, the current version of ESP can still be applied successfully to a broad range of almost 1400 different small molecules present in our dataset. Once more training data becomes available, model performance will very likely improve further (Fig. <xref rid="Fig5" ref-type="fig">5)</xref>. Mining other biochemical databases–such as BRENDA<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>, Sabio-RK<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>, and UniProt<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> – for new and non-overlapping data might be a low-cost way to expand the number of different small molecules in the dataset. Adding as few as two additional positive training data points for new molecules will typically lead to accurate predictions (Supplementary Figure <xref rid="MOESM1" ref-type="media">4)</xref>.</p>
    <p id="Par45">The recent development of AlphaFold<sup><xref ref-type="bibr" rid="CR55">55</xref></sup> and RoseTTAFold<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> facilitates predictions of the 3D structure for any protein with known amino acid sequence. Future work may also include input features extracted from such predicted enzyme structures. Our high-quality dataset with many positive and negative enzyme-small metabolite pairs, which is available on GitHub, might be a promising starting point to explore the utility of such features.</p>
    <p id="Par46">A main use case for the ESP model will be the prediction of possible substrate candidates for single enzymes. In contrast, ESP will likely not lead to satisfactory results when used to predict all enzyme-substrate pairs in a genome-scale metabolic model. This problem results from the trade-off between the True Positive Rate (TPR) and the False Postive Rate (FPR) for different classification thresholds (Fig. <xref rid="Fig3" ref-type="fig">3</xref>b). For example, choosing a classification threshold with a TPR of ~80% leads to a FPR of ~5%. If we consider a genome-scale model with approximately 2000 enzymes and 2000 metabolites, then there exist ~4 × 10<sup>6</sup> possible enzyme-small molecule pairs, of which only about 6000 will be true enzyme-substrate pairs. A TPR of 80% would lead to the successful detection of 4800 true pairs. At the same time, an FPR of 5% would lead to an additional ~200,000 false predictions.</p>
    <p id="Par47">If, on the other hand, ESP is applied to a set of pre-selected candidate substrates for a single enzyme, a false positive rate of 5% can be acceptable. If we choose 200 molecules as substrate candidates, where one of these 200 is a true substrate for the enzyme, an FPR of 5 % means that the model predicts only ~ 10 molecules falsely as a substrate, and there is an 80% chance that the true substrate is labeled correctly. This could help to bring down the experimental burden – and associated costs – of biochemical assays to levels where laboratory tests become tractable.</p>
  </sec>
  <sec id="Sec14">
    <title>Methods</title>
    <sec id="Sec15">
      <title>Software</title>
      <p id="Par48">All software was coded in Python<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. We implemented and trained the neural networks using the deep learning library PyTorch<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. We fitted the gradient boosting models using the library XGBoost<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>.</p>
    </sec>
    <sec id="Sec16">
      <title>Creating a database with enzyme-substrate pairs</title>
      <p id="Par49">To create a database with positive enzyme-substrate pairs, we searched the Gene Ontology (GO) annotation database for UniProt IDs<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> for experimentally confirmed annotations of the catalytic activity of enzymes. A GO annotation consists of a GO Term that is assigned to a UniProt ID, which is an identifier for proteins. GO Terms can contain information about the biological processes, molecular functions, and cellular components in which proteins are involved<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. We first created a list with all 6587 catalytic GO Terms containing information about enzyme-catalyzed reactions. For each of these GO Terms, we extracted identifiers for the substrates involved in the reaction. If the GO Term definition stated that the reaction is reversible, we treated all reactants (including products) as substrates; if a reaction was labeled as irreversible, we only extracted the reactants annotated as substrates. For this purpose, we used a RHEA reaction ID<sup><xref ref-type="bibr" rid="CR60">60</xref></sup> from the GO Term, which was available for 4086 out of 6587 GO Terms. If no RHEA reaction ID was listed for the GO Term, we extracted the substrate names via text mining from the GO Term definition. Substrate names were then mapped to KEGG and ChEBI identifiers via the synonym database from KEGG<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>, or, if no entry in KEGG was found, the PubChem synonym database<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>. We discarded all 824 catalytic GO Terms for which we could not map at least one substrate to an identifier.</p>
      <p id="Par50">Entries in the GO annotation database have different levels of evidence: experimental, phylogenetically-inferred, computational analysis, author statement, curator statement, and electronic evidence. For training our final model, we were interested only in entries with catalytic GO Terms based on experimental evidence. From these, we removed 6219 enzyme-substrate pairs with water, oxygen, and ions, as these small substrates did not lead to unique representations (see below). We extracted protein and substrate IDs for the remaining 18,351 enzyme-substrate pairs with experimental evidence. 15051 of these pairs resulted from a GO Term that was associated with a RHEA reaction ID, the rest were created via text mining of GO Term definitions. These data points are combinations of 12,156 unique enzymes and 1379 unique substrates.</p>
      <p id="Par51">Before training our models for predicting enzyme-substrate pairs, we pre-trained the ESM-1b protein representations to capture information relevant to enzyme-substrate binding. Due to the high dimensionality of the protein representations, much more data than the 18,351 enzyme-substrate pairs with experimental evidence was required for this task. Only for this pre-training, we thus additionally extracted protein and substrate IDs for 274,030 entries with catalytic GO Terms and phylogenetically inferred evidence (this set excludes 98 384 entries with water, oxygen, and ions as substrates). 200,634 of these enzyme-substrate pairs resulted from a GO Term associated with a RHEA reaction ID, the rest were constructed via text mining of GO Term definitions. These additional data points based on phylogenetic evidence are combinations of 198,259 unique enzymes and 661 unique substrates.</p>
      <p id="Par52">It might be surprising that although we found many more enzyme-substrate pairs with phylogenetically inferred evidence compared to data points with experimental evidence, the number of unique substrates is much smaller. To investigate if we can see a systematic difference between both groups, we plotted the distribution of the first digit of EC classes among the enzymes of both classes. However, no substantial difference was evident except for an over-representation of EC6 (ligases) in the data with phylogenetic evidence (Supplementary Fig. <xref rid="MOESM1" ref-type="media">5)</xref>. Hence, we assume that the data structure of phylogenetically inferred data points is not an important issue for the calculation of enzyme representations.</p>
      <p id="Par53">We downloaded all enzyme amino acid sequences via the UniProt mapping service<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>.</p>
    </sec>
    <sec id="Sec17">
      <title>Sampling negative data points</title>
      <p id="Par54">For every positive enzyme-substrate pair in our dataset, we created three negative data points for the same enzyme by randomly sampling small molecules. The distinction between true and false substrates is harder for small molecules that are similar to the true, known substrates. To challenge our model to learn this distinction, we restricted our sampling of negative data points to small molecules similar to the true substrate. For this purpose, we first calculated the pairwise similarity of all small molecules in our dataset with the function FingerprintSimilarity from the RDKit package DataStructs<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>. This function uses molecular fingerprints of the molecules as its input and computes values between zero (no similarity) and one (high similarity). If possible, we sampled small molecules with a similarity score between 0.7 and 0.95. If we did not find such molecules, we reduced the lower bound in steps of 0.2 until enough small molecules could be sampled. We had to reduce the lower bound in ~ 19% of enzyme-substrate pairs. We did not simply choose the three most similar compounds as negative data points, because if a substrate appears multiple times in our dataset, this would have led to selecting always the same three small molecules as non-substrates. Instead, we randomly picked three molecules from within the selected similarity range. During this sampling process, we took the distribution of the small molecules among the positive data points into account, i.e., molecules that occur more frequently as substrates among the positive data points also appear more frequently among the negative data points. To achieve this, we excluded small molecules from the sampling process if these molecules were already sampled enough times (i.e., three times their total occurrence in the set of positive enzyme-substrate pairs).</p>
    </sec>
    <sec id="Sec18">
      <title>Splitting the dataset into training and test sets</title>
      <p id="Par55">Before we split the dataset into training and test sets, we clustered all sequences by amino acid sequence identity using the CD-HIT algorithm<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>. The clusters were created in such a way that two sequences from different clusters do not have a pairwise sequence identity higher than 80%. We used these clusters to split the dataset randomly into 80% training data and 20% test data using a sequence identity cutoff of 80%, i.e., every enzyme in the test set has a maximal sequence identity of 80% compared to any enzyme in the training set. This was achieved by placing all sequences from one cluster either into the training or the test set. To analyze the ESP performance for different sequence identity levels, we further split the test set into subsets with maximal sequence identity to enzymes in the training set of 0–40%, 40–60%, and 60–80% using the CD-HIT algorithm<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>.</p>
    </sec>
    <sec id="Sec19">
      <title>Calculating extended-connectivity fingerprints</title>
      <p id="Par56">All small molecules in our final datasets were either assigned to a KEGG ID or ChEBI (Chemical Entities of Biological Interest) ID. For all small molecules with a KEGG ID, we downloaded an MDL Molfile with 2D projections of its atoms and bonds from KEGG<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>. If no MDL Molfile could be obtained in this way, we instead downloaded the International Chemical Idenitifier (InChI) string via the mapping service of MetaCyc<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>, if a ChEBI ID was available. We then used the package Chem from RDKit<sup><xref ref-type="bibr" rid="CR63">63</xref></sup> with the MDL Molfiles or InChI strings as the input to calculate the 1024-dimensional binary ECFPs<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> with a radius (number of iterations) of 3. We also calculated 512 and 2048-dimensional ECFPs to investigate if these lead to better model performance than 1024-dimensional ECFPs.</p>
    </sec>
    <sec id="Sec20">
      <title>Calculating task-specific fingerprints for small molecules</title>
      <p id="Par57">In addition to the pre-defined ECFPs, we also used a graph neural network (GNN) to calculate task-specific numerical representations for the small molecules. GNNs are neural networks that can take graphs as their input<sup><xref ref-type="bibr" rid="CR38">38</xref>–<xref ref-type="bibr" rid="CR40">40</xref></sup>. A molecule can be represented as a graph by interpreting the atoms and bonds of the molecule as nodes and edges, respectively.</p>
      <p id="Par58">We trained and implemented a variant of GNNs called Directed Message Passing Neural Network (D-MPNN)<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, using the Python package PyTorch<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. To provide the GNN with information about the small molecules, we calculated feature vectors for every bond and every atom in all molecules<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. For every atom, these features comprise the atomic number, number of bonds, charge, number of hydrogen bonds, mass, aromaticity, hybridization type, and chirality; for every bond, these features comprise bond type, part of ring, stereo configuration, and aromaticity. To input this information into a GNN, the graphs and the feature vectors are encoded with tensors and matrices. While a graph is processed by a GNN, all atom feature vectors are iteratively updated for a pre-defined number of steps by using information of neighboring bond and atom feature vectors. Afterwards, all atom feature vectors are pooled together by applying the element-wise mean to obtain a single graph representation. The dimension <italic>D</italic> of the updated atom feature vectors and of the final graph representation can be freely chosen; we chose <italic>D</italic> = 100.</p>
      <p id="Par59">This small molecule representation was then concatenated with a small representation of an enzyme; we chose to use a small enzyme representation instead of the full ESM-1b vector to keep the input dimension of the machine learning model used for learning the task-specific small molecule representation low. To compute the small enzyme representation, we performed principal component analysis (PCA)<sup><xref ref-type="bibr" rid="CR66">66</xref></sup> on the ESM-1b vectors (see below) and selected the first 50 principal components. The concatenated enzyme-small molecule vector was used as the input for a fully connected neural network (FCNN) with two hidden layers of size 100 and 32, which was trained for predicting whether the small molecule is a substrate for the enzyme. We trained the whole model (the GNN including the FCNN) end-to-end. Thereby, the model was challenged to store task-specific and meaningful information in the graph representations. After training, we extracted a graph representation for every small molecule in our training set, which was then used as input for the complete enzyme-substrate pair prediction model.</p>
      <p id="Par60">We performed a pre-training of the described GNN by training it for the related task of predicting the Michaelis constants <italic>K</italic><sub>M</sub> of enzyme-substrate pairs. As for the task of identifying potential enzyme-substrate pairs, the prediction of <italic>K</italic><sub>M</sub> is dependent on the interaction between enzymes and small molecules, and hence, this pre-training task challenged the GNN to learn interactions between an enzyme and a substrate. To train the model for the <italic>K</italic><sub>M</sub> prediction, we used a dataset that was previously constructed for a <italic>K</italic><sub>M</sub> prediction model<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. After pre-training, we fine-tuned the GNN by training it for the task of predicting enzyme-substrate pairs, i.e., we used all parameters that were learned during the pre-training task as initial parameters for the GNN that was fine-tuned.</p>
    </sec>
    <sec id="Sec21">
      <title>Calculating enzyme representations</title>
      <p id="Par61">We used the ESM-1b model<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> to calculate 1280-dimensional numerical representations of the enzymes. The ESM-1b model is a transformer network<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> that takes amino acid sequences as its input and produces numerical representations of the sequences. First, every amino acid in a sequence is converted into a 1280-dimensional representation, which encodes the type of the amino acid and its position in the sequence. Afterwards, every representation is updated iteratively for 33 update steps by using information about the representation itself as well as about all other representations of the sequence using the attention mechanism<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>. The attention mechanism allows the model to selectively focus only on relevant amino acid representations to make updates<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>. During training, ~15% of the amino acids in a sequence are masked at random, and the model is trained to predict the type of the masked amino acids. The ESM-1b model has been trained with ~27 million proteins from the UniRef50 dataset<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. To create a single representation for the whole enzyme, ESM-1b calculates the element-wise mean of all updated amino acids representations in a sequence<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. We created these representations for all enzymes in our dataset using the code and the trained ESM-1b model provided by the Facebook AI Research team on GitHub.</p>
    </sec>
    <sec id="Sec22">
      <title>Task-specific fine-tuning of the ESM-1b model</title>
      <p id="Par62">To create task-specific enzyme representations for our task of predicting enzyme-substrate pairs, we modified the ESM-1b model. For every input sequence, in addition to the representations of all individual amino acids, we added a token that represents the whole enzyme. This enzyme representation is updated in the same way as the amino acid representations. The parameters of this modified model are initialized with the parameters of the trained ESM-1b model, setting the additional enzyme token initially to the element-wise mean of the amino acid representations. After the last update layer of the model, i.e., after 33 update steps, we take the 1280-dimensional representation of the whole enzyme and concatenate it with a representation for a metabolite, the 1024-dimensional ECFP vector (see above).</p>
      <p id="Par63">This concatenated vector is then used as the input for a fully-connected neural network (FCNN) with two hidden layers of size 256 and 32. The whole model was trained end-to-end for the binary classification task of predicting whether the added metabolite is a substrate for the given enzyme. This training procedure challenged the model to store all necessary enzyme information for the prediction task in the enzyme representation. After training the modified model, we extracted the updated and task-specific representations, the ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors, for all enzymes in our dataset.</p>
      <p id="Par64">We implemented and trained this model using the Python package PyTorch<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. We trained the model with the extended dataset of 287,386 enzyme-substrate pairs with phylogenetically inferred or experimental evidence for 2 epochs on 6 NVIDA DGX A100s, each with 40GB RAM. Training the model for more epochs did not lead to improved results. Because of the immense computational power and long training times, it was not possible to perform a systematic hyperparameter optimization. We chose hyperparameters after trying a few selected hyperparameter settings with values similar to the ones that were used for training the original ESM-1b model.</p>
    </sec>
    <sec id="Sec23">
      <title>Fine-tuning the ESM-1b model without an additional token</title>
      <p id="Par65">To investigate the effect on model performance of adding a token for the whole enzyme to the ESM-1b model, we also re-trained the model without such an extra token. Instead, we calculated the element-wise mean of all amino acid representations after the last update layer of the model, as is done in the original ESM-1b model. We concatenated the resulting 1280-dimensional vector with a representation for a metabolite, the 1024-dimensional ECFP vector. As for the model described above, this concatenated vector is then used as the input for a fully-connected neural network (FCNN) with two hidden layers of size 256 and 32. The whole model was trained end-to-end for the binary classification task of predicting whether the added metabolite is a substrate for the given enzyme. The training procedure of this model was identical to the model with an additional token for the whole enzyme (see above).</p>
    </sec>
    <sec id="Sec24">
      <title>Hyperparameter optimization of the gradient-boosting models</title>
      <p id="Par66">To find the best hyperparameters for the gradient boosting models, we performed 5-fold cross-validations (CVs). To ensure a high diversity between all folds, we created the five folds in such a way that the same enzyme would not occur in two different folds. We used the Python package hyperopt<sup><xref ref-type="bibr" rid="CR68">68</xref></sup> to perform a random grid search for the following hyperparameters: learning rate, maximum tree depth, lambda and alpha coefficients for regularization, maximum delta step, minimum child weight, number of training epochs, and weight for negative data points. The last hyperparameter was added because our dataset is imbalanced; this parameter allows the model to assign a lower weight to the negative data points during training. To ensure that our model is indeed not assigning too many samples to the over-represented negative class, we used a custom loss function that contains the False Negative Rate, FNR, and the False Positive Rate, FPR. Our loss function, 2 × FNR<sup>2</sup> + FPR<sup>1.3</sup>, penalizes data points that are mistakenly assigned to the negative class stronger than data points that are mistakenly assigned to the positive class. After hyperparameter optimization, we chose the set of hyperparameters with the lowest mean loss during CV. We used the python package xgboost<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> for training the gradient boosting models.</p>
    </sec>
    <sec id="Sec25">
      <title>Displaying the results of cross-validations with boxplots</title>
      <p id="Par67">We used boxplots to display the results of the 5-fold cross-validations, which we performed to find the best set of hyperparameters. We used a 2×interquartile range for the whiskers, the boxes extend from the lower to upper quartile values, and the red horizontal lines are displaying the median of the data points.</p>
    </sec>
    <sec id="Sec26">
      <title>Training of additional machine learning models</title>
      <p id="Par68">To compare the performance of the gradient boosting model to additional machine learning models, we also trained a logistic regression model and a random forest model for the same prediction task. To find the best hyperparameters for the models, we again performed 5-fold CVs on the training set. For the random forest model, the hyperparameter optimized was the number of estimators, and for the logistic regression model we searched for the best penalty function and coefficient of regularization strength. We used the python package scikit-learn<sup><xref ref-type="bibr" rid="CR69">69</xref></sup> for training both models.</p>
    </sec>
    <sec id="Sec27">
      <title>Validating our model on two additional test sets</title>
      <p id="Par69">We compared the performance of ESP with two published models for predicting the substrate scope of single enzyme families. One of these models is a machine learning model developed by Mou et al. to predict the substrates of 12 different bacterial nitrilases<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. Their dataset consists of 240 data points, where each of the 12 nitrilases was tested with the same 20 small molecules. This dataset was randomly split by Mou et al. into 80% training data and 20% test data<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. We added all training data to our training set. After re-training, we validated our model performance on the test set from Ref. <sup><xref ref-type="bibr" rid="CR14">14</xref></sup>.</p>
      <p id="Par70">The second model that we compared to ESP is a decision tree-based model, called GT-predict, for predicting the substrate scope of glycosyltransferases of plants<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. As a training set, Yang et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> used 2847 data points with 59 different small molecules and 53 different enzymes from <italic>Arabidopsis thaliana</italic>. They used two independent test sets to validate model performance: one dataset with 266 data points comprising 7 enzymes from <italic>Avena strigose</italic> and 38 different small molecules, and a second dataset with 380 data points comprising 10 enzymes from <italic>Lycium barbarum</italic> and 38 different small molecules. We added all training data to our training set. After re-training, we validated ESP model performance on both test sets from Ref. <sup><xref ref-type="bibr" rid="CR15">15</xref></sup>.</p>
    </sec>
    <sec id="Sec28">
      <title>Analyzing the effect of training set size</title>
      <p id="Par71">To analyze the effect of different training set sizes, we created eight different subsets of our training set, with sizes ranging from 30% to 100% of the original training set size. To create these subsets, we first generated an enzyme list containing all enzymes of the training set in random order. To create the subsets, we extracted all training data points with enzymes that occur in the first 30%, 40%, …, 100% of the generated enzyme list. Afterwards, we re-trained our model on all different subsets of the training set and validated each version on our full test set.</p>
    </sec>
    <sec id="Sec29">
      <title>Statistical tests for model comparison</title>
      <p id="Par72">We tested if the difference in model performance between the two models with ESM-1b<sub><italic>t</italic><italic>s</italic></sub> and ECFP vectors compared to the model with ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors and GNN-generated fingerprints is statistically significant. For this purpose, we used McNemar’s test<sup><xref ref-type="bibr" rid="CR70">70</xref></sup> (implemented in the Python package Statsmodels<sup><xref ref-type="bibr" rid="CR71">71</xref></sup>), testing the null hypothesis that both models have a similar proportion of errors on our test set. We could reject the null hypothesis (<italic>p</italic> &lt; 10<sup>−9</sup>), concluding that combining ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors with GNN-generated fingerprints leads to a statistically significant improvement over a combination with ECFP vectors. We performed the same test to show that a model with fingerprints created with a pre-trained GNN achieves improved results compared to a model with fingerprints created with a not pre-trained GNN (<italic>p</italic> &lt; 10<sup>−7</sup>). Moreover, we used McNemar’s test to show that the model with ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors and GNN-generated fingerprints achieves significantly improved performance compared to the model with ESM-1b and ECFP vectors as the input (<italic>p</italic> &lt; 10<sup>−37</sup>) and also compared to the model with ESM-1b and GNN-generated fingerprints (<italic>p</italic> &lt; 10<sup>−19</sup>). Furthermore, we used the same test to show that the task-specific enzyme representations, the ESM-1b<sub><italic>t</italic><italic>s</italic></sub> vectors, that were created by fine-tuning the ESM-1b model with an extra token for the whole enzyme achieved improved performance compared to task-specific enzyme representations that resulted from fine-tuning the ESM-1b model without such an extra token (<italic>p</italic> = 0.040).</p>
      <p id="Par73">We also tested if the differences in model performance between the three different splits of our test set with different enzyme sequence identity levels (0–40%, 40–60%, and 60–80%) are statistically significant. Here, we used the non-parametric two-sided Mann–Whitney <italic>U</italic> test implemented in the Python package SciPy<sup><xref ref-type="bibr" rid="CR72">72</xref></sup> to test the null hypothesis that the prediction errors for the different splits are equally distributed.</p>
    </sec>
    <sec id="Sec30">
      <title>Reporting summary</title>
      <p id="Par74">Further information on research design is available in the <xref rid="MOESM3" ref-type="media">Nature Portfolio Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec31">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41467_2023_38347_MOESM1_ESM.pdf">
            <caption>
              <p>Supplementary Information</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="41467_2023_38347_MOESM2_ESM.pdf">
            <caption>
              <p>Peer Review File</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="41467_2023_38347_MOESM3_ESM.pdf">
            <caption>
              <p>Reporting Summary</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec32">
        <title>Source data</title>
        <p id="Par78">
          <media position="anchor" xlink:href="41467_2023_38347_MOESM4_ESM.xlsx" id="MOESM4">
            <caption>
              <p>Source Data</p>
            </caption>
          </media>
        </p>
      </sec>
    </app>
  </app-group>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>The online version contains supplementary material available at 10.1038/s41467-023-38347-2.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank Veronica Maurino for insightful discussions. Computational support and infrastructure was provided by the “Centre for Information and Media Technology” (ZIM) at the University of Düsseldorf (Germany). We acknowledge financial support to M.J.L. by the German Research Foundation (DFG) through CRC 1310, and, under Germany’s Excellence Strategy, through EXC2048/1 (Project ID:390686111), as well as through a grant by the Volkswagen Foundation under the “Life” initiative.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>S.R. implemented and trained the task-specific ESM-1b<sub><italic>t</italic><italic>s</italic></sub> model. A.K. designed the dataset and models and performed all other analyses. M.K.M.E. conceived of the study. M.J.L. supervised the study and acquired funding. A.K., M.K.M.E., and M.J.L. interpreted the results and wrote the manuscript.</p>
  </notes>
  <notes notes-type="peer-review">
    <title>Peer review</title>
    <sec id="FPar1">
      <title>Peer review information</title>
      <p id="Par75"><italic>Nature Communications</italic> thanks the anonymous reviewers for their contribution to the peer review of this work. A <xref rid="MOESM2" ref-type="media">peer reviewer file</xref> is available.</p>
    </sec>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open Access funding enabled and organized by Projekt DEAL.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>All data generated in this study and all processed data used to produce the results of this study have been deposited in the GitHub repository available at <ext-link ext-link-type="uri" xlink:href="https://github.com/AlexanderKroll/ESP">https://github.com/AlexanderKroll/ESP</ext-link><sup><xref ref-type="bibr" rid="CR73">73</xref></sup>. Source data for all figures are provided with this paper. <xref rid="Sec32" ref-type="sec">Source data</xref> are provided with this paper.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>The Python code used to generate all results is publicly available only at <ext-link ext-link-type="uri" xlink:href="https://github.com/AlexanderKroll/ESP">https://github.com/AlexanderKroll/ESP</ext-link><sup><xref ref-type="bibr" rid="CR73">73</xref></sup>.</p>
  </notes>
  <notes id="FPar2" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par76">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Cooper, G. M., Hausman, R. E. &amp; Hausman, R. E.<italic>The Cell: A Molecular Approach</italic>, vol. 4 (ASM press, Washington DC, 2007).</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Copley</surname>
            <given-names>SD</given-names>
          </name>
        </person-group>
        <article-title>Shining a light on enzyme promiscuity</article-title>
        <source>Curr. Opin. Struct. Biol.</source>
        <year>2017</year>
        <volume>47</volume>
        <fpage>167</fpage>
        <lpage>175</lpage>
        <pub-id pub-id-type="doi">10.1016/j.sbi.2017.11.001</pub-id>
        <?supplied-pmid 29169066?>
        <pub-id pub-id-type="pmid">29169066</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Khersonsky</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Tawfik</surname>
            <given-names>DS</given-names>
          </name>
        </person-group>
        <article-title>Enzyme promiscuity: a mechanistic and evolutionary perspective</article-title>
        <source>Annu. Rev. Biochem.</source>
        <year>2010</year>
        <volume>79</volume>
        <fpage>471</fpage>
        <lpage>505</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev-biochem-030409-143718</pub-id>
        <?supplied-pmid 20235827?>
        <pub-id pub-id-type="pmid">20235827</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nobeli</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Favia</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Thornton</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>Protein promiscuity and its implications for biotechnology</article-title>
        <source>Nat. Biotechnol.</source>
        <year>2009</year>
        <volume>27</volume>
        <fpage>157</fpage>
        <lpage>167</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt1519</pub-id>
        <?supplied-pmid 19204698?>
        <pub-id pub-id-type="pmid">19204698</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Adrio</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Demain</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>Microbial enzymes: tools for biotechnological processes</article-title>
        <source>Biomolecules</source>
        <year>2014</year>
        <volume>4</volume>
        <fpage>117</fpage>
        <lpage>139</lpage>
        <pub-id pub-id-type="doi">10.3390/biom4010117</pub-id>
        <?supplied-pmid 24970208?>
        <pub-id pub-id-type="pmid">24970208</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Engineering a synthetic pathway for gentisate in pseudomonas chlororaphis p3</article-title>
        <source>Front. Bioeng. Biotechnol.</source>
        <year>2021</year>
        <volume>8</volume>
        <fpage>1588</fpage>
        <pub-id pub-id-type="doi">10.3389/fbioe.2020.622226</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>M-C</given-names>
          </name>
          <name>
            <surname>Law</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Wilkinson</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Micklefield</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Bioengineering natural product biosynthetic pathways for therapeutic applications</article-title>
        <source>Curr. Opin. Biotechnol.</source>
        <year>2012</year>
        <volume>23</volume>
        <fpage>931</fpage>
        <lpage>940</lpage>
        <pub-id pub-id-type="doi">10.1016/j.copbio.2012.03.008</pub-id>
        <?supplied-pmid 22487048?>
        <pub-id pub-id-type="pmid">22487048</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">UniProt Consortium. Uniprot: the universal protein knowledgebase in 2021. <italic>Nucl. Acids Res</italic>. <bold>49</bold>, D480–D489 (2021).</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rembeza</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Boverio</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Fraaije</surname>
            <given-names>MW</given-names>
          </name>
          <name>
            <surname>Engqvist</surname>
            <given-names>MK</given-names>
          </name>
        </person-group>
        <article-title>Discovery of two novel oxidases using a high-throughput activity screen</article-title>
        <source>ChemBioChem</source>
        <year>2022</year>
        <volume>23</volume>
        <fpage>e202100510</fpage>
        <pub-id pub-id-type="doi">10.1002/cbic.202100510</pub-id>
        <?supplied-pmid 34709726?>
        <pub-id pub-id-type="pmid">34709726</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Longwell</surname>
            <given-names>CK</given-names>
          </name>
          <name>
            <surname>Labanieh</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Cochran</surname>
            <given-names>JR</given-names>
          </name>
        </person-group>
        <article-title>High-throughput screening technologies for enzyme engineering</article-title>
        <source>Curr. Opin. Biotechnol.</source>
        <year>2017</year>
        <volume>48</volume>
        <fpage>196</fpage>
        <lpage>202</lpage>
        <pub-id pub-id-type="doi">10.1016/j.copbio.2017.05.012</pub-id>
        <?supplied-pmid 28624724?>
        <pub-id pub-id-type="pmid">28624724</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Black</surname>
            <given-names>GW</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A high-throughput screening method for determining the substrate scope of nitrilases</article-title>
        <source>Chem. Commun.</source>
        <year>2015</year>
        <volume>51</volume>
        <fpage>2660</fpage>
        <lpage>2662</lpage>
        <pub-id pub-id-type="doi">10.1039/C4CC06021K</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Detlefsen</surname>
            <given-names>NS</given-names>
          </name>
          <name>
            <surname>Hauberg</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Boomsma</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Learning meaningful representations of protein sequences</article-title>
        <source>Nat. Commun.</source>
        <year>2022</year>
        <volume>13</volume>
        <fpage>1914</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-022-29443-w</pub-id>
        <?supplied-pmid 35395843?>
        <pub-id pub-id-type="pmid">35395843</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pertusi</surname>
            <given-names>DA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting novel substrates for enzymes with minimal experimental effort with active learning</article-title>
        <source>Metab. Eng.</source>
        <year>2017</year>
        <volume>44</volume>
        <fpage>171</fpage>
        <lpage>181</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ymben.2017.09.016</pub-id>
        <?supplied-pmid 29030274?>
        <pub-id pub-id-type="pmid">29030274</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mou</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Machine learning-based prediction of enzyme substrate scope: Application to bacterial nitrilases</article-title>
        <source>Proteins Struct. Funct. Bioinf.</source>
        <year>2021</year>
        <volume>89</volume>
        <fpage>336</fpage>
        <lpage>347</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.26019</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Functional and informatics analysis enables glycosyltransferase activity prediction</article-title>
        <source>Nat. Chem. Biol.</source>
        <year>2018</year>
        <volume>14</volume>
        <fpage>1109</fpage>
        <lpage>1117</lpage>
        <pub-id pub-id-type="doi">10.1038/s41589-018-0154-9</pub-id>
        <?supplied-pmid 30420693?>
        <pub-id pub-id-type="pmid">30420693</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Röttig</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rausch</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kohlbacher</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Combining structure and sequence information allows automated prediction of substrate specificities within enzyme families</article-title>
        <source>PLoS Comput. Biol.</source>
        <year>2010</year>
        <volume>6</volume>
        <fpage>e1000636</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000636</pub-id>
        <?supplied-pmid 20072606?>
        <pub-id pub-id-type="pmid">20072606</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chevrette</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Aicheler</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Kohlbacher</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Currie</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>Medema</surname>
            <given-names>MH</given-names>
          </name>
        </person-group>
        <article-title>Sandpuma: ensemble predictions of nonribosomal peptide chemistry reveal biosynthetic diversity across actinobacteria</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <fpage>3202</fpage>
        <lpage>3210</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx400</pub-id>
        <?supplied-pmid 28633438?>
        <pub-id pub-id-type="pmid">28633438</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goldman</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>KK</given-names>
          </name>
          <name>
            <surname>Coley</surname>
            <given-names>CW</given-names>
          </name>
        </person-group>
        <article-title>Machine learning modeling of family wide enzyme-substrate specificity screens</article-title>
        <source>PLoS Comput. Biol.</source>
        <year>2022</year>
        <volume>18</volume>
        <fpage>e1009853</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1009853</pub-id>
        <?supplied-pmid 35143485?>
        <pub-id pub-id-type="pmid">35143485</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Visani</surname>
            <given-names>GM</given-names>
          </name>
          <name>
            <surname>Hughes</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Hassoun</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Enzyme promiscuity prediction using hierarchy-informed multi-label classification</article-title>
        <source>Bioinformatics</source>
        <year>2021</year>
        <volume>37</volume>
        <fpage>2017</fpage>
        <lpage>2024</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btab054</pub-id>
        <?supplied-pmid 33515234?>
        <pub-id pub-id-type="pmid">33515234</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ryu</surname>
            <given-names>JY</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>HU</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>SY</given-names>
          </name>
        </person-group>
        <article-title>Deep learning enables high-quality and high-throughput prediction of enzyme commission numbers</article-title>
        <source>PNAS</source>
        <year>2019</year>
        <volume>116</volume>
        <fpage>13996</fpage>
        <lpage>14001</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1821905116</pub-id>
        <?supplied-pmid 31221760?>
        <pub-id pub-id-type="pmid">31221760</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DEEPre: sequence-based enzyme EC number prediction by deep learning</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>34</volume>
        <fpage>760</fpage>
        <lpage>769</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx680</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sanderson</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Bileschi</surname>
            <given-names>ML</given-names>
          </name>
          <name>
            <surname>Belanger</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Colwell</surname>
            <given-names>LJ</given-names>
          </name>
        </person-group>
        <article-title>Proteinfer, deep neural networks for protein functional inference</article-title>
        <source>eLife</source>
        <year>2023</year>
        <volume>12</volume>
        <fpage>e80942</fpage>
        <pub-id pub-id-type="doi">10.7554/eLife.80942</pub-id>
        <?supplied-pmid 36847334?>
        <pub-id pub-id-type="pmid">36847334</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Bileschi, M. L. et al. Using deep learning to annotate the protein universe. Nat. Biotechnol.10.1038/s41587-021-01179-w (2022).</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rembeza</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Engqvist</surname>
            <given-names>MK</given-names>
          </name>
        </person-group>
        <article-title>Experimental and computational investigation of enzyme functional annotations uncovers misannotation in the ec 1.1. 3.15 enzyme class</article-title>
        <source>PLoS Comput. Biol.</source>
        <year>2021</year>
        <volume>17</volume>
        <fpage>e1009446</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1009446</pub-id>
        <?supplied-pmid 34555022?>
        <pub-id pub-id-type="pmid">34555022</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Öztürk</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Özgür</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ozkirimli</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Deepdta: deep drug–target binding affinity prediction</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <fpage>i821</fpage>
        <lpage>i829</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty593</pub-id>
        <?supplied-pmid 30423097?>
        <pub-id pub-id-type="pmid">30423097</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Feng, Q., Dueva, E., Cherkasov, A. &amp; Ester, M. Padme: A deep learning-based framework for drug-target interaction prediction. <italic>Preprint at</italic>10.48550/arXiv.1807.09741 (2018).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Karimi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Deepaffinity: interpretable deep learning of compound–protein affinity through unified recurrent and convolutional neural networks</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <fpage>3329</fpage>
        <lpage>3338</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz111</pub-id>
        <?supplied-pmid 30768156?>
        <pub-id pub-id-type="pmid">30768156</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kroll</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Engqvist</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Heckmann</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lercher</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>Deep learning allows genome-scale prediction of michaelis constants from structural features</article-title>
        <source>PLoS Biol.</source>
        <year>2021</year>
        <volume>19</volume>
        <fpage>e3001402</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pbio.3001402</pub-id>
        <?supplied-pmid 34665809?>
        <pub-id pub-id-type="pmid">34665809</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning-based k cat prediction enables improved enzyme-constrained model reconstruction</article-title>
        <source>Nat. Catal.</source>
        <year>2022</year>
        <volume>5</volume>
        <fpage>662</fpage>
        <lpage>672</lpage>
        <pub-id pub-id-type="doi">10.1038/s41929-022-00798-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Weininger</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>SMILES, a chemical language and information system. 1. introduction to methodology and encoding rules</article-title>
        <source>J. Chem. Inf. Comput. Sci.</source>
        <year>1988</year>
        <volume>28</volume>
        <fpage>31</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="doi">10.1021/ci00057a005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rogers</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hahn</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Extended-connectivity fingerprints</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2010</year>
        <volume>50</volume>
        <fpage>742</fpage>
        <lpage>754</lpage>
        <pub-id pub-id-type="doi">10.1021/ci100050t</pub-id>
        <?supplied-pmid 20426451?>
        <pub-id pub-id-type="pmid">20426451</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Graph neural networks: A review of methods and applications</article-title>
        <source>AI Open</source>
        <year>2020</year>
        <volume>1</volume>
        <fpage>57</fpage>
        <lpage>81</lpage>
        <pub-id pub-id-type="doi">10.1016/j.aiopen.2021.01.001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Analyzing learned molecular representations for property prediction</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2019</year>
        <volume>59</volume>
        <fpage>3370</fpage>
        <lpage>3388</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jcim.9b00237</pub-id>
        <?supplied-pmid 31361484?>
        <pub-id pub-id-type="pmid">31361484</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rives</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>
        <source>PNAS</source>
        <year>2021</year>
        <volume>118</volume>
        <fpage>e2016239118</fpage>
        <pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id>
        <?supplied-pmid 33876751?>
        <pub-id pub-id-type="pmid">33876751</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alley</surname>
            <given-names>EC</given-names>
          </name>
          <name>
            <surname>Khimulya</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Biswas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>AlQuraishi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Church</surname>
            <given-names>GM</given-names>
          </name>
        </person-group>
        <article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title>
        <source>Nat. Methods.</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>1315</fpage>
        <lpage>1322</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0598-1</pub-id>
        <?supplied-pmid 31636460?>
        <pub-id pub-id-type="pmid">31636460</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep dive into machine learning models for protein engineering</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2020</year>
        <volume>60</volume>
        <fpage>2773</fpage>
        <lpage>2790</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jcim.0c00073</pub-id>
        <?supplied-pmid 32250622?>
        <pub-id pub-id-type="pmid">32250622</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. <italic>Preprint at</italic>10.48550/arXiv.1810.04805 (2018).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kearnes</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>McCloskey</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Berndl</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pande</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Riley</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Molecular graph convolutions: moving beyond fingerprints</article-title>
        <source>J. Comput. -Aided Mol. Des.</source>
        <year>2016</year>
        <volume>30</volume>
        <fpage>595</fpage>
        <lpage>608</lpage>
        <pub-id pub-id-type="doi">10.1007/s10822-016-9938-8</pub-id>
        <?supplied-pmid 27558503?>
        <pub-id pub-id-type="pmid">27558503</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Duvenaud, D. K. et al. Convolutional networks on graphs for learning molecular fingerprints. In <italic>Advances in Neural Information Processing Systems</italic>, 2224–2232 (2015).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Graph neural networks: A review of methods and applications</article-title>
        <source>AI Open</source>
        <year>2020</year>
        <volume>1</volume>
        <fpage>57</fpage>
        <lpage>81</lpage>
        <pub-id pub-id-type="doi">10.1016/j.aiopen.2021.01.001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dimmer</surname>
            <given-names>EC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The uniprot-go annotation database in 2011</article-title>
        <source>Nucl. Acids Res.</source>
        <year>2012</year>
        <volume>40</volume>
        <fpage>D565</fpage>
        <lpage>D570</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkr1048</pub-id>
        <?supplied-pmid 22123736?>
        <pub-id pub-id-type="pmid">22123736</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bekker</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Learning from positive and unlabeled data: A survey</article-title>
        <source>Mach. Learn.</source>
        <year>2020</year>
        <volume>109</volume>
        <fpage>719</fpage>
        <lpage>760</lpage>
        <pub-id pub-id-type="doi">10.1007/s10994-020-05877-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tian</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Skolnick</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>How well is enzyme function conserved as a function of pairwise sequence identity?</article-title>
        <source>J. Mol. Biol.</source>
        <year>2003</year>
        <volume>333</volume>
        <fpage>863</fpage>
        <lpage>882</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmb.2003.08.057</pub-id>
        <?supplied-pmid 14568541?>
        <pub-id pub-id-type="pmid">14568541</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>AlQuraishi</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Proteinnet: a standardized data set for machine learning of protein structure</article-title>
        <source>BMC Bioinforma.</source>
        <year>2019</year>
        <volume>20</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-2932-0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Hu, W. et al. Strategies for pre-training graph neural networks. <italic>Preprint at</italic>10.48550/arXiv.1905.12265 (2019).</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Capela, F., Nouchi, V., Van Deursen, R., Tetko, I. V. &amp; Godin, G. Multitask learning on graph neural networks applied to molecular property predictions. <italic>Preprint at</italic>10.48550/arXiv.1910.13124 (2019).</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Vaswani, A. et al. Attention is all you need. In <italic>Advances in neural information processing systems</italic>, 5998–6008 (2017).</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Suzek</surname>
            <given-names>BE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>31</volume>
        <fpage>926</fpage>
        <lpage>932</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu739</pub-id>
        <?supplied-pmid 25398609?>
        <pub-id pub-id-type="pmid">25398609</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">Elnaggar, A. et al. Prottrans: Towards cracking the language of lifes code through self-supervised deep learning and high performance computing. <italic>IEEE Trans. Pattern Anal. Mach. Intell</italic>. <bold>PP</bold>10.1109/TPAMI.2021.3095381 (2021).</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. In <italic>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic>, 785–794 (2016).</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Hsu, C., Nisonoff, H., Fannjiang, C. &amp; Listgarten, J. Learning protein fitness models from evolutionary and assay-labeled data. <italic>Nat. Biotechnol</italic>. <bold>40</bold>, 1–9 (2022).</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chicco</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Jurman</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>The advantages of the matthews correlation coefficient (mcc) over f1 score and accuracy in binary classification evaluation</article-title>
        <source>BMC Genomics.</source>
        <year>2020</year>
        <volume>21</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1186/s12864-019-6413-7</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chang</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>BRENDA, the ELIXIR core data resource in 2021: new developments and updates</article-title>
        <source>Nucl. Acids Res.</source>
        <year>2021</year>
        <volume>49</volume>
        <fpage>D498</fpage>
        <lpage>D508</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkaa1025</pub-id>
        <?supplied-pmid 33211880?>
        <pub-id pub-id-type="pmid">33211880</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wittig</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Rey</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Weidemann</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kania</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Sabio-rk: an updated resource for manually curated biochemical reaction kinetics</article-title>
        <source>Nucl. Acids Res.</source>
        <year>2018</year>
        <volume>46</volume>
        <fpage>D656</fpage>
        <lpage>D660</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkx1065</pub-id>
        <?supplied-pmid 29092055?>
        <pub-id pub-id-type="pmid">29092055</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jumper</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Highly accurate protein structure prediction with alphafold</article-title>
        <source>Nature</source>
        <year>2021</year>
        <volume>596</volume>
        <fpage>583</fpage>
        <lpage>589</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id>
        <?supplied-pmid 34265844?>
        <pub-id pub-id-type="pmid">34265844</pub-id>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Baek</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Accurate prediction of protein structures and interactions using a three-track neural network</article-title>
        <source>Science</source>
        <year>2021</year>
        <volume>373</volume>
        <fpage>871</fpage>
        <lpage>876</lpage>
        <pub-id pub-id-type="doi">10.1126/science.abj8754</pub-id>
        <?supplied-pmid 34282049?>
        <pub-id pub-id-type="pmid">34282049</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Van Rossum</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Drake</surname>
            <given-names>FL</given-names>
          </name>
        </person-group>
        <source>Python 3 Reference Manual</source>
        <year>2009</year>
        <publisher-loc>Scotts Valley</publisher-loc>
        <publisher-name>CreateSpace</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paszke</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>
        <source>Adv. Neur.</source>
        <year>2019</year>
        <volume>32</volume>
        <fpage>8026</fpage>
        <lpage>8037</lpage>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <mixed-citation publication-type="other">The gene ontology resource: enriching a gold mine. <italic>Nucl. Acids Res</italic>. <bold>49</bold>, D325–D334 (2021).</mixed-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <mixed-citation publication-type="other">Bansal, P. et al. Rhea, the reaction knowledgebase in 2022. <italic>Nucl. Acids Res</italic>. <bold>50</bold>, D693–D700 (2021).</mixed-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kanehisa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Goto</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Kegg: kyoto encyclopedia of genes and genomes</article-title>
        <source>Nucl. Acids Res.</source>
        <year>2000</year>
        <volume>28</volume>
        <fpage>27</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/28.1.27</pub-id>
        <?supplied-pmid 10592173?>
        <pub-id pub-id-type="pmid">10592173</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Pubchem in 2021: new data content and improved web interfaces</article-title>
        <source>Nucl. Acids Res.</source>
        <year>2021</year>
        <volume>49</volume>
        <fpage>D1388</fpage>
        <lpage>D1395</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkaa971</pub-id>
        <?supplied-pmid 33151290?>
        <pub-id pub-id-type="pmid">33151290</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <mixed-citation publication-type="other">Landrum, G. et al. RDKit: Open-source cheminformatics. <ext-link ext-link-type="uri" xlink:href="http://www.rdkit.org">http://www.rdkit.org</ext-link> (2006).</mixed-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Cd-hit: accelerated for clustering the next-generation sequencing data</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <fpage>3150</fpage>
        <lpage>3152</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts565</pub-id>
        <?supplied-pmid 23060610?>
        <pub-id pub-id-type="pmid">23060610</pub-id>
      </element-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Caspi</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The metacyc database of metabolic pathways and enzymes-a 2019 update</article-title>
        <source>Nucl. Acids Res.</source>
        <year>2020</year>
        <volume>48</volume>
        <fpage>D445</fpage>
        <lpage>D453</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkz862</pub-id>
        <?supplied-pmid 31586394?>
        <pub-id pub-id-type="pmid">31586394</pub-id>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <mixed-citation publication-type="other">Jolliffe, I. Principal component analysis. <italic>Encyclopedia of Statistics in Behavioral Science</italic> (2005).</mixed-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <mixed-citation publication-type="other">Bahdanau, D., Cho, K. &amp; Bengio, Y. Neural machine translation by jointly learning to align and translate. <italic>Preprint at</italic>10.48550/arXiv.1409.0473 (2014).</mixed-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <mixed-citation publication-type="other">Bergstra, J., Yamins, D. &amp; Cox, D. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In <italic>International conference on machine learning</italic>, 115-123 (PMLR, 2013).</mixed-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>J. Mach. Learn. Res.</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="CR70">
      <label>70.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dietterich</surname>
            <given-names>TG</given-names>
          </name>
        </person-group>
        <article-title>Approximate statistical tests for comparing supervised classification learning algorithms</article-title>
        <source>Neural Comput.</source>
        <year>1998</year>
        <volume>10</volume>
        <fpage>1895</fpage>
        <lpage>1923</lpage>
        <pub-id pub-id-type="doi">10.1162/089976698300017197</pub-id>
        <?supplied-pmid 9744903?>
        <pub-id pub-id-type="pmid">9744903</pub-id>
      </element-citation>
    </ref>
    <ref id="CR71">
      <label>71.</label>
      <mixed-citation publication-type="other">Seabold, S. &amp; Perktold, J. Statsmodels: Econometric and statistical modeling with python. In <italic>Proceedings of the 9th Python in Science Conference</italic>, vol. 57, 61 (Austin, TX, 2010).</mixed-citation>
    </ref>
    <ref id="CR72">
      <label>72.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Virtanen</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scipy 1.0: fundamental algorithms for scientific computing in python</article-title>
        <source>Nat. Methods.</source>
        <year>2020</year>
        <volume>17</volume>
        <fpage>261</fpage>
        <lpage>272</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>
        <?supplied-pmid 32015543?>
        <pub-id pub-id-type="pmid">32015543</pub-id>
      </element-citation>
    </ref>
    <ref id="CR73">
      <label>73.</label>
      <mixed-citation publication-type="other">Kroll, A., Ranjan, S., Engqvist, M. K. &amp; Lercher, M. J. A general model to predict small molecule substrates of enzymes based on machine and deep learning. <italic>GitHub</italic>10.5281/zenodo.7763142 (2023).</mixed-citation>
    </ref>
  </ref-list>
</back>
