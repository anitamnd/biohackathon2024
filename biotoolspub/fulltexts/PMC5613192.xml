<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName archivearticle.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neuroinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neuroinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5196</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5613192</article-id>
    <article-id pub-id-type="doi">10.3389/fninf.2017.00060</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Sleep: An Open-Source Python Software for Visualization, Analysis, and Staging of Sleep Data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Combrisson</surname>
          <given-names>Etienne</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn001">
          <sup>*</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn003">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/396401/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Vallat</surname>
          <given-names>Raphael</given-names>
        </name>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn002">
          <sup>*</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn003">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/413465/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Eichenlaub</surname>
          <given-names>Jean-Baptiste</given-names>
        </name>
        <xref ref-type="aff" rid="aff4">
          <sup>4</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/87227/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>O'Reilly</surname>
          <given-names>Christian</given-names>
        </name>
        <xref ref-type="aff" rid="aff5">
          <sup>5</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/120120/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lajnef</surname>
          <given-names>Tarek</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff6">
          <sup>6</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/211927/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Guillot</surname>
          <given-names>Aymeric</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/45874/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ruby</surname>
          <given-names>Perrine M.</given-names>
        </name>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn004">
          <sup>‡</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/28767/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jerbi</surname>
          <given-names>Karim</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn004">
          <sup>‡</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1676/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1">
      <sup>1</sup>
      <institution>Département de Psychologie, Université de Montréal</institution>
      <country>Montreal, QC, Canada</country>
    </aff>
    <aff id="aff2">
      <sup>2</sup>
      <institution>Inter-University Laboratory of Human Movement Biology, Université Claude Bernard Lyon 1, Université de Lyon</institution>
      <country>Lyon, France</country>
    </aff>
    <aff id="aff3">
      <sup>3</sup>
      <institution>Lyon Neuroscience Research Center, Brain Dynamics and Cognition team, INSERM UMRS 1028, CNRS UMR 5292, Université Claude Bernard Lyon 1, Université de Lyon</institution>
      <country>Lyon, France</country>
    </aff>
    <aff id="aff4">
      <sup>4</sup>
      <institution>Department of Neurology, Massachusetts General Hospital, Harvard Medical School</institution>
      <country>Boston, MA, United States</country>
    </aff>
    <aff id="aff5">
      <sup>5</sup>
      <institution>Blue Brain Project, École Polytechnique Fédérale de Lausanne</institution>
      <country>Geneva, Switzerland</country>
    </aff>
    <aff id="aff6">
      <sup>6</sup>
      <institution>Center for Advanced Research in Sleep Medicine, Hôpital du Sacré-Coeur de Montréal</institution>
      <country>Montreal, QC, Canada</country>
    </aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Pedro Antonio Valdes-Sosa, Joint China-Cuba Laboratory for Frontier Research in Translational Neurotechnology, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Nianming Zuo, Institute of Automation, Chinese Academy of Sciences, China; Vasil Kolev, Institute of Neurobiology (BAS), Bulgaria</p>
      </fn>
      <corresp id="fn001">*Correspondence: Etienne Combrisson <email xlink:type="simple">e.combrisson@gmail.com</email></corresp>
      <corresp id="fn002">Raphael Vallat <email xlink:type="simple">raphaelvallat9@gmail.com</email></corresp>
      <fn fn-type="other" id="fn003">
        <p>†These authors have contributed equally to this work.</p>
      </fn>
      <fn fn-type="other" id="fn004">
        <p>‡These senior authors have contributed equally to this work.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>21</day>
      <month>9</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2017</year>
    </pub-date>
    <volume>11</volume>
    <elocation-id>60</elocation-id>
    <history>
      <date date-type="received">
        <day>03</day>
        <month>7</month>
        <year>2017</year>
      </date>
      <date date-type="accepted">
        <day>06</day>
        <month>9</month>
        <year>2017</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2017 Combrisson, Vallat, Eichenlaub, O'Reilly, Lajnef, Guillot, Ruby and Jerbi.</copyright-statement>
      <copyright-year>2017</copyright-year>
      <copyright-holder>Combrisson, Vallat, Eichenlaub, O'Reilly, Lajnef, Guillot, Ruby and Jerbi</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>We introduce Sleep, a new Python open-source graphical user interface (GUI) dedicated to visualization, scoring and analyses of sleep data. Among its most prominent features are: (1) Dynamic display of polysomnographic data, spectrogram, hypnogram and topographic maps with several customizable parameters, (2) Implementation of several automatic detection of sleep features such as spindles, K-complexes, slow waves, and rapid eye movements (REM), (3) Implementation of practical signal processing tools such as re-referencing or filtering, and (4) Display of main descriptive statistics including publication-ready tables and figures. The software package supports loading and reading raw EEG data from standard file formats such as European Data Format, in addition to a range of commercial data formats. Most importantly, Sleep is built on top of the VisPy library, which provides GPU-based fast and high-level visualization. As a result, it is capable of efficiently handling and displaying large sleep datasets. Sleep is freely available (<ext-link ext-link-type="uri" xlink:href="http://visbrain.org/sleep">http://visbrain.org/sleep</ext-link>) and comes with sample datasets and an extensive documentation. Novel functionalities will continue to be added and open-science community efforts are expected to enhance the capacities of this module.</p>
    </abstract>
    <kwd-group>
      <kwd>polysomnography</kwd>
      <kwd>electroencephalography</kwd>
      <kwd>automatic detection</kwd>
      <kwd>graphoelements</kwd>
      <kwd>hypnogram</kwd>
      <kwd>scoring</kwd>
      <kwd>graphical user interface</kwd>
      <kwd>opengl</kwd>
    </kwd-group>
    <counts>
      <fig-count count="7"/>
      <table-count count="0"/>
      <equation-count count="2"/>
      <ref-count count="24"/>
      <page-count count="11"/>
      <word-count count="7092"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Introduction</title>
    <p>Polysomnography provides a comprehensive recording of the major physiological changes associated with sleep and is hence the gold standard for modern sleep analysis, both in research and clinical settings. At its simplest, it consists of monitoring at least 2 electroencephalogram (EEG), an electro-oculogram (EOG), and a submental electromyogram (EMG), providing sufficient information to identify sleep stages (sleep scoring) according to standard international established guidelines. A first set of rules were published by Rechtschaffen and Kales (<xref rid="B18" ref-type="bibr">1968</xref>) and proposed to divide sleep into 5 stages with distinct electrophysiological properties, named rapid-eye movement (REM) and non-REM (NREM) stages 1, 2, 3, 4. This nomenclature was updated in 2007 by the American Academy of Sleep Medicine (Iber et al., <xref rid="B11" ref-type="bibr">2007</xref>) and sleep stage 3 and 4 have been merged into stage N3. In humans, a normal night of sleep consists of a repetition of four or five cycles in which sleep stages tend to follow each other in a particular order. Sleep staging is generally done visually by inspecting consecutive polysomnographic segments of 30 s. It results in a hypnogram which represents the succession of sleep stages across time. Apart from being time-consuming, visual sleep scoring is subject to both inter and intra-rater variability and is thus far from being optimal. By contrast, automatic sleep scoring has the advantage of being fast, reproducible and with generally good agreement with visual scoring (Berthomier et al., <xref rid="B1" ref-type="bibr">2007</xref>; Lajnef et al., <xref rid="B13" ref-type="bibr">2015a</xref>), yet its usage is far from being widespread and most sleep laboratories still rely on visual scoring, using either commercial softwares or in-house packages. In many cases, these software tools come with their own data and hypnogram file formats, and this heterogeneity can represent a substantial obstacle for sharing of sleep data across laboratories or clinics. Some of the very few existing open sources graphical user interface (GUI) for reading and scoring sleep include Phypno<xref ref-type="fn" rid="fn0001"><sup>1</sup></xref>, written in Python, and the MATLAB-based toolboxes sleepSMG<xref ref-type="fn" rid="fn0002"><sup>2</sup></xref> or SpiSOP<xref ref-type="fn" rid="fn0003"><sup>3</sup></xref>.</p>
    <p>With this in mind, we developed <italic>Sleep</italic>, an intuitive and efficient open-source GUI dedicated to the visualization of polysomnographic recordings and scoring of sleep stages. <italic>Sleep</italic> supports a range of data file formats and provides several scoring aid including the detection of essential features of NREM and REM sleep such as spindles, K-complexes, slow waves, and REM. <italic>Sleep</italic> was written in Python, an easy-to-learn high-level programming language widely used in the scientific community. We developed <italic>Sleep</italic> on top of VisPy<xref ref-type="fn" rid="fn0004"><sup>4</sup></xref> (Campagnola et al., <xref rid="B3" ref-type="bibr">2015</xref>), a Python scientific library based on OpenGL which offloads graphics rendering to the graphics processing unit (GPU) in order to provide fast and high-quality visualization, even under heavy loads as is the case with large dataset. <italic>Sleep</italic> therefore benefits from the high performances provided by VisPy alongside Python's inherent qualities such as its portability and ease of use.</p>
  </sec>
  <sec sec-type="methods" id="s2">
    <title>Methods</title>
    <p>Scientific visualization often consists of finding the best possible way to explore the data and to illustrate results in an intuitive and straightforward manner. The huge variety of neuroscientific data types and acquisition modalities naturally requires a wide range of specific visualization tools. Ideally, software packages needed for the various applications should be free and capable of handling several types of brain data recordings. In this context, we are currently developing a Python package we called Visbrain<xref ref-type="fn" rid="fn0005"><sup>5</sup></xref> distributed under a BSD license, which provides and centralizes a number of useful brain data visualization utilities. Given the lack of software solutions that wrap together a portable and user-friendly interface for polysomnographic data visualization and edition, we set out to develop an open-source module (included within the Visbrain package) and named Sleep.</p>
    <sec>
      <title>The choice of python and the project vision</title>
      <p>The choice of the programming language naturally turned to Python as this high-level and open-source language benefits from many libraries, an extensive documentation and a dynamic community. From data analysis to the production of high-definition paper figures, Python offers all the tools needed by scientists, with the comfort of a clean and easy to read syntax. <italic>Sleep</italic> is a pure Python software built on top of NumPy, VisPy, PyQt4<xref ref-type="fn" rid="fn0006"><sup>6</sup></xref> and uses a limited number of functions from SciPy and Matplotlib. Thanks to the Python portability, the software can be installed and used on any platform. One of the initial objectives of the project was to provide a user-friendly and intuitive interface capable of loading and displaying large sleep dataset. To this end, we paid a particular attention to avoid deep data copy and display only what is necessary. Therefore, even very large recordings with a consequent number of channels can be handled by <italic>Sleep</italic> on any modern laptop with snappy GUI response. From a programming perspective, we did our best to provide a clean, commented and high-quality code, with a NumPy style documentation and using static analysis tool, as recommended by PEP 8. <italic>Sleep</italic> is hosted on GitHub and we encourage Python programmers and sleep scientists to collaborate in order to collectively improve this software by extending its functionalities and data compatibilities.</p>
    </sec>
    <sec>
      <title>Hardware accelerated graphics</title>
      <p>In addition to ergonomic considerations and providing a portable interface, a further important goal was to use a plotting library which would allow our <italic>Sleep</italic> module to support and process large sleep data. Using Matplotlib was an option we considered, but although it is particularly convenient to produce publication quality figures, it is not the best option when it comes to plotting and interacting in real-time. In contrast, VisPy is a scientific visualization library based on NumPy and OpenGL and was primarily designed to provide both high performances with real-time interactions and publication quality figures. VisPy provides a bridge between the intuitive Python syntax and modern shader-based OpenGL pipeline allowing the graphical rendering cost to be offloaded to the GPU. This package has been well-designed and is built on four levels, from a Matplotlib oriented one to the lowest-level (closer to OpenGL) which makes it more flexible and efficient at the cost of a potentially slower learning curve. Because all <italic>Sleep</italic> graphical elements are primitive 2D objects (line, points, and images) it was not a necessity to go down to the lowest level of VisPy (vispy.gloo). Indeed, all required objects were already implemented into the Visual library. Hence, any modern computer equipped with a GPU should see the benefits of the hardware accelerated graphics implemented in <italic>Sleep</italic>.</p>
    </sec>
    <sec>
      <title>Portable GUI through python</title>
      <p>Currently, among the major cross-platform GUI toolkits that interface with Python, wxWidgets<xref ref-type="fn" rid="fn0007"><sup>7</sup></xref> (wxPython), Tcl/Tk<xref ref-type="fn" rid="fn0008"><sup>8</sup></xref> (TkInter), and Qt<xref ref-type="fn" rid="fn0009"><sup>9</sup></xref> (PyQt/PySide) are probably the most known and used. We chose PyQt which is a python binding for the C++ Qt toolkit, and we used Qt Designer to design the GUI.</p>
      <p>Taken together, VisPy provides high-performance rendering graphics that are well-integrated in a portable, modular and responsive Qt GUI using Python PyQt package. The use of this library is therefore one of the major strengths of this open-source module, and is particularly important when it comes to handling large multi-dimensional brain data, such as full-night sleep EEG recordings.</p>
    </sec>
    <sec>
      <title>Automatic events detection</title>
      <p>One of the main objectives of <italic>Sleep</italic> was to provide a complete and easy-to-use interface for analyzing and staging sleep data. To this purpose, we implemented several algorithms for the automatic detection of sleep features, and embedded them within the software (“Detection” panels). This includes detection of spindles, K-complexes, slow waves, rapid-eye movements, muscle twitches, and signal peaks. With the exception of the latter, all these features are often used as landmarks of specific sleep stages and can be very helpful to guide experts in their identification of specific sleep stages within a period of sleep, i.e., sleep scoring or sleep staging (see Figure <xref ref-type="fig" rid="F1">1</xref>). The main characteristics of each of these features are summarized below.</p>
      <list list-type="bullet">
        <list-item>
          <p>Sleep spindles refer to burst of 12–14 Hz waves predominant over central scalp electrodes and lasting between 0.5 and 2 s (Rechtschaffen and Kales, <xref rid="B18" ref-type="bibr">1968</xref>). These bursts of oscillatory activity have been known as a defining characteristics of N2 sleep (although there is an increasing number of studies that analyze spindles in N3 stages). Several automatic spindle detection algorithms have been developed in recent years (reviews in Devuyst et al., <xref rid="B6" ref-type="bibr">2011</xref>; Warby et al., <xref rid="B24" ref-type="bibr">2014</xref>). The algorithm implemented in <italic>Sleep</italic> is based on a wavelet transform followed by amplitude threshold and duration criteria. The default algorithm parameters (duration, frequency, and power threshold) were chosen according to previously published detection methods (Devuyst et al., <xref rid="B6" ref-type="bibr">2011</xref>). The consecutive steps of the spindles automatic detection algorithm implemented in <italic>Sleep</italic> are detailed in Figure <xref ref-type="fig" rid="F2">2</xref>.</p>
        </list-item>
        <list-item>
          <p>K-complexes are defined as sharp negative waves followed by a positive component, prominent over frontal scalp electrodes and lasting more than 0.5 s. Along with spindles, they constitute one landmark of N2 sleep. Briefly, the algorithm implemented in <italic>Sleep</italic> comprises the following steps: (1) bandpass filtering of the signal in the delta frequency band (2) amplitude thresholding of the Teager-Keaser Energy Operator (Erdamar et al., <xref rid="B9" ref-type="bibr">2012</xref>; Parekh et al., <xref rid="B17" ref-type="bibr">2015</xref>) of the filtered signal (3) computation of the probability of detecting true K-complexes based on morphological criteria (duration and amplitude) and the presence of spindles in the vicinity.</p>
        </list-item>
        <list-item>
          <p>Slow-waves (or delta waves) are high-amplitude (&gt;75 μV) and low-frequency (&lt;3 Hz) oscillations that are present during the deepest NREM sleep stage, i.e., N3 sleep. According to the standard international guidelines, N3 sleep is defined by the presence of 20% or more slow waves in a given epoch. As period of N3 sleep are marked by a high delta power and low power in the other frequency bands (theta, alpha, beta), the algorithm implemented in <italic>Sleep</italic> is based on a thresholding of the delta relative band power.</p>
        </list-item>
        <list-item>
          <p>As its name suggests, REM sleep is characterized by rapid eye movements easily observable on the EOG channels. They consist of conjugate, irregular and sharply peaked eye movements, similar to some extent to those exhibited during wakefulness. The algorithm implemented for the detection of REMs is detailed elsewhere (Vallat et al., <xref rid="B23" ref-type="bibr">2017</xref>).</p>
        </list-item>
        <list-item>
          <p>Another fundamental aspect of REM sleep is its muscle atonia, as revealed by a low EMG activity. However, some transient muscle activity or muscle twitchings (MTs) can also be observed. These short irregular bursts of EMG activity are superimposed on the background of low EMG activity. The automatic detection of MTs is based on a thresholding of the Morlet's complex decomposition of the EMG signal followed by morphological criteria (duration and amplitude).</p>
        </list-item>
        <list-item>
          <p>Finally, <italic>Sleep</italic> implements a signal peak detection algorithm that is useful for example to calculate the heart rate, provided that an ECG channel is present. The algorithm implemented in <italic>Sleep</italic> searches for the highest point around which there are points lower by a certain duration on both sides<xref ref-type="fn" rid="fn0010"><sup>10</sup></xref>.</p>
        </list-item>
      </list>
      <fig id="F1" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Illustration of the different sleep features observed in a polysomnographic recording of one individual. To see examples of automatic detection actually performed by our software, see Figure <xref ref-type="fig" rid="F4">4</xref>. Spindles and K-complexes are landmarks of N2 sleep. Slow waves are present during N3 sleep (sometimes referred to as slow wave sleep). Rapid eye movements, observed in the EOG channel, and muscle twitches, observed on the EMG channel, are two essential features of rapid eye movement (REM) sleep.</p>
        </caption>
        <graphic xlink:href="fninf-11-00060-g0001"/>
      </fig>
      <fig id="F2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Method description for the automatic sleep spindles detection. First, the original signal is convoluted with a Morlet wavelet centered in the spindles frequency band [12–14 Hz]. From the resulting complex decomposition, we only keep the amplitude and find time indices where the amplitude exceeds the threshold (purple in 1). Then, we compute the normalized power in the sigma band and detect again time indices where the power exceeds a threshold (green in 2). The normalized sigma power is obtained by first computing absolute power in four frequency bands (delta = [0.5–4 Hz], theta = [4–8 Hz], alpha = [8–12 Hz], sigma = [12–16 Hz]) and then dividing each of them by the sum of these powers. As a result, for each time point the sum of powers in the four frequency bands equals 1. The time location of the initial detected spindles (gray line in 3) is the result of the intersection of exceeding both the amplitude index (purple line) and the power index (green line). Finally, time gaps are filled only for neighboring detected events (&lt;500 ms) and a final duration criteria is applied in order to suppress events with a duration inferior to 500 ms or superior to 2,000 ms (these thresholds can be set within <italic>Sleep</italic> interface, 4).</p>
        </caption>
        <graphic xlink:href="fninf-11-00060-g0002"/>
      </fig>
      <p>Altogether, the set of detectors implemented in our software offers a valuable help for scoring sleep stages through the identification of the main features of each sleep stages. Detections can also be used for a more in-depth analysis of the sleep microstructure (e.g., Vallat et al., <xref rid="B23" ref-type="bibr">2017</xref>). Comparisons of performances between our detections and visual scoring are reported for K-complexes and spindles in the Results section.</p>
    </sec>
    <sec>
      <title>Signal processing tools</title>
      <p>In addition to the automatic detections presented above, <italic>Sleep</italic> also provides a wide range of basic and advanced signal processing tools such as signal demeaning, detrending, and a filtering. The latter can be done either with Butterworth or Bessel filters and four filter designs are currently available: lowpass, highpass, bandpass, or bandstop. Importantly, further information can be extracted from the Morlet's wavelet complex decomposition (Tallon-Baudry et al., <xref rid="B22" ref-type="bibr">1996</xref>) such as time-resolved and band-specific amplitude, power or phase. Critically, each one of these signal processing tools are reversible and can therefore be activated and deactivated without altering the original data and without any memory-intensive data copy. Finally, loaded data can be re-referenced directly from the interface by either re-referencing to a selected single channel or common-average (frequently used for scalp EEG datasets) or by using bipolarization, which consists of subtracting neural activity from consecutives sites (classically used in intracranial EEG, see Jerbi et al., <xref rid="B12" ref-type="bibr">2009</xref>).</p>
    </sec>
    <sec>
      <title>Documentation and examples</title>
      <p><italic>Sleep</italic> comes with a detailed step-by-step documentation, built with Sphinx<xref ref-type="fn" rid="fn0011"><sup>11</sup></xref> and hosted on GitHub<xref ref-type="fn" rid="fn0012"><sup>12</sup></xref>. This documentation include a description of the graphical components and the main functionalities of the software. A PDF version of the documentation can also be downloaded from the “Help” contextual menu of the software. We also provide anonymized and free-to-use sample datasets, including the corresponding loading scripts. This will help users test the <italic>Sleep</italic> module and get familiar with its functionalities before trying it on their own data. Finally, we also implemented an interactive documentation using the tooltips provided by PyQt to describe each element of the interface.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>Results</title>
    <p>In the following we overview the current GUI and software functionalities of Sleep and provide details on hypnogram editing and event detection validation results.</p>
    <sec>
      <title>Graphical user interface</title>
      <p>The <italic>Sleep</italic> GUI is currently subdivided into six distinct components (Figure <xref ref-type="fig" rid="F3">3</xref>): (<italic>1</italic>) settings panel, (<italic>2</italic>) navigation bar, (<italic>3</italic>) hypnogram, (<italic>4</italic>) electrophysiological time series, (<italic>5</italic>) spectrogram canvas, (<italic>6</italic>) topographic map. As the user interface is built up in a modular way, each of these components can be hidden or displayed, depending on whether the user prefers a light or fully-featured interface. Using the contextual menu, users can save and subsequently load the current display properties in order to easily retrieve and continue working on a previous session.</p>
      <fig id="F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p><italic>Sleep</italic> main interface. Each element of the graphical user interface can either be displayed or hidden, <bold>(A)</bold> Settings panel containing all <italic>Sleep</italic> controls and parameters. The current displayed tab can be used to toggle channel visibility and to adjust individual amplitudes, <bold>(B)</bold> 30 s time window of electrophysiological data. Here, only 5 channels are currently displayed (Cz, Fz, Pz, EOG1, EMG1), <bold>(C)</bold> The spectrogram displays the time-frequency representation of a specific channel for the entire recording, and can be useful to identify global changes in the spectral properties of the signal often associated with changes in sleep stages. Any channel can be picked and further time-frequency controls are available in the settings panel, <bold>(D)</bold> Hypnogram with one specific color per stage. The stage order can be changed from the default <italic>Artefact, Wake, REM, N1, N2, N3</italic>, <bold>(E)</bold> Time axis with visual indicator, <bold>(F)</bold> Navigation bar with time settings: window length and step size, unit (seconds/minutes/hours), <bold>(G)</bold> Topographic data representation.</p>
        </caption>
        <graphic xlink:href="fninf-11-00060-g0003"/>
      </fig>
      <sec>
        <title>Settings panel and navigation bar</title>
        <p>All controls and properties are grouped in a settings panel. This panel is subdivided into five thematic tabs:
<list list-type="bullet"><list-item><p>Panels: manage the visibility and properties of each plotted canvas.</p></list-item><list-item><p>Tools: bundle of signal processing tools.</p></list-item><list-item><p>Infos: basic informations of the current recording (name, sampling rate) and sleep statistics computed using the hypnogram (sleep stage duration, latency, etc.). Note that the statistics can be exported in <sup>*</sup>.csv or <sup>*</sup>.txt file and are automatically updated when the hypnogram is edited.</p></list-item><list-item><p>Scoring: scoring table that can be used to inspect and edit where each stage starts and end. This panel represents one of the three methods available within the software to edit the hypnogram (see hypnogram edition section) and may be useful for example to score long periods of continuous and homogenous sleep by just providing the starting and ending times.</p></list-item><list-item><p>Detection: perform and manage the automatic detection of several sleep features.</p></list-item><list-item><p>Annotations: add notes or comments to specific time points within the recordings. Annotations can be saved and loaded using the File contextual menu or can be passed as an input parameter. Each annotation is then referenced in a table comprising the start and end time (in seconds) and the corresponding text. Selecting a row in the table centers the main window around it. This latter feature enables a quick access to annotated events for a faster navigation. Annotated events are also identified in the time axis as a green triangle.</p></list-item></list></p>
        <p>In addition to this setting panel, <italic>Sleep</italic> provides a navigation bar that can be used to set several temporal properties, such as the length of the current time window, time step between each window, time units and the use, if provided, of the absolute time of the current recording. This navigation bar also includes a <italic>grid toggle</italic> button that can either hide or display the grid, as well as a <italic>magnify</italic> option to enlarge short events (see Figure <xref ref-type="fig" rid="F4">4</xref>).</p>
        <fig id="F4" position="float">
          <label>Figure 4</label>
          <caption>
            <p>Example of bandpass filtering. Using the Tools panel (<bold>left</bold>), the EEG signals have been bandpass-filtered in the spindles frequency band (12–14 Hz, Butterworth filter). Using the “Enable” checkbox of the panel, this filtering operation can be disabled at any moment to retrieve the original EEG signals. Finally, by left-clicking on a specific time point in a channel or selecting the Magnify tools (<bold>bottom</bold>), users can enlarge events. This was used in this example to enlarge a sleep spindle observed on channel Pz.</p>
          </caption>
          <graphic xlink:href="fninf-11-00060-g0004"/>
        </fig>
      </sec>
      <sec>
        <title>Electrophysiological time series</title>
        <p><italic>Sleep</italic> offers a dynamic control of the displayed polysomnographic time series and most of the settings are in the “Panels” tab. Indeed, each channel can be added or removed from the list of the currently displayed canvas. By default, <italic>Sleep</italic> displays the time series by frames of 30 s, which is a standard duration for stage scoring (Iber et al., <xref rid="B11" ref-type="bibr">2007</xref>), but this value can be changed directly from the navigation bar. Furthermore, the amplitude of each channel can either be set independently, using a same range across all channels, or automatically adjusted according to the minimum/maximum of the currently displayed signals.</p>
      </sec>
      <sec>
        <title>Time-frequency representation</title>
        <p>The visibility and amplitude of each channel can be controlled from the GUI (see Figure <xref ref-type="fig" rid="F3">3</xref>). The same applies for the spectrogram, which corresponds to a time-frequency representation of the entire recording performed on one channel. Among the definable parameters of the spectrogram are the channel on which it is computed, lower and upper limit frequencies, length and overlap of the fast Fourier transform and colormap properties. Finally, a topographic map based on the Source Connectivity Toolbox (SCoT) and the Magnetoencephalography and Electroencephalography (MNE) toolbox implementations (Gramfort et al., <xref rid="B10" ref-type="bibr">2013</xref>; Billinger et al., <xref rid="B2" ref-type="bibr">2014</xref>) can also be embedded inside the GUI for full data inspection. The topological plot depicts the mean values computed from the time window currently displayed. This channel-space 2D topographical functionality provides a convenient and versatile tool to visualize various data types, including the raw data, the amplitude or power in specific frequency bands.</p>
      </sec>
      <sec>
        <title>Shortcuts</title>
        <p>Navigation and operations inside a software can be sometimes repetitive. For that reason, <italic>Sleep</italic> comes with numerous native shortcuts to facilitate the visualization and stage scoring. For a complete list we refer the reader to the “Shortcuts” paragraph of the documentation<xref ref-type="fn" rid="fn0013"><sup>13</sup></xref>.</p>
      </sec>
    </sec>
    <sec>
      <title>Supported electrophysiological and hypnogram data formats</title>
      <p><italic>Sleep</italic> natively supports several standard electrophysiological file formats, including European Data Format (EDF <sup>*</sup>.edf), Micromed (<sup>*</sup>.trc), Brain Vision (<sup>*</sup>.eeg), and Elan (<sup>*</sup>.eeg). In addition, it is possible to load directly NumPy array or Matlab file using the command-line parameters.</p>
      <p>The hypnogram of the corresponding dataset can also be loaded and then edited directly from the GUI. Accepted hypnogram file formats are <sup>*</sup>.txt, <sup>*</sup>.csv, or <sup>*</sup>.hyp. There is a great heterogeneity among sleep laboratories with respect to hypnogram format. This represents an obvious barrier for data sharing. To overcome this problem, <italic>Sleep</italic> allows the user to specify the hypnogram format in a separate text file. This file should contain the names and integer values assigned to each sleep stage in the hypnogram file, as well as the number of values per second. During loading, the hypnogram file will be converted to <italic>Sleep</italic> native hypnogram format described in the documentation<xref ref-type="fn" rid="fn0014"><sup>14</sup></xref>. An example description file can be found in the documentation<sup>14</sup>.</p>
    </sec>
    <sec>
      <title>Editing the hypnogram</title>
      <p>The hypnogram can be edited either from scratch or from an existing hypnogram file. There are three methods to edit the hypnogram using <italic>Sleep</italic> GUI:
<list list-type="bullet"><list-item><p>Using intuitive keyboard shortcuts. When a new stage is entered, the next window is shown.</p></list-item><list-item><p>Using a table where each stage can be specified by it starting and ending time point.</p></list-item><list-item><p>Using a drag and drop operation directly on the hypnogram canvas.</p></list-item></list></p>
      <p>At any moment, the user can export the hypnogram or save it as a black and white (or color) publication-ready figure using the contextual menu (Figure <xref ref-type="fig" rid="F5">5</xref>).</p>
      <fig id="F5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Example of publication-ready hypnogram figure exported using <italic>Sleep</italic> GUI.</p>
        </caption>
        <graphic xlink:href="fninf-11-00060-g0005"/>
      </fig>
    </sec>
    <sec>
      <title>GUI integration and validation of automatic events detection</title>
      <p>The automatic events detection can be performed on any selected or visible channel. When the detection is completed, detected events are depicted directly on the selected channel using a specific color-code for each feature. In addition, the starting point, duration and stage of occurrence of each one of the detected events are reported in the “Location table.” Users can then easily navigate between the detected events by clicking on a row, which automatically sets the time so that the event is centered on the screen. Furthermore, this table can be exported to a <sup>*</sup>.csv or <sup>*</sup>.txt file. Users can perform an unlimited number of detections in a row on a single channel and then switch from one to another using the “Location” panel. Last but not least, the location of each detected event is reported on the hypnogram using specific visual cues for each detection types. Integration of the detection inside the GUI is shown in Figure <xref ref-type="fig" rid="F6">6</xref>.</p>
      <fig id="F6" position="float">
        <label>Figure 6</label>
        <caption>
          <p>GUI integration of the automatic event detection. The top row illustrate examples of typical graphoelements detected with <italic>Sleep</italic> including spindles, K-complexes, rapid eye movements, slow-waves, muscle twitches, and peaks. The window below illustrate how detections of such events are visually integrated into the interface. First, each detected event are highlighted into the channel time-series. Then, all the detected events are displayed on top of the hypnogram (identified using different symbols and colors per detection type) and reported into a table embedded into the settings panel. A mouse click on a line centers the corresponding event on the screen. This table can be exported into a <sup>*</sup>.csv or a <sup>*</sup>.txt file.</p>
        </caption>
        <graphic xlink:href="fninf-11-00060-g0006"/>
      </fig>
      <p>To test how these detections performed on real datasets, we measured performances of the spindle and K-complex detection methods using visually-annotated EEG segments of N2 sleep collected from full-night polysomnographic recordings of 14 participants (Eichenlaub et al., <xref rid="B8" ref-type="bibr">2012</xref>, <xref rid="B7" ref-type="bibr">2014</xref>; Ruby P. et al., <xref rid="B19" ref-type="bibr">2013</xref>; Ruby P. M. et al., <xref rid="B20" ref-type="bibr">2013</xref>). Spindles and K-complexes were visually scored by an expert (JBE) as part of a previous work that focused specifically on the detection of these sleep features using machine-learning (Lajnef et al., <xref rid="B13" ref-type="bibr">2015a</xref>).</p>
      <p>To perform the detection methods using <italic>Sleep</italic> algorithm, all N2-sleep EEG segments were concatenated into a single file of 210 min with a single channel (C3) and with a sampling rate of 100 Hz (native downsampling frequency of <italic>Sleep</italic>). Then, to evaluate the performances of our detection, we used two standards metrics: the sensitivity <italic>(1)</italic>, which measures the proportion of correctly identified detected events and the False Detection Rate (FDR) <italic>(2)</italic> which assess the proportion of incorrectly detected events.</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M1">
          <mml:mi>S</mml:mi>
          <mml:mi>e</mml:mi>
          <mml:mi>n</mml:mi>
          <mml:mi>s</mml:mi>
          <mml:mi>i</mml:mi>
          <mml:mi>t</mml:mi>
          <mml:mi>i</mml:mi>
          <mml:mi>v</mml:mi>
          <mml:mi>i</mml:mi>
          <mml:mi>t</mml:mi>
          <mml:mi>y</mml:mi>
          <mml:mtext> </mml:mtext>
          <mml:mo>=</mml:mo>
          <mml:mtext> </mml:mtext>
          <mml:mfrac>
            <mml:mrow>
              <mml:mi>T</mml:mi>
              <mml:mi>r</mml:mi>
              <mml:mi>u</mml:mi>
              <mml:mi>e</mml:mi>
              <mml:mtext> </mml:mtext>
              <mml:mi>P</mml:mi>
              <mml:mi>o</mml:mi>
              <mml:mi>s</mml:mi>
              <mml:mi>i</mml:mi>
              <mml:mi>t</mml:mi>
              <mml:mi>i</mml:mi>
              <mml:mi>v</mml:mi>
              <mml:mi>e</mml:mi>
            </mml:mrow>
            <mml:mrow>
              <mml:mi>T</mml:mi>
              <mml:mi>r</mml:mi>
              <mml:mi>u</mml:mi>
              <mml:mi>e</mml:mi>
              <mml:mtext> </mml:mtext>
              <mml:mi>P</mml:mi>
              <mml:mi>o</mml:mi>
              <mml:mi>s</mml:mi>
              <mml:mi>i</mml:mi>
              <mml:mi>t</mml:mi>
              <mml:mi>i</mml:mi>
              <mml:mi>v</mml:mi>
              <mml:mi>e</mml:mi>
              <mml:mtext> </mml:mtext>
              <mml:mo>+</mml:mo>
              <mml:mtext> </mml:mtext>
              <mml:mi>F</mml:mi>
              <mml:mi>a</mml:mi>
              <mml:mi>l</mml:mi>
              <mml:mi>s</mml:mi>
              <mml:mi>e</mml:mi>
              <mml:mtext> </mml:mtext>
              <mml:mi>N</mml:mi>
              <mml:mi>e</mml:mi>
              <mml:mi>g</mml:mi>
              <mml:mi>a</mml:mi>
              <mml:mi>t</mml:mi>
              <mml:mi>i</mml:mi>
              <mml:mi>v</mml:mi>
              <mml:mi>e</mml:mi>
            </mml:mrow>
          </mml:mfrac>
        </mml:math>
      </disp-formula>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M2">
          <mml:mi>F</mml:mi>
          <mml:mi>a</mml:mi>
          <mml:mi>l</mml:mi>
          <mml:mi>s</mml:mi>
          <mml:mi>e</mml:mi>
          <mml:mtext> </mml:mtext>
          <mml:mi>D</mml:mi>
          <mml:mi>e</mml:mi>
          <mml:mi>t</mml:mi>
          <mml:mi>e</mml:mi>
          <mml:mi>c</mml:mi>
          <mml:mi>t</mml:mi>
          <mml:mi>i</mml:mi>
          <mml:mi>o</mml:mi>
          <mml:mi>n</mml:mi>
          <mml:mtext> </mml:mtext>
          <mml:mi>R</mml:mi>
          <mml:mi>a</mml:mi>
          <mml:mi>t</mml:mi>
          <mml:mi>e</mml:mi>
          <mml:mtext> </mml:mtext>
          <mml:mo>=</mml:mo>
          <mml:mtext> </mml:mtext>
          <mml:mfrac>
            <mml:mrow>
              <mml:mi>F</mml:mi>
              <mml:mi>a</mml:mi>
              <mml:mi>l</mml:mi>
              <mml:mi>s</mml:mi>
              <mml:mi>e</mml:mi>
              <mml:mtext> </mml:mtext>
              <mml:mi>P</mml:mi>
              <mml:mi>o</mml:mi>
              <mml:mi>s</mml:mi>
              <mml:mi>i</mml:mi>
              <mml:mi>t</mml:mi>
              <mml:mi>i</mml:mi>
              <mml:mi>v</mml:mi>
              <mml:mi>e</mml:mi>
            </mml:mrow>
            <mml:mrow>
              <mml:mi>F</mml:mi>
              <mml:mi>a</mml:mi>
              <mml:mi>l</mml:mi>
              <mml:mi>s</mml:mi>
              <mml:mi>e</mml:mi>
              <mml:mtext> </mml:mtext>
              <mml:mi>P</mml:mi>
              <mml:mi>o</mml:mi>
              <mml:mi>s</mml:mi>
              <mml:mi>i</mml:mi>
              <mml:mi>t</mml:mi>
              <mml:mi>i</mml:mi>
              <mml:mi>v</mml:mi>
              <mml:mi>e</mml:mi>
              <mml:mtext> </mml:mtext>
              <mml:mo>+</mml:mo>
              <mml:mtext> </mml:mtext>
              <mml:mi>T</mml:mi>
              <mml:mi>r</mml:mi>
              <mml:mi>u</mml:mi>
              <mml:mi>e</mml:mi>
              <mml:mtext> </mml:mtext>
              <mml:mi>P</mml:mi>
              <mml:mi>o</mml:mi>
              <mml:mi>s</mml:mi>
              <mml:mi>i</mml:mi>
              <mml:mi>t</mml:mi>
              <mml:mi>i</mml:mi>
              <mml:mi>v</mml:mi>
              <mml:mi>e</mml:mi>
            </mml:mrow>
          </mml:mfrac>
        </mml:math>
      </disp-formula>
      <p>where True Positive refers to the events scored by the expert and correctly detected by our methods, False Negative refers to the events scored by the expert but not detected by our method and False Positive refers to the events detected by our methods but not scored by the expert.</p>
      <p>Performances of the detection algorithm implemented in <italic>Sleep</italic> are reported in Figure <xref ref-type="fig" rid="F7">7</xref>. For both spindles and K-complexes, we used 25 different thresholds ranging from 0 to 5 with 0.2 steps. The optimal threshold was defined as the one that maximizes the difference between sensitivity and FDR (Lajnef et al., <xref rid="B13" ref-type="bibr">2015a</xref>). Regarding spindles, the best performance of our algorithm was obtained at a threshold of 2.4 standard deviations, yielding a sensitivity of 77.2% and a FDR of 40.1%. Regarding K-complexes, a threshold of 1.0 resulted in the best performances with a sensitivity of 70.7% and a FDR of 27.2%. These results are similar to those of previous detection methods (Devuyst et al., <xref rid="B6" ref-type="bibr">2011</xref>; Lajnef et al., <xref rid="B13" ref-type="bibr">2015a</xref>). Moreover, the time of execution of these two algorithms are very fast.</p>
      <fig id="F7" position="float">
        <label>Figure 7</label>
        <caption>
          <p>Performance metrics of the <italic>Sleep</italic> spindle and K-complex detection methods evaluated at 25 different thresholds (range = 0–5, step = 0.2). Dark orange and blue lines depict the sensitivity and false detection rate (FDR), respectively. Light orange lines show the difference between sensitivity and FDR. Red dotted lines depict the threshold values that maximized this difference.</p>
        </caption>
        <graphic xlink:href="fninf-11-00060-g0007"/>
      </fig>
    </sec>
    <sec>
      <title><italic>Sleep</italic> class inputs and code example</title>
      <p>From a programming point of view, the high-level interface with our software is provided by the <italic>Sleep</italic> class. This class can take into account a few input arguments. Hence, loading sleep data can be assessed in three ways adapted to a range of users, from non-programmers to advanced users. As shown in the <bold>Code Snippet 1</bold>, running <italic>Sleep</italic> without further input arguments will ask the user to specify the path to a supported sleep dataset (<sup>*</sup>.eeg, <sup>*</sup>.edf, or <sup>*</sup>.trc). In addition, the user can either use an existing hypnogram or start a new one from scratch. Alternatively, instead of using the interface to select the files, they can be directly passed as input arguments (<bold>Code Snippet 2</bold>). In this example, we also demonstrate how to change the default order of the sleep stages in the hypnogram using a simple command-line option. If this option is not specified, the default display of <italic>Sleep</italic> is as follows: <italic>Art, REM, Wake, N1, N2, N3</italic>. Finally, several others file formats such as EEGLab, Neuroscan, EGI, GDF, and BDF can be loaded using MNE Python package<xref ref-type="fn" rid="fn0015"><sup>15</sup></xref>. We report in <bold>Code Snippet 3</bold> a method to pass data to <italic>Sleep</italic> after loading them using MNE python.</p>
      <p><bold>Code Snippet 1</bold> | Simplest way to launch Sleep from a Python interpreter. This will open a window asking the user to select the EEG data and corresponding hypnogram.</p>
      <preformat>
# Load the Sleep module from visbrain:
from visbrain import Sleep
# Open the default Sleep window:
Sleep().show()
</preformat>
      <p><bold>Code Snippet 2</bold> | In this example, the paths to the EEG data and hypnogram are entered as inputs arguments of the main Sleep function, resulting in the software opening directly with the dataset and hypnogram loaded. We also show how to change the default display order of the hypnogram by changing the href argument of <italic>Sleep</italic> main function. The sleep stages will be displayed in the order defined in <italic>norder</italic> variable, with N3 on top and Art on bottom.</p>
      <preformat>
# Import the Sleep module from visbrain:
from visbrain import Sleep
# Define where the data are located:
dfile = '/home/perso/myfile.eeg'
# Define where the hypogram is located: hfile = '/home/perso/hypno.hyp'
# hfile = None # Eventually, start from a fresh one
# Inverse the default sleep stage order:
norder = ['n3', 'n2', 'n1', 'rem', 'wake', 'art']
# Finally, pass both file to the class:
Sleep(file=dfile, hypno_file=hfile, href=norder).show()
</preformat>
      <p><bold>Code Snippet 3</bold> | This example shows a method to pass data to <italic>Sleep</italic> after loading them using MNE-Python package (see <ext-link ext-link-type="uri" xlink:href="http://martinos.org/mne/dev/manual/io.html">http://martinos.org/mne/dev/manual/io.html</ext-link> for a full list of the data formats supported by MNE)</p>
      <preformat>
# Import the Sleep module and MNE:
import numpy as np
from visbrain import Sleep
from mne import io, Annotations
# - Biosemi Data Format (BDF)
raw = io.read_raw_edf('mybdffile.bdf', preload=True)
# - EGI format
# raw = io.read_raw_egi('myegifile.egi', preload=True)
# - EEGLab
# raw = io.read_raw_eeglab('myeeglabfile.set', preload=True)
# Extract data, sampling frequency and channels names
data, sf, chan = raw._data, raw.info['sfreq'],
raw.info['ch_names']
# Define annotations for this file:
onset = np.array([145., 235., 1045.]) # Onset of each
event (sec)
</preformat>
      <preformat>
dur = np.array([1., 5., 2.5]) # Duration (sec)

description = np.array(['First event', # Description

'Second event',

'Third event'

])

annot = Annotations(onset, dur, description)

# Now, pass all the arguments to the Sleep module:

Sleep(data=data, sf=sf, channels=chan, annotation_file=
annot).show()
</preformat>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>Discussion</title>
    <p>This paper introduces an open-source software module called Sleep which provides a user-friendly and efficient GUI dedicated to visualization, scoring and analysis of sleep data. This proposed module is part of a larger ongoing open-source Python project by our group called <italic>Visbrain</italic> dedicated to the visualization of neuroscientific data. The design and functionalities of <italic>Sleep</italic> are specifically geared toward scientists and students involved in sleep research.</p>
    <p><italic>Sleep</italic> comes with a GUI in which we embedded high-quality plots with graphical rendering offloaded to the GPU. As a result, plotting and user interactions can be processed in real-time. The software is capable of loading several widely-used sleep data files format, such as European Data Format and BrainVision, and to stream efficiently all of the polysomnographic channels, even on an average modern laptop. On top of that, <italic>Sleep</italic> also provides the possibility to display time-frequency (spectrogram) and topographic representations of the data, with several adjustable parameters for each. Regarding sleep staging and hypnogram editing, <italic>Sleep</italic> offers intuitive manual scoring functionalities, signal processing tools and automatic detection of sleep features in order to facilitate this fastidious process. Once completed, users can export sleep statistics, or publication-ready high-quality figure of the hypnogram in one click.</p>
    <sec>
      <title>Comparison with other solutions</title>
      <p>First, it is noteworthy that the scope and functionalities of the present module differs from a previous MATLAB tool we have released, called <italic>Spinky</italic> (Lajnef et al., <xref rid="B15" ref-type="bibr">2017</xref>) and which aims specifically to provide a joint spindle and K-complex detection framework using the tunable Q-factor wavelet transform (TQWT) (Selesnick, <xref rid="B21" ref-type="bibr">2011</xref>). In addition with being written entirely in Python, <italic>Sleep</italic> allows for a wide range of functionalities, such as sleep scoring, fast raw and spectral data visualization, edition and creation of hypnogram and annotation files, and automatic detection of several sleep features. <italic>Spinky</italic> and <italic>Sleep</italic> subserve distinct purposes and are thus highly complementary. Second, there are currently only a few freewares for human sleep scoring and analysis. The Python package Phypno and MATLAB-based toolbox SpiSOP both provide a GUI for scoring sleep stages, and include several other command line features to perform automatic detections and compute sleep statistics. However, one of the advantages of <italic>Sleep</italic> in comparison with these two solutions is the dynamic integration of these features into the GUI, which we believe will allow our software to be understood and accessible by users with no or little programming knowledge. Finally, <italic>Sleep</italic> offers several advantages compared to the numerous existing commercial solutions, the most obvious one being that it is free and therefore more easily accessible to students or small sleep laboratories. Also, the fact that it is open-source allows more easily the community to contribute to its extension and development. Furthermore, special emphasis was given to ensure compatibility with several electrophysiological and hypnogram file formats and thus liberate the data from proprietary formats that are dependent upon specific software. We firmly believe that this, in addition with the possibility to save and load automatic detection or configuration files, will promote and facilitate data sharing across sleep laboratories.</p>
    </sec>
    <sec>
      <title>Performance of the automatic detections</title>
      <p>Regarding the automatic detections, <italic>Sleep</italic> includes 6 robust algorithms for detecting some of the most prominent features of each sleep stage, including spindles, K-complexes, slow waves, REM, and muscle twitches. Spindle and K-complex detection algorithms were validated on a visually scored dataset including 210 min of N2 sleep from 14 participants and resulted in performances similar to those reported in recent publications. Last but not least, these detections are implemented inside the GUI in an ergonomic and intuitive manner. We think that these detections may represent a valuable help not only in the process of staging sleep, but also for researchers that are interested in the microstructure of sleep. The automatic detection algorithms proposed in <italic>Sleep</italic> can be used as a starting point for a semi-automatic procedure where users can correct or adjust the output of the detector. Beyond saving a lot of time, this approach has generally been shown to yield reliable and robust detection (O'Reilly and Nielsen, <xref rid="B16" ref-type="bibr">2015</xref>).</p>
    </sec>
  </sec>
  <sec id="s5">
    <title>Future directions and conclusion</title>
    <p>We are considering to extend the list of the default supported files and we encourage programmers or sleep scientists interested by this project to collaborate on it. Regarding sleep analysis we are working on an automatic scoring function based on machine-learning algorithms, inline with our previous work (Combrisson and Jerbi, <xref rid="B4" ref-type="bibr">2015</xref>; Lajnef et al., <xref rid="B14" ref-type="bibr">2015b</xref>; Combrisson et al., <xref rid="B5" ref-type="bibr">2017</xref>). Finally, as different users have different needs, we are constantly improving the interface and functionalities of the software thanks to the feedback we receive.</p>
    <p>With the release of <italic>Sleep</italic>, we offer a portable and cross-platform software, installable and usable on most configuration. While there is still room for improvement, <italic>Sleep</italic> already provides a complete and intuitive interface designed by and for scientists involved in sleep research. We hope this software will be used and further developed by many like-minded students and researchers with a strong commitment to open science and to high quality open-source software.</p>
  </sec>
  <sec id="s6">
    <title>Author contributions</title>
    <p>EC and RV contributed equally in the development of this software and writing of the article. JE provided visual scoring for the validation of K-complex and spindles detection. JE, CO, TL, AG, PR, and KJ actively helped in the writing process and with software testing.</p>
    <sec>
      <title>Conflict of interest statement</title>
      <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="fn0001">
      <p>
        <sup>1</sup>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/gpiantoni/phypno">https://github.com/gpiantoni/phypno</ext-link>
      </p>
    </fn>
    <fn id="fn0002">
      <p>
        <sup>2</sup>
        <ext-link ext-link-type="uri" xlink:href="http://sleepsmg.sourceforge.net/">http://sleepsmg.sourceforge.net/</ext-link>
      </p>
    </fn>
    <fn id="fn0003">
      <p>
        <sup>3</sup>
        <ext-link ext-link-type="uri" xlink:href="http://spisop.org/">http://spisop.org/</ext-link>
      </p>
    </fn>
    <fn id="fn0004">
      <p>
        <sup>4</sup>
        <ext-link ext-link-type="uri" xlink:href="http://vispy.org/">http://vispy.org/</ext-link>
      </p>
    </fn>
    <fn id="fn0005">
      <p>
        <sup>5</sup>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/EtienneCmb/visbrain">https://github.com/EtienneCmb/visbrain</ext-link>
      </p>
    </fn>
    <fn id="fn0006">
      <p>
        <sup>6</sup>
        <ext-link ext-link-type="uri" xlink:href="https://riverbankcomputing.com/software/pyqt/intro">https://riverbankcomputing.com/software/pyqt/intro</ext-link>
      </p>
    </fn>
    <fn id="fn0007">
      <p>
        <sup>7</sup>
        <ext-link ext-link-type="uri" xlink:href="https://www.wxwidgets.org/">https://www.wxwidgets.org/</ext-link>
      </p>
    </fn>
    <fn id="fn0008">
      <p>
        <sup>8</sup>
        <ext-link ext-link-type="uri" xlink:href="http://www.tcl.tk/">http://www.tcl.tk/</ext-link>
      </p>
    </fn>
    <fn id="fn0009">
      <p>
        <sup>9</sup>
        <ext-link ext-link-type="uri" xlink:href="https://www.qt.io/">https://www.qt.io/</ext-link>
      </p>
    </fn>
    <fn id="fn0010">
      <p>
        <sup>10</sup>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/DiamondLightSource/auto_tomo_calibration-experimental/blob/master/old_code_scripts/peak_detect.py">https://github.com/DiamondLightSource/auto_tomo_calibration-experimental/blob/master/old_code_scripts/peak_detect.py</ext-link>
      </p>
    </fn>
    <fn id="fn0011">
      <p>
        <sup>11</sup>
        <ext-link ext-link-type="uri" xlink:href="http://www.sphinx-doc.org/en/stable/">http://www.sphinx-doc.org/en/stable/</ext-link>
      </p>
    </fn>
    <fn id="fn0012">
      <p>
        <sup>12</sup>
        <ext-link ext-link-type="uri" xlink:href="http://visbrain.org/sleep.html">http://visbrain.org/sleep.html</ext-link>
      </p>
    </fn>
    <fn id="fn0013">
      <p>
        <sup>13</sup>
        <ext-link ext-link-type="uri" xlink:href="http://visbrain.org/sleep.html#shortcuts">http://visbrain.org/sleep.html#shortcuts</ext-link>
      </p>
    </fn>
    <fn id="fn0014">
      <p>
        <sup>14</sup>
        <ext-link ext-link-type="uri" xlink:href="http://visbrain.org/sleep.html#hypnogram">http://visbrain.org/sleep.html#hypnogram</ext-link>
      </p>
    </fn>
    <fn id="fn0015">
      <p>
        <sup>15</sup>
        <ext-link ext-link-type="uri" xlink:href="https://martinos.org/mne/stable/manual/io.html#importing-eeg-data">https://martinos.org/mne/stable/manual/io.html#importing-eeg-data</ext-link>
      </p>
    </fn>
  </fn-group>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> EC acknowledges support through a PhD Scholarship awarded by the Ecole Doctorale Inter-Disciplinaire Sciences-Santé (EDISS), Lyon, France, and by funding via a Natural Sciences and Engineering Research Council of Canada (NSERC). RV acknowledges support through a PhD scholarship awarded by the Ecole Doctorale NsCo, Lyon, France, and by funding via the framework LABEX CORTEX (ANR-11-LABX-0042) of Université de Lyon, within the program ANR-11-IDEX-0007. KJ acknowledges funding from the Canada Research Chairs program, NSERC Discovery Grant [grant number RGPIN-2015-04854] and FRQNT New Researcher Grant [RQT00121].</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berthomier</surname><given-names>C.</given-names></name><name><surname>Drouot</surname><given-names>X.</given-names></name><name><surname>Herman-Stoïca</surname><given-names>M.</given-names></name><name><surname>Berthomier</surname><given-names>P.</given-names></name><name><surname>Prado</surname><given-names>J.</given-names></name><name><surname>Bokar-Thire</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2007</year>). <article-title>Automatic analysis of single-channel sleep EEG: validation in healthy individuals</article-title>. <source>Sleep</source><volume>30</volume>, <fpage>1587</fpage>–<lpage>1595</lpage>. <pub-id pub-id-type="doi">10.1093/sleep/30.11.1587</pub-id><?supplied-pmid 18041491?><pub-id pub-id-type="pmid">18041491</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Billinger</surname><given-names>M.</given-names></name><name><surname>Brunner</surname><given-names>C.</given-names></name><name><surname>Müller-Putz</surname><given-names>G. R.</given-names></name></person-group> (<year>2014</year>). <source>SCoT: a Python Toolbox for EEG Source Connectivity</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://pdfs.semanticscholar.org/b196/7f587fbea9ecf4cb6be3f757a8136fc60ca8.pdf">https://pdfs.semanticscholar.org/b196/7f587fbea9ecf4cb6be3f757a8136fc60ca8.pdf</ext-link><?supplied-pmid 24653694?></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campagnola</surname><given-names>L.</given-names></name><name><surname>Klein</surname><given-names>A.</given-names></name><name><surname>Larson</surname><given-names>E.</given-names></name><name><surname>Rossant</surname><given-names>C.</given-names></name><name><surname>Rougier</surname><given-names>N. P.</given-names></name></person-group> (<year>2015</year>). <article-title>VisPy: harnessing the GPU for fast, high-level visualization,</article-title> in <source>Proceedings of the 14th Python in Science Conference</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://hal.inria.fr/hal-01208191/">https://hal.inria.fr/hal-01208191/</ext-link> (Accessed May 23, 2017).</mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Combrisson</surname><given-names>E.</given-names></name><name><surname>Jerbi</surname><given-names>K.</given-names></name></person-group> (<year>2015</year>). <article-title>Exceeding chance level by chance: the caveat of theoretical chance levels in brain signal classification and statistical assessment of decoding accuracy</article-title>. <source>J. Neurosci. Methods</source>
<volume>250</volume>, <fpage>126</fpage>–<lpage>136</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2015.01.010</pub-id><?supplied-pmid 25596422?><pub-id pub-id-type="pmid">25596422</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Combrisson</surname><given-names>E.</given-names></name><name><surname>Perrone-Bertolotti</surname><given-names>M.</given-names></name><name><surname>Soto</surname><given-names>J. L.</given-names></name><name><surname>Alamian</surname><given-names>G.</given-names></name><name><surname>Kahane</surname><given-names>P.</given-names></name><name><surname>Lachaux</surname><given-names>J.-P.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>From intentions to actions: neural oscillations encode motor processes through phase, amplitude and phase-amplitude coupling</article-title>. <source>Neuroimage</source><volume>147</volume>, <fpage>473</fpage>–<lpage>487</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.11.042</pub-id><?supplied-pmid 27915117?><pub-id pub-id-type="pmid">27915117</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devuyst</surname><given-names>S.</given-names></name><name><surname>Dutoit</surname><given-names>T.</given-names></name><name><surname>Stenuit</surname><given-names>P.</given-names></name><name><surname>Kerkhofs</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>). <article-title>Automatic sleep spindles detection—overview and development of a standard proposal assessment method</article-title>. <source>Conf. Proc. IEEE Eng. Med. Biol. Soc.</source>
<volume>2011</volume>, <fpage>1713</fpage>–<lpage>1716</lpage>. <pub-id pub-id-type="doi">10.1109/IEMBS.2011.6090491</pub-id><?supplied-pmid 22254656?><pub-id pub-id-type="pmid">22254656</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenlaub</surname><given-names>J.-B.</given-names></name><name><surname>Bertrand</surname><given-names>O.</given-names></name><name><surname>Morlet</surname><given-names>D.</given-names></name><name><surname>Ruby</surname><given-names>P.</given-names></name></person-group> (<year>2014</year>). <article-title>Brain reactivity differentiates subjects with high and low dream recall frequencies during both sleep and wakefulness</article-title>. <source>Cereb. Cortex</source>
<volume>24</volume>, <fpage>1206</fpage>–<lpage>1215</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhs388</pub-id><?supplied-pmid 23283685?><pub-id pub-id-type="pmid">23283685</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenlaub</surname><given-names>J.-B.</given-names></name><name><surname>Ruby</surname><given-names>P.</given-names></name><name><surname>Morlet</surname><given-names>D.</given-names></name></person-group> (<year>2012</year>). <article-title>What is the specificity of the response to the own first-name when presented as a novel in a passive oddball paradigm? An ERP study</article-title>. <source>Brain Res.</source>
<volume>1447</volume>, <fpage>65</fpage>–<lpage>78</lpage>. <pub-id pub-id-type="doi">10.1016/j.brainres.2012.01.072</pub-id><?supplied-pmid 22361115?><pub-id pub-id-type="pmid">22361115</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erdamar</surname><given-names>A.</given-names></name><name><surname>Duman</surname><given-names>F.</given-names></name><name><surname>Yetkin</surname><given-names>S.</given-names></name></person-group> (<year>2012</year>). <article-title>A wavelet and teager energy operator based method for automatic detection of K-complex in sleep EEG</article-title>. <source>Expert Syst. Appl.</source>
<volume>39</volume>, <fpage>1284</fpage>–<lpage>1290</lpage>. <pub-id pub-id-type="doi">10.1016/j.eswa.2011.07.138</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A.</given-names></name><name><surname>Luessi</surname><given-names>M.</given-names></name><name><surname>Larson</surname><given-names>E.</given-names></name><name><surname>Engemann</surname><given-names>D. A.</given-names></name><name><surname>Strohmeier</surname><given-names>D.</given-names></name><name><surname>Brodbeck</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2013</year>). <article-title>MEG and EEG data analysis with MNE-Python</article-title>. <source>Front. Neurosci.</source><volume>7</volume>:<fpage>267</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id><?supplied-pmid 24431986?><pub-id pub-id-type="pmid">24431986</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Iber</surname><given-names>C.</given-names></name><name><surname>Ancoli-Israel</surname><given-names>S.</given-names></name><name><surname>Chesson</surname><given-names>A. L.</given-names></name><name><surname>Quan</surname><given-names>S. F.</given-names></name></person-group> (<year>2007</year>). <source>The AASM Manual for the Scoring of Sleep and Associated Events: Rules, Terminology and Technical Specifications.</source>
<publisher-loc>Westchester, IL</publisher-loc>: <publisher-name>American Academy of Sleep Medicine</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jerbi</surname><given-names>K.</given-names></name><name><surname>Ossandón</surname><given-names>T.</given-names></name><name><surname>Hamamé</surname><given-names>C. M.</given-names></name><name><surname>Senova</surname><given-names>S.</given-names></name><name><surname>Dalal</surname><given-names>S. S.</given-names></name><name><surname>Jung</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2009</year>). <article-title>Task-related gamma-band dynamics from an intracerebral perspective: review and implications for surface EEG and MEG</article-title>. <source>Hum. Brain Mapp.</source><volume>30</volume>, <fpage>1758</fpage>–<lpage>1771</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.20750</pub-id><?supplied-pmid 19343801?><pub-id pub-id-type="pmid">19343801</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lajnef</surname><given-names>T.</given-names></name><name><surname>Chaibi</surname><given-names>S.</given-names></name><name><surname>Eichenlaub</surname><given-names>J.-B.</given-names></name><name><surname>Ruby</surname><given-names>P. M.</given-names></name><name><surname>Aguera</surname><given-names>P.-E.</given-names></name><name><surname>Samet</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2015a</year>). <article-title>Sleep spindle and K-complex detection using tunable Q-factor wavelet transform and morphological component analysis</article-title>. <source>Front. Hum. Neurosci.</source><volume>9</volume>:<fpage>414</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2015.00414</pub-id><?supplied-pmid 26283943?><pub-id pub-id-type="pmid">26283943</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lajnef</surname><given-names>T.</given-names></name><name><surname>Chaibi</surname><given-names>S.</given-names></name><name><surname>Ruby</surname><given-names>P.</given-names></name><name><surname>Aguera</surname><given-names>P.-E.</given-names></name><name><surname>Eichenlaub</surname><given-names>J.-B.</given-names></name><name><surname>Samet</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2015b</year>). <article-title>Learning machines and sleeping brains: automatic sleep stage classification using decision-tree multi-class support vector machines</article-title>. <source>J. Neurosci. Methods</source><volume>250</volume>, <fpage>94</fpage>–<lpage>105</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2015.01.022</pub-id><?supplied-pmid 25629798?><pub-id pub-id-type="pmid">25629798</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lajnef</surname><given-names>T.</given-names></name><name><surname>O'Reilly</surname><given-names>C.</given-names></name><name><surname>Combrisson</surname><given-names>E.</given-names></name><name><surname>Chaibi</surname><given-names>S.</given-names></name><name><surname>Eichenlaub</surname><given-names>J.-B.</given-names></name><name><surname>Ruby</surname><given-names>P. M.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Meet spinky: an open-source spindle and K-complex detection toolbox validated on the open-access Montreal Archive of Sleep Studies (MASS)</article-title>. <source>Front. Neuroinform.</source><volume>11</volume>:<fpage>15</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2017.00015</pub-id><?supplied-pmid 28303099?><pub-id pub-id-type="pmid">28303099</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Reilly</surname><given-names>C.</given-names></name><name><surname>Nielsen</surname><given-names>T.</given-names></name></person-group> (<year>2015</year>). <article-title>Automatic sleep spindle detection: benchmarking with fine temporal resolution using open science tools</article-title>. <source>Front. Hum. Neurosci.</source>
<volume>9</volume>:<fpage>353</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2015.00353</pub-id><?supplied-pmid 26157375?><pub-id pub-id-type="pmid">26157375</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parekh</surname><given-names>A.</given-names></name><name><surname>Selesnick</surname><given-names>I. W.</given-names></name><name><surname>Rapoport</surname><given-names>D. M.</given-names></name><name><surname>Ayappa</surname><given-names>I.</given-names></name></person-group> (<year>2015</year>). <article-title>Detection of K-complexes and sleep spindles (DETOKS) using sparse optimization</article-title>. <source>J. Neurosci. Methods</source>
<volume>251</volume>, <fpage>37</fpage>–<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2015.04.006</pub-id><?supplied-pmid 25956566?><pub-id pub-id-type="pmid">25956566</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rechtschaffen</surname><given-names>A.</given-names></name><name><surname>Kales</surname><given-names>A.</given-names></name></person-group> (<year>1968</year>). <source>A Manual of Standardized Terminology, Techniques and Scoring System for Sleep Stages of Human Subjects</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>United States Government Printing Office</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruby</surname><given-names>P.</given-names></name><name><surname>Blochet</surname><given-names>C.</given-names></name><name><surname>Eichenlaub</surname><given-names>J.-B.</given-names></name><name><surname>Bertrand</surname><given-names>O.</given-names></name><name><surname>Morlet</surname><given-names>D.</given-names></name><name><surname>Bidet-Caulet</surname><given-names>A.</given-names></name></person-group> (<year>2013</year>). <article-title>Alpha reactivity to complex sounds differs during REM sleep and wakefulness</article-title>. <source>PLoS ONE</source>
<volume>8</volume>:<fpage>e79989</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0079989</pub-id><?supplied-pmid 24260331?><pub-id pub-id-type="pmid">24260331</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruby</surname><given-names>P. M.</given-names></name><name><surname>Blochet</surname><given-names>C.</given-names></name><name><surname>Eichenlaub</surname><given-names>J.-B.</given-names></name><name><surname>Bertrand</surname><given-names>O.</given-names></name><name><surname>Morlet</surname><given-names>D.</given-names></name><name><surname>Bidet-Caulet</surname><given-names>A.</given-names></name></person-group> (<year>2013</year>). <article-title>Alpha reactivity to first names differs in subjects with high and low dream recall frequency</article-title>. <source>Front. Psychol.</source>
<volume>4</volume>:<fpage>419</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2013.00419</pub-id><?supplied-pmid 23966960?><pub-id pub-id-type="pmid">23966960</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Selesnick</surname><given-names>I. W.</given-names></name></person-group> (<year>2011</year>). <article-title>Wavelet transform with tunable Q-factor</article-title>. <source>IEEE Trans. Signal Proc.</source>
<volume>59</volume>, <fpage>3560</fpage>–<lpage>3575</lpage>. <pub-id pub-id-type="doi">10.1109/TSP.2011.2143711</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tallon-Baudry</surname><given-names>C.</given-names></name><name><surname>Bertrand</surname><given-names>O.</given-names></name><name><surname>Delpuech</surname><given-names>C.</given-names></name><name><surname>Pernier</surname><given-names>J.</given-names></name></person-group> (<year>1996</year>). <article-title>Stimulus specificity of phase-locked and non-phase-locked 40 Hz visual responses in human</article-title>. <source>J. Neurosci.</source>
<volume>16</volume>, <fpage>4240</fpage>–<lpage>4249</lpage>. <?supplied-pmid 8753885?><pub-id pub-id-type="pmid">8753885</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vallat</surname><given-names>R.</given-names></name><name><surname>Lajnef</surname><given-names>T.</given-names></name><name><surname>Eichenlaub</surname><given-names>J.-B.</given-names></name><name><surname>Berthomier</surname><given-names>C.</given-names></name><name><surname>Jerbi</surname><given-names>K.</given-names></name><name><surname>Morlet</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Increased evoked potentials to arousing auditory stimuli during sleep: implication for the understanding of dream recall</article-title>. <source>Front. Hum. Neurosci.</source><volume>11</volume>:<fpage>132</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2017.00132</pub-id><?supplied-pmid 28377708?><pub-id pub-id-type="pmid">28377708</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warby</surname><given-names>S. C.</given-names></name><name><surname>Wendt</surname><given-names>S. L.</given-names></name><name><surname>Welinder</surname><given-names>P.</given-names></name><name><surname>Munk</surname><given-names>E. G. S.</given-names></name><name><surname>Carrillo</surname><given-names>O.</given-names></name><name><surname>Sorensen</surname><given-names>H. B. D.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Sleep-spindle detection: crowdsourcing and evaluating performance of experts, non-experts and automated methods</article-title>. <source>Nat. Methods</source><volume>11</volume>, <fpage>385</fpage>–<lpage>392</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.2855</pub-id><?supplied-pmid 24562424?><pub-id pub-id-type="pmid">24562424</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
