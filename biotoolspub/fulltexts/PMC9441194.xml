<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?covid-19-tdm?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Real Time Image Process</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Real Time Image Process</journal-id>
    <journal-title-group>
      <journal-title>Journal of Real-Time Image Processing</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1861-8200</issn>
    <issn pub-type="epub">1861-8219</issn>
    <publisher>
      <publisher-name>Springer Berlin Heidelberg</publisher-name>
      <publisher-loc>Berlin/Heidelberg</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9441194</article-id>
    <article-id pub-id-type="pmid">36091622</article-id>
    <article-id pub-id-type="publisher-id">1249</article-id>
    <article-id pub-id-type="doi">10.1007/s11554-022-01249-5</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Research Paper</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>FAM: focal attention module for lesion segmentation of COVID-19 CT images</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Wu</surname>
          <given-names>Xiaoxin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <bio>
          <sec id="FPar1">
            <title>Xiaoxin Wu</title>
            <p id="Par1"> received the B.E. degree from ChongQing Medical University, China, in 2013, the Doctoral degree in Zhejiang University, China, in 2019. Currently he is an attending physician in the First Affiliated Hospital, Zhejiang University School of Medicine. His current research focuses on the diagnosis and treatment of emerging infectious diseases.<graphic position="anchor" xlink:href="11554_2022_1249_Figa_HTML" id="MO1"/></p>
          </sec>
        </bio>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Zhang</surname>
          <given-names>Zhihao</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <bio>
          <sec id="FPar2">
            <title>Zhihao Zhang</title>
            <p id="Par2"> received the B.E. degree in Electronical Information Science and Technology from Nanjing XiaoZhuang University, Nanjing, China, in 2018. He is currently working toward the M.S. degree in Artificial Intelligence and Big Data from Shanghai University of Electric Power. His current research focuses on image processing, deep learning and defect detection.<graphic position="anchor" xlink:href="11554_2022_1249_Figb_HTML" id="MO2"/></p>
          </sec>
        </bio>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Guo</surname>
          <given-names>Lingling</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <bio>
          <sec id="FPar3">
            <title>Lingling Guo</title>
            <p id="Par3"> received the B.E. degree from Zhejiang University, China, in 2011, the Ph.D. degree in Chemical Engineering from Alabama University, USA, in 2015. Currently she is an assistant professor in Zhejiang University of Technology. Her current research focuses on the industrial information and automation.<graphic position="anchor" xlink:href="11554_2022_1249_Figc_HTML" id="MO3"/></p>
          </sec>
        </bio>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Hui</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <bio>
          <sec id="FPar4">
            <title>Hui Chen</title>
            <p id="Par4"> received the B.E. degree from Tianjin University of Traditional Chinese Medicine, China, in 2012. Currently he is a supervisor nurse in the First Affiliated Hospital, Zhejiang University School of Medicine. His current research focuses on the treatment of emerging infectious diseases.<graphic position="anchor" xlink:href="11554_2022_1249_Figd_HTML" id="MO4"/></p>
          </sec>
        </bio>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Luo</surname>
          <given-names>Qiaojie</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <bio>
          <sec id="FPar5">
            <title>Qiaojie Luo</title>
            <p id="Par5"> received the Ph.D. degree in Stomatology from Zhejiang University in 2015. Currently she is a dentist and science researcher in the Affiliated Stomatology Hospital of Zhejiang University. Her current research focuses on implantology and adhesive dentistry.<graphic position="anchor" xlink:href="11554_2022_1249_Fige_HTML" id="MO5"/></p>
          </sec>
        </bio>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jin</surname>
          <given-names>Bei</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <bio>
          <sec id="FPar6">
            <title>Bei Jin</title>
            <p id="Par6"> received the B.S. degree in Stomatology from Wenzhou Medical University, Zhejiang, China, in 2005. He is currently an attending physician in Taizhou Hospital of Zhejiang Province. His current research focuses on Oral and Maxillofacial Surgery.<graphic position="anchor" xlink:href="11554_2022_1249_Figf_HTML" id="MO6"/></p>
          </sec>
        </bio>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gu</surname>
          <given-names>Weiyan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <bio>
          <sec id="FPar7">
            <title>Weiyan Gu</title>
            <p id="Par7"> received the M.S. degree in Stomatology from Wenzhou Medical University, Zhejiang, China, in 2017. She is currently an attending physician in Taizhou Hospital of Zhejiang Province. Her current research focuses on Prosthodontics and Oral Implantology.<graphic position="anchor" xlink:href="11554_2022_1249_Figg_HTML" id="MO7"/></p>
          </sec>
        </bio>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Lu</surname>
          <given-names>Fangfang</given-names>
        </name>
        <address>
          <email>lufangfang@shiep.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <bio>
          <sec id="FPar8">
            <title>Fangfang Lu</title>
            <p id="Par8"> received the Ph.D. degree in control theory and control engineering from Shanghai Jiaotong University, Shanghai, China, in 2013. She is currently an assistant professor in Shanghai University of Electric Power. Her current research focuses on medical image processing, machine learning, pattern recognition and image quality assessment.<graphic position="anchor" xlink:href="11554_2022_1249_Figh_HTML" id="MO8"/></p>
          </sec>
        </bio>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1737-3420</contrib-id>
        <name>
          <surname>Chen</surname>
          <given-names>Jingjing</given-names>
        </name>
        <address>
          <email>joyjchan@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff6">6</xref>
        <bio>
          <sec id="FPar9">
            <title>Jingjing Chen</title>
            <p id="Par9"> received the Ph. D in Computer Science from Hong Kong Baptist University in 2016. He was a research fellow in Blockchain with the Sunyard System Engineering Co. Ltd. during 2017–2019, and research fellow with the School of Traditional Chinese Medicine of Hong Kong Baptist University during 2019–2020. Currently he is a research fellow with the school of Economics of Fudan University. He also serves as research fellow with Fudan-Stanford China Institute for Financial Technology and Security. His research interests include: Blockchain, e-Government and Enterprise Information System.<graphic position="anchor" xlink:href="11554_2022_1249_Figi_HTML" id="MO9"/></p>
          </sec>
        </bio>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.452661.2</institution-id><institution-id institution-id-type="ISNI">0000 0004 1803 6319</institution-id><institution>State Key Laboratory for Diagnosis and Treatment of Infectious Diseases, National Clinical Research Center for Infectious Diseases, </institution><institution>First Affiliated Hospital, Zhejiang University School of Medicine, </institution></institution-wrap>Hangzhou, Zhejiang China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.440635.0</institution-id><institution-id institution-id-type="ISNI">0000 0000 9527 0839</institution-id><institution>College of Computer Science and Technology, </institution><institution>Shanghai University of Electric Power, </institution></institution-wrap>Shanghai, China </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.469325.f</institution-id><institution-id institution-id-type="ISNI">0000 0004 1761 325X</institution-id><institution>College of Chemical Engineering, </institution><institution>Zhejiang University of Technology, </institution></institution-wrap>Hangzhou, Zhejiang China </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.13402.34</institution-id><institution-id institution-id-type="ISNI">0000 0004 1759 700X</institution-id><institution>School of Stomatology, </institution><institution>Stomatology Hospital, Zhejiang University School of Medicine, </institution></institution-wrap>Hangzhou, Zhejiang China </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.452858.6</institution-id><institution>Department of Oral and Maxillofacial Surgery, </institution><institution>Taizhou Hospital, Wenzhou Medical University, </institution></institution-wrap>Taizhou, Zhejiang China </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.13402.34</institution-id><institution-id institution-id-type="ISNI">0000 0004 1759 700X</institution-id><institution>Zhejiang University City College, </institution></institution-wrap>Hangzhou, Zhejiang China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>4</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2022</year>
    </pub-date>
    <volume>19</volume>
    <issue>6</issue>
    <fpage>1091</fpage>
    <lpage>1104</lpage>
    <history>
      <date date-type="received">
        <day>15</day>
        <month>4</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>12</day>
        <month>8</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2022, Springer Nature or its licensor holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.</copyright-statement>
      <license>
        <license-p>This article is made available via the PMC Open Access Subset for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original source. These permissions are granted for the duration of the World Health Organization (WHO) declaration of COVID-19 as a global pandemic.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par10">The novel coronavirus pneumonia (COVID-19) is the world’s most serious public health crisis, posing a serious threat to public health. In clinical practice, automatic segmentation of the lesion from computed tomography (CT) images using deep learning methods provides an promising tool for identifying and diagnosing COVID-19. To improve the accuracy of image segmentation, an attention mechanism is adopted to highlight important features. However, existing attention methods are of weak performance or negative impact to the accuracy of convolutional neural networks (CNNs) due to various reasons (e.g. low contrast of the boundary between the lesion and the surrounding, the image noise). To address this issue, we propose a novel focal attention module (FAM) for lesion segmentation of CT images. FAM contains a channel attention module and a spatial attention module. In the spatial attention module, it first generates rough spatial attention, a shape prior of the lesion region obtained from the CT image using median filtering and distance transformation. The rough spatial attention is then input into two 7 × 7 convolution layers for correction, achieving refined spatial attention on the lesion region. FAM is individually integrated with six state-of-the-art segmentation networks (e.g. UNet, DeepLabV3+, etc.), and then we validated these six combinations on the public dataset including COVID-19 CT images. The results show that FAM improve the Dice Similarity Coefficient (DSC) of CNNs by 2%, and reduced the number of false negatives (FN) and false positives (FP) up to 17.6%, which are significantly higher than that using other attention modules such as CBAM and SENet. Furthermore, FAM significantly improve the convergence speed of the model training and achieve better real-time performance. The codes are available at GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/RobotvisionLab/FAM.git">https://github.com/RobotvisionLab/FAM.git</ext-link>).</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>COVID-19</kwd>
      <kwd>Lesion segmentation</kwd>
      <kwd>Attention mechanism</kwd>
      <kwd>Machine learning</kwd>
      <kwd>Public health</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004731</institution-id>
            <institution>Natural Science Foundation of Zhejiang Province</institution>
          </institution-wrap>
        </funding-source>
        <award-id>LQ21H190004</award-id>
        <principal-award-recipient>
          <name>
            <surname>Wu</surname>
            <given-names>Xiaoxin</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>China Postdoctoral Science Foundation</institution>
        </funding-source>
        <award-id>2020T130102ZX</award-id>
        <principal-award-recipient>
          <name>
            <surname>Wu</surname>
            <given-names>Xiaoxin</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100017940</institution-id>
            <institution>Zhejiang Provincial Postdoctoral Science Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>ZJ2020031</award-id>
        <principal-award-recipient>
          <name>
            <surname>Wu</surname>
            <given-names>Xiaoxin</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>the Educational Commission of Zhejiang Province of China</institution>
        </funding-source>
        <award-id>Y202147553</award-id>
        <principal-award-recipient>
          <name>
            <surname>Chen</surname>
            <given-names>Jingjing</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Springer-Verlag GmbH Germany, part of Springer Nature 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par11">According to the report from the Center for Systems Science and Engineering (CSSE) of Johns Hopkins University, until May 25, 2022, COVID-19 has resulted in 526,824,747 infections, of which, 6,280,794 deaths. Rapid detection of the infection is essential to prompt isolation and treatment of the patients. At present, reverse transcription-polymerase chain reaction (RT-PCR) is the most widely adopted method for COVID-19 diagnosis. However, RT-PCR suffers from some drawbacks such as time consuming, false negative caused by the sampling quality [<xref ref-type="bibr" rid="CR1">1</xref>]. The chest computed tomography (CT) images captured from COVID-19 patients frequently include patchy bilateral shadows or ground-glass opacity in the lung [<xref ref-type="bibr" rid="CR2">2</xref>], hence chest CT is adopted as an dominant method for the diagnosis of COVID-19 [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]. Compared with RT-PCR, chest CT image is easy to obtain in the clinical practice, therefore it can be used for the severity classification of COVID-19 patients, for which, contouring the lesion is an essential procedure. The traditional manual contouring is tedious, time-consuming, and heavily depending on doctor’s clinical experience, therefore, there is an urgent need for an automated lesion segmentation method specially designed for COVID-19 CT images.
</p>
    <p id="Par12">Nowadays, convolution neural network (CNN), a typical deep learning method, is becoming an essential for the segmentation of COVID-19 CT image. Widely used CNNs include FCN [<xref ref-type="bibr" rid="CR4">4</xref>], SegNet [<xref ref-type="bibr" rid="CR5">5</xref>], UNet series (UNet [<xref ref-type="bibr" rid="CR6">6</xref>], UNet++ [<xref ref-type="bibr" rid="CR7">7</xref>], UNet3+ [<xref ref-type="bibr" rid="CR8">8</xref>], etc.), and DeepLab series [<xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR11">11</xref>]. These methods usually rely on a large-scale dataset with high-quality pixel-level annotation of COVID-19 lesions. The need for large-scale data collection and data labeling before the model training prevents it from wide adoption in the context of public health.</p>
    <p id="Par13">Attention mechanism is a technology widely used in the fields such as natural language processing (NLP), statistical learning, image processing, voice recognition. It stems from the particularly selective attention of human vision. Attention mechanism focuses on important information with high weights, ignores unrelated information with low weights, and continuously adjusts weights so that important information can be selected in different situations, thus making it expandable and robust. With the consideration of application contexts, attention can be grouped into spatial, channel, layer, mixed, and temporal domains. Spatial and channel domains are most widely used in the tasks of image processing. Many excellent attention modules such as SENet [<xref ref-type="bibr" rid="CR12">12</xref>], CBAM [<xref ref-type="bibr" rid="CR13">13</xref>], SKNet [<xref ref-type="bibr" rid="CR14">14</xref>] are proposed. To improve the accuracy of deep learning models for COVID-19 lesion segmentation, attention modules (or their variations) have been integrated into the state-of-the-art segmentation networks. However, existing attention modules always cannot fully utilize the characteristics of CT images. Moreover, they always disrupt the original feature distribution of the input data, resulting in low segmentation accuracy and the inefficiency of the network training convergence.</p>
    <p id="Par14">To address the abovementioned issue, we propose a novel design, in which the lesion in CT image is treated as rough spatial attention and then combined with a channel attention module to achieve a novel plug-and-play attention module, (named focal attention module (FAM)) for lesion segmentation of COVID-19 CT images. The main contributions of this study are as follows:<list list-type="order"><list-item><p id="Par15">A novel spatial attention module is proposed. It introduces the shape prior information of the lesion region to improve the feature analysis and weight redistribution of the attention module and accelerates the convergence of the network training.</p></list-item><list-item><p id="Par16">We sequentially combine the spatial attention module in the form of the residual block with the channel attention module, constructing a novel Focal Attention Module (FAM) for lesion segmentation of COVID-19 CT images.</p></list-item><list-item><p id="Par17">FAM is integrated into six state-of-the-art networks and is validated on the public COVID-19 CT image dataset.</p></list-item></list></p>
    <p id="Par18">The rest of the paper is organized as follows: Sect. <xref rid="Sec1" ref-type="sec">2</xref> describes the work related to the proposed method. Section <xref rid="Sec5" ref-type="sec">3</xref> details the design and implementation of the proposed method. The experiment and discussion is described in Sect. <xref rid="Sec13" ref-type="sec">4</xref>. Finally, we conclude the study in Section.</p>
  </sec>
  <sec id="Sec2">
    <title>Related work</title>
    <p id="Par19">In this section, we first discuss these existing deep learning-based methods for lesion segmentation of COVID-19 CT images, followed by related work on the attention mechanism, in the end we introduce the applications of shape priors in image segmentation.</p>
    <sec id="Sec3">
      <title>Lesion segmentation of COVID-19 CT images</title>
      <p id="Par20">The data annotation is usually with labor cost and time-consuming, large-scale segmentation datasets of COVID-19 lesions are rarely available. Meanwhile, training networks on a small-scale dataset suffers from the issues such as over-fitting and poor generalization performance. Existing deep learning methods are proposed to attenuate these models’ reliance on a large-scale dataset. The attention mechanism is used to enhance the capability of feature extraction of the network. For example, Fan et al. [<xref ref-type="bibr" rid="CR15">15</xref>] combined a semi-supervised learning model and FCN8s network with implicit reverse attention and explicit edge attention mechanism to achieve a novel model. It achieves a sensitivity of 72.5% and an accuracy of 96.0%. Chen et al. [<xref ref-type="bibr" rid="CR16">16</xref>] proposed a residual attention UNet and applied a soft attention mechanism to enhance the capability of feature learning of the model. The proposed model achieves a performance with a segmentation accuracy of 89%. Zhao et al. [<xref ref-type="bibr" rid="CR17">17</xref>] integrated their proposed spatial-wise and channel-wise attention modules on UNet++ [<xref ref-type="bibr" rid="CR7">7</xref>]. The Dice Similarity Coefficients (DSC) of the model is 88.99%. A number of novel loss functions and special network modules are also proposed. For example, Wang et al. [<xref ref-type="bibr" rid="CR18">18</xref>] proposed noise-robust dice loss to solve the problem of poor training results caused by low-quality labels, and the DSC of the model is 80.72%. Inspired by contrast enhancement methods and Atrous Spatial Pyramid Pooling (ASPP) [<xref ref-type="bibr" rid="CR10">10</xref>], Yan et al. [<xref ref-type="bibr" rid="CR19">19</xref>] proposed a novel Progressive Atrous Spatial Pyramid Pooling (PASPP) module to progressively aggregate information and obtain more useful contextual features, and the DSC of the model is 72.60%. Elharrouss et al. [<xref ref-type="bibr" rid="CR20">20</xref>] proposed a multi-class segmentation network based on an encoder-decoder structure, and the multi-input stream of the network allows the model to learn more features. It achieves a sensitivity of 71.1%. In addition, multi-scale features fusion [<xref ref-type="bibr" rid="CR21">21</xref>], multipoint supervised training [<xref ref-type="bibr" rid="CR22">22</xref>], and conditional generation model [<xref ref-type="bibr" rid="CR23">23</xref>] are promising for improving the segmentation accuracy of COVID-19 lesions.</p>
    </sec>
    <sec id="Sec4">
      <title>Attention mechanism</title>
      <p id="Par21">Attention is an essential and complex cognitive function in the human brain [<xref ref-type="bibr" rid="CR24">24</xref>]. With attention, people can work methodically while receiving a large amount of information through vision, hearing, touch, etc. The human brain can select small portions of interested information from these large amounts of input information to focus on, meanwhile ignoring other portions.</p>
      <p id="Par22">In the context of computer vision, attentions can be divided into soft attention and hard attention [<xref ref-type="bibr" rid="CR25">25</xref>]. For soft attention, by calculating the attention weight, all data is included in the attention range, and no filter condition for the data feature is set. Hard attention sets the filtering condition after calculating the attention weight and forms a part of the attention weight value that does not meet the condition to 0. Contrarily, soft attention is probabilistic, end-to-end differentiable, and utilizes back-propagation and forward-propagation to learn the attention weight without the posterior sampling. There are a number of studies regarding the soft attention. Inspired by translation and rotation without deformation of the pooling mechanism, Jaderberg et al. [<xref ref-type="bibr" rid="CR26">26</xref>] proposed a spatial transformation module that could learn the transformation from the network. It was widely used for Optical Character Recognition (OCR). Hu et al. [<xref ref-type="bibr" rid="CR12">12</xref>] proposed a channel attention model (SENet), but SENet cannot capture spatial contextual information. Woo et al. [<xref ref-type="bibr" rid="CR13">13</xref>] expanded the SENet and proposed an attention module (CBAM) to constrain and enhance the input feature map from the channel and spatial dimensions. But, the spatial attention module of CBAM fails to capture information at different scales and is not able to establish a long-range dependency. Inspired by the classical non-local means method [<xref ref-type="bibr" rid="CR27">27</xref>] for image processing, Wang et al. [<xref ref-type="bibr" rid="CR28">28</xref>] proposed an attention module (non-local neural networks) for capturing long-range dependencies. Fu et al. [<xref ref-type="bibr" rid="CR29">29</xref>] amalgamated the advantages of CBAM and Non-local Neural Networks to propose the DANet, an attention module widely used in semantic segmentation. Drawing on the idea of residual networks, Wang et al. [<xref ref-type="bibr" rid="CR30">30</xref>] proposed a novel solution to solve the problem of information reduction caused by stacked attention modules. A Criss-Cross Attention is proposed by Huang et al. [<xref ref-type="bibr" rid="CR31">31</xref>], to reduce the calculations of Non-local Neural Networks. Gao et al. [<xref ref-type="bibr" rid="CR32">32</xref>] proposed a Spatially Modulated Co-Attention (SMCA) mechanism to accelerate training convergence, but it suffers from the increased time of computation and inference. A particular channel attention module [<xref ref-type="bibr" rid="CR33">33</xref>] was proposed to distinguish the esophagus and surrounding tissues from esophageal cancer. However, there are limited literatures regarding the hard attention, and studies [<xref ref-type="bibr" rid="CR34">34</xref>–<xref ref-type="bibr" rid="CR36">36</xref>] argued that reinforcement learning is required for training in hard attention due to its non-differentiability.</p>
      <p id="Par23">Although there are a number of studies [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR37">37</xref>, <xref ref-type="bibr" rid="CR38">38</xref>] introducing the attention mechanism for lesion segmentation on COVID-19 CT images, the improvement in the performance and accuracy of these models is still urgently expected in academia and industry.</p>
    </sec>
    <sec id="Sec5">
      <title>Shape priors in image segmentation</title>
      <p id="Par24">Traditional segmentation methods (e.g., thresholding, watershed, and region growing) usually suffer from the lack of robustness and poor segmentation accuracy due to the noise, low contrast, and complexity of objects in medical images. Recently, the rapid development of deep learning methods promoted the adoption of deep learning-based image segmentation algorithms in medical image segmentation. Studies [<xref ref-type="bibr" rid="CR39">39</xref>–<xref ref-type="bibr" rid="CR43">43</xref>] have shown that integrating prior knowledge of objects into rigorous segmentation formulas can improve the segmentation accuracy of a specific target. The prior knowledge has been utilized in various forms, e.g., user interaction, object shape and appearance [<xref ref-type="bibr" rid="CR44">44</xref>].</p>
      <p id="Par25">The shape is one of the most important geometric attributes of anatomical objects, and shape priors can reduce the search space of the potential segmentation outputs for deep learning models [<xref ref-type="bibr" rid="CR45">45</xref>]. Ravishankar et al. [<xref ref-type="bibr" rid="CR46">46</xref>] incorporated the shape model explicitly in FCN through a novel loss function that penalizes the deviation of the predicted segmentation mask from a learned shape model. Avanti et al. [<xref ref-type="bibr" rid="CR47">47</xref>] used stacked automatic encoders to infer the target shape, then the inferred shape is incorporated into deformable models to improve the accuracy and robustness. In addition, Ngo et al. [<xref ref-type="bibr" rid="CR48">48</xref>] and Cremers et al. [<xref ref-type="bibr" rid="CR40">40</xref>] combined level set and genetic algorithms with deep learning to improve the training effect of the model on small datasets. Zhao et al. [<xref ref-type="bibr" rid="CR49">49</xref>] obtained the shape prior of the lung region through threshold segmentation to optimize the segmentation of the lung.</p>
    </sec>
  </sec>
  <sec id="Sec6">
    <title>Proposed method</title>
    <sec id="Sec7">
      <title>Design rationale of focal attention module</title>
      <p id="Par26">Given an intermediate feature map <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf{F}\in {\mathbb{R}}^{C\times H\times W}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi mathvariant="bold">F</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq1.gif"/></alternatives></inline-formula>, and an input CT image <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf{I}\in {\mathbb{R}}^{1\times H\times W}$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi mathvariant="bold">I</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq2.gif"/></alternatives></inline-formula> is defined as the input. FAM sequentially infers a 1D channel attention map <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\varvec{M}}}_{{\varvec{c}}}\in {\mathbb{R}}^{C\times 1\times 1}$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq3.gif"/></alternatives></inline-formula> and a 2D spatial attention map <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{M}}_{s}\in {\mathbb{R}}^{1\times H\times W}$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq4.gif"/></alternatives></inline-formula> as illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The attention module is formularized as:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{F}}^{\prime } = {\mathbf{M}}_{{\text{c}}} \left( {\mathbf{F}} \right) \otimes {\mathbf{F}},$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mtext>c</mml:mtext></mml:msub><mml:mfenced close=")" open="("><mml:mi mathvariant="bold">F</mml:mi></mml:mfenced><mml:mo>⊗</mml:mo><mml:mi mathvariant="bold">F</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{*{20}c} {{\mathbf{F}}^{\prime \prime } = \left( {{\mathbf{M}}_{{\text{s}}} \left( {\mathbf{I}} \right) + 1} \right) \otimes {\mathbf{F}}^{\prime } } \\ \end{array} ,$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mo>″</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mtext>s</mml:mtext></mml:msub><mml:mfenced close=")" open="("><mml:mi mathvariant="bold">I</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>⊗</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq5"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\otimes$$\end{document}</tex-math><mml:math id="M14"><mml:mo>⊗</mml:mo></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq5.gif"/></alternatives></inline-formula> denotes element-wise multiplication. During multiplication, the attention values are broadcasted accordingly: channel attention values are broadcasted along the channel dimension, and vice versa. <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{F}}^{^{\prime\prime} }$$\end{document}</tex-math><mml:math id="M16"><mml:msup><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>″</mml:mo></mml:msup></mml:msup></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq6.gif"/></alternatives></inline-formula> is the final refined output. Different from naive stacking attention modules (e.g., CBAM, SENet), the feature map refined by the spatial attention module (as depicted in Fig. <xref rid="Fig2" ref-type="fig">2</xref>) is combined as a residual branch into the feature map refined by the channel attention module due to the following analysis:<fig id="Fig1"><label>Fig. 1</label><caption><p>The overview of focal attention module, which consists of channel attention module and spatial attention module</p></caption><graphic xlink:href="11554_2022_1249_Fig1_HTML" id="MO10"/></fig><fig id="Fig2"><label>Fig. 2</label><caption><p><bold>a</bold> The channel attention module: max-pooling, average-pooling outputs, and a multi-layer perceptron; <bold>b</bold> the spatial attention module obtains a rough shape prior of the lesion region by median filtering and distance transformation</p></caption><graphic xlink:href="11554_2022_1249_Fig2_HTML" id="MO11"/></fig><list list-type="order"><list-item><p id="Par27">When the input <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf{I}$$\end{document}</tex-math><mml:math id="M18"><mml:mi mathvariant="bold">I</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq7.gif"/></alternatives></inline-formula> is a negative sample, spatial attention obtained from its lung image <inline-formula id="IEq8"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf{L}$$\end{document}</tex-math><mml:math id="M20"><mml:mi mathvariant="bold">L</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq8.gif"/></alternatives></inline-formula> by distance transformation contains less feature information. In this case, stacking will degrade the value of features in deep layers.</p></list-item><list-item><p id="Par28">Residual branch works as feature selectors, which enhance good features and suppress noise from trunk features.</p></list-item><list-item><p id="Par29">Inspired by Residual Attention Network [<xref ref-type="bibr" rid="CR38">38</xref>], attention residual learning not only keeps good properties of original features but also allows to be refined by the spatial attention module.</p></list-item></list></p>
    </sec>
    <sec id="Sec8">
      <title>Channel attention module</title>
      <p id="Par30">The channel attention module focuses on “what” is meaningful given the feature maps. To compute the channel attention efficiently, spatial information of a feature map is first aggregated by average-pooling and max-pooling operations, respectively, thus two different spatial context descriptors (i.e. <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{F}}_{\text{avg }}^{\mathrm{c}}$$\end{document}</tex-math><mml:math id="M22"><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mtext>avg</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq9.gif"/></alternatives></inline-formula> and <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{F}}_{max}^{\mathrm{c}}$$\end{document}</tex-math><mml:math id="M24"><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow><mml:mi mathvariant="normal">c</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq10.gif"/></alternatives></inline-formula>) are obtained. Both descriptors are then forwarded to a multi-layer perceptron (MLP) with one hidden layer, achieving two output feature maps. Finally, the output feature maps are merged using element-wise summation. To reduce the number of parameters, the hidden layer size is set to <inline-formula id="IEq11"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbb{R}}^{C/r\times 1\times 1}$$\end{document}</tex-math><mml:math id="M26"><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>r</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq11.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq12"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r$$\end{document}</tex-math><mml:math id="M28"><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq12.gif"/></alternatives></inline-formula> is the reduction ratio. The channel attention is formularized as:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\mathbf{M}}_{c} \left( {\mathbf{F}} \right) &amp; = \sigma \left( {{\text{MLP}}\left( {{\text{AvgPool}}\left( {\mathbf{F}} \right)} \right) + {\text{MLP}}\left( {{\mathbf{MaxPool}}\left( {\mathbf{F}} \right)} \right)} \right) \\ &amp; = \sigma \left( {{\varvec{W}}_{1} \left( {{\varvec{W}}_{0} \left( {{\mathbf{F}}_{{\text{avg }}}^{c} } \right)} \right) + {\varvec{W}}_{1} \left( {{\varvec{W}}_{0} \left( {{\mathbf{F}}_{{\text{avg }}}^{c} } \right)} \right)} \right), \\ \end{aligned}$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:mi mathvariant="bold">F</mml:mi></mml:mfenced></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mtext>MLP</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:mtext>AvgPool</mml:mtext><mml:mfenced close=")" open="("><mml:mi mathvariant="bold">F</mml:mi></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mtext>MLP</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="bold">MaxPool</mml:mi><mml:mfenced close=")" open="("><mml:mi mathvariant="bold">F</mml:mi></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mfenced close=")" open="("><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mrow><mml:mtext>avg</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mfenced close=")" open="("><mml:msubsup><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mrow><mml:mtext>avg</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:mrow><mml:mi>c</mml:mi></mml:msubsup></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq13"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M32"><mml:mi>σ</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq13.gif"/></alternatives></inline-formula> represents the sigmoid function, <inline-formula id="IEq14"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{W}}_{0}\in {\mathbb{R}}^{C/r\times C}$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>r</mml:mi><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq14.gif"/></alternatives></inline-formula> and <inline-formula id="IEq15"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{W}}_{1}\in {\mathbb{R}}^{C\times C/r}$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>×</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq15.gif"/></alternatives></inline-formula>. <inline-formula id="IEq16"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{W}}_{0}$$\end{document}</tex-math><mml:math id="M38"><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq16.gif"/></alternatives></inline-formula> and <inline-formula id="IEq17"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{W}}_{1}$$\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq17.gif"/></alternatives></inline-formula> are the weights of the MLP and are shared for both inputs. The ReLU activation function is followed by <inline-formula id="IEq18"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{W}}_{0}$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq18.gif"/></alternatives></inline-formula>.</p>
    </sec>
    <sec id="Sec9">
      <title>Spatial attention module</title>
      <p id="Par31">The spatial attention focuses on “where’” is the interested in given feature maps. The spatial attention in CBAM is learned from a 7 × 7 convolution layer, and it has two shortcomings when dealing with the specific task of lesion segmentation for COVID-19 diagnosis: (1) low efficiency on learning process; (2) the change of spatial attention to the feature space easily cause the problematic convergence of training and poor network generalization performance, especially when the dataset is small or the parameters of the backbone network are few. Two mechanisms are introduced to address these issues: (1) adopting the residual structure and refined the feature maps with spatial attention while preserving trunk network features (as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>); (2) utilizing the shape prior of the COVID-19 lesion region to reduce the search space of the spatial attention module. The main steps (as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b) while computing the spatial attention include lung segmentation, median filtering, and distance transformation.</p>
      <sec id="Sec10">
        <title>Lung segmentation</title>
        <p id="Par32">To efficiently obtain the shape prior of the lesion region, the lung needs to be segmented from CT images. Currently, many excellent methods of lung segmentation have been proposed and widely used. These methods are mainly divided into three types: traditional image processing-based algorithms, deep learning-based algorithms and the combination of the two former methods. Because segmentation of the lung is not the focus of this paper, the lung region is segmented with a simple mask operation from labels in the dataset.</p>
      </sec>
      <sec id="Sec11">
        <title>Median filtering</title>
        <p id="Par33">Median filtering is introduced to eliminate partial noise pixels consisting of the pulmonary trachea and pulmonary vessels from the lung image. Median filtering, a nonlinear method that can preserve the details of the edges of an image while eliminating noise, has been widely used in fields such as image enhancement and image recovery. As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a, a few noise pixels in the lung region, such as regions of the tiny pulmonary trachea and pulmonary vessels, interfere significantly with the accurate segmentation of lesions. As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>b, the median filtering eliminates most of the small pulmonary trachea and pulmonary vessels. For the large pulmonary trachea and pulmonary vessels, the median filtering also reduces their pixel region. Meanwhile, median filtering retains the nature of the lesions (i.e. ground-glass opacity) with little reduction in the area of the lesions due to its large pixel region.<fig id="Fig3"><label>Fig. 3</label><caption><p>The process of eliminating noise pixels in the lung region of CT image step by step. <bold>a</bold> A lot of noise pixels (i.e. pulmonary trachea and pulmonary vessels inside the lung region). <bold>b</bold> Applying median filtering to partially eliminate the noise pixels. <bold>c</bold> Applying distance transformation to further eliminate the noise pixels and extract the main lesion region</p></caption><graphic xlink:href="11554_2022_1249_Fig3_HTML" id="MO12"/></fig></p>
      </sec>
      <sec id="Sec12">
        <title>Distance transformation</title>
        <p id="Par34">Distance transformation (DT) is to convert a digital binary image that consists of object and non-object pixels into another image in which each object pixel owns a value corresponding to the minimum distance from the non-object by a distance function [<xref ref-type="bibr" rid="CR50">50</xref>, <xref ref-type="bibr" rid="CR51">51</xref>]. Distance transformation is widely used for target thinning, object skeleton extraction. Euclidean distance, city block distance, and chessboard distance are widely used measures for distance transformation. The full workflow of distance transformation is introduced as follows:</p>
        <p id="Par35">Given an image <inline-formula id="IEq19"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J$$\end{document}</tex-math><mml:math id="M44"><mml:mi>J</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq19.gif"/></alternatives></inline-formula>, it’s binarized to get an image <inline-formula id="IEq20"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${J}_{b}$$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mi>J</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq20.gif"/></alternatives></inline-formula>. In <inline-formula id="IEq21"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${J}_{b}$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mi>J</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq21.gif"/></alternatives></inline-formula>, 1 is associated with object pixel and 0 with the background pixel. Hence, we have a pixel set <inline-formula id="IEq22"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal{O}$$\end{document}</tex-math><mml:math id="M50"><mml:mi mathvariant="script">O</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq22.gif"/></alternatives></inline-formula> represented by all the object pixels and <inline-formula id="IEq23"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{O}}^{c}$$\end{document}</tex-math><mml:math id="M52"><mml:msup><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq23.gif"/></alternatives></inline-formula> represented by all the background pixels.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{O}} = \left\{ {t\left| {J_{b} \left( t \right)} \right. = 1} \right\},$$\end{document}</tex-math><mml:math id="M54" display="block"><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>t</mml:mi><mml:mfenced open="|"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{*{20}c} {{\mathcal{O}}^{c} = \left\{ {b\left| {J_{b} \left( b \right)} \right. = 0} \right\}} \\ \end{array} ,$$\end{document}</tex-math><mml:math id="M56" display="block"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>b</mml:mi><mml:mfenced open="|"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:mi>b</mml:mi></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq24"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$t$$\end{document}</tex-math><mml:math id="M58"><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq24.gif"/></alternatives></inline-formula> and <inline-formula id="IEq25"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b$$\end{document}</tex-math><mml:math id="M60"><mml:mi>b</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq25.gif"/></alternatives></inline-formula> represent the pixel of objects and background respectively. The distance transformation (<inline-formula id="IEq26"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$DT$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi mathvariant="italic">DT</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq26.gif"/></alternatives></inline-formula>) generates a map <inline-formula id="IEq27"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D$$\end{document}</tex-math><mml:math id="M64"><mml:mi>D</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq27.gif"/></alternatives></inline-formula>, in which the value of each pixel in <inline-formula id="IEq28"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal{O}$$\end{document}</tex-math><mml:math id="M66"><mml:mi mathvariant="script">O</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq28.gif"/></alternatives></inline-formula> is the smallest distance from this pixel to <inline-formula id="IEq29"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{O}}^{c}$$\end{document}</tex-math><mml:math id="M68"><mml:msup><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq29.gif"/></alternatives></inline-formula>:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D\left( t \right) = \min \left\{ {d\left( {t,b} \right)\left| {t \in {\mathcal{O}}} \right.,\quad b \in {\mathcal{O}}^{c} } \right\},$$\end{document}</tex-math><mml:math id="M70" display="block"><mml:mrow><mml:mi>D</mml:mi><mml:mfenced close=")" open="("><mml:mi>t</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mo movablelimits="true">min</mml:mo><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>d</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced><mml:mfenced open="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">O</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where the image <inline-formula id="IEq30"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D$$\end{document}</tex-math><mml:math id="M72"><mml:mi>D</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq30.gif"/></alternatives></inline-formula> is called the distance map of <inline-formula id="IEq31"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J$$\end{document}</tex-math><mml:math id="M74"><mml:mi>J</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq31.gif"/></alternatives></inline-formula>. It is assumed that <inline-formula id="IEq32"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal{O}$$\end{document}</tex-math><mml:math id="M76"><mml:mi mathvariant="script">O</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq32.gif"/></alternatives></inline-formula> contains at least one pixel. Otherwise, the output of the <inline-formula id="IEq33"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$DT$$\end{document}</tex-math><mml:math id="M78"><mml:mrow><mml:mi mathvariant="italic">DT</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq33.gif"/></alternatives></inline-formula> is undefined, i.e., the outliers will be ignored in the distance transformation. Moreover, <inline-formula id="IEq34"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d(t,b)$$\end{document}</tex-math><mml:math id="M80"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq34.gif"/></alternatives></inline-formula> represents Euclidean distance, is formularized as:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{gathered} d\left( {t,b} \right) = \sqrt {\left( {t_{x} - b_{x} } \right)^{2} + \left( {t_{y} - b_{y} } \right)^{2} } \hfill \\ \begin{array}{*{20}c} {0 \le x &lt; W,\quad 0 \le y &lt; H,} \\ \end{array} \hfill \\ \end{gathered}$$\end{document}</tex-math><mml:math id="M82" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>d</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>y</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq35"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H$$\end{document}</tex-math><mml:math id="M84"><mml:mi>H</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq35.gif"/></alternatives></inline-formula> and <inline-formula id="IEq36"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W$$\end{document}</tex-math><mml:math id="M86"><mml:mi>W</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq36.gif"/></alternatives></inline-formula> represent the height and width of the image <inline-formula id="IEq37"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J$$\end{document}</tex-math><mml:math id="M88"><mml:mi>J</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq37.gif"/></alternatives></inline-formula> respectively.</p>
        <p id="Par36">As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>c, distance transformation is used to eliminate noise pixels (i.e., the pulmonary trachea and pulmonary vessels) and extract the main lesion region. By applying sequential median filtering and distance transformation, the distribution of connected regions in a lung image is shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. The connected regions containing more than 200 pixels represent the lesion region, and those with a small area represent the region of the pulmonary trachea and pulmonary vessels. By applying distance transformation, the distribution of connected regions in the lung image is close to the ground truth. Distance maps of several lung images are shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. By comparing the distance map with the corresponding lesion label, the main lesion region is extracted.<fig id="Fig4"><label>Fig. 4</label><caption><p>The distributions of connected regions in a lung image without/with applying sequential median filtering, with applying distance transformation</p></caption><graphic xlink:href="11554_2022_1249_Fig4_HTML" id="MO13"/></fig><fig id="Fig5"><label>Fig. 5</label><caption><p>Distance maps of several lung images: <bold>a</bold> lung region is segmented from CT images in the dataset. <bold>b</bold> Distance maps of lung images obtained by distance transformation. <bold>c</bold> By comparing the distance map with the corresponding lesion label, the main lesion region is extracted</p></caption><graphic xlink:href="11554_2022_1249_Fig5_HTML" id="MO14"/></fig></p>
        <p id="Par37">Normalization serves as an activation function to obtain the shape prior of the lesion region. As shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>b, distance transformation strengthens the weight of the main connected regions and weakens the weight of the edge parts (as shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>a). Furthermore, as shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>c, normalization removes both the edge and connected regions with a small area. The normalization function <inline-formula id="IEq38"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Norm$$\end{document}</tex-math><mml:math id="M90"><mml:mrow><mml:mi mathvariant="italic">Norm</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq38.gif"/></alternatives></inline-formula> is formularized as:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{Norm}}\left( X \right) = \frac{{X - X_{\min } }}{{X_{\max } - X_{\min } }},$$\end{document}</tex-math><mml:math id="M92" display="block"><mml:mrow><mml:mtext>Norm</mml:mtext><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>X</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mo movablelimits="true">min</mml:mo></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mo movablelimits="true">max</mml:mo></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mo movablelimits="true">min</mml:mo></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq39"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X$$\end{document}</tex-math><mml:math id="M94"><mml:mi>X</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq39.gif"/></alternatives></inline-formula> represents an image matrix, <inline-formula id="IEq40"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${X}_{\mathrm{min}}$$\end{document}</tex-math><mml:math id="M96"><mml:msub><mml:mi>X</mml:mi><mml:mi mathvariant="normal">min</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq40.gif"/></alternatives></inline-formula> and <inline-formula id="IEq41"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${X}_{\mathrm{max}}$$\end{document}</tex-math><mml:math id="M98"><mml:msub><mml:mi>X</mml:mi><mml:mi mathvariant="normal">max</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq41.gif"/></alternatives></inline-formula> represent the minimum and maximum values in <inline-formula id="IEq42"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X$$\end{document}</tex-math><mml:math id="M100"><mml:mi>X</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq42.gif"/></alternatives></inline-formula> respectively. Next, as illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b, two 7 × 7 convolution layers are utilized to learn the attention weight of edge pixels which has a low boundary contrast to surroundings, as well as adaptively tuning up attention weights to other regions. Finally, refined spatial attention is obtained. Spatial attention is formularized as:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{M}}_{s} \left( {\mathbf{I}} \right) = \sigma \left( {f_{7 \times 7}^{{{\text{cov}}}} \left( {f_{7 \times 7}^{{{\text{cov}}}} \left( {{\text{Norm}}\left( {{\text{DT}}\left( {f^{{\text{med }}} \left( {f^{{{\text{seg}}}} \left( {\mathbf{I}} \right)} \right)} \right)} \right)} \right)} \right)} \right),$$\end{document}</tex-math><mml:math id="M102" display="block"><mml:mrow><mml:msub><mml:mi mathvariant="bold">M</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:mi mathvariant="bold">I</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>7</mml:mn></mml:mrow><mml:mtext>cov</mml:mtext></mml:msubsup><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:mn>7</mml:mn></mml:mrow><mml:mtext>cov</mml:mtext></mml:msubsup><mml:mfenced close=")" open="("><mml:mrow><mml:mtext>Norm</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:mtext>DT</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>med</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow></mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mtext>seg</mml:mtext></mml:msup><mml:mfenced close=")" open="("><mml:mi mathvariant="bold">I</mml:mi></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq43"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf{I}$$\end{document}</tex-math><mml:math id="M104"><mml:mi mathvariant="bold">I</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq43.gif"/></alternatives></inline-formula> is the input CT image, <inline-formula id="IEq44"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M106"><mml:mi>σ</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq44.gif"/></alternatives></inline-formula> represents the sigmoid function, <inline-formula id="IEq45"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f}^{\mathrm{seg}}$$\end{document}</tex-math><mml:math id="M108"><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi mathvariant="normal">seg</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq45.gif"/></alternatives></inline-formula> and <inline-formula id="IEq46"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f}^{\text{med}}$$\end{document}</tex-math><mml:math id="M110"><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mtext>med</mml:mtext></mml:msup></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq46.gif"/></alternatives></inline-formula> represent the lung segmentation network and median filtering, respectively. <inline-formula id="IEq47"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f}_{7*7}^{\mathrm{cov}}$$\end{document}</tex-math><mml:math id="M112"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mn>7</mml:mn><mml:mrow/><mml:mo>∗</mml:mo><mml:mn>7</mml:mn></mml:mrow><mml:mi mathvariant="normal">cov</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq47.gif"/></alternatives></inline-formula> is the 7 × 7 convolution layer. <inline-formula id="IEq48"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{DT}$$\end{document}</tex-math><mml:math id="M114"><mml:mi mathvariant="normal">DT</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq48.gif"/></alternatives></inline-formula> represents the distance transformation.<fig id="Fig6"><label>Fig. 6</label><caption><p>A numerical example of distance transformation: <bold>a</bold> a binary image containing several connected regions; <bold>b</bold> the distance map of (<bold>a</bold>); <bold>c</bold> is the normalization of (<bold>b</bold>)</p></caption><graphic xlink:href="11554_2022_1249_Fig6_HTML" id="MO15"/></fig></p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec13">
    <title>Experiment</title>
    <sec id="Sec14">
      <title>Dataset and metrics</title>
      <p id="Par38">The dataset contains 20 groups of labeled CT scans of COVID-19 patients. Two radiologists manually labeled the regions of left lung, right lung, and COVID-19 lesions on the images and an extra verification of the labeled regions was carried out by another radiologist. CT slices are exported from the CT scans as 2D images (752 × 670 pixels), and a group of processed images are listed in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. The dataset contains a total of 3520 CT images and is divided into a training set and a testing set at a ratio of 4:1 (as detailed in Table <xref rid="Tab1" ref-type="table">1</xref>). It is noted that “positive” means CT images with the lesion, and “negative” means healthy CT images.<fig id="Fig7"><label>Fig. 7</label><caption><p>The dataset contains segmentation labels for the left lung, right lung and COVID-19 lesions. The lung region is segmented based on the lung label</p></caption><graphic xlink:href="11554_2022_1249_Fig7_HTML" id="MO16"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption><p>Details of the dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Positive</th><th align="left">Negative</th><th align="left">Total</th></tr></thead><tbody><tr><td align="left">Train</td><td char="." align="char">1474</td><td char="." align="char">1341</td><td char="." align="char">2815</td></tr><tr><td align="left">Test</td><td char="." align="char">369</td><td char="." align="char">336</td><td char="." align="char">705</td></tr><tr><td align="left">Total</td><td char="." align="char">1843</td><td char="." align="char">1677</td><td char="." align="char">3520</td></tr></tbody></table></table-wrap></p>
      <p id="Par39">The Dice Similarity Coefficients (DSC), false negatives (FN), false positives (FP) and inference time of the networks are adopted as evaluation metrics. DSC is a standard metric for comparing the pixel-wise results between the ground truth and predicted segmentation. It is formularized as follows:<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{DSC}}\left( {A,B} \right) = \frac{{2\left| {A \cap B} \right|}}{\left| A \right| + \left| B \right|},$$\end{document}</tex-math><mml:math id="M116" display="block"><mml:mrow><mml:mtext>DSC</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>A</mml:mi><mml:mo>∩</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced close="|" open="|"><mml:mi>A</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close="|" open="|"><mml:mi>B</mml:mi></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq49"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A$$\end{document}</tex-math><mml:math id="M118"><mml:mi>A</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq49.gif"/></alternatives></inline-formula> is the lesion label, and <inline-formula id="IEq50"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$B$$\end{document}</tex-math><mml:math id="M120"><mml:mi>B</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq50.gif"/></alternatives></inline-formula> denotes the segmented lesion image.</p>
    </sec>
    <sec id="Sec15">
      <title>Training method</title>
      <p id="Par40">An Adam optimizer with a learning rate of 0.001 is employed to minimize the binary cross-entropy (BCE) loss. The total number of training epochs and batch size is set to 100 and 1, respectively. The network weights are initialized with Kaiming initialization, and the network biases are initialized by 0. Moreover, the positive and negative samples are trained alternately, and the dataset is shuffled in each iteration. The formula of the BCE loss is as follows:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{Loss}}_{{{\text{BCE}}}} = - \left[ {G\log \left( P \right) + \left( {1 - G} \right)\log \left( {1 - P} \right)} \right],$$\end{document}</tex-math><mml:math id="M122" display="block"><mml:mrow><mml:msub><mml:mtext>Loss</mml:mtext><mml:mtext>BCE</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mi>G</mml:mi><mml:mo>log</mml:mo><mml:mfenced close=")" open="("><mml:mi>P</mml:mi></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:mfenced><mml:mo>log</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq51"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G$$\end{document}</tex-math><mml:math id="M124"><mml:mi>G</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq51.gif"/></alternatives></inline-formula> is the label and <inline-formula id="IEq52"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P$$\end{document}</tex-math><mml:math id="M126"><mml:mi>P</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq52.gif"/></alternatives></inline-formula> is the output of the network.</p>
    </sec>
    <sec id="Sec16">
      <title>Ablation analysis</title>
      <p id="Par41">FAM can be conveniently integrated into any CNNs as a lightweight and plug-and-play attention module. As shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>, FAM is integrated into convolution layers to refine the intermediate feature map. FAM and two state-of-the-art attention modules (i.e. SENet [<xref ref-type="bibr" rid="CR19">19</xref>] and CBAM [<xref ref-type="bibr" rid="CR20">20</xref>]) are respectively inserted into every two convolution layers of six state-of-the-art segmentation networks [<xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR21">21</xref>] for ablation experiments. All the related networks and modules are reproduced in the framework PyTorch, trained and inferenced on a single NVIDIA GeForce RTX 2080Ti GPU with CUDA v10.2 and cuDNN v7.6.5. The main concerns in this ablation analysis contains: shape prior, time complexity, the performance of the network and convergence rate of the network training.<fig id="Fig8"><label>Fig. 8</label><caption><p>The structure of the integration of FAM with the network</p></caption><graphic xlink:href="11554_2022_1249_Fig8_HTML" id="MO17"/></fig></p>
      <sec id="Sec17">
        <title>Shape prior</title>
        <p id="Par42">FAM is constructed based on the shape prior and the spatial attention module of CBAM. In the case that CBAM and FAM are integrated respectively between each convolution layer in SegNet (as shown in Fig. <xref rid="Fig9" ref-type="fig">9</xref>). A visualization of the spatial attention is built in Fig. <xref rid="Fig10" ref-type="fig">10</xref> to illustrate the effect of shape prior. Since that SegNet is a typical encode-decode structure, in which, low-level features and high-level semantic features are extracted accordingly, the feature map contained in the middle layers is abstract. CBAM adopts such a feature extraction workflow because of the connection between its spatial attention module and the network, i.e., the input of the spatial attention module comes from inside the network rather than outside. Thus, the spatial attention of CBAM integrated into the middle layers is abstract, and that integrated into other layers highlights only the lung region rather than the lesion region, resulting in the inaccuracy of the spatial attention learning of CBAM. The shape prior information of the lesion region is introduced in the spatial attention module to reduce the search space. In summary, FAM focuses on the lesion region without being disturbed by the no-lesion region during the spatial attention learning process.<fig id="Fig9"><label>Fig. 9</label><caption><p>The workflow of attention modules integrated with SegNet</p></caption><graphic xlink:href="11554_2022_1249_Fig9_HTML" id="MO18"/></fig><fig id="Fig10"><label>Fig. 10</label><caption><p>Spatial attention comparison between CBAM and FAM in SegNet. We visualized the spatial attention for sixteen attention modules of SegNet when using CBAM and FAM. <bold>c</bold> is the lung image segmented from the input CT image. <bold>d</bold> and <bold>e</bold> are corresponding lesion label and shape prior</p></caption><graphic xlink:href="11554_2022_1249_Fig10_HTML" id="MO19"/></fig></p>
      </sec>
      <sec id="Sec18">
        <title>Time complexity</title>
        <p id="Par43">Time complexity determines the training and inference time of the network. The network with high time complexity suffer from poor real-time performance. Floating Point Operations (FLOPs) is a classical metric of the time complexity. The time complexity of a network is formularized as follows:<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T = O\left( {\sum\limits_{i = 1}^{l} {f_{{{\text{FLOPs}}}} \left( {{\text{layer}}} \right)} } \right),$$\end{document}</tex-math><mml:math id="M128" display="block"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>l</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mtext>FLOPs</mml:mtext></mml:msub><mml:mfenced close=")" open="("><mml:mtext>layer</mml:mtext></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq53"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><mml:math id="M130"><mml:mi>l</mml:mi></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq53.gif"/></alternatives></inline-formula> denotes the number of layers in the network. <inline-formula id="IEq54"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f}_{FLOPs}$$\end{document}</tex-math><mml:math id="M132"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="italic">FLOPs</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq54.gif"/></alternatives></inline-formula> represents the function that calculates the FLOPs of a layer. FLOPs for each type of layer is formularized as:<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_{{{\text{FLOPs}}}}^{{{\text{conv}}}} = 2C_{{{\text{in}}}} C_{{{\text{out}}}} K_{w} K_{h} Q_{w} Q_{h} ,$$\end{document}</tex-math><mml:math id="M134" display="block"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>FLOPs</mml:mtext></mml:mrow><mml:mtext>conv</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>C</mml:mi><mml:mtext>in</mml:mtext></mml:msub><mml:msub><mml:mi>C</mml:mi><mml:mtext>out</mml:mtext></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:msub><mml:mi>K</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_{{{\text{FLOPs}}}}^{{{\text{linear}}}} = 2N_{{{\text{in}}}} N_{{{\text{out}}}} ,$$\end{document}</tex-math><mml:math id="M136" display="block"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>FLOPs</mml:mtext></mml:mrow><mml:mtext>linear</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mtext>in</mml:mtext></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mtext>out</mml:mtext></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_{{{\text{FLOPs}}}}^{{{\text{pooling}}}} = C_{in} Q_{w} Q_{h} ,$$\end{document}</tex-math><mml:math id="M138" display="block"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>FLOPs</mml:mtext></mml:mrow><mml:mtext>pooling</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_{{{\text{FLOPs}}}}^{{{\text{relu}}}} = C_{in} Q_{w} Q_{h} ,$$\end{document}</tex-math><mml:math id="M140" display="block"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>FLOPs</mml:mtext></mml:mrow><mml:mtext>relu</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_{{{\text{FLOPs}}}}^{{{\text{sigmoid}}}} = 4C_{in} Q_{w} Q_{h} ,$$\end{document}</tex-math><mml:math id="M142" display="block"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mtext>FLOPs</mml:mtext></mml:mrow><mml:mtext>sigmoid</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:msub><mml:mi>Q</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="11554_2022_1249_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq55"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f}_{\mathrm{FLOPs}}^{\mathrm{conv}}$$\end{document}</tex-math><mml:math id="M144"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">FLOPs</mml:mi></mml:mrow><mml:mi mathvariant="normal">conv</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq55.gif"/></alternatives></inline-formula>, <inline-formula id="IEq56"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f}_{\mathrm{FLOPs}}^{\mathrm{linear}}$$\end{document}</tex-math><mml:math id="M146"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">FLOPs</mml:mi></mml:mrow><mml:mi mathvariant="normal">linear</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq56.gif"/></alternatives></inline-formula>, <inline-formula id="IEq57"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f}_{\mathrm{FLOPs}}^{\mathrm{pooling}}$$\end{document}</tex-math><mml:math id="M148"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">FLOPs</mml:mi></mml:mrow><mml:mi mathvariant="normal">pooling</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq57.gif"/></alternatives></inline-formula>, <inline-formula id="IEq58"><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f}_{\mathrm{FLOPs}}^{\mathrm{relu}}$$\end{document}</tex-math><mml:math id="M150"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">FLOPs</mml:mi></mml:mrow><mml:mi mathvariant="normal">relu</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq58.gif"/></alternatives></inline-formula> and <inline-formula id="IEq59"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f}_{\mathrm{FLOPs}}^{\mathrm{sigmoid}}$$\end{document}</tex-math><mml:math id="M152"><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:mi mathvariant="normal">FLOPs</mml:mi></mml:mrow><mml:mi mathvariant="normal">sigmoid</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq59.gif"/></alternatives></inline-formula> represent the function that calculate the FLOPs of convolution layer, full connect layer, global pooling layer, Relu layer and sigmoid layer respectively. These five types of layers are used in FAM, SENet and CBAM. <inline-formula id="IEq60"><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${C}_{in}$$\end{document}</tex-math><mml:math id="M154"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq60.gif"/></alternatives></inline-formula> and <inline-formula id="IEq61"><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${C}_{out}$$\end{document}</tex-math><mml:math id="M156"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq61.gif"/></alternatives></inline-formula> denote the channel number of the input and output feature map respectively. <inline-formula id="IEq62"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${K}_{w}$$\end{document}</tex-math><mml:math id="M158"><mml:msub><mml:mi>K</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq62.gif"/></alternatives></inline-formula> and <inline-formula id="IEq63"><alternatives><tex-math id="M159">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${K}_{h}$$\end{document}</tex-math><mml:math id="M160"><mml:msub><mml:mi>K</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq63.gif"/></alternatives></inline-formula> denote the width and height of the convolution kernel respectively. <inline-formula id="IEq64"><alternatives><tex-math id="M161">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Q}_{w}$$\end{document}</tex-math><mml:math id="M162"><mml:msub><mml:mi>Q</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq64.gif"/></alternatives></inline-formula> and <inline-formula id="IEq65"><alternatives><tex-math id="M163">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Q}_{h}$$\end{document}</tex-math><mml:math id="M164"><mml:msub><mml:mi>Q</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq65.gif"/></alternatives></inline-formula> denote the width and height of the input feature map respectively. <inline-formula id="IEq66"><alternatives><tex-math id="M165">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${N}_{\mathrm{in}}$$\end{document}</tex-math><mml:math id="M166"><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">in</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq66.gif"/></alternatives></inline-formula> and <inline-formula id="IEq67"><alternatives><tex-math id="M167">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${N}_{\mathrm{out}}$$\end{document}</tex-math><mml:math id="M168"><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">out</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11554_2022_1249_Article_IEq67.gif"/></alternatives></inline-formula> denote the number of input and output neurons. We set the channel number of the input feature map to 16 and set the reduction ratio of the channel attention module to 16. The size of the input feature map is 652 × 752. FLOPs of FAM, SENet and CBAM are listed in Table <xref rid="Tab2" ref-type="table">2</xref>. SENet achieves the smallest FLOPs because it lacks a spatial attention module. FAM and CBAM share a similar structure, but FLOPs of the latter is less than the former by about 15 million. It is because FAM owns fewer pooling layers than CBAM. As shown in Table. <xref rid="Tab3" ref-type="table">3</xref>, FLOPs of six state-of-the-art segmentation networks increase very little when FAM is integrated between every two convolution layers. Moreover, the inference time of FAM is 4 ms faster than that for CBAM on average. In conclusion, FAM is able to greatly improve the performance of the network with very little time complexity increasement.<table-wrap id="Tab2"><label>Table 2</label><caption><p>FLOPs of FAM, SENet and CBAM</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Conv</th><th align="left">Linear</th><th align="left">Pooling</th><th align="left">Relu</th><th align="left">Sigmoid</th><th align="left">Total</th></tr></thead><tbody><tr><td align="left">SENet</td><td align="left">0</td><td align="left">64</td><td align="left">7,844,864</td><td align="left">1</td><td align="left">64</td><td align="left">7,844,993</td></tr><tr><td align="left">CBAM</td><td align="left">99,047,552</td><td align="left">0</td><td align="left">31,379,456</td><td align="left">2</td><td align="left">34,363,392</td><td align="left">164,790,402</td></tr><tr><td align="left"><bold>FAM</bold></td><td align="left"><bold>99,047,552</bold></td><td align="left"><bold>0</bold></td><td align="left"><bold>15,689,728</bold></td><td align="left"><bold>505,346</bold></td><td align="left"><bold>34,363,392</bold></td><td align="left"><bold>149,606,018</bold></td></tr></tbody></table><table-wrap-foot><p>Bold represents values of our proposed method</p></table-wrap-foot></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>Performance comparison of FAM, SENet and CBAM on state-of-the-art segmentation network</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Network</th><th align="left">Attention module</th><th align="left">#Param (k)</th><th align="left">FLOPs (G)</th><th align="left">DSC%</th><th align="left">FN + FP</th><th align="left">Inference time (ms)</th></tr></thead><tbody><tr><td align="left" rowspan="4">FCN</td><td align="left">–</td><td char="." align="char">19,169.03</td><td char="." align="char">201.18</td><td char="." align="char">85.80</td><td char="." align="char">824</td><td char="." align="char">29</td></tr><tr><td align="left">SENet</td><td char="." align="char">19,319.04</td><td char="." align="char">201.25</td><td char="." align="char">85.59</td><td char="." align="char">830</td><td char="." align="char">33</td></tr><tr><td align="left">CBAM</td><td char="." align="char">19,319.83</td><td char="." align="char">201.32</td><td char="." align="char">85.65</td><td char="." align="char">823</td><td char="." align="char">56</td></tr><tr><td align="left"><bold>FAM</bold></td><td char="." align="char"><bold>19,320.04</bold></td><td char="." align="char"><bold>201.30</bold></td><td char="." align="char"><bold>86.41</bold></td><td char="." align="char"><bold>762</bold></td><td char="." align="char"><bold>53</bold></td></tr><tr><td align="left" rowspan="4">UNet</td><td align="left">–</td><td char="." align="char">31,042.37</td><td char="." align="char">421.05</td><td char="." align="char">86.03</td><td char="." align="char">770</td><td char="." align="char">69</td></tr><tr><td align="left">SENet</td><td char="." align="char">31,260.48</td><td char="." align="char">421.44</td><td char="." align="char">86.13</td><td char="." align="char">781</td><td char="." align="char">72</td></tr><tr><td align="left">CBAM</td><td char="." align="char">31,261.37</td><td char="." align="char">422.12</td><td char="." align="char">85.22</td><td char="." align="char">818</td><td char="." align="char">114</td></tr><tr><td align="left"><bold>FAM</bold></td><td char="." align="char"><bold>31,262.25</bold></td><td char="." align="char"><bold>421.57</bold></td><td char="." align="char"><bold>86.56</bold></td><td char="." align="char"><bold>767</bold></td><td char="." align="char"><bold>110</bold></td></tr><tr><td align="left" rowspan="4">SegNet</td><td align="left">–</td><td char="." align="char">29,442.43</td><td char="." align="char">308.98</td><td char="." align="char">85.13</td><td char="." align="char">839</td><td char="." align="char">56</td></tr><tr><td align="left">SENet</td><td char="." align="char">29,742.47</td><td char="." align="char">309.79</td><td char="." align="char">85.03</td><td char="." align="char">837</td><td char="." align="char">60</td></tr><tr><td align="left">CBAM</td><td char="." align="char">29,744.05</td><td char="." align="char">309.93</td><td char="." align="char">85.80</td><td char="." align="char">796</td><td char="." align="char">106</td></tr><tr><td align="left"><bold>FAM</bold></td><td char="." align="char"><bold>29,744.83</bold></td><td char="." align="char"><bold>309.84</bold></td><td char="." align="char"><bold>87.13</bold></td><td char="." align="char"><bold>720</bold></td><td char="." align="char"><bold>101</bold></td></tr><tr><td align="left" rowspan="4">DeepLabV3+ </td><td align="left">–</td><td char="." align="char">59,233.51</td><td char="." align="char">170.07</td><td char="." align="char">87.42</td><td char="." align="char">712</td><td char="." align="char">42</td></tr><tr><td align="left">SENet</td><td char="." align="char">59,856.02</td><td char="." align="char">170.75</td><td char="." align="char">86.85</td><td char="." align="char">746</td><td char="." align="char">45</td></tr><tr><td align="left">CBAM</td><td char="." align="char">59,858.71</td><td char="." align="char">170.79</td><td char="." align="char">86.51</td><td char="." align="char">766</td><td char="." align="char">91</td></tr><tr><td align="left"><bold>FAM</bold></td><td char="." align="char"><bold>59,860.01</bold></td><td char="." align="char"><bold>170.76</bold></td><td char="." align="char"><bold>88.54</bold></td><td char="." align="char"><bold>601</bold></td><td char="." align="char"><bold>85</bold></td></tr><tr><td align="left" rowspan="4">PSPNet</td><td align="left">–</td><td char="." align="char">27,509.47</td><td char="." align="char">313.67</td><td char="." align="char">86.67</td><td char="." align="char">769</td><td char="." align="char">56</td></tr><tr><td align="left">SENet</td><td char="." align="char">27,677.10</td><td char="." align="char">313.74</td><td char="." align="char">86.98</td><td char="." align="char">763</td><td char="." align="char">60</td></tr><tr><td align="left">CBAM</td><td char="." align="char">27,677.45</td><td char="." align="char">313.81</td><td char="." align="char">86.93</td><td char="." align="char">772</td><td char="." align="char">92</td></tr><tr><td align="left"><bold>FAM</bold></td><td char="." align="char"><bold>27,677.83</bold></td><td char="." align="char"><bold>313.80</bold></td><td char="." align="char"><bold>88.12</bold></td><td char="." align="char"><bold>634</bold></td><td char="." align="char"><bold>90</bold></td></tr><tr><td align="left" rowspan="4">UNet++</td><td align="left">–</td><td char="." align="char">47,170.32</td><td char="." align="char">1539.32</td><td char="." align="char">85.67</td><td char="." align="char">820</td><td char="." align="char">187</td></tr><tr><td align="left">SENet</td><td char="." align="char">47,415.21</td><td char="." align="char">1539.58</td><td char="." align="char">85.85</td><td char="." align="char">800</td><td char="." align="char">191</td></tr><tr><td align="left">CBAM</td><td char="." align="char">47,417.20</td><td char="." align="char">1539.89</td><td char="." align="char">85.70</td><td char="." align="char">794</td><td char="." align="char">240</td></tr><tr><td align="left"><bold>FAM</bold></td><td char="." align="char"><bold>47,418.06</bold></td><td char="." align="char"><bold>1539.62</bold></td><td char="." align="char"><bold>87.06</bold></td><td char="." align="char"><bold>723</bold></td><td char="." align="char"><bold>232</bold></td></tr></tbody></table><table-wrap-foot><p>Bold represents values of our proposed method</p></table-wrap-foot></table-wrap></p>
      </sec>
      <sec id="Sec19">
        <title>Performance analysis of the networks with attention modules</title>
        <p id="Par44">The average DSC, FN and FP of all images in the test set are adopted to analyze the performance of the integration of attention modules to the networks (as detailed in Table <xref rid="Tab3" ref-type="table">3</xref>). FAM adds almost no extra parameters to the network, as it only has one more 7 × 7 convolution than CBAM. The integration of FAM improves the DSC of all these networks, with an improvement of 2% for SegNet. In addition, the integration of FAM reduces the FN and FP, with a reduction of 17.6% for PSPNet.</p>
        <p id="Par45">SENet improves the performance of UNet, PSPNet and UNet++, meanwhile it degrades the performance of FCN, SegNet and deeplabV3+. CBAM improves the performance of SegNet and PSPNet, meanwhile it degrades the performance of FCN, UNet, Deeplabv3 and UNet++. This finding shows that the effectiveness of the integration of the attention module depends on the structure of the network.</p>
        <p id="Par46">Figure <xref rid="Fig11" ref-type="fig">11</xref>. shows the segmentation result of SegNet integrated with different attention modules on the dataset. As shown in Fig. <xref rid="Fig11" ref-type="fig">11</xref>, SegNet without the integration of the attention module suffers from false detection in lesions of both left and right lobes to some extent. The integration of SENet alleviates it but CBAM exacerbates it. Although the SegNet integrated with FAM has many false detections, overall it has the highest segmentation accuracy.<fig id="Fig11"><label>Fig. 11</label><caption><p>The comparison of the segmentation results on lesions of COVID-19 CT images by applying various combinations of SegNet and attention modules</p></caption><graphic xlink:href="11554_2022_1249_Fig11_HTML" id="MO20"/></fig></p>
      </sec>
      <sec id="Sec20">
        <title>Convergence analysis of the network training</title>
        <p id="Par47">Six networks with attention modules are trained with the same experimental settings and dataset. As shown in Fig. <xref rid="Fig12" ref-type="fig">12</xref>, all networks converge within 60 iterations. FAM accelerates model training better than SegNet or CBAM does. For SegNet, UNet++ and DeepLabV3+, SENet does not significantly accelerate the model training. For UNet and UNet++, CBAM decelerates the convergence of the model training. However, FAM accelerates the training of FCN, UNet, SegNet, PSPNet and DeepLabV3+, as well as minimizing the loss.<fig id="Fig12"><label>Fig. 12</label><caption><p>Diagram of training networks integrated with various attention modules. <bold>a</bold>–<bold>c</bold> and <bold>d</bold>–<bold>f</bold> show that FAM accelerates the network training as well as minimizes the loss</p></caption><graphic xlink:href="11554_2022_1249_Fig12_HTML" id="MO21"/></fig></p>
        <p id="Par48">For UNet++, FAM performs better than the other two attention modules. For UNet and PSPNet, FAM makes the training process more stable. The result shows that FAM achieves better convergence rate and less converged loss value of the model training among these six networks than CBAM does. In addition, although SENet only applies its attention in the channel dimension, it achieves better performance than CBAM in specific networks such as FCN, UNet and PSPNet.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec21">
    <title>Conclusion</title>
    <p id="Par49">In this study, a lightweight and plug-and-play attention module, is proposed to improve the lesion segmentation performance of CNNs for COVID-19 CT images. FAM refines the input feature map from channel and space dimensions to maximize the network representation. In the spatial attention of FAM, shape prior of the lesion region is used to reduce the search space for attention learning. In addition, the feature map refined by spatial attention is added to the network as a residual branch. A set of experiments proved that: (1) FAM could improve the segmentation performance on a small-scale public COVID-19 CT image dataset; (2) FAM could accelerate the convergence speed of the model training; (3) FAM is capable of being stacked in a deep segmentation network without performance loss. (4) FAM could achieve better real-time performance.</p>
    <p id="Par50">FAM is promising for practical use in public health. In future, we will work towards improving the generated shape prior to enhance the generalization performance of FAM based on the up-to-date COVID-19 CT image datasets.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Xiaoxin Wu and Zhihao Zhang have contributed equally to this work and share first authorship.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>This study was supported by Natural Science Foundation of Zhejiang Province (No. LQ21H190004), China Postdoctoral Science Foundation (No. 2020T130102ZX), Postdoctoral Science Foundation of Zhejiang Province (No. ZJ2020031), the Educational Commission of Zhejiang Province of China (No. Y202147553).</p>
  </ack>
  <notes notes-type="data-availability">
    <title>Data availability </title>
    <p>The datasets generated during and analyzed during the current study are available from the corresponding author on reasonable request.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar10" notes-type="COI-statement">
      <title>Conflict of interest</title>
      <p id="Par51">The authors declare that they have no conflict of interest.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ai</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Hou</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Correlation of chest CT and RT-PCR testing for coronavirus disease 2019 (COVID-19) in China: a report of 1014 cases</article-title>
        <source>Radiology</source>
        <year>2020</year>
        <volume>296</volume>
        <issue>2</issue>
        <fpage>E32</fpage>
        <lpage>E40</lpage>
        <pub-id pub-id-type="doi">10.1148/radiol.2020200642</pub-id>
        <pub-id pub-id-type="pmid">32101510</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Adams</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Kwee</surname>
            <given-names>TC</given-names>
          </name>
          <name>
            <surname>Yakar</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Chest CT imaging signature of coronavirus disease 2019 infection: in pursuit of the scientific evidence</article-title>
        <source>Chest</source>
        <year>2020</year>
        <volume>158</volume>
        <issue>5</issue>
        <fpage>1885</fpage>
        <lpage>1895</lpage>
        <pub-id pub-id-type="doi">10.1016/j.chest.2020.06.025</pub-id>
        <pub-id pub-id-type="pmid">32592709</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Qi</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Dou</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>DisCOV: distributed COVID-19 detection on X-ray images with edge-cloud collaboration</article-title>
        <source>IEEE Trans. Serv. Comput.</source>
        <year>2022</year>
        <pub-id pub-id-type="doi">10.1109/TSC.2022.3142265</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shelhamer</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Long</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Darrell</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Fully convolutional networks for semantic segmentation</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2016</year>
        <volume>39</volume>
        <issue>4</issue>
        <fpage>640</fpage>
        <lpage>651</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2572683</pub-id>
        <pub-id pub-id-type="pmid">27244717</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Badrinarayanan</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Kendall</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cipolla</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Segnet: a deep convolutional encoder-decoder architecture for image segmentation</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2017</year>
        <volume>39</volume>
        <issue>12</issue>
        <fpage>2481</fpage>
        <lpage>2495</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id>
        <pub-id pub-id-type="pmid">28060704</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Ronneberger, O., Fischer, P., Brox, T.: “U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention. Springer, pp 234–241 (2015). 10.1007/978-3-319-24574-4_28</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Rahman Siddiquee</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Tajbakhsh</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Unet++: a nested u-net architecture for medical image segmentation</article-title>
        <source>Deep learning in medical image analysis and multimodal learning for clinical decision support</source>
        <year>2018</year>
        <publisher-name>Springer</publisher-name>
        <fpage>3</fpage>
        <lpage>11</lpage>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Huang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., Han, X., Chen, Y.-W., Wu, J.: Unet 3+: A full-scale connected UNET for medical image segmentation. In: ICASSP 2020–2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp. 1055–1059. (2020) 10.1109/ICASSP40776.2020.9053405</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Chen, L.-C., Papandreou, G., Kokkinos, I. et al.: Semantic image segmentation with deep convolutional nets and fully connected CRFS. (2014) [Online]. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.7062">https://arxiv.org/abs/1412.7062</ext-link></mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>L-C</given-names>
          </name>
          <name>
            <surname>Papandreou</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kokkinos</surname>
            <given-names>I</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFS</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2017</year>
        <volume>40</volume>
        <issue>4</issue>
        <fpage>834</fpage>
        <lpage>848</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id>
        <pub-id pub-id-type="pmid">28463186</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Florian, L.-C., Adam, S. H.: Rethinking atrous convolution for semantic image segmentation. In: Conference on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF, (2017) [Online]. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.05587">https://arxiv.org/abs/1706.05587</ext-link></mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Albanie</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Squeeze-and-excitation networks</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2020</year>
        <volume>42</volume>
        <issue>8</issue>
        <fpage>2011</fpage>
        <lpage>2023</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2019.2913372</pub-id>
        <pub-id pub-id-type="pmid">31034408</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Woo, S., Park, J., Lee, J.-Y., Kweon, I. S.: Cbam: Convolutional block attention module. In: Proceedings of the European conference on computer vision (ECCV), pp. 3–19 (2018) 10.1007/978-3-030-01234-2_1</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lei</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Sk-net: deep learning on point cloud via end-to-end discovery of spatial keypoints</article-title>
        <source>Proc. AAAI Conf. Artif. Intell.</source>
        <year>2020</year>
        <volume>34</volume>
        <issue>04</issue>
        <fpage>6422</fpage>
        <lpage>6429</lpage>
        <pub-id pub-id-type="doi">10.1609/aaai.v34i04.6113</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>DP</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>GP</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Inf-Net: automatic COVID-19 lung infection segmentation from CT images</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2020</year>
        <volume>PP</volume>
        <issue>99</issue>
        <fpage>1</fpage>
        <lpage>1</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2020.2996645</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Chen, X., Yao, L., Zhang, Y.: Residual attention u-net for automated multi-class segmentation of covid-19 chest CT images. (2020) [Online]. Available: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2004.05645">https://arxiv.org/abs/2004.05645</ext-link></mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SCOAT-Net: aa novel network for segmenting COVID-19 lung opacification from CT images</article-title>
        <source>Pattern Recogn.</source>
        <year>2021</year>
        <volume>119</volume>
        <fpage>108109</fpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2021.108109</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A noise-robust framework for automatic segmentation of COVID-19 pneumonia lesions from CT images</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2020</year>
        <volume>39</volume>
        <issue>8</issue>
        <fpage>2653</fpage>
        <lpage>2663</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2020.3000314</pub-id>
        <pub-id pub-id-type="pmid">32730215</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Yan, Q., Wang, B., Gong, D. et al.; COVID-19 chest CT image segmentation—a deep convolutional neural network solution. (2020) [Online]. Available: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2004.10987">https://arxiv.org/abs/2004.10987</ext-link></mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Elharrouss, O., Subramanian, N., Al-Maadeed, S.: An encoder-decoder-based method for COVID-19 lung infection segmentation. (2020) [Online]. Available: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2007.00861">https://arxiv.org/abs/2007.00861</ext-link></mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Qiu, Y., Liu, Y., Li, S., Xu, J.: Miniseg: an extremely minimum network for efficient COVID-19 segmentation. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 6, (2021) 4846–4854. <ext-link ext-link-type="uri" xlink:href="https://ojs.aaai.org/index.php/AAAI/article/view/16617">https://ojs.aaai.org/index.php/AAAI/article/view/16617</ext-link></mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pei</surname>
            <given-names>H-Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>G-R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MPS-net: multi-point supervised network for CT image segmentation of covid-19</article-title>
        <source>IEEE Access</source>
        <year>2021</year>
        <volume>9</volume>
        <fpage>47144</fpage>
        <lpage>47153</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2021.3067047</pub-id>
        <pub-id pub-id-type="pmid">34812388</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zhong</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CoSinGAN: learning COVID-19 infection segmentation from a single radiological image</article-title>
        <source>Diagnostics</source>
        <year>2020</year>
        <volume>10</volume>
        <issue>11</issue>
        <fpage>901</fpage>
        <pub-id pub-id-type="doi">10.3390/diagnostics10110901</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Itti</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Koch</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Niebur</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>A model of saliency-based visual attention for rapid scene analysis</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>1998</year>
        <volume>20</volume>
        <issue>11</issue>
        <fpage>1254</fpage>
        <lpage>1259</lpage>
        <pub-id pub-id-type="doi">10.1109/34.730558</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Wang, F., Tax, D. M.: Survey on the attention based RNN model and its applications in computer vision. (2016) [Online]. Available: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1601.06823">https://arxiv.org/abs/1601.06823</ext-link></mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Jaderberg, M., Simonyan, K., Zisserman, A.: Spatial transformer networks. Adv. Neural Inform. Process. Syst. <bold>28</bold> (2015). https://dl.acm.org/doi/abs/10.5555/2969442.2969465</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Buades, A., Coll, B., Morel, J.-M.: A non-local algorithm for image denoising. In: 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), vol. 2. IEEE, pp. 60–65 (2005). 10.1109/CVPR.2005.38</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 7794–7803 (2018). 10.1109/CVPR.2018.00813</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., Lu, H.: Dual attention network for scene segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 3146–3154 (2019). 10.1109/CVPR.2019.00326</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X., Tang, X.: Residual attention network for image classification. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 3156–3164 (2017). 10.1109/CVPR.2017.683</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., Liu, W.: Ccnet: Criss-cross attention for semantic segmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 603–612 (2019). 10.1109/TPAMI.2020.3007032</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Gao, P., Zheng, M., Wang, X., Dai, J., Li, H.: Fast convergence of detr with spatially modulated co-attention (2021) [Online]. 10.48550/arXiv.2108.02404</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Channel-attention U-Net: channel attention mechanism for semantic segmentation of esophagus and esophageal cancer</article-title>
        <source>IEEE Access</source>
        <year>2020</year>
        <volume>8</volume>
        <fpage>122798</fpage>
        <lpage>122810</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3007719</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Diversified visual attention networks for fine-grained object classification</article-title>
        <source>IEEE Trans. Multimed.</source>
        <year>2017</year>
        <volume>19</volume>
        <issue>6</issue>
        <fpage>1245</fpage>
        <lpage>1256</lpage>
        <pub-id pub-id-type="doi">10.1109/TMM.2017.2648498</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Mnih, V., Heess, N., Graves, A.: Recurrent models of visual attention. Adv. Neural Inform. processing Syst. <bold>27</bold> (2014). https://dl.acm.org/doi/abs/10.5555/2969033.2969073</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Liu, X., Xia, T., Wang, J. et al.: Fully convolutional attention networks for fine-grained recognition. (2016) [Online]. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.06765">https://arxiv.org/abs/1603.06765</ext-link></mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Zhao, X., Zhang, P., Song, F. et al.: D2a u-net: automatic segmentation of COVID-19 lesions from CT slices with dilated convolution and dual attention mechanism. (2021) [Online]. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2102.05210">https://arxiv.org/abs/2102.05210</ext-link></mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Canu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ruan</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Automatic COVID-19 CT segmentation using U-Net integrated spatial and channel attention mechanism</article-title>
        <source>Int. J. Imaging Syst. Technol.</source>
        <year>2021</year>
        <volume>31</volume>
        <issue>1</issue>
        <fpage>16</fpage>
        <lpage>27</lpage>
        <pub-id pub-id-type="doi">10.1002/ima.22527</pub-id>
        <pub-id pub-id-type="pmid">33362345</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Deep-learning- enhanced multitarget detection for end-edge-cloud surveillance in smart IoT</article-title>
        <source>IEEE Internet Things J.</source>
        <year>2021</year>
        <volume>8</volume>
        <issue>16</issue>
        <fpage>12588</fpage>
        <lpage>12596</lpage>
        <pub-id pub-id-type="doi">10.1109/JIOT.2021.3077449</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cremers</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Osher</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Soatto</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Kernel density estimation and intrinsic alignment for shape priors in level set segmentation</article-title>
        <source>Int. J. Comput. Vis.</source>
        <year>2006</year>
        <volume>69</volume>
        <issue>3</issue>
        <fpage>335</fpage>
        <lpage>351</lpage>
        <pub-id pub-id-type="doi">10.1007/s11263-006-7533-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Tao</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Adaptive optimal shape prior for easy interactive object segmentation</article-title>
        <source>IEEE Trans. Multimed.</source>
        <year>2015</year>
        <volume>17</volume>
        <issue>7</issue>
        <fpage>994</fpage>
        <lpage>1005</lpage>
        <pub-id pub-id-type="doi">10.1109/TMM.2015.2433795</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Wang, H., Zhang, H.: Adaptive shape prior in graph cut segmentation. In: 2010 IEEE International Conference on Image Pro- cessing. IEEE, pp 3029–3032 (2010). 10.1109/ICIP.2010.5653335</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Veksler, O.: Star shape prior for graph-cut image segmentation. In: European Conference on Computer Vision. Springer, pp 454–467 (2008). 10.1007/978-3-540-88690-7_34</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Nosrati, M. S., Hamarneh, G.: Incorporating prior knowledge in medical image segmentation: a survey (2021) [Online]. Available: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1607.01092">https://arxiv.org/abs/1607.01092</ext-link></mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>MCH</given-names>
          </name>
          <name>
            <surname>Petersen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Pawlowski</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Glocker</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Schaap</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>TeTrIS: template transformer networks for image segmentation with shape priors</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2019</year>
        <volume>38</volume>
        <issue>11</issue>
        <fpage>2596</fpage>
        <lpage>2606</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2019.2905990</pub-id>
        <pub-id pub-id-type="pmid">30908196</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Ravishankar, H., Venkataramani, R., Thiruvenkadam, S., Sudhakar, P., Vaidya, V.: Learning and incorporating shape models for semantic segmentation. In: International conference on medical image computing and computer-assisted intervention. Springer, pp 203–211 (2017). 10.1007/978-3-319-66182-7_24</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Avendi</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Kheradvar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Jafarkhani</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>A combined deep-learning and deformable-model approach to fully automatic segmentation of the left ventricle in cardiac MRI</article-title>
        <source>Med. Image Anal.</source>
        <year>2016</year>
        <volume>30</volume>
        <fpage>108</fpage>
        <lpage>119</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2016.01.005</pub-id>
        <pub-id pub-id-type="pmid">26917105</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ngo</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Carneiro</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Combining deep learning and level set for the automated segmentation of the left ventricle of the heart from cardiac cine magnetic resonance</article-title>
        <source>Med. Image Anal.</source>
        <year>2017</year>
        <volume>35</volume>
        <fpage>159</fpage>
        <lpage>171</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2016.05.009</pub-id>
        <pub-id pub-id-type="pmid">27423113</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Lung segmentation and automatic detection of COVID-19 using radiomic features from chest CT images</article-title>
        <source>Pattern Recogn.</source>
        <year>2021</year>
        <volume>119</volume>
        <fpage>108071</fpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2021.108071</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rosenfeld</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pfaltz</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>Sequential operations in digital picture processing</article-title>
        <source>J. ACM (JACM)</source>
        <year>1966</year>
        <volume>13</volume>
        <issue>4</issue>
        <fpage>471</fpage>
        <lpage>494</lpage>
        <pub-id pub-id-type="doi">10.1145/321356.321357</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shih</surname>
            <given-names>FY</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Y-T</given-names>
          </name>
        </person-group>
        <article-title>Fast Euclidean distance transformation in two scans using a 3x3 neighborhood</article-title>
        <source>Comput. Vis. Image Underst.</source>
        <year>2004</year>
        <volume>93</volume>
        <issue>2</issue>
        <fpage>195</fpage>
        <lpage>205</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cviu.2003.09.004</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
