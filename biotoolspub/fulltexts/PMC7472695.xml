<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neuroinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neuroinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5196</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7472695</article-id>
    <article-id pub-id-type="doi">10.3389/fninf.2020.00037</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Nutil: A Pre- and Post-processing Toolbox for Histological Rodent Brain Section Images</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Groeneboom</surname>
          <given-names>Nicolaas E.</given-names>
        </name>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1043357/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yates</surname>
          <given-names>Sharon C.</given-names>
        </name>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/601364/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Puchades</surname>
          <given-names>Maja A.</given-names>
        </name>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/318487/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bjaalie</surname>
          <given-names>Jan G.</given-names>
        </name>
        <xref ref-type="corresp" rid="c002">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/11/overview"/>
      </contrib>
    </contrib-group>
    <aff><institution>Neural Systems Laboratory, Institute of Basic Medical Sciences, University of Oslo</institution>, <addr-line>Oslo</addr-line>, <country>Norway</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: David A. Gutman, Emory University, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Sweyta Lohani, Yale University, United States; Bennett Allan Landman, Vanderbilt University, United States</p>
      </fn>
      <corresp id="c001">*Correspondence: Maja A. Puchades, <email>m.a.puchades@medisin.uio.no</email></corresp>
      <corresp id="c002">Jan G. Bjaalie, <email>j.g.bjaalie@medisin.uio.no</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>21</day>
      <month>8</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>14</volume>
    <elocation-id>37</elocation-id>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>6</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>7</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2020 Groeneboom, Yates, Puchades and Bjaalie.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Groeneboom, Yates, Puchades and Bjaalie</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>With recent technological advances in microscopy and image acquisition of tissue sections, further developments of tools are required for viewing, transforming, and analyzing the ever-increasing amounts of high-resolution data produced. In the field of neuroscience, histological images of whole rodent brain sections are commonly used for investigating brain connections as well as cellular and molecular organization in the normal and diseased brain, but present a problem for the typical neuroscientist with no or limited programming experience in terms of the pre- and post-processing steps needed for analysis. To meet this need we have designed <italic>Nutil</italic>, an open access and stand-alone executable software that enables automated transformations, post-processing, and analyses of 2D section images using multi-core processing (OpenMP). The software is written in C++ for efficiency, and provides the user with a clean and easy graphical user interface for specifying the input and output parameters. <italic>Nutil</italic> currently contains four separate tools: (1) A transformation toolchain named “Transform” that allows for rotation, mirroring and scaling, resizing, and renaming of very large tiled tiff images. (2) “TiffCreator” enables the generation of tiled TIFF images from other image formats such as PNG and JPEG. (3) A “Resize” tool completes the preprocessing toolset and allows downscaling of PNG and JPEG images with output in PNG format. (4) The fourth tool is a post-processing method called “Quantifier” that enables the quantification of segmented objects in the context of regions defined by brain atlas maps generated with the <italic>QuickNII</italic> software based on a 3D reference atlas (mouse or rat). The output consists of a set of report files, point cloud coordinate files for visualization in reference atlas space, and reference atlas images superimposed with color-coded objects. The <italic>Nutil</italic> software is made available by the Human Brain Project (<ext-link ext-link-type="uri" xlink:href="https://www.humanbrainproject.eu">https://www.humanbrainproject.eu</ext-link>) at <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/nutil/">https://www.nitrc.org/projects/nutil/</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>image processing</kwd>
      <kwd>rodent atlas</kwd>
      <kwd>brain</kwd>
      <kwd>QuickNII</kwd>
      <kwd>QUINT</kwd>
      <kwd>workflow</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">Horizon 2020 Framework Programme<named-content content-type="fundref-id">10.13039/100010661</named-content></funding-source>
        <award-id rid="cn001">720270</award-id>
        <award-id rid="cn001">785907</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="0"/>
      <equation-count count="0"/>
      <ref-count count="23"/>
      <page-count count="9"/>
      <word-count count="0"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <title>Introduction</title>
    <p>The process of changing data from one “raw” format to another to make the data suitable for analysis – often referred to as data wrangling or data transformation – is generally required when employing standardized analytical pipelines. A common example is the pre-processing of magnetic resonance data, involving a set of operations to “clean” the raw images before they can be analyzed and interpreted. In basic neuroscience, two-dimensional (2D) images depicting rodent histological brain sections are typical output of animal model studies designed to investigate brain structure, function, and disease. Large series of these images, generated with a broad repertoire of experimental techniques, serve as the starting point for quantification and mapping of features such as the distribution of molecules, presence of particular cell types, or connections between anatomical regions. The images commonly require transformation prior to analysis, but due to the sheer size and number of image files, transformations are difficult to execute and replicate for biological researchers with limited coding ability. Typically, researchers need access to technology packages such as <italic>Adobe Photoshop</italic><sup>TM</sup>, <italic>GIMP</italic><sup><xref ref-type="fn" rid="footnote1">1</xref></sup>, or <italic>NIH ImageJ</italic> (<xref rid="B19" ref-type="bibr">Schneider et al., 2012</xref>), but these all have their own limitations such as file size restrictions or lack of parallel processing support. Some basic scripting ability and access to data analysis software such as <italic>ImageMagick</italic> (<xref rid="B20" ref-type="bibr">The ImageMagick Development Team, 2020</xref>), <italic>R</italic> (<xref rid="B5" ref-type="bibr">Chambers, 2008</xref>) or <italic>Matlab</italic> (<xref rid="B13" ref-type="bibr">MATLAB, 2010</xref>) are useful. However, even with these tools and skills, transformations can be time consuming when applied to hundreds of images for whole brain comparative studies.</p>
    <p>With image pre-processing required for most analytic pipelines (<xref rid="B23" ref-type="bibr">Yates et al., 2019</xref>), there is a need for access to user-friendly tools that can perform the most commonly required transformations. Furthermore, for brain microscopy data, researchers typically endeavor to spatially analyze features in the images by sorting outputs according to anatomical brain region. In light of published 3D reference atlases for mouse and rat brains (<xref rid="B11" ref-type="bibr">Lein et al., 2007</xref>; <xref rid="B14" ref-type="bibr">Oh et al., 2014</xref>; <xref rid="B17" ref-type="bibr">Papp et al., 2014</xref>; <xref rid="B10" ref-type="bibr">Kjonigsen et al., 2015</xref>), and software for generating brain atlas maps that are customized to match the proportions and cutting plane of the sections (<xref rid="B18" ref-type="bibr">Puchades et al., 2019</xref>), we have created a pre- and post-processing toolbox for histological brain section images that aims to meet both these needs. An early beta version of the tool was an integral part of the QUINT workflow for quantification and spatial analysis of labeling in rodent brain sections (<xref rid="B23" ref-type="bibr">Yates et al., 2019</xref>), see <ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/watch?v=yPkAbSfla_c">https://www.youtube.com/watch?v=yPkAbSfla_c</ext-link>. Nutil has since been expanded and improved considerably with the present manuscript describing the full range of Nutil functionalities. Based on feedback from our users, we have implemented a proper graphical user interface (GUI) for entering analysis parameters and for selecting different options like i.e., object splitting or customized atlas regions. Nutil can also be used independently of the QUINT workflow to preprocess images in preparation for other downstream processes. It enables automated transformations such as rotation and scaling, cropping, resizing, and renaming; in addition to analytical post-processing of segmentations that are generated from the brain section images, based on input from customized reference atlas maps (<xref ref-type="fig" rid="F1">Figure 1</xref>). <italic>Nutil</italic> is designed specifically for operations on very large images and is therefore optimized for speed, memory usage and parallelization. The toolbox is intended for use in combination with one of the open source image analysis tools that are currently available, e.g., <italic>NIH ImageJ</italic> (<xref rid="B19" ref-type="bibr">Schneider et al., 2012</xref>) or ilastik (<xref rid="B2" ref-type="bibr">Berg et al., 2019</xref>).</p>
    <fig id="F1" position="float">
      <label>FIGURE 1</label>
      <caption>
        <p>The QUINT workflow utilizes three software tools to extract, quantify and spatially analyze labeled features in series of images of histological mouse or rat brain sections. Initially, the <italic>Nutil</italic> Transform operation is used to correct the orientation and serial order of the images, to rename the files, and to downscale the images. The images are then registered to a reference atlas with <italic>QuickNII</italic>, and segmented with an image analysis software such as <italic>ilastik</italic>. The atlas maps and segmentations are post-processed with <italic>Nutil</italic> Quantifier to extract quantifications per atlas region and coordinates for visualization in reference atlas space. Image Credit for 2D section images: Allen Institute.</p>
      </caption>
      <graphic xlink:href="fninf-14-00037-g001"/>
    </fig>
  </sec>
  <sec id="S2">
    <title>Overview of Functionality</title>
    <p><italic>Nutil</italic> aims to both simplify and streamline the mechanism of pre-and-post processing 2D brain image data. <italic>Nutil</italic> is developed as a stand-alone application for Windows with a GUI requiring little-to-no experience to execute. The user specifies the input and output parameters for the operations in a template in the GUI, which is saved as a text file (in .NUT format) prior to initiating the operation. Pre-processing operations include 2D transformations of extremely large tiled TIFF files (rotation, flipping, and scaling), in addition to renaming, copying, and downsizing (Transform). The tiled TIFF format was selected for the Transform operation as it allows for optimal memory usage, as well as being an output file type of microscope scanners. Some users may need to convert their images to tiled TIFF format to enable transformation with <italic>Nutil</italic> Transform, and so an image format converter (JPEG/PNG to tiled TIFF format) is included in the <italic>Nutil</italic> software (TiffCreator). To complete the set, <italic>Nutil</italic> also includes a resizing operation (Resize) that enables downscaling of PNG and JPEG images to PNG format. Post-processing with <italic>Nutil</italic> (Quantifier) is based on analysis of segmented images in the context of brain regions defined by a 3D reference atlas such as the Allen Adult Mouse brain reference atlas (<xref rid="B11" ref-type="bibr">Lein et al., 2007</xref>; <xref rid="B14" ref-type="bibr">Oh et al., 2014</xref>) or the Waxholm Space Atlas of the Sprague Dawley rat brain (<xref rid="B17" ref-type="bibr">Papp et al., 2014</xref>; <xref rid="B10" ref-type="bibr">Kjonigsen et al., 2015</xref>; <xref rid="B15" ref-type="bibr">Osen et al., 2019</xref>). All functions operate in batch mode, and operate in parallel on multiple CPUs.</p>
    <sec id="S2.SS1">
      <title>Transform</title>
      <p>The <italic>Nutil</italic> Transform function is seemingly straight-forward, since in reality it only applies a generic 2D TSR (Translation, Scaling, and Rotation) matrix to an image. However, due to the sheer size of the image files to be transformed, <italic>Nutil</italic> has been designed with several optimization methods to prevent memory problems and to increase execution speed. For example, a single image of size 70,000 × 50,000 pixels takes up almost 10 GB of memory, with a need for additional copies to be stored in memory during rotation. Even worse, if the user were to perform eight transformations on multiple cores, the amount of RAM required would exceed 160 GB.</p>
      <p>In order to solve the memory problem, <italic>Nutil</italic> Transform (rotate, flipping, and renaming) operates on tiled TIFF files only, with a separate TiffCreator tool included for converting JPEG/PNG images to the required tiled TIFF format. Tiled TIFF files are effectively compressed (JPEG/LZW) folders of smaller-sized images (tiles) that collectively make up the entire image. By operating (loading/releasing) on these tiles instead of an entire image, memory consumption is greatly reduced; however, this comes at the expense of increased code complexity, with a requirement to keep track of/sort stacks of recently used tiles. To enhance execution speed, <italic>Nutil</italic> is written in C++, which is highly optimized for low-level execution of memory-intensive parallel code.</p>
      <p>Nutil performs two kinds of downsampling of TIFF images: original scanner images that are large (usually around 30,000 × 18,000 and up to 80,000 × 70,000 pixels) to about 1–25% of the original area, and original images to very small thumbnails. Nutil does not currently support any form of antialiasing when downsampling TIFF images, only pointwise sampling. In its most basic form, single pixels are selected as representative for the area. <xref ref-type="supplementary-material" rid="FS1">Supplementary File 1</xref> summarizes Nutil operating times for image transformations in batch mode on several platforms.</p>
      <sec id="S2.SS1.SSS1">
        <title>Pseudocode Block</title>
        <list list-type="simple">
          <list-item>
            <label>1.</label>
            <p>Read input list of files with their corresponding 2D transform parameters.</p>
          </list-item>
          <list-item>
            <label>2.</label>
            <p>For each input file, verify that parameters are correct and image files exist.</p>
          </list-item>
          <list-item>
            <label>3.</label>
            <p>For each file (in separate threads) do:</p>
            <list list-type="simple">
              <list-item>
                <label>(a)</label>
                <p>Open the input TIFF file, verify that it is indeed tiled.</p>
              </list-item>
              <list-item>
                <label>(b)</label>
                <p>Calculate new file resolution and boundaries – rotating an image will alter the resolution.</p>
              </list-item>
              <list-item>
                <label>(c)</label>
                <p>Perform 2D transformation on each separate tile, keep an optimized list of recently accessed tiles for speed improvements. Also, simultaneously calculate new bounding boxes for the rotated image (auto-cropping step).</p>
              </list-item>
              <list-item>
                <label>(d)</label>
                <p>Save the rotated image to a temporary file, close file handle.</p>
              </list-item>
              <list-item>
                <label>(e)</label>
                <p>Create a new file with auto-cropped boundaries calculated in (c) and start copying from the temporary file to the final image.</p>
              </list-item>
              <list-item>
                <label>(f)</label>
                <p>If the thumbnails creation option is toggled, open the finished auto-cropped image and downsize it. Save as a.png (or.jpg) in a separate thumbnail folder. No anti-aliasing step is performed here.</p>
              </list-item>
            </list>
          </list-item>
        </list>
      </sec>
    </sec>
    <sec id="S2.SS2">
      <title>TiffCreator</title>
      <p>Even though the tiled TIFF format is useful, very few image services exist that are able to efficiently convert images to tiled TIFF format. Standard image software such as <italic>GIMP</italic> and <italic>Adobe Photoshop</italic> are certainly able to load tiled TIFFs (that are not too large), but are unable to save them. The conversion tool in <italic>ImageMagick</italic> can successfully create images, but require the knowledge of command line parameters in addition to being slow and non-parallelized. The <italic>Nutil</italic> TiffCreator function solves this problem by producing tiled TIFF files from JPEG or PNG images, and employs the support of multiple CPUs for efficient, parallelized operations. TiffCreator lets the user specify the input directory containing JPEG/PNG files and the output directory, in addition to some TIFF-specific parameters (size of sub-tiles, etc.).</p>
      <sec id="S2.SS2.SSS1">
        <title>Pseudocode Block</title>
        <list list-type="simple">
          <list-item>
            <label>1.</label>
            <p>Read input directory and create a list of all image files.</p>
          </list-item>
          <list-item>
            <label>2.</label>
            <p>For each file in the list</p>
            <list list-type="simple">
              <list-item>
                <label>(a)</label>
                <p>Load the image</p>
              </list-item>
              <list-item>
                <label>(b)</label>
                <p>Convert to a tiled TIFF format</p>
              </list-item>
              <list-item>
                <label>(c)</label>
                <p>Save the tiled TIFF</p>
              </list-item>
            </list>
          </list-item>
        </list>
      </sec>
    </sec>
    <sec id="S2.SS3">
      <title>Resize</title>
      <p>The <italic>Nutil</italic> Resize operation is a helpful post processing operation that allows the user to downscale PNG and JPEG files in bulk. An input directory, output directory, compression type and resize factor are specified in the GUI, and the program automatically identifies and transforms all files from the source directory to the target directory. The resize factor can be either a fixed-width number or a percentage of the original size. Nutil does not currently support enlargement of images as this serves no purpose in the context of histological image analysis. As with the other operations in <italic>Nutil</italic>, Resize operates in parallel and utilizes all (or as many as specified) cores on the system it is running.</p>
      <sec id="S2.SS3.SSS1">
        <title>Pseudocode Block</title>
        <p>For each file in the list</p>
        <list list-type="simple">
          <list-item>
            <label>1.</label>
            <p>Open the file</p>
          </list-item>
          <list-item>
            <label>2.</label>
            <p>Resize the file (using either bilinear filtering or no filtering)</p>
          </list-item>
          <list-item>
            <label>3.</label>
            <p>Save the file</p>
          </list-item>
        </list>
      </sec>
    </sec>
    <sec id="S2.SS4">
      <title>Quantifier</title>
      <p>Quantifier is a post processing operation for the extraction, quantification and spatial analysis of labeling in 2D rodent brain section images (for example, immunohistochemical labeling). Nutil contains all the necessary label files for analysis of features in the following atlases: Allen brain atlas Common Coordinate Framework v3, 2015 and 2017 versions (<sup>®</sup>2004 Allen Institute for Brain Science. Allen Mouse Brain Atlas. Available from: <ext-link ext-link-type="uri" xlink:href="http://download.alleninstitute.org/informatics-archive/current-release/mouse_ccf/annotation/">http://download.alleninstitute.org/informatics-archive/current-release/mouse_ccf/annotation/</ext-link>) (<xref rid="B11" ref-type="bibr">Lein et al., 2007</xref>; <xref rid="B14" ref-type="bibr">Oh et al., 2014</xref>) and the Waxholm Space Atlas of the Sprague Dawley rat brain v2 and v3 (Available from: <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/whs-sd-atlas">https://www.nitrc.org/projects/whs-sd-atlas</ext-link>) (<xref rid="B17" ref-type="bibr">Papp et al., 2014</xref>; <xref rid="B10" ref-type="bibr">Kjonigsen et al., 2015</xref>; <xref rid="B15" ref-type="bibr">Osen et al., 2019</xref>). To run Quantifier, <italic>Nutil</italic> requires segmentations that are generated from the brain section images, with the labeling of interest displayed in a unique RGB color (Red Green Blue color model) and reference atlas maps customized to match the proportions and cutting plane of the sections (generated with the QuickNII software, available from: <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/quicknii">https://www.nitrc.org/projects/quicknii</ext-link>) (<xref rid="B18" ref-type="bibr">Puchades et al., 2019</xref>). Quantifications may be performed on the entire image or in regions defined by masks. <italic>Nutil</italic> quantifies labeling in each parcellated brain region, extracts spatial coordinates for visualization in 3D reference atlas space, and generates output including: reports (in CSV or HTML format), coordinates (in JSON format), and reference atlas map images superimposed with the extracted features that are color-coded according to their assigned anatomical location (<xref rid="B23" ref-type="bibr">Yates et al., 2019</xref>).</p>
      <p>Quantifier starts by applying a breadth-first search (BFS) method to identify areas of interest in a series of images as specified by colors defined in the <italic>Nutil</italic> GUI. This method proceeds by separating foreground areas (of interest) from background areas, before calculating various statistical measures (area, centroid, shape). When this process is complete, Quantifier continues by anchoring each identified object to a specific brain region by looking up the individual pixel coordinates in the anchoring map provided by the <italic>QuickNII</italic> output data. If an area overlaps N regions of the brain, the area is either assigned to one of the regions at random (with object splitting switched OFF) or split into N distinct areas with IDs coupled to their respective regions (with object splitting switched ON). In addition, various filters can be applied such as minimum and maximum object size, which are useful for removing noise. Functionality tailored specifically for connectivity data has been implemented, with support for masks to enable differentiation of connections in the right and left hemispheres. The object splitting feature was also implemented for connectivity data as connections typically span multiple atlas regions. With object splitting switched ON each object pixel is registered to its respective atlas region. However, users will typically want to switch this feature OFF for small objects such as cells, to enable accurate counting. Object splitting invalidates the object counts as objects are split at region boundaries.</p>
      <p>The output results are assembled based on information provided in the <italic>Nutil</italic> GUI, which includes the option to define customized regions (ensembles of reference brain region IDs) together with a name (“Cortex”) and color (“red”) (via an Excel template; see <xref ref-type="supplementary-material" rid="FS1">Supplementary File 2</xref>). This enables the user to constrain the output data to specific areas of the brain (e.g., “Amygdala,”, “Hippocampus,” “Primary Somatosensory Cortex”). Report outputs are written as a series of files in CSV or HTML format containing report information per section, and for all sections combined (global information), and are organized first by all the regions listed in the relevant reference atlas (e.g., Allen adult mouse brain reference atlas) and second by the optional user-defined custom regions. Moreover, <italic>Nutil</italic> generates 3D point clouds, colorized based on report specifications that may be visualized in the online <italic>MeshView</italic> atlas viewer<sup><xref ref-type="fn" rid="footnote2">2</xref></sup>. The point clouds are exported in a simple JSON-formatted text file that is compatible with Python, containing a raw list of 3D coordinates collected together with color-coding based on the input report. Support for specifying the required point cloud density has been implemented to enable the extraction of coordinates for datasets with very large objects (such as those typical of connectivity data, where a user may, for example, request extraction of one coordinate per 5th object pixel).</p>
      <sec id="S2.SS4.SSS1">
        <title>Pseudocode Block</title>
        <list list-type="simple">
          <list-item>
            <label>1.</label>
            <p>For each input file, verify that parameters are correct and that the files in the file list exist.</p>
          </list-item>
          <list-item>
            <label>2.</label>
            <p>Create output directories.</p>
          </list-item>
          <list-item>
            <label>3.</label>
            <p>For each slice (in separate threads) do:</p>
          </list-item>
          <list-item>
            <label>4.</label>
            <p>Open segmented image.</p>
          </list-item>
          <list-item>
            <label>5.</label>
            <p>Apply masks, calculate some statistics.</p>
          </list-item>
          <list-item>
            <label>6.</label>
            <p>Perform a breadth-first algorithm that counts the number of areas, and place them into an object list.</p>
          </list-item>
          <list-item>
            <label>7.</label>
            <p>Open the corresponding atlas slice and assign each area to a unique atlas ID.</p>
          </list-item>
          <list-item>
            <label>8.</label>
            <p>Calculate various area statistics.</p>
          </list-item>
          <list-item>
            <label>9.</label>
            <p>Create a visual.png file by merging the segmented image file with the atlas image file, and add label text and colors to the identified areas.</p>
          </list-item>
          <list-item>
            <label>10.</label>
            <p>When all threads have completed, combine areas into one object and calculate both individual slice and global statistics.</p>
          </list-item>
          <list-item>
            <label>11.</label>
            <p>Write reports:</p>
          </list-item>
          <list-item>
            <label>12.</label>
            <p>Individual slice reports.</p>
          </list-item>
          <list-item>
            <label>13.</label>
            <p>Combined area reports.</p>
          </list-item>
          <list-item>
            <label>14.</label>
            <p>Write 3D data (perform an inverse matrix operation to get back to atlas space).</p>
          </list-item>
          <list-item>
            <label>15.</label>
            <p>Clean up memory and temporary files.</p>
          </list-item>
        </list>
      </sec>
    </sec>
  </sec>
  <sec id="S3">
    <title>Technical Specifications</title>
    <p><italic>Nutil</italic> is written in C++ using standard Qt<sup><xref ref-type="fn" rid="footnote3">3</xref></sup> libraries, and is optimized for parallel operations on multiple CPUs. <italic>Nutil</italic> is downloaded as a zip archive file and can be extracted and run anywhere on the computer. No installation executable is necessary, and the directory can be moved around the file system as required. Settings data are stored in the local program folder. Nutil does not currently utilize any GPU extensions. The external libraries that are used in <italic>Nutil</italic> are:</p>
    <list list-type="simple">
      <list-item>
        <label>1.</label>
        <p>Libtiff for fast and efficient TIFF file handling<sup><xref ref-type="fn" rid="footnote4">4</xref></sup></p>
      </list-item>
      <list-item>
        <label>2.</label>
        <p>LibXLNT for Excel file I/O<sup><xref ref-type="fn" rid="footnote5">5</xref></sup></p>
      </list-item>
    </list>
    <sec id="S3.SS1">
      <title>Hosting and Updates</title>
      <p><italic>Nutil</italic> is available from two locations:</p>
      <list list-type="simple">
        <list-item>
          <label>1.</label>
          <p>Windows binaries (no installer required): <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/nutil/">https://www.nitrc.org/projects/nutil/</ext-link></p>
        </list-item>
        <list-item>
          <label>2.</label>
          <p>Source code: <ext-link ext-link-type="uri" xlink:href="https://github.com/leuat/nutil/">https://github.com/leuat/nutil/</ext-link></p>
        </list-item>
      </list>
    </sec>
    <sec id="S3.SS2">
      <title>Hardware Requirements</title>
      <p><italic>Nutil</italic> is a stand-alone program that can be executed on either a server, desktop or laptop computer. <italic>Nutil</italic> will employ all the cores that are available to your current system. While there are no specific hardware limitations, the processing time is dependent on the system’s compute power. The more cores and memory you have available, the faster the operations will be performed. However, running <italic>Nutil</italic> on a single-core laptop is also possible. This way, we allow the user to not be constrained by any hardware restrictions, as Nutil should be able to run on almost any computer.</p>
    </sec>
  </sec>
  <sec id="S4">
    <title>Use Cases</title>
    <sec id="S4.SS1">
      <title>Use Case Material</title>
      <p>Three mouse brain image series were exported from the Allen Brain Atlas Data Portal with a Python script. The images were exported in JPEG format, and served as the test series for the TiffCreator, Transform, and Quantifier functions. The first series (use case A) included 20 sagittal mouse brain sections from the left hemisphere labeled for parvalbumin by <italic>in situ</italic> hybridization, and may be viewed here: <ext-link ext-link-type="uri" xlink:href="http://mouse.brain-map.org/experiment/show/75457579">http://mouse.brain-map.org/experiment/show/75457579</ext-link> (Specimen 06-0419, Probe RP_060523_03_E07; <sup>®</sup>2004 Allen Institute for Brain Science). The second <italic>in situ</italic> hybridization series (use case B) include 57 coronal sections labeled for parvalbumin<sup><xref ref-type="fn" rid="footnote6">6</xref></sup> (Specimen 335-1125, Probe 071204_01_E06; <sup>®</sup>2004 Allen Institute for Brain Science. Allen Mouse Brain Atlas. Available from: <ext-link ext-link-type="uri" xlink:href="mouse.brain-map.org">mouse.brain-map.org</ext-link>) (<xref rid="B11" ref-type="bibr">Lein et al., 2007</xref>; <xref rid="B14" ref-type="bibr">Oh et al., 2014</xref>). The third series (use case C) was a connectivity dataset displaying projections from the primary somatosensory mouth area. Nine images were selected for the use case and can be viewed here: <ext-link ext-link-type="uri" xlink:href="https://connectivity.brain-map.org/projection/experiment/112936582">https://connectivity.brain-map.org/projection/experiment/112936582</ext-link> (Mouse strain: C57BL/6J, Tracer type: EGFP; <sup>®</sup>2011 Allen Institute for Brain Science. Allen Mouse Brain Connectivity Atlas).</p>
    </sec>
    <sec id="S4.SS2">
      <title>TiffCreator and Transform</title>
      <p>TiffCreator enables batch conversion of JPEG/PNG images to the tiled TIFF format that is compatible with the Transform function. Transform then allows the user to perform a variety of image pre-processing steps such as scaling, rotation, and renaming, which are prerequisites for analysis by the method described in the other use cases (illustrated in <xref ref-type="fig" rid="F2">Figure 2</xref>), but also for analysis by other means. For both TiffCreator and Transform, all the input and output parameters were entered in the <italic>Nutil</italic> GUI, and the desired operation run in batch mode. As <italic>Nutil</italic> Transform operates on tiled TIFF images only, the first operation for all the use cases was to convert the JPEG images to tiled TIFF using TiffCreator. Next, Transform was used to rename the files – to make them compatible with <italic>QuickNII</italic> and <italic>Nutil</italic> Quantifier – and to create the thumbnails required for registration to reference atlas space. As the images were well-organized and small in size, conversion from tiled TIFF to PNG was the only additional step required before segmentation with <italic>ilastik</italic> (<xref rid="B2" ref-type="bibr">Berg et al., 2019</xref>).</p>
      <fig id="F2" position="float">
        <label>FIGURE 2</label>
        <caption>
          <p>The Nutil Transform operation takes series of images as input, and outputs copies of the images that have been formatted based on parameters defined by the user in the <italic>Nutil</italic> GUI. Nutil Transform includes the option to rotate and mirror, rename, crop, resize, and convert file type. Image Credit for 2D section images: Allen Institute.</p>
        </caption>
        <graphic xlink:href="fninf-14-00037-g002"/>
      </fig>
      <p>Note that all <italic>Nutil</italic> Transform operations – such as rotation, scaling and thumbnail creation – may be implemented in one process or separately. The thumbnail feature is particularly useful for visualizing the result of a transformation without having to open large images, as it allows the application of a resize factor with image export in PNG format, in addition to export of the original size transformed images. For rotation, the user may choose any angle and define the background color (i.e., black or white) in the new image. The rotation feature includes an auto-cropping function, which ensures the removal of redundant pixels. The <italic>Nutil</italic> GUI has an inbuilt user manual, accessible via help buttons. A user manual is also part of the <italic>Nutil</italic> software package and will be updated with new releases.</p>
    </sec>
    <sec id="S4.SS3">
      <title>Quantifier: <italic>In situ</italic> Hybridization Dataset (Use Cases A and B)</title>
      <p>Quantifier allows the quantification and spatial analysis of labeling in series of mouse or rat brain section images based on input from segmentations and customized atlas maps, both generated from the section images (<xref ref-type="fig" rid="F3">Figure 3</xref>). Quantifier was used to extract, quantify and assign anatomical location to parvalbumin positive cells that were extracted from the image series that is described in section “Use Case Material”. The images were pre-processed with <italic>Nutil</italic> TiffCreator and Transform as described above. For the sagittal parvalbumin example, derived input files (customized atlas maps and segmentations) and output files (Csv reports, point cloud, and atlas map images with superimposed features) are available for download from the EBRAINS KnowledgeGraph database ( doi: 10.25493/6DYS-M3W)<sup><xref ref-type="fn" rid="footnote7">7</xref></sup>.</p>
      <fig id="F3" position="float">
        <label>FIGURE 3</label>
        <caption>
          <p>The <italic>Nutil</italic> Quantifier operation takes atlas maps and segmentations as input, and outputs reports with quantifications per atlas region, atlas maps with superimposed color-coded objects and point clouds for visualization in reference atlas space. The <italic>Nutil</italic> GUI supports specification of parameters such as color for extraction, minimum and maximum object size, and coordinate file requirements. The GUI includes an in build user manual accessible via help buttons.</p>
        </caption>
        <graphic xlink:href="fninf-14-00037-g003"/>
      </fig>
      <p>To generate the atlas maps, the mouse brain section images were registered to Allen Adult Mouse Brain Reference Atlas space (Common Coordinate Framework version 3, 2015) with the <italic>QuickNII</italic> software version 2.1 (Available from: <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/quicknii">https://www.nitrc.org/projects/quicknii</ext-link>) (<xref ref-type="fig" rid="F3">Figure 3</xref>, Input 1). In a parallel procedure, the section images were classified with the Pixel Classification workflow in the <italic>ilastik</italic> software to identify the parvalbumin positive cells (version 1.3.2rc1; available from: <ext-link ext-link-type="uri" xlink:href="https://www.ilastik.org">https://www.ilastik.org</ext-link>). Segmentations were generated in PNG format, with RGB color value (0, 0, 0) (black) applied to the class representing the positive cells (<xref ref-type="fig" rid="F3">Figure 3</xref>, Input 2). The Quantifier template in the <italic>Nutil</italic> GUI was populated with the input and output directories, as well as parameters such as minimum and maximum object size and coordinate file requirements. Object splitting was switched OFF. Quantifier was subsequently applied to all the images in batch mode. The point cloud was visualized with the <italic>MeshView</italic> Atlas Viewer (AMBA version 3 2015, available from: <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/meshview">https://www.nitrc.org/projects/meshview</ext-link>). The same procedure was repeated for use case B, and the datasets were co-visualized in 3D (<xref ref-type="fig" rid="F4">Figure 4</xref>). The processing of the dataset with Nutil Quantifier on a standard laptop with 2 to 4 cores and 16 GB of memory took less than 10 min, including selection of the files and analysis parameters in the GUI interface.</p>
      <fig id="F4" position="float">
        <label>FIGURE 4</label>
        <caption>
          <p>By processing several image series with the QUINT workflow, point clouds representing labeled features can be covisualized in reference atlas space allowing comparisons of expression patterns. Here, parvalbumin positive cells are extracted from two mouse brain section series: one cut in the sagittal plane <bold>(A)</bold> and the other in the coronal plane <bold>(B)</bold>. The cells are covisualized in Allen Adult Mouse Brain Reference atlas space with the <italic>MeshView</italic> atlas viewer. Likewise, the connectivity use case <bold>(C)</bold> displays projections from the primary somatosensory mouth area, labeling in all brain areas is quantified and visualized in 3D with differential color coding dependent on the brain region (neocortex, red; corpus callosum, pink; striatum, black). Image Credit for 2D section images: Allen Institute. Series: <ext-link ext-link-type="uri" xlink:href="http://mouse.brain-map.org/experiment/show/75457579">http://mouse.brain-map.org/experiment/show/75457579</ext-link>, <ext-link ext-link-type="uri" xlink:href="http://mouse.brain-map.org/experiment/show/79556738">http://mouse.brain-map.org/experiment/show/79556738</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://connectivity.brain-map.org/projection/experiment/112936582">https://connectivity.brain-map.org/projection/experiment/112936582</ext-link>.</p>
        </caption>
        <graphic xlink:href="fninf-14-00037-g004"/>
      </fig>
    </sec>
    <sec id="S4.SS4">
      <title>Quantifier: Connectivity Dataset (Use Case C)</title>
      <p>The connectivity dataset was preprocessed with Nutil Transform, segmented with <italic>ilastik</italic> and registered to Allen Adult Mouse Brain Reference Atlas space (Common Coordinate Framework version 3, 2015) with <italic>QuickNII</italic> in the same way as described for the <italic>in situ</italic> hybridization datasets. The dataset was processed with Nutil Quantifier with object splitting switched ON, with masks to differentiate connections in the right and left hemisphere and with a reduced point cloud density to enable visualization in 3D (<xref ref-type="fig" rid="F4">Figure 4</xref>).</p>
    </sec>
    <sec id="S4.SS5">
      <title>Quantifier Validation</title>
      <p>To validate the use case output, we compared the results obtained with <italic>Nutil</italic> Quantifier for the <italic>in situ</italic> hybridization dataset with the raw expression values provided by the Allen Brain Institute for the same dataset and brain regions. Despite considerable differences in quantification methods (<xref rid="B4" ref-type="bibr">Bohland et al., 2010</xref>) and minor differences in the atlas registration, the outputs were comparable as demonstrated in <xref ref-type="supplementary-material" rid="FS3">Supplementary File 3</xref>.</p>
      <p>A proper validation of <italic>Nutil</italic> Quantifier was done with a synthetic dataset of two sections, each with a set number of objects of known size (in pixels) and anatomical location. This dataset confirms that the quantification data delivered by <italic>Nutil</italic> Quantifier are correct (<xref ref-type="supplementary-material" rid="FS3">Supplementary File 4</xref>). Further validation is provided in <xref rid="B23" ref-type="bibr">Yates et al. (2019)</xref>.</p>
    </sec>
  </sec>
  <sec id="S5">
    <title>Discussion</title>
    <p>With the development of microscopes and scanners for whole section imaging and volumetric data acquisition, scientists are able to collect high-resolution images of different organs including the brain. The sheer size of these images has necessitated the development of new software tools for viewing, transforming and analyzing imaging data; but are typically only available commercially, and often with restrictions in terms of functionality that do not promote optimal analysis. For human brains, methods for 2D image reconstruction from multiple blocks have allowed researchers to conduct multimodal imaging studies (<xref rid="B8" ref-type="bibr">Hashimoto et al., 2016</xref>). There are also some open-access tools such as “HistoStitcher” (<xref rid="B6" ref-type="bibr">Chappelow et al., 2011</xref>) that enable specific pre-processing operations like reassembly of tissue fragments. Most pre-processing steps may be done with scripts, many available online; however, for a neuroscientist with limited programming experience, pre-processing can be a slow and painful process. There is a need for open-access tools that operate without the need for coding expertise with functionality tailored specifically for series of histological brain section images.</p>
    <p>With <italic>Nutil</italic>, neuroscientists will find a user-friendly and convenient tool, specifically designed with serial rodent brain sections in mind. It enables image transformations in an automated batch operation, making downstream image analysis accessible to the wider community. In a separate post-processing operation, <italic>Nutil</italic> allows the quantification and spatial analysis of labeling in 2D image series of rodent brain sections, when used in combination with <italic>QuickNII</italic> (<xref rid="B18" ref-type="bibr">Puchades et al., 2019</xref>) – an open-access software for registering section images to reference atlases – in addition to an image segmentation tool such as <italic>NIH ImageJ</italic> (<xref rid="B19" ref-type="bibr">Schneider et al., 2012</xref>) or <italic>ilastik</italic> (<xref rid="B2" ref-type="bibr">Berg et al., 2019</xref>). Several groups have developed custom codes and workflows along the same lines, for the analysis of whole brain volumes by registration to reference atlases. In a recent study, <xref rid="B9" ref-type="bibr">Kim et al. (2017)</xref> used the 3D Allen mouse reference atlas (<xref rid="B11" ref-type="bibr">Lein et al., 2007</xref>; <xref rid="B14" ref-type="bibr">Oh et al., 2014</xref>) for quantification of several interneuronal types in serial two-photon tomography data. By registering all datasets to the same reference space, they were able to discover sex differences in specific atlas sub-regions. Other studies rely on block-face photography (<xref rid="B21" ref-type="bibr">Vandenberghe et al., 2016</xref>) or 3D reconstruction (<xref rid="B12" ref-type="bibr">Majka et al., 2012</xref>). Likewise, <italic>Nutil</italic> has the potential to contribute to discoveries based on histological experimental data without the need for 3D volumes or 3D reconstruction. Several other groups have proposed tools for analysis of 2D mouse brain section images, without the need for 3D reconstruction (<xref rid="B7" ref-type="bibr">Furth et al., 2018</xref>; <xref rid="B22" ref-type="bibr">Xiong et al., 2018</xref>); however, the requirement for coding knowledge restricts the accessibility of these methods. The AIDAhisto tool enable atlas based quantifications but lack multi-angle adjustment (<xref rid="B16" ref-type="bibr">Pallast et al., 2019</xref>).</p>
    <p>In contrast to the cited studies, <italic>Nutil</italic> Quantifier may be applied to both mouse and rat brain data, as both mouse and rat brain reference atlases are inbuilt in the <italic>Nutil</italic> software, with <italic>QuickNII</italic> versions available for both species. <italic>Nutil</italic> Quantifier supports quantification of both small objects such as cells, typical of immunohistochemistry and <italic>in situ</italic> hybridization data, and the exploration of large objects that span multiple atlas regions, such as those typical of connectivity data. There is also a feature for combining masks with the atlas maps, to allow comparative analysis of right and left hemisphere expression. As the TiffCreator, Transform, Resize, and Quantifier operations run independently, TiffCreator, Transform, and Resize may be used to process any type of image, and can be incorporated into workflows that do not include the Quantifier operation. However, the pre-processing package is ideally suited for use with Quantifier; with all operations running on multiple CPUs, which drastically increases processing speed, making them ideal for batch processing of section images. By combining information from segmented images with corresponding atlas maps produced with <italic>QuickNII</italic> (<xref rid="B18" ref-type="bibr">Puchades et al., 2019</xref>; <xref rid="B23" ref-type="bibr">Yates et al., 2019</xref>), several datasets may be analyzed and then compared (<xref ref-type="fig" rid="F4">Figure 4</xref>), offering new analytical possibilities as discussed in depth in <xref rid="B3" ref-type="bibr">Bjerke et al. (2018)</xref>. The present developments are part of a larger European effort to establish tools and services for neuroscience research (<xref rid="B1" ref-type="bibr">Amunts et al., 2019</xref>).</p>
  </sec>
  <sec sec-type="data-availability" id="S6">
    <title>Data Availability Statement</title>
    <p>Publicly available datasets were analyzed in this study. This data can be found here: <ext-link ext-link-type="uri" xlink:href="http://mouse.brain-map.org/experiment/show/75457579">http://mouse.brain-map.org/experiment/show/75457579</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.25493/6DYS-M3W">10.25493/6DYS-M3W</ext-link>.</p>
  </sec>
  <sec id="S7">
    <title>Author Contributions</title>
    <p>NG created the <italic>Nutil</italic> software, contributed to the writing of the technical parts of the manuscript, and to the design of the validation studies. SY performed <italic>Nutil</italic> analyses, performed the validation studies, contributed to the development of the <italic>Nutil</italic> software, and contributed to writing the manuscript. MP conceived the study, supervised the analysis and the development of <italic>Nutil</italic>, and wrote the manuscript. JB conceived the study, supervised development of software tools, contributed with infrastructure, and contributed to writing of the manuscript. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> This research was supported by the EBRAINS research infrastructure, funded from the European Union’s Horizon 2020 Framework Programme for Research and Innovation under the Specific Grant Agreement No. 785907 (Human Brain Project SGA2) and Specific Grant Agreement No. 945539 (Human Brain Project SGA3).</p>
    </fn>
  </fn-group>
  <ack>
    <p>We thank Gergely Csucs and Dmitri Darine for discussions and expert technical assistance.</p>
  </ack>
  <fn-group>
    <fn id="footnote1">
      <label>1</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="http://www.gimp.org">www.gimp.org</ext-link>
      </p>
    </fn>
    <fn id="footnote2">
      <label>2</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/meshview">https://www.nitrc.org/projects/meshview</ext-link>
      </p>
    </fn>
    <fn id="footnote3">
      <label>3</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="https://www.qt.io/">https://www.qt.io/</ext-link>
      </p>
    </fn>
    <fn id="footnote4">
      <label>4</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="http://www.libtiff.org/">http://www.libtiff.org/</ext-link>
      </p>
    </fn>
    <fn id="footnote5">
      <label>5</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/tfussell/xlnt/">https://github.com/tfussell/xlnt/</ext-link>
      </p>
    </fn>
    <fn id="footnote6">
      <label>6</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="http://mouse.brain-map.org/experiment/show/79556738">http://mouse.brain-map.org/experiment/show/79556738</ext-link>
      </p>
    </fn>
    <fn id="footnote7">
      <label>7</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="https://kg.ebrains.eu/search/instances/Dataset/8e8ac473-08fc-4510-bf81-e6d94c9c15d6">https://kg.ebrains.eu/search/instances/Dataset/8e8ac473-08fc-4510-bf81-e6d94c9c15d6</ext-link>
      </p>
    </fn>
  </fn-group>
  <sec id="S10" sec-type="supplementary material">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fninf.2020.00037/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fninf.2020.00037/full#supplementary-material</ext-link></p>
    <supplementary-material content-type="local-data" id="FS1">
      <media xlink:href="Data_Sheet_1.PDF">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="FS2">
      <media xlink:href="Data_Sheet_2.PDF">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="FS3">
      <media xlink:href="Data_Sheet_3.PDF">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="TS1">
      <media xlink:href="Table_1.XLSX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amunts</surname><given-names>K.</given-names></name><name><surname>Knoll</surname><given-names>A. C.</given-names></name><name><surname>Lippert</surname><given-names>T.</given-names></name><name><surname>Pennartz</surname><given-names>C. M. A.</given-names></name><name><surname>Ryvlin</surname><given-names>P.</given-names></name><name><surname>Destexhe</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>The Human Brain Project-Synergy between neuroscience, computing, informatics, and brain-inspired technologies.</article-title>
<source><italic>PLoS Biol.</italic></source>
<volume>17</volume>:<issue>e3000344</issue>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3000344</pub-id>
<?supplied-pmid 31260438?><pub-id pub-id-type="pmid">31260438</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>S.</given-names></name><name><surname>Kutra</surname><given-names>D.</given-names></name><name><surname>Kroeger</surname><given-names>T.</given-names></name><name><surname>Straehle</surname><given-names>C. N.</given-names></name><name><surname>Kausler</surname><given-names>B. X.</given-names></name><name><surname>Haubold</surname><given-names>C.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>ilastik: interactive machine learning for (bio)image analysis.</article-title>
<source><italic>Nat. Methods</italic></source>
<volume>16</volume>
<fpage>1226</fpage>–<lpage>1232</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0582-9</pub-id>
<?supplied-pmid 31570887?><pub-id pub-id-type="pmid">31570887</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bjerke</surname><given-names>I. E.</given-names></name><name><surname>Ovsthus</surname><given-names>M.</given-names></name><name><surname>Papp</surname><given-names>E. A.</given-names></name><name><surname>Yates</surname><given-names>S. C.</given-names></name><name><surname>Silvestri</surname><given-names>L.</given-names></name><name><surname>Fiorilli</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Data integration through brain atlasing: human brain project tools and strategies.</article-title>
<source><italic>Eur. Psychiatry</italic></source>
<volume>50</volume>
<fpage>70</fpage>–<lpage>76</lpage>. <pub-id pub-id-type="doi">10.1016/j.eurpsy.2018.02.004</pub-id>
<?supplied-pmid 29519589?><pub-id pub-id-type="pmid">29519589</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bohland</surname><given-names>J. W.</given-names></name><name><surname>Bokil</surname><given-names>H.</given-names></name><name><surname>Pathak</surname><given-names>S. D.</given-names></name><name><surname>Lee</surname><given-names>C. K.</given-names></name><name><surname>Ng</surname><given-names>L.</given-names></name><name><surname>Lau</surname><given-names>C.</given-names></name><etal/></person-group> (<year>2010</year>). <article-title>Clustering of spatial gene expression patterns in the mouse brain and comparison with classical neuroanatomy.</article-title>
<source><italic>Methods</italic></source>
<volume>50</volume>
<fpage>105</fpage>–<lpage>112</lpage>. <pub-id pub-id-type="doi">10.1016/j.ymeth.2009.09.001</pub-id>
<?supplied-pmid 19733241?><pub-id pub-id-type="pmid">19733241</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chambers</surname><given-names>J. M.</given-names></name></person-group> (<year>2008</year>). <source><italic>Software for Data Analysis: Programming with R.</italic></source>
<publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chappelow</surname><given-names>J.</given-names></name><name><surname>Tomaszewski</surname><given-names>J. E.</given-names></name><name><surname>Feldman</surname><given-names>M.</given-names></name><name><surname>Shih</surname><given-names>N.</given-names></name><name><surname>Madabhushi</surname><given-names>A.</given-names></name></person-group> (<year>2011</year>). <article-title>HistoStitcher((c)): an interactive program for accurate and rapid reconstruction of digitized whole histological sections from tissue fragments.</article-title>
<source><italic>Comput. Med. Imaging Graph</italic></source>
<volume>35</volume>
<fpage>557</fpage>–<lpage>567</lpage>. <pub-id pub-id-type="doi">10.1016/j.compmedimag.2011.01.010</pub-id>
<?supplied-pmid 21397459?><pub-id pub-id-type="pmid">21397459</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furth</surname><given-names>D.</given-names></name><name><surname>Vaissiere</surname><given-names>T.</given-names></name><name><surname>Tzortzi</surname><given-names>O.</given-names></name><name><surname>Xuan</surname><given-names>Y.</given-names></name><name><surname>Martin</surname><given-names>A.</given-names></name><name><surname>Lazaridis</surname><given-names>I.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>An interactive framework for whole-brain maps at cellular resolution.</article-title>
<source><italic>Nat. Neurosci.</italic></source>
<volume>21</volume>
<fpage>139</fpage>–<lpage>149</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-017-0027-7</pub-id>
<?supplied-pmid 29203898?><pub-id pub-id-type="pmid">29203898</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hashimoto</surname><given-names>N.</given-names></name><name><surname>Bautista</surname><given-names>P. A.</given-names></name><name><surname>Haneishi</surname><given-names>H.</given-names></name><name><surname>Snuderl</surname><given-names>M.</given-names></name><name><surname>Yagi</surname><given-names>Y.</given-names></name></person-group> (<year>2016</year>). <article-title>Development of a 2D image reconstruction and viewing system for histological images from multiple tissue blocks: towards high-resolution whole-organ 3D histological images.</article-title>
<source><italic>Pathobiology</italic></source>
<volume>83</volume>
<fpage>127</fpage>–<lpage>139</lpage>. <pub-id pub-id-type="doi">10.1159/000443278</pub-id>
<?supplied-pmid 27100217?><pub-id pub-id-type="pmid">27100217</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>G. R.</given-names></name><name><surname>Pradhan</surname><given-names>K.</given-names></name><name><surname>Venkataraju</surname><given-names>K. U.</given-names></name><name><surname>Bota</surname><given-names>M.</given-names></name><name><surname>Garcia Del</surname></name><etal/></person-group> (<year>2017</year>). <article-title>Brain-wide maps reveal stereotyped cell-type-based cortical architecture and subcortical sexual dimorphism.</article-title>
<source><italic>Cell</italic></source>
<volume>171</volume>
<fpage>456</fpage>–<lpage>469.e22</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2017.09.020</pub-id>
<?supplied-pmid 28985566?><pub-id pub-id-type="pmid">28985566</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kjonigsen</surname><given-names>L. J.</given-names></name><name><surname>Lillehaug</surname><given-names>S.</given-names></name><name><surname>Bjaalie</surname><given-names>J. G.</given-names></name><name><surname>Witter</surname><given-names>M. P.</given-names></name><name><surname>Leergaard</surname><given-names>T. B.</given-names></name></person-group> (<year>2015</year>). <article-title>Waxholm Space atlas of the rat brain hippocampal region: three-dimensional delineations based on magnetic resonance and diffusion tensor imaging.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>108</volume>
<fpage>441</fpage>–<lpage>449</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.12.080</pub-id>
<?supplied-pmid 25585022?><pub-id pub-id-type="pmid">25585022</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lein</surname><given-names>E. S.</given-names></name><name><surname>Hawrylycz</surname><given-names>M. J.</given-names></name><name><surname>Ao</surname><given-names>N.</given-names></name><name><surname>Ayres</surname><given-names>M.</given-names></name><name><surname>Bensinger</surname><given-names>A.</given-names></name><name><surname>Bernard</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2007</year>). <article-title>Genome-wide atlas of gene expression in the adult mouse brain.</article-title>
<source><italic>Nature</italic></source>
<volume>445</volume>
<fpage>168</fpage>–<lpage>176</lpage>. <pub-id pub-id-type="doi">10.1038/nature05453</pub-id>
<?supplied-pmid 17151600?><pub-id pub-id-type="pmid">17151600</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Majka</surname><given-names>P.</given-names></name><name><surname>Kublik</surname><given-names>E.</given-names></name><name><surname>Furga</surname><given-names>G.</given-names></name><name><surname>Wojcik</surname><given-names>D. K.</given-names></name></person-group> (<year>2012</year>). <article-title>Common atlas format and 3D brain atlas reconstructor: infrastructure for constructing 3D brain atlases.</article-title>
<source><italic>Neuroinformatics</italic></source>
<volume>10</volume>
<fpage>181</fpage>–<lpage>197</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-011-9138-6</pub-id>
<?supplied-pmid 22227717?><pub-id pub-id-type="pmid">22227717</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="book"><collab>MATLAB</collab> (<year>2010</year>). <source><italic>Version 7.10.0 (R2010a)</italic></source>. <publisher-loc>Natick, Massachusetts</publisher-loc>: <publisher-name>The MathWorks Inc.</publisher-name></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oh</surname><given-names>S. W.</given-names></name><name><surname>Harris</surname><given-names>J. A.</given-names></name><name><surname>Ng</surname><given-names>L.</given-names></name><name><surname>Winslow</surname><given-names>B.</given-names></name><name><surname>Cain</surname><given-names>N.</given-names></name><name><surname>Mihalas</surname><given-names>S.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>A mesoscale connectome of the mouse brain.</article-title>
<source><italic>Nature</italic></source>
<volume>508</volume>
<fpage>207</fpage>–<lpage>214</lpage>. <pub-id pub-id-type="doi">10.1038/nature13186</pub-id>
<?supplied-pmid 24695228?><pub-id pub-id-type="pmid">24695228</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osen</surname><given-names>K. K.</given-names></name><name><surname>Imad</surname><given-names>J.</given-names></name><name><surname>Wennberg</surname><given-names>A. E.</given-names></name><name><surname>Papp</surname><given-names>E. A.</given-names></name><name><surname>Leergaard</surname><given-names>T. B.</given-names></name></person-group> (<year>2019</year>). <article-title>Waxholm Space atlas of the rat brain auditory system: three-dimensional delineations based on structural and diffusion tensor magnetic resonance imaging.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>199</volume>
<fpage>38</fpage>–<lpage>56</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.05.016</pub-id>
<?supplied-pmid 31100433?><pub-id pub-id-type="pmid">31100433</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pallast</surname><given-names>N.</given-names></name><name><surname>Wieters</surname><given-names>F.</given-names></name><name><surname>Fink</surname><given-names>G. R.</given-names></name><name><surname>Aswendt</surname><given-names>M.</given-names></name></person-group> (<year>2019</year>). <article-title>Atlas-based imaging data analysis tool for quantitative mouse brain histology (AIDAhisto).</article-title>
<source><italic>J. Neurosci. Methods</italic></source>
<volume>326</volume>:<issue>108394</issue>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.108394</pub-id>
<?supplied-pmid 31415844?><pub-id pub-id-type="pmid">31415844</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papp</surname><given-names>E. A.</given-names></name><name><surname>Leergaard</surname><given-names>T. B.</given-names></name><name><surname>Calabrese</surname><given-names>E.</given-names></name><name><surname>Johnson</surname><given-names>G. A.</given-names></name><name><surname>Bjaalie</surname><given-names>J. G.</given-names></name></person-group> (<year>2014</year>). <article-title>Waxholm Space atlas of the Sprague Dawley rat brain.</article-title>
<source><italic>Neuroimage</italic></source>
<volume>97</volume>
<fpage>374</fpage>–<lpage>386</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.04.001</pub-id>
<?supplied-pmid 24726336?><pub-id pub-id-type="pmid">24726336</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puchades</surname><given-names>M. A.</given-names></name><name><surname>Csucs</surname><given-names>G.</given-names></name><name><surname>Ledergerber</surname><given-names>D.</given-names></name><name><surname>Leergaard</surname><given-names>T. B.</given-names></name><name><surname>Bjaalie</surname><given-names>J. G.</given-names></name></person-group> (<year>2019</year>). <article-title>Spatial registration of serial microscopic brain images to three-dimensional reference atlases with the QuickNII tool.</article-title>
<source><italic>PLoS One</italic></source>
<volume>14</volume>:<issue>e0216796</issue>. <pub-id pub-id-type="doi">10.1371/journal.pone.0216796</pub-id>
<?supplied-pmid 31141518?><pub-id pub-id-type="pmid">31141518</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>C. A.</given-names></name><name><surname>Rasband</surname><given-names>W. S.</given-names></name><name><surname>Eliceiri</surname><given-names>K. W.</given-names></name></person-group> (<year>2012</year>). <article-title>NIH image to ImageJ: 25 years of image analysis.</article-title>
<source><italic>Nat. Methods</italic></source>
<volume>9</volume>
<fpage>671</fpage>–<lpage>675</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.2089</pub-id>
<?supplied-pmid 22930834?><pub-id pub-id-type="pmid">22930834</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><collab>The ImageMagick Development Team</collab> (<year>2020</year>). <source><italic>ImageMagick. Version 7.0.10</italic></source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://imagemagick.org">https://imagemagick.org</ext-link>
<comment>(accessed June 12, 2020)</comment>.</mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vandenberghe</surname><given-names>M. E.</given-names></name><name><surname>Herard</surname><given-names>A. S.</given-names></name><name><surname>Souedet</surname><given-names>N.</given-names></name><name><surname>Sadouni</surname><given-names>E.</given-names></name><name><surname>Santin</surname><given-names>M. D.</given-names></name><name><surname>Briet</surname><given-names>D.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>High-throughput 3D whole-brain quantitative histopathology in rodents.</article-title>
<source><italic>Sci. Rep.</italic></source>
<volume>6</volume>:<issue>20958</issue>. <pub-id pub-id-type="doi">10.1038/srep20958</pub-id>
<?supplied-pmid 26876372?><pub-id pub-id-type="pmid">26876372</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>J.</given-names></name><name><surname>Ren</surname><given-names>J.</given-names></name><name><surname>Luo</surname><given-names>L.</given-names></name><name><surname>Horowitz</surname><given-names>M.</given-names></name></person-group> (<year>2018</year>). <article-title>Mapping histological slice sequences to the allen mouse brain atlas without 3D reconstruction.</article-title>
<source><italic>Front. Neuroinform.</italic></source>
<volume>12</volume>:<issue>93</issue>. <pub-id pub-id-type="doi">10.3389/fninf.2018.00093</pub-id>
<?supplied-pmid 30618698?><pub-id pub-id-type="pmid">30618698</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yates</surname><given-names>S. C.</given-names></name><name><surname>Groeneboom</surname><given-names>N. E.</given-names></name><name><surname>Coello</surname><given-names>C.</given-names></name><name><surname>Lichtenthaler</surname><given-names>S. F.</given-names></name><name><surname>Kuhn</surname><given-names>P. H.</given-names></name><name><surname>Demuth</surname><given-names>H. U.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>QUINT: workflow for quantification and spatial analysis of features in histological images from rodent brain.</article-title>
<source><italic>Front. Neuroinform.</italic></source>
<volume>13</volume>:<issue>75</issue>. <pub-id pub-id-type="doi">10.3389/fninf.2019.00075</pub-id>
<?supplied-pmid 31849633?><pub-id pub-id-type="pmid">31849633</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
