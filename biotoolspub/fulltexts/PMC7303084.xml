<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Int J Comput Assist Radiol Surg</journal-id>
    <journal-id journal-id-type="iso-abbrev">Int J Comput Assist Radiol Surg</journal-id>
    <journal-title-group>
      <journal-title>International Journal of Computer Assisted Radiology and Surgery</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1861-6410</issn>
    <issn pub-type="epub">1861-6429</issn>
    <publisher>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7303084</article-id>
    <article-id pub-id-type="publisher-id">2186</article-id>
    <article-id pub-id-type="doi">10.1007/s11548-020-02186-z</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepSeg: deep neural network framework for automatic brain tumor segmentation using magnetic resonance FLAIR images</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8630-9046</contrib-id>
        <name>
          <surname>Zeineldin</surname>
          <given-names>Ramy A.</given-names>
        </name>
        <address>
          <email>Ramy.Zeineldin@Reutlingen-University.DE</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0474-4723</contrib-id>
        <name>
          <surname>Karar</surname>
          <given-names>Mohamed E.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-3677-8258</contrib-id>
        <name>
          <surname>Coburger</surname>
          <given-names>Jan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8358-1813</contrib-id>
        <name>
          <surname>Wirtz</surname>
          <given-names>Christian R.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7118-4730</contrib-id>
        <name>
          <surname>Burgert</surname>
          <given-names>Oliver</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.434088.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 0666 4420</institution-id><institution>Research Group Computer Assisted Medicine (CaMed), </institution><institution>Reutlingen University, </institution></institution-wrap>72762 Reutlingen, Germany </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.411775.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 0621 4712</institution-id><institution>Faculty of Electronic Engineering (FEE), </institution><institution>Menoufia University, </institution></institution-wrap>Menouf, 32952 Egypt </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.6582.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9748</institution-id><institution>Department of Neurosurgery, </institution><institution>University of Ulm, </institution></institution-wrap>89312 Günzburg, Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>5</day>
      <month>5</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>5</day>
      <month>5</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2020</year>
    </pub-date>
    <volume>15</volume>
    <issue>6</issue>
    <fpage>909</fpage>
    <lpage>920</lpage>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>1</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>4</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Purpose</title>
        <p id="Par1">Gliomas are the most common and aggressive type of brain tumors due to their infiltrative nature and rapid progression. The process of distinguishing tumor boundaries from healthy cells is still a challenging task in the clinical routine. Fluid-attenuated inversion recovery (FLAIR) MRI modality can provide the physician with information about tumor infiltration. Therefore, this paper proposes a new generic deep learning architecture, namely DeepSeg, for fully automated detection and segmentation of the brain lesion using FLAIR MRI data.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p id="Par2">The developed DeepSeg is a modular decoupling framework. It consists of two connected core parts based on an encoding and decoding relationship. The encoder part is a convolutional neural network (CNN) responsible for spatial information extraction. The resulting semantic map is inserted into the decoder part to get the full-resolution probability map. Based on modified U-Net architecture, different CNN models such as residual neural network (ResNet), dense convolutional network (DenseNet), and NASNet have been utilized in this study.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par3">The proposed deep learning architectures have been successfully tested and evaluated on-line based on MRI datasets of brain tumor segmentation (BraTS 2019) challenge, including s336 cases as training data and 125 cases for validation data. The dice and Hausdorff distance scores of obtained segmentation results are about 0.81 to 0.84 and 9.8 to 19.7 correspondingly.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par4">This study showed successful feasibility and comparative performance of applying different deep learning models in a new DeepSeg framework for automated brain tumor segmentation in FLAIR MR images. The proposed DeepSeg is open source and freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/razeineldin/DeepSeg/">https://github.com/razeineldin/DeepSeg/</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keyword</title>
      <kwd>Brain tumor</kwd>
      <kwd>Computer-aided diagnosis</kwd>
      <kwd>Convolutional neural networks</kwd>
      <kwd>Deep learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001655</institution-id>
            <institution>Deutscher Akademischer Austauschdienst</institution>
          </institution-wrap>
        </funding-source>
        <award-id>91705803</award-id>
        <principal-award-recipient>
          <name>
            <surname>Zeineldin</surname>
            <given-names>Ramy A.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© CARS 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par5">Brain tumors are one of the leading causes of death for cancer patients, especially children and young people. The American Cancer Society reported that 23,820 new brain cancer cases in the USA were discovered in 2019 [<xref ref-type="bibr" rid="CR1">1</xref>]. Brain tumors can be categorized into two types as follows: primary brain tumors that originate in the brain cells, and secondary brain tumors developed through the spreading of malignant cells from other parts of the body to the brain. One of the most frequent primary tumors is glioma [<xref ref-type="bibr" rid="CR2">2</xref>]. It affects not only the glial cells of the brain, but it invades also the surrounding tissues. The high-grade glioma (HGG) or glioblastoma (GBM) is the most common and aggressive type with a median survival rate of one to two years [<xref ref-type="bibr" rid="CR3">3</xref>]. Another slower-growing low-grade glioma (LGG) such as astrocytoma has slightly longer survival time. Treatment methods such as radiotherapy and chemotherapy may be used to destroy the tumor cells that cannot be physically resected or to slow their growth.</p>
    <p id="Par6">Therefore, neurosurgery still presents the initial and, in some cases, the only therapy for many brain tumors [<xref ref-type="bibr" rid="CR4">4</xref>]. However, modern surgical treatment of brain tumors faces the most challenging practice conditions because of the nature and structure of the brain. In addition, distinguishing tumor tissue from normal brain parenchyma is difficult for neurosurgeons based on visual inspection alone [<xref ref-type="bibr" rid="CR5">5</xref>]. Magnetic resonance imaging (MRI) is widely used as a common choice for diagnosing and evaluating the intraoperative treatment response of brain tumors [<xref ref-type="bibr" rid="CR6">6</xref>]. Furthermore, MRI provides detailed images of the brain tumor cellularity, vascularity, and blood–brain barrier using different produced multimodal protocols such as T1-weighted, T2-weighted, and T2-FLAIR images. These various images provide information to neurosurgeons and can be valuable in diagnostics. However, interpreting these data during neurosurgery is a very challenging task and an appropriate visualization of lesion structure apart from healthy brain tissues is crucial [<xref ref-type="bibr" rid="CR7">7</xref>].</p>
    <p id="Par7">Manual segmentation of the brain tumor is a vital procedure and needs a group of clinical experts to accurately define the location and the type of the tumor. Moreover, the process of lesion localization is very labor based and highly dependent on the physicians’ experience, skills, and their slice-by-slice decisions. Alternatively, automated computer-based segmentation methods present a good solution to save the surgeon’s time and to provide reliable and accurate results, while reducing the exerted efforts of experienced physicians to accomplish the procedures of diagnosis or evaluation for every single patient [<xref ref-type="bibr" rid="CR8">8</xref>]. Formerly, numerous machine learning algorithms were developed for the segmentation of normal and abnormal brain tissues using MRI images [<xref ref-type="bibr" rid="CR9">9</xref>]. However, choosing features that enable this operation to be fully automated is very challenging and requires a combination of computer engineering and medical expertise. Therefore, classical approaches depend heavily on the applied application and do not generalize well. Nevertheless, developing fully automated brain tumor methods is still challenging task, because malignant areas varied in terms of shape, size, and localization, and they can only be defined through the intensity changes relative to surrounding healthy cells.</p>
    <p id="Par8">Recently, deep learning becomes an attractive field of machine learning that outperforms traditional computer vision algorithms in a wide range of applications such as object detection [<xref ref-type="bibr" rid="CR10">10</xref>], semantic segmentation [<xref ref-type="bibr" rid="CR11">11</xref>] as well as other applications such as navigation guidance [<xref ref-type="bibr" rid="CR12">12</xref>]. Convolutional neural networks (CNNs) have proved during the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) [<xref ref-type="bibr" rid="CR13">13</xref>] and their ability to accurately detect and localize different types of objects. In 2012, an advanced pretrained CNN model called AlexNet [<xref ref-type="bibr" rid="CR14">14</xref>] showed the best results in the image classification challenge. Then, other CNN models have dominated the ILSVRC competition, namely visual geometry group network (VGGNet) [<xref ref-type="bibr" rid="CR15">15</xref>], residual neural network (ResNet) [<xref ref-type="bibr" rid="CR16">16</xref>], dense convolutional network (DenseNet) [<xref ref-type="bibr" rid="CR17">17</xref>], Xception [<xref ref-type="bibr" rid="CR18">18</xref>], MobileNet [<xref ref-type="bibr" rid="CR19">19</xref>], NASNet [<xref ref-type="bibr" rid="CR20">20</xref>], and MobileNetV2 [<xref ref-type="bibr" rid="CR21">21</xref>]. Moreover, CNN methods have been applied to perform MRI tumor segmentation [<xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR23">23</xref>].</p>
    <p id="Par9">Semantic segmentation is currently one of the most important tasks in the field of computer vision towards complete scene understanding. Early approaches of applying semantic segmentation in the medical field use patch-wise image classification [<xref ref-type="bibr" rid="CR24">24</xref>]. However, it suffers from two main problems: First, the training patches are much larger than the training samples, which require a higher number of computation cycles resulting in a large running time consumption. Second, the segmentation accuracy depends heavily on the appropriate size of patches. Accordingly, new network architecture was introduced, refer to Fig. <xref rid="Fig1" ref-type="fig">1</xref>, which is able to solve these problems by using two main paths: a contracting path (or encoder) and an expansive path (or decoder) [<xref ref-type="bibr" rid="CR25">25</xref>]. The encoder is typically a CNN consisting of consecutive two 3 × 3 convolutional layers, each followed by a rectified linear unit (ReLU) and 2 × 2 spatial max pooling. Contrariwise, the decoder aims at upsampling the resultant feature map using deconvolution layers followed by 2 × 2 up-convolution, a concatenation layer with the corresponding downsampled layer from the encoder, two 3 × 3 convolutions, and a ReLU. Finally, the upsampled features are then directed to a 1 × 1 convolution layer to output the final segmentation map. Remarkably, the networks are able to achieve precise segmentation results using only few training images with the help of data augmentation [<xref ref-type="bibr" rid="CR26">26</xref>]. Furthermore, the tiling strategy allows the model to employ high-resolution images in the training stage with low GPU memory requirements.<fig id="Fig1"><label>Fig. 1</label><caption><p>Modified U-Net network consists of convolutional blocks (<italic>blue boxes</italic>), maximum pooling (<italic>orange arrows</italic>), upsampling (<italic>grey arrows</italic>), and softmax output (<italic>green block</italic>)</p></caption><graphic xlink:href="11548_2020_2186_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par10">This study aims at developing a new fully automated MRI brain tumor segmentation based on modified U-Net models, including the following contributions:<list list-type="bullet"><list-item><p id="Par11">Presenting the developed modular design of DeepSeg to include new segmentation architectures for the FLAIR modal brain MRI.</p></list-item><list-item><p id="Par12">Proposing a generic modular architecture of the brain tumor segmentation with two elements: feature extraction and image expanding paths, in order to support applying different deep neural network models successfully.</p></list-item><list-item><p id="Par13">A detailed ablation study of the state-of-the-art deep learning models highlighting the computational performance during training and prediction processes.</p></list-item><list-item><p id="Par14">Validating the proof of concept to apply various deep learning models for assisting the clinical procedures of the brain tumor surgery using FLAIR modality on the BraTS 2019 dataset.</p></list-item></list></p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <p id="Par15">DeepSeg is a generic decoupled framework for automatic tumor segmentation, as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. Thanks to the basic U-Net structure [<xref ref-type="bibr" rid="CR27">27</xref>], it consists of two main parts: a feature extractor (or encoder) part and an image upscaling (or decoder) part. This universal design has two main advantages: First, it allows the extensibility of the system, i.e., different encoders and decoders can be added easily. Moreover, a discriminative comparison between the various proposed models can be done straightforwardly. In the following, the proposed architecture is described in detail.<fig id="Fig2"><label>Fig. 2</label><caption><p>DeepSeg architecture for using different feature extractor models of MRI brain tumors</p></caption><graphic xlink:href="11548_2020_2186_Fig2_HTML" id="MO2"/></fig></p>
  </sec>
  <sec id="Sec3">
    <title>Feature extractor</title>
    <p id="Par16">The modified U-Net encoder has been implemented by using advances in CNNs including dropout and batch normalization (BN) [<xref ref-type="bibr" rid="CR28">28</xref>, <xref ref-type="bibr" rid="CR29">29</xref>]. In addition, state-of-the-art deep neural network architectures are integrated into our benchmarking system to extract the feature map. These models are utilized to achieve better performance than the obtained results in the ILSVRC competition until now [<xref ref-type="bibr" rid="CR13">13</xref>]. Apparently, every proposed model has its own set of parameters and computational resources requirements, as described below.</p>
    <p id="Par17">VGGNet [<xref ref-type="bibr" rid="CR15">15</xref>] is proposed by the Visual Geometry Group from the University of Oxford and is the winner of the ILSVRC 2014 in the localization task. It is chosen to be the baseline model because of its simplicity, consisting only of small 3 × 3 convolutional layers and max-pooling layers for the downsampling process followed by two fully connected layers for feature extraction.</p>
    <p id="Par18">In fact, increasing the neural network layer would increase the accuracy of the training phase; however, there is a significant problem with this approach; for example, vanishing gradients [<xref ref-type="bibr" rid="CR30">30</xref>] cause the neural network accuracy to saturate and then degrade rapidly. In ILSVRC 2015, a novel micro-architecture called ResNet [<xref ref-type="bibr" rid="CR16">16</xref>] was introduced to solve this exploding behavior. The ResNet consists of residual blocks as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a, and each block consists of the original two convolutional layers in addition to a shortcut connection from the input of the first layer to the output of the second layer. By employing skip connections to the deep neural network, neither more additional parameters nor computational complexity are added to the network. Owing to this approach, they are able to train up to 152-layer deep neural network while maintaining a lower complexity than the above VGG models.<fig id="Fig3"><label>Fig. 3</label><caption><p>Comparison of the basic blocks for different feature extractors. <bold>a</bold> The residual block of ResNet; <bold>b</bold> a 3-layer dense block; <bold>c</bold> an extreme module of inception (Xception module); <bold>d</bold> a depth-wise-based module of MobileNet; <bold>e</bold> MobileNetV2 blocks with two stride values</p></caption><graphic xlink:href="11548_2020_2186_Fig3_HTML" id="MO3"/></fig></p>
    <p id="Par19">DenseNet [<xref ref-type="bibr" rid="CR17">17</xref>] uses the feature map of the preceding layers as inputs into all the following layers, as depicted in Fig. <xref rid="Fig3" ref-type="fig">3</xref>b. This type of deep neural network model has <italic>L</italic>(<italic>L</italic> + 1)/2 connections for a CNN with <italic>L</italic> layers, whereas traditional networks would have only L connections. Remarkably, they are able to achieve additional improvements such as a smaller number of parameters besides the ability to scale the network to hundreds of layers.</p>
    <p id="Par20">Xception presents an extreme version of the inception network [<xref ref-type="bibr" rid="CR18">18</xref>]. The inception model aimed at improving the utilization of the computing resources within the neural network through special modules. Each inception module is a multilevel feature extractor by stacking 1 × 1 and 3 × 3 convolutions beside each other in the same layer rather than using only one convolutional layer. The Xception, as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>c, achieved a slightly better result than inception models on ImageNet; however, it showed superior improvement when the used dataset becomes larger.</p>
    <p id="Par21">Google presented MobileNet [<xref ref-type="bibr" rid="CR19">19</xref>] in 2017 as an efficient lightweight network for mobile application, as presented in Fig. <xref rid="Fig3" ref-type="fig">3</xref>d. Additionally, the BN is applied after each convolution followed by a ReLU activation. Then MobileNetV2 [<xref ref-type="bibr" rid="CR21">21</xref>] is introduced, which enhanced the state-of-the-art performance of mobile models based on inverted residual blocks as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>e. These bottleneck blocks are similar to the residual block of ResNet where the input of the block is added to the output of the narrow layers. ReLU6 is also utilized, because of its robustness in low-precision computation, to remove the nonlinearities in the bottleneck layers. Although MobileNetV2 shows a similar performance to the previous MobileNet, it uses only 2.5 times fewer operations than the first version.</p>
    <p id="Par22">Google Brain introduced NASNe<bold>t</bold> [<xref ref-type="bibr" rid="CR20">20</xref>] to obtain state-of-the-art segmentation results with relatively smaller model size. The basic architecture of NASNet is made up of two main repeated blocks, namely normal cell and reduction cell. The first type is consisting of convolutional layers with output features of the same dimensions, and the height and width of the other type’s output are reduced by a stride of 2. ScheduledDropPath is also presented to make the model generalize well, where each path in the cell can be dropped with an increased probability over the training sequence.</p>
  </sec>
  <sec id="Sec4">
    <title>Image upscaling</title>
    <p id="Par23">In semantic segmentation, it is very crucial to use both semantic and spatial information so that the neural network can perform well. Hence, the decoder should recover the missing spatial information to get the full-resolution segmented map from the consequential encoded features. By skip connections (Fig. <xref rid="Fig1" ref-type="fig">1</xref>), U-Net can obtain the semantic feature map from the bottleneck and recombine it with higher-resolution outputs from the encoder, respectively.</p>
    <p id="Par24">Unlike standard U-Net decoder, some modifications were incorporated for further exceptional segmentation results. Firstly, a BN layer is applied between each convolution and ReLU to make each layer learn independently from other layers and thus contribute to faster learning. Additionally, a smaller filter size of 32 as the base filter is selected and doubled at the following layers, in order to apply the full size as input rather than using patches or small regions of the input. Finally, the output of the network is passed into a softmax output layer which converts the output logics into a list of probability distributions.</p>
  </sec>
  <sec id="Sec5">
    <title>Data</title>
    <p id="Par25">This study was performed using the FLAIR MRI data from the BraTS 2019 challenge [<xref ref-type="bibr" rid="CR31">31</xref>]. Although T1KM is the standard imaging for glioma, FLAIR is becoming increasingly relevant in the case of malignant tumors, since there is a trend to also resect the FLAIR-positive areas [<xref ref-type="bibr" rid="CR32">32</xref>]. Moreover, the advantages of FLAIR images in the brain surgery of low-grade gliomas (LGG) have been investigated by our clinical partners in [<xref ref-type="bibr" rid="CR6">6</xref>].</p>
    <p id="Par26">BraTS dataset contains multi-institutional preoperative MRI of 336 heterogeneous (in shape, appearance, size, and texture) glioma patients (259 HGG and 76 LGG). Each patient has four multimodal scans: native T1-weighted, post-contrasted T1-weighted, T2-weighted, and T2-FLAIR. MRI data were acquired with various clinical protocols and different scanners from 19 institutions. The manual segmentation of the data was done by experienced neuroradiologists, from 1 to 4, following the same annotation procedure. After that, the MRI scans are resampled and interpolated to the same resolution 1 mm<sup>3</sup>. Figure <xref rid="Fig4" ref-type="fig">4</xref> displays the provided segmented labels: the necrotic and non-enhancing tumor core (label 1), the peritumoral edema (label 2), and enhancing tumor (label 4).<fig id="Fig4"><label>Fig. 4</label><caption><p><bold>a</bold> T2-FLAIR image; <bold>b</bold> brain tumor structures including non-enhancing core and necrotic (<italic>red</italic>); solid core (<italic>yellow</italic>) and edema (<italic>orange</italic>)</p></caption><graphic xlink:href="11548_2020_2186_Fig4_HTML" id="MO4"/></fig></p>
    <p id="Par27">Since MRI scans come from different sources, protocols, and institutions, the training images may suffer from bias field noise, which can be defined as undesirable artifacts that arise during the process of image acquisition. To eliminate these effects, the improved N3 bias correction tool [<xref ref-type="bibr" rid="CR33">33</xref>] is used for performing image-wise normalization and bias correction. Then, a data normalization for each slice of FLAIR MRI scans is applied by subtracting the mean of each slice and dividing by its standard deviation.</p>
    <p id="Par28">Providing we are training large neural networks using limited training data, some precautions should be taken to prevent the problem of overfitting. One of them is data augmentation, which is the process of creating new artificial training data from the original one in order to improve the model performance by making the model generalize well to the new testing data. In this study, a set of simple on-the-fly data augmentation methods is applied (as listed in Table <xref rid="Tab1" ref-type="table">1</xref>) by horizontal and vertical flipping, rotation, scaling, shearing, and shift. Unfortunately, these simple methods are not enough to get sufficient immune training data; therefore, more complex methods are also introduced such as elastic distortion corresponding to uncontrolled noise of MRI sensors, where σ is the elasticity coefficient and α is the multiplying factor of the displacement fields which controls the intensity of deformation. Figure <xref rid="Fig5" ref-type="fig">5</xref> shows some examples of the applied augmentation techniques.<table-wrap id="Tab1"><label>Table 1</label><caption><p>List of the applied data augmentation methods</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Methods</th><th align="left">Parameters</th></tr></thead><tbody><tr><td align="left">Flip horizontally</td><td align="left">20% of all images</td></tr><tr><td align="left">Flip vertically</td><td align="left">20% of all images</td></tr><tr><td align="left">Scale</td><td align="left"> ± 20% on both horizontal and vertical direction</td></tr><tr><td align="left">Translation</td><td align="left"> ± 20% on both horizontal and vertical direction</td></tr><tr><td align="left">Rotation</td><td align="left"> ± 25°</td></tr><tr><td align="left">Shear</td><td align="left"> ± 8°</td></tr><tr><td align="left">Elastic transformation</td><td align="left">α = 720, σ = 24</td></tr></tbody></table></table-wrap><fig id="Fig5"><label>Fig. 5</label><caption><p>Random augmented image transformation. The first row shows the original image. Next three rows present horizontal and vertical flipping, scaling, translation, rotation, and shearing methods. The elastic transformation is presented in the last row</p></caption><graphic xlink:href="11548_2020_2186_Fig5_HTML" id="MO5"/></fig></p>
  </sec>
  <sec id="Sec6">
    <title>Experiments</title>
    <sec id="Sec7">
      <title>Experimental setup</title>
      <p id="Par29">The proposed methods of this study were run on AMD Ryzen 2920X (32 M Cache, 3.50 GHz) CPU with a single 11 GB NVIDIA RTX 2080Ti GPU. Proposed models are implemented in Python using Keras library and Tensor Flow backend. Experiments are done using FLAIR MRI sequences with a resolution of 224 × 224 in order to use all the proposed feature extractor networks. All networks are trained for 35 epochs and a batch size of 16. During the training process, spatial dropout with a rate of 0.5 was used after the feature extractor path. This is a simple type of regularization to ensure that the neural networks generalize well without overfitting the training dataset. Adam optimizer [<xref ref-type="bibr" rid="CR34">34</xref>] has been applied with a learning rate of 0.00001. Nevertheless, the BraTS dataset suffers from a data imbalance problem where the tumor pixels are less than 2% and the healthy pixels are mostly 98% of the whole dataset. To solve this problem in two steps: Firstly, the models were trained on the brain sequences and ignored the empty slices; secondly, a weighted cross-entropy loss for each class was used to pay more attention to the malignant labels than the background as defined by<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ L = - \mathop \sum \limits_{n = 1}^{N} y_{c} \log \left( {p_{c} } \right)*w $$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>log</mml:mo><mml:mfenced close=")" open="("><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfenced><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:math><graphic xlink:href="11548_2020_2186_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>N</italic> is the number of classes including the background and the tumor cells in this study, <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_{c}$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11548_2020_2186_Article_IEq1.gif"/></alternatives></inline-formula> represents the true labels for the <italic>n</italic>th class, <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p_{c}$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11548_2020_2186_Article_IEq2.gif"/></alternatives></inline-formula> is the predicted softmax output for those true labels, and <italic>w</italic> is the proposed weight map of (0.05, 0.95) to focus on the tumor pixels rather than the background. For the evaluation of our segmentation results, four metrics, namely dice similarity coefficient (DSC), sensitivity, specificity and the Hausdorff distance (HD), are computed. DSC score calculates the overlap of the segmented region and the ground truth <italic>y</italic> and is applied to the network softmax predictions p as follows:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{DSC}} = \frac{2* \sum yp + \varepsilon }{{\sum y + \sum p + \varepsilon }} $$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:mtext>DSC</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mrow/><mml:mo>∗</mml:mo><mml:mo>∑</mml:mo><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo>∑</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mo>∑</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="11548_2020_2186_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>Note that <italic>ε</italic> is the smooth parameter to make the dice function differentiable. This dice overlap can take values from 0 (represents lower overlap) to 1 (indicates a full overlap). Specificity and sensitivity are given by:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{Specificity}} = \frac{{{\text{TN}}}}{{{\text{TN}} + {\text{FP}}}} $$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mtext>Specificity</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mtext>TN</mml:mtext><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="11548_2020_2186_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{Sensitivity}} = \frac{{{\text{TP}}}}{{\text{TP } + \text{ FN}}} $$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:mtext>Sensitivity</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mspace width="0.333333em"/><mml:mo>+</mml:mo><mml:mspace width="0.333333em"/><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="11548_2020_2186_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where true positives (TP) and false positives (FP) refer to the number of retrieved points that are correct/incorrect, and similarly for true and false negatives, TN and FN, respectively.</p>
      <p id="Par30">Dice, sensitivity, and sensitivity metrics are measures of pixel-based overlap between the ground truth and the predicted segmentations. In addition, the HD gives the largest distance of the segmentation set to the nearest point in the truth set, as defined by<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{HD}}\left( {S,T} \right) = {\max}\left\{ {h\left( {P,T} \right), h\left( {T,P} \right)} \right\} $$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mtext>HD</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo movablelimits="true">max</mml:mo><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>h</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="11548_2020_2186_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>with<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h\left( {S,T} \right) = \mathop {\max }\limits_{{s \in S}} \left\{ {\mathop {\min }\limits_{{t \in T}} \left\{ {d\left( {s,t} \right)} \right\}} \right\} $$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:mi>h</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">max</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mfenced close="}" open="{"><mml:mrow><mml:munder><mml:mo movablelimits="false">min</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:munder><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>d</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="11548_2020_2186_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where the shortest Euclidian distance <inline-formula id="IEq3"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d\left( {s,{ }t} \right)$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mi>d</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mrow/><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="11548_2020_2186_Article_IEq3.gif"/></alternatives></inline-formula> is calculated for every point <italic>s</italic> of the segmentation set <italic>S</italic>, with respect to the ground truth point <italic>t</italic> in the image.</p>
    </sec>
  </sec>
  <sec id="Sec8">
    <title>Ablation study</title>
    <p id="Par31">Thanks to the DeepSeg framework, several methods were analyzed and compared simultaneously. Table <xref rid="Tab2" ref-type="table">2</xref> illustrates different characteristics of these automated methods with the corresponding computational times. Training and prediction times present the average estimated time of applying each algorithm about 35 times during the training and validation, respectively. These tests showed that MobileNet encoder requires smallest resources with only 22 MB memory and roughly 5.6 thousand parameters. It is worth mentioning that MobileNet and MobileNetV2 are mainly developed for mobile and embedded applications where the hardware resources are limited. Likewise, U-Net, modified U-Net, and NASNet consume a small amount of memory of 30 MB, 30 MB, and 37 MB, respectively. Obviously, there is a proportional relationship between the number of parameters and the demanded memory. In contrary, ResNet model consumes the largest amount of memory of 118 MB, which is not considered a problem since modern GPUs possess a memory of several gigabytes. Other models such as DenseNet, VGGNet, and Xception are located in the middle level of memory consumption of 51 MB, 71 MB, and 103 MB, respectively.<table-wrap id="Tab2"><label>Table 2</label><caption><p>A comparative performance of the employed models. Average computational times for each encoder of 35 results during training and validation phases</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Encoder</th><th align="left">Size (MB)</th><th align="left">Training time (s)</th><th align="left">Prediction time (s)</th><th align="left">Parameters</th><th align="left">Layers</th></tr></thead><tbody><tr><td align="left">U-NET</td><td char="." align="char">30</td><td char="." align="char"><bold>381</bold></td><td char="." align="char"><bold>1.1</bold></td><td char="." align="char">7,760,642</td><td char="." align="char"><bold>39</bold></td></tr><tr><td align="left">Modified U-NET</td><td char="." align="char">30</td><td char="." align="char">385</td><td char="." align="char">1.3</td><td char="." align="char">7,763,050</td><td char="." align="char">74</td></tr><tr><td align="left">VGGNet</td><td char="." align="char">71</td><td char="." align="char">540</td><td char="." align="char">1.6</td><td char="." align="char">18,540,938</td><td char="." align="char">56</td></tr><tr><td align="left">ResNet</td><td char="." align="char">118</td><td char="." align="char">446</td><td char="." align="char">2.3</td><td char="." align="char">30,546,458</td><td char="." align="char">223</td></tr><tr><td align="left">DenseNet</td><td char="." align="char">51</td><td char="." align="char">482</td><td char="." align="char">3.2</td><td char="." align="char">12,947,674</td><td char="." align="char">474</td></tr><tr><td align="left">Xception</td><td char="." align="char">103</td><td char="." align="char">580</td><td char="." align="char">1.9</td><td char="." align="char">26,769,602</td><td char="." align="char">184</td></tr><tr><td align="left">MobileNet</td><td char="." align="char">30</td><td char="." align="char">385</td><td char="." align="char">1.5</td><td char="." align="char">7,590,746</td><td char="." align="char">129</td></tr><tr><td align="left">NASNet</td><td char="." align="char">37</td><td char="." align="char">684</td><td char="." align="char">4.4</td><td char="." align="char">8,652,846</td><td char="." align="char">818</td></tr><tr><td align="left">MobileNetV2</td><td char="." align="char"><bold>22</bold></td><td char="." align="char">386</td><td char="." align="char">1.8</td><td char="." align="char"><bold>5,591,770</bold></td><td char="." align="char">202</td></tr></tbody></table><table-wrap-foot><p>Best values are shown in bold</p></table-wrap-foot></table-wrap></p>
    <p id="Par32">Moreover, the number of layers has a significant influence on both the training and prediction time. For instance, the training time of one epoch using U-Net with the smallest number of layers (39 layers) is 381 s and the prediction time is just 1.1 s. But the NASNet model with 818 layers requires 684 s for one epoch to train and the prediction of one patient took 4.4 s. Nevertheless, this is not the general rule since modified U-Net, MobileNet, and MobileNetV2 share the second place with a training time of 385 s even though they have various numbers of layers of 74, 129, and 202, respectively. The main reason is the internal building architecture of MobileNet variants which is developed for smartphone devices.</p>
  </sec>
  <sec id="Sec9">
    <title>Segmentation results</title>
    <p id="Par33">The DeepSeg framework consists of several automated feature extractors in addition to an image expanding path. The corresponding evaluation results have been obtained by running twofold cross-validation on the 336 training cases of the BraTS 2019 dataset divided as follows: 270 cases for training and 66 for validation. Table <xref rid="Tab3" ref-type="table">3</xref> summarizes the comparison and the overall measurement results of all tested methods.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Mean DSC, sensitivity, specificity and Hausdorff distance scores of testing different encoders on BraTS 2019 training data</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Encoder</th><th align="left">DSC</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">HD</th></tr></thead><tbody><tr><td align="left">U-NET</td><td char="." align="char">0.809</td><td char="." align="char">0.799</td><td char="." align="char"><bold>0.998</bold></td><td char="." align="char">12.926</td></tr><tr><td align="left">Modified U-NET</td><td char="." align="char">0.814</td><td char="." align="char">0.783</td><td char="." align="char"><bold>0.999</bold></td><td char="." align="char">13.341</td></tr><tr><td align="left">VGGNet</td><td char="." align="char"><bold>0.837</bold></td><td char="." align="char">0.819</td><td char="." align="char"><bold>0.998</bold></td><td char="." align="char">12.633</td></tr><tr><td align="left">ResNet</td><td char="." align="char">0.811</td><td char="." align="char">0.789</td><td char="." align="char"><bold>0.998</bold></td><td char="." align="char">13.652</td></tr><tr><td align="left">DenseNet</td><td char="." align="char"><bold>0.839</bold></td><td char="." align="char">0.827</td><td char="." align="char"><bold>0.998</bold></td><td char="." align="char">13.156</td></tr><tr><td align="left">Xception</td><td char="." align="char"><bold>0.839</bold></td><td char="." align="char"><bold>0.856</bold></td><td char="." align="char"><bold>0.998</bold></td><td char="." align="char">11.337</td></tr><tr><td align="left">MobileNet</td><td char="." align="char"><bold>0.835</bold></td><td char="." align="char">0.843</td><td char="." align="char"><bold>0.998</bold></td><td char="." align="char"><bold>10.924</bold></td></tr><tr><td align="left">NASNet</td><td char="." align="char">0.834</td><td char="." align="char">0.826</td><td char="." align="char"><bold>0.998</bold></td><td char="." align="char">12.608</td></tr><tr><td align="left">MobileNetV2</td><td char="." align="char">0.827</td><td char="." align="char">0.822</td><td char="." align="char"><bold>0.998</bold></td><td char="." align="char">12.029</td></tr></tbody></table><table-wrap-foot><p>Best values are shown in bold</p></table-wrap-foot></table-wrap></p>
    <p id="Par34">The proposed deep learning architectures were able to accurately detect tumor regions in the validation set with mean DSC scores ranging from 0.809 to 0.839, while the mean dice score of the expert’s annotation for the whole tumor core is about 0.85 as reported in [<xref ref-type="bibr" rid="CR35">35</xref>]. Although statistical analysis of results is relatively close or identical (like specificity), these results give an important indication that fully automated deep learning models maybe utilized in the task of brain tumor segmentation. As illustrated in Table <xref rid="Tab3" ref-type="table">3</xref>, the DenseNet, Xception, VGGNet, and MobileNet encoders achieved the best DSC scores of 0.839, 0.839, 0.837, and 0.835, respectively. Although the Xception encoder showed the best value for the sensitivity of 0.856 with approximately 7% better than the original U-Net model, it achieved the same value of the specificity. This result confirms that point-based approaches are not enough for evaluating brain tumor segmentation method. Therefore, the HD measurements were applied to verify both the best accuracy and performance among all tested deep encoders. The MobileNet showed the shortest HD value of 10.924.</p>
    <p id="Par35">Figures <xref rid="Fig6" ref-type="fig">6</xref> and <xref rid="Fig7" ref-type="fig">7</xref> show segmentation results for the proposed architectures generated from the validation set (67 cases). In both figures, the first row indicates the FLAIR images in gray color mode and the manual ground truth segmentations are shown in the second row. In the following rows, segmentation results of different automated methods are presented. It can be observed that segmented tumor boundaries (indicated in red) from proposed encoders are very similar to the manual annotation even when the lesion region is heterogeneous in shape, volume, and texture. For instance, a small-sized tumor in case TCIA12_470_1 was accurately segmented by proposed methods; however, when the heterogeneity of malignant cells increases, the performance varied remarkably. This is clear in TCIA10_103_1 case since some encoders such as U-Net, VGGNet, MobileNet tends to over-segment the tumor area, while modified U-Net, ResNet, Xception, NASNet, and MobileNetV2 tend to under-segment. This result showed superior accuracy of the Xception, DenseNet encoders compared to other tested architectures for the most difficult tumor segmentation case, e.g., the case of TCIA10_351_1. Although the DenseNet encoder provided a lower score of tumor segmentation result in the case of TCIA10_490_1, it is valid and clinically accepted. However, other encoders such as U-Net and NASNet are failed to give accepted segmentation results.<fig id="Fig6"><label>Fig. 6</label><caption><p>Brain tumor segmentation results. T2-FLAIR, ground truth and output of original U-Net, modified U-Net, VGGNet, ResNet, and DenseNet. tumor regions are indicated in red</p></caption><graphic xlink:href="11548_2020_2186_Fig6_HTML" id="MO6"/></fig><fig id="Fig7"><label>Fig. 7</label><caption><p>Brain tumor segmentation results. T2-FLAIR, ground truth and output of Xception, MobileNet, NASNet, and MobileNetV2</p></caption><graphic xlink:href="11548_2020_2186_Fig7_HTML" id="MO7"/></fig></p>
  </sec>
  <sec id="Sec10">
    <title>Evaluation</title>
    <p id="Par36">For consistency with other publications, the proposed architectures have been also tested on the validation datasets of BraTS 2019 (125 cases). Table <xref rid="Tab4" ref-type="table">4</xref> presents the compared scores of mean dice similarity coefficient, sensitivity, specificity, and HD, similar to the online evaluation platform (<ext-link ext-link-type="uri" xlink:href="https://ipp.cbica.upenn.edu/">https://ipp.cbica.upenn.edu/</ext-link>). These results showed that the proposed models are robust and able to deal with MRI segmentation task. In Table <xref rid="Tab4" ref-type="table">4</xref>, the DenseNet architecture outperformed other models with respect to the DSC (0.841) as well as in the training set; however, it ranked second with a HD (10.595) which is clinically accepted. The dice metrics and HD are the most important measurements when evaluating and comparing among deep learning models, because they show the percentage of the overlapping between ground truth segmentation and predictions. In contrast, the lack of false positives indicated high values of both specificity and sensitivity, which may not precisely reflect the actual performance.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Mean DSC, ss models on BraTS 2019 validation data</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Encoder</th><th align="left">DSC</th><th align="left">Sensitivity</th><th align="left">Specificity</th><th align="left">HD</th></tr></thead><tbody><tr><td align="left">U-NET</td><td char="." align="char">0.813</td><td char="." align="char">0.841</td><td char="." align="char">0.987</td><td align="left">19.747</td></tr><tr><td align="left">Modified U-NET</td><td char="." align="char">0.820</td><td char="." align="char">0.853</td><td char="." align="char">0.987</td><td align="left">12.014</td></tr><tr><td align="left">VGGNet</td><td char="." align="char">0.829</td><td char="." align="char">0.837</td><td char="." align="char">0.990</td><td align="left">9.756</td></tr><tr><td align="left">ResNet</td><td char="." align="char">0.823</td><td char="." align="char">0.832</td><td char="." align="char">0.990</td><td align="left">10.005</td></tr><tr><td align="left">DenseNet</td><td char="." align="char">0.841</td><td char="." align="char">0.860</td><td char="." align="char">0.989</td><td align="left">10.595</td></tr><tr><td align="left">Xception</td><td char="." align="char">0.834</td><td char="." align="char">0.865</td><td char="." align="char">0.988</td><td align="left">12.571</td></tr><tr><td align="left">MobileNet</td><td char="." align="char">0.830</td><td char="." align="char">0.855</td><td char="." align="char">0.989</td><td align="left">11.696</td></tr><tr><td align="left">NASNet</td><td char="." align="char">0.830</td><td char="." align="char">0.861</td><td char="." align="char">0.988</td><td align="left">11.673</td></tr><tr><td align="left">MobileNetV2</td><td char="." align="char">0.822</td><td char="." align="char">0.854</td><td char="." align="char">0.988</td><td align="left">13.894</td></tr><tr><td align="left">Questionmarks</td><td char="." align="char">0.909</td><td char="." align="char">0.924</td><td char="." align="char">0.994</td><td align="left">NA</td></tr></tbody></table></table-wrap></p>
    <p id="Par37">In Table <xref rid="Tab4" ref-type="table">4</xref>, the top-score team “Questionmarks” wins currently the first place of segmentation task rankings from the BraTS 2019 challenge. The performance of our proposed methods does not exceed this score. However, our segmentation results are still clinically accepted due to the following reasons: First, the DeepSeg methods are trained using only FLAIR MRI data dissimilar to participating teams in BraTS 2019, because multi-MRI modalities are not always applicable and sometimes would be unfeasible in clinical experiments. Finally, the online evaluation system presents unranked leaderboard and the calculated score is an average of all the submissions made by the team.</p>
  </sec>
  <sec id="Sec11">
    <title>Conclusions</title>
    <p id="Par38">This study demonstrated the feasibility of employing deep learning approaches for assisting the procedures of brain surgery. The DeepSeg framework is developed successfully for fully automated segmenting brain tumors in MR FLAIR images, based on different architectures of deep CNN models. Moreover, the findings of this comparative study have been validated using the BraTS online evaluation platform, as illustrated in Table <xref rid="Tab4" ref-type="table">4</xref>.</p>
    <p id="Par39">Currently, we are working on extending the validation of our DeepSeg framework by adding more image datasets from other different MRI modalities such as T1- and T2-weighted to verify its potential impact on the planning procedures of the brain tumor surgery, with our clinical partners at the Department of Neurosurgery, University of Ulm. Furthermore, processing 3-D convolutions with like atrous spatial pyramid pooling (ASPP) [<xref ref-type="bibr" rid="CR36">36</xref>] over all slices will advance the DeepSeg framework to cover the clinical requirements for accurate segmentation of brain tumors during MRI-guided interventions.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publishers Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Open Access funding provided by Projekt DEAL. The corresponding author is funded by the German Academic Exchange Service (DAAD) under Scholarship No. 91705803.</p>
  </ack>
  <notes notes-type="ethics">
    <title>Compliance with ethical standards</title>
    <notes id="FPar1" notes-type="COI-statement">
      <title>Conflict of interest</title>
      <p id="Par40">The authors have no conflict of interest to disclose.</p>
    </notes>
    <notes id="FPar2">
      <title>Ethical approval</title>
      <p id="Par41">This article does not contain any studies with human participants or animals performed by any of the authors.</p>
    </notes>
    <notes id="FPar3">
      <title>Informed consent</title>
      <p id="Par42">This article does not contain patient data.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Siegel</surname>
            <given-names>RL</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>KD</given-names>
          </name>
          <name>
            <surname>Jemal</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Cancer statistics, 2019 (US statistics)</article-title>
        <source>CA A Cancer J Clin</source>
        <year>2019</year>
        <volume>69</volume>
        <fpage>7</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="doi">10.3322/caac.21551</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Holland</surname>
            <given-names>EC</given-names>
          </name>
        </person-group>
        <article-title>Progenitor cells and glioma formation</article-title>
        <source>Curr Opin Neurol</source>
        <year>2001</year>
        <volume>14</volume>
        <fpage>683</fpage>
        <lpage>688</lpage>
        <pub-id pub-id-type="doi">10.1097/00019052-200112000-00002</pub-id>
        <pub-id pub-id-type="pmid">11723374</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Buckner JC (2003) Factors influencing survival in high-grade gliomas. Seminars in oncology, vol 30. W.B. Saunders. 10.1053/j.seminoncol.2003.11.031</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lemke</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Scheele</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kapapa</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>von Karstedt</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wirtz</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>Henne-Bruns</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kornmann</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Brain metastases in gastrointestinal cancers: is there a role for surgery?</article-title>
        <source>Int J Mol Sci</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>9</issue>
        <fpage>16816</fpage>
        <lpage>16830</lpage>
        <pub-id pub-id-type="doi">10.3390/ijms150916816</pub-id>
        <?supplied-pmid 25247579?>
        <pub-id pub-id-type="pmid">25247579</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Miner</surname>
            <given-names>RC</given-names>
          </name>
        </person-group>
        <article-title>Image-guided neurosurgery</article-title>
        <source>J Med Imag Radiat Sci</source>
        <year>2017</year>
        <volume>48</volume>
        <issue>4</issue>
        <fpage>328</fpage>
        <lpage>335</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmir.2017.06.005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Coburger</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Merkel</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Scherer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Schwartz</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Gessler</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Roder</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Pala</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Konig</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Bullinger</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Nagel</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Jungk</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Bisdas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Nabavi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ganslandt</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Seifert</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Tatagiba</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Senft</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Mehdorn</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Unterberg</surname>
            <given-names>AW</given-names>
          </name>
          <name>
            <surname>Rossler</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Wirtz</surname>
            <given-names>CR</given-names>
          </name>
        </person-group>
        <article-title>Low-grade glioma surgery in intraoperative magnetic resonance imaging: results of a multicenter retrospective assessment of the German study group for intraoperative magnetic resonance imaging</article-title>
        <source>Neurosurgery</source>
        <year>2016</year>
        <volume>78</volume>
        <issue>6</issue>
        <fpage>775</fpage>
        <lpage>786</lpage>
        <pub-id pub-id-type="doi">10.1227/NEU.0000000000001081</pub-id>
        <?supplied-pmid 26516822?>
        <pub-id pub-id-type="pmid">26516822</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Siekmann</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lothes</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Konig</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wirtz</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>Coburger</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Experimental study of sector and linear array ultrasound accuracy and the influence of navigated 3D-reconstruction as compared to MRI in a brain tumor model</article-title>
        <source>Int J Comput Assist Radiol Surg</source>
        <year>2018</year>
        <volume>13</volume>
        <issue>3</issue>
        <fpage>471</fpage>
        <lpage>478</lpage>
        <pub-id pub-id-type="doi">10.1007/s11548-018-1705-y</pub-id>
        <?supplied-pmid 29368236?>
        <pub-id pub-id-type="pmid">29368236</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Karar</surname>
            <given-names>ME</given-names>
          </name>
          <name>
            <surname>Merk</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Falk</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Burgert</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>A simple and accurate method for computer-aided transapical aortic valve replacement</article-title>
        <source>Comput Med Imaging Graph</source>
        <year>2016</year>
        <volume>50</volume>
        <fpage>31</fpage>
        <lpage>41</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compmedimag.2014.09.005</pub-id>
        <?supplied-pmid 25306532?>
        <pub-id pub-id-type="pmid">25306532</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>AYC</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Corso</surname>
            <given-names>JJ</given-names>
          </name>
        </person-group>
        <article-title>Brain tumor detection and segmentation in a CRF (conditional random fields) framework with pixel-pairwise affinity and superpixel-level features</article-title>
        <source>Int J Comput Assist Radiol Surg</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>241</fpage>
        <lpage>253</lpage>
        <pub-id pub-id-type="doi">10.1007/s11548-013-0922-7</pub-id>
        <?supplied-pmid 23860630?>
        <pub-id pub-id-type="pmid">23860630</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ouyang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Loy</surname>
            <given-names>CC</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>DeepID-Net: object detection with deformable part based convolutional neural networks</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>2017</year>
        <volume>39</volume>
        <fpage>1320</fpage>
        <lpage>1334</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2587642</pub-id>
        <?supplied-pmid 27392342?>
        <pub-id pub-id-type="pmid">27392342</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shelhamer</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Long</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Darrell</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Fully convolutional networks for semantic segmentation</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>2017</year>
        <volume>39</volume>
        <fpage>640</fpage>
        <lpage>651</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2572683</pub-id>
        <?supplied-pmid 27244717?>
        <pub-id pub-id-type="pmid">27244717</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Saleh K, Zeineldin RA, Hossny M, Nahavandi S, El-Fishawy N (2018) End-to-end indoor navigation assistance for the visually impaired using monocular camera. Paper presented at the 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC),</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Russakovsky</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Krause</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Satheesh</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Karpathy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Khosla</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bernstein</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Berg</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Fei-Fei</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>ImageNet large scale visual recognition challenge</article-title>
        <source>Int J Comput Vision</source>
        <year>2015</year>
        <volume>115</volume>
        <fpage>211</fpage>
        <lpage>252</lpage>
        <pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Krizhevsky A, Sutskever I, Hinton GE (2017) ImageNet classification with deep convolutional neural networks, vol 60. Association for Computing Machinery. 10.1145/3065386</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Simonyan K, Zisserman A (2014) Very deep convolutional networks for large-scale image recognition</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recognition. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition, vol 2016-Decem. 10.1109/CVPR.2016.90</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Huang G, Liu Z, Van Der Maaten L, Weinberger KQ (2017) Densely connected convolutional networks. In: Proceedings - 30th IEEE conference on computer vision and pattern recognition, CVPR 2017, vol 2017-January. Institute of Electrical and Electronics Engineers Inc. 10.1109/CVPR.2017.243</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Chollet F (2017) Xception: deep learning with depthwise separable convolutions. In: Proceedings–30th IEEE conference on computer vision and pattern recognition, CVPR 2017, vol 2017-January. Institute of Electrical and Electronics Engineers Inc. 10.1109/CVPR.2017.195</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, Andreetto M, Adam H (2017) MobileNets: efficient convolutional neural networks for mobile vision applications.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Zoph B, Vasudevan V, Shlens J, Le QV (2018) Learning transferable architectures for scalable image recognition. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition, pp 8697–8710. 10.1109/CVPR.2018.00907</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Sandler M, Howard A, Zhu M, Zhmoginov A, Chen LC (2018) MobileNetV2: inverted residuals and linear bottlenecks. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition, pp 4510–4520. 10.1109/CVPR.2018.00474</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Naceur</surname>
            <given-names>MB</given-names>
          </name>
          <name>
            <surname>Saouli</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Akil</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kachouri</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Fully automatic brain tumor segmentation using end-to-end incremental deep neural networks in MRI images</article-title>
        <source>Comput Methods Programs Biomed</source>
        <year>2018</year>
        <volume>166</volume>
        <fpage>39</fpage>
        <lpage>49</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2018.09.007</pub-id>
        <?supplied-pmid 30415717?>
        <pub-id pub-id-type="pmid">30415717</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Havaei</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Davy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Warde-Farley</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Biard</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Courville</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Pal</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Jodoin</surname>
            <given-names>PM</given-names>
          </name>
          <name>
            <surname>Larochelle</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Brain tumor segmentation with deep neural networks</article-title>
        <source>Med Image Anal</source>
        <year>2017</year>
        <volume>35</volume>
        <fpage>18</fpage>
        <lpage>31</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2016.05.004</pub-id>
        <?supplied-pmid 27310171?>
        <pub-id pub-id-type="pmid">27310171</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Cireşan DC, Giusti A, Gambardella LM, Schmidhuber J (2012) Deep neural networks segment neuronal membranes in electron microscopy images. In: Advances in neural information processing systems, vol 4</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ronneberger</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>U-net: convolutional networks for biomedical image segmentation</article-title>
        <source>Lect Notes Comput Sciss</source>
        <year>2015</year>
        <volume>9351</volume>
        <fpage>234</fpage>
        <lpage>241</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Dosovitskiy A, Springenberg T, Riedmiller M, Brox T discriminative unsupervised feature learning with convolutional neural networks. In: Advances in neural information processing systems</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ronneberger</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>U-net: convolutional networks for biomedical image segmentation</article-title>
        <source>Lect Notes Comput Sci</source>
        <year>2015</year>
        <pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dropout: A simple way to prevent neural networks from overfitting</article-title>
        <source>J Mach Learn Res</source>
        <year>2014</year>
        <volume>15</volume>
        <fpage>1929</fpage>
        <lpage>1958</lpage>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Ioffe S, Szegedy C (2015) Batch normalization: accelerating deep network training by reducing internal covariate shift. In: 32nd international conference on machine learning, ICML 2015, vol 1. International Machine Learning Society (IMLS)</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Glorot X, Bengio Y (2010) Understanding the difficulty of training deep feedforward neural networks. J Mach Learn Res, vol 9.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Menze</surname>
            <given-names>BH</given-names>
          </name>
          <name>
            <surname>Jakab</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bauer</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kalpathy-Cramer</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Farahani</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kirby</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Burren</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Porz</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Slotboom</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wiest</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lanczi</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Gerstner</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Weber</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Arbel</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Avants</surname>
            <given-names>BB</given-names>
          </name>
          <name>
            <surname>Ayache</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Buendia</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Collins</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Cordier</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Corso</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Criminisi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Delingette</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Demiralp</surname>
            <given-names>Ç</given-names>
          </name>
          <name>
            <surname>Durst</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>Dojat</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Doyle</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Festa</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Forbes</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Geremia</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Glocker</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Golland</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Hamamci</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Iftekharuddin</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Jena</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>John</surname>
            <given-names>NM</given-names>
          </name>
          <name>
            <surname>Konukoglu</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Lashkari</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Mariz</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Meier</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Pereira</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Precup</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Price</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Raviv</surname>
            <given-names>TR</given-names>
          </name>
          <name>
            <surname>Reza</surname>
            <given-names>SMS</given-names>
          </name>
          <name>
            <surname>Ryan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sarikaya</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Schwartz</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Shin</surname>
            <given-names>HC</given-names>
          </name>
          <name>
            <surname>Shotton</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Silva</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Sousa</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Subbanna</surname>
            <given-names>NK</given-names>
          </name>
          <name>
            <surname>Szekely</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Thomas</surname>
            <given-names>OM</given-names>
          </name>
          <name>
            <surname>Tustison</surname>
            <given-names>NJ</given-names>
          </name>
          <name>
            <surname>Unal</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Vasseur</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Wintermark</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>DH</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zikic</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Prastawa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Reyes</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Van Leemput</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>The multimodal brain tumor image segmentation benchmark (BRATS)</article-title>
        <source>IEEE Trans Med Imaging</source>
        <year>2015</year>
        <volume>34</volume>
        <fpage>1993</fpage>
        <lpage>2024</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2014.2377694</pub-id>
        <?supplied-pmid 25494501?>
        <pub-id pub-id-type="pmid">25494501</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>YM</given-names>
          </name>
          <name>
            <surname>Suki</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hess</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sawaya</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>The influence of maximum safe resection of glioblastoma on survival in 1229 patients: can we do better than gross-total resection?</article-title>
        <source>J Neurosurg</source>
        <year>2016</year>
        <volume>124</volume>
        <issue>4</issue>
        <fpage>977</fpage>
        <lpage>988</lpage>
        <pub-id pub-id-type="doi">10.3171/2015.5.JNS142087</pub-id>
        <?supplied-pmid 26495941?>
        <pub-id pub-id-type="pmid">26495941</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tustison</surname>
            <given-names>NJ</given-names>
          </name>
          <name>
            <surname>Avants</surname>
            <given-names>BB</given-names>
          </name>
          <name>
            <surname>Cook</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Egan</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Yushkevich</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>Gee</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>N4ITK: improved N3 bias correction</article-title>
        <source>IEEE Trans Med Imaging</source>
        <year>2010</year>
        <volume>29</volume>
        <issue>6</issue>
        <fpage>1310</fpage>
        <lpage>1320</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id>
        <?supplied-pmid 20378467?>
        <pub-id pub-id-type="pmid">20378467</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J (2014) Adam: a method for stochastic optimization</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Diba A, Sharma V, Pazandeh A, Pirsiavash H, Gool LV (2017) Weakly supervised cascaded convolutional networks. In: 2017 IEEE conference on computer vision and pattern recognition (CVPR), 21–26 July 2017, pp 5131–5139. 10.1109/CVPR.2017.545</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>LC</given-names>
          </name>
          <name>
            <surname>Papandreou</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kokkinos</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Yuille</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>2018</year>
        <volume>40</volume>
        <fpage>834</fpage>
        <lpage>848</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id>
        <?supplied-pmid 28463186?>
        <pub-id pub-id-type="pmid">28463186</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
