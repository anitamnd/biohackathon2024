<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9272808</article-id>
    <article-id pub-id-type="pmid">35652721</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac377</article-id>
    <article-id pub-id-type="publisher-id">btac377</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Structural Bioinformatics</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Effective drug–target interaction prediction with mutual interaction neural network</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3685-7164</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Fei</given-names>
        </name>
        <aff><institution>School of Computer Science, Fudan University</institution>, Shanghai 200438, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Ziqiao</given-names>
        </name>
        <aff><institution>School of Computer Science, Fudan University</institution>, Shanghai 200438, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Guan</surname>
          <given-names>Jihong</given-names>
        </name>
        <aff><institution>Department of Computer Science and Technology, Tongji University</institution>, Shanghai 201804, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1949-2768</contrib-id>
        <name>
          <surname>Zhou</surname>
          <given-names>Shuigeng</given-names>
        </name>
        <aff><institution>School of Computer Science, Fudan University</institution>, Shanghai 200438, <country country="CN">China</country></aff>
        <aff><institution>Shanghai Key Lab of Intelligent Information Processing</institution>, Shanghai 200438, <country country="CN">China</country></aff>
        <xref rid="btac377-cor1" ref-type="corresp"/>
        <!--sgzhou@fudan.edu.cn-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Martelli</surname>
          <given-names>Pier Luigi</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btac377-cor1">To whom correspondence should be addressed. Email: <email>sgzhou@fudan.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>15</day>
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-06-02">
      <day>02</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>02</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <volume>38</volume>
    <issue>14</issue>
    <fpage>3582</fpage>
    <lpage>3589</lpage>
    <history>
      <date date-type="received">
        <day>11</day>
        <month>2</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>09</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>25</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>31</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>23</day>
        <month>6</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac377.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Accurately predicting <italic toggle="yes">drug–target interaction</italic> (DTI) is a crucial step to drug discovery. Recently, deep learning techniques have been widely used for DTI prediction and achieved significant performance improvement. One challenge in building deep learning models for DTI prediction is how to appropriately represent drugs and targets. Target distance map and molecular graph are low dimensional and informative representations, which however have not been jointly used in DTI prediction. Another challenge is how to effectively model the mutual impact between drugs and targets. Though attention mechanism has been used to capture the one-way impact of targets on drugs or vice versa, the mutual impact between drugs and targets has not yet been explored, which is very important in predicting their interactions.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Therefore, in this article we propose MINN-DTI, a new model for DTI prediction. MINN-DTI combines an interacting-transformer module (called Interformer) with an improved Communicative Message Passing Neural Network (CMPNN) (called Inter-CMPNN) to better capture the two-way impact between drugs and targets, which are represented by molecular graph and distance map, respectively. The proposed method obtains better performance than the state-of-the-art methods on three benchmark datasets: DUD-E, human and BindingDB. MINN-DTI also provides good interpretability by assigning larger weights to the amino acids and atoms that contribute more to the interactions between drugs and targets.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The data and code of this study are available at <ext-link xlink:href="https://github.com/admislf/MINN-DTI" ext-link-type="uri">https://github.com/admislf/MINN-DTI</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>2021 Tencent AI Lab Rhino-Bird Focused Research Program</institution>
          </institution-wrap>
        </funding-source>
        <award-id>JR202104</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>61972100</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NSFC</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>61772367</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>In drug discovery and design, verifying whether a drug interacts with a certain target is a key step to prove the effectiveness of the drug. Since large-scale <italic toggle="yes">in vitro</italic> and <italic toggle="yes">in vivo</italic> experiments are high-cost and time consuming, computational methods for <italic toggle="yes">drug–</italic><italic toggle="yes">target interaction</italic> (DTI in short) prediction have received increasing attention. However, traditional computational methods have obvious limitations. For example, the widely used molecular docking is inefficient and sometimes ineffective because of its huge amount of computation and inaccurate scoring function (<xref rid="btac377-B28" ref-type="bibr">Su <italic toggle="yes">et al.</italic>, 2019</xref>). On the other hand, traditional machine learning models such as Random Forest (RF) and Support Vector Machine (SVM) have also been used for DTI prediction (<xref rid="btac377-B2" ref-type="bibr">Ballester and Mitchell, 2010</xref>; <xref rid="btac377-B3" ref-type="bibr">Bleakley and Yamanishi, 2009</xref>; <xref rid="btac377-B16" ref-type="bibr">Liu <italic toggle="yes">et al.</italic>, 2015</xref>). These methods are generally simple and efficient, but the performance is far from satisfaction. Recently, the introduction of deep learning models to DTI prediction has greatly advanced this area (<xref rid="btac377-B1" ref-type="bibr">Bagherian <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac377-B29" ref-type="bibr">Tian <italic toggle="yes">et al.</italic>, 2016</xref>). In deep learning-based works, DTI prediction can be characterized as a binary classification, a ranking task or a regression task for binding affinity. Usually, deep learning models for DTI prediction are composed of a target feature extraction module, a drug feature extraction module and a prediction module. These modules are carefully designed according to various practical factors including the input representations.</p>
    <p>The most commonly used representations of drugs and targets are one-dimensional (1D) sequences such as Simplified Molecular Input Line Entry Specification (SMILES) strings for drugs and amino acid sequences for targets (<xref rid="btac377-B12" ref-type="bibr">Karimi <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac377-B17" ref-type="bibr">Liu <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac377-B22" ref-type="bibr">Peng <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac377-B32" ref-type="bibr">Tsubaki <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2020</xref>). The models with 1D sequences as input generally use Convolutional Neural network (CNN), Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM) blocks or Gated Recurrent Unit (GRU) blocks to extract drug and target features. For instance, Ozturk <italic toggle="yes">et al.</italic> used CNN modules to extract hidden representations of amino acid sequences and SMILES strings, which were later combined and input to a multi-layer perceptron to perform the prediction (<xref rid="btac377-B20" ref-type="bibr">Ozturk <italic toggle="yes">et al.</italic>, 2018</xref>). Besides sequences, feature vectors representing physicochemical properties of targets/drugs and drug molecular fingerprints are also common forms of 1D input to DTI prediction models (<xref rid="btac377-B14" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac377-B15" ref-type="bibr">Lenselink <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac377-B25" ref-type="bibr">Rifaioglu <italic toggle="yes">et al.</italic>, 2021</xref>). Slightly different from the work of Ozturk <italic toggle="yes">et al.</italic>, instead of SMILES strings, Lee <italic toggle="yes">et al.</italic> used molecular fingerprints with fully connected networks to extract drug features to predict DTI prediction (<xref rid="btac377-B14" ref-type="bibr">Lee <italic toggle="yes">et al.</italic>, 2019</xref>). According to the research of <xref rid="btac377-B15" ref-type="bibr">Lenselink et al. (2017)</xref>, with feature vectors composed of physicochemical properties of targets/drugs and molecular fingerprints as input, Deep Neural Network (DNN) models outperform Naive Bayes, RF, SVM, logistic regression models in DTI prediction. Two-dimensional (2D) paired feature map has also been used for DTI prediction, which uses a matrix of a specific property calculated for amino acid pairs in the corresponding protein sequence to represent a target (<xref rid="btac377-B25" ref-type="bibr">Rifaioglu <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2020</xref>). For example, Rifaioglu <italic toggle="yes">et al.</italic> used multi-channel protein feature maps involving sequence, structural, evolutionary and physicochemical properties to represent proteins for proteochemometric protein-drug binding affinity prediction (<xref rid="btac377-B25" ref-type="bibr">Rifaioglu <italic toggle="yes">et al.</italic>, 2021</xref>). 2D structural images are intuitive representations of drugs and have been used in DTI prediction (<xref rid="btac377-B24" ref-type="bibr">Rifaioglu <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac377-B35" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2021</xref>). Wang <italic toggle="yes">et al.</italic> proposed an efficient DTI prediction system using only 2D images as input, which includes CNN-based models for 704 targets (<xref rid="btac377-B24" ref-type="bibr">Rifaioglu <italic toggle="yes">et al.</italic>, 2020</xref>). Two-dimensional (2D) molecular graph is another effective representation of drugs, which has been widely used in Graph Neural Network (GNN)-based models for predicting molecular properties (<xref rid="btac377-B8" ref-type="bibr">Gilmer <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac377-B27" ref-type="bibr">Song <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac377-B37" ref-type="bibr">Xiong <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac377-B38" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac377-B39" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic>, 2021</xref>). Recently, molecular graph has also been increasingly used in DTI prediction (<xref rid="btac377-B19" ref-type="bibr">Nguyen <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac377-B30" ref-type="bibr">Torng and Altman, 2019</xref>; <xref rid="btac377-B32" ref-type="bibr">Tsubaki <italic toggle="yes">et al.</italic>, 2019</xref>). Tsubaki <italic toggle="yes">et al.</italic> used GNN to extract the information of a small molecule as a feature vector, which is then concatenated with the target feature vector extracted by CNN from amino acid sequence to make prediction (<xref rid="btac377-B32" ref-type="bibr">Tsubaki <italic toggle="yes">et al.</italic>, 2019</xref>). Compared with SMILES-based models, graph-based models can readily exploit topological information of molecules, which show obvious advantages in the task of DTI prediction.</p>
    <p>Although 1D and 2D-based DTI prediction models have made significant progress recently, as DTI is in essence three-dimensional (3D) physical interaction, so it is natural and reasonable to predict DTI using three-dimensional structural information. Many studies directly use 3D Cartesian coordinates to represent the 3D structures of targets, but the limited samples cannot cover such a huge space, which leads to poor performance (<xref rid="btac377-B23" ref-type="bibr">Ragoza <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac377-B34" ref-type="bibr">Wallach <italic toggle="yes">et al.</italic>, 2015</xref>; <xref rid="btac377-B40" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2019</xref>). Some models apply 3D voxel grids to represent targets and use 3D-CNN to extract target features, their accuracy is limited because the grid coordinates are not accurate enough to represent the spatial positions of targets’ atoms. On the other hand, 2D paired distance map represents 3D structure of each target by a matrix of internal pairwise distances between the amino acids of the target, which has been mainly used in protein structure prediction (<xref rid="btac377-B26" ref-type="bibr">Skolnick <italic toggle="yes">et al.</italic>, 1997</xref>). Recently, Zheng <italic toggle="yes">et al.</italic> proposed an advanced model called drugVQA to predict DTI, where LSTM and dynamic 2D-CNN were used to extract features from SMILES strings and distance maps, respectively (<xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2020</xref>), and achieved satisfactory performance. Though 2D paired distance map and molecular graph are promising representations of targets and drugs, respectively. However, they have not yet been jointly used for DTI prediction.</p>
    <p>After extracting feature vectors from the drug and the target, respectively, the features are usually concatenated and input to a MLP to predict DTI. In most existing models, targets and drugs are represented and processed separately, they can hardly capture the interacting context between targets and drug molecules. The attention mechanism is often used to acquire the contributions of different components of a drug or target to the interaction (<xref rid="btac377-B12" ref-type="bibr">Karimi <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac377-B32" ref-type="bibr">Tsubaki <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2020</xref>), which has been recently used to characterize the interactions between targets and drugs (<xref rid="btac377-B4" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020</xref>, <xref rid="btac377-B5" ref-type="bibr">2021</xref>). The attention mechanism was found to be able to better capture the impact of targets on drugs and vice versa, so as to obtain better representations. However, these models pay attention only to the impact of one participant of a DTI on the counterpart, such as target on drug, but ignore the reverse impact. According to the induced-fit theory (<xref rid="btac377-B11" ref-type="bibr">Johnson, 2008</xref>), interacting drug molecules and targets are mutually impacted. Therefore, it is natural and reasonable to consider their mutual impacts when learning to represent the drugs and targets for DTI prediction.</p>
    <p>In this article, to overcome the above-mentioned drawbacks of existing DTI prediction models, we propose a new model for DTI prediction. In this model, 2D paired distance maps of proteins and molecular graphs are served as inputs for targets and drugs, respectively. To capture the interactive impacts between targets and drugs, we design a mutual interaction neural network (MINN) by innovatively combining two interacting-transformers (Interformer in short) with an improved Communicative Message Passing Neural Network (CMPNN) (called Inter-CMPNN). In the experiments, our model achieves better performance than state-of-the-art methods on DUD-E, human and BindingDB benchmark datasets. Case studies show that individual contributions of residues in the target and atoms in the drug to the formation of DTI can be inferred from learned attention weights, which indicates that our proposed model is interpretable and can help to explain the drug action mechanism, and suggests the direction of drug optimization in the future.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Overview</title>
      <p>The architecture of our proposed model MINN-DTI is shown in <xref rid="btac377-F1" ref-type="fig">Figure 1</xref>. It consists of three modules: a target preprocessing network (TPN), a MINN and an interaction prediction network (IPN). MINN is the core component of our model, which consists of an Interformer module and an Inter-CMPNN module. The Interformer module is constructed by two interacting transformer decoders, and the Inter-CMPNN module is a variant of CMPNN. With these two modules, we can extract the latent vectors of targets and drugs while considering their interacting contexts.</p>
      <fig position="float" id="btac377-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Architecture of MINN-DTI</p>
        </caption>
        <graphic xlink:href="btac377f1" position="float"/>
      </fig>
      <p>The 2D distance map and 2D molecular graph are served as the input representation of a given target and a molecule, respectively. The molecular graph is directly used, while the distance map is firstly preprocessed by the target preprocessing network (TPN<bold>)</bold>. The latent feature vectors of both the target and the molecule are extracted by MINN. These latent feature vectors are then concatenated and fed to the interaction prediction network (IPN) to predict DTI. The details of these modules are presented in the following sections.</p>
    </sec>
    <sec>
      <title>2.2 Target preprocessing network</title>
      <p>Given a target, we calculate a 2D paired distance map (<xref rid="btac377-B26" ref-type="bibr">Skolnick <italic toggle="yes">et al.</italic>, 1997</xref>), which is subsequently preprocessed by the target preprocessing network (TPN) into a fixed-size matrix following drugVQA (<xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2020</xref>). TPN is implemented by a dynamic attentive CNN. As shown in <xref rid="btac377-F2" ref-type="fig">Figure 2</xref>, the dynamic attentive CNN is composed of a Dynamic CNN (DyCNN) block and a Sequential Self-Attention (SSA) block. The DyCNN block contains a number of residual blocks and an average pooling layer as ResNet (<xref rid="btac377-B10" ref-type="bibr">He <italic toggle="yes">et al.</italic>, 2016</xref>). To handle targets with different lengths, the pooling layers between the residual blocks are eliminated. Through the DyCNN, a 2D paired distance map <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mi>P</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is transformed into a feature map <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, where <italic toggle="yes">f</italic> is the number of filters of the residual block. With the SSA block, a weight matrix <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is derived by a two-layer perceptron without bias from <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>:
<disp-formula id="E1"><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mi>tanh</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>P</mml:mi><mml:mi>c</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are learnable parameters. This multilayer perceptron (MLP<bold>)</bold> block can be regarded as a multi-head attention where the number of neurons <italic toggle="yes">r</italic> in the last layer is interpreted as the number of attentional heads. The attentional feature map <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>×</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal"> </mml:mi></mml:math></inline-formula>is derived by multiplying <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, which indicates the relative importance of amino acid sites.</p>
      <fig position="float" id="btac377-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>The structure of target preprocessing network</p>
        </caption>
        <graphic xlink:href="btac377f2" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.3 Mutual interaction neural network (MINN)</title>
      <p>The goal of MINN is to learn the representations of targets and drugs while considering their interacting contexts. For this purpose, we design an Interformer module to interact the information extracted from targets and small molecules and an Inter-CMPNN module to support information interacting from the drug side. Thus, we can get more comprehensive representations of the targets and drugs, and consequently boost DTI prediction.</p>
      <sec>
        <label>2.3.1</label>
        <title>Interformer</title>
        <p>Here, we use two interacting transformer decoders to extract feature vectors of targets and drugs, which is called Interformer in short. The structure of Interformer is shown in <xref rid="btac377-F3" ref-type="fig">Figure 3</xref>. Each decoder of Interformer consists of one or more identical layers, similar to transformer (<xref rid="btac377-B33" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic>, 2017</xref>). Each layer of Interformer consists of three sublayers: a multi-head self-attention layer, an interaction attention layer and a fully connected feed-forward network. The multi-head self-attention sublayer and the feed-forward sublayer are essentially consistent with transformer, except that the mask operation is eliminated to leverage complete drug and target information following the work of <xref rid="btac377-B4" ref-type="bibr">Chen <italic toggle="yes">et al.</italic> (2020)</xref>.</p>
        <fig position="float" id="btac377-F3">
          <label>Fig. 3.</label>
          <caption>
            <p>The structure of Interformer</p>
          </caption>
          <graphic xlink:href="btac377f3" position="float"/>
        </fig>
        <p>The interaction attention layer in each decoder of Interformer adopts a multi-head scaled dot attention block to receive the external information from another decoder. The source of external information is the biggest difference between interaction attention layer of Interformer and encode-decoder layer of transformer, where the source of external information is the encoder. A scaled-dot attention block can be expressed as:
<disp-formula id="E2"><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo stretchy="true">)</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">Q</italic> is the linearly transformed output of the multi head self-attention layer of the decoder, <italic toggle="yes">K</italic> and <italic toggle="yes">V</italic> are linearly transformed outputs of the last layer of another transformer decoder, and <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the dimension of <italic toggle="yes">K</italic> and <italic toggle="yes">V</italic>.
<disp-formula id="E3"><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mtext>head</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>Attention</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>Q</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>K</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>V</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>MultiHead</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>Concat</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mtext>head</mml:mtext></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext>head</mml:mtext></mml:mrow><mml:mi>h</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>O</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> are parameter matrices. With the Interformer, the representation of each small molecule/target is involved with the information of the corresponding interacted target/drug, which conforms to the real situation of DTI.</p>
      </sec>
      <sec>
        <label>2.3.2</label>
        <title>Inter-CMPNN</title>
        <p>The Inter-CMPNN module is implemented by an improved CMPNN, which is a variant of message passing neural network based on directed graph (<xref rid="btac377-B27" ref-type="bibr">Song <italic toggle="yes">et al.</italic>, 2020</xref>). CMPNN strengthens the message interaction between nodes and edges through three well designed modules (AGGREGATE, COMMUNICATE, UPDATE) for <italic toggle="yes">L</italic> iterations:
<disp-formula id="E4"><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>AGGREGATE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>COMMUNICATE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>UPDATE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is the message obtained by node <italic toggle="yes">v</italic> in iteration <italic toggle="yes">k</italic>, <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is the hidden representation of node <italic toggle="yes">v</italic> in iteration <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mi>k</mml:mi></mml:math></inline-formula>, <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the hidden representation of edge <italic toggle="yes">e</italic> in iteration <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mi>k</mml:mi></mml:math></inline-formula>. After <italic toggle="yes">L</italic> iterations, one more iteration is executed to exchange information more thoroughly:
<disp-formula id="E5"><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mtext>AGGREGATE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mi>L</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mtext>COMMUNICATE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mi>L</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mi>x</mml:mi></mml:math></inline-formula> is the raw features of atoms. The <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mi mathvariant="normal">AGGREGATE</mml:mi></mml:math></inline-formula> module incorporates a message booster, which processes the information of edges with a maximum pooling layer and calculates the element-wise product of the maximum pooling layer’s result and the sum of the hidden representations of edges. The <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mi mathvariant="normal">UPDATE</mml:mi></mml:math></inline-formula> module is a single-layer neural network with a skip connection, and the <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mi mathvariant="normal">COMMUNICATE</mml:mi></mml:math></inline-formula> module takes the form of a multilayer perception.</p>
        <p>Here, an Interformer is adopted after the <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mi mathvariant="normal">COMMUNICATE</mml:mi></mml:math></inline-formula> function to completely exploit the mutual impacts between targets and drug molecules:
<disp-formula id="E6"><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>AGGREGATE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>COMMUNICATE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>m</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>INTERFORMER</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>A</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>UPDATE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is the target feature map in iteration <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mi>k</mml:mi></mml:math></inline-formula>. Similarly, one more iteration is executed:
<disp-formula id="E7"><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">G</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mi>L</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">U</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mi>L</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>a</mml:mi><mml:mi>o</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msubsup><mml:mi>A</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is the final drug graph feature and target feature map. The schematic diagram of message interaction between Interformer and Inter-CMPNN is shown in <xref rid="btac377-F4" ref-type="fig">Figure 4</xref>. The last hidden atom representations of each molecular graph and the feature map vectors of each target are averaged to obtain fix-sized vectors of the target and the small molecule, which are then fed to the interaction prediction network.</p>
        <fig position="float" id="btac377-F4">
          <label>Fig. 4.</label>
          <caption>
            <p>Message interaction between Interformer and Inter-CMPNN</p>
          </caption>
          <graphic xlink:href="btac377f4" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>2.4 Interaction prediction network (IPN)</title>
      <p>The obtained target feature vector <italic toggle="yes">T</italic> and small molecule feature vector <italic toggle="yes">D</italic> are concatenated and fed to a two-layer perceptron without bias to obtain the prediction result:
<disp-formula id="E8"><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are learnable weight parameters. Since the prediction of DTI is regarded as a binary classification problem, cross entropy is used as the loss function to train the model.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Experiments and results</title>
    <sec>
      <title>3.1 Datasets</title>
      <p>We evaluated our model MINN-DTI and compared it with state-of-the-art DTI prediction methods on three widely used public datasets, human dataset, DUD-E dataset and BindingDB dataset.</p>
      <sec>
        <title>3.1.1 The DUD-E dataset</title>
        <p>The DUD-E dataset consists of 22 886 active compounds against 102 targets. For each active compound, 50 decoys are generated, which have similar physico-chemical properties but dissimilar 2-D topologies to the active compound (<xref rid="btac377-B18" ref-type="bibr">Mysinger <italic toggle="yes">et al.</italic>, 2012</xref><bold>)</bold>. We processed the DUD-E dataset following the works of <xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic> (2020</xref><bold>)</bold> and <xref rid="btac377-B23" ref-type="bibr">Ragoza <italic toggle="yes">et al.</italic> (2017)</xref>, we obtained 22 645 positive examples and 1 407 145 negative examples, which were split according to a threefold cross-validation strategy. Ligands for the targets belonging to the same target family are put into the same fold. We randomly selected the same number of negative samples as the active samples in training to obtain a balanced model, but we used unbalanced data in model evaluation.</p>
      </sec>
      <sec>
        <label>3.1.2</label>
        <title>The human dataset</title>
        <p>The human dataset contains highly credible positive and negative CPI samples extracted by a systematic screening framework according to a similarity rule (<xref rid="btac377-B16" ref-type="bibr">Liu <italic toggle="yes">et al.</italic>, 2015</xref>). Following the works of <xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic> (2020)</xref> and <xref rid="btac377-B32" ref-type="bibr">Tsubaki <italic toggle="yes">et al.</italic> (2019)</xref>, we used a dataset with equal number of positive and negative samples, forming 3369 positive interactions between 1052 unique compounds and 852 unique targets. Then, the dataset was randomly divided into a training set, a validation set and a test set according to the ratio of 8:1:1. To evaluate the generalization power of our model on different data, we redivided the human dataset according to molecular scaffold similarities between drugs. The scaffold-based split method implemented in the open source DeepChem package of MoleculeNet (<xref rid="btac377-B36" ref-type="bibr">Wu <italic toggle="yes">et al.</italic>, 2018</xref>) was used to divide structurally different molecules into training/validation/test sets according to the ratio of 8:1:1.</p>
      </sec>
      <sec>
        <label>3.1.3</label>
        <title>The BindingDB dataset</title>
        <p>The BindingDB dataset (<xref rid="btac377-B7" ref-type="bibr">Gao <italic toggle="yes">et al.</italic>, 2018</xref>) was a customized subset of the Binding database (<xref rid="btac377-B9" ref-type="bibr">Gilson <italic toggle="yes">et al.</italic>, 2016</xref>), which is a publicly accessible database that mainly contains the interaction affinities between targets and drug-like small molecules. The BindingDB dataset contains 39 747 positive samples and 31 218 negative samples, which was divided into a large training set (50 155 samples), a validation set (5607 samples) and a test set (5508 samples). To evaluate the generalization power of our model on novel targets, the data of the test set were divided into two parts to check the model performance according to whether the targets appear in the training set.</p>
      </sec>
    </sec>
    <sec>
      <title>3.2 Implementation details and experimental settings</title>
      <p>We implemented MINN-DTI with Pytorch 1.7.1 (<xref rid="btac377-B21" ref-type="bibr">Paszke <italic toggle="yes">et al.</italic>, 2019</xref>). The Adam optimizer was used in training and the learning rate was set to 0.0001 (<xref rid="btac377-B13" ref-type="bibr">Kingma and Ba, 2015</xref>). The number of residual blocks, the number of filters and the dimension of molecular graph features were set to 32. We explored hyperparameters on the human dataset, and the best parameters are listed in <xref rid="btac377-T1" ref-type="table">Table 1</xref>. Following the work of <xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic> (2020)</xref>, hyperparameter optimization was not performed for the DUD-E and BindingDB datasets. All experiments were conducted on NVIDIA RTX3090 GPUs.</p>
      <table-wrap position="float" id="btac377-T1">
        <label>Table 1.</label>
        <caption>
          <p>Hyperparameter setting in MINN-DTI</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Hyperparameter</th>
              <th rowspan="1" colspan="1">Value</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Learning rate</td>
              <td rowspan="1" colspan="1">0.0001</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Number of residual blocks</td>
              <td rowspan="1" colspan="1">32</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Number of filters</td>
              <td rowspan="1" colspan="1">32</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Graph feature size</td>
              <td rowspan="1" colspan="1">32</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Attention heads</td>
              <td rowspan="1" colspan="1">8</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Hidden size of Decoder</td>
              <td rowspan="1" colspan="1">32</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Iterations of message passing</td>
              <td rowspan="1" colspan="1">4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Dropout</td>
              <td rowspan="1" colspan="1">0.2</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>3.3 Performance metrics</title>
      <p>Here, different metrics were used on different datasets following previous works (<xref rid="btac377-B4" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2020</xref>). The area under the receiver operating characteristic curve (AUC) was used as the main metric to evaluate our model. Besides, the ROC enrichment metric (RE) that describes the ratio of the true positive rate (TPR) to the false positive rate (FPR) at a given FPR threshold was used for performance assessment on the DUD-E dataset, where FPR was set to 0.5%, 1%, 2% and 5%, respectively as the threshold. Moreover, recall and precision were used for performance evaluation on the human dataset, while the area under precision recall curve (PRC) was applied to performance evaluation on the BindingDB dataset. We repeated each experiment three times with different seeds to calculate the mean and the standard deviation as in the work of <xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic> (2020)</xref>.</p>
    </sec>
    <sec>
      <title>3.4 Results</title>
      <sec>
        <label>3.4.1</label>
        <title>Performance on the DUD-E dataset</title>
        <p>Here, we compared our model with different types of existing methods on DUD-E. These compared methods include two traditional machine learning-based methods NNscore (<xref rid="btac377-B6" ref-type="bibr">Durrant and McCammon, 2011</xref>) and RF-score (<xref rid="btac377-B2" ref-type="bibr">Ballester and Mitchell, 2010</xref>), a docking-based method Vina (<xref rid="btac377-B31" ref-type="bibr">Trott and Olson, 2009</xref>) and three recent deep learning-based methods 3D-CNN (<xref rid="btac377-B23" ref-type="bibr">Ragoza <italic toggle="yes">et al.</italic>, 2017</xref>), PocketGCN (<xref rid="btac377-B30" ref-type="bibr">Torng and Altman, 2019</xref>) and DrugVQA (<xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2020</xref>). As shown in <xref rid="btac377-T2" ref-type="table">Table 2</xref>, MINN-DTI has significant advantage in terms of AUC and RE. The AUC of MINN-DTI is about 2.5% higher than that of the state-of-the-art method DrugVQA, and over 10% higher than that of the other methods. In terms of RE, MINN-DTI is at least twice as high as all the other methods. These results show that our method is more effective in drug screening, since the number of positive samples in DUD-E is much less than that of negative samples, which is close to the actual situation of virtual screening. In addition, deep learning-based methods are obviously superior to the descriptor-based traditional machine learning methods and the docking-based method in terms of AUC and RE, which suggests that deep learning-based methods are more effective in learning the representations of drugs and targets.</p>
        <table-wrap position="float" id="btac377-T2">
          <label>Table 2.</label>
          <caption>
            <p>Performance comparison between our model and existing methods on the DUD-E dataset</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Model</th>
                <th align="center" rowspan="1" colspan="1">AUC</th>
                <th align="center" rowspan="1" colspan="1">0.5% RE</th>
                <th align="center" rowspan="1" colspan="1">1.0% RE</th>
                <th align="center" rowspan="1" colspan="1">2.0% RE</th>
                <th align="center" rowspan="1" colspan="1">5.0% RE</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">NNscore<xref rid="tblfn2" ref-type="table-fn"><sup>a</sup></xref></td>
                <td rowspan="1" colspan="1">0.584</td>
                <td rowspan="1" colspan="1">4.166</td>
                <td rowspan="1" colspan="1">2.980</td>
                <td rowspan="1" colspan="1">2.460</td>
                <td rowspan="1" colspan="1">1.891</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">RF-score<xref rid="tblfn2" ref-type="table-fn"><sup>a</sup></xref></td>
                <td rowspan="1" colspan="1">0.622</td>
                <td rowspan="1" colspan="1">5.628</td>
                <td rowspan="1" colspan="1">4.274</td>
                <td rowspan="1" colspan="1">3.499</td>
                <td rowspan="1" colspan="1">2.678</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Vina<sup>a</sup></td>
                <td rowspan="1" colspan="1">0.716</td>
                <td rowspan="1" colspan="1">9.139</td>
                <td rowspan="1" colspan="1">7.321</td>
                <td rowspan="1" colspan="1">5.811</td>
                <td rowspan="1" colspan="1">4.444</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">3D-CNN<xref rid="tblfn2" ref-type="table-fn"><sup>a</sup></xref></td>
                <td rowspan="1" colspan="1">0.868</td>
                <td rowspan="1" colspan="1">42.559</td>
                <td rowspan="1" colspan="1">26.655</td>
                <td rowspan="1" colspan="1">19.363</td>
                <td rowspan="1" colspan="1">10.710</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">PocketGCN<xref rid="tblfn2" ref-type="table-fn"><sup>a</sup></xref></td>
                <td rowspan="1" colspan="1">0.886</td>
                <td rowspan="1" colspan="1">44.406</td>
                <td rowspan="1" colspan="1">29.748</td>
                <td rowspan="1" colspan="1">19.408</td>
                <td rowspan="1" colspan="1">10.735</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">DrugVQA<xref rid="tblfn2" ref-type="table-fn"><sup>a</sup></xref></td>
                <td rowspan="1" colspan="1">0.972 ± 0.003</td>
                <td rowspan="1" colspan="1">88.17 ± 4.88</td>
                <td rowspan="1" colspan="1">58.71 ± 2.74</td>
                <td rowspan="1" colspan="1">35.06 ± 1.91</td>
                <td rowspan="1" colspan="1">17.39 ± 0.94</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MINN-DTI</td>
                <td rowspan="1" colspan="1">
                  <bold>0.992 ± 0.007</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>175.89 ± 12.02</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>90.77 ± 5.81</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>46.49 ± 2.63</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>19.10 ± 0.71</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn1">
              <p><italic toggle="yes">Note</italic>: The first row, the percentage before RE is the given threshold of FPR. Best results of the corresponding experiments were represented in bold.</p>
            </fn>
            <fn id="tblfn2">
              <label>a</label>
              <p>Means results obtained from the article (<xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2020</xref>).</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec>
        <label>3.4.2</label>
        <title>Performance on the human dataset</title>
        <p>Here, we compared our method with nine existing methods on the human dataset to further evaluate our model, including k-nearest neighbor (k-NN), random forest (RF), L2-logistic (L2), support vector machine (SVM), graph neural network (GNN) (<xref rid="btac377-B32" ref-type="bibr">Tsubaki <italic toggle="yes">et al.</italic>, 2019</xref>), graph convolution network (GCN), GraphDTA (<xref rid="btac377-B19" ref-type="bibr">Nguyen <italic toggle="yes">et al.</italic>, 2021</xref>), TransformerCPI (<xref rid="btac377-B4" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020</xref>) and DrugVQA (<xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2020</xref>). The results are presented in <xref rid="btac377-T3" ref-type="table">Table 3</xref>, from which we can see that our model is 0.2% better than the state-of-the-art method DrugVQA in AUC. However, in terms of recall and precision, SVM is the best. Actually, it is difficult to objectively and completely evaluate the performance of a model by using only recall and precision, as the performance of SVM is generally inferior to deep learning models according to many existing works. Given the reliability of AUC, we can still claim that our method is the best one.</p>
        <table-wrap position="float" id="btac377-T3">
          <label>Table 3.</label>
          <caption>
            <p>Performance comparison between our model and existing methods on the random splitting human dataset</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Model</th>
                <th align="center" rowspan="1" colspan="1">AUC</th>
                <th align="center" rowspan="1" colspan="1">Recall</th>
                <th align="center" rowspan="1" colspan="1">Precision</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">k-NN<xref rid="tblfn3" ref-type="table-fn"><sup>a</sup></xref></td>
                <td rowspan="1" colspan="1">0.860</td>
                <td rowspan="1" colspan="1">0.927</td>
                <td rowspan="1" colspan="1">0.798</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">RF<xref rid="tblfn3" ref-type="table-fn"><sup>a</sup></xref></td>
                <td rowspan="1" colspan="1">0.940</td>
                <td rowspan="1" colspan="1">0.897</td>
                <td rowspan="1" colspan="1">0.861</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">L2<xref rid="tblfn3" ref-type="table-fn"><sup>a</sup></xref></td>
                <td rowspan="1" colspan="1">0.911</td>
                <td rowspan="1" colspan="1">0.913</td>
                <td rowspan="1" colspan="1">0.861</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">SVM<xref rid="tblfn4" ref-type="table-fn"><sup>b</sup></xref></td>
                <td rowspan="1" colspan="1">0.910</td>
                <td rowspan="1" colspan="1">
                  <bold>0.966</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.969</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GNN<xref rid="tblfn3" ref-type="table-fn"><sup>a</sup></xref></td>
                <td rowspan="1" colspan="1">0.970</td>
                <td rowspan="1" colspan="1">0.918</td>
                <td rowspan="1" colspan="1">0.923</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GCN<xref rid="tblfn4" ref-type="table-fn"><sup>b</sup></xref></td>
                <td rowspan="1" colspan="1">0.956 ± 0.004</td>
                <td rowspan="1" colspan="1">0.862 ± 0.006</td>
                <td rowspan="1" colspan="1">0.928 ± 0.010</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GraphDTA<xref rid="tblfn4" ref-type="table-fn"><sup>b</sup></xref></td>
                <td rowspan="1" colspan="1">0.960 ± 0.005</td>
                <td rowspan="1" colspan="1">0.882 ± 0.040</td>
                <td rowspan="1" colspan="1">0.912 ± 0.040</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TransformerCPI<xref rid="tblfn4" ref-type="table-fn"><sup>b</sup></xref></td>
                <td rowspan="1" colspan="1">0.973 ± 0.002</td>
                <td rowspan="1" colspan="1">0.916 ± 0.006</td>
                <td rowspan="1" colspan="1">0.925 ± 0.006</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">DrugVQA<xref rid="tblfn3" ref-type="table-fn"><sup>a</sup></xref></td>
                <td rowspan="1" colspan="1">0.979 ± 0.003</td>
                <td rowspan="1" colspan="1">0.961 ± 0.002</td>
                <td rowspan="1" colspan="1">0.954 ± 0.03</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MINN-DTI</td>
                <td rowspan="1" colspan="1">
                  <bold>0.981 ± 0.003</bold>
                </td>
                <td rowspan="1" colspan="1">0.945 ± 0.030</td>
                <td rowspan="1" colspan="1">0.902 ± 0.045</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn3">
              <label>a</label>
              <p>Means results obtained from the article (<xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2020</xref>).</p>
            </fn>
            <fn id="tblfn4">
              <label>b</label>
              <p>Means results obtained from the article (<xref rid="btac377-B4" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020</xref>).</p>
            </fn>
            <fn id="tblfn5">
              <p>Best results of the corresponding experiments were represented in bold.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>A newly constructed dataset with a scaffold-based split was also used to train and test the above-mentioned methods. As shown in <xref rid="btac377-T4" ref-type="table">Table 4</xref>, our model consistently exceeds all the competitors in AUC. Compared to the results on the random splitting human dataset, the performance of all models on the scaffold splitting human dataset degrades. However, our model has the slightest drop in AUC, i.e. 1.4%, about half of that of the second-place DrugVQA. These results suggest that more comprehensive extraction of DTI information could help enhance our model's ability to identify novel interactions.</p>
        <table-wrap position="float" id="btac377-T4">
          <label>Table 4.</label>
          <caption>
            <p>Performance comparison between our model and existing methods on the scaffold-based splitting human dataset</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">AUC</th>
                <th rowspan="1" colspan="1">Recall</th>
                <th rowspan="1" colspan="1">Precision</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">k-NN</td>
                <td rowspan="1" colspan="1">0.841</td>
                <td rowspan="1" colspan="1">0.803</td>
                <td rowspan="1" colspan="1">0.892</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">RF</td>
                <td rowspan="1" colspan="1">0.885</td>
                <td rowspan="1" colspan="1">0.890</td>
                <td rowspan="1" colspan="1">0.832</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">L2</td>
                <td rowspan="1" colspan="1">0.881</td>
                <td rowspan="1" colspan="1">0.832</td>
                <td rowspan="1" colspan="1">0.827</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">SVM</td>
                <td rowspan="1" colspan="1">0.892</td>
                <td rowspan="1" colspan="1">0.857</td>
                <td rowspan="1" colspan="1">0.883</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GNN</td>
                <td rowspan="1" colspan="1">0.921 ± 0.002</td>
                <td rowspan="1" colspan="1">0.843 ± 0.004</td>
                <td rowspan="1" colspan="1">0.855 ± 0.003</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GCN</td>
                <td rowspan="1" colspan="1">0.905 ± 0.010</td>
                <td rowspan="1" colspan="1">0.823 ± 0.012</td>
                <td rowspan="1" colspan="1">0.902 ± 0.008</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GraphDTA</td>
                <td rowspan="1" colspan="1">0.926 ± 0.008</td>
                <td rowspan="1" colspan="1">0.855 ± 0.004</td>
                <td rowspan="1" colspan="1">0.898 ± 0.006</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TransformerCPI</td>
                <td rowspan="1" colspan="1">0.948 ± 0.005</td>
                <td rowspan="1" colspan="1">
                  <bold>0.930 ± 0.003</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.933 ± 0.005</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">DrugVQA</td>
                <td rowspan="1" colspan="1">0.952 ± 0.002</td>
                <td rowspan="1" colspan="1">0.925 ± 0.004</td>
                <td rowspan="1" colspan="1">0.928 ± 0.006</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MINN-DTI</td>
                <td rowspan="1" colspan="1">
                  <bold>0.967 ± 0.003</bold>
                </td>
                <td rowspan="1" colspan="1">0.922 ± 0.013</td>
                <td rowspan="1" colspan="1">0.931 ± 0.021</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn6">
              <p>Best results of the corresponding experiments were represented in bold.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec>
        <label>3.4.3</label>
        <title>Performance on the BindingDB dataset</title>
        <p>We also compared our model with GCN, GNN (<xref rid="btac377-B32" ref-type="bibr">Tsubaki <italic toggle="yes">et al.</italic>, 2019</xref>), GraphDTA (<xref rid="btac377-B19" ref-type="bibr">Nguyen <italic toggle="yes">et al.</italic>, 2021</xref>), DrugVQA (<xref rid="btac377-B41" ref-type="bibr">Zheng <italic toggle="yes">et al.</italic>, 2020</xref>) and TransfomerCPI (<xref rid="btac377-B4" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2020</xref>) on the BindingDB dataset. As shown in <xref rid="btac377-T5" ref-type="table">Table 5</xref>, our model achieves the highest AUC and PRC, which is in line with our expectation. Although this advantage is not as obvious as that on the DUD-E dataset, considering that AUC has less room to improve at the level of more than 95% on these datasets, such progress is still considerable.</p>
        <table-wrap position="float" id="btac377-T5">
          <label>Table 5.</label>
          <caption>
            <p>Performance comparison between our model and existing methods on the BindingDB dataset</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Task</th>
                <th rowspan="1" colspan="1">AUC</th>
                <th rowspan="1" colspan="1">PRC</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">GNN</td>
                <td rowspan="1" colspan="1">0.909 ± 0.002</td>
                <td rowspan="1" colspan="1">0.901 ± 0.005</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GCN</td>
                <td rowspan="1" colspan="1">0.912 ± 0.003</td>
                <td rowspan="1" colspan="1">0.907 ± 0.002</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GraphDTA</td>
                <td rowspan="1" colspan="1">0.923 ± 0.003</td>
                <td rowspan="1" colspan="1">0.916 ± 0.004</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">DrugVQA</td>
                <td rowspan="1" colspan="1">0.936 ± 0.005</td>
                <td rowspan="1" colspan="1">0.928 ± 0.007</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TransformerCPI</td>
                <td rowspan="1" colspan="1">0.950 ± 0.002</td>
                <td rowspan="1" colspan="1">0.949 ± 0.005</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">MINN-DTI</td>
                <td rowspan="1" colspan="1">
                  <bold>0.961 ± 0.009</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.956 ± 0.002</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn7">
              <p>Best results of the corresponding experiments were represented in bold.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>We further investigated the generalization capability of our models on the test subsets with/without training targets, the results are shown in <xref rid="btac377-F5" ref-type="fig">Figure 5</xref>. We can see that our model performs better than the other models regardless of whether or not the tested targets are in the training set. Our model achieves an AUC of 0.972 <bold>±</bold> 0.008 and a PRC of 0.957 ± 0.002 when test targets are in the training set. While for new targets, our model still maintains an AUC of 0.957 ± 0.009 and a PRC of 0.951 ± 0.002. However, the other methods all show a larger drop of performance than MINN-DTI on the subset without the training targets, which means that the performance gap between MINN-DTI and these methods is widened. These results suggest that the generalization ability of our model on new targets is higher than that of these existing methods.</p>
        <fig position="float" id="btac377-F5">
          <label>Fig. 5.</label>
          <caption>
            <p>Performance comparisons on seen and unseen protein targets from the BindingDB dataset. Error bars indicate the standard deviations</p>
          </caption>
          <graphic xlink:href="btac377f5" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>3.5 Ablation study</title>
      <p>We conducted ablation studies on the DUD-E and human datasets to check the effectiveness of different configurations of MINN-DTI. All experiments were repeated three times on the divided datasets used above with different seeds to calculate the mean and the standard deviation. Models with different configurations and their performance are presented in <xref rid="btac377-T6" ref-type="table">Table 6</xref>. The first model concatenates the output vectors of DynCNN and CMPNN and uses MLP to predict DTI. The second model replaces Interformer in MINN-DTI with transformer. There are two possible settings: the encoder of transformer receives the information of protein, the decoder of transformer receives the information of drug, or vice versa. For mimicking and competing with Inter-CMPNN, the first setting was used to extract drug information during each round of message passing with the decoder in second model. For the third model, only an Interformer is deployed behind the whole CMPNN module instead of deploying an Interformer during each round of message passing using multiple Interformer as MINN-DTI. As shown in <xref rid="btac377-T6" ref-type="table">Table 6</xref>, MINN-DTI maintains the best performance on both datasets and the model without Interformer module (the first model) performs worst. The performance of the third model is close to the second model, with a difference of only 0.002/0.003 in AUC. However, the results are different on the two datasets. The second model is better than the third model on the human dataset, while the opposite is true on the DUD-E dataset. We speculate that the main reason for the different results may be due to the different data distribution of the two datasets: the test set of DUD-E has unbalanced positive and negative samples, while human dataset is balanced. The second and third models achieve significantly improved performance compared with the first model, which indicates that adopting transformer or Interformer to extract the interaction information between drug and target is helpful for DTI prediction. The second model is still much worse than MINN-DTI, which shows that the Interformer model is more effective than transformer in DTI prediction. As MINN-DTI shows considerable performance advantage over the third model, we believe that the iterative use of Interformer in Inter-CMPNN is beneficial to the extraction of drug and protein interaction information. The ablation results show that MINN-DTI combining Interformer with Inter-CMPNN can indeed improve the prediction performance to a considerable extent.</p>
      <table-wrap position="float" id="btac377-T6">
        <label>Table 6.</label>
        <caption>
          <p>Ablation results on the DUD-E and human datasets</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">DUD-E</th>
              <th rowspan="1" colspan="1">Human</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Without Interformer</td>
              <td rowspan="1" colspan="1">0.953 ± 0.003</td>
              <td rowspan="1" colspan="1">0.967 ± 0.004</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Transformer &amp; CMPNN</td>
              <td rowspan="1" colspan="1">0.975 ± 0.004</td>
              <td rowspan="1" colspan="1">0.971 ± 0.007</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Single Interformer</td>
              <td rowspan="1" colspan="1">0.978 ± 0.006</td>
              <td rowspan="1" colspan="1">0.969 ± 0.005</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MINN-DTI</td>
              <td rowspan="1" colspan="1">
                <bold>0.992 ± 0.007</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.981 ± 0.003</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn8">
            <p>Best results of the corresponding experiments were represented in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.6 Interpretability and visualization</title>
      <p>Two representative DTIs were selected to illustrate the interpretability of our model. By inputting the target distance map and the compound graph representations, the model generates multi-head attentions of the target and the compound, and the weights of each amino acid residue and each atom in the target and drug molecule are calculated to identify their importance in the prediction. Protein distance maps and attention bars representing attention weights of residues and ligand atoms are shown in <xref rid="btac377-F6" ref-type="fig">Figure 6</xref>. Here, we visualize the top-5 weighted residues of the example target as pink skeletons, and highlight the top-10 weighted atoms by red dots. We found that the significantly weighted residues and atoms are highly consistent with the actual interacting residues and atoms. In CXCR4 protein co-crystal complex 3ODU, GLU288, ASP97 and CYS186 form polar interactions with the drug molecule, and HIS113 and TYR116 form hydrophobic interactions with the drug molecule. These residues and atoms that form the interactions have larger attention weights, which can also be seen in the target ALK5 with co-crystal 3HMM. Meanwhile, there are also residues or atoms with low weight, which are considered to have low contribution to the formation of interactions. For example, in the drug attention bar of complex 3ODU, atoms 21–26 of ligand have small weights, and this group is actually exposed to the solvent without participating in the formation of interaction. The above exploration shows that our model can learn and highlight the important target amino acid residues and drug atoms. This helps us to understand DTIs more comprehensively, which is beneficial to the study on the structure-activity relationship and action mechanism of drugs.</p>
      <fig position="float" id="btac377-F6">
        <label>Fig. 6.</label>
        <caption>
          <p>Attention visualization of DTIs. <italic toggle="yes">Left</italic>: Protein distance maps are displayed in the form of heat maps. The corresponding targets’ attention bars are shown. <italic toggle="yes">Middle:</italic> Ligands and predicted important residues are represented as green and pink skeletons, respectively. Predicted important atoms of ligands are highlighted in red. Known hydrogen bonds are marked with yellow dashed lines. Local target structures are painted grey as the background. <italic toggle="yes">Right</italic>: Ligands are represented by 2D Kekule formula. The corresponding predicted important atoms are highlighted by light red dots. Ligands’ attention bars are shown (A color version of this figure appears in the online version of this article.)</p>
        </caption>
        <graphic xlink:href="btac377f6" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>In this article, we proposed a novel model MINN-DTI to boost DTI prediction by comprehensively mining the mutual impacts between the targets and drugs, which are represented by protein distance maps and molecular graphs, respectively. To this end, a MINN was designed by combining a newly designed Interformer with an improved CMPNN (called Inter-CMPNN) to capture the interacting context of drugs and targets. Compared with the existing models, MINN-DTI achieves the best performance on three public datasets, due to the fact that MINN-DTI can more effectively exploit the interacting information between targets and drugs. We believe that the Interformer and Inter-CMPNN-based MINN should be also effective in other related tasks, including target–target, target–peptide and drug–drug interaction prediction.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>F.L., Z.Z. and S.Z. were partially supported by 2021 Tencent AI Lab Rhino-Bird Focused Research Program [JR202104] and National Natural Science Foundation of China (NSFC) [61972100]; J.G. was supported by NSFC [61772367].</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>Reference</title>
    <ref id="btac377-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bagherian</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>Machine learning approaches and databases for prediction of drug–target interaction: a survey paper</article-title>. <source>Brief Bioinform</source>., <volume>22</volume>, <fpage>247</fpage>–<lpage>269</lpage>.<pub-id pub-id-type="pmid">31950972</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ballester</surname><given-names>P.J.</given-names></string-name>, <string-name><surname>Mitchell</surname><given-names>J.B.</given-names></string-name></person-group> (<year>2010</year>) <article-title>A machine learning approach to predicting protein-ligand binding affinity with applications to molecular docking</article-title>. <source>Bioinformatics</source>, <volume>26</volume>, <fpage>1169</fpage>–<lpage>1175</lpage>.<pub-id pub-id-type="pmid">20236947</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bleakley</surname><given-names>K.</given-names></string-name>, <string-name><surname>Yamanishi</surname><given-names>Y.</given-names></string-name></person-group> (<year>2009</year>) <article-title>Supervised prediction of drug–target interactions using bipartite local models</article-title>. <source>Bioinformatics</source>, <volume>25</volume>, <fpage>2397</fpage>–<lpage>2403</lpage>.<pub-id pub-id-type="pmid">19605421</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>TransformerCPI: improving compound-protein interaction prediction by sequence-based deep learning with self-attention mechanism and label reversal experiments</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>4406</fpage>–<lpage>4414</lpage>.<pub-id pub-id-type="pmid">32428219</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>Predicting drug–target interactions with deep-embedding learning of graphs and sequences</article-title>. <source>J. Phys. Chem. A</source>, <volume>125</volume>, <fpage>5633</fpage>–<lpage>5642</lpage>.<pub-id pub-id-type="pmid">34142824</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Durrant</surname><given-names>J.D.</given-names></string-name>, <string-name><surname>McCammon</surname><given-names>J.A.</given-names></string-name></person-group> (<year>2011</year>) <article-title>NNScore 2.0: a neural-network receptor–ligand scoring function</article-title>. <source>J. Chem. Inf. Model</source>., <volume>51</volume>, <fpage>2897</fpage>–<lpage>2903</lpage>.<pub-id pub-id-type="pmid">22017367</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gao</surname><given-names>K.Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Interpretable drug target prediction using deep neural representation</article-title>. In: <italic toggle="yes">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18), Stockholm, Sweden</italic>, pp. <fpage>3371</fpage>–<lpage>3377</lpage>.</mixed-citation>
    </ref>
    <ref id="btac377-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gilmer</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Neural message passing for quantum chemistry</article-title>. <source>Proc. Mach. Learn. Res</source>., <volume>70</volume>, <fpage>1263</fpage>–<lpage>1272</lpage>.</mixed-citation>
    </ref>
    <ref id="btac377-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gilson</surname><given-names>M.K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>BindingDB in 2015: a public database for medicinal chemistry, computational chemistry and systems pharmacology</article-title>. <source>Nucleic Acids Res</source>., <volume>44</volume>, <fpage>D1045</fpage>–<lpage>1053</lpage>.<pub-id pub-id-type="pmid">26481362</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>He</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>Deep residual learning for image recognition</article-title>. In: <italic toggle="yes">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, Nevada</italic>. pp. <fpage>770</fpage>–<lpage>778</lpage>.</mixed-citation>
    </ref>
    <ref id="btac377-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnson</surname><given-names>K.A.</given-names></string-name></person-group> (<year>2008</year>) <article-title>Role of induced fit in enzyme specificity: a molecular forward/reverse switch</article-title>. <source>J. Biol. Chem</source>., <volume>283</volume>, <fpage>26297</fpage>–<lpage>26301</lpage>.<pub-id pub-id-type="pmid">18544537</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Karimi</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>DeepAffinity: interpretable deep learning of compound-protein affinity through unified recurrent and convolutional neural networks</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>3329</fpage>–<lpage>3338</lpage>.<pub-id pub-id-type="pmid">30768156</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kingma</surname><given-names>D.</given-names></string-name>P. and <string-name><surname>Ba</surname><given-names>J.</given-names></string-name></person-group> (<year>2015</year>) <article-title>Adam: a method for stochastic optimization</article-title>. In: <italic toggle="yes">Proceedings of the 3rd International Conference on Learning Representations (ICLR 2015), San Diego, CA</italic>.</mixed-citation>
    </ref>
    <ref id="btac377-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname><given-names>I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>DeepConv-DTI: prediction of drug–target interactions via deep learning with convolution on protein sequences</article-title>. <source>PLoS Comput. Biol</source>., <volume>15</volume>, <fpage>e1007129</fpage>.<pub-id pub-id-type="pmid">31199797</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lenselink</surname><given-names>E.B.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Beyond the hype: deep neural networks outperform established methods using a ChEMBL bioactivity benchmark set</article-title>. <source>J. Cheminform</source>., <volume>9</volume>, <fpage>45</fpage>.<pub-id pub-id-type="pmid">29086168</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) <article-title>Improving compound-protein interaction prediction by building up highly credible negative samples</article-title>. <source>Bioinformatics</source>, <volume>31</volume>, <fpage>i221</fpage>–<lpage>229</lpage>.<pub-id pub-id-type="pmid">26072486</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>HNet-DNN: inferring new drug–disease associations with deep neural network based on heterogeneous network features</article-title>. <source>J. Chem. Inf. Model</source>., <volume>60</volume>, <fpage>2367</fpage>–<lpage>2376</lpage>.<pub-id pub-id-type="pmid">32118415</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mysinger</surname><given-names>M.M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) <article-title>Directory of useful decoys, enhanced (DUD-E): better ligands and decoys for better benchmarking</article-title>. <source>J. Med. Chem</source>., <volume>55</volume>, <fpage>6582</fpage>–<lpage>6594</lpage>.<pub-id pub-id-type="pmid">22716043</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nguyen</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>GraphDTA: predicting drug–target binding affinity with graph neural networks</article-title>. <source>Bioinformatics</source>, <volume>37</volume>, <fpage>1140</fpage>–<lpage>1147</lpage>.<pub-id pub-id-type="pmid">33119053</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ozturk</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>DeepDTA: deep drug–target binding affinity prediction</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>i821</fpage>–<lpage>i829</lpage>.<pub-id pub-id-type="pmid">30423097</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Paszke</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>PyTorch: an imperative style, high-performance deep learning library</article-title>. <source>Adv. Neural Inf. Process. Syst. (Nips 2019)</source>, <volume>32</volume>, <fpage>8026</fpage>–<lpage>8037</lpage>.</mixed-citation>
    </ref>
    <ref id="btac377-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>TOP: a deep mixture representation learning method for boosting molecular toxicity prediction</article-title>. <source>Methods</source>, <volume>179</volume>, <fpage>55</fpage>–<lpage>64</lpage>.<pub-id pub-id-type="pmid">32446957</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ragoza</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Protein–ligand scoring with convolutional neural networks</article-title>. <source>J. Chem. Inf. Model</source>., <volume>57</volume>, <fpage>942</fpage>–<lpage>957</lpage>.<pub-id pub-id-type="pmid">28368587</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rifaioglu</surname><given-names>A.S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>DEEPScreen: high performance drug–target interaction prediction with convolutional neural networks using 2-D structural compound representations</article-title>. <source>Chem Sci</source>., <volume>11</volume>, <fpage>2531</fpage>–<lpage>2557</lpage>.<pub-id pub-id-type="pmid">33209251</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rifaioglu</surname><given-names>A.S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>MDeePred: novel multi-channel protein featurization for deep learning-based binding affinity prediction in drug discovery</article-title>. <source>Bioinformatics</source>, <volume>37</volume>, <fpage>693</fpage>–<lpage>704</lpage>.<pub-id pub-id-type="pmid">33067636</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Skolnick</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>1997</year>) <article-title>MONSSTER: a method for folding globular proteins with a small number of distance restraints</article-title>. <source>J. Mol. Biol</source>., <volume>265</volume>, <fpage>217</fpage>–<lpage>241</lpage>.<pub-id pub-id-type="pmid">9020984</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Song</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Communicative representation learning on attributed molecular graphs</article-title>. In: <italic toggle="yes">Proceedings of the 29th International Joint Conference on Artificial Intelligence (IJCAI 2020), Yokohama, Japan</italic>, pp. <fpage>2831</fpage>–<lpage>2838</lpage>.</mixed-citation>
    </ref>
    <ref id="btac377-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Su</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Comparative assessment of scoring functions: the CASF-2016 update</article-title>. <source>J. Chem. Inf. Model</source>., <volume>59</volume>, <fpage>895</fpage>–<lpage>913</lpage>.<pub-id pub-id-type="pmid">30481020</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>Boosting compound–protein interaction prediction by deep learning</article-title>. <source>Methods</source>, <volume>110</volume>, <fpage>64</fpage>–<lpage>72</lpage>.<pub-id pub-id-type="pmid">27378654</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Torng</surname><given-names>W.</given-names></string-name>, <string-name><surname>Altman</surname><given-names>R.B.</given-names></string-name></person-group> (<year>2019</year>) <article-title>Graph convolutional neural networks for predicting drug–target interactions</article-title>. <source>J. Chem. Inf. Model</source>., <volume>59</volume>, <fpage>4131</fpage>–<lpage>4149</lpage>.<pub-id pub-id-type="pmid">31580672</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Trott</surname><given-names>O.</given-names></string-name>, <string-name><surname>Olson</surname><given-names>A.J.</given-names></string-name></person-group> (<year>2010</year>) <article-title>AutoDock Vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading</article-title>. <source>J. Comput. Chem</source>, <volume>31</volume>, <fpage>455</fpage>–<lpage>461</lpage>.<pub-id pub-id-type="pmid">19499576</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tsubaki</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Compound–protein interaction prediction with end-to-end learning of neural networks for graphs and sequences</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>309</fpage>–<lpage>318</lpage>.<pub-id pub-id-type="pmid">29982330</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Attention is all you need</article-title>. In: <italic toggle="yes">Proceedings of 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA</italic>.</mixed-citation>
    </ref>
    <ref id="btac377-B34">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wallach</surname><given-names>I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) AtomNet: a deep convolutional neural network for bioactivity prediction in structure-based drug discovery. <italic toggle="yes">arXiv preprint arXiv:1510.02855</italic>.</mixed-citation>
    </ref>
    <ref id="btac377-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>X.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>CSConv2d: a 2-D structural convolution neural network with a channel and spatial attention mechanism for protein–ligand binding affinity prediction</article-title>. <source>Biomolecules</source>, <volume>11</volume>, <fpage>643</fpage>.<pub-id pub-id-type="pmid">33925310</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>Z.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>MoleculeNet: a benchmark for molecular machine learning</article-title>. <source>Chem. Sci</source>., <volume>9</volume>, <fpage>513</fpage>–<lpage>530</lpage>.<pub-id pub-id-type="pmid">29629118</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xiong</surname><given-names>Z.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism</article-title>. <source>J. Med. Chem</source>., <volume>63</volume>, <fpage>8749</fpage>–<lpage>8760</lpage>.<pub-id pub-id-type="pmid">31408336</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Analyzing learned molecular representations for property prediction</article-title>. <source>J. Chem. Inf. Model</source>., <volume>59</volume>, <fpage>3370</fpage>–<lpage>3388</lpage>.<pub-id pub-id-type="pmid">31361484</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>Z.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>FraGAT: a fragment-oriented multi-scale graph attention model for molecular property prediction</article-title>. <source>Bioinformatics</source>, <volume>37</volume>, <fpage>2981</fpage>–<lpage>2987</lpage>.<pub-id pub-id-type="pmid">33769437</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zheng</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Identifying structure-property relationships through SMILES syntax analysis with self-attention mechanism</article-title>. <source>J. Chem. Inf. Model</source>., <volume>59</volume>, <fpage>914</fpage>–<lpage>923</lpage>.<pub-id pub-id-type="pmid">30669836</pub-id></mixed-citation>
    </ref>
    <ref id="btac377-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zheng</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Predicting drug–protein interaction using quasi-visual question answering system</article-title>. <source>Nat. Mach. Intell</source>., <volume>2</volume>, <fpage>134</fpage>–<lpage>140</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
