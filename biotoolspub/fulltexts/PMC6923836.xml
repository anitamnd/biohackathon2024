<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6923836</article-id>
    <article-id pub-id-type="publisher-id">3245</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-3245-z</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SigUNet: signal peptide recognition based on semantic segmentation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Wu</surname>
          <given-names>Jhe-Ming</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Yu-Chen</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Chang</surname>
          <given-names>Darby Tien-Hao</given-names>
        </name>
        <address>
          <email>darby@mail.ncku.edu.tw</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0532 3255</institution-id><institution-id institution-id-type="GRID">grid.64523.36</institution-id><institution>Department of Electrical Engineering, </institution><institution>National Cheng Kung University, </institution></institution-wrap>Tainan, Taiwan </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>20</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <issue>Suppl 24</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. Supplement editors were not involved in the review process of any of the manuscripts that they co-authored. The Supplement Editors declare that they have no other competing interests.</issue-sponsor>
    <elocation-id>677</elocation-id>
    <permissions>
      <copyright-statement>© The Author(s). 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Signal peptides play an important role in protein sorting, which is the mechanism whereby proteins are transported to their destination. Recognition of signal peptides is an important first step in determining the active locations and functions of proteins. Many computational methods have been proposed to facilitate signal peptide recognition. In recent years, the development of deep learning methods has seen significant advances in many research fields. However, most existing models for signal peptide recognition use one-hidden-layer neural networks or hidden Markov models, which are relatively simple in comparison with the deep neural networks that are used in other fields.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">This study proposes a convolutional neural network without fully connected layers, which is an important network improvement in computer vision. The proposed network is more complex in comparison with current signal peptide predictors. The experimental results show that the proposed network outperforms current signal peptide predictors on eukaryotic data. This study also demonstrates how model reduction and data augmentation helps the proposed network to predict bacterial data.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">The study makes three contributions to this subject: (a) an accurate signal peptide recognizer is developed, (b) the potential to leverage advanced networks from other fields is demonstrated and (c) important modifications are proposed while adopting complex networks on signal peptide recognition.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Signal peptide</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Semantic segmentation</kwd>
    </kwd-group>
    <conference xlink:href="http://icibm2019.org/">
      <conf-name>The International Conference on Intelligent Biology and Medicine (ICIBM) 2019</conf-name>
      <conf-acronym>ICIBM 2019</conf-acronym>
      <conf-loc>Columbia, OH, USA</conf-loc>
      <conf-date>09-11 June 2019</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par4">Protein sorting is the mechanism whereby proteins are transported to their destination inside and/or outside cells. Signal peptides play an important role in this process [<xref ref-type="bibr" rid="CR1">1</xref>]. Proteins with signal peptides enter the secretory pathway and are then be transported to appropriate organelles, where the proteins fulfill their functions. Signal peptides operate as a permission gateway for the transport of proteins into the endoplasmic reticulum. Blobel and Sabatini [<xref ref-type="bibr" rid="CR2">2</xref>] observed an interaction between ribosome and endoplasmic reticulum in 1971. In 1972, Milstein et al. [<xref ref-type="bibr" rid="CR3">3</xref>] proposed that an extra sequence fragment might exist at the N-terminus of a polypeptide, which serves as a signal transmitter for the translocation of proteins. In 1975, Blobel and Dobberstein [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>] proposed a signal hypothesis that believed the signal sequence is located at the N-terminus of a polypeptide and is downgraded after protein translocation.</p>
    <p id="Par5">The term “signal peptide” was first coined in a study by von Heijne [<xref ref-type="bibr" rid="CR1">1</xref>], which defined some basic properties of signal peptides. The study found that signal peptides are short amino acid sequences that are located at the N-terminus of proteins. The length of a signal peptide ranges from 11 to 27 residues. From the N-terminus, a signal peptide is composed of three sections. The first section is a positively charged n-region with about 1~5 residues. The second section is a hydrophobic h-region with about 7~15 residues. The final section is a polar uncharged c-region with about 3~7 residues. The end of signal peptides is called cleavage site.</p>
    <p id="Par6">The recognition of signal peptides is an important first step in determining the active locations and functions of proteins [<xref ref-type="bibr" rid="CR6">6</xref>]. An effective method of determining signal peptide sequences is to read the sequences of a newborn protein and the corresponding mature protein via in vitro experiments. However, these in vitro experiments are considerably costly. Therefore, many computational methods have been proposed to facilitate signal peptide recognition. The first computational method for signal peptide recognition was proposed in 1983. Von Heijen proposed a statistical method based on 78 eukaryotic proteins [<xref ref-type="bibr" rid="CR7">7</xref>]. A (− 3, − 1)-rule was proposed, which refers to a specific pattern at the first and the third positions before the cleavage site. In 1986, the same research group proposed an algorithm that uses a weight matrix to recognize signal peptides [<xref ref-type="bibr" rid="CR8">8</xref>]. In 1998, Nielsen and Krogh used a hidden Markov model (HMM) to fit the three section-property and (− 3, − 1)-rule of signal peptides [<xref ref-type="bibr" rid="CR9">9</xref>]. In 1997, Nielsen et al. proposed a method that uses a neural network (NN) and achieved much better performance than other contemporary methods [<xref ref-type="bibr" rid="CR10">10</xref>]. In 2004, Bendtsen et al. proposed the SignalP 3.0 algorithm, which combines HMM and NN [<xref ref-type="bibr" rid="CR11">11</xref>]. In 2011, the same research group proposed the SignalP 4.0 algorithm, which combines two neural networks [<xref ref-type="bibr" rid="CR12">12</xref>]. The SignalP 4.0 algorithm has become a paradigm in the field of signal peptide recognition. The study also showed that many methods produce high false-positive rates for misclassified proteins that treat transmembrane helices as signal peptides.</p>
    <p id="Par7">In recent years, the development of deep learning methods has seen significant advances in many research fields. Specifically, convolutional neural networks (CNN) [<xref ref-type="bibr" rid="CR13">13</xref>] have been used to achieve excellent performance in image classification [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>]. Recurrent neural networks (RNN) [<xref ref-type="bibr" rid="CR16">16</xref>] have been used for time series data [<xref ref-type="bibr" rid="CR17">17</xref>]. In addition, the networks have been used with great success in the field of molecular biology [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>]. In 2017, Savojardo et al. proposed the DeepSig algorithm [<xref ref-type="bibr" rid="CR6">6</xref>], which is the first CNN-based method that predicts whether an amino acid sequence contains signal peptides.</p>
    <p id="Par8">This study proposes a CNN architecture without fully connected layers for signal peptide recognition. Neural networks without fully connected layers have been widely used in semantic segmentation of images with great success. For example, the fully convolutional network (FCN) [<xref ref-type="bibr" rid="CR20">20</xref>], U-Net [<xref ref-type="bibr" rid="CR21">21</xref>] and DeepLab [<xref ref-type="bibr" rid="CR22">22</xref>] are three CNN architectures that are designed for semantic segmentation of images. This study modifies U-Net to process protein sequences. The modified network, named SigUNet in the context, is different to U-Net in that it (a) processes one-dimensional data, (b) adjusts the down-sampling strategy to prevent the loss of information, (c) reduces model complexity for small datasets and (d) is a trainable network architecture. The experimental results in this study show that SigUNet outperforms current signal peptide predictors on eukaryotic data. This study also demonstrates how model reduction and data augmentation helps the proposed network to predict bacterial data.</p>
  </sec>
  <sec id="Sec2">
    <title>Results</title>
    <sec id="Sec3">
      <title>Experimental design</title>
      <p id="Par9">Similar to previous studies [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR12">12</xref>], Matthews Correlation Coefficient (MCC) and the false-positive rate for transmembrane proteins (<italic>FPR</italic><sub><italic>TM</italic></sub>) are two main evaluation indices adopted in this study. MCC measures the correlation between the observed and predicted classes. <italic>FPR</italic><sub><italic>TM</italic></sub> measures the probability that a transmembrane protein is misclassified as a signal peptide. Signal peptides and N-terminal transmembrane helices are highly similar, except that transmembrane helices usually have longer hydrophobic regions and have no cleavage sites. <italic>FPR</italic><sub><italic>TM</italic></sub> is used to measure the ability to discriminate signal peptides from transmembrane proteins. This study also uses precision, recall and F1 measure as supplemental indices. Precision measures the fraction of real signal peptides in samples that are predicted to be signal peptides. Recall measures the fraction of signal peptides that are correctly predicted to be signal peptides. F1 measure is the harmonic mean of precision and recall. The three indices are widely used in binary classification. The details of these evaluation indices are described in the Materials and Methods section.</p>
      <p id="Par10">Table <xref rid="Tab1" ref-type="table">1</xref> shows the datasets that are used to evaluate signal peptide recognition. The details of how the datasets are constructed are in the Materials and Methods section. The SignalP dataset was constructed in 2011 by Petersen et al. [<xref ref-type="bibr" rid="CR12">12</xref>] and the SPDS17 dataset was constructed in 2017 by Savojardo et al. [<xref ref-type="bibr" rid="CR6">6</xref>]. Petersen et al. defined a subset of the SignalP dataset as a comparison dataset. Savojardo et al. constructed the SPDS17 dataset as another comparison dataset to accommodate newly discovered proteins. Both datasets are separated into Eukaryotes, Gram-positive bacteria and Gram-negative bacteria subsets because Hejine showed that signal peptides in different groups of organisms have different lengths and amino acid compositions [<xref ref-type="bibr" rid="CR1">1</xref>]. Pertersen el al. and Savojardo et al. adopted a nested cross validation procedure to evaluate their methods. The procedure uses an inner cross validation to prevent peeking at the comparison dataset while the hyper-parameters are tuned. This study uses the same evaluation procedure. The details of the dataset construction and the nested cross validation are described in the Materials and Methods section.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Statistics of the datasets that are used in this study</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Organism</th><th colspan="2">Signal Peptides</th><th colspan="2">Transmembrane</th><th colspan="2">Cytosolic or Nuclear</th><th rowspan="2">Total</th></tr><tr><th>Train</th><th>Comp</th><th>Train</th><th>Comp</th><th>Train</th><th>Comp</th></tr></thead><tbody><tr><td colspan="8">SignalP</td></tr><tr><td> Eukaryotes</td><td>1640</td><td>606</td><td>987</td><td>939</td><td>5133</td><td>1000</td><td>7760</td></tr><tr><td> Gram-positive</td><td>208</td><td>48</td><td>117</td><td>117</td><td>360</td><td>213</td><td>685</td></tr><tr><td> Gram-negative</td><td>423</td><td>104</td><td>523</td><td>523</td><td>912</td><td>260</td><td>1858</td></tr><tr><td colspan="8">SPDS17</td></tr><tr><td> Eukaryotes</td><td>–</td><td>46</td><td>–</td><td>323</td><td>–</td><td>689</td><td>1058</td></tr><tr><td> Gram-positive</td><td>–</td><td>9</td><td>–</td><td>189</td><td>–</td><td>240</td><td>438</td></tr><tr><td> Gram-negative</td><td>–</td><td>23</td><td>–</td><td>89</td><td>–</td><td>99</td><td>211</td></tr></tbody></table><table-wrap-foot><p>The SignalP dataset is from the UniProtKB/Swiss-Prot in accordance with the identity list in Pertersen et al.’s study [<xref ref-type="bibr" rid="CR12">12</xref>]; The SPDS17 dataset is from the UniProtKB/Swiss-Prot in accordance with the identity list in Savojardo et al.’s study [<xref ref-type="bibr" rid="CR6">6</xref>].</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec4">
      <title>The performance on the eukaryotes datasets</title>
      <p id="Par11">Table <xref rid="Tab2" ref-type="table">2</xref> compares the results of ten alternative methods and SigUNet on the Eukaryotes dataset. Of the 11 methods, DeepSig and SigUNet use deep neural networks. The other nine methods use one-hidden-layer NN or HMM models and SignalP 4.0 is the most accurate of them. SigUNet outperforms the other models in terms of both MCC and <italic>FPR</italic><sub><italic>TM</italic></sub>. For the SignalP dataset, DeepSig achieves a comparable MCC and a better <italic>FPR</italic><sub><italic>TM</italic></sub> than SignalP 4.0. SigUNet gives a similar <italic>FPR</italic><sub><italic>TM</italic></sub> and a 3.0% better MCC than DeepSig. The 4.3% gap in recall between SigUNet and DeepSig shows that SigUNet captures more signal peptides. For the SPDS17 dataset, DeepSig outperforms SignalP 4.0 in terms of both MCC and <italic>FPR</italic><sub><italic>TM</italic></sub>. SigUNet gives a 3.5% better MCC than DeepSig. Unlike the SignalP dataset, this improvement is due to a low <italic>FPR</italic><sub><italic>TM</italic></sub> and not a high recall. Namely, SigUNet discriminates more transmembrane proteins from signal peptides on the SPDS17 dataset. These results show that SigUNet performs well on eukaryotic signal peptides, regardless of the dataset that is used.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>The performance on the Eukaryotes datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>MCC (%)</th><th><italic>FPR</italic><sub><italic>TM</italic></sub> (%)</th><th>Precision (%)</th><th>Recall (%)</th><th>F1 measure (%)</th></tr></thead><tbody><tr><td colspan="6">The SignalP dataset</td></tr><tr><td> Phobius</td><td>81.1</td><td>15.3</td><td>77.6</td><td>95.2</td><td>85.5</td></tr><tr><td> PrediSi</td><td>56.1</td><td>52.6</td><td>52.0</td><td>91.3</td><td>66.3</td></tr><tr><td> SignalP3.0-HMM</td><td>75.9</td><td>23.5</td><td>69.5</td><td>97.4</td><td>81.1</td></tr><tr><td> SignalP3.0-NN</td><td>56.2</td><td>64.1</td><td>48.4</td><td><bold>98.8</bold></td><td>65.0</td></tr><tr><td> PolyPhobius</td><td>80.6</td><td>12.5</td><td>79.5</td><td>91.9</td><td>85.2</td></tr><tr><td> Philius</td><td>80.4</td><td>13.4</td><td>77.8</td><td>93.7</td><td>85.0</td></tr><tr><td> SPOCTOPUS</td><td>80.1</td><td>14.0</td><td>79.0</td><td>91.7</td><td>84.9</td></tr><tr><td> SignalP 4.0</td><td>87.4</td><td>6.1</td><td>–</td><td>–</td><td>–</td></tr><tr><td> TOPCONS2</td><td>84.6</td><td>9.6</td><td>83.6</td><td>93.6</td><td>88.3</td></tr><tr><td> DeepSig</td><td>87.2</td><td>4.2</td><td>92.5</td><td>87.8</td><td>90.1</td></tr><tr><td> SigUNet</td><td><bold>90.2</bold></td><td><bold>4.0</bold></td><td><bold>93.0</bold></td><td>92.1</td><td><bold>92.5</bold></td></tr><tr><td colspan="6">The SPDS17 dataset</td></tr><tr><td> Phobius</td><td>65.8</td><td>9.6</td><td>47.8</td><td><bold>95.7</bold></td><td>63.8</td></tr><tr><td> PrediSi</td><td>38.5</td><td>43.3</td><td>20.7</td><td>89.1</td><td>33.6</td></tr><tr><td> SignalP3.0-HMM</td><td>51.6</td><td>22.3</td><td>31.2</td><td><bold>95.7</bold></td><td>47.1</td></tr><tr><td> SignalP3.0-NN</td><td>36.0</td><td>59.1</td><td>17.5</td><td><bold>95.7</bold></td><td>29.5</td></tr><tr><td> PolyPhobius</td><td>72.0</td><td>8.0</td><td>56.4</td><td><bold>95.7</bold></td><td>71.0</td></tr><tr><td> Philius</td><td>62.3</td><td>6.5</td><td>44.3</td><td>93.5</td><td>60.1</td></tr><tr><td> SPOCTOPUS</td><td>54.0</td><td>16.4</td><td>37.9</td><td>84.8</td><td>52.3</td></tr><tr><td> SignalP 4.0</td><td>81.9</td><td>4.0</td><td>75.0</td><td>91.3</td><td>82.3</td></tr><tr><td> TOPCONS2</td><td>73.9</td><td>5.6</td><td>60.6</td><td>93.5</td><td>73.5</td></tr><tr><td> DeepSig</td><td>86.1</td><td>2.5</td><td>82.4</td><td>91.3</td><td>86.6</td></tr><tr><td> SigUNet</td><td><bold>89.6</bold></td><td><bold>1.2</bold></td><td><bold>91.1</bold></td><td>89.1</td><td><bold>90.1</bold></td></tr></tbody></table><table-wrap-foot><p>The performances of Phoibus, PrediSi and SignalP 3.0 are obtained from their online services (Phobius: <ext-link ext-link-type="uri" xlink:href="http://phobius.sbc.su.se/;">http://phobius.sbc.su.se/;</ext-link> PrediSi: <ext-link ext-link-type="uri" xlink:href="http://www.predisi.de/predisi/;">http://www.predisi.de/predisi/;</ext-link> SignalP 3.0: <ext-link ext-link-type="uri" xlink:href="http://www.cbs.dtu.dk/services/SignalP-3.0/">http://www.cbs.dtu.dk/services/SignalP-3.0/</ext-link>) [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR24">24</xref>]. The performances of PolyPhobius, Philius, SPOCTOPUS and TOPCONS2 are obtained from the TOPCONS2 software (<ext-link ext-link-type="uri" xlink:href="https://github.com/ElofssonLab/TOPCONS2">https://github.com/ElofssonLab/TOPCONS2</ext-link>) [<xref ref-type="bibr" rid="CR25">25</xref>–<xref ref-type="bibr" rid="CR28">28</xref>]. The performance of SignalP 4.0 on the SignalP dataset is obtained from the original paper [<xref ref-type="bibr" rid="CR12">12</xref>] and the performance on the SPDS17 dataset is obtained from its online service (<ext-link ext-link-type="uri" xlink:href="http://www.cbs.dtu.dk/services/SignalP-4.0/">http://www.cbs.dtu.dk/services/SignalP-4.0/</ext-link>). The performance of DeepSig on the SignalP dataset is obtained by reproducing the algorithm and the performance on the SPDS17 dataset is obtained using the source code (<ext-link ext-link-type="uri" xlink:href="https://github.com/BolognaBiocomp/deepsig">https://github.com/BolognaBiocomp/deepsig</ext-link>). For each dataset, the best performance is highlighted in bold.</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec5">
      <title>The performance on the bacteria datasets</title>
      <p id="Par12">Table <xref rid="Tab3" ref-type="table">3</xref> shows the results on the Gram-positive datasets. The performance of SignalP 4.0, DeepSig and SigUNet shows no consistent order on the SignalP and SPDS17 datasets. DeepSig gives the worst MCC on the SignalP dataset but the best MCC on the SPDS17 dataset. The results on the Gram-negative datasets show a similar phenomenon (Table <xref rid="Tab4" ref-type="table">4</xref>). SignalP 4.0 gives the best MCC on the SignalP dataset but the worst MCC on the SPDS17 dataset. As a result, Tables <xref rid="Tab3" ref-type="table">3</xref> and <xref rid="Tab4" ref-type="table">4</xref> show that SigUNet does not achieve a dominant performance as it shows in Table <xref rid="Tab2" ref-type="table">2</xref>. In comparison with the Eukaryotes datasets, the bacteria datasets are smaller. The SignalP Gram-positive dataset possesses 685 samples, which is merely 8.8% in comparison with the 7760 samples of the SignalP Eukaryotes dataset. It is speculated that the small size of the bacterial datasets affects the performance of SigUNet. The next section discusses the size issue in more detail.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>The performance on the Gram-positive datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>MCC (%)</th><th><italic>FPR</italic><sub><italic>TM</italic></sub> (%)</th><th>Precision (%)</th><th>Recall (%)</th><th>F1 measure (%)</th></tr></thead><tbody><tr><td colspan="6">The SignalP dataset</td></tr><tr><td> Phobius</td><td>67.7</td><td>20.5</td><td>60.0</td><td>87.5</td><td>71.2</td></tr><tr><td> PrediSi</td><td>40.9</td><td>54.7</td><td>35.0</td><td>75.0</td><td>47.7</td></tr><tr><td> SignalP3.0-HMM</td><td>55.8</td><td>43.6</td><td>44.3</td><td>89.6</td><td>59.3</td></tr><tr><td> SignalP3.0-NN</td><td>47.2</td><td>56.4</td><td>34.9</td><td><bold>91.7</bold></td><td>50.6</td></tr><tr><td> PolyPhobius</td><td>71.1</td><td>16.2</td><td>66.1</td><td>85.4</td><td>74.5</td></tr><tr><td> Philius</td><td>69.6</td><td>15.4</td><td>64.1</td><td>85.4</td><td>73.2</td></tr><tr><td> SPOCTOPUS</td><td>73.9</td><td>15.4</td><td>67.2</td><td>89.6</td><td>76.8</td></tr><tr><td> SignalP 4.0</td><td><bold>85.1</bold></td><td><bold>2.6</bold></td><td>–</td><td>–</td><td>–</td></tr><tr><td> TOPCONS2</td><td>81.6</td><td>6.8</td><td>80.8</td><td>87.5</td><td><bold>84.0</bold></td></tr><tr><td> DeepSig</td><td>73.9</td><td>6.8</td><td>81.4</td><td>72.9</td><td>76.9</td></tr><tr><td> SigUNet</td><td>76.1</td><td>5.1</td><td><bold>85.4</bold></td><td>72.9</td><td>78.7</td></tr><tr><td colspan="6">The SPDS17 dataset</td></tr><tr><td> Phobius</td><td>35.0</td><td>13.6</td><td>17.9</td><td><bold>77.8</bold></td><td>29.2</td></tr><tr><td> PrediSi</td><td>14.3</td><td>64.0</td><td>5.0</td><td><bold>77.8</bold></td><td>9.5</td></tr><tr><td> SignalP3.0-HMM</td><td>27.3</td><td>27.0</td><td>11.9</td><td><bold>77.8</bold></td><td>20.6</td></tr><tr><td> SignalP3.0-NN</td><td>16.1</td><td>45.5</td><td>5.7</td><td><bold>77.8</bold></td><td>10.7</td></tr><tr><td> PolyPhobius</td><td>34.5</td><td>13.2</td><td>17.5</td><td><bold>77.8</bold></td><td>28.6</td></tr><tr><td> Philius</td><td>30.3</td><td>79.0</td><td>16.2</td><td>66.7</td><td>26.1</td></tr><tr><td> SPOCTOPUS</td><td>30.3</td><td>13.8</td><td>16.2</td><td>66.7</td><td>26.1</td></tr><tr><td> SignalP 4.0</td><td>50.3</td><td><bold>0.0</bold></td><td>40.0</td><td>66.7</td><td>50.0</td></tr><tr><td> TOPCONS2</td><td>38.1</td><td>4.2</td><td>24.0</td><td>66.7</td><td>35.3</td></tr><tr><td> DeepSig</td><td><bold>54.5</bold></td><td>0.1</td><td><bold>46.2</bold></td><td>66.7</td><td><bold>54.4</bold></td></tr><tr><td> SigUNet</td><td>40.9</td><td>2.1</td><td>40.0</td><td>44.4</td><td>42.1</td></tr></tbody></table><table-wrap-foot><p>The best performance is highlighted in bold</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab4"><label>Table 4</label><caption><p>The performance on the Gram-negative datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>MCC (%)</th><th><italic>FPR</italic><sub><italic>TM</italic></sub> (%)</th><th>Precision (%)</th><th>Recall (%)</th><th>F1 measure (%)</th></tr></thead><tbody><tr><td colspan="6">The SignalP dataset</td></tr><tr><td> Phobius</td><td>59.9</td><td>22.6</td><td>43.9</td><td>94.2</td><td>59.9</td></tr><tr><td> PrediSi</td><td>30.6</td><td>69.0</td><td>19.7</td><td>86.5</td><td>32.1</td></tr><tr><td> SignalP3.0-HMM</td><td>47.7</td><td>39.2</td><td>31.6</td><td>93.3</td><td>47.2</td></tr><tr><td> SignalP3.0-NN</td><td>36.7</td><td>61.0</td><td>22.1</td><td><bold>95.2</bold></td><td>35.9</td></tr><tr><td> PolyPhobius</td><td>60.7</td><td>21.4</td><td>45.0</td><td>94.2</td><td>60.9</td></tr><tr><td> Philius</td><td>65.9</td><td>14.9</td><td>51.3</td><td>94.2</td><td>66.4</td></tr><tr><td> SPOCTOPUS</td><td>64.7</td><td>17.0</td><td>50.8</td><td>92.3</td><td>65.5</td></tr><tr><td> SignalP 4.0</td><td><bold>84.8</bold></td><td><bold>1.5</bold></td><td>–</td><td>–</td><td>–</td></tr><tr><td> TOPCONS2</td><td>70.8</td><td>13.2</td><td>57.2</td><td><bold>95.2</bold></td><td>71.5</td></tr><tr><td> DeepSig</td><td>81.2</td><td>1.7</td><td><bold>88.9</bold></td><td>76.9</td><td><bold>82.5</bold></td></tr><tr><td> SigUNet</td><td>80.6</td><td><bold>1.5</bold></td><td>88.8</td><td>76.0</td><td>81.9</td></tr><tr><td colspan="6">The SPDS17 dataset</td></tr><tr><td> Phobius</td><td>69.5</td><td>18.0</td><td>56.4</td><td>95.7</td><td>71.0</td></tr><tr><td> PrediSi</td><td>35.4</td><td>66.3</td><td>25.0</td><td>87.0</td><td>38.8</td></tr><tr><td> SignalP3.0-HMM</td><td>65.4</td><td>21.3</td><td>51.2</td><td>95.7</td><td>66.7</td></tr><tr><td> SignalP3.0-NN</td><td>49.1</td><td>44.9</td><td>33.8</td><td>95.7</td><td>50.0</td></tr><tr><td> PolyPhobius</td><td>75.9</td><td>13.5</td><td>62.2</td><td>100.0</td><td>76.7</td></tr><tr><td> Philius</td><td>88.7</td><td>2.2</td><td>84.6</td><td>95.7</td><td>89.8</td></tr><tr><td> SPOCTOPUS</td><td>62.5</td><td>20.2</td><td>50.0</td><td>91.3</td><td>64.6</td></tr><tr><td> SignalP 4.0</td><td>92.5</td><td><bold>0.0</bold></td><td><bold>100.0</bold></td><td>87.0</td><td>93.0</td></tr><tr><td> TOPCONS2</td><td>85.9</td><td>5.6</td><td>76.7</td><td><bold>100.0</bold></td><td>86.8</td></tr><tr><td> DeepSig</td><td>95.0</td><td><bold>0.0</bold></td><td><bold>100.0</bold></td><td>91.3</td><td>95.5</td></tr><tr><td> SigUNet</td><td><bold>97.6</bold></td><td>1.1</td><td>95.8</td><td><bold>100.0</bold></td><td><bold>97.9</bold></td></tr></tbody></table><table-wrap-foot><p>The best performance is highlighted in bold</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec6">
      <title>Model reduction and data augmentation</title>
      <p id="Par13">The SignalP 4.0 model has only one hidden layer and less than 20,000 trainable weights. The DeepSig model uses convolutional layers and has 20,000~100,000 trainable weights. SigUNet has 100,000~300,000 trainable weights which is three to five folds more than that of DeepSig. This study conducts two experiments to explore whether (a) model reduction and (b) data augmentation improves the performance of SigUNet on the bacteria datasets. For the first experiment, a reduced version of SigUNet, named SigUNet-light, is implemented. The number of trainable weights of SigUNet-light is reduced to 60,000~200,000. The model details are described in the Materials and Methods section. The reduced version gives a 0.8~2.3% increase in the MCC over SigUNet on the bacteria datasets, but the same effect is not observed on the SPDS17 Gram-negative dataset (Table <xref rid="Tab5" ref-type="table">5</xref>). The reduced version gives a worse performance than SigUNet on the Eukaryotes datasets. This reveals that the Eukaryotes data is sufficient to train SigUNet and no model reduction is required.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>The performance of model reduction</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Method</th><th colspan="2">Eukaryotes</th><th colspan="2">Gram-positive</th><th colspan="2">Gram-negative</th></tr><tr><th>MCC (%)</th><th><italic>FPR</italic><sub><italic>TM</italic></sub> (%)</th><th>MCC (%)</th><th><italic>FPR</italic><sub><italic>TM</italic></sub> (%)</th><th>MCC (%)</th><th><italic>FPR</italic><sub><italic>TM</italic></sub> (%)</th></tr></thead><tbody><tr><td colspan="7">The SignalP dataset</td></tr><tr><td> SigUNet</td><td>90.2</td><td>4.0</td><td>76.1</td><td>5.1</td><td>80.6</td><td>1.5</td></tr><tr><td> SigUNet-light</td><td>89.4</td><td>4.3</td><td><bold>77.7</bold></td><td>5.1</td><td><bold>82.9</bold></td><td>1.9</td></tr><tr><td colspan="7">The SPDS17 dataset</td></tr><tr><td> SigUNet</td><td>89.6</td><td>1.2</td><td>40.9</td><td>2.1</td><td>97.6</td><td>1.1</td></tr><tr><td> SigUNet-light</td><td>84.8</td><td>3.7</td><td><bold>51.7</bold></td><td><bold>1.6</bold></td><td>92.8</td><td>1.1</td></tr></tbody></table><table-wrap-foot><p>Performances that are improved after model reduction are highlighted in bold.</p></table-wrap-foot></table-wrap></p>
      <p id="Par14">For the second experiment, training data from different organisms is merged to construct larger training sets (Table <xref rid="Tab6" ref-type="table">6</xref> and Table <xref rid="Tab7" ref-type="table">7</xref>). For the Eukaryotes datasets in both tables, the best MCC is achieved by training SigUNet using only the Eukaryotes data. This echoes that the Eukaryotes data is sufficient to train SigUNet. Adding bacteria data to the training set introduces noises, which mitigate the benefit of data augmentation.
<table-wrap id="Tab6"><label>Table 6</label><caption><p>The performance of data augmentation on the SignalP dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Comp</th><th colspan="2">Eukaryotes</th><th colspan="2">Gram-positive</th><th colspan="2">Gram-negative</th></tr><tr><th>Train</th><th>MCC (%)</th><th><italic>FPR</italic><sub><italic>TM</italic></sub> (%)</th><th>MCC (%)</th><th><italic>FPR</italic><sub><italic>TM</italic></sub> (%)</th><th>MCC (%)</th><th><italic>FPR</italic><sub><italic>TM</italic></sub> (%)</th></tr></thead><tbody><tr><td colspan="7">SigUNet</td></tr><tr><td> As comp<sup>a</sup></td><td><bold>90.2</bold></td><td>4.0</td><td>76.1</td><td>5.1</td><td>80.6</td><td>1.5</td></tr><tr><td> All organisms<sup>b</sup></td><td>89.9</td><td><bold>3.2</bold></td><td>80.9</td><td>3.1</td><td>82.1</td><td>3.6</td></tr><tr><td> Bacteria<sup>c</sup></td><td>–</td><td>–</td><td>79.3</td><td><bold>1.9</bold></td><td>83.5</td><td><bold>0.3</bold></td></tr><tr><td colspan="7">SigUNet-light</td></tr><tr><td> As comp</td><td>89.4</td><td>4.3</td><td>77.7</td><td>5.1</td><td>82.9</td><td>1.9</td></tr><tr><td> All organisms</td><td>88.9</td><td>3.9</td><td><bold>82.5</bold></td><td>3.1</td><td>81.4</td><td>3.5</td></tr><tr><td> Bacteria</td><td>–</td><td>–</td><td>80.2</td><td><bold>1.9</bold></td><td><bold>83.9</bold></td><td>2.7</td></tr></tbody></table><table-wrap-foot><p><sup>a</sup>The model is trained using the same organism as the comparison dataset. <sup>b</sup>The model is trained using all organisms. <sup>c</sup>The model is trained using all of the bacteria data. The best performance is highlighted in bold</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab7"><label>Table 7</label><caption><p>The performance of data augmentation on the SPDS17 dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Comp</th><th colspan="2">Eukaryotes</th><th colspan="2">Gram-positive</th><th colspan="2">Gram-negative</th></tr><tr><th>Train</th><th>MCC (%)</th><th><italic>FPR</italic><sub><italic>TM</italic></sub> (%)</th><th>MCC (%)</th><th><italic>FPR</italic><sub><italic>TM</italic></sub> (%)</th><th>MCC (%)</th><th><italic>FPR</italic><sub><italic>TM</italic></sub> (%)</th></tr></thead><tbody><tr><td colspan="7">SigUNet</td></tr><tr><td> As comp<sup>a</sup></td><td><bold>89.6</bold></td><td><bold>1.2</bold></td><td>40.9</td><td>2.1</td><td>97.6</td><td>1.1</td></tr><tr><td> All organisms<sup>b</sup></td><td>89.2</td><td>2.2</td><td>46.1</td><td>1.6</td><td><bold>100.0</bold></td><td><bold>0.0</bold></td></tr><tr><td> Bacteria<sup>c</sup></td><td>–</td><td>–</td><td>49.5</td><td><bold>1.1</bold></td><td>97.6</td><td>1.1</td></tr><tr><td colspan="7">SigUNet-light</td></tr><tr><td> As comp</td><td>84.8</td><td>3.7</td><td><bold>51.7</bold></td><td>1.6</td><td>92.8</td><td>1.1</td></tr><tr><td> All organisms</td><td>89.1</td><td>2.2</td><td>43.3</td><td>1.6</td><td><bold>100.0</bold></td><td><bold>0.0</bold></td></tr><tr><td> Bacteria</td><td>–</td><td>–</td><td>49.5</td><td><bold>1.1</bold></td><td><bold>100.0</bold></td><td><bold>0.0</bold></td></tr></tbody></table><table-wrap-foot><p><sup>a</sup>The model is trained using the same organism as the comparison dataset. <sup>b</sup>The model is trained using all organisms. <sup>c</sup>The model is trained using all of the bacteria data. The best performance is highlighted in bold</p></table-wrap-foot></table-wrap></p>
      <p id="Par15">If training involves all organisms, the <italic>FPR</italic><sub><italic>TM</italic></sub> is improved in three of the four scenarios (SigUNet and SigUNet-light on the SignalP dataset and SigUNet-light on the SPDS17 dataset). A better <italic>FPR</italic><sub><italic>TM</italic></sub> indicates that more transmembrane proteins are discriminated from signal peptides. This suggests that the properties of transmembrane proteins are less different to those of signal peptides across organisms. On the Gram-positive datasets, The best <italic>FPR</italic><sub><italic>TM</italic></sub> is achieved using bacteria data for training. This suggests that some Gram-positive transmembrane proteins are similar to eukaryotic signal peptides, which decreases the ability to discriminate Gram-positive transmembrane proteins from signal peptides. On the Gram-negative datasets, both data augmentation strategies work. Training with bacterial data gives the best MCC and <italic>FPR</italic><sub><italic>TM</italic></sub> on the SignalP Gram-negative dataset; while training with all organisms gives the best MCC and <italic>FPR</italic><sub><italic>TM</italic></sub> on the SPDS17 Gram-negative dataset. These results reveal that data augmentation improves the performance of SigUNet on the bacterial datasets.</p>
      <p id="Par16">In summary, SigUNet is suited to the recognition of eukaryotic signal peptides. Its network architecture requires a relatively large dataset for training. Model reduction and data augmentation are useful, but increasing the amount of data is still required to ensure that SigUNet recognizes bacterial signal peptides.</p>
    </sec>
  </sec>
  <sec id="Sec7">
    <title>Discussion</title>
    <p id="Par17">The Results section compares the performance of the methods and demonstrates the issues of SigUNet in terms of data size. This section discusses the variation in performance by analyzing the sequence composition. Training speed, which is highly dependent on data size, is also discussed in this section.</p>
    <p id="Par18">To analyze the sequence composition, the sequences of each dataset are plotted into sequence logos as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The sequence logo for 96 positions in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a is too confusing to analyze, so the first 20 positions of each dataset are shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>b, c and d for clarity. The top left subplot of Fig. <xref rid="Fig1" ref-type="fig">1</xref>b, c and d are sequence logos plotted for the signal peptides in the SignalP datasets. Although the sequences are from different organisms, the three subplots exhibit a similar pattern. The pattern begins with a fixed M in position one followed by charged (red) amino acids and then by non-polar (green) amino acids. This is consistent with the current knowledge that signal peptides comprise a charged n-region, a hydrophobic h-region and a polar c-region.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Sequence logos generated by WebLogo [<xref ref-type="bibr" rid="CR29">29</xref>]. The <italic>x</italic>-axis indicates the position of the amino acid and the <italic>y</italic>-axis shows the probabilities of amino acids across a given sequence set. <bold>a</bold> Sequence logo for 96 positions for the SignalP Eukaryotes dataset. <bold>b</bold> Sequence logos for the first 20 positions for the Eukaryotes datasets. <bold>c</bold> Sequence logos for the first 20 positions for the Gram-positive datasets. <bold>d</bold> Sequence logos for the first 20 positions for the Gram-negative datasets. Non-polar, charged and polar amino acids are respectively colored green, red and blue</p></caption><graphic xlink:href="12859_2019_3245_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par19">The sequence logos of SPDS17 show a larger variation than those of SignalP across organisms. The top right subplot of Fig. <xref rid="Fig1" ref-type="fig">1</xref>c is more random than other sequence logos that are plotted for signal peptides. This explains why no method gives satisfactory results on the SPDS17 Gram-positive data. Conversely, both of the top left and top right subplots of Figure <xref rid="Fig1" ref-type="fig">1</xref>d have three obvious ‘K’s in positions 2, 3 and 4. This explains why SigUNet and other methods perform well on the SPDS17 Gram-negative data.</p>
    <p id="Par20">To analyze the training speed, SigUNet was trained using datasets of different sizes. Figure <xref rid="Fig2" ref-type="fig">2</xref> shows the epoch-loss plots. Figure <xref rid="Fig2" ref-type="fig">2</xref>a shows that SigUNet stops after a similar number of epochs when 100, 80 and 60% of the data is used. As the time that is required to train an epoch is proportional to the size of the dataset, the training time for SigUNet is linearly proportional to the size of the dataset. The validation losses of the three lines are similar, which shows that 60% of Eukaryotes data is sufficient to train SigUNet. When only 40% or 20% of the data is used, the validation loss is bumpy and SigUNet requires more epochs to train. SigUNet-light gives a similar result. Figure <xref rid="Fig2" ref-type="fig">2</xref>b shows that SigUNet-light stops after a similar number of epochs when 100, 80, 60 and 40% of the data is used. Namely, 40% of the Eukaryotes data is sufficient to train the reduced version of SigUNet.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Epoch-loss plots of training SigUNet. <bold>a</bold> Training SigUNet using different ratios of SignalP Eukaryotes data. <bold>b</bold> Training SigUNet-light using different ratios of SignalP Eukaryotes data. <bold>c</bold> Training DeepSig and SigUNet using the SignalP Eukaryotes data</p></caption><graphic xlink:href="12859_2019_3245_Fig2_HTML" id="MO2"/></fig></p>
    <p id="Par21">Figure <xref rid="Fig2" ref-type="fig">2</xref>c compares the training speed of SigUNet with that for DeepSig. DeepSig stops earlier than SigUNet, but SigUNet gives a lower validation loss. SigUNet is more complex than DeepSig, so these observations are consistent with the common knowledge that simpler models converge faster but perform worse. An interesting observation is that the validation loss of DeepSig is bumpier than that of SigUNet. This shows that SigUNet has more stable training process than DeepSig. In addition to network architecture, there is an obvious difference between DeepSig and SigUNnet in terms of the loss function. The loss function of DeepSig calculates the protein-level cross entropy and SigUNet calculates the amino acid-level cross entropy. Figure <xref rid="Fig2" ref-type="fig">2</xref>c shows that the gradient that is generated by the loss function of SigUNet updates the model more smoothly. This observation is pertinent to future signal peptide studies for the development of loss functions.</p>
  </sec>
  <sec id="Sec8">
    <title>Conclusions</title>
    <p id="Par22">This study proposes a new deep learning model for signal peptide recognition. The proposed model is more complex than those of previous studies by leveraging network improvements that have been developed in computer vision. This study also proposes network modifications to enhance the performance on protein data. The experimental results show that the proposed model outperforms conventional neural networks. This conclusion is consistent with SignalP 5.0 [<xref ref-type="bibr" rid="CR30">30</xref>], which was published on 18 February 2019. Although SignalP 5.0 uses a different evaluation procedure, it gives similar results when advanced network architectures are used.</p>
  </sec>
  <sec id="Sec9">
    <title>Materials and methods</title>
    <sec id="Sec10">
      <title>Evaluation indices</title>
      <p id="Par23">This work uses the Matthews Correlation Coefficient (MCC) to evaluate signal peptide recognition. The MCC measures the correlation between two series of binary data. In practice, the MCC is usually used as an overall index for binary classification by establishing the observed classes as one data series and the predicted classes as the other data series. The MCC is shown as below:
<list list-type="bullet"><list-item><p id="Par24">The definition of the Matthews Correlation Coefficient</p></list-item></list></p>
      <p id="Par25">
        <disp-formula id="Equ1">
          <label>1</label>
          <alternatives>
            <tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{MCC}=\frac{TP\times TN- FP\times FN}{\sqrt{\left( TP+ FP\right)\times \left( TP+ FN\right)\times \left( TN+ FP\right)\times \left( TN+ FN\right)}} $$\end{document}</tex-math>
            <mml:math id="M2" display="block">
              <mml:mi>MCC</mml:mi>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mi mathvariant="italic">TP</mml:mi>
                  <mml:mo>×</mml:mo>
                  <mml:mi mathvariant="italic">TN</mml:mi>
                  <mml:mo>−</mml:mo>
                  <mml:mi mathvariant="italic">FP</mml:mi>
                  <mml:mo>×</mml:mo>
                  <mml:mi mathvariant="italic">FN</mml:mi>
                </mml:mrow>
                <mml:msqrt>
                  <mml:mrow>
                    <mml:mfenced close=")" open="(">
                      <mml:mrow>
                        <mml:mi mathvariant="italic">TP</mml:mi>
                        <mml:mo>+</mml:mo>
                        <mml:mi mathvariant="italic">FP</mml:mi>
                      </mml:mrow>
                    </mml:mfenced>
                    <mml:mo>×</mml:mo>
                    <mml:mfenced close=")" open="(">
                      <mml:mrow>
                        <mml:mi mathvariant="italic">TP</mml:mi>
                        <mml:mo>+</mml:mo>
                        <mml:mi mathvariant="italic">FN</mml:mi>
                      </mml:mrow>
                    </mml:mfenced>
                    <mml:mo>×</mml:mo>
                    <mml:mfenced close=")" open="(">
                      <mml:mrow>
                        <mml:mi mathvariant="italic">TN</mml:mi>
                        <mml:mo>+</mml:mo>
                        <mml:mi mathvariant="italic">FP</mml:mi>
                      </mml:mrow>
                    </mml:mfenced>
                    <mml:mo>×</mml:mo>
                    <mml:mfenced close=")" open="(">
                      <mml:mrow>
                        <mml:mi mathvariant="italic">TN</mml:mi>
                        <mml:mo>+</mml:mo>
                        <mml:mi mathvariant="italic">FN</mml:mi>
                      </mml:mrow>
                    </mml:mfenced>
                  </mml:mrow>
                </mml:msqrt>
              </mml:mfrac>
            </mml:math>
            <graphic xlink:href="12859_2019_3245_Article_Equ1.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p id="Par26">In Eq. <xref rid="Equ1" ref-type="">1</xref>, TP indicates true positive, which is the number of signal peptides that are correctly predicted to be signal peptides; TN indicates true negative, which is the number of non-signal peptides that are correctly predicted to be non-signal peptides; FP indicates false positive, which is the number of non-signal peptides that are incorrectly predicted to be signal peptides; and FN indicates false negative, which is the number of signal peptides that are incorrectly predicted to be non-signal peptides. The characteristics of signal peptides and N-terminal transmembrane helices are similar, so signal peptide predictors must be able to discriminate signal peptides from transmembrane proteins. This study uses the false positive rate for transmembrane proteins (<italic>FPR</italic><sub><italic>TM</italic></sub>) to measure this ability:
<list list-type="bullet"><list-item><p id="Par27">The definition of the false positive rate for transmembrane proteins</p></list-item></list></p>
      <p id="Par28">
        <disp-formula id="Equ2">
          <label>2</label>
          <alternatives>
            <tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ { FP R}_{TM}=\frac{FP_{TM}}{N_{TM}} $$\end{document}</tex-math>
            <mml:math id="M4" display="block">
              <mml:msub>
                <mml:mi mathvariant="italic">FPR</mml:mi>
                <mml:mi mathvariant="italic">TM</mml:mi>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:msub>
                  <mml:mi mathvariant="italic">FP</mml:mi>
                  <mml:mi mathvariant="italic">TM</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>N</mml:mi>
                  <mml:mi mathvariant="italic">TM</mml:mi>
                </mml:msub>
              </mml:mfrac>
            </mml:math>
            <graphic xlink:href="12859_2019_3245_Article_Equ2.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p id="Par29">In Equation <xref rid="Equ2" ref-type="">2</xref>, <italic>N</italic><sub><italic>TM</italic></sub> represents the total quantity of transmembrane proteins and <italic>FP</italic><sub><italic>TM</italic></sub> represents the number of transmembrane proteins that are misclassified as signal peptides. MCC and <italic>FPR</italic><sub><italic>TM</italic></sub> are the main evaluation indices adopted in SignalP 4.0 and DeepSig. This study also uses precision, recall and F1 measure, which are widely used evaluation indices for binary classification:
<list list-type="bullet"><list-item><p id="Par30">The definition of precision</p></list-item></list></p>
      <p id="Par31">
        <disp-formula id="Equ3">
          <label>3</label>
          <alternatives>
            <tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Precision=\frac{TP}{TP+ FP} $$\end{document}</tex-math>
            <mml:math id="M6" display="block">
              <mml:mtext mathvariant="italic">Precision</mml:mtext>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mi mathvariant="italic">TP</mml:mi>
                <mml:mrow>
                  <mml:mi mathvariant="italic">TP</mml:mi>
                  <mml:mo>+</mml:mo>
                  <mml:mi mathvariant="italic">FP</mml:mi>
                </mml:mrow>
              </mml:mfrac>
            </mml:math>
            <graphic xlink:href="12859_2019_3245_Article_Equ3.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <list list-type="bullet">
          <list-item>
            <p id="Par32">The definition of recall</p>
          </list-item>
        </list>
      </p>
      <p id="Par33">
        <disp-formula id="Equ4">
          <label>4</label>
          <alternatives>
            <tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Recall=\frac{TP}{TP+ FN} $$\end{document}</tex-math>
            <mml:math id="M8" display="block">
              <mml:mtext mathvariant="italic">Recall</mml:mtext>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mi mathvariant="italic">TP</mml:mi>
                <mml:mrow>
                  <mml:mi mathvariant="italic">TP</mml:mi>
                  <mml:mo>+</mml:mo>
                  <mml:mi mathvariant="italic">FN</mml:mi>
                </mml:mrow>
              </mml:mfrac>
            </mml:math>
            <graphic xlink:href="12859_2019_3245_Article_Equ4.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
        <list list-type="bullet">
          <list-item>
            <p id="Par34">The definition of F1 measure</p>
          </list-item>
        </list>
      </p>
      <p id="Par35">
        <disp-formula id="Equ5">
          <label>5</label>
          <alternatives>
            <tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ F1=\frac{2\times Precision\times Recall}{Precision+ Recall}=\frac{2\times TP}{2\times TP+ FN+ FP} $$\end{document}</tex-math>
            <mml:math id="M10" display="block">
              <mml:mi>F</mml:mi>
              <mml:mn>1</mml:mn>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mn>2</mml:mn>
                  <mml:mo>×</mml:mo>
                  <mml:mtext mathvariant="italic">Precision</mml:mtext>
                  <mml:mo>×</mml:mo>
                  <mml:mtext mathvariant="italic">Recall</mml:mtext>
                </mml:mrow>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">Precision</mml:mtext>
                  <mml:mo>+</mml:mo>
                  <mml:mtext mathvariant="italic">Recall</mml:mtext>
                </mml:mrow>
              </mml:mfrac>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mn>2</mml:mn>
                  <mml:mo>×</mml:mo>
                  <mml:mi mathvariant="italic">TP</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>2</mml:mn>
                  <mml:mo>×</mml:mo>
                  <mml:mi mathvariant="italic">TP</mml:mi>
                  <mml:mo>+</mml:mo>
                  <mml:mi mathvariant="italic">FN</mml:mi>
                  <mml:mo>+</mml:mo>
                  <mml:mi mathvariant="italic">FP</mml:mi>
                </mml:mrow>
              </mml:mfrac>
            </mml:math>
            <graphic xlink:href="12859_2019_3245_Article_Equ5.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p id="Par36">Precision measures the ratio of correctness when a protein is reported to be a signal peptide; recall measures the fraction of signal peptides that are correctly captured. Precision is an index of exactness or quality and recall is an index of completeness or quantity. F1 measure, which is the harmonic mean of precision and recall, is commonly optimized to balance precision and recall.</p>
    </sec>
    <sec id="Sec11">
      <title>Datasets</title>
      <p id="Par37">Two datasets are used in this study: the SignalP and SPDS17 datasets (Table <xref rid="Tab1" ref-type="table">1</xref>). The SignalP dataset contains three subsets: Eukaryotes, Gram-positive and Gram-negative bacteria. It uses proteins from the UniProtKB/Swiss-Prot release 2010_05 [<xref ref-type="bibr" rid="CR31">31</xref>] and excludes hypothetical proteins and proteins with less than 30 amino acids. Positive samples in the SignalP dataset are signal peptides with experimentally verified cleavage sites. Negative samples are (a) proteins whose subcellular locations are only nuclear or cytosolic and (b) proteins whose first 70 amino acids are tagged as a transmembrane region. A homology reduction algorithm that was proposed by Hobohm et al. [<xref ref-type="bibr" rid="CR32">32</xref>] is applied to the first 70 amino acids. This algorithm considers two proteins for which the local alignment has more than 17 identical amino acids as redundant for Eukaryotes and two proteins for which the local alignment has more than 21 identical amino acids as redundant for bacteria. A small part of the SignalP dataset was used as a comparison dataset by Petersen et al. [<xref ref-type="bibr" rid="CR12">12</xref>].</p>
      <p id="Par38">The SPDS17 dataset was constructed by Savojardo et al. [<xref ref-type="bibr" rid="CR6">6</xref>]. It contains proteins from UniProtKB/Swiss-Prot releases 2015_06 to 2017_04. Similar to the SignalP dataset, the SPDS17 dataset separates proteins into three subsets: Eukaryotes, Gram-positive bacteria and Gram-negative bacteria. The definitions of positive and negative samples are identical to those in the SignalP dataset. Namely, the SPDS17 dataset is a comparison dataset for the SignalP dataset that accommodates newly discovered proteins. The homology of the SPDS17 is reduced using the blastclust algorithm with an E-value of 0.001 [<xref ref-type="bibr" rid="CR33">33</xref>]. Proteins with greater than a 25% similarity are considered as redundant. Proteins with a similarity higher than 25% to any protein in the SignalP dataset are removed.</p>
    </sec>
    <sec id="Sec12">
      <title>Data preprocessing</title>
      <p id="Par39">Signal peptides only appear at the front of amino acid chains, so only a fixed number of amino acids from each protein sequence are used as an input. This study uses 96 as the input length, which is the same as DeepSig. The first 96 amino acids of a protein are one-hot encoded. Namely, every amino acid is encoded into a 20-dimensional binary vector, where 19 positions are zero and only the position that corresponds to the amino acid is one. An uncommon or unknown amino acid such as ‘X’ is encoded as a zero vector. To encode all proteins into a 96 × 20 matrix, zeros are padded to vectors for proteins that have less than 96 amino acids. To determine the ability to discriminate signal peptides from transmembrane proteins, this study classifies amino acids into three classes. If an amino acid is located in a signal peptide region, it is labeled ‘S’. If an amino acid is located in a transmembrane region, it is labeled ‘T’. If an amino acid is not located in a signal peptide nor a transmembrane region, it is labeled ‘N’. The class of a protein is one-hot encoded as a 96 × 3 matrix. In summary, given a protein sequence, this study encodes it into a 96 × 20 matrix as the input. The output is a 96 × 3 matrix, which includes amino acid-level predictions for the given protein sequence.</p>
    </sec>
    <sec id="Sec13">
      <title>Network architecture</title>
      <p id="Par40">The network architecture of this work is based on U-Net, which achieves excellent results for the semantic segmentation of medical images [<xref ref-type="bibr" rid="CR21">21</xref>]. Medical image datasets are much smaller than other common computer vision datasets and U-Net is tailored to this situation. Figure <xref rid="Fig3" ref-type="fig">3</xref> shows the architecture of U-Net. The model input is a 572 × 572 grey scale image and the output is a 388x388x2 semantic segmented image. Convolutional layers (denoted as ‘conv 3x3 ReLU’ blue arrows and ‘conv 1 × 1’ teal arrows in Fig. <xref rid="Fig3" ref-type="fig">3</xref>) use filters to recognize local patterns [<xref ref-type="bibr" rid="CR13">13</xref>]. A filter is a matrix that is convolved across the width and height of the input image to generate a feature map. The suffix (‘3x3 ReLU’ and ‘1 × 1’) indicates the size of the filter and the activation functions of the corresponding convolutional layers. The ‘copy and crop’ gray arrows in Fig. <xref rid="Fig3" ref-type="fig">3</xref> copy the output of a source layer (the left end of the arrow) and crop it to fit the size of the destination layer (the right end of the arrow). Pooling layers (denoted as ‘max pool 2x2’ red arrows in Fig. <xref rid="Fig3" ref-type="fig">3</xref>) merge adjacent output values from previous layers into one value to reduce network complexity [<xref ref-type="bibr" rid="CR34">34</xref>]. Max pooling uses the maximum value of a local area as the output. The suffix (‘2x2’) indicates the size of each local area that is to be merged. Up-convolutional layers (denoted as ‘up-conv 2x2’ green arrows in Fig. <xref rid="Fig3" ref-type="fig">3</xref>), which perform an inverse operation to convolutional layers, expand the information that is compressed by convolutional and pooling layers [<xref ref-type="bibr" rid="CR35">35</xref>].
<fig id="Fig3"><label>Fig. 3</label><caption><p>The network architecture of U-Net [<xref ref-type="bibr" rid="CR21">21</xref>]</p></caption><graphic xlink:href="12859_2019_3245_Fig3_HTML" id="MO3"/></fig></p>
      <p id="Par41">U-Net is used for two-dimensional images, so this study refines it for use with one-dimensional protein sequences. Each two-dimensional operation becomes one-dimensional and each position in a sequence is represented by a 20-channel vector. However, this trivial one-dimensional U-Net does not allow efficient signal peptide recognition (Table <xref rid="Tab8" ref-type="table">8</xref>). To solve the problem, this study refines the number of channels in each layer (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). The network architecture is named SigUNet. The original U-Net fixes the channel size of the first convolutional layer to 64 and doubles the channel size to 128, 256, 512 and 1024 after each pooling layer. This made number of parameters of U-Net increases exponentially. In SigUNet, the channel size starts from <italic>m</italic> and increases linearly by <italic>n</italic>. Both <italic>m</italic> and <italic>n</italic> are hyper-parameters that are determined using nested cross validation. Unlike pixels in an image, it is hypothesized that each amino acid contains important information and is not disposable. Using max pooling, the information in an amino acid can be lost if its neighbor has a large value. Therefore, average pooling is adopted in SigUNet. Table <xref rid="Tab8" ref-type="table">8</xref> shows the performance of using different pooling operations. A reduced version of SigUNet for bacteria signal peptides is shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. The reduced SigUNet is named SigUNet-light.
<table-wrap id="Tab8"><label>Table 8</label><caption><p>The performance of different network architectures on the SignalP Eukaryotes dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Architecture</th><th>MCC (%)</th><th><italic>FPR</italic><sub><italic>TM</italic></sub> (%)</th><th>Recall (%)</th><th>Precision (%)</th><th>F1 measure (%)</th></tr></thead><tbody><tr><td>U-Net-1D<sup>a</sup></td><td>84.1</td><td>6.8</td><td>87.3</td><td>88.5</td><td>87.9</td></tr><tr><td>SigUNet-max<sup>b</sup></td><td>88.6</td><td>5.0</td><td>91.4</td><td>91.3</td><td>91.3</td></tr><tr><td>SigUNet</td><td>90.2</td><td>4.0</td><td>92.1</td><td>93.0</td><td>92.5</td></tr></tbody></table><table-wrap-foot><p><sup>a</sup>A one-dimensional U-Net that has the same network configuration as Fig. <xref rid="Fig3" ref-type="fig">3</xref>, but the input and output layer are modified for protein sequences. <sup>b</sup>The max pooling layers in Fig. <xref rid="Fig4" ref-type="fig">4</xref> are replaced with average pooling layers</p></table-wrap-foot></table-wrap>
<fig id="Fig4"><label>Fig. 4</label><caption><p>The network architecture of SigUNet</p></caption><graphic xlink:href="12859_2019_3245_Fig4_HTML" id="MO4"/></fig>
<fig id="Fig5"><label>Fig. 5</label><caption><p>The network architecture of SigUNet-light, which is a reduced version of SigUNet</p></caption><graphic xlink:href="12859_2019_3245_Fig5_HTML" id="MO5"/></fig></p>
      <p id="Par42">The architecture of SigUNet outputs a 96 × 3 matrix that represents the probabilities of the 96 amino acids being classified as either a signal peptide, a transmembrane region or neither. The loss function is cross entropy shown as below:
<list list-type="bullet"><list-item><p id="Par43">The loss function of SigUNet</p></list-item></list></p>
      <p id="Par44"><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{Loss}\left(\mathrm{x},\mathrm{y}\right)=-\sum \limits_{i=1}^{96}\sum \limits_{j=1}^3{y}_{ij}\mathit{\ln}\left(h{\left(\mathrm{x}\right)}_{ij}\right) $$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mtext>Loss</mml:mtext><mml:mfenced close=")" open="(" separators=","><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>96</mml:mn></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>3</mml:mn></mml:munderover><mml:msub><mml:mi>y</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo mathvariant="italic">ln</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi>h</mml:mi><mml:msub><mml:mfenced close=")" open="("><mml:mi mathvariant="normal">x</mml:mi></mml:mfenced><mml:mi mathvariant="italic">ij</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2019_3245_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>Here x represents an input sample, which is a 96 × 20 matrix; y represents the real class of the input sample, which is one-hot encoded to a 96 × 3 matrix; <italic>y</italic><sub><italic>ij</italic></sub> is a binary value that indicates whether the <italic>i</italic>-th amino acid is of the <italic>j</italic>-th class; <italic>h</italic>(x) represents the network output, which is a 96 × 3 matrix; and <italic>h</italic>(x)<sub><italic>ij</italic></sub> represents the probability of the <italic>i</italic>-th amino being of the <italic>j</italic>-th class. The 96 × 3 output matrix for an input sequence is then transformed to a binary prediction. If the probability of any four consecutive amino acids being a signal peptide is greater than a threshold, the input sequence is classified to be a signal peptide. The threshold is a hyper-parameter of SigUNet and is determined using nested cross validation.</p>
    </sec>
    <sec id="Sec14">
      <title>Nested cross validation</title>
      <p id="Par45">Cross validation is used in machine learning to prevent overfitting. For a <italic>k</italic>-fold cross validation, the data is split into <italic>k</italic> partitions. Each partition is used for testing and the remaining <italic>k</italic>-1 partitions are used to train a model. However, if the performance of cross validation is used to determine hyper-parameters, it is no longer an appropriate indicator for model performance. To solve this issue, this work adopts a nested cross validation procedure (Fig. <xref rid="Fig6" ref-type="fig">6</xref>), whereby hyper-parameters are determined using an inner <italic>k</italic>-1-fold cross validation on the <italic>k</italic>-1 training partitions. For each testing partition, the inner <italic>k</italic>-1-fold cross validation constructs <italic>k</italic>-1 models and their predictions on the testing partition are averaged. This procedure does not peek at the testing partition when the hyper-parameters are tuned. Therefore, the performance of the outer cross validation can be used to represent the model performance. The nested cross validation and <italic>k</italic> = 5 are the same as the evaluation procedure in SignalP 4.0 and DeepSig.
<fig id="Fig6"><label>Fig. 6</label><caption><p>The pseudo code of nested cross validation</p></caption><graphic xlink:href="12859_2019_3245_Fig6_HTML" id="MO6"/></fig></p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank Yi-Ying Lin for reviewing our manuscript and his valuable suggestions.</p>
    <sec id="FPar1">
      <title>About this supplement</title>
      <p id="Par46">This article has been published as part of <italic>BMC Bioinformatics Volume 20 Supplement 24, 2019: The International Conference on Intelligent Biology and Medicine (ICIBM) 2019.</italic> The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-20-supplement-24">https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-20-supplement-24</ext-link> .</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>JMW and DTHC conceived the research topic. JMW developed the algorithm and conducted the experiments. YCL helped to organize the experimental results. DTHC wrote the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Publication costs are funded by Ministry of Science and Technology, Taiwan (MOST 107–2221-E-006-219-).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The source code of SigUNet is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/mbilab/SigUNet">https://github.com/mbilab/SigUNet</ext-link>.</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p id="Par47">Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p id="Par48">Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par49">The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>von Heijne</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>The signal peptide</article-title>
        <source>J Membr Biol</source>
        <year>1990</year>
        <volume>115</volume>
        <issue>3</issue>
        <fpage>195</fpage>
        <lpage>201</lpage>
        <pub-id pub-id-type="doi">10.1007/BF01868635</pub-id>
        <pub-id pub-id-type="pmid">2197415</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <mixed-citation publication-type="other">Blobel G, Sabatini DD. Ribosome-membrane interaction in eukaryotic cells. In: <italic>Biomembranes</italic>: Springer; 1971. p. 193–5.</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Milstein</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Brownlee</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Harrison</surname>
            <given-names>TM</given-names>
          </name>
          <name>
            <surname>Mathews</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>A possible precursor of immunoglobulin light chains</article-title>
        <source>Nat New Biol</source>
        <year>1972</year>
        <volume>239</volume>
        <issue>91</issue>
        <fpage>117</fpage>
        <pub-id pub-id-type="doi">10.1038/newbio239117a0</pub-id>
        <?supplied-pmid 4507519?>
        <pub-id pub-id-type="pmid">4507519</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blobel</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Dobberstein</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Transfer of proteins across membranes. I. Presence of proteolytically processed and unprocessed nascent immunoglobulin light chains on membrane-bound ribosomes of murine myeloma</article-title>
        <source>J Cell Biol</source>
        <year>1975</year>
        <volume>67</volume>
        <issue>3</issue>
        <fpage>835</fpage>
        <lpage>851</lpage>
        <pub-id pub-id-type="doi">10.1083/jcb.67.3.835</pub-id>
        <?supplied-pmid 811671?>
        <pub-id pub-id-type="pmid">811671</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blobel</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Dobberstein</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Transfer of proteins across membranes. II. Reconstitution of functional rough microsomes from heterologous components</article-title>
        <source>J Cell Biol</source>
        <year>1975</year>
        <volume>67</volume>
        <issue>3</issue>
        <fpage>852</fpage>
        <lpage>862</lpage>
        <pub-id pub-id-type="doi">10.1083/jcb.67.3.852</pub-id>
        <?supplied-pmid 811672?>
        <pub-id pub-id-type="pmid">811672</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Savojardo</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Martelli</surname>
            <given-names>PL</given-names>
          </name>
          <name>
            <surname>Fariselli</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Casadio</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>DeepSig: deep learning improves signal peptide detection in proteins</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>34</volume>
        <issue>10</issue>
        <fpage>1690</fpage>
        <lpage>1696</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx818</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Von Heijne</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Patterns of amino acids near signal-sequence cleavage sites</article-title>
        <source>Eur J Biochem</source>
        <year>1983</year>
        <volume>133</volume>
        <issue>1</issue>
        <fpage>17</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1432-1033.1983.tb07424.x</pub-id>
        <pub-id pub-id-type="pmid">6852022</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Von Heijne</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>A new method for predicting signal sequence cleavage sites</article-title>
        <source>Nucleic Acids Res</source>
        <year>1986</year>
        <volume>14</volume>
        <issue>11</issue>
        <fpage>4683</fpage>
        <lpage>4690</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/14.11.4683</pub-id>
        <pub-id pub-id-type="pmid">3714490</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Nielsen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Krogh</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Prediction of signal peptides and signal anchors by a hidden Markov model</article-title>
        <source>
          <italic>Ismb</italic>
        </source>
        <year>1998</year>
        <fpage>122</fpage>
        <lpage>130</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nielsen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Engelbrecht</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Brunak</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>von Heijne</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Identification of prokaryotic and eukaryotic signal peptides and prediction of their cleavage sites</article-title>
        <source>Protein Eng</source>
        <year>1997</year>
        <volume>10</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1093/protein/10.1.1</pub-id>
        <?supplied-pmid 9051728?>
        <pub-id pub-id-type="pmid">9051728</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bendtsen</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>von Heijne</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Brunak</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Improved prediction of signal peptides: SignalP 3.0</article-title>
        <source>J Mol Biol</source>
        <year>2004</year>
        <volume>340</volume>
        <issue>4</issue>
        <fpage>783</fpage>
        <lpage>795</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmb.2004.05.028</pub-id>
        <?supplied-pmid 15223320?>
        <pub-id pub-id-type="pmid">15223320</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Petersen</surname>
            <given-names>TN</given-names>
          </name>
          <name>
            <surname>Brunak</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>von Heijne</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>SignalP 4.0: discriminating signal peptides from transmembrane regions</article-title>
        <source>Nat Methods</source>
        <year>2011</year>
        <volume>8</volume>
        <issue>10</issue>
        <fpage>785</fpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.1701</pub-id>
        <?supplied-pmid 21959131?>
        <pub-id pub-id-type="pmid">21959131</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bottou</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Haffner</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Gradient-based learning applied to document recognition</article-title>
        <source>Proc IEEE</source>
        <year>1998</year>
        <volume>86</volume>
        <issue>11</issue>
        <fpage>2278</fpage>
        <lpage>2324</lpage>
        <pub-id pub-id-type="doi">10.1109/5.726791</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>GE</given-names>
          </name>
        </person-group>
        <article-title>Imagenet classification with deep convolutional neural networks</article-title>
        <source>
          <italic>Advances in neural information processing systems</italic>
        </source>
        <year>2012</year>
        <fpage>1097</fpage>
        <lpage>1105</lpage>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. <italic>arXiv preprint arXiv</italic>. 2014:<italic>1409</italic>.<italic>1556</italic>.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>1780</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Vinyals</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>QV</given-names>
          </name>
        </person-group>
        <article-title>Sequence to sequence learning with neural networks</article-title>
        <source>
          <italic>Advances in neural information processing systems</italic>
        </source>
        <year>2014</year>
        <fpage>3104</fpage>
        <lpage>3112</lpage>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Joshi</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>MusiteDeep: a deep-learning framework for general and kinase-specific phosphorylation site prediction</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <issue>24</issue>
        <fpage>3909</fpage>
        <lpage>3916</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx496</pub-id>
        <?supplied-pmid 29036382?>
        <pub-id pub-id-type="pmid">29036382</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Mao</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yi</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Protein secondary structure prediction by using deep learning method</article-title>
        <source>Knowl-Based Syst</source>
        <year>2017</year>
        <volume>118</volume>
        <fpage>115</fpage>
        <lpage>123</lpage>
        <pub-id pub-id-type="doi">10.1016/j.knosys.2016.11.015</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Long</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shelhamer</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Darrell</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Fully convolutional networks for semantic segmentation</article-title>
        <source>
          <italic>Proceedings of the IEEE conference on computer vision and pattern recognition</italic>
        </source>
        <year>2015</year>
        <fpage>3431</fpage>
        <lpage>3440</lpage>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation. In: <italic>International Conference on Medical image computing and computer-assisted intervention</italic>: Springer; <italic>2015</italic>. p. 234–41.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>L-C</given-names>
          </name>
          <name>
            <surname>Papandreou</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kokkinos</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Yuille</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>2018</year>
        <volume>40</volume>
        <issue>4</issue>
        <fpage>834</fpage>
        <lpage>848</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id>
        <?supplied-pmid 28463186?>
        <pub-id pub-id-type="pmid">28463186</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hiller</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Grote</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Scheer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Münch</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Jahn</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>PrediSi: prediction of signal peptides and their cleavage positions</article-title>
        <source>Nucleic Acids Res</source>
        <year>2004</year>
        <volume>32</volume>
        <issue>suppl_2</issue>
        <fpage>W375</fpage>
        <lpage>W379</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkh378</pub-id>
        <?supplied-pmid 15215414?>
        <pub-id pub-id-type="pmid">15215414</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Käll</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Krogh</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sonnhammer</surname>
            <given-names>EL</given-names>
          </name>
        </person-group>
        <article-title>A combined transmembrane topology and signal peptide prediction method</article-title>
        <source>J Mol Biol</source>
        <year>2004</year>
        <volume>338</volume>
        <issue>5</issue>
        <fpage>1027</fpage>
        <lpage>1036</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmb.2004.03.016</pub-id>
        <?supplied-pmid 15111065?>
        <pub-id pub-id-type="pmid">15111065</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Käll</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Krogh</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sonnhammer</surname>
            <given-names>EL</given-names>
          </name>
        </person-group>
        <article-title>An HMM posterior decoder for sequence feature prediction that includes homology information</article-title>
        <source>Bioinformatics</source>
        <year>2005</year>
        <volume>21</volume>
        <issue>suppl_1</issue>
        <fpage>i251</fpage>
        <lpage>i257</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bti1014</pub-id>
        <?supplied-pmid 15961464?>
        <pub-id pub-id-type="pmid">15961464</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reynolds</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Käll</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Riffle</surname>
            <given-names>ME</given-names>
          </name>
          <name>
            <surname>Bilmes</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Noble</surname>
            <given-names>WS</given-names>
          </name>
        </person-group>
        <article-title>Transmembrane topology and signal peptide prediction using dynamic bayesian networks</article-title>
        <source>PLoS Comput Biol</source>
        <year>2008</year>
        <volume>4</volume>
        <issue>11</issue>
        <fpage>e1000213</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000213</pub-id>
        <?supplied-pmid 18989393?>
        <pub-id pub-id-type="pmid">18989393</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tsirigos</surname>
            <given-names>KD</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Shu</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Käll</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Elofsson</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>The TOPCONS web server for consensus prediction of membrane protein topology and signal peptides</article-title>
        <source>Nucleic Acids Res</source>
        <year>2015</year>
        <volume>43</volume>
        <issue>W1</issue>
        <fpage>W401</fpage>
        <lpage>W407</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkv485</pub-id>
        <?supplied-pmid 25969446?>
        <pub-id pub-id-type="pmid">25969446</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Viklund</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Bernsel</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Skwark</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Elofsson</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>SPOCTOPUS: a combined predictor of signal peptides and membrane protein topology</article-title>
        <source>Bioinformatics</source>
        <year>2008</year>
        <volume>24</volume>
        <issue>24</issue>
        <fpage>2928</fpage>
        <lpage>2929</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btn550</pub-id>
        <?supplied-pmid 18945683?>
        <pub-id pub-id-type="pmid">18945683</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Crooks</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Hon</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Chandonia</surname>
            <given-names>J-M</given-names>
          </name>
          <name>
            <surname>Brenner</surname>
            <given-names>SE</given-names>
          </name>
        </person-group>
        <article-title>WebLogo: a sequence logo generator</article-title>
        <source>Genome Res</source>
        <year>2004</year>
        <volume>14</volume>
        <issue>6</issue>
        <fpage>1188</fpage>
        <lpage>1190</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.849004</pub-id>
        <?supplied-pmid 15173120?>
        <pub-id pub-id-type="pmid">15173120</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Almagro Armenteros</surname>
            <given-names>José Juan</given-names>
          </name>
          <name>
            <surname>Tsirigos</surname>
            <given-names>Konstantinos D.</given-names>
          </name>
          <name>
            <surname>Sønderby</surname>
            <given-names>Casper Kaae</given-names>
          </name>
          <name>
            <surname>Petersen</surname>
            <given-names>Thomas Nordahl</given-names>
          </name>
          <name>
            <surname>Winther</surname>
            <given-names>Ole</given-names>
          </name>
          <name>
            <surname>Brunak</surname>
            <given-names>Søren</given-names>
          </name>
          <name>
            <surname>von Heijne</surname>
            <given-names>Gunnar</given-names>
          </name>
          <name>
            <surname>Nielsen</surname>
            <given-names>Henrik</given-names>
          </name>
        </person-group>
        <article-title>SignalP 5.0 improves signal peptide predictions using deep neural networks</article-title>
        <source>Nature Biotechnology</source>
        <year>2019</year>
        <volume>37</volume>
        <issue>4</issue>
        <fpage>420</fpage>
        <lpage>423</lpage>
        <pub-id pub-id-type="doi">10.1038/s41587-019-0036-z</pub-id>
        <?supplied-pmid 30778233?>
        <pub-id pub-id-type="pmid">30778233</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Consortium</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>The universal protein resource (UniProt) in 2010</article-title>
        <source>Nucleic Acids Res</source>
        <year>2009</year>
        <volume>38</volume>
        <issue>suppl_1</issue>
        <fpage>D142</fpage>
        <lpage>D148</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkp846</pub-id>
        <pub-id pub-id-type="pmid">19843607</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hobohm</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Scharf</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Selection of representative protein data sets</article-title>
        <source>Protein Sci</source>
        <year>1992</year>
        <volume>1</volume>
        <issue>3</issue>
        <fpage>409</fpage>
        <lpage>417</lpage>
        <pub-id pub-id-type="doi">10.1002/pro.5560010313</pub-id>
        <?supplied-pmid 1304348?>
        <pub-id pub-id-type="pmid">1304348</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Altschul</surname>
            <given-names>SF</given-names>
          </name>
          <name>
            <surname>Gish</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Myers</surname>
            <given-names>EW</given-names>
          </name>
          <name>
            <surname>Lipman</surname>
            <given-names>DJ</given-names>
          </name>
        </person-group>
        <article-title>Basic local alignment search tool</article-title>
        <source>J Mol Biol</source>
        <year>1990</year>
        <volume>215</volume>
        <issue>3</issue>
        <fpage>403</fpage>
        <lpage>410</lpage>
        <pub-id pub-id-type="doi">10.1016/S0022-2836(05)80360-2</pub-id>
        <?supplied-pmid 2231712?>
        <pub-id pub-id-type="pmid">2231712</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Scherer D, Müller A, Behnke S. Evaluation of pooling operations in convolutional architectures for object recognition. In: <italic>International conference on artificial neural networks</italic>: Springer; 2010. p. 92–101.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Deep convolutional neural network for image deconvolution</article-title>
        <source>
          <italic>Advances in Neural Information Processing Systems</italic>
        </source>
        <year>2014</year>
        <fpage>1790</fpage>
        <lpage>1798</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
