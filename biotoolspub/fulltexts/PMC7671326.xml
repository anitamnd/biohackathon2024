<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">NAR Genom Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">NAR Genom Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">nargab</journal-id>
    <journal-title-group>
      <journal-title>NAR Genomics and Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2631-9268</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7671326</article-id>
    <article-id pub-id-type="doi">10.1093/nargab/lqz015</article-id>
    <article-id pub-id-type="publisher-id">lqz015</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methods Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ELECTOR: evaluator for long reads correction methods</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Marchet</surname>
          <given-names>Camille</given-names>
        </name>
        <!--<email>marchetcamille@gmail.com</email>-->
        <xref ref-type="aff" rid="AFF1">1</xref>
        <xref ref-type="aff" rid="AFF4">4</xref>
        <xref ref-type="corresp" rid="COR1"/>
        <xref ref-type="author-notes" rid="FN1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Morisse</surname>
          <given-names>Pierre</given-names>
        </name>
        <xref ref-type="aff" rid="AFF2">2</xref>
        <xref ref-type="author-notes" rid="FN1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lecompte</surname>
          <given-names>Lolita</given-names>
        </name>
        <xref ref-type="aff" rid="AFF1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lefebvre</surname>
          <given-names>Arnaud</given-names>
        </name>
        <xref ref-type="aff" rid="AFF3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lecroq</surname>
          <given-names>Thierry</given-names>
        </name>
        <xref ref-type="aff" rid="AFF3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-0776-6407</contrib-id>
        <name>
          <surname>Peterlongo</surname>
          <given-names>Pierre</given-names>
        </name>
        <xref ref-type="aff" rid="AFF1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Limasset</surname>
          <given-names>Antoine</given-names>
        </name>
        <xref ref-type="aff" rid="AFF4">4</xref>
      </contrib>
    </contrib-group>
    <aff id="AFF1"><label>1</label><institution>Univ Rennes, CNRS, Inria</institution>, IRISA—UMR 6074, F-35000 Rennes, <country country="FR">France</country></aff>
    <aff id="AFF2"><label>2</label><institution>Normandie Université, UNIROUEN, INSA Rouen</institution>, LITIS, 76000 Rouen, <country country="FR">France</country></aff>
    <aff id="AFF3"><label>3</label><institution>Normandie Univ, UNIROUEN, LITIS</institution>, 76000 Rouen, <country country="FR">France</country></aff>
    <aff id="AFF4"><label>4</label><institution>Univ. Lille, CNRS</institution>, UMR 9189 - CRIStAL, 59655 Villeneuve-d'Ascq, <country country="FR">France</country></aff>
    <author-notes>
      <corresp id="COR1">To whom correspondence should be addressed. Tel: +33 3 28 77 85 41; Email: <email>marchetcamille@gmail.com</email></corresp>
      <fn id="FN1">
        <p>The authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-11-14">
      <day>14</day>
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>14</day>
      <month>11</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>2</volume>
    <issue>1</issue>
    <elocation-id>lqz015</elocation-id>
    <history>
      <date date-type="received">
        <day>05</day>
        <month>8</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>24</day>
        <month>9</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>16</day>
        <month>10</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press on behalf of NAR Genomics and Bioinformatics.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact <email>journals.permissions@oup.com</email></license-p>
      </license>
    </permissions>
    <self-uri xlink:href="lqz015.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>The error rates of third-generation sequencing data have been capped &gt;5%, mainly containing insertions and deletions. Thereby, an increasing number of diverse long reads correction methods have been proposed. The quality of the correction has huge impacts on downstream processes. Therefore, developing methods allowing to evaluate error correction tools with precise and reliable statistics is a crucial need. These evaluation methods rely on costly alignments to evaluate the quality of the corrected reads. Thus, key features must allow the fast comparison of different tools, and scale to the increasing length of the long reads. Our tool, ELECTOR, evaluates long reads correction and is directly compatible with a wide range of error correction tools. As it is based on multiple sequence alignment, we introduce a new algorithmic strategy for alignment segmentation, which enables us to scale to large instances using reasonable resources. To our knowledge, we provide the unique method that allows producing reproducible correction benchmarks on the latest ultra-long reads (&gt;100 k bases). It is also faster than the current state-of-the-art on other datasets and provides a wider set of metrics to assess the read quality improvement after correction. ELECTOR is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/kamimrcht/ELECTOR">https://github.com/kamimrcht/ELECTOR</ext-link>) and Bioconda.</p>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">Inria</named-content>
          <named-content content-type="funder-identifier">10.13039/100012950</named-content>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="12"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="SEC1">
    <title>INTRODUCTION</title>
    <sec id="SEC1-1">
      <title>Motivations</title>
      <p>Pacific Biosciences (PB) and Oxford Nanopore Technologies (ONT) long reads, despite their high error rates and complex error profiles, were rapidly adopted for various applications. An increasing number of projects, especially for assembly (<xref rid="B1" ref-type="bibr">1</xref>), long-distance haplotyping or structural variant calling (<xref rid="B2" ref-type="bibr">2</xref>), indeed benefit from the long-range information these reads provide. These reads display high error rates (from 5% to 12%, according to technologies and libraries, to as much as 30% for the oldest datasets), that largely surpass those of Illumina reads. Given these high error rates, the first step of many applications is error correction. However, this stage can be a time bottleneck (<xref rid="B2" ref-type="bibr">2</xref>).</p>
      <p>Moreover, contrary to Illumina, where the majority of errors are substitutions, long reads mainly contain insertions and deletions (indels) (ONT reads are more deletion-prone, whereas PB reads contain more insertions). This combination of issues requires novel and specific algorithmic developments. To this extent, dozens of error correction methods directly targeting these long reads emerged in the last 5 years. The first range of error correction tools, called ‘hybrid correctors’, uses both short and long reads to perform error correction, relying on the deep coverage and low error rate of the short reads to enhance long reads sequences. The second group of methods, called ‘self-correctors’, intends to correct long reads with the sole information contained in their sequences (see (<xref rid="B3" ref-type="bibr">3</xref>) for a review of correctors). Both paradigms include quite diverse algorithmic solutions, which make it difficult to globally compare the correction results (in terms of corrected bases, quality and performances) without a proper benchmark. Besides, the quality of the error correction has considerable impacts on downstream processes. Hence, it is interesting to know beforehand which corrector is best suited for a particular dataset (according to its coverage, its error rate, the sequenced genome or the sequencing technology, for instance). Developing methods allowing to evaluate error correction tools with precise and reliable statistics is, therefore, a crucial need.</p>
      <p>Methods for evaluating correctors should allow tracking the novelties of the methods. Indeed, since long read technologies still evolve, current correctors implementations are prone to many changes. Methods for evaluating correctors must be usable on datasets of various complexity (from bacteria to eukaryotes) to reproduce a wide variety of possible scenarios. They should also be fast and lightweight, and should not be orders of magnitude more resource and time consuming than the actual correction methods they evaluate. This aspect is particularly critical, since correction evaluators also stand in the perspective of new correction methods developments, as they can help to provide accurate and quick comparisons to the state-of-the-art. For developers as well as users, correction evaluators should describe with precision the correction method’s behavior (i.e. the number of corrected bases, introduced errors or read breakups) to identify its potential pitfalls.</p>
    </sec>
    <sec id="SEC1-2">
      <title>Previous works</title>
      <p>Works introducing novel correction methods usually evaluate the quality of their tools based on how well the corrected long reads realign to the reference. Despite being useful, this information remains incomplete. In particular, it is likely not to mention poor quality reads, or regions to which it is difficult to align.</p>
      <p>In (<xref rid="B6" ref-type="bibr">6</xref>), La <italic>et al.</italic> introduced a new way to obtain metrics describing the quality of the error correction that does not solely present the similarities between the aligned corrected reads and the reference genome. Relying on simulated data, they proposed the idea of a three-way alignment between the reference genome, the uncorrected reads and the corrected reads. They presented results on PB data for hybrid error correction tools, by introducing LRCstats, an evaluation tool aiming at answering to the problematics above.</p>
      <p>With its three-way alignment scheme, LRCstats provides reads’ error rate before and after correction, as well as the detailed count of every type of error. However, only studying the reads’ error rate after correction is not a satisfying indication of the corrector’s behavior. For instance, there is no clue about the putative insertions of new errors by the corrector. To perform such analysis of the method’s pros and cons, we need additional metrics such as precision (relevant corrected bases among all bases modified by the corrector) and recall (correct bases that have been retrieved by the corrector among all bases to be corrected). Such metrics have already been proposed in earlier works dedicated to short reads, such as the error correction evaluation toolkit introduced in (<xref rid="B7" ref-type="bibr">7</xref>). However, this contribution is out of the scope of this work. Indeed, algorithms to process short reads are different from those at stake in our case, due to the length, the high error rates and the complex error profiles of the long reads.</p>
      <p>Moreover, LRCstats relies on a multiple alignment scheme that suffers from high resource needs when processing large numbers of reads, i.e. when coverage or genome sizes are large. For the same reason, LRCstats alignment scheme becomes limited when sequences to process grow. However, the sequencing depth and the length of the long reads keep on increasing, especially with so-called ONT ultra-long reads (up to 1 M bases) starting to appear in recent works for larger genomes (<xref rid="B8" ref-type="bibr">8</xref>). Moreover, deep coverages are expected to help the correction of very long sequences (<xref rid="B2" ref-type="bibr">2</xref>). Thus, novel methods must be proposed in order to evaluate the correction of such datasets in a reasonable amount of time.</p>
    </sec>
    <sec id="SEC1-3">
      <title>Contribution</title>
      <p>To cope with the identified limits of LRCstats, we propose ELECTOR, a new evaluation tool for long read error correction methods. ELECTOR can evaluate the correction of simulated as well as real long read datasets, provided a reference genome that is available for the sequenced species. It takes as input a reference genome in FASTA format, a set of corrected reads in FASTA format, and the corresponding uncorrected reads, either via a FASTA format file in the case of real data or via the suite of files provided by the simulator in case of simulated data. In its output, ELECTOR provides a broader range of metrics than LRCstats, which evaluates the actual quality of the correction. In particular, it measures recall, precision and error rate for each read. ELECTOR also informs about typical difficulties long read correctors can encounter, such as homopolymers, and reads that have been trimmed, split or extended during the correction. Finally, it also provides reads remapping and assembly metrics.</p>
      <p>In order for ELECTOR to provide these additional metrics, we propose a novel multiple sequence alignment (MSA) strategy. This new algorithmic approach is designed to allow the MSA computation to scale to ultra-long reads and to large datasets of several billions of base-pairs. It compares in a fast way three different versions of each read: the ‘corrected’ version, the ‘uncorrected’ version and the ‘reference’ version, which is a substring of the reference genome. For each read, we perform a MSA of its triplet. A key idea of this strategy is a divide-and-conquer approach that divides the reads into smaller sequences with an anchoring process, and thus allows to compute several smaller MSAs. These multiple, smaller MSAs, are then combined to obtain the final MSA, of the whole length of the sequences. The anchoring process is designed to work with erroneous sequences and takes into account gapped alignment due to truncated corrected reads. We believe that the interests of this novelty are not limited to the ELECTOR framework. Indeed, it may be a useful strategy for any domain requiring MSAs of long and highly erroneous sequences.</p>
      <p>For simulated reads, ELECTOR is compatible with state-of-the-art long reads simulation tools, such as NanoSim (<xref rid="B9" ref-type="bibr">9</xref>) or SimLoRD (<xref rid="B10" ref-type="bibr">10</xref>), on which introduced errors are precisely known. Moreover, it is meant to be a user-friendly tool, which delivers its results through different output formats, such as graphics that can be directly integrated into the users’ projects. This tool was designed to be directly compatible with a wide range of state-of-the-art error correction tools without requiring any pre-processing by the user. In particular, ELECTOR is compatible with the latest self-correction methods, and we thus present novel results on such tools, which were not tackled by LRCstats.</p>
    </sec>
  </sec>
  <sec sec-type="materials|methods" id="SEC2">
    <title>MATERIALS AND METHODS</title>
    <sec id="SEC2-1">
      <title>Input sequences</title>
      <p>ELECTOR is compatible with long reads simulators SimLoRD and NanoSim, and real read sequences (see Figure <xref ref-type="fig" rid="F1">1</xref> for an overview of ELECTOR). When using long reads simulated with one of these tools, the reference sequences are directly retrieved by ELECTOR, by parsing the files generated during the simulation. When using these state-of-the-art long reads simulation tools, we ensure to take as input sequences that closely simulate the actual characteristics of the long reads. However, it is possible to use other long reads simulation tools. In this case, the user must provide the ‘reference’ sequences itself. The genome used for the simulation, the files generated by the simulator and the corrected reads, output by the desired correction method, are then provided as an input. ELECTOR then compares three different versions of each read: the ‘uncorrected’ version, as provided by the sequencing experiment or by the read simulator, the ‘corrected’ version, as provided by the error correction method and the ‘reference’ version, which is a portion of the reference genome, representing a perfect version of the original read, without any error. For real data, the ‘reference’ sequences are retrieved by aligning the ‘uncorrected’ reads to the reference genome, using Minimap2 (<xref rid="B4" ref-type="bibr">4</xref>). Only the best hit for each read is kept and used to determine the corresponding ‘reference’ sequence. In the case a read cannot align to the reference genome, and thus cannot produce a ‘reference’ sequence, the read is excluded from the analysis.</p>
      <fig id="F1" orientation="portrait" position="float">
        <label>Figure 1.</label>
        <caption>
          <p>Overview of ELECTOR. Inputs are the sequences at different stages: without errors (from the reference genome), with errors (simulated or real uncorrected reads) and corrected (after running a correction method). We compute a multiple sequence alignment of the three versions of each sequence and analyze the results to provide correction quality measures. In order to provide additional information, reads are assembled using Minimap2 and Miniasm and both the reads and the contigs are aligned to the reference genome. A text summary, plots and a pdf summary are output.</p>
        </caption>
        <graphic xlink:href="lqz015fig1"/>
      </fig>
    </sec>
    <sec id="SEC2-2">
      <title>Scalable triplet multiple alignment</title>
      <p>With real or simulated reads, the core of the algorithmic novelty is to propose the comparison of the three different versions of each read (reference, uncorrected and corrected) in a triplet multiple alignment. These three versions of each read undergo a multiple sequence alignment, to collect their differences and similarities at each position of the alignment.</p>
      <sec id="SEC2-3-1">
        <title>Principle</title>
        <p>With the three versions of each read, our triplet multiple alignment strategy computes an MSA, using a partial order alignment algorithm. The MSA is initialized with the ‘reference’ sequence, and the ‘corrected’ and ‘uncorrected’ sequences are then sequentially added. This step yields a multiple alignment matrix that is output in pseudo FASTA (PIR) format for each triplet. The triplet multiple alignments are computed using an implementation of partial order alignment graphs (<xref rid="B11" ref-type="bibr">11</xref>). Partial order alignment graphs are used as structures containing the information of the multiple aligned sequences. To this extent, a directed acyclic graph (DAG) contains the previous multiple sequence alignment result. The vertices store consecutive nucleotides from the sequences. Each new sequence is aligned to this DAG in a generalization of the Needleman–Wunsch algorithm. Paths in the DAG represent the successive alignments.</p>
        <p>However, such a procedure can be time-consuming when applied to noisy long reads (see Table <xref rid="tbl2" ref-type="table">2</xref>). Thus, we propose a novel multiple sequence alignment heuristic. We recall the values of all the parameters mentioned in the following paragraphs in <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S1</xref>.</p>
      </sec>
      <sec id="SEC2-3-2">
        <title>Segmentation strategy for the MSA</title>
        <p>To reduce the computation time of our approach, we propose a segmentation strategy, as sketched in Figure <xref ref-type="fig" rid="F2">2</xref>. It consists of dividing the global multiple alignment into several smaller instances. Drawing inspiration from MUMmer’s (<xref rid="B12" ref-type="bibr">12</xref>) and Minimap’s (<xref rid="B5" ref-type="bibr">5</xref>) longest increasing subsequence approaches, we select a sequence of seeds <italic>S</italic><sub>1</sub>, …<italic>S</italic><sub><italic>N</italic></sub> that can be found (in the given order) within the three sequences. From this sequence of seeds, we extract the <italic>N</italic> + 1 substrings (<italic>W</italic><sub>0</sub>, <italic>W</italic><sub>1</sub>, …, <italic>W</italic><sub><italic>N</italic></sub>) delimited by the seeds in the three versions of the read. We thus extract <inline-formula><tex-math id="M4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$str[0:position\_S_1],str[position\_S_1+length\_S_1:position\_S_2],\ldots ,\:str[position\_S_i+length\_S_i:position\_S_{i+1}],\ldots ,\:str[position\_S_N:str\_size]$\end{document}</tex-math></inline-formula>, with <italic>str</italic> being the sequence of the ‘reference’, ‘corrected’ or ‘uncorrected’ version, and call these substrings ‘windows’. We, therefore, compute independent MSAs for each window triplet <inline-formula><tex-math id="M5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$(W_i\_reference,W_i\_corrected,W_i\_uncorrected)$\end{document}</tex-math></inline-formula>, and then reconstitute the global multiple alignment by concatenation. We now describe the procedure more in detail. For each triplet, we compute the <italic>k</italic>-mers that will be used as seeds (called ‘seed’ <italic>k</italic>-mers) so that they comply with the following properties:<list list-type="roman-lower"><list-item><p>They appear in each of the three versions of the sequence.</p></list-item><list-item><p>They are not repeated across any of the versions of the sequence.</p></list-item><list-item><p>They are not overlapping in any of the versions of the sequence.</p></list-item></list></p>
        <fig id="F2" orientation="portrait" position="float">
          <label>Figure 2.</label>
          <caption>
            <p>Segmentation strategy to compute a multiple sequence alignment for a triplet of ‘reference’, ‘uncorrected’ and ‘corrected’ versions of a read. Instead of computing an MSA on the whole length of the sequences, we rather divide this problem into smaller instances. As each version is different, to decide where to start and end the alignments, we look for seed <italic>k</italic>-mers (in black) that are exact local matches between the three sequences. We then compute individual, separate MSAs, for subsequences bordered by seeds (or located at the extremities of the sequences). These multiple MSAs are finally concatenated, along with the seed <italic>k</italic>-mers, to obtain a single, full MSA, of the whole length of the sequences.</p>
          </caption>
          <graphic xlink:href="lqz015fig2"/>
        </fig>
        <p>Using dynamic programming, the longest increasing subsequence of seed <italic>k</italic>-mers, <italic>S</italic><sub>1</sub>, …<italic>S</italic><sub><italic>N</italic></sub> is computed. Pairs of successive seed <italic>k</italic>-mers, <italic>S</italic><sub><italic>i</italic></sub>, <italic>S</italic><sub><italic>i</italic> + 1</sub> delineate windows. The size of these seed <italic>k</italic>-mers is adapted according to the current observed error rates (<xref rid="B5" ref-type="bibr">5</xref>,<xref rid="B13" ref-type="bibr">13</xref>), and ranges between 9 and 15 nucleotides. As it is difficult to <italic>a priori</italic> select a <italic>k</italic>-mer size, we designed a quick iterative strategy that tries several values of <italic>k</italic> to choose the most suitable for a given triplet. Starting from <italic>k</italic> = <italic>k</italic><sub>max</sub> (set to 15 by default), we keep on decreasing <italic>k</italic> until the size of the largest window no longer decreases. Whenever the largest window’s size no longer decreases, or <italic>k</italic><sub>min</sub> (set to 9 by default) is reached, the process stops. Minimizing the size of the largest window as such allows us to ensure that we compute MSAs on the smallest possible windows, in order to reduce the computational costs as much as possible.</p>
        <p>Once windows are computed, we produce MSAs of each window triplet <inline-formula><tex-math id="M6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$(W_i\_reference,W_i\_corrected,W_i\_uncorrected)$\end{document}</tex-math></inline-formula> independently, as described in the previous paragraph, using subsequently smaller alignment graphs. Finally, the multiple small MSAs are concatenated, along with the seed <italic>k</italic>-mers, to obtain a single MSA of the whole length of the read triplet.</p>
        <p>If we were able to bound the size of the windows, we could guarantee an asymptotic time linear to the read length for the alignment computation. In practice, our implementation can produce large windows, but we observe a running time almost linear in the length of the reads, as shown in our experimental results.</p>
        <p>To avoid computing metrics on poorly corrected reads, we filter out corrected reads whose length is below a given parameter (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S1</xref> for its default value) and triplets for which no seed <italic>k</italic>-mers can be found. These two types of filtered reads are tagged and reported apart in ELECTOR’s outputs to inform the user about their numbers.</p>
      </sec>
      <sec id="SEC2-3-3">
        <title>Handle reads of different sizes in the segmentation strategy</title>
        <p>In the case of a truncated corrected read (trimmed/split), the ‘corrected’ version is shortened in comparison to the two other versions. A part of the ‘reference’ and ‘uncorrected’ sequences is thus missing in the ‘corrected’ sequence. A prefix, a suffix or both can be missing depending on the case. Trimmed and split scenarios are outlined in Figure <xref ref-type="fig" rid="F4">4</xref>. As we only use anchors shared among the three sequences, in the case of a missing prefix in the corrected version, <inline-formula><tex-math id="M7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$W_0\_reference$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$W_0\_uncorrected$\end{document}</tex-math></inline-formula> will, therefore, be larger than <inline-formula><tex-math id="M9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$W_0\_corrected$\end{document}</tex-math></inline-formula> (see an example of a missing suffix in Figure <xref ref-type="fig" rid="F3">3</xref>). Computing an MSA between those three sequences would thus be irrelevant. Furthermore computing an MSA on two possibly long sequences (as a large sequence may be missing) is pricey. As corrected reads can be truncated at the beginning, at the end, or both, the symmetrical scenario can occur for suffixes.</p>
        <fig id="F3" orientation="portrait" position="float">
          <label>Figure 3.</label>
          <caption>
            <p>Segmentation strategy when the ‘corrected’ read is smaller. As in Figure 2, R,U,C stand for the reference, uncorrected and corrected read triplet. Here, the ‘corrected’ read is shortened on its right end. To avoid passing subsequences starting from seed 2 to the end of each sequence to the MSA, which would be costly to compute, we perform a second segmentation strategy. This strategy allows us to retrieve a new set of seeds (gray seeds 3 and 4). This new set of seeds divides the remaining subsequences (suffixes in this case) in ‘reference’ and ‘uncorrected’ into windows on which we compute MSA separately. The full MSA is reconstructed by concatenation, and dots are added to complete the ‘corrected’ MSA line.</p>
          </caption>
          <graphic xlink:href="lqz015fig3"/>
        </fig>
        <p>To cope with this problem, we detect such cases by checking the length of the first windows. If <inline-formula><tex-math id="M10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$W_0\_reference$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$W_0\_uncorrected$\end{document}</tex-math></inline-formula> are large (≥1000 nucleotide) and at least two times larger than <inline-formula><tex-math id="M12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$W_0\_corrected$\end{document}</tex-math></inline-formula>, we use a segmentation scheme only with <italic>k</italic>-mers from ‘reference’ and ‘uncorrected’, and only align their two prefixes.</p>
        <p>This way, we can efficiently compute an MSA when the corrected reads do not cover the whole genome region they originally come from, avoiding to run a MSA on large/unrelated sequences. The procedure is symmetrical for suffixes.</p>
        <p>This procedure is essential for correctors that output numerous split reads, which would induce extremely long run-time due to large sequence MSA computations described before.</p>
      </sec>
    </sec>
    <sec id="SEC2-4">
      <title>Inference of quality evaluation metrics from MSA</title>
      <sec id="SEC2-4-1">
        <title>Classification of corrected reads</title>
        <p>ELECTOR reports different categories of ‘corrected’ reads: regular reads, trimmed/split reads, extended reads, soft-clipped reads, bad quality reads and short reads. Figure <xref ref-type="fig" rid="F4">4</xref> shows how we deduce the trimmed, split and extended categories from the MSA result.</p>
        <fig id="F4" orientation="portrait" position="float">
          <label>Figure 4.</label>
          <caption>
            <p>Three scenarios of corrected read categories in MSA results. Trimmed reads have a ‘corrected’ version with a missing prefix and/or suffix (gray region). Split reads have been fragmented into several parts during the correction, and subsequences can be missing between consecutive fragments (gray regions). Extended corrected reads have a ‘corrected’ version with an additional prefix and/or suffix which is (are) not present in the two other versions (missing regions in gray). Soft-clipped reads have a ‘reference’ version with a missing prefix and/or suffix (gray region).</p>
          </caption>
          <graphic xlink:href="lqz015fig4"/>
        </fig>
        <p><bold>Regular</bold> reads are neither trimmed, split, extended nor soft-clipped.</p>
        <p><bold>Trimmed</bold> reads are reads that lack a part of their prefix, suffix or both (first scenario in Figure <xref ref-type="fig" rid="F4">4</xref>).</p>
        <p><bold>Split</bold> reads are reads composed of several fragments that come from a single original read, which could only be corrected on several distinct parts (second scenario in Figure <xref ref-type="fig" rid="F4">4</xref>). Split reads are aligned as trimmed reads are. However, in the case of split reads, we gather all fragments that come from a single initial read, in order to build a single MSA from the several, distinct MSAs induced by the different fragments. <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S1</xref> illustrates this process.</p>
        <p>We thus report how many reads were trimmed or split during the correction. Moreover, for each trimmed or split corrected read, we report the total uncorrected length of its associated ‘reference’ read (i.e. the length that is not covered by any fragment).</p>
        <p><bold>Extended</bold> reads are reads that have a prefix and/or a suffix that was not present in the ‘reference’ sequence (third scenario in Figure <xref ref-type="fig" rid="F4">4</xref>). These reads can be chimeras from the correction step, and can, for instance, come from chimeric connections between unrelated parts of the graph (<xref rid="B14" ref-type="bibr">14</xref>) or the assembly of unrelated short reads (<xref rid="B15" ref-type="bibr">15</xref>).</p>
        <p>However, they can also be reads that were over-corrected by a graph-based correction method, which kept on traversing the graph after reaching the ‘uncorrected’ reads’ extremities. We do not compute quality evaluation metrics on the extended regions, but we report the number of extended reads, as well as their mean extension size, with respect to the ‘reference’ reads.</p>
        <p>We define a split/trimmed/extended region as the prefix or suffix (or both) of the MSA in which no ‘corrected’ nucleotide appears (for split and trimmed), or no ‘uncorrected’ and ‘reference’ nucleotide appear (for extended). These regions are represented in gray in Figure <xref ref-type="fig" rid="F4">4</xref>.</p>
        <p><bold>Soft-clipped</bold> reads are reads for which the extremities were soft clipped during the alignment to the reference genome (last scenario in Figure <xref ref-type="fig" rid="F4">4</xref>). This category only arises in real data mode, as we only retrieve ‘reference’ reads by aligning the ‘uncorrected’ reads to the reference genome in this case. For such reads, we do not compute quality evaluation metrics on the soft clipped regions, as they could not be appropriately aligned to the reference genome, and were therefore not used to determine the ‘reference’ read.</p>
        <p><bold>Bad quality</bold> reads are low-quality reads that were removed before the MSA step to avoid computing metrics on poorly corrected reads. As mentioned previously, these are the reads for which no seed <italic>k</italic>-mers were found during the segmentation process. These reads are tagged and reported apart in ELECTOR’s output to inform the user about their number. We only report their number as no metric can be computed since they are not aligned.</p>
        <p><bold>Short reads</bold> are reads that are shorter than ℓ% of the ‘reference’ sequence length (ℓ being a parameter set to 10 by default). As for the bad quality reads, these reads are also removed before the MSA step, and only the number of such reads is reported.</p>
      </sec>
      <sec id="SEC2-4-2">
        <title>Recall, precision, error rate</title>
        <p>Once the MSA is computed, we have access to information about the differences and similarities in nucleotide content for each position of the three versions of a sequence. Insertions and deletions are represented by a ‘.’ in the deleted parts, and by the corresponding nucleotide (<monospace>A,C,T</monospace> or <monospace>G</monospace>) in the inserted parts. Let us denote, respectively, by <italic>nt</italic>(<italic>R</italic>, <italic>p</italic><sub><italic>i</italic></sub>), <italic>nt</italic>(<italic>C</italic>, <italic>p</italic><sub><italic>i</italic></sub>), <italic>nt</italic>(<italic>U</italic>, <italic>p</italic><sub><italic>i</italic></sub>) the characters of ‘reference’, ‘corrected’ and ‘uncorrected’ versions in {<italic>A</italic>, <italic>C</italic>, <italic>G</italic>, <italic>T</italic>, .}, at position <italic>p</italic><sub><italic>i</italic></sub> (0 ≤ <italic>i</italic> &lt; <italic>N</italic>), in an MSA of size <italic>N</italic>. Figure <xref ref-type="fig" rid="F5">5</xref> shows how recall and precision are computed. The set <inline-formula><tex-math id="M13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {P}$\end{document}</tex-math></inline-formula> of positions to correct is composed of positions <italic>p</italic><sub><italic>i</italic></sub> such as <italic>nt</italic>(<italic>R</italic>, <italic>p</italic><sub><italic>i</italic></sub>) ≠ <italic>nt</italic>(<italic>U</italic>, <italic>p</italic><sub><italic>i</italic></sub>). The set <inline-formula><tex-math id="M14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {E}$\end{document}</tex-math></inline-formula> of existing positions in the corrected version is defined by including any position <italic>p</italic><sub><italic>x</italic></sub> from the ‘corrected’ version that is not counted in a trimmed/split/extended/soft-clipped region. The processed positions set <inline-formula><tex-math id="M15">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {C}$\end{document}</tex-math></inline-formula> is defined as <inline-formula><tex-math id="M16">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {P}\cup \lbrace p_j / nt(C,p_j) \ne nt(R,p_j) \rbrace \cap \mathcal {E}$\end{document}</tex-math></inline-formula>. The correct positions set <inline-formula><tex-math id="M17">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {C}o$\end{document}</tex-math></inline-formula> is defined as <inline-formula><tex-math id="M18">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {C} \cap \lbrace p_j/ nt(C,p_j) = nt(R, p_j)\rbrace$\end{document}</tex-math></inline-formula>. The recall, precision and error rate are computed as follows:<disp-formula id="M1"><label>(1)</label><tex-math id="M19">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} {\rm Recall} = \dfrac{{\rm card}(\mathcal {C}\cap \mathcal {P})}{{\rm card}(\mathcal {P})} \end{equation*}$$\end{document}</tex-math></disp-formula><disp-formula id="M2"><label>(2)</label><tex-math id="M20">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} {\rm Precision} = \dfrac{{\rm card}(\mathcal {C}o\cap \mathcal {C})}{{\rm card}(\mathcal {C})} \end{equation*}$$\end{document}</tex-math></disp-formula><disp-formula id="M3"><label>(3)</label><tex-math id="M21">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*} {\rm Error}\, {\rm rate} = 1-\frac{{\rm card}(\mathcal {C}o)}{\sum \limits _{i=0}^{c-1}i} \end{equation*}$$\end{document}</tex-math></disp-formula>with <italic>c</italic> the length of the corrected read.</p>
        <fig id="F5" orientation="portrait" position="float">
          <label>Figure 5.</label>
          <caption>
            <p>Computation of recall and precision using triple base-wise comparison at each MSA’s position. <italic>nt</italic>(<italic>R</italic>) (respectively <italic>nt</italic>(<italic>U</italic>), <italic>nt</italic>(<italic>C</italic>)) represents the character in ‘reference’ (respectively ‘uncorrected’, ‘corrected’) line of the MSA at a given position.</p>
          </caption>
          <graphic xlink:href="lqz015fig5"/>
        </fig>
      </sec>
      <sec id="SEC2-4-3">
        <title>Additional metrics</title>
        <p>ELECTOR provides the number of trimmed or split corrected reads, and the mean missing size of these reads, as well as the number of extended reads, and the mean extension size of these reads. The size distribution of the sequences, before and after correction, is reported graphically.</p>
        <p>In the case of split reads, we report the length of each fragment in the distribution. The %GC of the ‘corrected’ and ‘reference’ reads is also output, as well as the total number of insertions, deletion and substitution, in the ‘uncorrected’ and ‘corrected’ reads. ONT reads are known to be more error-prone than PB reads in homopolymers. Thus, we propose metrics to examine these particular regions. We show the ratio of homopolymer sizes in the ‘corrected’ version over the ‘reference’ version. The closer it is to one, the better the corrector overcame possible systematic errors in ONT reads.</p>
        <p>More details on the computation of the insertions, deletions, substitutions counts, and on the ratio of homopolymer sizes are shown, respectively, in <xref ref-type="supplementary-material" rid="sup1">Supplementary Figures S2 and S3</xref>.</p>
      </sec>
      <sec id="SEC2-4-4">
        <title>Remapping of corrected reads</title>
        <p>In addition to all previously presented metric computations, we also take advantage of the presence of the reference genome to evaluate corrected reads quality. We perform remapping of the corrected reads to the reference genome using Minimap2. We report the number of corrected reads, the total number of bases, the average length of the reads, the percentage of aligned reads, the mean identity of the alignments, as well as the genome coverage, i.e. the percentage of bases of the reference genome to which at least a nucleotide aligned.</p>
      </sec>
      <sec id="SEC2-4-5">
        <title>Post-correction assembly metrics</title>
        <p>Again, in addition to metrics obtained thanks to our MSA strategy, we assess the correction quality through its consequences on the assembly quality of the corrected reads. We perform the assembly of the corrected reads using Miniasm (<xref rid="B5" ref-type="bibr">5</xref>), as we mainly seek to develop a pipeline providing fast results. We acknowledge that assemblers such as Smartdenovo (<ext-link ext-link-type="uri" xlink:href="https://github.com/ruanjue/smartdenovo">https://github.com/ruanjue/smartdenovo</ext-link>) or Canu (<xref rid="B16" ref-type="bibr">16</xref>) are more sensitive, but as they display much larger runtimes, Miniasm provides a satisfying compromise.</p>
        <p>As for the metrics of the assembly, we output the overall number of contigs, the number of contigs that could be aligned, the number of breakpoints of the aligned contigs, the NGA50 and NGA75 sizes of the aligned contigs, as well as the genome coverage. Using the assemblies that we provide, further analyses can be performed using dedicated software such as QUAST-LG (<xref rid="B17" ref-type="bibr">17</xref>).</p>
        <p>We also perform the alignment of the contigs with Minimap2. The computation of the different metrics, for remapping and assembly assessment, is then performed by parsing the generated SAM file.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="results" id="SEC3">
    <title>RESULTS</title>
    <sec id="SEC3-1">
      <title>Validation of the segmentation strategy for MSA</title>
      <p>To validate our segmentation strategy for MSA, we show to which extent its results differ from the classical MSA approach. In particular, we expect that recall, precision and error rate hardly differ, thus showing that both behaviors produce very similar results. Conversely, we expect a decisive gain in time with our segmentation strategy compared to the original algorithm. We thus compared multiple alignment results obtained with our strategy to results obtained with the regular implementation of partial order alignment graphs on multiple datasets of different read lengths, which affects the run-time of the alignments. Results are presented in Table <xref rid="tbl2" ref-type="table">2</xref>. We observe that while the two strategies provide very similar metrics, the segmentation strategy can reduce the runtime by orders of magnitude compared to the regular approach, especially when the reads grow longer.</p>
    </sec>
    <sec id="SEC3-2">
      <title>Validation on synthetic datasets</title>
      <p>In this section, we present the results of ELECTOR and LRCstats on several simulated datasets from different species. Further details about these datasets are given in Table <xref rid="tbl1" ref-type="table">1</xref>. The choice of synthetic data was motivated by the need to know the ‘reference’ sequences (which are portions of the reference genome, representing perfect versions of the original reads, on which no error would have been introduced) to precisely control the results brought by the assessed correction method.</p>
      <table-wrap id="tbl1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Description of the datasets used in our experiments</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Dataset</th>
              <th rowspan="1" colspan="1">
                <italic>E. coli</italic>
              </th>
              <th rowspan="1" colspan="1">
                <italic>S. cerevisiae</italic>
              </th>
              <th rowspan="1" colspan="1">
                <italic>C. elegans</italic>
              </th>
              <th rowspan="1" colspan="1">
                <italic>H. sapiens</italic>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td colspan="5" align="left" rowspan="1">
                <bold>Reference organism</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Strain</td>
              <td rowspan="1" colspan="1">K-12 substr. MG1655</td>
              <td rowspan="1" colspan="1">W303</td>
              <td rowspan="1" colspan="1">Bristol N2</td>
              <td rowspan="1" colspan="1">GRCh38</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Reference sequence</td>
              <td rowspan="1" colspan="1">NC_000913<sup>a</sup></td>
              <td rowspan="1" colspan="1">scf7180000000{084-13}<sup>b</sup></td>
              <td rowspan="1" colspan="1">GCA_000002985.3<sup>c</sup></td>
              <td rowspan="1" colspan="1">NC_000001.11<sup>d</sup></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Genome size</td>
              <td rowspan="1" colspan="1">4.6 Mbp</td>
              <td rowspan="1" colspan="1">12.2 Mbp</td>
              <td rowspan="1" colspan="1">100 Mbp</td>
              <td rowspan="1" colspan="1">249 Mbp</td>
            </tr>
            <tr>
              <td colspan="5" align="left" rowspan="1">
                <bold>Simulated Pacific Biosciences data</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Number of reads</td>
              <td rowspan="1" colspan="1">11 306</td>
              <td rowspan="1" colspan="1">30 132</td>
              <td rowspan="1" colspan="1">244 277</td>
              <td rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Average length (bases)</td>
              <td rowspan="1" colspan="1">8226</td>
              <td rowspan="1" colspan="1">8204</td>
              <td rowspan="1" colspan="1">8204</td>
              <td rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Number of bases (M bases)</td>
              <td rowspan="1" colspan="1">93</td>
              <td rowspan="1" colspan="1">247</td>
              <td rowspan="1" colspan="1">2004</td>
              <td rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Coverage</td>
              <td rowspan="1" colspan="1">20×</td>
              <td rowspan="1" colspan="1">20×</td>
              <td rowspan="1" colspan="1">20×</td>
              <td rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Error rate (%)</td>
              <td rowspan="1" colspan="1">18.6</td>
              <td rowspan="1" colspan="1">18.6</td>
              <td rowspan="1" colspan="1">18.6</td>
              <td rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td colspan="5" align="left" rowspan="1">
                <bold>Real Oxford Nanopore data</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Accession</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">PRJEB23027<sup>e</sup></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Number of reads</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">1 075 867</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Average length (bases)</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">6744</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Number of bases (M bases)</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">7256</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Coverage</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">29×</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Error rate (%)</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">17.60</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="T1TFN1">
            <p><sup>a</sup><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/nuccore/NC_000913">https://www.ncbi.nlm.nih.gov/nuccore/NC_000913</ext-link>.</p>
          </fn>
          <fn id="T1TFN2">
            <p><sup>b</sup><ext-link ext-link-type="uri" xlink:href="http://www.genoscope.cns.fr/externe/nas/references/yeast/W303_pacbio_assembly.fa.gz">www.genoscope.cns.fr/externe/nas/references/yeast/W303_pacbio_assembly.fa.gz</ext-link>.</p>
          </fn>
          <fn id="T1TFN3">
            <p><sup>c</sup><ext-link ext-link-type="uri" xlink:href="https://www.ebi.ac.uk/ena/data/view/GCA_000002985.3">https://www.ebi.ac.uk/ena/data/view/GCA_000002985.3</ext-link>.</p>
          </fn>
          <fn id="T1TFN4">
            <p><sup>d</sup>Only chromosome 1 was used. <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/nuccore/NC_000001.11">https://www.ncbi.nlm.nih.gov/nuccore/NC_000001.11</ext-link>.</p>
          </fn>
          <fn id="T1TFN5">
            <p><sup>e</sup>Only reads from chromosome 1 were used.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap id="tbl2" orientation="portrait" position="float">
        <label>Table 2.</label>
        <caption>
          <p>Comparison of the two multiple alignment strategies on simulated <italic>E. coli</italic> datasets</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Experiment</th>
              <th rowspan="1" colspan="1">Recall (%)</th>
              <th rowspan="1" colspan="1">Precision (%)</th>
              <th rowspan="1" colspan="1">Error rate (%)</th>
              <th rowspan="1" colspan="1">Time</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">‘1k’ MSA</td>
              <td rowspan="1" colspan="1">99.712</td>
              <td rowspan="1" colspan="1">98.996</td>
              <td rowspan="1" colspan="1">1.02</td>
              <td rowspan="1" colspan="1">2 h 05 min</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">‘1k” segmentation +MSA</td>
              <td rowspan="1" colspan="1">99.769</td>
              <td rowspan="1" colspan="1">98.992</td>
              <td rowspan="1" colspan="1">1.021</td>
              <td rowspan="1" colspan="1">
                <bold>28 min</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">‘10k’ MSA</td>
              <td rowspan="1" colspan="1">99.921</td>
              <td rowspan="1" colspan="1">99.781</td>
              <td rowspan="1" colspan="1">0.206</td>
              <td rowspan="1" colspan="1">20 h 50 min</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">‘10k’ segmentation + MSA</td>
              <td rowspan="1" colspan="1">99.921</td>
              <td rowspan="1" colspan="1">99.795</td>
              <td rowspan="1" colspan="1">0.207</td>
              <td rowspan="1" colspan="1">
                <bold>29 min</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">‘100k’ MSA</td>
              <td rowspan="1" colspan="1">99.913</td>
              <td rowspan="1" colspan="1">99.925</td>
              <td rowspan="1" colspan="1">0.044</td>
              <td rowspan="1" colspan="1">8 days 18 h 38 min</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">‘100k’ segmentation +MSA</td>
              <td rowspan="1" colspan="1">99.924</td>
              <td rowspan="1" colspan="1">99.903</td>
              <td rowspan="1" colspan="1">0.098</td>
              <td rowspan="1" colspan="1">
                <bold>1 h 11 min</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="T2TFN1">
            <p>Three datasets were simulated, with a 10% error rate, a coverage of 100× and a fixed read length of 1 k bases, 10 k bases and 100 k bases, respectively. The reads were corrected using Canu with default parameters.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <sec id="SEC3-2-1">
        <title>ELECTOR sample output</title>
        <p>As previously mentioned, ELECTOR computes general metrics: recall, precision, error rate, among other metrics, and provides a graphic representation of their distributions.</p>
        <p>A subset of the metrics produced by ELECTOR using reads corrected by the following tools: HALC (<xref rid="B18" ref-type="bibr">18</xref>), HG-CoLoR (<xref rid="B19" ref-type="bibr">19</xref>), LoRDEC (<xref rid="B20" ref-type="bibr">20</xref>), Canu (<xref rid="B16" ref-type="bibr">16</xref>), Daccord (Tischler, G., &amp; Myers, E. W. (2017). Non-hybrid long read consensus using local de Bruijn graph assembly. bioRxiv, 106252.) and MECAT (<xref rid="B21" ref-type="bibr">21</xref>) is presented in Table <xref rid="tbl3" ref-type="table">3</xref>. These metrics are consistent with the results presented in the respective tools’ publications. The whole set of metrics, including remapping and assembly assessment, are presented in <xref ref-type="supplementary-material" rid="sup1">Supplementary Tables S3 and S4</xref>.</p>
        <table-wrap id="tbl3" orientation="portrait" position="float">
          <label>Table 3.</label>
          <caption>
            <p>Examples of the main metrics reported by ELECTOR on <italic>E. coli, S. Cerevisiae</italic> and <italic>C. elegans</italic> datasets</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th colspan="2" align="center" rowspan="1">HALC</th>
                <th colspan="2" align="center" rowspan="1">HG-CoLoR</th>
                <th colspan="2" align="center" rowspan="1">LoRDEC</th>
                <th colspan="2" align="center" rowspan="1">CANU</th>
                <th colspan="2" align="center" rowspan="1">Daccord</th>
                <th colspan="2" align="center" rowspan="1">MECAT</th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1">Metric</th>
                <th rowspan="1" colspan="1">Uncorrected</th>
                <th rowspan="1" colspan="1">Corrected</th>
                <th rowspan="1" colspan="1">Uncorrected</th>
                <th rowspan="1" colspan="1">Corrected</th>
                <th rowspan="1" colspan="1">Uncorrected</th>
                <th rowspan="1" colspan="1">Corrected</th>
                <th rowspan="1" colspan="1">Uncorrected</th>
                <th rowspan="1" colspan="1">Corrected</th>
                <th rowspan="1" colspan="1">Uncorrected</th>
                <th rowspan="1" colspan="1">Corrected</th>
                <th rowspan="1" colspan="1">Uncorrected</th>
                <th rowspan="1" colspan="1">Corrected</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td colspan="12" align="left" rowspan="1">
                  <italic>E. coli</italic>
                </td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Processed bases</td>
                <td rowspan="1" colspan="1">91 950 978</td>
                <td rowspan="1" colspan="1">81 199 351</td>
                <td rowspan="1" colspan="1">93 003 632</td>
                <td rowspan="1" colspan="1">84 089 814</td>
                <td rowspan="1" colspan="1">89 077 682</td>
                <td rowspan="1" colspan="1">77 969 503</td>
                <td rowspan="1" colspan="1">91 933 413</td>
                <td rowspan="1" colspan="1">86 443 218</td>
                <td rowspan="1" colspan="1">92 936 636</td>
                <td rowspan="1" colspan="1">83 773 362</td>
                <td rowspan="1" colspan="1">80 380 557</td>
                <td rowspan="1" colspan="1">58 979 203</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Error rate</td>
                <td rowspan="1" colspan="1">0.1415</td>
                <td rowspan="1" colspan="1">0.0015</td>
                <td rowspan="1" colspan="1">0.1428</td>
                <td rowspan="1" colspan="1">0.0007</td>
                <td rowspan="1" colspan="1">0.1384</td>
                <td rowspan="1" colspan="1">0.0015</td>
                <td rowspan="1" colspan="1">0.1432</td>
                <td rowspan="1" colspan="1">0.0524</td>
                <td rowspan="1" colspan="1">0.1433</td>
                <td rowspan="1" colspan="1">0.004</td>
                <td rowspan="1" colspan="1">0.1332</td>
                <td rowspan="1" colspan="1">0.0052</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Recall (%)</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9999</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">1.0</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9999</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9495</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9988</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9983</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Precision (%)</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9985</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9993</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9986</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9476</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9961</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9949</td>
              </tr>
              <tr>
                <td colspan="12" align="left" rowspan="1">
                  <italic>S. cerevisiae</italic>
                </td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Processed bases</td>
                <td rowspan="1" colspan="1">238 309 333</td>
                <td rowspan="1" colspan="1">212 266 193</td>
                <td rowspan="1" colspan="1">245 700 616</td>
                <td rowspan="1" colspan="1">219 744 436</td>
                <td rowspan="1" colspan="1">196 676 910</td>
                <td rowspan="1" colspan="1">188 228 237</td>
                <td rowspan="1" colspan="1">244 560 743</td>
                <td rowspan="1" colspan="1">229 555 492</td>
                <td rowspan="1" colspan="1">246 455 883</td>
                <td rowspan="1" colspan="1">222 050 951</td>
                <td rowspan="1" colspan="1">217 284 712</td>
                <td rowspan="1" colspan="1">162 057 920</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Error rate</td>
                <td rowspan="1" colspan="1">0.1403</td>
                <td rowspan="1" colspan="1">0.0042</td>
                <td rowspan="1" colspan="1">0.1414</td>
                <td rowspan="1" colspan="1">0.003</td>
                <td rowspan="1" colspan="1">0.1325</td>
                <td rowspan="1" colspan="1">0.0054</td>
                <td rowspan="1" colspan="1">0.1425</td>
                <td rowspan="1" colspan="1">0.0506</td>
                <td rowspan="1" colspan="1">0.1426</td>
                <td rowspan="1" colspan="1">0.0054</td>
                <td rowspan="1" colspan="1">0.1339</td>
                <td rowspan="1" colspan="1">0.0066</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Recall (%)</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9997</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9999</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9995</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9515</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9986</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.998</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Precision (%)</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9959</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9971</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9947</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9495</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9946</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9936</td>
              </tr>
              <tr>
                <td colspan="12" align="left" rowspan="1">
                  <italic>C.elegans</italic>
                </td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Processed bases</td>
                <td rowspan="1" colspan="1">1 731 103 921</td>
                <td rowspan="1" colspan="1">1 588 220 052</td>
                <td rowspan="1" colspan="1">1 988 381 391</td>
                <td rowspan="1" colspan="1">1 726 223 265</td>
                <td rowspan="1" colspan="1">1 299 187 175</td>
                <td rowspan="1" colspan="1">1 154 508 245</td>
                <td rowspan="1" colspan="1">1 997 798 872</td>
                <td rowspan="1" colspan="1">1 873 188 109</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">1 270 739 795</td>
                <td rowspan="1" colspan="1">870 965 775</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Error rate</td>
                <td rowspan="1" colspan="1">0.1377</td>
                <td rowspan="1" colspan="1">0.0153</td>
                <td rowspan="1" colspan="1">0.1397</td>
                <td rowspan="1" colspan="1">0.0065</td>
                <td rowspan="1" colspan="1">0.1242</td>
                <td rowspan="1" colspan="1">0.0126</td>
                <td rowspan="1" colspan="1">0.1427</td>
                <td rowspan="1" colspan="1">0.0496</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.1199</td>
                <td rowspan="1" colspan="1">0.0065</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Recall (%)</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9989</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9997</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9989</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9527</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9982</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Precision (%)</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.985</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9936</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9875</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9505</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9936</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="T3TFN1">
              <p>A dash in the uncorrected columns indicates that the metric is not computed for the ‘uncorrected’ reads. Daccord could not be run on the <italic>C. elegans</italic> dataset, and reported an error.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec id="SEC3-2-2">
        <title>Comparison to state-of-the-art</title>
        <p>Recently, several benchmark analysis were proposed for long reads (comparison of hybrid correction methods (<xref rid="B22" ref-type="bibr">22</xref>), comparison of hybrid and self-correction methods (Zhang, H. <italic>et al.</italic> (2019). A comprehensive evaluation of long read error correction methods. (BioRxiv, 519330.), analysis of long read correction on transcriptomic reads (<xref rid="B23" ref-type="bibr">23</xref>)). In this work, we focus on the methodological basis allowing to efficiently perform and reproduce such benchmarks, rather than highlighting the pros and cons of available correction methods. The presented correction performances are thus showed for validation purposes and are not intended to be a benchmark of existing correction methods. In the rest of the result section, we report comparisons to the only other automated evaluation tool for long reads correction: LRCstats.</p>
        <p>In Table <xref rid="tbl4" ref-type="table">4</xref>, we compare the metrics displayed by ELECTOR and LRCstats. Corrections of the <italic>S. cerevisiae</italic> dataset by HALC (a hybrid correction method) and Canu (self-correction method) are evaluated and reported as an example output. The complete results provided by LCRstats and ELECTOR, for each correction tool, and on each dataset, are presented in <xref ref-type="supplementary-material" rid="sup1">Supplementary Tables S2 and S3</xref>.</p>
        <table-wrap id="tbl4" orientation="portrait" position="float">
          <label>Table 4.</label>
          <caption>
            <p>Comparison of ELECTOR’s and LRCstats’s outputs</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th colspan="2" align="center" rowspan="1">Uncorrected</th>
                <th colspan="2" align="center" rowspan="1">Corrected</th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1">Metric</th>
                <th rowspan="1" colspan="1">ELECTOR</th>
                <th rowspan="1" colspan="1">LRCstats</th>
                <th rowspan="1" colspan="1">ELECTOR</th>
                <th rowspan="1" colspan="1">LRCstats</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td colspan="2" align="left" rowspan="1">
                  <bold>Corrected by HALC</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Processed bases</td>
                <td rowspan="1" colspan="1">238 309 333</td>
                <td rowspan="1" colspan="1">237 655 341</td>
                <td rowspan="1" colspan="1">212 266 193</td>
                <td rowspan="1" colspan="1">214 152 119</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Error rate</td>
                <td rowspan="1" colspan="1">0.1403</td>
                <td rowspan="1" colspan="1">0.1751</td>
                <td rowspan="1" colspan="1">0.0042</td>
                <td rowspan="1" colspan="1">0.0023</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Insertions</td>
                <td rowspan="1" colspan="1">28 772 841</td>
                <td rowspan="1" colspan="1">32 589 970</td>
                <td rowspan="1" colspan="1">100 874</td>
                <td rowspan="1" colspan="1">215 507</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Deletions</td>
                <td rowspan="1" colspan="1">5 235 890</td>
                <td rowspan="1" colspan="1">8 991 984</td>
                <td rowspan="1" colspan="1">1 035 978</td>
                <td rowspan="1" colspan="1">120 743</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Substitutions</td>
                <td rowspan="1" colspan="1">4 058 953</td>
                <td rowspan="1" colspan="1">1 633 123</td>
                <td rowspan="1" colspan="1">198 853</td>
                <td rowspan="1" colspan="1">221 646</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Recall (%)</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9997</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Precision (%)</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9959</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Trimmed/split</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">12.043</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Mean missing size</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">577.5</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Extended</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">71</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Mean extension size</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">53.2</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Low quality reads</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">160</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Small reads</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">3436</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td colspan="2" align="center" rowspan="1"/>
                <td colspan="2" align="left" rowspan="1">
                  <bold>Corrected by Canu</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ELECTOR</td>
                <td rowspan="1" colspan="1">LRCstats</td>
                <td rowspan="1" colspan="1">ELECTOR</td>
                <td rowspan="1" colspan="1">LRCstats</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Processed bases</td>
                <td rowspan="1" colspan="1">244 560 743</td>
                <td rowspan="1" colspan="1">244 633 066</td>
                <td rowspan="1" colspan="1">229 555 492</td>
                <td rowspan="1" colspan="1">229 825 812</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Error rate</td>
                <td rowspan="1" colspan="1">0.1425</td>
                <td rowspan="1" colspan="1">0.1781</td>
                <td rowspan="1" colspan="1">0.0506</td>
                <td rowspan="1" colspan="1">0.0694</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Insertions</td>
                <td rowspan="1" colspan="1">30 090 583</td>
                <td rowspan="1" colspan="1">34 105 075</td>
                <td rowspan="1" colspan="1">12 252 413</td>
                <td rowspan="1" colspan="1">12 942 568</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Deletions</td>
                <td rowspan="1" colspan="1">5 483 119</td>
                <td rowspan="1" colspan="1">9 489 618</td>
                <td rowspan="1" colspan="1">2 574 320</td>
                <td rowspan="1" colspan="1">3 134 365</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Substitutions</td>
                <td rowspan="1" colspan="1">4 375 017</td>
                <td rowspan="1" colspan="1">1 748 302</td>
                <td rowspan="1" colspan="1">2 197 172</td>
                <td rowspan="1" colspan="1">1 591 650</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Recall (%)</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9515</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Precision (%)</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.9495</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Trimmed/split</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">2.216</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Mean missing size</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">35.1</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Extended</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">178</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Mean extension size</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">30.7</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Low quality reads</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">43.0</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Small reads</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">–</td>
                <td rowspan="1" colspan="1">0.0</td>
                <td rowspan="1" colspan="1">✗</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="T4TFN1">
              <p>Both tools were evaluated on the <italic>S. cerevisiae</italic> dataset, using a hybrid corrector (HALC) and a self corrector (Canu). A dash in the Uncorrected columns indicates that the metric is not computed for the ‘uncorrected’ reads. A cross indicates that LRCstats does not provide the metric.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>Both LRCstats and ELECTOR compute metrics on ‘corrected’ reads and the corresponding ‘uncorrected’ sequences of those reads (reported respectively as ‘corrected’ and ‘uncorrected’).</p>
        <p>The first result to notice in Table <xref rid="tbl4" ref-type="table">4</xref> is that the error rates and the amount of processed bases announced in the ‘uncorrected’ reads can differ from one correction method to the other, both for ELECTOR and LRCstats. Such differences can be explained by the fact that HALC and Canu do not correct the same set of reads, which leads to different set of ‘uncorrected’ reads to evaluate.</p>
        <p>As ELECTOR and LRCstats rely on different rules to exclude reads from the analysis, and do not align split reads in the same way, we observe that they do not process the same quantity of reads.</p>
        <p>LRCstats concatenates the different parts of a split read before aligning the concatenation, even if a missing region can exist between two consecutive fragments. This behavior can complicate the alignment task and introduce a bias in the output metrics. On the contrary, ELECTOR processes the different fragments separately before reconstituting the whole alignment and thus takes into account missing regions. These differences thus have an impact on the metrics displayed for corrected reads. ELECTOR processes slightly more bases than LRCstats on the two studied datasets. However, reads falling into particular categories (very short reads and low-quality reads) are not taken into account in ELECTOR’s redcounts, and are reported apart, while they are absent from LRCstats’s output.</p>
        <p>Different alignment strategies in both tools also have impacts on the results, which explains the differences seen in indels and substitutions counts. However, ELECTOR and LRCstats globally report the same trends of two successful corrections that decreased the error rates.</p>
        <p>Additional metrics, specific to ELECTOR, point out noteworthy differences between the two correction methods, such as the high quantity of trimmed or split reads when using HALC in comparison to Canu. These metrics are essential for further steps such as assembly since less advantage is taken from shortened reads to resolve repeats. They also help to understand more in-depth the correctors’ behavior. In this example, Canu corrects with lower recall and precision than HALC, but this is nuanced because ELECTOR reports it produces less trimmed/split reads.</p>
      </sec>
    </sec>
    <sec id="SEC3-3">
      <title>Performance comparison</title>
      <p>In this section, we compare LRCstats and ELECTOR runtime and memory consumption on several datasets chosen to represent different use cases. Results are presented in Tables <xref rid="tbl5" ref-type="table">5</xref> and <xref rid="tbl6" ref-type="table">6</xref>. For the experiments presented in Table <xref rid="tbl5" ref-type="table">5</xref>, both tools were ran on a 20-core cluster node equipped with 250 GB of RAM. For experiments presented in Table <xref rid="tbl6" ref-type="table">6</xref>, we used a 16-core computer equipped with 64 GB of RAM. In order to compare similar operations, ELECTOR’s runtime and memory consumption do not consider the remapping and assembly steps. We present the metrics and resource consumption of this module apart, in <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S4</xref>.</p>
      <table-wrap id="tbl5" orientation="portrait" position="float">
        <label>Table 5.</label>
        <caption>
          <p>Evolution of ELECTOR and LRCstats runtime and memory consumption according to the read length.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Tool</th>
              <th rowspan="1" colspan="1">Read length (bases)</th>
              <th rowspan="1" colspan="1">Memory (MB)</th>
              <th rowspan="1" colspan="1">Elapsed time</th>
              <th rowspan="1" colspan="1">CPU time</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">LRCstats</td>
              <td rowspan="1" colspan="1">1 k</td>
              <td rowspan="1" colspan="1">1803</td>
              <td rowspan="1" colspan="1">42 min</td>
              <td rowspan="1" colspan="1">8 h 18 min</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ELECTOR</td>
              <td rowspan="1" colspan="1">1 k</td>
              <td rowspan="1" colspan="1">1030</td>
              <td rowspan="1" colspan="1">12 min</td>
              <td rowspan="1" colspan="1">28 min</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LRCstats</td>
              <td rowspan="1" colspan="1">10 k</td>
              <td rowspan="1" colspan="1">13 484</td>
              <td rowspan="1" colspan="1">4 h 51 min</td>
              <td rowspan="1" colspan="1">70 h 38 min</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ELECTOR</td>
              <td rowspan="1" colspan="1">10 k</td>
              <td rowspan="1" colspan="1">3091</td>
              <td rowspan="1" colspan="1">13 min</td>
              <td rowspan="1" colspan="1">29 min</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LRCstats</td>
              <td rowspan="1" colspan="1">100 k</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ELECTOR</td>
              <td rowspan="1" colspan="1">100 k</td>
              <td rowspan="1" colspan="1">12 231</td>
              <td rowspan="1" colspan="1">28 min</td>
              <td rowspan="1" colspan="1">1 h 11 min</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LRCstats</td>
              <td rowspan="1" colspan="1">1 M</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ELECTOR</td>
              <td rowspan="1" colspan="1">1 M</td>
              <td rowspan="1" colspan="1">24 881</td>
              <td rowspan="1" colspan="1">2 h 44 min</td>
              <td rowspan="1" colspan="1">11 h 05 min</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="T5TFN1">
            <p>The datasets were simulated from the <italic>E. coli</italic> genome, with a 100× coverage, a 10% error rate and fixed read length of 1 k bases, 10 k bases, 100 k bases and 1 M bases. The reads were corrected by Canu, using default parameters.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <table-wrap id="tbl6" orientation="portrait" position="float">
        <label>Table 6.</label>
        <caption>
          <p>Runtimes of ELECTOR and LRCstats on different datasets and different correction tools</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">HALC</th>
              <th rowspan="1" colspan="1">HG-CoLoR</th>
              <th rowspan="1" colspan="1">LoRDEC</th>
              <th rowspan="1" colspan="1">Canu</th>
              <th rowspan="1" colspan="1">Daccord</th>
              <th rowspan="1" colspan="1">MECAT</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>E. coli</italic>
              </td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Corrector</td>
              <td rowspan="1" colspan="1">24 min</td>
              <td rowspan="1" colspan="1">45 min</td>
              <td rowspan="1" colspan="1">8 min</td>
              <td rowspan="1" colspan="1">12 min</td>
              <td rowspan="1" colspan="1">27 min</td>
              <td rowspan="1" colspan="1">52 s</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LRCstats</td>
              <td rowspan="1" colspan="1">4h 58 min</td>
              <td rowspan="1" colspan="1">5 h 02 min</td>
              <td rowspan="1" colspan="1">4 h 37 min</td>
              <td rowspan="1" colspan="1">4 h 05 min</td>
              <td rowspan="1" colspan="1">4 h 20 min</td>
              <td rowspan="1" colspan="1">2 h 30 min</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ELECTOR</td>
              <td rowspan="1" colspan="1">
                <bold>28 min</bold>
              </td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>13 min</bold>
                </underline>
              </td>
              <td rowspan="1" colspan="1">
                <bold>1 h 17 min</bold>
              </td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>11 min</bold>
                </underline>
              </td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>12 min</bold>
                </underline>
              </td>
              <td rowspan="1" colspan="1">
                <bold>11 min</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>S. cerevisiae</italic>
              </td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Corrector</td>
              <td rowspan="1" colspan="1">1 h 19 min</td>
              <td rowspan="1" colspan="1">4 h 32 s</td>
              <td rowspan="1" colspan="1">28 min</td>
              <td rowspan="1" colspan="1">31 min</td>
              <td rowspan="1" colspan="1">1 h 15 min</td>
              <td rowspan="1" colspan="1">2 min</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LRCstats</td>
              <td rowspan="1" colspan="1">10 h 56 min</td>
              <td rowspan="1" colspan="1">12 h 26 min</td>
              <td rowspan="1" colspan="1">12 h 14 min</td>
              <td rowspan="1" colspan="1">10 h 46 min</td>
              <td rowspan="1" colspan="1">12 h 04 min</td>
              <td rowspan="1" colspan="1">6 h 59 min</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ELECTOR</td>
              <td rowspan="1" colspan="1">
                <bold>1 h 55 min</bold>
              </td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>1 h 07 min</bold>
                </underline>
              </td>
              <td rowspan="1" colspan="1">
                <bold>4 h 59 min</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>32 min</bold>
              </td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>44 min</bold>
                </underline>
              </td>
              <td rowspan="1" colspan="1">
                <bold>32 min</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>C. elegans</italic>
              </td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Corrector</td>
              <td rowspan="1" colspan="1">5 h 59 min</td>
              <td rowspan="1" colspan="1">88 h 56 min</td>
              <td rowspan="1" colspan="1">6 h 01 min</td>
              <td rowspan="1" colspan="1">4 h 33 min</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">22 min</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LRCstats</td>
              <td rowspan="1" colspan="1">83 h 29 min</td>
              <td rowspan="1" colspan="1">81 h 05 min</td>
              <td rowspan="1" colspan="1">70 h</td>
              <td rowspan="1" colspan="1">85 h 08 min</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">ELECTOR</td>
              <td rowspan="1" colspan="1">
                <bold>32 h 35 min</bold>
              </td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>10 h 30 min</bold>
                </underline>
              </td>
              <td rowspan="1" colspan="1">
                <bold>29 h 48 min</bold>
              </td>
              <td rowspan="1" colspan="1">
                <underline>
                  <bold>4 h 19 min</bold>
                </underline>
              </td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">
                <bold>3 h 12 min</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="T6TFN1">
            <p>Both ELECTOR and LRCstats were launched with 9 threads. The different correction methods were launched with 16 threads. The runtimes of the correctors are also included as a matter of comparison. The fastest evaluation method is shown in bold for each case. When the evaluation method is also quicker than the correction method itself, it is underlined. Daccord could not be run on the <italic>C. elegans</italic> dataset, and reported an error. LRCstats crashed while assessing the <italic>C. elegans</italic> dataset corrected by MECAT.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>We first assess, in Table <xref rid="tbl5" ref-type="table">5</xref>, the performances of both tools on several simulated <italic>E. coli</italic> datasets with different read lengths, ranging from 1 k bases to 1 M bases. As expected, the runtime and memory consumption of both tools grow with the read length. However, ELECTOR can handle reads &gt;10 k bases better than LRCstats, thanks to its segmentation strategy. In particular, ELECTOR is several orders of magnitude faster than LRCstats on the 10 k bases experiment, and can also handle longer reads, up to 1 M bases, using moderate resources. LRCstats was much more memory consuming and was thus unable to run on reads longer than 10 k bases, despite having access to 250 GB of RAM. These results underline that ELECTOR can scale to extremely long reads. Considering the ever-growing length of the long reads and the tremendous impact of such very long sequences, we believe that this ability is a significant advantage of ELECTOR obtained thanks to its segmentation technique.</p>
      <p>We also observe, in <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S5</xref>, that the error rate of the input reads has a negligible impact on the performances of the tools.</p>
      <p>In Table <xref rid="tbl6" ref-type="table">6</xref>, we compare the performances of different correctors with the time needed to evaluate their outputs, using ELECTOR and LRCstats. Interestingly, we observe that LRCstats is mostly slower than the correction step itself, which is not desirable. ELECTOR is often faster than or comparable to the corrector itself, except for MECAT that is distinctly efficient. These reduced runtimes could be a beneficial gain for benchmark analysis, and could also be critical for the development of new correction methods. Another observation from Table <xref rid="tbl6" ref-type="table">6</xref> is that we can notice large divergence in ELECTOR runtimes on the same dataset corrected by different tools. This behavior can be due to two factors. On the one hand, ELECTOR’s runtime optimization is prone to be more or less pronounced according to the read length (segmentation is expected to be easier with larger reads) and quality of the correction (more errors make it more difficult to find common seeds). On the other, ELECTOR’s runtime is also related to the number of split corrected reads output by the corrector. Indeed, a larger number of split reads imply a more significant number of triplet multiple alignments, and thus an increased runtime. In particular, in the experiments presented here, the largest runtimes can be observed for the evaluation of LoRDEC and HALC on the <italic>C. elegans</italic> dataset. As shown in <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S3</xref>, these tools are also those that produced the most considerable amount of trimmed/split reads on this dataset. A way to accelerate ELECTOR analysis would be to adapt its parameters to avoid small read fragments.</p>
    </sec>
    <sec id="SEC3-4">
      <title>Simulations for the validation of ELECTOR’s real data mode</title>
      <p>In order to validate ELECTOR’s real data mode, we ran the following experiment. We used a simulated dataset, and we assessed its correction using the two different modes of ELECTOR: simulated and real data. First, we ran it classically, by providing the simulation files as an input so that ELECTOR could retrieve the actual ‘reference’ reads by parsing the files. Second, we ran it by only providing the FASTA file of simulated reads as an input, so ELECTOR had to retrieve the ‘reference’ reads by aligning the uncorrected long reads to the reference genome, as if they were not simulated. We ran this experiment on the <italic>S. cerevisiae</italic> dataset. To further validate ELECTOR’s behavior on real data, we assessed the correction of both a hybrid corrector, HALC and a self-corrector, Canu. Results of these experiments are shown in Table <xref rid="tbl7" ref-type="table">7</xref>.</p>
      <table-wrap id="tbl7" orientation="portrait" position="float">
        <label>Table 7.</label>
        <caption>
          <p>Comparison of the results output by ELECTOR, using simulated and real data modes</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th colspan="2" align="center" rowspan="1">Uncorrected</th>
              <th colspan="2" align="center" rowspan="1">Corrected</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Metric</th>
              <th rowspan="1" colspan="1">Simulated</th>
              <th rowspan="1" colspan="1">Real</th>
              <th rowspan="1" colspan="1">Simulated</th>
              <th rowspan="1" colspan="1">Real</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td colspan="2" align="left" rowspan="1">
                <bold>Corrected by Halc</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Processed bases</td>
              <td rowspan="1" colspan="1">238 309 333</td>
              <td rowspan="1" colspan="1">238 119 170</td>
              <td rowspan="1" colspan="1">212 266 193</td>
              <td rowspan="1" colspan="1">212 141 319</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Error rate</td>
              <td rowspan="1" colspan="1">0.1403</td>
              <td rowspan="1" colspan="1">0.1449</td>
              <td rowspan="1" colspan="1">0.0042</td>
              <td rowspan="1" colspan="1">0.0104</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Recall (%)</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">0.9997</td>
              <td rowspan="1" colspan="1">0.9938</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Precision (%)</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">0.9959</td>
              <td rowspan="1" colspan="1">0.9897</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Insertions</td>
              <td rowspan="1" colspan="1">28 772 841</td>
              <td rowspan="1" colspan="1">26 796 500</td>
              <td rowspan="1" colspan="1">100 874</td>
              <td rowspan="1" colspan="1">90 737</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Deletions</td>
              <td rowspan="1" colspan="1">5 235 890</td>
              <td rowspan="1" colspan="1">5 042 365</td>
              <td rowspan="1" colspan="1">1 035 978</td>
              <td rowspan="1" colspan="1">1 490 680</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Substitutions</td>
              <td rowspan="1" colspan="1">4 058 953</td>
              <td rowspan="1" colspan="1">3 682 863</td>
              <td rowspan="1" colspan="1">198 853</td>
              <td rowspan="1" colspan="1">182 590</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Trimmed/split</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">12 043</td>
              <td rowspan="1" colspan="1">13 320</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Mean missing size</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">577.5</td>
              <td rowspan="1" colspan="1">896.0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Extended</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">71.0</td>
              <td rowspan="1" colspan="1">39.0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Mean extension size</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">53.2</td>
              <td rowspan="1" colspan="1">72.0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Low quality reads</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">160.0</td>
              <td rowspan="1" colspan="1">152.0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Small reads</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">3436.0</td>
              <td rowspan="1" colspan="1">3438.0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td colspan="2" align="center" rowspan="1"/>
              <td colspan="2" align="left" rowspan="1">
                <bold>Corrected by Canu</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Processed bases</td>
              <td rowspan="1" colspan="1">244 560 743</td>
              <td rowspan="1" colspan="1">244 402 568</td>
              <td rowspan="1" colspan="1">229 555 492</td>
              <td rowspan="1" colspan="1">229 403 697</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Error rate</td>
              <td rowspan="1" colspan="1">0.1425</td>
              <td rowspan="1" colspan="1">0.1442</td>
              <td rowspan="1" colspan="1">0.0506</td>
              <td rowspan="1" colspan="1">0.052</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Recall (%)</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">0.9515</td>
              <td rowspan="1" colspan="1">0.9499</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Precision (%)</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">0.9495</td>
              <td rowspan="1" colspan="1">0.9481</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Insertions</td>
              <td rowspan="1" colspan="1">30 090 583</td>
              <td rowspan="1" colspan="1">28 452 967</td>
              <td rowspan="1" colspan="1">12 252 413</td>
              <td rowspan="1" colspan="1">10 965 458</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Deletions</td>
              <td rowspan="1" colspan="1">5 483 119</td>
              <td rowspan="1" colspan="1">5 800 286</td>
              <td rowspan="1" colspan="1">2 574 320</td>
              <td rowspan="1" colspan="1">2 916 564</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Substitutions</td>
              <td rowspan="1" colspan="1">4 375 017</td>
              <td rowspan="1" colspan="1">4 081 445</td>
              <td rowspan="1" colspan="1">2 197 172</td>
              <td rowspan="1" colspan="1">1 940 888</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Trimmed/split</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">2216.0</td>
              <td rowspan="1" colspan="1">4943.0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Mean missing size</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">35.1</td>
              <td rowspan="1" colspan="1">74.7</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Extended</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">178.0</td>
              <td rowspan="1" colspan="1">169.0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Mean extension size</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">30.7</td>
              <td rowspan="1" colspan="1">31.9</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Low quality reads</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">43.0</td>
              <td rowspan="1" colspan="1">42.0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Small reads</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">0.0</td>
              <td rowspan="1" colspan="1">0.0</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="T7TFN1">
            <p>The two experiments were run on the same <italic>S. cerevisiae</italic> dataset, using a hybrid corrector (HALC) and a self corrector (Canu).</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>We observe that ELECTOR’s results are consistent, both in simulated and real data mode. In particular, recall and precision are very similar. The two modes display some differences in the input uncorrected reads (as shown by the amount of processed bases), which have an impact on the differences observed between their results. This behavior is due to the bias induced by the additional alignment step that the real data mode requires. The main differences that appear occur on metrics that are highly dependent on the alignment results, such as the number of trimmed, split and extended reads, and the sizes of these events, as well as indels and substitutions counts.</p>
    </sec>
    <sec id="SEC3-5">
      <title>Results on a real human dataset</title>
      <p>To demonstrate ELECTOR’s results in a realistic scenario for large genomes, we evaluate the correction of a real human dataset. We report results, as well as runtime of the evaluation, in Table <xref rid="tbl8" ref-type="table">8</xref>. The reads were corrected with MECAT, using default parameters, before running ELECTOR. Using 20 threads, we were able to obtain the results for the 650 771 corrected reads of the dataset in &lt;19 h. Results reported by ELECTOR show that MECAT can correct human reads with a 20% error rate with &gt;90% of recall and precision, which is consistent with the published results.</p>
      <table-wrap id="tbl8" orientation="portrait" position="float">
        <label>Table 8.</label>
        <caption>
          <p>Evaluation of the correction of a real human dataset with ELECTOR</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">Uncorrected</th>
              <th rowspan="1" colspan="1">Corrected with MECAT</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Processed bases</td>
              <td rowspan="1" colspan="1">5 605 157 590</td>
              <td rowspan="1" colspan="1">5 451 767 836</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Recall (%)</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">92.70</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Precision (%)</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">91.50</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Error rate</td>
              <td rowspan="1" colspan="1">0.1974</td>
              <td rowspan="1" colspan="1">0.0861</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Trimmed/split</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">570 635</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Mean missing size</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">362.0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Extended</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">275</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Mean extension size</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">62.4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Low quality reads</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">4279</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Small reads</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">356</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Insertions</td>
              <td rowspan="1" colspan="1">247 953 086</td>
              <td rowspan="1" colspan="1">10 144 736</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Deletions</td>
              <td rowspan="1" colspan="1">746 165 024</td>
              <td rowspan="1" colspan="1">473 239 036</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Substitutions</td>
              <td rowspan="1" colspan="1">162 822 923</td>
              <td rowspan="1" colspan="1">7 521 389</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Homopolymer ratio</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">0.7570</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Runtime</td>
              <td rowspan="1" colspan="1">–</td>
              <td rowspan="1" colspan="1">18 h 27 min</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="T8TFN1">
            <p>The reads were corrected with MECAT, using default parameters, before the evaluation. ELECTOR evaluated a total of 650 771 reads. Small reads are corrected reads whose length is &lt;10% of the original read. Low quality corrected reads are reads for which an insufficient number of seeds was found during the segmentation process. Homopolymer ratio is the ratio of homopolymer sizes in corrected versus reference. We reported the wallclock time of the run, using 20 threads.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
  </sec>
  <sec sec-type="discussion" id="SEC4">
    <title>DISCUSSION</title>
    <p>In ELECTOR, we propose a novel efficient algorithmic approach of segmentation strategy for multiple sequence alignment. We adapted this task for this original and specific application of long reads comparison. New segmentation strategies for MSA were recently proposed (Nogales, E. G. <italic>et al.</italic> (2018). Fast and accurate large multiple sequence alignments using root-to-leave regressive computation. (bioRxiv, 490235.). However, these methods are not specifically designed for noisy long reads. On such data, both the high error rates and lengths are troublesome factors for the multiple sequence alignment computation. In such a perspective, a generalization of our segmentation strategy, allowing long reads multiple sequence alignments of more than three sequences would be very interesting. Such a generalization could indeed be relevant for critical applications such as assembly, consensus or variant detection.</p>
    <p>ELECTOR’s real data mode uses a prior alignment of the reads to a reference genome, in order to retrieve the ‘reference’ versions of the reads. We demonstrated that ELECTOR’s metrics in its real data mode remain highly similar to what would be obtained in its simulated mode. However, we can point out two limitations of ELECTOR. First, even if the data can come from an actual sequencing experiment, a reference genome needs to exist for the sequenced species, in order to retrieve the ‘reference’ reads, and thus perform the evaluation. Second, we encourage users to be very cautious about ELECTOR’s results on real data, especially when looking at the number of trimmed, split or extended reads and at the sizes of such events. Indeed, these metrics are highly dependent on the result of the alignment of the ‘uncorrected’ reads to the reference. These metrics can thus be subject to errors, especially when aligning relatively short or highly erroneous/chimeric reads, or reads coming from repeated regions.</p>
    <p>A future application is the evaluation of correction methods directly targeted at RNA long reads sequencing. As shown in a recent study (<xref rid="B23" ref-type="bibr">23</xref>), RNA long reads have specific requirements that are not met by current methods, which calls for new correctors in the future. ELECTOR could be coupled with a reference transcriptome or a RNA long read simulator, although, currently, such a simulation software does not exist to our knowledge.</p>
  </sec>
  <sec sec-type="conclusions" id="SEC5">
    <title>CONCLUSION</title>
    <p>We presented ELECTOR, a tool that enables the evaluation of self and hybrid long reads correction methods, and that allows evaluating the behavior of a given correction tool in a controlled situation. ELECTOR provides a wide range of metrics that include base-wise computation of recall, precision, error rates of corrected and uncorrected reads as well as insertions, deletions and substitutions counts, and homopolymers correction. In particular, we believe that recall and precision are of prime interest to characterize a correction tool behavior. Indeed, this metrics allows spotting specific pitfalls, or undesired effects, which remain unclear when only looking at the error rates of the corrected reads. ELECTOR reports a text summary of its different metrics, along with pdf and png versions, including plots of the key figures. This allows users to easily integrate ELECTOR’s outputs into reports.</p>
    <p>Even though ELECTOR relies on multiple sequence alignment techniques that can be very resource-consuming, we were able to evaluate the behavior of a representative list of state-of-the-art hybrid and self-correctors, ran on reads from small bacterial to large mammal genomes. We also showed that ELECTOR’s performances allow it to scale to very long reads, displaying lengths up to 1 M bases, with moderate resource needs.</p>
    <p>In particular, ELECTOR is typically faster than most error correction methods. ELECTOR’s ability to quickly handle real-world datasets with low memory consumption is pre-eminently valuable when working on long read exploitation routines, and represents a significant improvement in comparison to the state-of-the-art.</p>
    <p>The efficiency of ELECTOR relies on an innovative and promising segmentation algorithm for multiple sequence alignment of noisy long reads. This procedure drastically reduces the time footprint of the multiple sequence alignment, making it able to scale to very long sequences. We believe this algorithm could be improved and applied to a broad range of applications implying multiple sequence alignment of long, noisy sequences.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>lqz015_Supplemental_File</label>
      <media xlink:href="lqz015_supplemental_file.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ACK1">
    <title>ACKNOWLEDGEMENTS</title>
    <p>We thank Pierre Marijon for his help with the Bioconda integration. Part of this work was performed using the computing resources of CRIANN (Normandy, France).</p>
  </ack>
  <sec id="SEC6">
    <title>SUPPLEMENTARY DATA</title>
    <p><ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/nargab/article-lookup/doi/10.1093/nargab/lqz015#supplementary-data">Supplementary Data</ext-link> are available at NARGAB Online.</p>
  </sec>
  <sec id="SEC7">
    <title>FUNDING</title>
    <p>Inria - Department of Scientific Affairs.</p>
    <p><italic>Conflict of interest statement</italic>. None declared.</p>
  </sec>
  <ref-list id="REF1">
    <title>REFERENCES</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gordon</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Huddleston</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Chaisson</surname><given-names>M.J.</given-names></name>, <name name-style="western"><surname>Hill</surname><given-names>C.M.</given-names></name>, <name name-style="western"><surname>Kronenberg</surname><given-names>Z.N.</given-names></name>, <name name-style="western"><surname>Munson</surname><given-names>K.M.</given-names></name>, <name name-style="western"><surname>Malig</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Raja</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Fiddes</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Hillier</surname><given-names>L.W.</given-names></name><etal>et al</etal>.</person-group><article-title>Long-read sequence assembly of the gorilla genome</article-title>. <source>Science</source>. <year>2016</year>; <volume>352</volume>:<fpage>aae0344</fpage>.<pub-id pub-id-type="pmid">27034376</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sedlazeck</surname><given-names>F.J.</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Darby</surname><given-names>C.A.</given-names></name>, <name name-style="western"><surname>Schatz</surname><given-names>M.C.</given-names></name></person-group><article-title>Piercing the dark matter: bioinformatics of long-range sequencing and mapping</article-title>. <source>Nat. Rev. Genet.</source><year>2018</year>; <volume>19</volume>:<fpage>329</fpage>–<lpage>346</lpage>.<pub-id pub-id-type="pmid">29599501</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Laehnemann</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Borkhardt</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>McHardy</surname><given-names>A.C.</given-names></name></person-group><article-title>Denoising DNA deep sequencing data-high-throughput sequencing errors and their correction</article-title>. <source>Brief. Bioinform.</source><year>2015</year>; <volume>17</volume>:<fpage>154</fpage>–<lpage>179</lpage>.<pub-id pub-id-type="pmid">26026159</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>Minimap2: pairwise alignment for nucleotide sequences</article-title>. <source>Bioinformatics</source>. <year>2018</year>; <volume>34</volume>:<fpage>3094</fpage>–<lpage>3100</lpage>.<pub-id pub-id-type="pmid">29750242</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><article-title>Minimap and miniasm: fast mapping and de novo assembly for noisy long sequences</article-title>. <source>Bioinformatics</source>. <year>2016</year>; <volume>32</volume>:<fpage>2103</fpage>–<lpage>2110</lpage>.<pub-id pub-id-type="pmid">27153593</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>La</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Haghshenas</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Chauve</surname><given-names>C.</given-names></name></person-group><article-title>LRCstats, a tool for evaluating long reads correction methods</article-title>. <source>Bioinformatics</source>. <year>2017</year>; <volume>33</volume>:<fpage>3652</fpage>–<lpage>3654</lpage>.<pub-id pub-id-type="pmid">29036421</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>X.</given-names></name>, <name name-style="western"><surname>Chockalingam</surname><given-names>S.P.</given-names></name>, <name name-style="western"><surname>Aluru</surname><given-names>S.</given-names></name></person-group><article-title>A survey of error-correction methods for next-generation sequencing</article-title>. <source>Brief. Bioinform.</source><year>2012</year>; <volume>14</volume>:<fpage>56</fpage>–<lpage>66</lpage>.<pub-id pub-id-type="pmid">22492192</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jain</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Koren</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Miga</surname><given-names>K.H.</given-names></name>, <name name-style="western"><surname>Quick</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Rand</surname><given-names>A.C.</given-names></name>, <name name-style="western"><surname>Sasani</surname><given-names>T.A.</given-names></name>, <name name-style="western"><surname>Tyson</surname><given-names>J.R.</given-names></name>, <name name-style="western"><surname>Beggs</surname><given-names>A.D.</given-names></name>, <name name-style="western"><surname>Dilthey</surname><given-names>A.T.</given-names></name>, <name name-style="western"><surname>Fiddes</surname><given-names>I.T.</given-names></name><etal>et al</etal>.</person-group><article-title>Nanopore sequencing and assembly of a human genome with ultra-long reads</article-title>. <source>Nat. Biotechnol.</source><year>2018</year>; <volume>36</volume>:<fpage>338</fpage>–<lpage>345</lpage>.<pub-id pub-id-type="pmid">29431738</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Chu</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Warren</surname><given-names>R.L.</given-names></name>, <name name-style="western"><surname>Birol</surname><given-names>I.</given-names></name></person-group><article-title>NanoSim: nanopore sequence read simulator based on statistical characterization</article-title>. <source>GigaScience</source>. <year>2017</year>; <volume>6</volume>:<fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stöcker</surname><given-names>B.K.</given-names></name>, <name name-style="western"><surname>Köster</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Rahmann</surname><given-names>S.</given-names></name></person-group><article-title>Simlord: Simulation of long read data</article-title>. <source>Bioinformatics</source>. <year>2016</year>; <volume>32</volume>:<fpage>2704</fpage>–<lpage>2706</lpage>.<pub-id pub-id-type="pmid">27166244</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Grasso</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Sharlow</surname><given-names>M.F.</given-names></name></person-group><article-title>Multiple sequence alignment using partial order graphs</article-title>. <source>Bioinformatics</source>. <year>2002</year>; <volume>18</volume>:<fpage>452</fpage>–<lpage>464</lpage>.<pub-id pub-id-type="pmid">11934745</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Delcher</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Salzberg</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Phillippy</surname><given-names>A.</given-names></name></person-group><article-title>Using MUMmer to identify similar regions in large sequence sets</article-title>. <source>Curr.Protoc. Bioinform.</source><year>2003</year>; <comment>doi:10.1002/0471250953.bi1003s00</comment>.</mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chaisson</surname><given-names>M.J.</given-names></name>, <name name-style="western"><surname>Tesler</surname><given-names>G.</given-names></name></person-group><article-title>Mapping single molecule sequencing reads using basic local alignment with successive refinement (BLASR): application and theory</article-title>. <source>BMC Bioinform.</source><year>2012</year>; <volume>13</volume>:<fpage>238</fpage>.</mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Miclotte</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Heydari</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Demeester</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Rombauts</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Van de Peer</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Audenaert</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Fostier</surname><given-names>J.</given-names></name></person-group><article-title>Jabba: hybrid error correction for long sequencing reads</article-title>. <source>Algorithm. Mol. Biol.</source><year>2016</year>; <volume>11</volume>:<fpage>10</fpage>.</mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Madoui</surname><given-names>M.-A.</given-names></name>, <name name-style="western"><surname>Engelen</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Cruaud</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Belser</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Bertrand</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Alberti</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Lemainque</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Wincker</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Aury</surname><given-names>J.-M.</given-names></name></person-group><article-title>Genome assembly using Nanopore-guided long and error-free DNA reads</article-title>. <source>BMC Genom.</source><year>2015</year>; <volume>16</volume>:<fpage>327</fpage>.</mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Koren</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Walenz</surname><given-names>B.P.</given-names></name>, <name name-style="western"><surname>Berlin</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>J.R.</given-names></name>, <name name-style="western"><surname>Bergman</surname><given-names>N.H.</given-names></name>, <name name-style="western"><surname>Phillippy</surname><given-names>A.M.</given-names></name></person-group><article-title>Canu: scalable and accurate long-read assembly via adaptive k-mer weighting and repeat separation</article-title>. <source>Genome Res.</source><year>2017</year>; <volume>27</volume>:<fpage>722</fpage>–<lpage>736</lpage>.<pub-id pub-id-type="pmid">28298431</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mikheenko</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Prjibelski</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Saveliev</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Antipov</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Gurevich</surname><given-names>A.</given-names></name></person-group><article-title>Versatile genome assembly evaluation with QUAST-LG</article-title>. <source>Bioinformatics</source>. <year>2018</year>; <volume>34</volume>:<fpage>i142</fpage>–<lpage>i150</lpage>.<pub-id pub-id-type="pmid">29949969</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bao</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Lan</surname><given-names>L.</given-names></name></person-group><article-title>HALC: High throughput algorithm for long read error correction</article-title>. <source>BMC Bioinform.</source><year>2017</year>; <volume>18</volume>:<fpage>204</fpage>.</mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Morisse</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Lecroq</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Lefebvre</surname><given-names>A.</given-names></name></person-group><article-title>Hybrid correction of highly noisy long reads using a variable-order de Bruijn graph</article-title>. <source>Bioinformatics</source>. <year>2018</year>; <volume>34</volume>:<fpage>4213</fpage>–<lpage>4222</lpage>.<pub-id pub-id-type="pmid">29955770</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Salmela</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Rivals</surname><given-names>E.</given-names></name></person-group><article-title>LoRDEC: accurate and efficient long read error correction</article-title>. <source>Bioinformatics</source>. <year>2014</year>; <volume>30</volume>:<fpage>3506</fpage>–<lpage>3514</lpage>.<pub-id pub-id-type="pmid">25165095</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xiao</surname><given-names>C.-L.</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Xie</surname><given-names>S.-Q.</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>K.-N.</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Han</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Luo</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Xie</surname><given-names>Z.</given-names></name></person-group><article-title>MECAT: fast mapping, error correction, and de novo assembly for single-molecule sequencing reads</article-title>. <source>Nat. Methods</source>. <year>2017</year>; <volume>14</volume>:<fpage>1072</fpage>.<pub-id pub-id-type="pmid">28945707</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Au</surname><given-names>K.F.</given-names></name></person-group><article-title>A comparative evaluation of hybrid error correction methods for error-prone long reads</article-title>. <source>Genome Biol.</source><year>2019</year>; <volume>20</volume>:<fpage>26</fpage>.<pub-id pub-id-type="pmid">30717772</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lima</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Marchet</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Caboche</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Da Silva</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Istace</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Aury</surname><given-names>J.-M.</given-names></name>, <name name-style="western"><surname>Touzet</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Chikhi</surname><given-names>R.</given-names></name></person-group><article-title>Comparative assessment of long-read error correction software applied to Nanopore RNA-sequencing data</article-title>. <source>Brief. Bioinform.</source><year>2019</year>; <comment>doi:10.1093/bib/bbz058</comment>.</mixed-citation>
    </ref>
  </ref-list>
</back>
