<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Oncol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Oncol</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Oncol.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Oncology</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2234-943X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7855703</article-id>
    <article-id pub-id-type="doi">10.3389/fonc.2020.586292</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Oncology</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SuperHistopath: A Deep Learning Pipeline for Mapping Tumor Heterogeneity on Low-Resolution Whole-Slide Digital Histopathology Images</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zormpas-Petridis</surname>
          <given-names>Konstantinos</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="https://loop.frontiersin.org/people/484211"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Noguera</surname>
          <given-names>Rosa</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ivankovic</surname>
          <given-names>Daniela Kolarevic</given-names>
        </name>
        <xref ref-type="aff" rid="aff4">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Roxanis</surname>
          <given-names>Ioannis</given-names>
        </name>
        <xref ref-type="aff" rid="aff5">
          <sup>5</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn002">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="https://loop.frontiersin.org/people/1146999"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jamin</surname>
          <given-names>Yann</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn002">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="https://loop.frontiersin.org/people/555121"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yuan</surname>
          <given-names>Yinyin</given-names>
        </name>
        <xref ref-type="aff" rid="aff6">
          <sup>6</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn001">
          <sup>*</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn002">
          <sup>†</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Division of Radiotherapy and Imaging, The Institute of Cancer Research</institution>, <addr-line>London</addr-line>, <country>United Kingdom</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Department of Pathology, Medical School, University of Valencia-INCLIVA Biomedical Health Research Institute</institution>, <addr-line>Valencia</addr-line>, <country>Spain</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Low Prevalence Tumors, Centro de Investigación Biomédica en Red de Cáncer (CIBERONC), Instituto de Salud Carlos III</institution>, <addr-line>Madrid</addr-line>, <country>Spain</country></aff>
    <aff id="aff4"><sup>4</sup><institution>The Royal Marsden NHS Foundation Trust</institution>, <addr-line>London</addr-line>, <country>United Kingdom</country></aff>
    <aff id="aff5"><sup>5</sup><institution>Breast Cancer Now Toby Robins Research Centre, The Institute of Cancer Research</institution>, <addr-line>London</addr-line>, <country>United Kingdom</country></aff>
    <aff id="aff6"><sup>6</sup><institution>Division of Molecular Pathology, The Institute of Cancer Research</institution>, <addr-line>London</addr-line>, <country>United Kingdom</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Youyong Kong, Southeast University, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Li Liu, Donghua University, China; Mark Hansen, University of the West of England, United Kingdom; Eleftheria Panagiotaki, University College London, United Kingdom</p>
      </fn>
      <corresp id="fn001">*Correspondence: Konstantinos Zormpas-Petridis, <email xlink:href="mailto:Konstantinos.Zormpas-Petridis@icr.ac.uk" xlink:type="simple">Konstantinos.Zormpas-Petridis@icr.ac.uk</email>; Yinyin Yuan, <email xlink:href="mailto:yinyin.yuan@icr.ac.uk" xlink:type="simple">yinyin.yuan@icr.ac.uk</email>
</corresp>
      <fn fn-type="other" id="fn002">
        <p>†These authors share senior authorship</p>
      </fn>
      <fn fn-type="other" id="fn003">
        <p>This article was submitted to Cancer Imaging and Image-directed Interventions, a section of the journal Frontiers in Oncology</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>10</volume>
    <elocation-id>586292</elocation-id>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>7</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>30</day>
        <month>11</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2021 Zormpas-Petridis, Noguera, Ivankovic, Roxanis, Jamin and Yuan</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Zormpas-Petridis, Noguera, Ivankovic, Roxanis, Jamin and Yuan</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>High computational cost associated with digital pathology image analysis approaches is a challenge towards their translation in routine pathology clinic. Here, we propose a computationally efficient framework (SuperHistopath), designed to map global context features reflecting the rich tumor morphological heterogeneity. SuperHistopath efficiently combines i) a segmentation approach using the linear iterative clustering (SLIC) superpixels algorithm applied directly on the whole-slide images at low resolution (5x magnification) to adhere to region boundaries and form homogeneous spatial units at tissue-level, followed by ii) classification of superpixels using a convolution neural network (CNN). To demonstrate how versatile SuperHistopath was in accomplishing histopathology tasks, we classified tumor tissue, stroma, necrosis, lymphocytes clusters, differentiating regions, fat, hemorrhage and normal tissue, in 127 melanomas, 23 triple-negative breast cancers, and 73 samples from transgenic mouse models of high-risk childhood neuroblastoma with high accuracy (98.8%, 93.1% and 98.3% respectively). Furthermore, SuperHistopath enabled discovery of significant differences in tumor phenotype of neuroblastoma mouse models emulating genomic variants of high-risk disease, and stratification of melanoma patients (high ratio of lymphocyte-to-tumor superpixels (p = 0.015) and low stroma-to-tumor ratio (p = 0.028) were associated with a favorable prognosis). Finally, SuperHistopath is efficient for annotation of ground-truth datasets (as there is no need of boundary delineation), training and application (~5 min for classifying a whole-slide image and as low as ~30 min for network training). These attributes make SuperHistopath particularly attractive for research in rich datasets and could also facilitate its adoption in the clinic to accelerate pathologist workflow with the quantification of phenotypes, predictive/prognosis markers.</p>
    </abstract>
    <kwd-group>
      <kwd>deep learning</kwd>
      <kwd>machine learning</kwd>
      <kwd>digital pathology</kwd>
      <kwd>computational pathology</kwd>
      <kwd>tumor region classification</kwd>
      <kwd>melanoma</kwd>
      <kwd>neuroblastoma</kwd>
      <kwd>breast cancer</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">Rosetrees Trust<named-content content-type="fundref-id">10.13039/501100000833</named-content></funding-source>
        <award-id rid="cn001">M593</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn002">Instituto de Salud Carlos III<named-content content-type="fundref-id">10.13039/501100004587</named-content></funding-source>
        <award-id rid="cn002">PI17/01558</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn003">European Regional Development Fund<named-content content-type="fundref-id">10.13039/501100008530</named-content></funding-source>
        <award-id rid="cn003">CB16/12/00484</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn004">Cancer Research UK<named-content content-type="fundref-id">10.13039/501100000289</named-content></funding-source>
        <award-id rid="cn004">C45982/A21808</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn005">Breast Cancer Now<named-content content-type="fundref-id">10.13039/501100007913</named-content></funding-source>
        <award-id rid="cn005">2015NovPR638</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn006">European Commission<named-content content-type="fundref-id">10.13039/501100000780</named-content></funding-source>
        <award-id rid="cn006">H2020-MSCA-ITN-2019</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn007">Wellcome Trust<named-content content-type="fundref-id">10.13039/100004440</named-content></funding-source>
        <award-id rid="cn007">105104/Z/14/Z</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn008">CHILDREN with CANCER UK<named-content content-type="fundref-id">10.13039/501100001273</named-content></funding-source>
        <award-id rid="cn008">2014/176</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="6"/>
      <table-count count="6"/>
      <equation-count count="1"/>
      <ref-count count="68"/>
      <page-count count="13"/>
      <word-count count="5824"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Introduction</title>
    <p>The analysis of histopathological images of surgical tissue specimens stained with hematoxylin and eosin (H&amp;E) remains a critical decision-making tool used for the routine management of patients with cancer and the evaluation of new therapeutic strategies in clinical trials (<xref rid="B1" ref-type="bibr">1</xref>–<xref rid="B3" ref-type="bibr">3</xref>). In several precision medicine settings, there is an increasing demand for accurate quantification of histological features. However, in their diagnostic practice, pathologists exercise a predominantly qualitative or semi-quantitative assessment with an inherent degree of inter- and intra-observer variability, which occasionally hampers their consistency (<xref rid="B4" ref-type="bibr">4</xref>–<xref rid="B7" ref-type="bibr">7</xref>). In the new era of digital pathology, advanced computational image analysis techniques are revolutionizing the field of histopathology by providing objective, robust and reproducible quantification of tumor components, thereby assisting pathologists in tasks such as tumor identification and tumor grading (<xref rid="B8" ref-type="bibr">8</xref>, <xref rid="B9" ref-type="bibr">9</xref>). Histopathological image analysis can now be performed in high-resolution H&amp;E-stained whole-slide images (WSI) using state-of-the-art deep learning and classical machine learning approaches for single cell segmentation and/or classification. The new ability to map the spatial context of each single cell also opened new avenues for the study of the tumor micro-environment (<xref rid="B10" ref-type="bibr">10</xref>–<xref rid="B16" ref-type="bibr">16</xref>), which is key to guide the delivery of precision medicine including immunotherapy.</p>
    <p>However, computational pathology is still not widely adopted in the oncological setting. One of the challenges lies in the gigabyte sizes of high-resolution WSIs, which result in computationally expensive approaches. WSIs need to be divided into images patches (typical size: 256x256) before being processed by deep networks such as convolutional neural networks (CNNs) (<xref rid="B17" ref-type="bibr">17</xref>). Secondly, single-cell approaches provide markers that are often hard-to-be-evaluated or even interpreted by the pathologists and can be prone to the generalization errors when applied in new unseen dataset. As a result, many promising markers eventually fail to reach the clinic due to a lack of cross-validation in new independent datasets. On the other hand, tissue classification approaches, which target multicellular assemblies and paucicellular areas where individual cells are incorporated into the region segmentation, would be accessible for visual validation by pathologists. Such algorithms would enable the characterization of the distribution and interrelationship of global features that are currently detectable by human perception but not quantifiable without artificial intelligence- (AI-)assisted numerical expression.</p>
    <p>Current computed pathology tools primarily focus on individual cell analysis at high-resolution (40x/20x magnification) with limited local context features, whereas pathologists frequently employ collateral information, taking into account the overall tissue microarchitecture. Many established clinical markers are actually identified at low or intermediate magnifications, including tumor architecture-based grading systems (<xref rid="B18" ref-type="bibr">18</xref>, <xref rid="B19" ref-type="bibr">19</xref>), stroma-tumor ratio (<xref rid="B20" ref-type="bibr">20</xref>, <xref rid="B21" ref-type="bibr">21</xref>), infiltrating lymphocytes (TILs) (<xref rid="B22" ref-type="bibr">22</xref>, <xref rid="B23" ref-type="bibr">23</xref>) and necrosis (<xref rid="B24" ref-type="bibr">24</xref>–<xref rid="B26" ref-type="bibr">26</xref>). This has not been yet fully emulated by computational pathology methodologies. However, some methods for the classification of tissue components have been suggested either using image patch classification typically with a CNN or pixel-level classification/segmentation typically with a U-Net-like architecture (<xref rid="B27" ref-type="bibr">27</xref>), mainly for tasks such as the dichotomized classification of tissue (e.g. cancerous vs non-cancerous) (<xref rid="B28" ref-type="bibr">28</xref>, <xref rid="B29" ref-type="bibr">29</xref>), the segmentation of a feature of interest (e.g. glands) (<xref rid="B16" ref-type="bibr">16</xref>, <xref rid="B30" ref-type="bibr">30</xref>) or multi-type tissue classification (<xref rid="B9" ref-type="bibr">9</xref>, <xref rid="B31" ref-type="bibr">31</xref>–<xref rid="B35" ref-type="bibr">35</xref>). For segmentation purposes, U-Net-like architectures are usually preferred over CNNs, which have established limitations in conforming to object contours. Yet, CNNs have also resulted in promising segmentation approaches (<xref rid="B36" ref-type="bibr">36</xref>–<xref rid="B38" ref-type="bibr">38</xref>) with the enhanced capability of classifying a large number of categories (<xref rid="B39" ref-type="bibr">39</xref>). Multi-scale approaches incorporating information from various image resolutions have also been proposed (<xref rid="B40" ref-type="bibr">40</xref>–<xref rid="B43" ref-type="bibr">43</xref>). Different approaches have been explored for the classification of epithelium or stroma using superpixels-based segmentation of image patches with either hand-crafted or deep learning features (<xref rid="B44" ref-type="bibr">44</xref>, <xref rid="B45" ref-type="bibr">45</xref>). Bejnordi and colleagues used a similar method for their multi-scale approach for the classification of tissue or non-tissue components on low resolution images and stroma and background regions from intermediate and high resolution images (<xref rid="B46" ref-type="bibr">46</xref>). However, these methods are typically performed on high-magnifications image patches (20-40x and more rarely 10x) and are associated with a high computational cost.</p>
    <p>Here, we propose a framework (SuperHistopath), which can map most of the global context features that contribute to the rich tumor morphological heterogeneity visible to pathologists at low resolution and used for clinical decision making in a computationally efficient manner. We first apply the well-established simple linear iterative clustering (SLIC) superpixels algorithm (<xref rid="B47" ref-type="bibr">47</xref>) directly on the WSI at low resolution (5x magnification) and subsequently classify the superpixels into different tumor region categories using a CNN based on pathologists’ annotations. SuperHistopath particularly capitalizes on:</p>
    <list list-type="roman-lower">
      <list-item>
        <p>the use of superpixels which provide visually homogeneous areas of similar size respecting the region boundaries and avoid the potential degradation of classification performance associated with image patches, (no matter how small) spanning over multiple tissue categories.</p>
      </list-item>
      <list-item>
        <p>the use of CNN necessary to accurately classify and map the multiple tissue categories that constitute the rich and complex histological intratumoral heterogeneity.</p>
      </list-item>
      <list-item>
        <p>the computational efficiency, faster processing speed and lower memory requirements associated with processing the WSI at low resolution.</p>
      </list-item>
    </list>
    <p>We applied SuperHistopath to H&amp;E-stained images from three different cancer types: clinical cutaneous melanoma, triple-negative breast cancer and tumors arising in genetically-engineered mouse models of high-risk childhood neuroblastoma.</p>
  </sec>
  <sec sec-type="materials|methods" id="s2">
    <title>Materials and Methods</title>
    <sec id="s2_1">
      <title>Datasets</title>
      <p>All digitized whole-slide images (WSI) used in this study were H&amp;E-stained, formalin-fixed and paraffin-embedded (FFPE) sections, and scaled to 5x magnification as presented in <xref rid="T1" ref-type="table"><bold>Table 1</bold></xref> (image sizes at 5x varied from ~8000x8000 to ~12000x12000 pixels). We applied our framework to clinical patient samples of cutaneous melanoma and triple-negative breast cancer, in addition to tumor samples from transgenic mouse models of childhood neuroblastoma. Both the Th-<italic>MYCN</italic> and Th-<italic>ALK<sup>F1174L</sup>/MYCN</italic> mouse models have been shown to spontaneously develop abdominal tumors, which mirror the major histopathological characteristics of childhood high-risk disease (<xref rid="B50" ref-type="bibr">50</xref>, <xref rid="B51" ref-type="bibr">51</xref>).</p>
      <table-wrap id="T1" position="float">
        <label>Table 1</label>
        <caption>
          <p>Summary of the datasets used.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">Cancer type</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Number of WSIs</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Digital scanner</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Pixel resolution (5x magnification)</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Dataset</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Cutaneous melanoma</td>
              <td valign="top" align="center" rowspan="1" colspan="1">127</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Aperio ImageScope</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2.016 μm</td>
              <td valign="top" align="left" rowspan="1" colspan="1">The Cancer Genome Atlas (TCGA)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Triple-negative breast cancer</td>
              <td valign="top" align="center" rowspan="1" colspan="1">23</td>
              <td valign="top" align="left" rowspan="1" colspan="1">NanoZoomer XR</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2.3 μm</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Internal dataset,<break/>Collaboration with The Serbian Institute of Oncology</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">High-risk neuroblastoma (mouse models)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73</td>
              <td valign="top" align="left" rowspan="1" colspan="1">NanoZoomer XR</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2.3 μm</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Internal dataset<break/>Tumors samples coming from established Th-<italic>MYCN</italic> and Th-<italic>ALK<sup>F1174L</sup>/MYCN</italic> transgenic mouse colonies (<xref rid="B48" ref-type="bibr">48</xref>, <xref rid="B49" ref-type="bibr">49</xref>) and processed by a clinical histopathological core facility</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="s2_2">
      <title>Region Classification</title>
      <p>First, each dataset was pre-processed using the Reinhard stain normalization (<xref rid="B52" ref-type="bibr">52</xref>) to account for stain variabilities that could affect classification. Then, all images were segmented using the simple linear iterative clustering (SLIC) superpixels algorithm, which groups together similar neighboring pixels. With our pathologist’s input, we selected the optimal number of superpixels by visually identifying a superpixel size that capture only homogeneous areas and adhere to image boundaries. This is a critical step for ensuring accurate tissue segmentation, and therefore, classification (<xref ref-type="fig" rid="f1"><bold>Figure 1</bold></xref>). The number of superpixels was adapted for each image to ensure a homogenous superpixel size across the datasets and was automatically set based on the image size according to <italic>Equation 1</italic> (<xref rid="B53" ref-type="bibr">53</xref>).</p>
      <disp-formula>
        <label>(1)</label>
        <mml:math id="M1">
          <mml:mrow>
            <mml:msub>
              <mml:mi>N</mml:mi>
              <mml:mi>i</mml:mi>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mi>c</mml:mi>
            <mml:mi>e</mml:mi>
            <mml:mi>i</mml:mi>
            <mml:mi>l</mml:mi>
            <mml:mi>i</mml:mi>
            <mml:mi>n</mml:mi>
            <mml:mi>g</mml:mi>
            <mml:mrow>
              <mml:mo>(</mml:mo>
              <mml:mrow>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>S</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mi>U</mml:mi>
                </mml:mfrac>
              </mml:mrow>
              <mml:mo>)</mml:mo>
            </mml:mrow>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <fig id="f1" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Representative examples of the SLIC superpixels segmentation and ground-truth annotations in TCGA melanoma samples <bold>(A)</bold> Whole-slide image segmentation using the SLIC superpixels algorithm. Note how the superpixels adhere to the boundaries of the different components of the tumor with each superpixel containing a single type of tissue <bold>(B)</bold> Ground-truth annotations are provided by the pathologists by marking samples of the region components (the different colors represent different regions) without the need for delineating the boundaries of the tumor components.</p>
        </caption>
        <graphic xlink:href="fonc-10-586292-g001"/>
      </fig>
      <p>where <italic>Ni</italic> is the number of superpixels in the <italic>i<sup>th</sup></italic> image, <italic>Si</italic> is the size of image <italic>i</italic> in pixels, and <italic>U</italic> is a constant held across all images that defined the desired superpixels size.</p>
      <p>The SLIC algorithm inherently provides a roughly uniform superpixel size. Setting <italic>U</italic> = 1500, <italic>Equation 1</italic> gave a mean superpixels size of 51 × 51 pixels, equivalent to an area of approximately 117 × 117 μm<sup>2</sup>. Bilinear interpolation was subsequently use to resize each superpixel to a fixed size of 56 x 56 or 75 x 75 pixels (the minimum input size for inception-like network architectures).</p>
      <p>Region annotations were provided by a senior pathologist with over 20 years of experience for the melanoma and breast cancer clinical datasets, and a senior pediatric neuropathologist with over 20 years of experience for the neuroblastoma mouse datasets. For training and testing, superpixels were assigned to each category based on their isocenter location within the annotated regions. Note that region annotations for our algorithm do not need to delineate boundaries as illustrated in <xref ref-type="fig" rid="f1"><bold>Figure 1B</bold></xref>.</p>
      <p>The numbers of clinically relevant tissue categories, number of WSIs and superpixels used for training and testing are summarized for each tumor types in <xref rid="T2" ref-type="table"><bold>Table 2</bold></xref>. Standard image augmentations, such as rotations (90°, -90°, 180°), flips (horizontal and vertical), and contrast (histogram equalization) were performed in each case to capture more variation and even out the training dataset imbalances.</p>
      <table-wrap id="T2" position="float">
        <label>Table 2</label>
        <caption>
          <p>Summary of the datasets used for training and testing the convolutional neural network.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">Cancer type</th>
              <th valign="top" colspan="2" align="center" rowspan="1">Number of WSIs used for network training</th>
              <th valign="top" colspan="2" align="center" rowspan="1">Regional classification</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" rowspan="7" align="left" colspan="1">Cutaneous melanoma</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Total</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27</td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>
                  <italic>6 categories</italic>
                </bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>
                  <italic>Superpixels for training</italic>
                </bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Training</td>
              <td valign="top" align="center" rowspan="1" colspan="1">22</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Tumor tissue</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21940</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Testing</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Stroma</td>
              <td valign="top" align="center" rowspan="1" colspan="1">12419</td>
            </tr>
            <tr>
              <td valign="top" rowspan="4" colspan="2" align="left"/>
              <td valign="top" align="left" rowspan="1" colspan="1">Normal epidermis</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1646</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Lymphocytes cluster</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2367</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Fat</td>
              <td valign="top" align="center" rowspan="1" colspan="1">15484</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Empty/white space</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3412</td>
            </tr>
            <tr>
              <td valign="top" rowspan="7" align="left" colspan="1">Triple-negative breast cancer</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Total</td>
              <td valign="top" align="center" rowspan="1" colspan="1">23</td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>
                  <italic>6 categories</italic>
                </bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>
                  <italic>Superpixels for training</italic>
                </bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Training</td>
              <td valign="top" align="center" rowspan="1" colspan="1">18</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Tumor tissue</td>
              <td valign="top" align="center" rowspan="1" colspan="1">18873</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Testing</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Stroma</td>
              <td valign="top" align="center" rowspan="1" colspan="1">24220</td>
            </tr>
            <tr>
              <td valign="top" rowspan="4" colspan="2" align="left"/>
              <td valign="top" align="left" rowspan="1" colspan="1">Necrosis</td>
              <td valign="top" align="center" rowspan="1" colspan="1">15102</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Lymphocytes cluster</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3472</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Fat</td>
              <td valign="top" align="center" rowspan="1" colspan="1">10044</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Empty/white space</td>
              <td valign="top" align="center" rowspan="1" colspan="1">16473</td>
            </tr>
            <tr>
              <td valign="top" rowspan="9" align="left" colspan="1">High-risk neuroblastoma (mouse model)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Total</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60</td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>
                  <italic>8 categories</italic>
                </bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>
                  <italic>Superpixels for training</italic>
                </bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Training</td>
              <td valign="top" align="center" rowspan="1" colspan="1">44</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Region of undifferentiated neuroblasts</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20512</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Testing</td>
              <td valign="top" align="center" rowspan="1" colspan="1">16</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Tissue damage (necrosis/apoptosis)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">17645</td>
            </tr>
            <tr>
              <td valign="top" rowspan="6" colspan="2" align="left"/>
              <td valign="top" align="left" rowspan="1" colspan="1">Differentiation region</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5740</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Lymphocytes cluster</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4009</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Hemorrhage (blood)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6124</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Muscle</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6415</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Kidney</td>
              <td valign="top" align="center" rowspan="1" colspan="1">14976</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Empty/white space</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21470</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn>
            <p>Note that the testing datasets consisted of whole-slide images from different patients from the training dataset.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="s2_3">
      <title>Training of the Convolutional Neural Networks</title>
      <p>Our custom-designed CNN for superpixel classification consists of 6 convolutional layers (32, 32, 64, 64, 128, 128 neurons, respectively) of 3 x 3 filter size and 3 max-pooling layers, followed by a “flatten” layer and a dense layer of 256 neurons (<xref ref-type="fig" rid="f2"><bold>Figure 2</bold></xref>). A superpixel RGB image (post-interpolation) was used as input into the network and normalized from range 0–255 to range 0–1 using the maximum value. The output of the network was a label assigned to each superpixel based on which region category it belonged to. After empirical experimentation, a ReLU activation function was used in all layers except for the last layer where standard softmax was used for classification. The weights incident to each hidden unit were constrained to have a norm value less than or equal to 3 and a dropout unit of 0.2 was used before every max-pooling operation to avoid overfitting (<xref rid="B54" ref-type="bibr">54</xref>). The weights of the layers were randomly initialized using “Glorot uniform” initialization (<xref rid="B55" ref-type="bibr">55</xref>), and the network was optimized using the Adam method (<xref rid="B56" ref-type="bibr">56</xref>) with a learning rate of 10<sup>-3</sup> and a categorical cross-entropy cost function. The number of trainable parameters for our custom-made network is ~1.9 M. The network was implemented in python (v. 3.6.5) using the Keras/Tensorflow libraries (v. 2.2.4/1.12.0, respectively).</p>
      <fig id="f2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Architecture of our custom-designed convolutional neural network for the classification of superpixels into different tissue-level categories.</p>
        </caption>
        <graphic xlink:href="fonc-10-586292-g002"/>
      </fig>
      <p>To choose the best network for our framework, we tested other known neural network architectures as implemented in the Keras framework, including InceptionV3 (<xref rid="B57" ref-type="bibr">57</xref>), Xception (<xref rid="B58" ref-type="bibr">58</xref>), InceptionResNetV2 (<xref rid="B59" ref-type="bibr">59</xref>), and ResNet (<xref rid="B60" ref-type="bibr">60</xref>). We initialized the weights using the pre-trained ImageNet weights. To optimize each network, we excluded the final classification layer, and added three additional layers, i) a global average pooling layer, ii) a dense layer of 256 neurons with ReLu activation, constrained to have a norm value less than or equal to 3, and iii) a dense layer tailored to the number of classes of each cancer type using the softmax function for classification.</p>
      <p>For inception-like architectures (Inception v3, InceptionResNetV2, Xception) only superpixels of size 75 x 75 were used. We trained all the networks for 50 epochs using batch sizes of 150 and 256 for superpixels of sizes 75 x 75 and 56 x 56, respectively, and kept the models with the highest validation accuracy.</p>
      <p>The Xception and custom-made networks were re-trained from the beginning for each cancer type, without applying any further changes.</p>
    </sec>
    <sec id="s2_4">
      <title>Application of SuperHistopath for the Quantification of Clinical Features of Interest</title>
      <p>In the melanoma dataset, we calculated the number of pixels belonging to each classified category. For each patient we derived i) the ratio of pixels classified as stroma region to all pixels in tumor compartments, and ii) the ratio of pixels classified as clusters of lymphocytes to all pixels in tumor compartments; we evaluated the prognostic value of these quantitative indices using survival analysis. Patients were divided into high- and low-risk groups based on split at the median value of all scores to ensure both groups were of similar size. Kaplan-Meier estimation was used to compare overall survival in the 127 patients. Differences between survival estimates were assessed with the log-rank test and hazard ratios were calculated using Cox’s proportional-hazard regression.</p>
      <p>In the neuroblastoma dataset, we evaluated the differences in phenotype between the Th-<italic>ALK<sup>F1174L</sup></italic>/<italic>MYCN</italic> (n=7) and Th-<italic>MYCN</italic> tumors (n=6) by quantifying the proportion of pixels classified by our SuperHistopath as regions rich in undifferentiated neuroblasts, differentiating neuroblasts, tissue damage (necrosis/apoptosis) hemorrhage and clusters of lymphocytes. Note that i) we did not quantify stroma in these tumors as they faithfully mirror the stroma-poor phenotype which define high-risk disease ii) lymphocytes clusters universally correspond to encapsulation of lymph node by the tumor, rather that tumor infiltrates, consistent with the “cold” immune phenotype of high-risk disease. We focus on identifying any significant difference in the ratio of differentiation or the ratio of hemorrhagic regions to all tumor compartments between the two tumor types using the Mann-Whitney U test, with a 5% level of significance.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>Results</title>
    <sec id="s3_1">
      <title>SuperHistopath Can Accurately Map the Complex Histological Heterogeneity of Tumors</title>
      <sec id="s3_1_1">
        <title>Melanoma</title>
        <p>We first developed and evaluated our framework on the H&amp;E-stained, FFPE sections of clinical specimen of cutaneous melanoma scaled to 5x magnification. <xref ref-type="fig" rid="f1"><bold>Figure 1</bold></xref> shows the results of the segmentation using the simple linear iterative clustering (SLIC) superpixels algorithm, which groups together similar neighboring pixels.</p>
        <p>The optimized Xception network achieved the highest score and classified the melanoma sample regions into 6 predefined tissue categories of interest: tumor tissue, stroma, cluster of lymphocytes, normal epidermis, fat, and empty/white space with an overall accuracy of 98.8%, an average precision of 96.9%, and an average recall of 98.5% over 14,092 superpixels in a separate test set of five images (<xref rid="T3" ref-type="table"><bold>Tables 3</bold></xref>, <xref rid="T4" ref-type="table"><bold>4</bold></xref>). Our custom CNN also achieved comparable performance to the state-of-the-art networks with an overall accuracy of 96.7%, an average precision of 93.6%, and an average recall of 93.6% (<xref ref-type="fig" rid="f2"><bold>Figure 2</bold></xref>, <xref ref-type="supplementary-material" rid="ST1"><bold>Supplementary Table 1</bold></xref>). The confusion matrices for the XCeption and our custom CNN networks are presented in <xref rid="T4" ref-type="table"><bold>Table 4</bold></xref> and <xref ref-type="supplementary-material" rid="ST1"><bold>Supplementary Table 1</bold></xref>, respectively. <xref ref-type="fig" rid="f3"><bold>Figure 3</bold></xref> shows qualitative results of our approach’s regional classification in representative melanoma WSIs using the optimized Xception network.</p>
        <table-wrap id="T3" position="float">
          <label>Table 3</label>
          <caption>
            <p>Evaluation metrics of the different neural network architectures in the TCGA melanoma test dataset.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" align="left" rowspan="1" colspan="1">Network</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Accuracy (%)</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Precision (%)</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Recall (%)</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Parameters (in millions)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>InceptionV3</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">97.5</td>
                <td valign="top" align="center" rowspan="1" colspan="1">94.2</td>
                <td valign="top" align="center" rowspan="1" colspan="1">96.7</td>
                <td valign="top" align="center" rowspan="1" colspan="1">~22.4</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>InceptionResNetV2</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">97.7</td>
                <td valign="top" align="center" rowspan="1" colspan="1">94.1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">97.3</td>
                <td valign="top" align="center" rowspan="1" colspan="1">~54.8</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>ResNet50</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">93.8</td>
                <td valign="top" align="center" rowspan="1" colspan="1">92.2</td>
                <td valign="top" align="center" rowspan="1" colspan="1">88.9</td>
                <td valign="top" align="center" rowspan="1" colspan="1">~24.2</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Xception</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>98.8</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>96.9</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>98.5</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">~21.4</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Our custom-made CNN</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">96.7</td>
                <td valign="top" align="center" rowspan="1" colspan="1">93.6</td>
                <td valign="top" align="center" rowspan="1" colspan="1">93.6</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>~1.9</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn>
              <p>The bold values in the Accuracy (%), Precision (%) and Recall (%) fields indicate the highest value i.e. the best performance achieved amongst the networks under comparison. The bold value in the Parameters (in millions) field indicate the network with the fewer parameters used amongst the networks under comparison.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <table-wrap id="T4" position="float">
          <label>Table 4</label>
          <caption>
            <p>Confusion matrix of the classification of superpixels using the optimized Xception network in melanoma patients in 6 categories: tumor, stroma, normal epidermis, cluster of lymphocytes (Lym), fat and empty/white space (separate test set of 5 whole-slide images).</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" align="left" rowspan="1" colspan="1"/>
                <th valign="top" align="center" rowspan="1" colspan="1">Tumor</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Stroma</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Epidermis</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Lym</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Fat</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Empty space</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Tumor</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>5286</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">10</td>
                <td valign="top" align="center" rowspan="1" colspan="1">7</td>
                <td valign="top" align="center" rowspan="1" colspan="1">8</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Stroma</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">9</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>986</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">2</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Epidermis</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">22</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>545</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Lym</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>821</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Fat</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">9</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>5603</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">3</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Empty space</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">98</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>681</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn>
              <p>Overall accuracy = 98.8%, average precision = 96.9%, average recall = 98.5%.</p>
              <p>The bold values indicate the correct predictions of the network.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <fig id="f3" position="float">
          <label>Figure 3</label>
          <caption>
            <p><bold>(A–F)</bold> Representative examples of the results obtained from the application of the SuperHistopath pipeline in whole-slide images of tumors (5x) of the Cancer Genome Atlas (TCGA) melanoma dataset [<bold>(G)</bold> Magnified regions of interest]. Note the important clinically-relevant phenotypes characterized by clusters of lymphocytes infiltrating the tumor in samples <bold>(B, D)</bold>. or the majority of clusters of lymphocytes residing just outside the tumor area (left and central part) with only a few clusters infiltrating the tumor (right part) in sample <bold>(C)</bold>.</p>
          </caption>
          <graphic xlink:href="fonc-10-586292-g003"/>
        </fig>
      </sec>
      <sec id="s3_1_2">
        <title>Breast Cancer</title>
        <p>SuperHistopath classified sample regions into 6 predefined tissue categories of interest: tumor, necrosis, stroma, cluster of lymphocytes, fat, and lumen/empty space with an overall accuracy of 93.1%, an average precision of 93.9%, and an average recall of 93.6% using Xception and 91.7%, 92.5%, 91.8% respectively using our custom-made CNN over 10,349 superpixels in the independent test set of five images. The confusion matrices for the XCeption and our custom CNN networks are presented in <xref rid="T5" ref-type="table"><bold>Table 5</bold></xref> and <xref ref-type="supplementary-material" rid="ST2"><bold>Supplementary Table 2</bold></xref>, respectively. <xref ref-type="fig" rid="f4"><bold>Figure 4</bold></xref> shows qualitative results our approach’s regional classification in representative triple-negative breast cancer WSIs.</p>
        <table-wrap id="T5" position="float">
          <label>Table 5</label>
          <caption>
            <p>Confusion matrix of the classification of superpixels using the optimized Xception network in triple-negative breast cancer patients in six categories: tumor, necrosis, cluster of lymphocytes (Lym), stroma, fat, and lumen/empty space (separate test set of five whole-slide images).</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" align="left" rowspan="1" colspan="1"/>
                <th valign="top" align="center" rowspan="1" colspan="1">Tumor</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Necrosis</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Lym</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Stroma</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Fat</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Empty space</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Tumor</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>1830</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">13</td>
                <td valign="top" align="center" rowspan="1" colspan="1">15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">42</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Necrosis</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">50</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>1446</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">2</td>
                <td valign="top" align="center" rowspan="1" colspan="1">320</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Lym</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">4</td>
                <td valign="top" align="center" rowspan="1" colspan="1">2</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>705</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">10</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Stroma</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">42</td>
                <td valign="top" align="center" rowspan="1" colspan="1">120</td>
                <td valign="top" align="center" rowspan="1" colspan="1">20</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>3836</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Fat</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>562</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">5</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Empty space</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">67</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>1257</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn>
              <p>Overall accuracy = 93.1%, average precision = 93.9%, average recall = 93.6%.</p>
              <p>The bold values indicate the correct predictions of the network.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <fig id="f4" position="float">
          <label>Figure 4</label>
          <caption>
            <p><bold>(A–F)</bold>. Representative examples of the results obtained from the application of the SuperHistopath pipeline in whole-slide images of tumors (5x) of the triple-negative breast cancer <bold>(G)</bold> Magnified regions of interest. Note the important clinically-relevant features, such as the amount of tumor necrosis inside tumors <bold>(A)</bold> and <bold>(B)</bold>, lymphocytes which, are infiltrating the tumor in large number in samples <bold>(C, D)</bold>, but are surrounding the stroma barrier without infiltrating the tumor in samples <bold>(A, B, E, F)</bold>.</p>
          </caption>
          <graphic xlink:href="fonc-10-586292-g004"/>
        </fig>
      </sec>
      <sec id="s3_1_3">
        <title>Neuroblastoma</title>
        <p>SuperHistopath classified the tumor regions into eight predefined tissue categories of interest: undifferentiated neuroblasts, tissue damage (necrosis/apoptosis), areas of differentiation, cluster of lymphocytes, hemorrhage, muscle, kidney, and empty/white space with an overall accuracy of 98.3%, an average precision of 98.5%, and an average recall of 98.4% using Xception and 96.8%, 97.1%, 97.2% respectively using our custom-made CNN over 9,868 superpixels in the independent test set of 16 images. The confusion matrices for the XCeption and our custom CNN networks are presented in <xref rid="T6" ref-type="table"><bold>Table 6</bold></xref> and <xref ref-type="supplementary-material" rid="ST3"><bold>Supplementary Table 3</bold></xref>, respectively. <xref ref-type="fig" rid="f5"><bold>Figure 5</bold></xref> shows qualitative results of our approach’s regional classification in representative WSIs of neuroblastoma arising in the Th-<italic>MYCN</italic> mouse model.</p>
        <table-wrap id="T6" position="float">
          <label>Table 6</label>
          <caption>
            <p>Confusion matrix of the classification of superpixels using the optimized Xception network in the Th-<italic>MYCN</italic> and <italic>Th-</italic>ALK<sup>F1174L</sup>
<italic>/</italic>MYCN mouse models in eight categories: region of undifferentiated neuroblasts, necrosis, cluster of lymphocytes (Lym), hemorrhage (blood), empty/white space, muscle tissue and kidney (separate test set of 16 whole-slide images).</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" align="left" rowspan="1" colspan="1"/>
                <th valign="top" align="center" rowspan="1" colspan="1">Undifferentiated region</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Necrosis</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Lym</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Differentiation</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Blood</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Empty space</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Muscle</th>
                <th valign="top" align="center" rowspan="1" colspan="1">Kidney</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Undifferentiated region</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>1403</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">3</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Necrosis</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">13</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>1642</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">26</td>
                <td valign="top" align="center" rowspan="1" colspan="1">49</td>
                <td valign="top" align="center" rowspan="1" colspan="1">2</td>
                <td valign="top" align="center" rowspan="1" colspan="1">5</td>
                <td valign="top" align="center" rowspan="1" colspan="1">18</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Lym</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">6</td>
                <td valign="top" align="center" rowspan="1" colspan="1">5</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>1150</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">3</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Differentiation</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>1261</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Blood</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">7</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>1327</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">9</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Empty space</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">2</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>560</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">3</td>
                <td valign="top" align="center" rowspan="1" colspan="1">2</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Muscle</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">2</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>1176</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Kidney</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">0</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>1176</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn>
              <p>Overall accuracy = 98.3%, average precision = 98.5%, average recall = 98.4%.</p>
              <p>The bold values indicate the correct predictions of the network.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <fig id="f5" position="float">
          <label>Figure 5</label>
          <caption>
            <p><bold>(A)</bold> Representative examples of the results obtained from the application of the SuperHistopath pipeline in whole-slide images of tumors (5x) arising in genetically-engineered mouse models of high-risk neuroblastoma [<bold>(B)</bold> Magnified region of interest].</p>
          </caption>
          <graphic xlink:href="fonc-10-586292-g005"/>
        </fig>
      </sec>
    </sec>
    <sec id="s3_2">
      <title>SuperHistopath Pipeline for the Analysis of Low-Resolution WSI Affords Significant Speed Advantages</title>
      <p>The average time for the SLIC superpixels algorithm to segment a WSI in 5x magnification was &lt; 2 min using a 3.5 GHz Intel core i7 processor. The average time for both the Xception and our custom-made CNN network to classify every superpixel in the images was 1–2 min using the same processor. A quick convergence of the networks (around epoch 30) was observed in all cases, which needed ~3 h for Xception and only ~30 min for our custom-made CNN using a Tesla P100-PCIE-16GB GPU card, and therefore the latter was used for experimenting.</p>
    </sec>
    <sec id="s3_3">
      <title>SuperHistopath Can Provide Robust Quantification of Clinically Relevant Features</title>
      <sec id="s3_3_1">
        <title>Stroma-to-Tumor Ratio and Clusters of Lymphocytes Abundance as Predictive Markers of Survival in Melanoma</title>
        <p>We first use SuperHistopath to quantify both the stroma-to-tumor ratio and the immune infiltrate, which have both shown to provide prognostic and predictive information in patient with solid tumors, including melanoma (<xref rid="B20" ref-type="bibr">20</xref>, <xref rid="B21" ref-type="bibr">21</xref>, <xref rid="B23" ref-type="bibr">23</xref>). The important role of immune hotspots has been established based on density analysis of single cell classification of lymphocytes in high-resolution images (<xref rid="B61" ref-type="bibr">61</xref>, <xref rid="B62" ref-type="bibr">62</xref>). Here, we demonstrate in our melanoma dataset of 127 WSIs i) that a high stromal ratio as identified in low resolution WSIs is a predictor of poor prognosis (SuperHistopath: p = 0.028, Coxph-Regression [discretized by median]: HR = 2.1, p = 0.0315; <xref ref-type="fig" rid="f6"><bold>Figure 6A</bold></xref>) and ii) that clusters of lymphocytes hold predictive information in our melanoma dataset, with a high lymphocyte ratio being an indicator of favorable prognosis [SuperHistopath: p = 0.015, Coxph-Regression (discretized by median): HR = 0.4, p = 0.018; <xref ref-type="fig" rid="f6"><bold>Figure 6B</bold></xref>]. Pearson’s correlation showed no significant correlation between stromal ratio and clusters of lymphocytes ratio (r = -0.13, p = 0.13), and between absolute sizes of stroma and clusters of lymphocytes (r = 0.13, p=0.11). Taken together, our data, captured from low resolution (5x) WSIs, are consistent with those extracted from single-cell analysis in high-resolution WSIs (<xref rid="B53" ref-type="bibr">53</xref>).</p>
        <fig id="f6" position="float">
          <label>Figure 6</label>
          <caption>
            <p>Quantification of clinically relevant features with SuperHistopath. <bold>(A, B)</bold> show associations between survival outcomes and SuperHistopath-defined risk groups in the Cancer Genome Atlas (TCGA) cohorts of patients with melanoma. <bold>(A)</bold> Kaplan-Meier Survival curves for patients in the high-risk group (blue) and low risk group (red) classified by stromal cells ratio derived from SuperHistopath and <bold>(B)</bold> Kaplan-Meier Survival curves for patients in the high-risk group (blue) and low risk group (red) classified by immune infiltrate based on lymphocytes cluster ratio derived from SuperHistopath. <bold>(C, D)</bold> show the SuperHistopath-based quantification of tumor phenotype in genetically-engineered mouse model of high-risk neuroblastoma. <bold>(C)</bold> Representative SuperHistopath-segmented whole-slide images (5x) and pie chart showing the Super-CNN quantified mean composition of the tumors arising in Th-MYCN (n=6) and Th-ALK<sup>F1174L</sup>/MYCN (n=7) mouse models of high-risk neuroblastoma. Note the marked difference of phenotype induced by the expression of the ALK<sup>F1174L</sup> mutation characterized by <bold>(D)</bold> a significantly increased neuroblastoma differentiation neuroblasts and the total abrogation of the characteristic hemorrhagic phenotype of Th-MYCN tumors.</p>
          </caption>
          <graphic xlink:href="fonc-10-586292-g006"/>
        </fig>
      </sec>
      <sec id="s3_3_2">
        <title>Necrosis Quantification</title>
        <p>We use the SuperHistopath to quantity tumor necrosis in our breast cancer and childhood neuroblastoma preclinical datasets. Tumor necrosis, defined as confluent cell death or large area of tissue damage hold predictive and prognostic information, both at diagnosis and after chemotherapy, in many solid tumors including breast cancer and childhood malignancies (<xref rid="B24" ref-type="bibr">24</xref>–<xref rid="B26" ref-type="bibr">26</xref>, <xref rid="B63" ref-type="bibr">63</xref>, <xref rid="B64" ref-type="bibr">64</xref>). While visible at 5x objective lens magnification, its quantification can often be a challenging task even for experienced pathologists. Here, we show that SuperHistopath can provide satisfactory quantification of necrosis in clinical breast cancer samples by distinguishing from stroma with high specificity (91.5%) and satisfactory precision (79.5%) and in the high-risk neuroblastoma mouse models with high precision and specificity (93.5% and 98.9% respectively).</p>
      </sec>
      <sec id="s3_3_3">
        <title>Quantification of Neuroblastoma Differentiation</title>
        <p>We used SuperHistopath to quantify the phenotype of MYCN-driven transgenic mouse models of high-risk stroma-poor neuroblastoma. We show that SuperHistopath can identify areas of differentiation, a critical feature for the stratification of children neuroblastoma, with both high precision and specificity (100% and 96.9% respectively). SuperHistopath also showed that expression of <italic>ALK<sup>F1174L</sup></italic> mutation significantly shift the MYCN-driven phenotype from poorly-differentiated and hemorrhagic phenotype (Th-<italic>MYCN</italic>: 1.8 ± 1.3% differentiating area and 29.2 ± 6.7% hemorrhage, <xref ref-type="fig" rid="f6"><bold>Figure 6C</bold></xref>) into a differentiating phenotype also characterized by the almost complete abrogation of the hemorrhagic phenotype (Th-<italic>ALK<sup>F1174L</sup></italic>/<italic>MYCN</italic>: 20.3 ± 3.1% differentiating area and 0.2 ± 0.1% hemorrhage, p=0.0003 and p=0.0008 respectively, <xref ref-type="fig" rid="f6"><bold>Figure 6D</bold></xref>) as previously demonstrated (<xref rid="B51" ref-type="bibr">51</xref>, <xref rid="B65" ref-type="bibr">65</xref>).</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>Discussion</title>
    <p>In this study, we implemented SuperHistopath: a digital pathology pipeline for the classification of tumor regions and the mapping of tumor heterogeneity from low-resolution H&amp;E-stained WSIs, which we demonstrated to be highly accurate in three types of cancer. Combining the application of the SLIC superpixels algorithm directly on low magnification WSIs (5x) with a CNN architecture for the classification of superpixels, contributes to SuperHistopath computational efficiency allowing for fast processing, whilst affording the quantification of robust and easily interpretable clinically-relevant markers.</p>
    <p>Applying our computational approach on low-resolution images leads to markedly increased processing speed, for both the classification of new samples and network training. Here, we chose the (5x) magnification as a compromise between tumor structures visibility and computational cost. Specific metrics such as stroma-to-tumor ratio could potentially be derived from images at even lower magnifications (e.g. 1.25x) as recently shown (<xref rid="B53" ref-type="bibr">53</xref>). Digital histology images are conventionally processed at 40x (or 20x) magnification where cell morphology is most visible. At those resolutions, WSIs are large (representative size at 20x: 60000 x 60000 pixels), requiring of a lot of memory and images to be divided into patches (tiles) for processing. Under these conditions, the training of new networks for cell segmentation and classification typically requires days and the application to new WSI samples can take hours prior to code optimization. In contrast, the training of our neural network until acceptable convergence needed as little as ~30 min and application on new samples ~5 min (for both superpixel segmentation and classification) in our study. High-resolution images are essential when studying cell-to-cell interactions, however we show that the processing of low resolution images is appropriate for the extraction of specific global context features.</p>
    <p>Furthermore, SuperHistopath combines the main advantages of regional classification and segmentation approaches. On one hand, classification approaches applied on smaller patches resulting from splitting WSIs allow the use of CNN for the robust classification of many categories necessary to capture intratumor heterogeneity (<xref rid="B39" ref-type="bibr">39</xref>), yet at the expense of higher risk of misclassification, especially close to regional boundaries where an image patch, regardless of its size, may contain multiple tumor components. Overlapping (sliding) window approaches can improve the issue, yet at an increased computational cost. On the other hand, segmentation approaches such as U-Net-like architectures can resolve the regional boundaries issue but appear to work better for few classes, typically two. SuperHistopath efficiently combines the use of a segmentation approach using superpixels to adhere to region boundaries with CNN classification to cover the rich tumor histological heterogeneity (here 6-8 region categories depending on the cancer type).</p>
    <p>Our method also markedly simplifies and accelerates the process of preparing ground-truth (annotations) datasets as <italic>i)</italic> the use of superpixels alleviate the need for careful boundary delineation of the tumor components of interest (<xref ref-type="fig" rid="f1"><bold>Figure 1B</bold></xref>), a cumbersome and time-consuming process necessary for using U-Net-like architectures and ii) each annotated region contains large numbers of superpixels facilitating the collection of the large datasets traditionally required by deep learning methods.</p>
    <p>The appropriate choice of superpixel size is crucial to warrant both accurate tissue segmentation and classification. Equation 1 ensured a uniform superpixel size for every whole-slide image regardless of their original size. The main considerations for choosing superpixels size (i.e. setting the constant <italic>U</italic>) is to ensure that they only contain a single tissue type, while being large enough to contain sufficient tissue information. In our study, we found that classification is not sensitive to small changes of <italic>U</italic>. However larger superpixels (<italic>U</italic> &gt; 1750) did not adhere well to the tissue boundaries, whereas smaller superpixels (<italic>U</italic> &lt; 1250) indeed led to a slight decrease in classification performance.</p>
    <p>Many promising computational pathology-derived biomarkers ultimately fail to translate in the clinic due to their inherent complexity and the difficulty for pathologists to evaluate them in new datasets. In this proof-of-concept study, we showed that SuperHistopath can quantify well-understood features/markers already used, albeit only qualitatively or semi-quantitatively, by pathologists, including the stroma-to-tumor ratio, lymphocyte infiltration, tumor necrosis, and neuroblastoma differentiation. We also show that SuperHistopath-derived results corroborated those obtained from single-cell analysis on high-resolution samples (<xref rid="B53" ref-type="bibr">53</xref>). The computational efficiency of SuperHistopath, combined with the simple superpixels-enabled data collection, could facilitate its adoption in the clinic to accelerate pathologist workflow, could assist in intra-operative pathological diagnosis and should facilitate working with large datasets in clinical research.</p>
    <p>Moving forward, we plan to expand the types of global context features extractable from SuperHistopath in more cancer types. We will also evaluate the accuracy of SuperHistopath on digitized frozen tissue sections to demonstrate its potential to assist in the rapid intra-operative pathological diagnostic. We will also update our previous framework (SuperCRF) which incorporates region classification information to improve cell classification (<xref rid="B53" ref-type="bibr">53</xref>) using SuperHistopath. Together both SuperHistopath and SuperCRF would provide invaluable tools to study spatial interactions across length scales to provide a deeper understanding of the cancer-immune-stroma interface, key to further unlock the potential of cancer immunotherapy (<xref rid="B17" ref-type="bibr">17</xref>).</p>
    <p>In this proof-of-concept study, we applied our method to three cancer types with disparate histology without any changes (just retraining). While the approach could thus be virtually extended to any type of cancer, improvement could be made tailored to a specific global feature, cancer type or dataset and could include further exploring i) the use of SVM to combine the CNN-extracted features with handcrafted ones, ii) the use of other image color spaces which has been shown to improve classification in certain cases (<xref rid="B66" ref-type="bibr">66</xref>) and iii) alternative superpixel algorithms such as the efficient topology preserving segmentation (ETPS) algorithm (<xref rid="B67" ref-type="bibr">67</xref>). Additionally, further improvement of this proof-of-concept framework could be sought <italic>via</italic> experimentation with hyperparameter tuning, or the use of other custom and well-established architectures (<xref rid="B59" ref-type="bibr">59</xref>, <xref rid="B68" ref-type="bibr">68</xref>). Since superpixels only capture small homogeneous areas, combination with other approaches such as classification of larger image patches with a deepCNN or U-net-like architectures might be more appropriate for the single purpose of segmenting some large and multi-component tumor structures, e.g. certain types of glands (<xref rid="B16" ref-type="bibr">16</xref>).</p>
    <p>To conclude, our novel pipeline, SuperHistopath can accurately classify and map the complex tumor heterogeneity from low-resolution H&amp;E-stained histology images. The resulting enhanced speed for both training and application (~5 min for classifying a WSI and as low as ~30 min for network training) and the efficient and simple collection of ground-truth datasets make SuperHistopath particularly attractive for research in rich datasets and would facilitate its adoption in the clinic to accelerate pathologist workflow in the quantification of predictive/prognosis markers derived from global features of interest.</p>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data Availability Statement</title>
    <p>The melanoma dataset comes the publicly available TCGA dataset. The neuroblastoma dataset is available from the corresponding authors upon reasonable request. The images from the triple-negative breast cancer dataset cannot be released yet due to ongoing clinical studies. The codes that support the findings of this study are available from the corresponding authors upon reasonable request.</p>
  </sec>
  <sec id="s6">
    <title>Ethics Statement</title>
    <p>The breast cancer clinical dataset was generated from diagnostic H&amp;E images provided anonymised to the researchers by the Serbian Institute of Oncology. The neuroblastoma preclinical dataset was built from H&amp;E images collected during previous in vivo studies approved by The Institute of Cancer Research Animal Welfare and Ethical Review Body and performed in accordance with the UK Home Office Animals (Scientific Procedures) Act 1986. The melanoma clinical samples come from the publicly available TCGA dataset (<xref rid="T1" ref-type="table"><bold>Table 1</bold></xref>).</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>Conception and design: KZ-P, IR, YJ, YY. Development of methodology: KZ-P Analysis and interpretation of data: KZ-P, RN, IR, YJ, YY. Administrative and/or material support: RN, DK, IR, YJ. Writing and review of the manuscript: KZ-P, IR, YJ, YY. IR, YJ, and YY are co-senior authors of this study. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-statement" id="s8">
    <title>Funding</title>
    <p>We acknowledge financial support by The Rosetrees Trust (KZ-P, M593). RN acknowledges funding from ISCIII (FIS) and FEDER (European Regional Development Fund) PI17/01558 and CB16/12/00484. YY acknowledges funding from Cancer Research UK Career Establishment Award (C45982/A21808), Breast Cancer Now (2015NovPR638), Children’s Cancer and Leukaemia Group (CCLGA201906), NIH U54 CA217376, and R01 CA185138, CDMRP Breast Cancer Research Program Award BC132057, CRUK Brain Tumor Awards (TARGET-GBM), European Commission ITN (H2020-MSCA-ITN-2019), Wellcome Trust (105104/Z/14/Z), and The Royal Marsden/ICR National Institute of Health Research Biomedical Research Centre. YJ is a Children with Cancer UK Research Fellow (2014/176). We thank Breast Cancer Now for funding IR as part of Programme Funding to the Breast Cancer Now Toby Robins Research Centre.</p>
  </sec>
  <sec sec-type="COI-statement" id="s9">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>We acknowledge Dr Snezana Susnjar’s and Dr Natasa Medic Milijic’s contribution to the creation and curation of the triple negative breast cancer dataset at The Serbian Institute of Oncology, which has made this study possible.</p>
  </ack>
  <sec id="s10" sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fonc.2020.586292/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fonc.2020.586292/full#supplementary-material</ext-link></p>
    <supplementary-material content-type="local-data" id="ST1">
      <media xlink:href="Table_1.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="ST2">
      <media xlink:href="Table_2.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="ST3">
      <media xlink:href="Table_3.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <label>1</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabesh</surname><given-names>A</given-names></name><name><surname>Teverovskiy</surname><given-names>M</given-names></name><name><surname>Pang</surname><given-names>H-Y</given-names></name><name><surname>Kumar</surname><given-names>VP</given-names></name><name><surname>Verbel</surname><given-names>D</given-names></name><name><surname>Kotsianti</surname><given-names>A</given-names></name><etal/></person-group><article-title>Multifeature prostate cancer diagnosis and Gleason grading of histological images</article-title>. <source>IEEE Trans Med Imaging</source> (<year>2007</year>) <volume>26</volume>(<issue>10</issue>):<page-range>1366–78</page-range>. <pub-id pub-id-type="doi">10.1109/TMI.2007.898536</pub-id>
</mixed-citation>
    </ref>
    <ref id="B2">
      <label>2</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madabhushi</surname><given-names>A</given-names></name></person-group><article-title>Digital pathology image analysis: opportunities and challenges</article-title>. <source>Imaging Med</source> (<year>2009</year>) <volume>1</volume>(<issue>1</issue>):<fpage>7</fpage>. <pub-id pub-id-type="doi">10.2217/iim.09.9</pub-id>
<pub-id pub-id-type="pmid">30147749</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>R</given-names></name><name><surname>Srivastava</surname><given-names>R</given-names></name><name><surname>Srivastava</surname><given-names>S</given-names></name></person-group><article-title>Detection and classification of cancer from microscopic biopsy images using clinically significant and biologically interpretable features</article-title>. <source>J Med Eng</source> (<year>2015</year>) <volume>2015</volume>. <pub-id pub-id-type="doi">10.1155/2015/457906</pub-id>
</mixed-citation>
    </ref>
    <ref id="B4">
      <label>4</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allard</surname><given-names>FD</given-names></name><name><surname>Goldsmith</surname><given-names>JD</given-names></name><name><surname>Ayata</surname><given-names>G</given-names></name><name><surname>Challies</surname><given-names>TL</given-names></name><name><surname>Najarian</surname><given-names>RM</given-names></name><name><surname>Nasser</surname><given-names>IA</given-names></name><etal/></person-group><article-title>Intraobserver and interobserver variability in the assessment of dysplasia in ampullary mucosal biopsies</article-title>. <source>Am J Surg Pathol</source> (<year>2018</year>) <volume>42</volume>(<issue>8</issue>):<page-range>1095–100</page-range>. <pub-id pub-id-type="doi">10.1097/PAS.0000000000001079</pub-id>
</mixed-citation>
    </ref>
    <ref id="B5">
      <label>5</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomes</surname><given-names>DS</given-names></name><name><surname>Porto</surname><given-names>SS</given-names></name><name><surname>Balabram</surname><given-names>D</given-names></name><name><surname>Gobbi</surname><given-names>H</given-names></name></person-group><article-title>Inter-observer variability between general pathologists and a specialist in breast pathology in the diagnosis of lobular neoplasia, columnar cell lesions, atypical ductal hyperplasia and ductal carcinoma in situ of the breast</article-title>. <source>Diagn Pathol</source> (<year>2014</year>) <volume>9</volume>(<issue>1</issue>):<fpage>121</fpage>. <pub-id pub-id-type="doi">10.1186/1746-1596-9-121</pub-id>
<pub-id pub-id-type="pmid">24948027</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krupinski</surname><given-names>EA</given-names></name><name><surname>Tillack</surname><given-names>AA</given-names></name><name><surname>Richter</surname><given-names>L</given-names></name><name><surname>Henderson</surname><given-names>JT</given-names></name><name><surname>Bhattacharyya</surname><given-names>AK</given-names></name><name><surname>Scott</surname><given-names>KM</given-names></name><etal/></person-group><article-title>Eye-movement study and human performance using telepathology virtual slides. Implications for medical education and differences with experience</article-title>. <source>Hum Pathol</source> (<year>2006</year>) <volume>37</volume>(<issue>12</issue>):<page-range>1543–56</page-range>. <pub-id pub-id-type="doi">10.1016/j.humpath.2006.08.024</pub-id>
</mixed-citation>
    </ref>
    <ref id="B7">
      <label>7</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mukhopadhyay</surname><given-names>S</given-names></name><name><surname>Feldman</surname><given-names>MD</given-names></name><name><surname>Abels</surname><given-names>E</given-names></name><name><surname>Ashfaq</surname><given-names>R</given-names></name><name><surname>Beltaifa</surname><given-names>S</given-names></name><name><surname>Cacciabeve</surname><given-names>NG</given-names></name><etal/></person-group><article-title>Whole slide imaging versus microscopy for primary diagnosis in surgical pathology: a multicenter blinded randomized noninferiority study of 1992 cases (pivotal study)</article-title>. <source>Am J Surg Pathol</source> (<year>2018</year>) <volume>42</volume>(<issue>1</issue>):<fpage>39</fpage>. <pub-id pub-id-type="doi">10.1097/PAS.0000000000000948</pub-id>
<pub-id pub-id-type="pmid">28961557</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kothari</surname><given-names>S</given-names></name><name><surname>Phan</surname><given-names>JH</given-names></name><name><surname>Stokes</surname><given-names>TH</given-names></name><name><surname>Wang</surname><given-names>MD</given-names></name></person-group><article-title>Pathology imaging informatics for quantitative analysis of whole-slide images</article-title>. <source>J Am Med Inform Assoc</source> (<year>2013</year>) <volume>20</volume>(<issue>6</issue>):<page-range>1099–108</page-range>. <pub-id pub-id-type="doi">10.1136/amiajnl-2012-001540</pub-id>
</mixed-citation>
    </ref>
    <ref id="B9">
      <label>9</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campanella</surname><given-names>G</given-names></name><name><surname>Hanna</surname><given-names>MG</given-names></name><name><surname>Geneslaw</surname><given-names>L</given-names></name><name><surname>Miraflor</surname><given-names>A</given-names></name><name><surname>Silva</surname><given-names>VWK</given-names></name><name><surname>Busam</surname><given-names>KJ</given-names></name><etal/></person-group><article-title>Clinical-grade computational pathology using weakly supervised deep learning on whole slide images</article-title>. <source>Nat Med</source> (<year>2019</year>) <volume>25</volume>(<issue>8</issue>):<page-range>1301–9</page-range>. <pub-id pub-id-type="doi">10.1038/s41591-019-0508-1</pub-id>
</mixed-citation>
    </ref>
    <ref id="B10">
      <label>10</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>TR</given-names></name><name><surname>Kang</surname><given-names>IH</given-names></name><name><surname>Wheeler</surname><given-names>DB</given-names></name><name><surname>Lindquist</surname><given-names>RA</given-names></name><name><surname>Papallo</surname><given-names>A</given-names></name><name><surname>Sabatini</surname><given-names>DM</given-names></name><etal/></person-group><article-title>CellProfiler Analyst: data exploration and analysis software for complex image-based screens</article-title>. <source>BMC Bioinf</source> (<year>2008</year>) <volume>9</volume>(<issue>1</issue>):<fpage>482</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-9-482</pub-id>
</mixed-citation>
    </ref>
    <ref id="B11">
      <label>11</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>Y</given-names></name><name><surname>Failmezger</surname><given-names>H</given-names></name><name><surname>Rueda</surname><given-names>OM</given-names></name><name><surname>Ali</surname><given-names>HR</given-names></name><name><surname>Gräf</surname><given-names>S</given-names></name><name><surname>Chin</surname><given-names>S-F</given-names></name><etal/></person-group><article-title>Quantitative image analysis of cellular heterogeneity in breast tumors complements genomic profiling</article-title>. <source>Sci Transl Med</source> (<year>2012</year>) <volume>4</volume>(<issue>157</issue>):<page-range>157ra43–ra43</page-range>. <pub-id pub-id-type="doi">10.1126/scitranslmed.3004330</pub-id>
</mixed-citation>
    </ref>
    <ref id="B12">
      <label>12</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>CL</given-names></name><name><surname>Mahjoubfar</surname><given-names>A</given-names></name><name><surname>Tai</surname><given-names>L-C</given-names></name><name><surname>Blaby</surname><given-names>IK</given-names></name><name><surname>Huang</surname><given-names>A</given-names></name><name><surname>Niazi</surname><given-names>KR</given-names></name><etal/></person-group><article-title>Deep learning in label-free cell classification</article-title>. <source>Sci Rep</source> (<year>2016</year>) <volume>6</volume>:<fpage>21471</fpage>. <pub-id pub-id-type="doi">10.1038/srep21471</pub-id>
<pub-id pub-id-type="pmid">26975219</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <label>13</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sirinukunwattana</surname><given-names>K</given-names></name><name><surname>Raza</surname><given-names>SEA</given-names></name><name><surname>Tsang</surname><given-names>Y-W</given-names></name><name><surname>Snead</surname><given-names>DR</given-names></name><name><surname>Cree</surname><given-names>IA</given-names></name><name><surname>Rajpoot</surname><given-names>NM</given-names></name></person-group><article-title>Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images</article-title>. <source>IEEE Trans Med Imaging</source> (<year>2016</year>) <volume>35</volume>(<issue>5</issue>):<page-range>1196–206</page-range>. <pub-id pub-id-type="doi">10.1109/TMI.2016.2525803</pub-id>
</mixed-citation>
    </ref>
    <ref id="B14">
      <label>14</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bankhead</surname><given-names>P</given-names></name><name><surname>Loughrey</surname><given-names>MB</given-names></name><name><surname>Fernández</surname><given-names>JA</given-names></name><name><surname>Dombrowski</surname><given-names>Y</given-names></name><name><surname>McArt</surname><given-names>DG</given-names></name><name><surname>Dunne</surname><given-names>PD</given-names></name><etal/></person-group><article-title>QuPath: Open source software for digital pathology image analysis</article-title>. <source>Sci Rep</source> (<year>2017</year>) <volume>7</volume>(<issue>1</issue>):<fpage>16878</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-017-17204-5</pub-id>
<pub-id pub-id-type="pmid">29203879</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <label>15</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Khoshdeli</surname><given-names>M</given-names></name><name><surname>Cong</surname><given-names>R</given-names></name><name><surname>Parvin</surname><given-names>B</given-names></name></person-group> eds. “<article-title>Detection of nuclei in H&amp;E stained sections using convolutional neural networks. Biomedical &amp; Health Informatics (BHI)</article-title>”. In: <source>2017 IEEE EMBS International Conference on</source>. <publisher-loc>New York, US</publisher-loc>: <publisher-name>IEEE</publisher-name>. <pub-id pub-id-type="doi">10.1109/BHI.2017.7897216</pub-id>
</mixed-citation>
    </ref>
    <ref id="B16">
      <label>16</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raza</surname><given-names>SEA</given-names></name><name><surname>Cheung</surname><given-names>L</given-names></name><name><surname>Shaban</surname><given-names>M</given-names></name><name><surname>Graham</surname><given-names>S</given-names></name><name><surname>Epstein</surname><given-names>D</given-names></name><name><surname>Pelengaris</surname><given-names>S</given-names></name><etal/></person-group><article-title>Micro-Net: A unified model for segmentation of various objects in microscopy images</article-title>. <source>Med Image Anal</source> (<year>2019</year>) <volume>52</volume>:<page-range>160–73</page-range>. <pub-id pub-id-type="doi">10.1016/j.media.2018.12.003</pub-id>
</mixed-citation>
    </ref>
    <ref id="B17">
      <label>17</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Komura</surname><given-names>D</given-names></name><name><surname>Ishikawa</surname><given-names>S</given-names></name></person-group><article-title>Machine learning methods for histopathological image analysis</article-title>. <source>Comput Struct Biotechnol J</source> (<year>2018</year>) <volume>16</volume>:<fpage>34</fpage>–<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1016/j.csbj.2018.01.001</pub-id>
<pub-id pub-id-type="pmid">30275936</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <label>18</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humphrey</surname><given-names>PA</given-names></name><name><surname>Moch</surname><given-names>H</given-names></name><name><surname>Cubilla</surname><given-names>AL</given-names></name><name><surname>Ulbright</surname><given-names>TM</given-names></name><name><surname>Reuter</surname><given-names>VE</given-names></name></person-group><article-title>The 2016 WHO classification of tumors of the urinary system and male genital organs—part B: prostate and bladder tumors</article-title>. <source>Eur Urol</source> (<year>2016</year>) <volume>70</volume>(<issue>1</issue>):<page-range>106–19</page-range>. <pub-id pub-id-type="doi">10.1016/j.eururo.2016.02.028</pub-id>
</mixed-citation>
    </ref>
    <ref id="B19">
      <label>19</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rakha</surname><given-names>EA</given-names></name><name><surname>Reis-Filho</surname><given-names>JS</given-names></name><name><surname>Baehner</surname><given-names>F</given-names></name><name><surname>Dabbs</surname><given-names>DJ</given-names></name><name><surname>Decker</surname><given-names>T</given-names></name><name><surname>Eusebi</surname><given-names>V</given-names></name><etal/></person-group><article-title>Breast cancer prognostic classification in the molecular era: the role of histological grade</article-title>. <source>Breast Cancer Res</source> (<year>2010</year>) <volume>12</volume>(<issue>4</issue>):<fpage>207</fpage>. <pub-id pub-id-type="doi">10.1186/bcr2607</pub-id>
<pub-id pub-id-type="pmid">20804570</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <label>20</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Yu</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Tan</surname><given-names>B</given-names></name><etal/></person-group><article-title>Tumor-stroma ratio is an independent predictor for survival in esophageal squamous cell carcinoma</article-title>. <source>J Thorac Oncol</source> (<year>2012</year>) <volume>7</volume>(<issue>9</issue>):<page-range>1457–61</page-range>. <pub-id pub-id-type="doi">10.1097/JTO.0b013e318260dfe8</pub-id>
</mixed-citation>
    </ref>
    <ref id="B21">
      <label>21</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name></person-group><article-title>Prognostic significance of the tumor-stroma ratio in epithelial ovarian cancer</article-title>. <source>BioMed Res Int</source> (<year>2015</year>) <volume>2015</volume>. <pub-id pub-id-type="doi">10.1155/2015/589301</pub-id>
</mixed-citation>
    </ref>
    <ref id="B22">
      <label>22</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruan</surname><given-names>M</given-names></name><name><surname>Tian</surname><given-names>T</given-names></name><name><surname>Rao</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>X</given-names></name><name><surname>Yu</surname><given-names>B</given-names></name><name><surname>Yang</surname><given-names>W</given-names></name><etal/></person-group><article-title>Predictive value of tumor-infiltrating lymphocytes to pathological complete response in neoadjuvant treated triple-negative breast cancers</article-title>. <source>Diagn Pathol</source> (<year>2018</year>) <volume>13</volume>(<issue>1</issue>):<fpage>66</fpage>. <pub-id pub-id-type="doi">10.1186/s13000-018-0743-7</pub-id>
<pub-id pub-id-type="pmid">30170605</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <label>23</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnes</surname><given-names>TA</given-names></name><name><surname>Amir</surname><given-names>E</given-names></name></person-group><article-title>HYPE or HOPE: the prognostic value of infiltrating immune cells in cancer</article-title>. <source>Br J Cancer</source> (<year>2017</year>) <volume>117</volume>(<issue>4</issue>):<page-range>451–60</page-range>. <pub-id pub-id-type="doi">10.1038/bjc.2017.220</pub-id>
</mixed-citation>
    </ref>
    <ref id="B24">
      <label>24</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renshaw</surname><given-names>AA</given-names></name><name><surname>Cheville</surname><given-names>JC</given-names></name></person-group><article-title>Quantitative tumor necrosis is an independent predictor of overall survival in clear cell renal cell carcinoma</article-title>. <source>Pathology</source> (<year>2015</year>) <volume>47</volume>(<issue>1</issue>):<page-range>34–7</page-range>. <pub-id pub-id-type="doi">10.1097/PAT.0000000000000193</pub-id>
</mixed-citation>
    </ref>
    <ref id="B25">
      <label>25</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichler</surname><given-names>M</given-names></name><name><surname>Hutterer</surname><given-names>GC</given-names></name><name><surname>Chromecki</surname><given-names>TF</given-names></name><name><surname>Jesche</surname><given-names>J</given-names></name><name><surname>Kampel-Kettner</surname><given-names>K</given-names></name><name><surname>Rehak</surname><given-names>P</given-names></name><etal/></person-group><article-title>Histologic tumor necrosis is an independent prognostic indicator for clear cell and papillary renal cell carcinoma</article-title>. <source>Am J Clin Pathol</source> (<year>2012</year>) <volume>137</volume>(<issue>2</issue>):<page-range>283–9</page-range>. <pub-id pub-id-type="doi">10.1309/AJCPLBK9L9KDYQZP</pub-id>
</mixed-citation>
    </ref>
    <ref id="B26">
      <label>26</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bredholt</surname><given-names>G</given-names></name><name><surname>Mannelqvist</surname><given-names>M</given-names></name><name><surname>Stefansson</surname><given-names>IM</given-names></name><name><surname>Birkeland</surname><given-names>E</given-names></name><name><surname>Bø</surname><given-names>TH</given-names></name><name><surname>Øyan</surname><given-names>AM</given-names></name><etal/></person-group><article-title>Tumor necrosis is an important hallmark of aggressive endometrial cancer and associates with hypoxia, angiogenesis and inflammation responses</article-title>. <source>Oncotarget</source> (<year>2015</year>) <volume>6</volume>(<issue>37</issue>):<fpage>39676</fpage>. <pub-id pub-id-type="doi">10.18632/oncotarget.5344</pub-id>
<pub-id pub-id-type="pmid">26485755</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <label>27</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group> eds. “<article-title>U-net: Convolutional networks for biomedical image segmentation</article-title>”. In: <source>International Conference on Medical image computing and computer-assisted intervention</source>. <publisher-loc>New York, US</publisher-loc>: <publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B28">
      <label>28</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>D</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Gargeya</surname><given-names>R</given-names></name><name><surname>Irshad</surname><given-names>H</given-names></name><name><surname>Beck</surname><given-names>AH</given-names></name></person-group> “<article-title>Deep learning for identifying metastatic breast cancer</article-title>”. arXiv preprint (<year>2016</year>) arXiv:160605718.</mixed-citation>
    </ref>
    <ref id="B29">
      <label>29</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nahid</surname><given-names>A-A</given-names></name><name><surname>Mehrabi</surname><given-names>MA</given-names></name><name><surname>Kong</surname><given-names>Y</given-names></name></person-group><article-title>Histopathological breast cancer image classification by deep neural network techniques guided by local clustering</article-title>. <source>BioMed Res Int</source> (<year>2018</year>) <volume>2018</volume>. <pub-id pub-id-type="doi">10.1155/2018/2362108</pub-id>
</mixed-citation>
    </ref>
    <ref id="B30">
      <label>30</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bándi</surname><given-names>P</given-names></name><name><surname>van de Loo</surname><given-names>R</given-names></name><name><surname>Intezar</surname><given-names>M</given-names></name><name><surname>Geijs</surname><given-names>D</given-names></name><name><surname>Ciompi</surname><given-names>F</given-names></name><name><surname>van Ginneken</surname><given-names>B</given-names></name><etal/></person-group> eds. “<article-title>Comparison of different methods for tissue segmentation in histopathological whole-slide images</article-title>”. In: <source>2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)</source>. <publisher-loc>New York, US</publisher-loc>: <publisher-name>IEEE</publisher-name> (<year>2017</year>). <pub-id pub-id-type="doi">10.1109/ISBI.2017.7950590</pub-id>
</mixed-citation>
    </ref>
    <ref id="B31">
      <label>31</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bejnordi</surname><given-names>BE</given-names></name><name><surname>Zuidhof</surname><given-names>G</given-names></name><name><surname>Balkenhol</surname><given-names>M</given-names></name><name><surname>Hermsen</surname><given-names>M</given-names></name><name><surname>Bult</surname><given-names>P</given-names></name><name><surname>van Ginneken</surname><given-names>B</given-names></name><etal/></person-group><article-title>Context-aware stacked convolutional neural networks for classification of breast carcinomas in whole-slide histopathology images</article-title>. <source>J Med Imaging</source> (<year>2017</year>) <volume>4</volume>(<issue>4</issue>):<fpage>044504</fpage>. <pub-id pub-id-type="doi">10.1117/1.JMI.4.4.044504</pub-id>
</mixed-citation>
    </ref>
    <ref id="B32">
      <label>32</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vu</surname><given-names>QD</given-names></name><name><surname>Graham</surname><given-names>S</given-names></name><name><surname>Kurc</surname><given-names>T</given-names></name><name><surname>To</surname><given-names>MNN</given-names></name><name><surname>Shaban</surname><given-names>M</given-names></name><name><surname>Qaiser</surname><given-names>T</given-names></name><etal/></person-group><article-title>Methods for segmentation and classification of digital microscopy tissue images</article-title>. <source>Front Bioeng Biotechnol</source> (<year>2019</year>) <volume>7</volume>:<elocation-id>53</elocation-id>. <pub-id pub-id-type="doi">10.3389/fbioe.2019.00053</pub-id>
<pub-id pub-id-type="pmid">31001524</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <label>33</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Araújo</surname><given-names>T</given-names></name><name><surname>Aresta</surname><given-names>G</given-names></name><name><surname>Castro</surname><given-names>E</given-names></name><name><surname>Rouco</surname><given-names>J</given-names></name><name><surname>Aguiar</surname><given-names>P</given-names></name><name><surname>Eloy</surname><given-names>C</given-names></name><etal/></person-group><article-title>Classification of breast cancer histology images using convolutional neural networks</article-title>. <source>PLoS One</source> (<year>2017</year>) <volume>12</volume>(<issue>6</issue>):<fpage>e0177544</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0177544</pub-id>
<pub-id pub-id-type="pmid">28570557</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <label>34</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wetteland</surname><given-names>R</given-names></name><name><surname>Engan</surname><given-names>K</given-names></name><name><surname>Eftestøl</surname><given-names>T</given-names></name><name><surname>Kvikstad</surname><given-names>V</given-names></name><name><surname>Janssen</surname><given-names>EA</given-names></name></person-group> eds. “<article-title>Multiclass tissue classification of whole-slide histological images using convolutional neural networks</article-title>”. In: <source>Proceedings of the 8th International Conference on Pattern Recognition Applications and Methods</source>. <publisher-loc>New York, US</publisher-loc>: <publisher-name>Springer</publisher-name> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="B35">
      <label>35</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Moro</surname><given-names>CF</given-names></name><name><surname>Kuznyecov</surname><given-names>D</given-names></name><name><surname>Bozóky</surname><given-names>B</given-names></name><name><surname>Dong</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>Q</given-names></name></person-group> eds. “<article-title>Tissue Region Growing for Hispathology Image Segmentation</article-title>”. In: <source>Proceedings of the 2018 3rd International Conference on Biomedical Imaging, Signal Processing</source>. <publisher-loc>New York, US</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name> (<year>2018</year>). <pub-id pub-id-type="doi">10.1145/3288200.3288213</pub-id>
</mixed-citation>
    </ref>
    <ref id="B36">
      <label>36</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>L</given-names></name><name><surname>Hosseini</surname><given-names>MS</given-names></name><name><surname>Rowsell</surname><given-names>C</given-names></name><name><surname>Plataniotis</surname><given-names>KN</given-names></name><name><surname>Damaskinos</surname><given-names>S</given-names></name></person-group> eds. “<article-title>Histosegnet: Semantic segmentation of histological tissue type in whole slide images</article-title>”. In: <source>Proceedings of the IEEE International Conference on Computer Vision</source>. <publisher-loc>New York, US</publisher-loc>: <publisher-name>IEEE</publisher-name> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="B37">
      <label>37</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L-C</given-names></name><name><surname>Papandreou</surname><given-names>G</given-names></name><name><surname>Kokkinos</surname><given-names>I</given-names></name><name><surname>Murphy</surname><given-names>K</given-names></name><name><surname>Yuille</surname><given-names>AL</given-names></name></person-group><article-title>Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</article-title>. <source>IEEE Trans Pattern Anal Mach Intelligence</source> (<year>2018</year>) <volume>40</volume>(<issue>4</issue>):<page-range>834–48</page-range>. <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id>
</mixed-citation>
    </ref>
    <ref id="B38">
      <label>38</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Jia</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>L-B</given-names></name><name><surname>Ai</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>Lai</surname><given-names>M</given-names></name><etal/></person-group><article-title>Large scale tissue histopathology image classification, segmentation, and visualization via deep convolutional activation features</article-title>. <source>BMC Bioinf</source> (<year>2017</year>) <volume>18</volume>(<issue>1</issue>):<fpage>281</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-017-1685-x</pub-id>
</mixed-citation>
    </ref>
    <ref id="B39">
      <label>39</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group> eds. “<article-title>Imagenet classification with deep convolutional neural networks</article-title>”. In: <source>Adv Neural Inf Process Syst</source>. <publisher-loc>Cambridge, Massachusetts, US</publisher-loc>: <publisher-name>MIT Press</publisher-name> (<year>2012</year>).</mixed-citation>
    </ref>
    <ref id="B40">
      <label>40</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Ni</surname><given-names>D</given-names></name><name><surname>Lei</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name></person-group><article-title>Accurate segmentation of cervical cytoplasm and nuclei based on multiscale convolutional network and graph partitioning</article-title>. <source>IEEE Trans BioMed Eng</source> (<year>2015</year>) <volume>62</volume>(<issue>10</issue>):<page-range>2421–33</page-range>. <pub-id pub-id-type="doi">10.1109/TBME.2015.2430895</pub-id>
</mixed-citation>
    </ref>
    <ref id="B41">
      <label>41</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Romo</surname><given-names>D</given-names></name><name><surname>García-Arteaga</surname><given-names>JD</given-names></name><name><surname>Arbeláez</surname><given-names>P</given-names></name><name><surname>Romero</surname><given-names>E</given-names></name></person-group> eds. “<article-title>A discriminant multi-scale histopathology descriptor using dictionary learning</article-title>”. In: <source>Medical Imaging 2014: Digital Pathology; 2014: International Society for Optics and Photonics</source>. <publisher-loc>Bellingham, Washington USA</publisher-loc>: <publisher-name>SPIE</publisher-name> (<year>2014</year>). <pub-id pub-id-type="doi">10.1117/12.2043935</pub-id>
</mixed-citation>
    </ref>
    <ref id="B42">
      <label>42</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Zeng</surname><given-names>J</given-names></name><name><surname>Chai</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><article-title>Large-scale tissue histopathology image segmentation based on feature pyramid</article-title>. <source>EURASIP J Image Video Processing</source> (<year>2018</year>) <volume>2018</volume>(<issue>1</issue>):<fpage>75</fpage>. <pub-id pub-id-type="doi">10.1186/s13640-018-0320-8</pub-id>
</mixed-citation>
    </ref>
    <ref id="B43">
      <label>43</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>Q</given-names></name></person-group> eds. “<article-title>Multi-scale context-aware networks for quantitative assessment of colorectal liver metastases</article-title>”. In: <source>2018 IEEE EMBS International Conference on Biomedical &amp; Health Informatics (BHI)</source>. <publisher-loc>New York, US</publisher-loc>: <publisher-name>IEEE</publisher-name> (<year>2018</year>). <pub-id pub-id-type="doi">10.1109/BHI.2018.8333445</pub-id>
</mixed-citation>
    </ref>
    <ref id="B44">
      <label>44</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>AH</given-names></name><name><surname>Sangoi</surname><given-names>AR</given-names></name><name><surname>Leung</surname><given-names>S</given-names></name><name><surname>Marinelli</surname><given-names>RJ</given-names></name><name><surname>Nielsen</surname><given-names>TO</given-names></name><name><surname>Van De Vijver</surname><given-names>MJ</given-names></name><etal/></person-group><article-title>Systematic analysis of breast cancer morphology uncovers stromal features associated with survival</article-title>. <source>Sci Transl Med</source> (<year>2011</year>) <volume>3</volume>(<issue>108</issue>):<page-range>108ra13–ra13</page-range>. <pub-id pub-id-type="doi">10.1126/scitranslmed.3002564</pub-id>
</mixed-citation>
    </ref>
    <ref id="B45">
      <label>45</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Luo</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>G</given-names></name><name><surname>Gilmore</surname><given-names>H</given-names></name><name><surname>Madabhushi</surname><given-names>A</given-names></name></person-group><article-title>A deep convolutional neural network for segmenting and classifying epithelial and stromal regions in histopathological images</article-title>. <source>Neurocomputing</source> (<year>2016</year>) <volume>191</volume>:<page-range>214–23</page-range>. <pub-id pub-id-type="doi">10.1016/j.neucom.2016.01.034</pub-id>
</mixed-citation>
    </ref>
    <ref id="B46">
      <label>46</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bejnordi</surname><given-names>BE</given-names></name><name><surname>Litjens</surname><given-names>G</given-names></name><name><surname>Hermsen</surname><given-names>M</given-names></name><name><surname>Karssemeijer</surname><given-names>N</given-names></name><name><surname>van der Laak</surname><given-names>JA</given-names></name></person-group> eds. “<article-title>A multi-scale superpixel classification approach to the detection of regions of interest in whole slide histopathology images</article-title>”. In: <source>Medical Imaging 2015: Digital Pathology; 2015: International Society for Optics and Photonics</source>. <publisher-loc>Bellingham, Washington USA</publisher-loc>: <publisher-name>SPIE</publisher-name> (<year>2015</year>). <pub-id pub-id-type="doi">10.1117/12.2081768</pub-id>
</mixed-citation>
    </ref>
    <ref id="B47">
      <label>47</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Achanta</surname><given-names>R</given-names></name><name><surname>Shaji</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>K</given-names></name><name><surname>Lucchi</surname><given-names>A</given-names></name><name><surname>Fua</surname><given-names>P</given-names></name><name><surname>Süsstrunk</surname><given-names>S</given-names></name></person-group><article-title>SLIC superpixels compared to state-of-the-art superpixel methods</article-title>. <source>IEEE Trans Pattern Anal Mach Intelligence</source> (<year>2012</year>) <volume>34</volume>(<issue>11</issue>):<page-range>2274–82</page-range>. <pub-id pub-id-type="doi">10.1109/TPAMI.2012.120</pub-id>
</mixed-citation>
    </ref>
    <ref id="B48">
      <label>48</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brockmann</surname><given-names>M</given-names></name><name><surname>Poon</surname><given-names>E</given-names></name><name><surname>Berry</surname><given-names>T</given-names></name><name><surname>Carstensen</surname><given-names>A</given-names></name><name><surname>Deubzer</surname><given-names>HE</given-names></name><name><surname>Rycak</surname><given-names>L</given-names></name><etal/></person-group><article-title>Small molecule inhibitors of aurora-a induce proteasomal degradation of N-myc in childhood neuroblastoma</article-title>. <source>Cancer Cell</source> (<year>2013</year>) <volume>24</volume>(<issue>1</issue>):<fpage>75</fpage>–<lpage>89</lpage>. <pub-id pub-id-type="doi">10.1016/j.ccr.2013.05.005</pub-id>
<pub-id pub-id-type="pmid">23792191</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <label>49</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berry</surname><given-names>T</given-names></name><name><surname>Luther</surname><given-names>W</given-names></name><name><surname>Bhatnagar</surname><given-names>N</given-names></name><name><surname>Jamin</surname><given-names>Y</given-names></name><name><surname>Poon</surname><given-names>E</given-names></name><name><surname>Sanda</surname><given-names>T</given-names></name><etal/></person-group><article-title>The ALK(F1174L) mutation potentiates the oncogenic activity of MYCN in neuroblastoma</article-title>. <source>Cancer Cell</source> (<year>2012</year>) <volume>22</volume>(<issue>1</issue>):<page-range>117–30</page-range>. <pub-id pub-id-type="doi">10.1016/j.ccr.2012.06.001</pub-id>
</mixed-citation>
    </ref>
    <ref id="B50">
      <label>50</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>HC</given-names></name><name><surname>Wood</surname><given-names>KM</given-names></name><name><surname>Jackson</surname><given-names>MS</given-names></name><name><surname>Lastowska</surname><given-names>MA</given-names></name><name><surname>Hall</surname><given-names>D</given-names></name><name><surname>Imrie</surname><given-names>H</given-names></name><etal/></person-group><article-title>Histological profile of tumors from MYCN transgenic mice</article-title>. <source>J Clin Pathol</source> (<year>2008</year>) <volume>61</volume>(<issue>10</issue>):<page-range>1098–103</page-range>. <pub-id pub-id-type="doi">10.1136/jcp.2007.054627</pub-id>
</mixed-citation>
    </ref>
    <ref id="B51">
      <label>51</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jamin</surname><given-names>Y</given-names></name><name><surname>Glass</surname><given-names>L</given-names></name><name><surname>Hallsworth</surname><given-names>A</given-names></name><name><surname>George</surname><given-names>R</given-names></name><name><surname>Koh</surname><given-names>DM</given-names></name><name><surname>Pearson</surname><given-names>AD</given-names></name><etal/></person-group><article-title>Intrinsic susceptibility MRI identifies tumors with ALKF1174L mutation in genetically-engineered murine models of high-risk neuroblastoma</article-title>. <source>PLoS One</source> (<year>2014</year>) <volume>9</volume>(<issue>3</issue>):<fpage>e92886</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0092886</pub-id>
<pub-id pub-id-type="pmid">24667968</pub-id></mixed-citation>
    </ref>
    <ref id="B52">
      <label>52</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinhard</surname><given-names>E</given-names></name><name><surname>Adhikhmin</surname><given-names>M</given-names></name><name><surname>Gooch</surname><given-names>B</given-names></name><name><surname>Shirley</surname><given-names>P</given-names></name></person-group><article-title>Color transfer between images</article-title>. <source>IEEE Comput Graphics Applications</source> (<year>2001</year>) <volume>21</volume>(<issue>5</issue>):<fpage>34</fpage>–<lpage>41</lpage>. <pub-id pub-id-type="doi">10.1109/38.946629</pub-id>
</mixed-citation>
    </ref>
    <ref id="B53">
      <label>53</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zormpas-Petridis</surname><given-names>K</given-names></name><name><surname>Failmezger</surname><given-names>H</given-names></name><name><surname>Raza</surname><given-names>SEA</given-names></name><name><surname>Roxanis</surname><given-names>I</given-names></name><name><surname>Jamin</surname><given-names>Y</given-names></name><name><surname>Yuan</surname><given-names>Y</given-names></name></person-group><article-title>Superpixel-based Conditional Random Fields (SuperCRF): Incorporating global and local context for enhanced deep learning in melanoma histopathology</article-title>. <source>Front Oncol</source> (<year>2019</year>) <volume>9</volume>:<elocation-id>1045</elocation-id>. <pub-id pub-id-type="doi">10.3389/fonc.2019.01045</pub-id>
<pub-id pub-id-type="pmid">31681583</pub-id></mixed-citation>
    </ref>
    <ref id="B54">
      <label>54</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name></person-group><article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>. <source>J Mach Learn Res</source> (<year>2014</year>) <volume>15</volume>(<issue>1</issue>):<page-range>1929–58</page-range>. <pub-id pub-id-type="doi">10.5555/2627435.2670313</pub-id>
</mixed-citation>
    </ref>
    <ref id="B55">
      <label>55</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Glorot</surname><given-names>X</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group> eds. “<article-title>Understanding the difficulty of training deep feedforward neural networks</article-title>”. In: <source>Proceedings of the thirteenth international conference on artificial intelligence and statistics</source>. <publisher-name>Proceedings of Machine Learning Research (PMLR)</publisher-name> (<year>2010</year>).</mixed-citation>
    </ref>
    <ref id="B56">
      <label>56</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><article-title>Adam: A method for stochastic optimization</article-title>. <source>arXiv preprint</source> (<year>2014</year>) arXiv:14126980.</mixed-citation>
    </ref>
    <ref id="B57">
      <label>57</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Wojna</surname><given-names>Z</given-names></name></person-group> eds. “<article-title>Rethinking the inception architecture for computer vision</article-title>”. In: <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>. <publisher-loc>New York, US</publisher-loc>: <publisher-name>IEEE</publisher-name> (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="B58">
      <label>58</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chollet</surname><given-names>F</given-names></name></person-group> ed. “<article-title>Xception: Deep learning with depthwise separable convolutions</article-title>”. In: <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>. <publisher-loc>New York, US</publisher-loc>: <publisher-name>IEEE</publisher-name> (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="B59">
      <label>59</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Alemi</surname><given-names>AA</given-names></name></person-group> eds. “<article-title>Inception-v4, inception-resnet and the impact of residual connections on learning</article-title>”. In: <source>Thirty-First AAAI Conference on Artificial Intelligence</source>. <publisher-loc>California, US</publisher-loc>: <publisher-name>AAAI</publisher-name> (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="B60">
      <label>60</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group> eds. “<article-title>Deep residual learning for image recognition</article-title>”. In: <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>. <publisher-loc>New York, US</publisher-loc>: <publisher-name>IEEE</publisher-name> (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="B61">
      <label>61</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heindl</surname><given-names>A</given-names></name><name><surname>Sestak</surname><given-names>I</given-names></name><name><surname>Naidoo</surname><given-names>K</given-names></name><name><surname>Cuzick</surname><given-names>J</given-names></name><name><surname>Dowsett</surname><given-names>M</given-names></name><name><surname>Yuan</surname><given-names>Y</given-names></name></person-group><article-title>Relevance of spatial heterogeneity of immune infiltration for predicting risk of recurrence after endocrine therapy of ER+ breast cancer</article-title>. <source>JNCI: J Natl Cancer Institute</source> (<year>2018</year>) <volume>110</volume>(<issue>2</issue>):<page-range>166–75</page-range>. <pub-id pub-id-type="doi">10.1093/jnci/djx137</pub-id>
</mixed-citation>
    </ref>
    <ref id="B62">
      <label>62</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nawaz</surname><given-names>S</given-names></name><name><surname>Heindl</surname><given-names>A</given-names></name><name><surname>Koelble</surname><given-names>K</given-names></name><name><surname>Yuan</surname><given-names>Y</given-names></name></person-group><article-title>Beyond immune density: critical role of spatial heterogeneity in estrogen receptor-negative breast cancer</article-title>. <source>Mod Pathol</source> (<year>2015</year>) <volume>28</volume>(<issue>6</issue>):<fpage>766</fpage>. <pub-id pub-id-type="doi">10.1038/modpathol.2015.37</pub-id>
<pub-id pub-id-type="pmid">25720324</pub-id></mixed-citation>
    </ref>
    <ref id="B63">
      <label>63</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilchrist</surname><given-names>KW</given-names></name><name><surname>Gray</surname><given-names>R</given-names></name><name><surname>Fowble</surname><given-names>B</given-names></name><name><surname>Tormey</surname><given-names>DC</given-names></name><name><surname>Taylor 4th</surname><given-names>S</given-names></name></person-group><article-title>Tumor necrosis is a prognostic predictor for early recurrence and death in lymph node-positive breast cancer: a 10-year follow-up study of 728 Eastern Cooperative Oncology Group patients</article-title>. <source>J Clin Oncol</source> (<year>1993</year>) <volume>11</volume>(<issue>10</issue>):<page-range>1929–35</page-range>. <pub-id pub-id-type="doi">10.1200/JCO.1993.11.10.1929</pub-id>
</mixed-citation>
    </ref>
    <ref id="B64">
      <label>64</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanafy</surname><given-names>E</given-names></name><name><surname>Al Jabri</surname><given-names>A</given-names></name><name><surname>Gadelkarim</surname><given-names>G</given-names></name><name><surname>Dasaq</surname><given-names>A</given-names></name><name><surname>Nazim</surname><given-names>F</given-names></name><name><surname>Al Pakrah</surname><given-names>M</given-names></name></person-group><article-title>Tumor histopathological response to neoadjuvant chemotherapy in childhood solid malignancies: is it still impressive</article-title>? <source>J Invest Med</source> (<year>2018</year>) <volume>66</volume>(<issue>2</issue>):<page-range>289–97</page-range>. <pub-id pub-id-type="doi">10.1136/jim-2017-000531</pub-id>
</mixed-citation>
    </ref>
    <ref id="B65">
      <label>65</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambertz</surname><given-names>I</given-names></name><name><surname>Kumps</surname><given-names>C</given-names></name><name><surname>Claeys</surname><given-names>S</given-names></name><name><surname>Lindner</surname><given-names>S</given-names></name><name><surname>Beckers</surname><given-names>A</given-names></name><name><surname>Janssens</surname><given-names>E</given-names></name><etal/></person-group><article-title>Upregulation of MAPK Negative Feedback Regulators and RET in Mutant ALK Neuroblastoma: Implications for Targeted Treatment</article-title>. <source>Clin Cancer Res</source> (<year>2015</year>) <volume>21</volume>(<issue>14</issue>):<page-range>3327–39</page-range>. <pub-id pub-id-type="doi">10.1158/1078-0432.CCR-14-2024</pub-id>
</mixed-citation>
    </ref>
    <ref id="B66">
      <label>66</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gowda</surname><given-names>SN</given-names></name><name><surname>Yuan</surname><given-names>C</given-names></name></person-group> eds. “<article-title>ColorNet: Investigating the importance of color spaces for image classification</article-title>”. In: <source>Asian Conference on Computer Vision</source>. <publisher-loc>New York, US</publisher-loc>: <publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B67">
      <label>67</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>J</given-names></name><name><surname>Boben</surname><given-names>M</given-names></name><name><surname>Fidler</surname><given-names>S</given-names></name><name><surname>Urtasun</surname><given-names>R</given-names></name></person-group> eds. “<article-title>Real-time coarse-to-fine topologically preserving segmentation</article-title>”. In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>. <publisher-loc>New York, US</publisher-loc>: <publisher-name>IEEE</publisher-name> (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="B68">
      <label>68</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>M</given-names></name><name><surname>Le</surname><given-names>QV</given-names></name></person-group><article-title>Efficientnet: Rethinking model scaling for convolutional neural networks</article-title>. <source>arXiv preprint</source> (<year>2019</year>) arXiv:190511946.</mixed-citation>
    </ref>
  </ref-list>
</back>
