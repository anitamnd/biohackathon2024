<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Gigascience</journal-id>
    <journal-id journal-id-type="iso-abbrev">Gigascience</journal-id>
    <journal-id journal-id-type="publisher-id">gigascience</journal-id>
    <journal-title-group>
      <journal-title>GigaScience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2047-217X</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7199472</article-id>
    <article-id pub-id-type="doi">10.1093/gigascience/giaa042</article-id>
    <article-id pub-id-type="publisher-id">giaa042</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Technical Note</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00960</subject>
        <subject>AcademicSubjects/SCI02254</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MaRe: Processing Big Data with application containers on Apache Spark</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-4851-759X</contrib-id>
        <name>
          <surname>Capuccini</surname>
          <given-names>Marco</given-names>
        </name>
        <!--<email>m.capuccini@gmail.com</email>-->
        <xref ref-type="aff" rid="aff1">1</xref>
        <xref ref-type="aff" rid="aff2">2</xref>
        <xref ref-type="corresp" rid="cor1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dahlö</surname>
          <given-names>Martin</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">2</xref>
        <xref ref-type="aff" rid="aff3">3</xref>
        <xref ref-type="aff" rid="aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Toor</surname>
          <given-names>Salman</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-8083-2864</contrib-id>
        <name>
          <surname>Spjuth</surname>
          <given-names>Ola</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">2</xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><label>1</label><institution>Department of Information Technology, Uppsala University</institution>, Box 337, 75105, Uppsala, <country country="SE">Sweden</country></aff>
    <aff id="aff2"><label>2</label><institution>Department of Pharmaceutical Biosciences, Uppsala University, Box 591, 751 24, Uppsala</institution>, <country country="SE">Sweden</country></aff>
    <aff id="aff3"><label>3</label><institution>Science for Life Laboratory, Uppsala University</institution>, Box 591, 751 24, Uppsala, <country country="SE">Sweden</country></aff>
    <aff id="aff4"><label>4</label><institution>Uppsala Multidisciplinary Center for Advanced Computational Science, Uppsala University</institution>, Box 337, 75105, Uppsala, <country country="SE">Sweden</country></aff>
    <author-notes>
      <corresp id="cor1">Correspondence address. Marco Capuccini, Uppsala University, Box 591, 751 24, Uppsala, Sweden. E-mail: <email>m.capuccini@gmail.com</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>5</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-05-05">
      <day>05</day>
      <month>5</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>5</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>9</volume>
    <issue>5</issue>
    <elocation-id>giaa042</elocation-id>
    <history>
      <date date-type="received">
        <day>09</day>
        <month>5</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>10</day>
        <month>2</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>07</day>
        <month>4</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="giaa042.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="abs1">
        <title>Background</title>
        <p>Life science is increasingly driven by Big Data analytics, and the MapReduce programming model has been proven successful for data-intensive analyses. However, current MapReduce frameworks offer poor support for reusing existing processing tools in bioinformatics pipelines. Furthermore, these frameworks do not have native support for application containers, which are becoming popular in scientific data processing.</p>
      </sec>
      <sec id="abs2">
        <title>Results</title>
        <p>Here we present MaRe, an open source programming library that introduces support for Docker containers in Apache Spark. Apache Spark and Docker are the MapReduce framework and container engine that have collected the largest open source community; thus, MaRe provides interoperability with the cutting-edge software ecosystem. We demonstrate MaRe on 2 data-intensive applications in life science, showing ease of use and scalability.</p>
      </sec>
      <sec id="abs3">
        <title>Conclusions</title>
        <p>MaRe enables scalable data-intensive processing in life science with Apache Spark and application containers. When compared with current best practices, which involve the use of workflow systems, MaRe has the advantage of providing data locality, ingestion from heterogeneous storage systems, and interactive processing. MaRe is generally applicable and available as open source software.</p>
      </sec>
    </abstract>
    <kwd-group kwd-group-type="keywords">
      <kwd>MapReduce</kwd>
      <kwd>application containers</kwd>
      <kwd>Big Data</kwd>
      <kwd>Apache Spark</kwd>
      <kwd>workflows</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Horizon 2020</institution>
            <institution-id institution-id-type="DOI">10.13039/501100007601</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>654241</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="12"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>Findings</title>
    <sec id="sec1-1">
      <title>Background and purpose</title>
      <p>Life science is increasingly driven by Big Data analytics. From genomics, proteomics, and metabolomics to bioimaging and drug discovery, scientists need to analyze larger and larger amounts of data [<xref rid="bib1" ref-type="bibr">1–5</xref>]. This means that datasets can no longer be stored and processed in a researcher’s workstation but instead need to be handled on distributed systems, at the organization level. For instance, the European Bioinformatics Institute, in Hinxton (United Kingdom), offers a total storage capacity of &gt;160 petabytes for biologically significant data [<xref rid="bib6" ref-type="bibr">6</xref>]. Such amounts of data pose major challenges for scientific analyses. First, there is a need to efficiently scale existing processing tools over massive datasets. In fact, bioinformatics software that was originally developed with the simplistic view of small-scale data will not scale on distributed computing platforms out of the box. The process of adapting such tools may introduce disruptive changes to the existing codebase, and it is generally unsustainable for most organizations. Second, the complexity in programming distributed systems may be hard to cope with for most researchers, who instead need to focus on the biological problem at hand. In addition, because life science is exploratory, scientists increasingly demand the ability to run interactive analyses rather than submitting jobs to batch systems. Third, when handling Big Data in distributed systems, data locality is a major concern. Indeed, if once data could be shuffled with little regard, with massive datasets it is not only inefficient [<xref rid="bib7" ref-type="bibr">7</xref>] but also prohibitively expensive in terms of power consumption—estimated to be on the order of several hundred thousand dollars per year for a single next-generation high-performance computing (HPC) cluster [<xref rid="bib8" ref-type="bibr">8</xref>]. For geographically dispersed datasets, locality awareness becomes even more challenging because computing resources need to be dynamically acquired close to the data [<xref rid="bib9" ref-type="bibr">9</xref>]. Cloud computing solves this problem by enabling the allocation of virtual infrastructure on demand [<xref rid="bib10" ref-type="bibr">10</xref>]. However, heterogeneity in storage systems for cloud providers [<xref rid="bib11" ref-type="bibr">11</xref>] makes it hard to abstract data ingestion from many different sources. Finally, because bioinformatics software is characterized by complex software dependencies, deploying and managing a vast collection of tools in a large distributed system also represents a major challenge [<xref rid="bib12" ref-type="bibr">12</xref>].</p>
      <p>Current bionformatics best practices make use of workflow systems to orchestrate analyses over distributed computing platforms [<xref rid="bib13" ref-type="bibr">13</xref>]. Workflow systems provide high-level APIs that allow for defining an execution graph of existing processing tools. At run time, the execution graph is used to pipeline the analysis on distributed cloud or HPC resources. Hence, the parallelization of the analysis is transparently carried out, by executing non-dependent tasks at the same time. Cutting-edge workflow systems, such as Luigi [<xref rid="bib14" ref-type="bibr">14</xref>], NextFlow [<xref rid="bib15" ref-type="bibr">15</xref>], Galaxy [<xref rid="bib16" ref-type="bibr">16</xref>], and Pachyderm [<xref rid="bib17" ref-type="bibr">17</xref>], allow for running processing tools as application containers. This lightweight packaging technology allows for encapsulating complete software environments, so that distributed systems can run the processing tools with no need of additional dependencies, in an isolated manner [<xref rid="bib18" ref-type="bibr">18</xref>]. Hence, container-enabled workflow systems provide a fairly easy way to define distributed analyses comprising existing bioinformatics tools, and eliminating the need for managing complex software delivery process and dependency management. Nevertheless, workflow-oriented processing falls short when it comes to Big Data analyses. To the best of our knowledge, all of these systems use a decoupled shared storage system for synchronization and intermediate results storage. When dealing with large datasets, this translates to a massive and unnecessary communication in the underlying infrastructure. In addition, workflow systems usually support a limited amount of storage back ends, not seldom only POSIX file systems, making it hard to ingest data from heterogeneous cloud resources. Finally, owing to their batch-oriented nature, it is also intrinsically hard to enable interactive, exploratory analyses using workflow-oriented frameworks.</p>
      <p>Google’s MapReduce programming model and its associated implementation pioneered uncomplicated Big Data analytics on distributed computing platforms [<xref rid="bib19" ref-type="bibr">19</xref>]. When MapReduce is used, the analysis is defined in a high-level programming language that hides challenging parallel programming details including fault tolerance, data distribution, and locality-aware scheduling. Open source implementations of MapReduce are well established in industrial and scientific applications [<xref rid="bib20" ref-type="bibr">20</xref>, <xref rid="bib21" ref-type="bibr">21</xref>], and numerous success stories in life science have been reported [<xref rid="bib22" ref-type="bibr">22–24</xref>].</p>
      <p>Apache Spark has emerged as the project that collected the largest community in the open source MapReduce ecosystems [<xref rid="bib25" ref-type="bibr">25</xref>]. In addition to the MapReduce implementation, Apache Spark also provides increasingly important features, such as in-memory, interactive, and stream processing. Furthermore, owing to broad collaborations in the open source community, Apache Spark supports all of the major storage systems, enabling data ingestion from heterogeneous cloud resources. These characteristics are particularly appealing for the case of Big Data in life science. Nevertheless, Apache Spark and other similar frameworks offer poor support for composing analyses out of existing processing tools. This is usually limited to calling external programs, which can only access data sequentially, without support for application containers [<xref rid="bib26" ref-type="bibr">26</xref>]. In fact, the main way of implementing analytics in MapReduce-oriented environments is to code each transformation using 1 of the available APIs. This way of implementing analyses contrasts with current best practices in bioinformatics, which promote the use of existing tools as application containers with the goal of improving the delivery, interoperability, and reproducibility of scientific pipelines [<xref rid="bib15" ref-type="bibr">15</xref>].</p>
      <p>Here we introduce MaRe—an open source programming library that extends Apache Spark, introducing comprehensive support for external tools and application containers in MapReduce. Similarly to container-enabled workflow systems, MaRe allows analyses to be defined in a high-level language, in which data transformations are performed by application containers. In addition, MaRe provides seamless management of data locality, as well as full interoperability, with the Apache Spark ecosystem. This last point allows MaRe analyses to ingest data from heterogeneous cloud storage systems and also provides support for interactive processing. Finally, by supporting Docker, the de facto standard container engine [<xref rid="bib27" ref-type="bibr">27</xref>], MaRe is compatible with numerous existing container images.</p>
      <p>In summary, the key contributions of the presented work are as follows:</p>
      <list list-type="bullet">
        <list-item>
          <p>We introduce MaRe, an open source MapReduce-oriented programming library for container-based data processing on top of Apache Spark.</p>
        </list-item>
        <list-item>
          <p>We benchmark MaRe on 2 data-intensive applications in life science, showing ease of use and scalability.</p>
        </list-item>
      </list>
    </sec>
    <sec id="sec1-2">
      <title>MaRe</title>
      <sec id="sec1-2-1">
        <title>Programming model</title>
        <p>We introduce the MaRe programming model using a simple, yet interesting, example in genomics. A DNA sequence can be represented as a text file written in a language of 4 characters: A, T, G, C. The guanine-cytosine (GC) content in a DNA sequence has interesting biological implications; for instance, there is evidence that GC-rich genes are expressed more efficiently than GC-poor genes [<xref rid="bib28" ref-type="bibr">28</xref>]. Hence, within a large DNA sequence it can be interesting to count G and C occurrences. Given an Ubuntu Docker image [<xref rid="bib29" ref-type="bibr">29</xref>], the task can easily be implemented in MaRe using POSIX tools. Listing 1 shows such an implementation.</p>
        <table-wrap id="tbl1" orientation="portrait" position="float">
          <label>Listing 1.</label>
          <caption>
            <p>GC count in MaRe</p>
          </caption>
          <table frame="hsides" rules="groups">
            <tbody>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <inline-graphic xlink:href="giaa042ufig1.jpg" mimetype="image"/>
                </td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>Being based on Apache Spark, MaRe has a similar programming model. The control flow of the analysis is coded in Scala [<xref rid="bib30" ref-type="bibr">30</xref>], by the program in Listing 1. Such a program is called a "driver" in the Apache Spark terminology. The driver program can be packaged and submitted to a cluster (in batch mode), or executed interactively using a notebook environment such as Jupyter [<xref rid="bib31" ref-type="bibr">31</xref>] or Apache Zeppelin [<xref rid="bib32" ref-type="bibr">32</xref>]. Listing 1 starts by instantiating a <monospace>MaRe</monospace> object, which takes a resilient distributed dataset (RDD) [<xref rid="bib33" ref-type="bibr">33</xref>], containing the input genome file in text format. Such an RDD can be easily loaded using the Apache Spark API from any of the supported storage backends. The <monospace>map</monospace> primitive (line 1–8) applies a command from the Docker image to each partition of the RDD. In our example we specify the Ubuntu image on line 4, and we use a command that combines <monospace>grep</monospace> and <monospace>wc</monospace> to filter and count GC occurrences (on line 6). The partitions are mounted in the Docker containers at the configured input mount point (<monospace>”/dna”</monospace> at line 2), and the command results are loaded back to MaRe from the configured output mount point (<monospace>”/count”</monospace> on line 3). In the example we use "TextFile" mount points because the input data are in text format. By default, MaRe considers each line in a text file as a separate record, but custom record separators can also be configured using the TextFile constructor.</p>
        <p>At this point it is important to mention that MaRe can also handle binary files. For such data formats, the driver program should specify mount points of type <monospace>"BinaryFiles</monospace>." In this case, each RDD record is considered as a distinct binary file; thus, the specified mount point results in a directory containing multiple files (as opposed to <monospace>TextFile</monospace>, which mounts the records in a single file). We provide an example of the <monospace>BinaryFiles</monospace> mount point in the Evaluation section.</p>
        <p>Coming back to Listing 1, after applying the <monospace>"map</monospace>" primitive, each RDD partition is transformed into a distinct GC count. The <monospace>"reduce</monospace>" primitive (lines 8–15) aggregates the counts in each partition to a cumulative sum. Again, we use mount points of type <monospace>TextFile</monospace> to mount the intermediate counts in the containers (<monospace>”/counts”</monospace> on line 9) and to read back the cumulative sum (<monospace>”/sum”</monospace> on line 10). The sum is computed using the <monospace>awk</monospace> command from the Ubuntu image (lines 11–14). Finally, the result is returned to the <monospace>gcCount</monospace> variable at line 1.</p>
        <p>From the GC example, the reader may have noticed that our programming model is strongly inspired by MapReduce. In addition, Apache Spark users may have noticed that the GC count problem can easily be solved in pure Spark code. Indeed, the aim of the example is just to provide an easy introduction to MaRe, and 2 real-world applications are available in the Evaluation section.</p>
        <p>Apart from <monospace>map</monospace> and <monospace>reduce</monospace>, MaRe provides an additional primitive. For real-world applications, we noticed that it is often necessary to group dataset records according to a specific logic before applying <monospace>map</monospace> or <monospace>reduce</monospace>. For this reason, MaRe also provides a <monospace>"repartitionBy"</monospace> primitive, which repartitions the RDD records according to a configurable grouping rule. More specifically, the <monospace>repartitionBy</monospace> primitive takes into account a user-provided <monospace>keyBy</monospace> function, which is used to compute a key for each record in the dataset. Then, the repartitioning is performed accordingly so that records with the same key end up in the same partition. An example of <monospace>repartitionBy</monospace> is available in the Evaluation section.</p>
      </sec>
      <sec id="sec1-2-2">
        <title>Implementation</title>
        <p>MaRe comes as a thin layer on top of the RDD API [<xref rid="bib33" ref-type="bibr">33</xref>], and it relies on Apache Spark to provide important features such as data locality, data ingestion, interactive processing, and fault tolerance. The implementation effort consists of (i) leveraging the RDD API to implement the MaRe primitives and (ii) handling data between containers and RDD structures.</p>
        <sec id="sec1-2-2-1">
          <title>Primitives</title>
          <p>Each instance of a <monospace>MaRe</monospace> object retains an underlying RDD, which represents an abstraction of a dataset that is partitioned across Apache Spark workers. The <monospace>map</monospace>, <monospace>reduce</monospace>, and <monospace>repartitionBy</monospace> primitives utilize the underlying RDD API to operate such a dataset.</p>
          <p>Fig. <xref ref-type="fig" rid="fig1">1</xref> shows the execution diagram for the <monospace>map</monospace> primitive. For simplicity, in Fig. <xref ref-type="fig" rid="fig1">1</xref> we show a single partition per worker, but in reality workers may retain multiple partitions. This primitive takes an input RDD that is partitioned over <italic>N</italic> nodes, and it transforms each partition using a Docker container command—thus returning a new RDD′. This logic is implemented using "mapPartitions" from the RDD API. When calling mapPartitions, MaRe specifies a lambda expression that (i) makes the data available in the input mount point, (ii) runs the Docker container, and (iii) retrieves the results from the output mount point. When using mapPartitions, Apache Spark generates a single stage; thus, no data shuffle is performed.</p>
          <fig id="fig1" orientation="portrait" position="float">
            <label>Figure 1:</label>
            <caption>
              <p>Execution diagram for the <monospace>map</monospace> primitive. The primitive takes an RDD that is partitioned over <italic>N</italic> nodes, it transforms each partition using a Docker container, and it returns a new RDD′. The logic is implemented using mapPartitions from the RDD API. Because mapPartitions generates a single stage, data are not shuffled between nodes.</p>
            </caption>
            <graphic xlink:href="giaa042fig1"/>
          </fig>
          <p>Fig. <xref ref-type="fig" rid="fig2">2</xref> shows the execution diagram for the <monospace>reduce</monospace> primitive. This primitive takes an input RDD, partitioned over <italic>N</italic> nodes, and it iteratively aggregates records, reducing the number of partitions until an RDD′, containing a single result partition, is returned. Again, the input RDD may retain multiple partitions per node. However, as opposed to the <monospace>map</monospace> primitive, RDD′ always contains a single partition when it is returned. Given a user-configured depth <italic>K</italic>, the records in the RDD are aggregated using a tree-like algorithm. In each of the <italic>K</italic> levels in the tree, the records within each partition are first aggregated using a Docker container command. Like the <monospace>map</monospace> primitive, this first transformation is implemented using "mapPartitions" from the RDD API. Then, the number of partitions is decreased using "repartition" from the RDD API. This process is repeated <italic>K</italic> times until 1 single partition is left. At this point the records within the remaining partition are aggregated again using mapPartitions (from the RDD API), and RDD′ is returned. A new stage is generated each time repartition is used. Hence, <monospace>reduce</monospace> leads to <italic>K</italic> data shuffles. For this reason, when aggregating records, the user-provided command should always reduce the size of the partition. In addition, for consistent results, the command should perform an associative and commutative operation. By default MaRe sets <italic>K</italic> to 2; however, the user may chose a higher tree depth when it is not possible to sufficiently reduce the dataset size in 1 go.</p>
          <fig id="fig2" orientation="portrait" position="float">
            <label>Figure 2:</label>
            <caption>
              <p>Execution diagram for the <monospace>reduce</monospace> primitive. The primitive takes an input RDD, partitioned over <italic>N</italic> nodes, and it iteratively aggregates records using a Docker container, reducing the number of partitions until an RDD′, containing a single result partition, is returned. The logic is implemented using mapPartitions and repartition from the RDD API, to aggregate records in partitions and to decrease the number of partitions, respectively. Because <monospace>repartition</monospace> is called in each of the <italic>K</italic> iterations, <italic>K</italic> stages are generated, giving place to <italic>K</italic> data shuffles.</p>
            </caption>
            <graphic xlink:href="giaa042fig2"/>
          </fig>
          <p>Finally, the <monospace>repartitionBy</monospace> primitive is implemented by using keyBy and then repartition from the RDD API. MaRe uses the user-provided grouping rule with keyBy to compute a key for each RDD record, and then it applies <monospace>repartition</monospace> in conjunction with HashPartitioner [<xref rid="bib34" ref-type="bibr">34</xref>], which makes sure that records with the same key end up in the same partition.</p>
        </sec>
        <sec id="sec1-2-2-2">
          <title>Data handling</title>
          <p>One of the advantages of Apache Spark over other MapReduce-like systems is its ability to retain data in memory. To achieve this when passing the data to the application containers, there are a few options available: (i) Unix pipes [<xref rid="bib35" ref-type="bibr">35</xref>], (ii) memory-mapped files [<xref rid="bib36" ref-type="bibr">36</xref>], and (iii) tmpfs [<xref rid="bib37" ref-type="bibr">37</xref>]. Solutions (i) and (ii) are the most memory-efficient because they do not need to materialize the data when passing it to the containers. However, (i) allows records to be seen only once in a stream-like manner, while (ii) requires the container-wrapped tools to be able to read from a memory-mapped file. Apache Spark loads data in memory sequentially and partition-wise. Partition size is configurable and often equals the block size in the underlying storage system. For the Hadoop distributed file system (HDFS) this value defaults to 128 MB, meaning that on an 8-core machine materializing again partitions on an in-memory file system would require 2 GB of memory in total—which is usually not a problem for modern data centers. Therefore, to support any wrapped tool, we decided to start by implementing solution (iii). This means that MaRe uses an in-memory tmpfs file system as temporary file space for the input and output mount points. The solution allows a standard POSIX mount point to be provided to the containers, while still retaining reasonable performance [<xref rid="bib37" ref-type="bibr">37</xref>]. However, MaRe also provides users with the option of selecting any other disk-based file system for the temporary mount points. Even if this could in principle edge performance, this can be useful when a dockerized tool does not allow for splitting large partitions in smaller chunks of records; we show an example of this in the Evaluation section.</p>
        </sec>
      </sec>
    </sec>
    <sec id="sec1-3">
      <title>Evaluation</title>
      <p>We evaluate MaRe on 2 data-intensive applications in life science. The first application can be decomposed to somewhat independent jobs, where the data assigned to each job can be relatively small. This is where MapReduce-oriented programming libraries such as MaRe excel. Conversely, the second application requires larger chunks of data to be computed all at once, thus allowing us to show the performance penalty that is introduced in such a case. We evaluate in more detail (i) how the analyses can be implemented in MaRe and (ii) how the analyses scale over multiple nodes. To the best of our knowledge, no stable Spark-native implementation of the tools presented in the analyses is publicly available, making a fair performance comparison with a system that does not delegate data processing to an external application container unfeasible. To this extent, we would like to add that if such implementation were available there would be no advantage in rewriting the analyses using our programming library.</p>
      <p>The scalability experiments were carried out on cPouta, an OpenStack-based cloud service operated by the Information Technology Center for Science (CSC) in Finland [<xref rid="bib38" ref-type="bibr">38</xref>]. The driver programs were run interactively using an Apache Zeppelin environment [<xref rid="bib32" ref-type="bibr">32</xref>], and the notebooks were made available to sustain reproducibility [<xref rid="bib39" ref-type="bibr">39</xref>]. In addition, we also made available a deployment automation that enables our set-up to be replicated on cPouta, as well as any other OpenStack-based cloud provider [<xref rid="bib40" ref-type="bibr">40</xref>].</p>
      <sec id="sec1-3-1">
        <title>Virtual screening</title>
        <p>Virtual screening (VS) is a computer-based method to identify potential drug candidates by evaluating the binding affinity of virtual compounds against a biological target protein [<xref rid="bib41" ref-type="bibr">41</xref>]. Given a 3D target structure, a molecular docking software is run against a large library of known molecular representations. For each compound in the virtual molecular library the docking software produces a pose, representing the orientation of the molecule in the target structure, and a binding affinity score. The poses with the highest affinity scores can be considered as potential drug leads for the target protein.</p>
        <p>VS is data intensive because molecular libraries usually contain millions of compounds. A simple, yet effective, approach to scale VS consists of (i) distributing the molecular library over several nodes, (ii) running the docking software in parallel, and (iii) aggregating the top-scoring poses. Listing 2 shows how this logic can be implemented in MaRe, using FRED [<xref rid="bib42" ref-type="bibr">42</xref>] as molecular docking software and sdsorter [<xref rid="bib43" ref-type="bibr">43</xref>] to filter the top-scoring poses.</p>
        <table-wrap id="tbl2" orientation="portrait" position="float">
          <label>Listing 2.</label>
          <caption>
            <p>Virtual screening in MaRe</p>
          </caption>
          <table frame="hsides" rules="groups">
            <tbody>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <inline-graphic xlink:href="giaa042ufig2.jpg" mimetype="image"/>
                </td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>In Listing 2, we initialize MaRe by passing it a molecular library that was previously loaded as an RDD (<monospace>libraryRDD</monospace> on line 1). We implement the parallel molecular docking using the <monospace>map</monospace> primitive. On lines 2 and 3, we set input and output mount points as text files, and assuming the library to be in Structure-Data File (SDF) format [<xref rid="bib44" ref-type="bibr">44</xref>] we use the custom record separator: <monospace>"\n</monospace><inline-formula><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\$\$\$\$}$\end{document}</tex-math></inline-formula><monospace>\n</monospace>”. On line 4, we specify a Docker image containing FRED. The image is not publicly available because it also contains our FRED license, but the license can be obtained free of charge for research purposes and we provide a Dockerfile [<xref rid="bib39" ref-type="bibr">39</xref>] to build the image. On line 5, we specify the FRED command. We use an HIV-1 protease receptor [<xref rid="bib45" ref-type="bibr">45</xref>] as target (which is wrapped in the Docker image), and we set (i) <monospace>-hitlist_size 0</monospace> to not filter the poses in this stage, (ii) <monospace>-conftest none</monospace> to consider the input molecules as single conformations, (iii) <monospace>-dbase /in.sdf</monospace> to read the input molecules from the input mount point, and (iv) <monospace>-docked_molecule_file /out.sdf</monospace> to write the poses to the output mount point.</p>
        <p>The map phase produces a pose for each molecule in <monospace>libraryRDD</monospace>. On line 12, we use the <monospace>reduce</monospace> primitive to filter the top 30 poses. On lines 13 and 14, we set the input and output mount points as we do for the <monospace>map</monospace> primitive. On line 15, we specify a publicly available Docker image containing sdsorter. On line 16, we specify the sdsorter command, and we set (i) <monospace>-reversesort=”FRED Chemgauss4 score”</monospace> to sort the poses from highest to lowest FRED score, (ii) <monospace>-keep-tag=”FRED Chemgauss4 score</monospace> to keep the score in the results, (iii) <monospace>-nbest=30</monospace> to output the top 30 poses, and (iv) <monospace>/in.sdf /out.sdf</monospace> to read and write from the input mount point and to the output mount point, respectively. Note that this command performs an associative and commutative operation, thus ensuring correctness in the reduce phase. Finally, the results are returned to <monospace>topPosesRDD</monospace>, on line 1.</p>
        <p>We benchmarked the analysis coded in Listing 2 against the SureChEMBL library [<xref rid="bib46" ref-type="bibr">46</xref>] retrieved from the ZINC database [<xref rid="bib47" ref-type="bibr">47</xref>], containing ∼2.2M molecules. The benchmark ran on top of a stand-alone Apache Spark cluster composed of 1 master and 12 worker nodes. Each node provided 10 cores and 43 GB of memory, thus resulting in a total of 120 cores and 516 GB of memory. The data were made available to the workers using a co-located HDFS storage. Under these settings, we evaluated the scalability in terms of weak scaling efficiency (WSE). This performance metric shows how the system scales when the amount of data and parallelism increase. To compute the WSEs we first ran the benchmark on 1/12 of the dataset using the dockerized tools on a worker node using their built-in, single-node parallelization. Then, we reran the pipeline using MaRe on 2/12, 4/12, 6/12, ... and 12/12 of the datasets, using 2, 4, 6, ...and 12 worker nodes, respectively. The WSE is then computed as the time for processing 1/12 of the data using the built-in, single-node parallelization, divided by the time for processing <italic>N</italic>/12 of the data using <italic>N</italic> nodes (for <italic>N</italic> = 2, 4, 6, ...,12). The ideal case, when the number of nodes is doubled, is to be able to process twice as much data in the same amount of time. Hence, a higher WSE indicates better performance.</p>
        <p>Fig. <xref ref-type="fig" rid="fig3">3</xref> shows the WSE for the full analysis, when using tmpfs and a disk-based, ext4 file system [<xref rid="bib48" ref-type="bibr">48</xref>] as temporary mount points. From the experiments it emerges that there is little difference between the 2 methods in terms of scaling efficiency—tmpfs improved the WSE by 0.02 at most. Indeed, the results in Fig. <xref ref-type="fig" rid="fig3">3</xref> indicate very good scalability, with a WSE close to ideal for both tmpfs and ext4. For 120 cores, the full benchmark ran in 2 hours and 21 minutes while 1/12 of the input data were processed by the built-in, single-node parallelization in 2 hours and 14 minutes—resulting in 0.94 WSE. This means that the overhead introduced by MaRe accounts for only 7 minutes in total.</p>
        <fig id="fig3" orientation="portrait" position="float">
          <label>Figure 3:</label>
          <caption>
            <p>WSE for the VS application implemented in MaRe (Listing 2). The results are produced by using SureChEMBL as input, and we show the WSE when using tmpfs and ext4 as temporary mount point for passing the data to the containers.</p>
          </caption>
          <graphic xlink:href="giaa042fig3"/>
        </fig>
        <p>Finally, to ensure the correctness of the parallelization, we ran sdsorter and FRED on a single core against 1,000 molecules that we randomly sampled from SureChEMBL, and we compared the results with those produced by the code in Listing 2.</p>
      </sec>
      <sec id="sec1-3-2">
        <title>Single-nucleotide polymorphism calling</title>
        <p>A single-nucleotide polymorphism (SNP) is a position in a DNA sequence where a single nucleotide (or base pair) is different from another DNA sequence to which it is being compared [<xref rid="bib49" ref-type="bibr">49</xref>]. When considering multiple samples, DNA sequences are usually compared individually to a reference genome: an agreed-upon sequence that is considered to represent an organism's genome. Once each DNA sequence has had its SNPs detected, or "called," the differences between the samples can be compared.</p>
        <p>SNPs occur frequently. In fact, in humans roughly every 850th base pair is an SNP [<xref rid="bib50" ref-type="bibr">50</xref>]. Calling SNPs has several use cases. For instance, SNPs can be used as high-resolution markers when comparing genomic regions between samples [<xref rid="bib51" ref-type="bibr">51</xref>], as well as indicators of diseases in an individual [<xref rid="bib52" ref-type="bibr">52</xref>]. Modern high-throughput sequencing methods for reading DNA often make use of a technique called "massively parallel sequencing" to read sequences longer than ∼200 bp, with a sufficiently small error rate. This is done by cleaving multiple copies of the source DNA into random fragments (called "reads") that are small enough to be accurately read, and then by aligning them to a reference genome. The overlapping fragments together form the sequence of the source DNA.</p>
        <p>In order to accurately sequence 3 billion bases from a single human individual, 30-fold more read data need to be sequenced [<xref rid="bib1" ref-type="bibr">1</xref>]. This makes SNP calling data-intensive, thus requiring parallelization. A simple MapReduce-oriented approach consists of (i) distributing the reads across several nodes, (ii) aligning the reads to a reference genome in parallel, and (iii) calling the SNPs with respect to the reference genome. The last step requires all the reads from a chromosome to be included in the SNP calling; thus, the maximum allowed parallelism is equal to the total number of chromosomes. Listing 3 shows how the described parallelization can be implemented in MaRe, using BWA for the alignment [<xref rid="bib53" ref-type="bibr">53</xref>] and GATK [<xref rid="bib54" ref-type="bibr">54</xref>] for the SNP calling. In contrast to the VS example, BWA and GATK provide a multithreaded implementation of the algorithms. Therefore, in Listing 3, we leverage this implementation for single-node parallelization.</p>
        <table-wrap id="tbl3" orientation="portrait" position="float">
          <label>Listing 3.</label>
          <caption>
            <p>SNP calling in MaRe</p>
          </caption>
          <table frame="hsides" rules="groups">
            <tbody>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <inline-graphic xlink:href="giaa042ufig3.jpg" mimetype="image"/>
                </td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>In Listing 3, MaRe is initialized by passing an RDD containing the reads for a human individual in interleaved FASTQ format [<xref rid="bib55" ref-type="bibr">55</xref>] (<monospace>readsRDD</monospace> on line 1). We implement the parallel reads alignment using the <monospace>map</monospace> primitive. From lines 2 to 4, we set the mount points as text files, and we specify a publicly available Docker image containing the necessary software tools. On line 5 we specify the BWA command and we set (i) <monospace>-t 8</monospace> to utilize 8 threads, (ii) <monospace>-p /ref/human_g1k_v37.fasta</monospace> to specify the reference genome location (in the container), and (iii) the input mount point <monospace>/in.fastq</monospace>. In addition, on line 9 we pipe the results to another piece of software, called <monospace>samtools</monospace> [<xref rid="bib56" ref-type="bibr">56</xref>], to convert them from the binary BAM format [<xref rid="bib56" ref-type="bibr">56</xref>] to the text SAM format [<xref rid="bib56" ref-type="bibr">56</xref>]. Converting the results to text format makes it easier to parse the chromosome location in the next step.</p>
        <p>When calling SNPs, GATK needs to read all of the aligned reads for a certain DNA region. Using chromosomes to define the regions makes sure that no reads will span a region break point—a problem that would need to be handled if chromosomes were to be split in smaller regions. To achieve this we need to (i) perform a chromosome-wise repartition of the dataset and (ii) allow MaRe to write temporary mount point data to disk. Point (ii) is enabled by setting the <monospace>TMPDIR</monospace> environment variable to a disk mount, in the Apache Zeppelin configuration. Even if this could potentially edge performance, this is necessary because the full partition size exceeds the tmpfs capacity in our worker nodes. Point (i) is implemented by using the <monospace>repartitionBy</monospace> primitive, on line 11. In particular, we specify a <monospace>keyBy</monospace> function that parses and returns the chromosome identifier (on line 12), and a number of partitions that is equal to the number of worker nodes (on line 13).</p>
        <p>The <monospace>map</monospace> primitive (on line 14) uses the chromosome-wise partitioning to perform the SNP calling, with GATK. Because the data are in SAM format, we set the input mount point as text file (line 15). However, because we are going to zip the results before aggregating the SNPs (line 32), we set the output mount point as a binary files directory (<monospace>”/out”</monospace>, on line 16). On line 17, we set the same Docker image that we used for the initial mapping step, and, on line 18, we specify a command that (i) prepends the necessary SAM header to the input data (which is available inside the container under <monospace>/ref/human_g1k_v37.dict</monospace>, on line 19), (ii) coverts the SAM input to BAM format (line 23), (iii) builds an index for the BAM format (line 26), and (iv) runs the multithreaded SNP calling using GATK, producing a Variant Call Format (VCF) file [<xref rid="bib57" ref-type="bibr">57</xref>] (line 28). A detailed description of the options used for each command can be found in the GATK documentation [<xref rid="bib58" ref-type="bibr">58</xref>].</p>
        <p>Finally, to aggregate the SNPs to a single zipped file, we use the <monospace>reduce</monospace> primitive. In this case we use binary file mount points (lines 35 and 36) and a publicly available image containing the VCFtools software [<xref rid="bib57" ref-type="bibr">57</xref>] (line 37). On line 39, the specified command uses <monospace>vcf-concat</monospace> to merge all of the VCF files in the input mount point, and then it zips and writes them to the output mount point (line 40). Because MaRe applies the reduce command iteratively, intermediate partitions will contain multiple files. Therefore, to avoid file-name clashes, we include a random identifier in the command output (${RANDOM} at line 40).</p>
        <p>We benchmarked the analysis in Listing 3 against the full individual reads dataset <monospace>HG02666</monospace> (∼30 GB compressed FASTQ files), from the 1000 Genomes Project (1KGP) [<xref rid="bib50" ref-type="bibr">50</xref>]. The benchmark ran on top of a stand-alone Apache Spark cluster composed of 1 master and 14 worker nodes. Each node provided 8 cores and 40 GB of memory, thus resulting in a total of 112 cores and 480 GB of memory. In addition, because after the chromosome-wise repartitioning, the partition size exceeded the tmpfs space in our workers, we used instance favors with a local solid state drive (SSD). This allowed the temporary mount point data to be written and read faster when compared to the previous benchmark. The data were made available to the workers using a co-located HDFS storage. Under these settings, we evaluated the scalability in terms of strong scaling efficiency (SSE). This performance metric shows how the system scales when the parallelism is increased while keeping the input size static. We evaluated this benchmark using SSE instead of WSE because there is no trivial way for downsampling the reference genome while keeping the behavior of the tools unaltered; the algorithms end up taking longer as they perform an exhaustive search when the reference genome is downsampled. To compute the SSEs we first ran the benchmark using the dockerized tools on a worker node with their built-in, single-node parallelization. Then, we reran the pipeline using MaRe on 6, 8, 10, 12, and 14 worker nodes. Then, letting <italic>T</italic><sub>1</sub> be the time for running the benchmark using the built-in, single-node parallelization and <italic>T<sub>N</sub></italic> be the time for running the benchmark using <italic>N</italic> nodes (for <italic>N</italic> = 6, 8, 10, 12), we computed the SSE as <italic>T</italic><sub>1</sub>/(<italic>N</italic> × <italic>T<sub>N</sub></italic>) (we did not run on 2 and 4 nodes because the dataset size exceeded the total memory available to the Spark workers in these settings). The ideal case, when doubling the number of nodes, is to be able to run the benchmark twice as fast. Hence, a higher SSE indicates better performance.</p>
        <p>Fig. <xref ref-type="fig" rid="fig4">4</xref> shows the SSE for the full analysis. The SSE starts at 0.76 for 48 cores and decreases to 0.59 when running on 112 cores. Even if this does not show optimal performance, as in the VS use case, it still indicates good scalability. Indeed, the full benchmark ran in 3 hours and 24 minutes using MaRe on 112 cores, while it took 28 hours and 14 minutes using the built-in, single-node parallelization—leading to a speedup of 8.3.</p>
        <fig id="fig4" orientation="portrait" position="float">
          <label>Figure 4:</label>
          <caption>
            <p>SSE for the SNP calling implemented in MaRe (Listing 3). The results are produced by using a full individual dataset from the 1KGP as input.</p>
          </caption>
          <graphic xlink:href="giaa042fig4"/>
        </fig>
        <p>The alignment portion of the benchmark uses BWA, which allows the reads to be input using pipes. It is interesting to compare how the SSE differs when using this input method as opposed to materializing the data on a temporary ext4 file space. Even though the standard RDD API provides a pipe method to do so, as we mentioned previously, this built-in implementation runs the external tool for each RDD record—which would result in considerable overhead. Instead, we compare the SSE achieved by MaRe with a pipePartition method, available in our benchmark repository [<xref rid="bib39" ref-type="bibr">39</xref>], which pipes entire RDD partitions though a single dockerized tool instance. Fig. <xref ref-type="fig" rid="fig5">5</xref> shows the results of this comparison. Using pipes improved the SSE by ∼0.15 when running on 48 and 64 cores, by ∼0.08 when running on 80 and 96 cores, and by ∼0.12 when running on 112 cores. However, this improvement accounted for saving 6 minutes when running on 112 cores, which is negligible because the full analysis (including variant calling) took &gt;3 hours to complete in such a setting.</p>
        <fig id="fig5" orientation="portrait" position="float">
          <label>Figure 5:</label>
          <caption>
            <p>SSE for the SNP calling alignment stage implemented in MaRe (Listing 3, lines 1–13). The results are produced by using a full individual dataset from the 1KGP as input, and we show the SSE when using an SSD-based, ext4 temporary mount point as well as Unix pipes for passing the data to the containers.</p>
          </caption>
          <graphic xlink:href="giaa042fig5"/>
        </fig>
      </sec>
    </sec>
    <sec id="sec1-4">
      <title>Discussion and conclusions</title>
      <p>Big Data applications are getting increasing momentum in life science. Nowadays data are stored and processed in distributed systems, often in a geographically dispersed manner. This introduces a layer of complexity that MapReduce frameworks, such as Apache Spark, excel at handling [<xref rid="bib59" ref-type="bibr">59</xref>]. Container engines, and in particular Docker, are also becoming an essential part of bioinformatics pipelines because they improve delivery, interoperability, and reproducibility of scientific analyses. By enabling application containers in MapReduce, MaRe constitutes an important advance in the scientific data-processing software ecosystem. When compared to current best practices in bioinformatics, relying solely on using workflow systems to orchestrate data pipelines, MaRe has the advantage of providing locality-aware scheduling, transparent ingestion from heterogeneous storage systems, and interactivity. As data become larger and more globally distributed, we envision scientists instantiating MaRe close to the data, and performing interactive analyses via cloud-oriented resources. In addition to the interactive mode, MaRe also support batch-oriented processing. This is important because it enables integration with existing bioinformatics pipelines. In practical terms, a packaged MaRe application can be launched by a workflow engine to enable data-intensive phases in a pipeline, and submitted to any of the resource managers supported by the Apache Spark community (including HPC systems [<xref rid="bib60" ref-type="bibr">60</xref>]).</p>
      <p>In the Evaluation section we show how researchers can easily implement 2 widely used applications in life science using MaRe. Both analyses can be coded in &lt;50 lines of code, and they are seamlessly parallelized. The results show near optimal scalability for the VS application, with tmpfs improving performance over ext4 only by a negligible factor. The reason why there is no relevant performance improvement in using the former is that the container's running time dominates the time for materializing data on the temporary file space. Even though this may vary in other applications, in our experience this will often be the case for bioinformatics analyses, not justifying the additional effort in setting up a tmpfs space.</p>
      <p>Scalability in the SNP-calling analysis is reasonably good but far from optimal. The reason for this is that before running the haplotype caller, a reasonable amount of data need to be shuffled across the nodes because GATK needs to see all of the data for a single chromosome at once in order to function properly, thus causing a large amount of data to be materialized on disk. Such overhead can be partly mitigated by enabling data streams via standard input and output between MaRe and containers, as the results in Fig. <xref ref-type="fig" rid="fig5">5</xref> show. This constitutes an area for future improvement; however, because GATK is unable to read data from the standard input, such improvement would not be directly applicable to the presented use case.</p>
      <p>ADAM [<xref rid="bib61" ref-type="bibr">61</xref>], a genomics data-processing framework built on top of Apache Spark, shows ideal scalability for a few, commonly used preprocessing steps in genomics pipelines—such as the SNP pipeline that we show in this article. Nevertheless, in real-world scenarios external software would still need to be used to compose end-to-end workflows. Indeed, ADAM itself provides a utility to integrate external tools into its pipelines [<xref rid="bib62" ref-type="bibr">62</xref>]. Because this utility is based on pipes and it does not support application containers natively, it provides less flexibility in comparison with MaRe. Indeed, because MaRe is fully interoperable with Apache Spark, our recommendation for running genomics pipelines would be to use ADAM for the supported preprocessing steps and then MaRe to integrate external tools in the workflow.</p>
      <p>The benchmarks that we show in this article are representative of 2 classes of problems where the application of MaRe could lead to different results in terms of performance. Materializing data is necessary to support any containerized tool, but our results show that this edges performance when records in large partitions need to be processed all together. In this case, reimplementing the analyses natively in Spark using the language of choice could lead to better performance; ADAM is a good example of this approach. It is however important to point out that the effort of reimplementing existing bioinformatics tools is seldom sustainable by research organizations. To give the reader an idea of this, ADAM is the product of a large collaboration maintaining thousands of lines of code. Owing to the current proliferation and heterogeneity of bioinformatics tools [<xref rid="bib63" ref-type="bibr">63</xref>, <xref rid="bib64" ref-type="bibr">64</xref>], it is hard to imagine that such effort would generally be sustainable for many other applications. To this extent, MaRe stands out because it enables bioinformaticians to develop interoperable, distributed pipelines that scale reasonably well without the need to rewrite the existing codebase.</p>
      <p>In conclusion, MaRe provides a MapReduce-oriented model to enable container-based bioinformatics analyses at scale. The project is available on GitHub [<xref rid="bib65" ref-type="bibr">65</xref>] under an open source license, along with all of the code to reproduce the analyses in the Evaluation section [<xref rid="bib39" ref-type="bibr">39</xref>].</p>
    </sec>
  </sec>
  <sec sec-type="methods" id="sec2">
    <title>Methods</title>
    <sec id="sec2-1">
      <title>Apache Spark</title>
      <p>Apache Spark is an open source cluster-computing framework for the analysis of large-scale datasets [<xref rid="bib66" ref-type="bibr">66</xref>]. The project originally started with the aim of overcoming the lack of in-memory processing in traditional MapReduce frameworks. Today, Apache Spark has evolved into a unified analytics engine, encompassing high-level APIs for machine learning, streaming, graph processing, and SQL, and it has become the largest open source project in Big Data analytics, with &gt;1,000 contributors and &gt;1,000 adopting organizations [<xref rid="bib25" ref-type="bibr">25</xref>].</p>
      <sec id="sec2-1-1">
        <title>Clustering model</title>
        <p>The Apache Spark clustering model includes a driver program, 1 or more worker nodes, and a cluster manager. The driver program is written by the user and controls the flow of the programmed analysis. For interactive analysis the driver program can run in notebooks environments such as Jupyter [<xref rid="bib31" ref-type="bibr">31</xref>] and Apache Zeppelin [<xref rid="bib32" ref-type="bibr">32</xref>]. Worker nodes communicate with the driver program, thus executing the distributed analysis as defined by the user. Finally, a cluster manager handles resources in the cluster, allowing for the executing processes to acquire them in the worker nodes. Apache Spark is cluster-manager agnostic and it can run in stand-alone settings, as well as on some popular platforms (e.g., Kubernetes [<xref rid="bib67" ref-type="bibr">67</xref>], Mesos [<xref rid="bib68" ref-type="bibr">68</xref>], and Hadoop YARN [<xref rid="bib69" ref-type="bibr">69</xref>]).</p>
      </sec>
      <sec id="sec2-1-2">
        <title>Resilient distributed datasets</title>
        <p>RDDs [<xref rid="bib33" ref-type="bibr">33</xref>] are central to the Apache Spark programming model. RDDs are an abstraction of a dataset that is partitioned across the worker nodes. Hence, partitions can be operated in parallel in a scalable and fault-tolerant manner, and possibly cached in memory for recurrent access. As a unified processing engine, Apache Spark offers support for ingesting RDDs from numerous Big-Data–oriented storage systems. RDDs can be operated through Scala [<xref rid="bib30" ref-type="bibr">30</xref>], Python [<xref rid="bib70" ref-type="bibr">70</xref>], Java [<xref rid="bib71" ref-type="bibr">71</xref>], and R [<xref rid="bib72" ref-type="bibr">72</xref>] APIs. Such APIs expose RDDs as object collections, and they offer high-level methods to transform the datasets.</p>
        <p>The mapPartition and repartition methods, from the RDD API, are useful to understand the MaRe implementation. The mapPartition method is inspired by functional programming languages. It takes as an argument a lambda expression that codes a data transformation, and it applies it to each partition, returning a new RDD. The repartition method, as the name suggests, changes the way the dataset records are partitioned across the worker nodes. It can be used to increase and decrease the number of partitions, thus affecting the level of parallelism, and it can also sort records in partitions, according to custom logics. In this case, an additional RDD method, namely, keyBy, needs to be used to compute a key for each RDD record. Similarly to mapPartition, keyBy applies a user-provided lambda expression to compute the record keys. Such keys are then used by repartition in conjunction with an extension of the Partitioner class [<xref rid="bib34" ref-type="bibr">34</xref>] to assign records to partitions. For instance, when using HashPartitioner [<xref rid="bib73" ref-type="bibr">73</xref>] records with same key always end up in the same RDD partition.</p>
      </sec>
      <sec id="sec2-1-3">
        <title>Stages and data locality</title>
        <p>RDD methods are lazily applied to the underlying dataset. This means that until something needs to be written to a storage system or returned to the driver program, nothing is computed. In this way, Apache Spark can build a direct acyclic graph and thus optimize the physical execution plan. A physical execution plan is composed of processing tasks that are organized in stages. Typically, inside each stage the physical execution plan preserves data locality, while between stages a data shuffle occurs. In particular, a sequence of mapPartition methods generate a single stage, giving place to almost no communication in the physical execution plan. In contrast, each time repartition is applied to an RDD, a new stage is generated (and data shuffling occurs).</p>
      </sec>
    </sec>
    <sec id="sec2-2">
      <title>Docker</title>
      <p>Docker has emerged as the de facto standard application container engine [<xref rid="bib27" ref-type="bibr">27</xref>]. Like virtual machines (VMs), application containers enable the encapsulation of software components so that any compliant computer system can execute them with no additional dependencies [<xref rid="bib18" ref-type="bibr">18</xref>]. The advantage of Docker and similar container engines over virtualization consists of eliminating the need to run an operating system (OS) for each isolated environment. In contrast to hypervisors, container engines leverage kernel namespaces to isolate software environments, and thus run containers straight on the host OS. This makes application containers considerably lighter than VMs, enabling a more granular compartmentalization of software components.</p>
      <sec id="sec2-2-1">
        <title>Software Delivery</title>
        <p>By enabling the encapsulation of entire software stacks, container engines have the potential to considerably simplify application delivery. Engines such as LXC [<xref rid="bib74" ref-type="bibr">74</xref>] and Jails [<xref rid="bib75" ref-type="bibr">75</xref>] have been available for almost 2 decades. Nevertheless, when compared to Docker these systems are poor in terms of software delivery functionalities. This is the reason why software containers' popularity exploded only when Docker emerged.</p>
        <p>Docker containers can be defined using a text specification language. Using such language, users compose a Dockerfile that is parsed by Docker and then compiled into a Docker image. Docker images can then be released to public or private registries, becoming immediately available over the Internet. Therefore, by running the Docker engine, the end users can conveniently start the released containers locally.</p>
      </sec>
      <sec id="sec2-2-2">
        <title>Volumes</title>
        <p>When using Docker containers for data processing, volumes play an important role. Indeed, there is a need for a mechanism to pass the input data to the containers and to retrieve the processed output from the isolated environment. Docker volumes allow for defining shared file spaces between containers and the host OS. Such volumes can be easily created when starting containers, by specifying a mapping between host OS file, or directories, and container mount points. Inside the containers these shared objects simply appear as regular files, or directories, under the specified mount point.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Availability of Supporting Source Code and Requirements</title>
    <p>Project name: MaRe</p>
    <p>Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/mcapuccini/MaRe">https://github.com/mcapuccini/MaRe</ext-link></p>
    <p>Operating system(s): Platform independent</p>
    <p>Programming language: Scala</p>
    <p>Other requirements: Apache Spark and Docker</p>
    <p>License: Apache License 2.0</p>
    <p>
      <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/RRID:SCR_018069">RRID:SCR_018069</ext-link>
    </p>
  </sec>
  <sec sec-type="materials" id="sec4">
    <title>Availability of Supporting Data and Materials</title>
    <p>The dataset supporting the VS evaluation in this article is available in the ZINC database [<xref rid="bib47" ref-type="bibr">47</xref>]. The specific subset that we used is available at <ext-link ext-link-type="uri" xlink:href="http://zinc12.docking.org/catalogs/surechembl">http://zinc12.docking.org/catalogs/surechembl</ext-link>.</p>
    <p>The 1KPG [<xref rid="bib50" ref-type="bibr">50</xref>] dataset supporting the SNP evaluation is available on Amazon S3 (<ext-link ext-link-type="uri" xlink:href="https://www.s3//1000genomes/phase3/data/HG02666">s3://1000genomes/phase3/data/HG02666</ext-link>). The relative BioProject accession number is <monospace>PRJNA28889</monospace>.</p>
    <p>Images, results in tabular format, and an archival copy of the code are also available via GigaDB [<xref rid="bib76" ref-type="bibr">76</xref>].</p>
  </sec>
  <sec id="sec5">
    <title>Abbreviations</title>
    <p>1KGP: 1000 Genome Project; API: application programming interface; bp: base pairs; BWA: Burrows-Wheeler Aligner; CSC: Information Technology Center for Science; GATK: Genome Analysis Toolkit; GC: guanine-cytosine; HDFS: Hadoop distributed file system; HIV: human immunodeficiency virus; HPC: high-performance computing; OS: operating system; POSIX: Portable Operating System Interface; RDD: resilient distributed dataset; SDF: structure-data file; SNP: single-nucleotide polymorphism; SSD: solid state drive; SSE: strong scaling efficiency; VCF: variant call format; VM: virtual machine; VS: virtual screening; WSE: weak scaling efficiency.</p>
  </sec>
  <sec id="sec6">
    <title>Ethics Approval and Consent to Participate</title>
    <p>All of the 1KGP data are consented for analysis, publication, and distribution. Ethics and consents are extensively explained in the 1KGP publications [<xref rid="bib50" ref-type="bibr">50</xref>].</p>
  </sec>
  <sec id="sec7">
    <title>Competing Interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </sec>
  <sec sec-type="funding" id="sec8">
    <title>Funding</title>
    <p>This research was supported by The European Commission’s Horizon 2020 program under grant agreement No. 654241 (PhenoMeNal).</p>
  </sec>
  <sec id="sec9">
    <title>Authors' Contributions</title>
    <p>M.C. and O.S. conceived the project. M.C. designed and implemented MaRe. M.C. and M.D. carried out the evaluation experiments. M.D. provided expertise in genomics. S.T. provided expertise in cloud computing. All authors read and approved the final manuscript.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>giaa042_GIGA-D-19-00170_Original_Submission</label>
      <media xlink:href="giaa042_giga-d-19-00170_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup2">
      <label>giaa042_GIGA-D-19-00170_Revision_1</label>
      <media xlink:href="giaa042_giga-d-19-00170_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup3">
      <label>giaa042_GIGA-D-19-00170_Revision_2</label>
      <media xlink:href="giaa042_giga-d-19-00170_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup4">
      <label>giaa042_GIGA-D-19-00170_Revision_3</label>
      <media xlink:href="giaa042_giga-d-19-00170_revision_3.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup5">
      <label>giaa042_Response_to_Reviewer_Comments_Original_Submission</label>
      <media xlink:href="giaa042_response_to_reviewer_comments_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup6">
      <label>giaa042_Response_to_Reviewer_Comments_Revision_1</label>
      <media xlink:href="giaa042_response_to_reviewer_comments_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup7">
      <label>giaa042_Response_to_Reviewer_Comments_Revision_2</label>
      <media xlink:href="giaa042_response_to_reviewer_comments_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup8">
      <label>giaa042_Reviewer_1_Report_Original_Submission</label>
      <caption>
        <p>Umberto Ferraro Petrillo -- 6/14/2019 Reviewed</p>
      </caption>
      <media xlink:href="giaa042_reviewer_1_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup9">
      <label>giaa042_Reviewer_1_Report_Revision_1</label>
      <caption>
        <p>Umberto Ferraro Petrillo -- 2/19/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa042_reviewer_1_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup10">
      <label>giaa042_Reviewer_2_Report_Original_Submission</label>
      <caption>
        <p>Andrew Lonie -- 6/24/2019 Reviewed</p>
      </caption>
      <media xlink:href="giaa042_reviewer_2_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>ACKNOWLEDGEMENTS</title>
    <p>We kindly acknowledge contributions to cloud resources by CSC (<ext-link ext-link-type="uri" xlink:href="https://www.csc.fi">https://www.csc.fi</ext-link>), the Nordic e-Infrastructure Collaboration (<ext-link ext-link-type="uri" xlink:href="https://neic.no">https://neic.no</ext-link>), and the SNIC Science Cloud [<xref rid="bib77" ref-type="bibr">77</xref>]. The academic license for the docking software was provided by OpenEye Scientific.</p>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="bib1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stephens</surname><given-names>ZD</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>SY</given-names></name>, <name name-style="western"><surname>Faghri</surname><given-names>F</given-names></name>, <etal>et al</etal>.</person-group><article-title>Big data: astronomical or genomical?</article-title>. <source>PLoS Biol</source>. <year>2015</year>; <volume>13</volume>(<issue>7</issue>):<fpage>e1002195</fpage>.<pub-id pub-id-type="pmid">26151137</pub-id></mixed-citation>
    </ref>
    <ref id="bib2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Foster</surname><given-names>LJ</given-names></name>, <name name-style="western"><surname>DeMarco</surname><given-names>ML</given-names></name></person-group><article-title>At the intersection of proteomics and big data science</article-title>. <source>Clin Chem</source>. <year>2017</year>;<volume>63</volume>(<issue>10</issue>):<fpage>1663</fpage>.<pub-id pub-id-type="pmid">32100821</pub-id></mixed-citation>
    </ref>
    <ref id="bib3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Peters</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Bradbury</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Bergmann</surname><given-names>S</given-names></name>, <etal>et al</etal>.</person-group><article-title>PhenoMeNal: Processing and analysis of metabolomics data in the cloud</article-title>. <source>Gigascience</source>. <year>2018</year>;<volume>8</volume>(<issue>2</issue>):<fpage>giy149</fpage>.</mixed-citation>
    </ref>
    <ref id="bib4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>H</given-names></name></person-group><article-title>Bioimage informatics: a new area of engineering biology</article-title>. <source>Bioinformatics</source>. <year>2008</year>;<volume>24</volume>(<issue>17</issue>):<fpage>1827</fpage>–<lpage>36</lpage>.<pub-id pub-id-type="pmid">18603566</pub-id></mixed-citation>
    </ref>
    <ref id="bib5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brown</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Cambruzzi</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Cox</surname><given-names>PJ</given-names></name>, <etal>et al</etal>.</person-group><article-title>Big Data in drug discovery</article-title>. <source>Prog Med Chem</source>. <year>2018</year>;<volume>57</volume>:<fpage>277</fpage>–<lpage>356</lpage>.<pub-id pub-id-type="pmid">29680150</pub-id></mixed-citation>
    </ref>
    <ref id="bib6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cook</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Lopez</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Stroe</surname><given-names>O</given-names></name>, <etal>et al</etal>.</person-group><article-title>The European Bioinformatics Institute in 2018: tools, infrastructure and training</article-title>. <source>Nucleic Acids Res</source>. <year>2018</year>;<volume>47</volume>(<issue>D1</issue>):<fpage>D15</fpage>–<lpage>D22</lpage>.</mixed-citation>
    </ref>
    <ref id="bib7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tan</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Meng</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>L</given-names></name></person-group><article-title>Delay tails in MapReduce scheduling</article-title>. <source>ACM SIGMETRICS Perform Eval Rev</source>. <year>2012</year>;<volume>40</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>16</lpage>.</mixed-citation>
    </ref>
    <ref id="bib8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><collab>Lawrence Livermore National Laboratory</collab>. <article-title>Gearing up for the next challenge in high-performance computing</article-title>. <year>2015</year><ext-link ext-link-type="uri" xlink:href="https://str.llnl.gov/march-2015/still">https://str.llnl.gov/march-2015/still</ext-link>. <comment>Accessed 25 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Convolbo</surname><given-names>MW</given-names></name>, <name name-style="western"><surname>Chou</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hsu</surname><given-names>CH</given-names></name>, <etal>et al</etal>.</person-group><article-title>GEODIS: towards the optimization of data locality-aware job scheduling in geo-distributed data centers</article-title>. <source>Computing</source>. <year>2018</year>;<volume>100</volume>(<issue>1</issue>):<fpage>21</fpage>–<lpage>46</lpage>.</mixed-citation>
    </ref>
    <ref id="bib10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fox</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Griffith</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Joseph</surname><given-names>A</given-names></name>, <etal>et al</etal>.</person-group><article-title>Above the clouds: A Berkeley view of cloud computing</article-title>. <source>Dept Electrical Eng Comput Sci Univ Calif Berkeley Rep UCB/EECS</source>. <year>2009</year>;<volume>28</volume>(<issue>13</issue>):<fpage>2009</fpage>.</mixed-citation>
    </ref>
    <ref id="bib11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mansouri</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Toosi</surname><given-names>AN</given-names></name>, <name name-style="western"><surname>Buyya</surname><given-names>R</given-names></name></person-group><article-title>Data storage management in cloud environments: Taxonomy, survey, and future directions</article-title>. <source>ACM Comput Surv</source>. <year>2018</year>;<volume>50</volume>(<issue>6</issue>):<fpage>91</fpage>.</mixed-citation>
    </ref>
    <ref id="bib12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Williams</surname><given-names>CL</given-names></name>, <name name-style="western"><surname>Sica</surname><given-names>JC</given-names></name>, <name name-style="western"><surname>Killen</surname><given-names>RT</given-names></name>, <etal>et al</etal>.</person-group><article-title>The growing need for microservices in bioinformatics</article-title>. <source>J Pathol Inform</source>. <year>2016</year>;<volume>7</volume>:<fpage>45</fpage>.<pub-id pub-id-type="pmid">27994937</pub-id></mixed-citation>
    </ref>
    <ref id="bib13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Leipzig</surname><given-names>J</given-names></name></person-group><article-title>A review of bioinformatic pipeline frameworks</article-title>. <source>Brief Bioinform</source>. <year>2017</year>;<volume>18</volume>(<issue>3</issue>):<fpage>530</fpage>–<lpage>6</lpage>.<pub-id pub-id-type="pmid">27013646</pub-id></mixed-citation>
    </ref>
    <ref id="bib14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lampa</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Alvarsson</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Spjuth</surname><given-names>O</given-names></name></person-group><article-title>Towards agile large-scale predictive modelling in drug discovery with flow-based programming design principles</article-title>. <source>J Cheminform</source>. <year>2016</year>;<volume>8</volume>(<issue>1</issue>):<fpage>67</fpage>.<pub-id pub-id-type="pmid">27942268</pub-id></mixed-citation>
    </ref>
    <ref id="bib15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Di Tommaso</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Chatzou</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Floden</surname><given-names>EW</given-names></name>, <etal>et al</etal>.</person-group><article-title>Nextflow enables reproducible computational workflows</article-title>. <source>Nat Biotechnol</source>. <year>2017</year>;<volume>35</volume>(<issue>4</issue>):<fpage>316</fpage>.<pub-id pub-id-type="pmid">28398311</pub-id></mixed-citation>
    </ref>
    <ref id="bib16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moreno</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Pireddu</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Roger</surname><given-names>P</given-names></name>, <etal>et al</etal>.</person-group><article-title>Galaxy-Kubernetes integration: scaling bioinformatics workflows in the cloud</article-title>. <source>BioRxiv</source>. <year>2018</year>:<fpage>488643</fpage>.</mixed-citation>
    </ref>
    <ref id="bib17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Novella</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Emami Khoonsari</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Herman</surname><given-names>S</given-names></name>, <etal>et al</etal>.</person-group><article-title>Container-based bioinformatics with Pachyderm</article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>35</volume>(<issue>5</issue>):<fpage>839</fpage>–<lpage>46</lpage>.</mixed-citation>
    </ref>
    <ref id="bib18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><collab>Open Container Initiative</collab>. <source>The 5 principles of Standard Containers</source>. <year>2016</year><ext-link ext-link-type="uri" xlink:href="https://github.com/opencontainers/runtime-spec/blob/master/principles.md">https://github.com/opencontainers/runtime-spec/blob/master/principles.md</ext-link>. Accessed 25 April 2019.</mixed-citation>
    </ref>
    <ref id="bib19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dean</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Ghemawat</surname><given-names>S</given-names></name></person-group><article-title>MapReduce: simplified data processing on large clusters</article-title>. <source>Commun ACM</source>. <year>2008</year>;<volume>51</volume>(<issue>1</issue>):<fpage>107</fpage>–<lpage>13</lpage>.</mixed-citation>
    </ref>
    <ref id="bib20">
      <label>20.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bhandarkar</surname><given-names>M</given-names></name></person-group><article-title>MapReduce programming with Apache Hadoop</article-title>. In: <source>2010 IEEE International Symposium on Parallel and Distributed Processing (IPDPS), Atlanta, GA</source>. <year>2010</year>, doi:<pub-id pub-id-type="doi">10.1109/IPDPS.2010.5470377</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib21">
      <label>21.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gunarathne</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>TL</given-names></name>, <name name-style="western"><surname>Qiu</surname><given-names>J</given-names></name>, <etal>et al</etal>.</person-group><article-title>MapReduce in the clouds for science</article-title>. In: <source>2010 IEEE Second International Conference on Cloud Computing Technology and Science</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2010</year>:<fpage>565</fpage>–<lpage>72</lpage>.</mixed-citation>
    </ref>
    <ref id="bib22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mohammed</surname><given-names>EA</given-names></name>, <name name-style="western"><surname>Far</surname><given-names>BH</given-names></name>, <name name-style="western"><surname>Naugler</surname><given-names>C</given-names></name></person-group><article-title>Applications of the MapReduce programming framework to clinical big data analysis: current landscape and future trends</article-title>. <source>BioData Min</source>. <year>2014</year>;<volume>7</volume>(<issue>1</issue>):<fpage>22</fpage>.<pub-id pub-id-type="pmid">25383096</pub-id></mixed-citation>
    </ref>
    <ref id="bib23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Zhao</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Zou</surname><given-names>Q</given-names></name>, <etal>et al</etal>.</person-group><article-title>Bioinformatics applications on Apache Spark</article-title>. <source>Gigacience</source>. <year>2018</year>;<volume>7</volume>(<issue>8</issue>):<fpage>giy098</fpage>.</mixed-citation>
    </ref>
    <ref id="bib24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schönherr</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Forer</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Weißensteiner</surname><given-names>H</given-names></name>, <etal>et al</etal>.</person-group><article-title>Cloudgene: a graphical execution platform for MapReduce programs on private and public clouds</article-title>. <source>BMC Bioinform</source>. <year>2012</year>;<volume>13</volume>(<issue>1</issue>):<fpage>200</fpage>.</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zaharia</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Xin</surname><given-names>RS</given-names></name>, <name name-style="western"><surname>Wendell</surname><given-names>P</given-names></name>, <etal>et al</etal>.</person-group><article-title>Apache spark: a unified engine for big data processing</article-title>. <source>Commun ACM</source>. <year>2016</year>;<volume>59</volume>(<issue>11</issue>):<fpage>56</fpage>–<lpage>65</lpage>.</mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Zheng</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Lu</surname><given-names>Y</given-names></name>, <etal>et al</etal>.</person-group><article-title>More convenient more overhead: the performance evaluation of Hadoop streaming</article-title>. In: <source>Proceedings of the 2011 ACM Symposium on Research in Applied Computation</source>. <year>2011</year>:<fpage>307</fpage>–<lpage>13</lpage>.</mixed-citation>
    </ref>
    <ref id="bib27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shimel</surname><given-names>A</given-names></name></person-group><source>Docker becomes de facto Linux standard</source>. <year>2016</year><ext-link ext-link-type="uri" xlink:href="http://www.networkworld.com/article/2226751/opensource-subnet/docker-becomes-de-facto-linux-standard.html">http://www.networkworld.com/article/2226751/opensource-subnet/docker-becomes-de-facto-linux-standard.html</ext-link>. Accessed 25 April 2019.</mixed-citation>
    </ref>
    <ref id="bib28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kudla</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Lipinski</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Caffin</surname><given-names>F</given-names></name>, <etal>et al</etal>.</person-group><article-title>High guanine and cytosine content increases mRNA levels in mammalian cells</article-title>. <source>PLoS Biol</source>. <year>2006</year>;<volume>4</volume>(<issue>6</issue>):<fpage>e180</fpage>.<pub-id pub-id-type="pmid">16700628</pub-id></mixed-citation>
    </ref>
    <ref id="bib29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><comment>Ubuntu Docker Image. <ext-link ext-link-type="uri" xlink:href="https://hub.docker.com/_/ubuntu">https://hub.docker.com/_/ubuntu</ext-link>. Accessed 25 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib30">
      <label>30.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Odersky</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Altherr</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Cremet</surname><given-names>V</given-names></name>, <etal>et al</etal>.</person-group><source>An overview of the Scala programming language</source>. Technical Report LAMP-REPORT-2006-001. École Polytechnique Fédérale de Lausanne (EPFL) 1015 Lausanne, Switzerland. <year>2006</year>.</mixed-citation>
    </ref>
    <ref id="bib31">
      <label>31.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Kluyver</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ragan-Kelley</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Pérez</surname><given-names>F</given-names></name>, <etal>et al</etal>.</person-group><article-title>Jupyter Notebooks-a publishing format for reproducible computational workflows</article-title>. In: <source>ELPUB</source>. <year>2016</year>:<fpage>87</fpage>–<lpage>90</lpage>.</mixed-citation>
    </ref>
    <ref id="bib32">
      <label>32.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>FC</given-names></name>, <name name-style="western"><surname>Jing</surname><given-names>S</given-names></name>, <etal>et al</etal>.</person-group><article-title>Building big data processing and visualization pipeline through Apache Zeppelin</article-title>. In: <source>PEARC ’18: Practice and Experience in Advanced Research Computing, Pittsburgh, PA</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>; <year>2018</year>:<fpage>57</fpage>, doi:<pub-id pub-id-type="doi">10.1145/3219104.3229288</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zaharia</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Chowdhury</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Das</surname><given-names>T</given-names></name>, <etal>et al</etal>.</person-group><article-title>Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing</article-title>. In: <source>Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation</source>. <year>2012</year>:<fpage>15</fpage>–<lpage>28</lpage>.</mixed-citation>
    </ref>
    <ref id="bib34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Laskowski</surname><given-names>J</given-names></name>, <collab>HashPartitioner</collab></person-group><ext-link ext-link-type="uri" xlink:href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-HashPartitioner.html">https://books.japila.pl/apache-spark-internals/apache-spark-internals/latest/rdd/spark-rdd-HashPartitioner.html</ext-link>. Accessed 28 April 2020.</mixed-citation>
    </ref>
    <ref id="bib35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Peek</surname><given-names>J</given-names></name>, <name name-style="western"><surname>O’Reilly</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Loukides</surname><given-names>M</given-names></name></person-group><source>UNIX power tools</source>. <year>O'Reilly media. 1997</year>.ISBN 1-56592-260-3.</mixed-citation>
    </ref>
    <ref id="bib36">
      <label>36.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Tevanian</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Rashid</surname><given-names>RF</given-names></name>, <name name-style="western"><surname>Young</surname><given-names>M</given-names></name>, <etal>et al</etal>.</person-group><article-title>A UNIX interface for shared memory and memory mapped files under Mach</article-title>. In: <source>USENIX Summer Citeseer</source>. <year>1987</year>:<fpage>53</fpage>–<lpage>68</lpage>.</mixed-citation>
    </ref>
    <ref id="bib37">
      <label>37.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Snyder</surname><given-names>P</given-names></name></person-group><article-title>tmpfs: A virtual memory file system</article-title>. In: <source>Proceedings of the Autumn 1990 EUUG Conference</source>. <year>1990</year>:<fpage>241</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="bib38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><comment>cPouta Community Cloud. <ext-link ext-link-type="uri" xlink:href="https://research.csc.fi/cpouta">https://research.csc.fi/cpouta</ext-link>. Accessed 25 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><comment>MaRe Benchmarks. <ext-link ext-link-type="uri" xlink:href="https://github.com/mcapuccini/mare-benchmarks">https://github.com/mcapuccini/mare-benchmarks</ext-link>. Accessed 25 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><comment>OpenStack Apache Spark Terraform Module. <ext-link ext-link-type="uri" xlink:href="https://github.com/mcapuccini/terraform-openstack-spark">https://github.com/mcapuccini/terraform-openstack-spark</ext-link>. Accessed 25 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheng</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Zhou</surname><given-names>Z</given-names></name>, <etal>et al</etal>.</person-group><article-title>Structure-based virtual screening for drug discovery: a problem-centric review</article-title>. <source>AAPS J</source>. <year>2012</year>;<volume>14</volume>(<issue>1</issue>):<fpage>133</fpage>–<lpage>41</lpage>.<pub-id pub-id-type="pmid">22281989</pub-id></mixed-citation>
    </ref>
    <ref id="bib42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McGann</surname><given-names>M</given-names></name></person-group><article-title>FRED pose prediction and virtual screening accuracy</article-title>. <source>J Chem Inform Model</source>. <year>2011</year>;<volume>51</volume>(<issue>3</issue>):<fpage>578</fpage>–<lpage>96</lpage>.</mixed-citation>
    </ref>
    <ref id="bib43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><comment>sdsorter. <ext-link ext-link-type="uri" xlink:href="https://sourceforge.net/projects/sdsorter">https://sourceforge.net/projects/sdsorter</ext-link>. Accessed 25 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dalby</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Nourse</surname><given-names>JG</given-names></name>, <name name-style="western"><surname>Hounshell</surname><given-names>WD</given-names></name>, <etal>et al</etal>.</person-group><article-title>Description of several chemical structure file formats used by computer programs developed at Molecular Design Limited</article-title>. <source>J Chem Inform Comput Sci</source>. <year>1992</year>;<volume>32</volume>(<issue>3</issue>):<fpage>244</fpage>–<lpage>55</lpage>.</mixed-citation>
    </ref>
    <ref id="bib45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bäckbro</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Löwgren</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Österlund</surname><given-names>K</given-names></name>, <etal>et al</etal>.</person-group><article-title>Unexpected binding mode of a cyclic sulfamide HIV-1 protease inhibitor</article-title>. <source>J Med Chem</source>. <year>1997</year>;<volume>40</volume>(<issue>6</issue>):<fpage>898</fpage>–<lpage>902</lpage>.<pub-id pub-id-type="pmid">9083478</pub-id></mixed-citation>
    </ref>
    <ref id="bib46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Papadatos</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Davies</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Dedman</surname><given-names>N</given-names></name>, <etal>et al</etal>.</person-group><article-title>SureChEMBL: a large-scale, chemically annotated patent document database</article-title>. <source>Nucleic Acids Res</source>. <year>2015</year>;<volume>44</volume>(<issue>D1</issue>):<fpage>D1220</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">26582922</pub-id></mixed-citation>
    </ref>
    <ref id="bib47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Irwin</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Sterling</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Mysinger</surname><given-names>MM</given-names></name>, <etal>et al</etal>.</person-group><article-title>ZINC: a free tool to discover chemistry for biology</article-title>. <source>J Chem Inform Model</source>. <year>2012</year>;<volume>52</volume>(<issue>7</issue>):<fpage>1757</fpage>–<lpage>68</lpage>.</mixed-citation>
    </ref>
    <ref id="bib48">
      <label>48.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Mathur</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Cao</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Bhattacharya</surname><given-names>S</given-names></name>, <etal>et al</etal>.</person-group><article-title>The new ext4 filesystem: current status and future plans</article-title>. In: <source>Proceedings of the Linux symposium</source>, vol. <volume>2</volume><publisher-name>World Scientific and Engineering Academy and Society (WSEAS)</publisher-name>; <year>2007</year>:<fpage>21</fpage>–<lpage>33</lpage>.</mixed-citation>
    </ref>
    <ref id="bib49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Karki</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Pandya</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Elston</surname><given-names>RC</given-names></name>, <etal>et al</etal>.</person-group><article-title>Defining ”mutation” and ”polymorphism” in the era of personal genomics</article-title>. <source>BMC Med Genomics</source>. <year>2015</year>;<volume>8</volume>(<issue>1</issue>):<fpage>37</fpage>.<pub-id pub-id-type="pmid">26173390</pub-id></mixed-citation>
    </ref>
    <ref id="bib50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><collab>1000 Genomes Project Consortium</collab><person-group person-group-type="author"><name name-style="western"><surname>Auton</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Brooks</surname><given-names>LD</given-names></name>, <name name-style="western"><surname>Durbin</surname><given-names>RM</given-names></name>1000 Genomes Project Consortium, <etal>et al</etal>.</person-group><article-title>A global reference for human genetic variation</article-title>. <source>Nature</source>. <year>2015</year>;<volume>526</volume>(<issue>7571</issue>):<fpage>68</fpage>–<lpage>74</lpage>.<pub-id pub-id-type="pmid">26432245</pub-id></mixed-citation>
    </ref>
    <ref id="bib51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Collins</surname><given-names>FS</given-names></name></person-group><article-title>Medical and societal consequences of the Human Genome Project</article-title>. <source>N Engl J Med</source>. <year>1999</year>;<volume>341</volume>(<issue>1</issue>):<fpage>28</fpage>–<lpage>37</lpage>.<pub-id pub-id-type="pmid">10387940</pub-id></mixed-citation>
    </ref>
    <ref id="bib52">
      <label>52.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kruglyak</surname><given-names>L</given-names></name></person-group><article-title>Prospects for whole-genome linkage disequilibrium mapping of common disease genes</article-title>. <source>Nat Genet</source>. <year>1999</year>;<volume>22</volume>(<issue>2</issue>):<fpage>139</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">10369254</pub-id></mixed-citation>
    </ref>
    <ref id="bib53">
      <label>53.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Durbin</surname><given-names>R</given-names></name></person-group><article-title>Fast and accurate short read alignment with Burrows–Wheeler transform</article-title>. <source>Bioinformatics</source>. <year>2009</year>;<volume>25</volume>(<issue>14</issue>):<fpage>1754</fpage>–<lpage>60</lpage>.<pub-id pub-id-type="pmid">19451168</pub-id></mixed-citation>
    </ref>
    <ref id="bib54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>McKenna</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Hanna</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Banks</surname><given-names>E</given-names></name>, <etal>et al</etal>.</person-group><article-title>The Genome Analysis Toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data</article-title>. <source>Genome Res</source>. <year>2010</year>;<volume>20</volume>(<issue>9</issue>):<fpage>1297</fpage>–<lpage>303</lpage>.<pub-id pub-id-type="pmid">20644199</pub-id></mixed-citation>
    </ref>
    <ref id="bib55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cock</surname><given-names>PJA</given-names></name>, <name name-style="western"><surname>Fields</surname><given-names>CJ</given-names></name>, <name name-style="western"><surname>Goto</surname><given-names>N</given-names></name>, <etal>et al</etal>.</person-group><article-title>The Sanger FASTQ file format for sequences with quality scores, and the Solexa/Illumina FASTQ variants</article-title>. <source>Nucleic Acids Res</source>. <year>2010</year>;<volume>38</volume>(<issue>6</issue>):<fpage>1767</fpage>–<lpage>71</lpage>.<pub-id pub-id-type="pmid">20015970</pub-id></mixed-citation>
    </ref>
    <ref id="bib56">
      <label>56.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Handsaker</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Wysoker</surname><given-names>A</given-names></name>, <etal>et al</etal>.</person-group><article-title>The Sequence Alignment/Map format and SAMtools</article-title>. <source>Bioinformatics</source>. <year>2009</year>;<volume>25</volume>(<issue>16</issue>):<fpage>2078</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">19505943</pub-id></mixed-citation>
    </ref>
    <ref id="bib57">
      <label>57.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Danecek</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Auton</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Abecasis</surname><given-names>G</given-names></name>, <etal>et al</etal>.</person-group><article-title>The variant call format and VCFtools</article-title>. <source>Bioinformatics</source>. <year>2011</year>;<volume>27</volume>(<issue>15</issue>):<fpage>2156</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">21653522</pub-id></mixed-citation>
    </ref>
    <ref id="bib58">
      <label>58.</label>
      <mixed-citation publication-type="journal"><collab>GATK Documentation</collab>. <ext-link ext-link-type="uri" xlink:href="https://software.broadinstitute.org/gatk/documentation/tooldocs/current">https://software.broadinstitute.org/gatk/documentation/tooldocs/current</ext-link>. <comment>Accessed 25 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib59">
      <label>59.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khanam</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Agarwal</surname><given-names>S</given-names></name></person-group><article-title>Map-reduce implementations: survey and performance comparison</article-title>. <source>Int J Comput Sci Inf Technol</source>. <year>2015</year>;<volume>7</volume>(<issue>4</issue>), doi:<pub-id pub-id-type="doi">10.5121/ijcsit.2015.7410</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib60">
      <label>60.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chaimov</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Malony</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Canon</surname><given-names>S</given-names></name>, <etal>et al</etal>.</person-group><article-title>Scaling Spark on HPC systems</article-title>. In: <source>Proceedings of the 25th ACM International Symposium on High-Performance Parallel and Distributed Computing</source>. <publisher-name>ACM</publisher-name>; <year>2016</year>:<fpage>97</fpage>–<lpage>110</lpage>.</mixed-citation>
    </ref>
    <ref id="bib61">
      <label>61.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Nothaft</surname><given-names>FA</given-names></name>, <name name-style="western"><surname>Massie</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Danford</surname><given-names>T</given-names></name>, <etal>et al</etal>.</person-group><article-title>Rethinking data-intensive science using scalable analytics systems</article-title>. In: <source>Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data</source>. <year>2015</year>:<fpage>631</fpage>–<lpage>646</lpage>.</mixed-citation>
    </ref>
    <ref id="bib62">
      <label>62.</label>
      <mixed-citation publication-type="journal"><comment>Using ADAM’s Pipe API</comment> . <ext-link ext-link-type="uri" xlink:href="https://adam.readthedocs.io/en/latest/api/pipes/">https://adam.readthedocs.io/en/latest/api/pipes/</ext-link>. <comment>Accessed 25 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib63">
      <label>63.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Duck</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Nenadic</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Filannino</surname><given-names>M</given-names></name>, <etal>et al</etal>.</person-group><article-title>A survey of bioinformatics database and software usage through mining the literature</article-title>. <source>PLoS One</source>. <year>2016</year>;<volume>11</volume>(<issue>6</issue>):<fpage>e0157989</fpage>.<pub-id pub-id-type="pmid">27331905</pub-id></mixed-citation>
    </ref>
    <ref id="bib64">
      <label>64.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dahlö</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Scofield</surname><given-names>DG</given-names></name>, <name name-style="western"><surname>Schaal</surname><given-names>W</given-names></name>, <etal>et al</etal>.</person-group><article-title>Tracking the NGS revolution: managing life science research on shared high-performance computing clusters</article-title>. <source>Gigascience</source>. <year>2018</year>;<volume>7</volume>(<issue>5</issue>), doi:<pub-id pub-id-type="doi">10.1093/gigascience/giy028</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib65">
      <label>65.</label>
      <mixed-citation publication-type="journal"><comment>MaRe</comment><ext-link ext-link-type="uri" xlink:href="https://github.com/mcapuccini/MaRe">https://github.com/mcapuccini/MaRe</ext-link>. <comment>Accessed 25 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib66">
      <label>66.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zaharia</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Chowdhury</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Franklin</surname><given-names>MJ</given-names></name>, <etal>et al</etal>.</person-group><article-title>Spark: cluster computing with working sets</article-title>. In: <source>HotCloud'10: Proceedings of the 2nd USENIX conference on hot topics in cloud computing</source>. <publisher-loc>Berkeley, CA</publisher-loc>: <publisher-name>USENIX Assoc</publisher-name>; <year>2010</year>:<fpage>95</fpage>.</mixed-citation>
    </ref>
    <ref id="bib67">
      <label>67.</label>
      <mixed-citation publication-type="journal"><comment>Kubernetes</comment><ext-link ext-link-type="uri" xlink:href="https://kubernetes.io">https://kubernetes.io.</ext-link><comment>Accessed 25 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib68">
      <label>68.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Hindman</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Konwinski</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Zaharia</surname><given-names>M</given-names></name>, <etal>et al</etal>.</person-group><article-title>Mesos: a platform for fine-grained resource sharing in the data center</article-title>. In: <source>NSDI'11: Proceedings of the 8th USENIX conference on Networked Systems Design and Implementation, Boston, MA</source>. <publisher-loc>Berkeley, CA</publisher-loc>: <publisher-name>USENIX Assoc</publisher-name>; <year>2011</year>:<fpage>295</fpage>–<lpage>308</lpage>.</mixed-citation>
    </ref>
    <ref id="bib69">
      <label>69.</label>
      <mixed-citation publication-type="journal"><comment>Hadoop YARN</comment><ext-link ext-link-type="uri" xlink:href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html">https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html</ext-link>. <comment>Accessed 25 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib70">
      <label>70.</label>
      <mixed-citation publication-type="journal"><comment>The Python Programming Language</comment><ext-link ext-link-type="uri" xlink:href="http://www.python.org">http://www.python.org</ext-link>. <comment>Accessed 25 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib71">
      <label>71.</label>
      <mixed-citation publication-type="journal"><comment>The Java Programming Language</comment><ext-link ext-link-type="uri" xlink:href="https://docs.oracle.com/javase/8/docs/technotes/guides/language/index.html">https://docs.oracle.com/javase/8/docs/technotes/guides/language/index.html</ext-link>. <comment>Accessed 25 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib72">
      <label>72.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ihaka</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Gentleman</surname><given-names>R</given-names></name></person-group><article-title>R: a language for data analysis and graphics</article-title>. <source>J Comput Graph Stat</source>. <year>1996</year>;<volume>5</volume>(<issue>3</issue>):<fpage>299</fpage>–<lpage>314</lpage>.</mixed-citation>
    </ref>
    <ref id="bib73">
      <label>73.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Laskowski</surname><given-names>J</given-names></name></person-group><comment>Partitioner</comment><ext-link ext-link-type="uri" xlink:href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-partitions.html">https://books.japila.pl/apache-spark-internals/apache-spark-internals/latest/rdd/spark-rdd-Partitioner.html</ext-link>. <comment>Accessed 28 April 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib74">
      <label>74.</label>
      <mixed-citation publication-type="journal"><comment>LXC</comment><ext-link ext-link-type="uri" xlink:href="https://linuxcontainers.org/">https://linuxcontainers.org/</ext-link>. <comment>Accessed 25 April 2019</comment>.</mixed-citation>
    </ref>
    <ref id="bib75">
      <label>75.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kamp</surname><given-names>PH</given-names></name>, <name name-style="western"><surname>Watson</surname><given-names>RN</given-names></name></person-group><article-title>Jails: Confining the omnipotent root</article-title>. In: <source>Proceedings of the 2nd International SANE Conference</source>, vol. <volume>43</volume>; <year>2000</year>:<fpage>116</fpage>.</mixed-citation>
    </ref>
    <ref id="bib76">
      <label>76.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Capuccini</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Dahlö</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Toor</surname><given-names>S</given-names></name>, <etal>et al</etal>.</person-group><article-title>Supporting data for ”MaRe: Processing Big Data with application containers on Apache Spark.”</article-title>. <source>GigaScience Database</source>. <year>2020</year><pub-id pub-id-type="doi">10.5524/100733</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib77">
      <label>77.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Toor</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Lindberg</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Falman</surname><given-names>I</given-names></name>, <etal>et al</etal>.</person-group><article-title>SNIC science cloud (SSC): A national-scale cloud infrastructure for Swedish academia</article-title>. In: <source>2017 IEEE 13th International Conference on e-Science (e-Science)</source>. <publisher-name>IEEE</publisher-name>; <year>2017</year>:<fpage>219</fpage>–<lpage>27</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
