<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with OASIS Tables with MathML3 v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archive-oasis-article1-mathml3.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats-oasis2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">New Phytol</journal-id>
    <journal-id journal-id-type="iso-abbrev">New Phytol</journal-id>
    <journal-id journal-id-type="doi">10.1111/(ISSN)1469-8137</journal-id>
    <journal-id journal-id-type="publisher-id">NPH</journal-id>
    <journal-title-group>
      <journal-title>The New Phytologist</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0028-646X</issn>
    <issn pub-type="epub">1469-8137</issn>
    <publisher>
      <publisher-name>John Wiley and Sons Inc.</publisher-name>
      <publisher-loc>Hoboken</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9804377</article-id>
    <article-id pub-id-type="pmid">35851958</article-id>
    <article-id pub-id-type="doi">10.1111/nph.18387</article-id>
    <article-id pub-id-type="publisher-id">NPH18387</article-id>
    <article-id pub-id-type="other">2022-39507</article-id>
    <article-categories>
      <subj-group subj-group-type="overline">
        <subject>Methods</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
        <subj-group subj-group-type="heading">
          <subject>Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>R<sc>oot</sc>P<sc>ainter</sc>: deep learning segmentation of biological images with corrective annotation</article-title>
    </title-group>
    <contrib-group>
      <contrib id="nph18387-cr-0001" contrib-type="author" corresp="yes">
        <name>
          <surname>Smith</surname>
          <given-names>Abraham George</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-9782-2825</contrib-id>
        <xref rid="nph18387-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="nph18387-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <address>
          <email>ags@di.ku.dk</email>
        </address>
      </contrib>
      <contrib id="nph18387-cr-0002" contrib-type="author">
        <name>
          <surname>Han</surname>
          <given-names>Eusun</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6338-2454</contrib-id>
        <xref rid="nph18387-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="nph18387-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib id="nph18387-cr-0003" contrib-type="author">
        <name>
          <surname>Petersen</surname>
          <given-names>Jens</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-0138-0693</contrib-id>
        <xref rid="nph18387-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib id="nph18387-cr-0004" contrib-type="author">
        <name>
          <surname>Olsen</surname>
          <given-names>Niels Alvin Faircloth</given-names>
        </name>
        <xref rid="nph18387-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib id="nph18387-cr-0005" contrib-type="author">
        <name>
          <surname>Giese</surname>
          <given-names>Christian</given-names>
        </name>
        <xref rid="nph18387-aff-0004" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib id="nph18387-cr-0006" contrib-type="author">
        <name>
          <surname>Athmann</surname>
          <given-names>Miriam</given-names>
        </name>
        <xref rid="nph18387-aff-0005" ref-type="aff">
          <sup>5</sup>
        </xref>
      </contrib>
      <contrib id="nph18387-cr-0007" contrib-type="author">
        <name>
          <surname>Dresbøll</surname>
          <given-names>Dorte Bodin</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6374-7257</contrib-id>
        <xref rid="nph18387-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib id="nph18387-cr-0008" contrib-type="author">
        <name>
          <surname>Thorup‐Kristensen</surname>
          <given-names>Kristian</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-5476-985X</contrib-id>
        <xref rid="nph18387-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="nph18387-aff-0001">
      <label>
        <sup>1</sup>
      </label>
      <named-content content-type="organisation-division">Department of Plant and Environmental Science</named-content>
      <institution>University of Copenhagen</institution>
      <named-content content-type="street">Højbakkegårds Alle 13</named-content>
      <city>Tåstrup</city>
      <postal-code>2630</postal-code>
      <country country="DK">Denmark</country>
    </aff>
    <aff id="nph18387-aff-0002">
      <label>
        <sup>2</sup>
      </label>
      <named-content content-type="organisation-division">Department of Computer Science</named-content>
      <institution>University of Copenhagen</institution>
      <named-content content-type="street">Universitetsparken 1</named-content>
      <postal-code>2100</postal-code>
      <city>Copenhagen</city>
      <country country="DK">Denmark</country>
    </aff>
    <aff id="nph18387-aff-0003">
      <label>
        <sup>3</sup>
      </label>
      <institution>CSIRO Agriculture and Food</institution>
      <named-content content-type="street">PO Box 1700</named-content>
      <city>Canberra</city>
      <named-content content-type="country-part">ACT</named-content>
      <postal-code>2601</postal-code>
      <country country="AU">Australia</country>
    </aff>
    <aff id="nph18387-aff-0004">
      <label>
        <sup>4</sup>
      </label>
      <named-content content-type="organisation-division">Department of Agroecology and Organic Farming</named-content>
      <institution>University of Bonn</institution>
      <named-content content-type="street">Regina‐Pacis‐Weg 3</named-content>
      <postal-code>53113</postal-code>
      <city>Bonn</city>
      <country country="DE">Germany</country>
    </aff>
    <aff id="nph18387-aff-0005">
      <label>
        <sup>5</sup>
      </label>
      <named-content content-type="organisation-division">Department of Organic Farming and Plant Production</named-content>
      <institution>University of Kassel</institution>
      <named-content content-type="street">Nordbahnhofstr. 1a</named-content>
      <postal-code>D‐37213</postal-code>
      <city>Witzenhausen</city>
      <country country="DE">Germany</country>
    </aff>
    <author-notes>
      <corresp id="correspondenceTo"><label>*</label>
Author for correspondence:<break/>
<italic toggle="yes">Abraham George Smith</italic>
<break/>
<italic toggle="yes">Email:</italic> 
<email>ags@di.ku.dk</email>
<break/>
</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>10</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <volume>236</volume>
    <issue seq="360">2</issue>
    <issue-id pub-id-type="doi">10.1111/nph.v236.2</issue-id>
    <fpage>774</fpage>
    <lpage>791</lpage>
    <history>
      <date date-type="received">
        <day>08</day>
        <month>3</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>30</day>
        <month>6</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <!--Copyright &#x000a9; 2022 New Phytologist Trust-->
      <copyright-statement content-type="article-copyright">© 2022 The Authors. New Phytologist © 2022 New Phytologist Foundation.</copyright-statement>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link> License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="file:NPH-236-774.pdf"/>
    <abstract>
      <title>Summary</title>
      <p>
        <list list-type="bullet" id="nph18387-list-0001">
          <list-item id="nph18387-li-0001">
            <p>Convolutional neural networks (CNNs) are a powerful tool for plant image analysis, but challenges remain in making them more accessible to researchers without a machine‐learning background. We present R<sc>oot</sc>P<sc>ainter</sc>, an open‐source graphical user interface based software tool for the rapid training of deep neural networks for use in biological image analysis.</p>
          </list-item>
          <list-item id="nph18387-li-0002">
            <p>We evaluate R<sc>oot</sc>P<sc>ainter</sc> by training models for root length extraction from chicory (<italic toggle="no">Cichorium intybus</italic> L.) roots in soil, biopore counting, and root nodule counting. We also compare dense annotations with corrective ones that are added during the training process based on the weaknesses of the current model.</p>
          </list-item>
          <list-item id="nph18387-li-0003">
            <p>Five out of six times the models trained using R<sc>oot</sc>P<sc>ainter</sc> with corrective annotations created within 2 h produced measurements strongly correlating with manual measurements. Model accuracy had a significant correlation with annotation duration, indicating further improvements could be obtained with extended annotation.</p>
          </list-item>
          <list-item id="nph18387-li-0004">
            <p>Our results show that a deep‐learning model can be trained to a high accuracy for the three respective datasets of varying target objects, background, and image quality with &lt; 2 h of annotation time. They indicate that, when using R<sc>oot</sc>P<sc>ainter</sc>, for many datasets it is possible to annotate, train, and complete data processing within 1 d.</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <kwd-group kwd-group-type="author-generated">
      <kwd id="nph18387-kwd-0001">biopore</kwd>
      <kwd id="nph18387-kwd-0002">deep learning</kwd>
      <kwd id="nph18387-kwd-0003">GUI</kwd>
      <kwd id="nph18387-kwd-0004">interactive machine learning</kwd>
      <kwd id="nph18387-kwd-0005">phenotyping</kwd>
      <kwd id="nph18387-kwd-0006">rhizotron</kwd>
      <kwd id="nph18387-kwd-0007">root nodule</kwd>
      <kwd id="nph18387-kwd-0008">segmentation</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="funding-0001">
        <funding-source>
          <institution-wrap>
            <institution>Villum Foundation
</institution>
            <institution-id institution-id-type="doi">10.13039/100008398</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>VKR023338</award-id>
      </award-group>
      <award-group id="funding-0002">
        <funding-source>
          <institution-wrap>
            <institution>German Research Foundation
</institution>
            <institution-id institution-id-type="doi">10.13039/501100001659</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0003">
        <funding-source>European Union's Horizon 2020 Research and Innovation Programme</funding-source>
        <award-id>884364</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="10"/>
      <table-count count="3"/>
      <page-count count="791"/>
      <word-count count="15105"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>source-schema-version-number</meta-name>
        <meta-value>2.0</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>cover-date</meta-name>
        <meta-value>October 2022</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>details-of-publishers-convertor</meta-name>
        <meta-value>Converter:WILEY_ML3GV2_TO_JATSPMC version:6.2.3 mode:remove_FC converted:31.12.2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body id="nph18387-body-0001">
  <sec id="nph18387-sec-0001">
    <title>Introduction</title>
    <p>Plant research is important because we need to find ways to feed a growing population whilst limiting damage to the environment (Lynch, <xref rid="nph18387-bib-0032" ref-type="bibr">2007</xref>). Plant studies often involve the measurement of traits from images, which may be used in phenotyping for genome‐wide association studies (Rebolledo <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0045" ref-type="bibr">2016</xref>), comparing cultivars for traditional breeding (Walter <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0078" ref-type="bibr">2019</xref>), or testing a hypothesis related to plant physiology (Rasmussen <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0042" ref-type="bibr">2020</xref>). Plant image analysis has been identified as a bottleneck in plant research (Minervini <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0034" ref-type="bibr">2015</xref>). A variety of software exists to quantify plant images (Lobet <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0031" ref-type="bibr">2013</xref>) but is typically limited to a specific type of data or task, such as leaf counting (Ubbens <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0074" ref-type="bibr">2018</xref>), pollen counting (Tello <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0066" ref-type="bibr">2018</xref>), or root architecture extraction (Yasrab <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0081" ref-type="bibr">2019</xref>).</p>
    <p>Convolutional neural networks (CNNs) are a class of deep‐learning models that represent the state of the art for image analysis and are currently among the most popular methods in computer vision research. They are a type of multilayered neural network that uses convolution in at least one layer and are designed to process grid‐like data, such as images (LeCun <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0028" ref-type="bibr">2015</xref>). CNNs, such as U‐Net (Ronneberger <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0047" ref-type="bibr">2015</xref>), receive an image as input and then output another image, with each pixel in the output image representing a prediction for each pixel in the input image. CNNs excel at tasks such as segmentation and classification. They have been found to be effective for various tasks in agricultural research (Kamilaris &amp; Prenafeta‐Boldú, <xref rid="nph18387-bib-0022" ref-type="bibr">2018</xref>; Santos <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0049" ref-type="bibr">2020</xref>) and plant image analysis, including plant stress phenotyping (Jiang &amp; Li, <xref rid="nph18387-bib-0021" ref-type="bibr">2020</xref>), wheat spike counting (Pound <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0041" ref-type="bibr">2017</xref>), leaf counting (Ubbens &amp; Stavness, <xref rid="nph18387-bib-0075" ref-type="bibr">2017</xref>), and accession classification (Taghavi Namin <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0065" ref-type="bibr">2018</xref>).</p>
    <p>For a CNN model to perform a particular task, it must be trained on a suitable dataset of examples. These examples are referred to as training data and are typically a collection of input images paired with the desired output for that image. In the case of segmentation, each input image is paired with a set of labels corresponding to each of the pixels in the input image. The process of creating such labelled training data is referred to as annotation, and this can be time consuming, as annotation of complex images can be labour intensive and many images may be desired, as larger training datasets typically result in improvements in trained model performance (Nakkiran <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0036" ref-type="bibr">2021</xref>).</p>
    <p>Convolutional neural network model training involves a process called stochastic gradient descent (SGD) optimizing the parameters of a model such that the error is reduced. The error, commonly referred to as the loss, is a measure of the difference between the model's predictions and the correct labels for the training data examples. A separate validation dataset, consisting of similar examples, is used to provide information on the performance of the model during the training procedure on examples that have not been used as part of SGD optimization. Validation set performance may be used to decide when to stop training or assist in tuning hyperparameters, which are variables controlling fundamentals of the model that are not directly optimized by SGD.</p>
    <p>Developing a CNN‐based system for a new image analysis task or dataset is challenging, because dataset design, model training, and hyperparameter tuning are time‐consuming tasks requiring competencies in both programming and machine learning.</p>
    <p>Three questions that need answering when attempting a supervised learning project such as training a CNN are: how to split the data between training, validation, and test datasets; how to manually annotate or label the data; and how to decide how much data needs to be collected, labelled, and used for training in order to obtain a model with acceptable performance. The choices of optimal hyperparameters and network architecture are also considered to be a ‘black art’ requiring years of experience, and a need has been recognized to make the application of deep learning easier in practice (Smith, <xref rid="nph18387-bib-0060" ref-type="bibr">2018</xref>).</p>
    <p>The question of how much data to use in training and validation is explored in theoretical work that gives indications of a model's generalization performance based on dataset size and number of parameters (Vapnik, <xref rid="nph18387-bib-0076" ref-type="bibr">2000</xref>). These theoretical insights may be useful for simpler models but provide an inadequate account of the behaviour of CNNs in practice (Zhang <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0082" ref-type="bibr">2017</xref>).</p>
    <p>Manual annotation may be challenging, as proprietary tools may be used that are not freely available (Xu <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0080" ref-type="bibr">2019</xref>) and can increase the skill set required. Creating dense per‐pixel annotations for training is often a time‐consuming process. It has been argued that tens of thousands of images are required, making small‐scale plant‐image datasets unsuitable for training deep‐learning models (Ubbens <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0074" ref-type="bibr">2018</xref>).</p>
    <p>The task of collecting datasets for the effective training of models is further confounded by the unique attributes of each dataset. All data are not created equal, with great variability in the utility of each annotated pixel for the model training process (Kellenberger <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0024" ref-type="bibr">2019</xref>). It may be necessary to add harder examples after observing weaknesses in an initial trained model (Soltaninejad <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0062" ref-type="bibr">2019</xref>), or to correct for a class imbalance in the data where many examples exist of a majority class (Buda <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0006" ref-type="bibr">2018</xref>).</p>
    <p>Interactive segmentation methods using CNNs (e.g. Hu <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0020" ref-type="bibr">2018</xref>; Sakinis <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0048" ref-type="bibr">2019</xref>) provide ways to improve the annotation procedure by allowing user input to be used in the inference process and can be an effective way to create large high‐quality datasets in less time (Benenson <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0003" ref-type="bibr">2019</xref>).</p>
    <p>When used in a semi‐automatic setting, such tools will speed up the labelling process but may still be unsuitable for situations where the speed and consistency of a fully automated solution are required. For example, when processing data from large‐scale root phenotyping facilities (e.g. Svane <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0064" ref-type="bibr">2019</xref>) where in the order of 100 000 images or more need to be analysed.</p>
    <p>In this study we present and evaluate our software R<sc>oot</sc>P<sc>ainter</sc>, which makes the process of creating a dataset, training a neural network, and using it for plant image analysis accessible to ordinary computer users by facilitating all required operations with a cross‐platform, open‐source, and freely available user interface. The R<sc>oot</sc>P<sc>ainter</sc> software was initially developed for quantification of roots in images from rhizotron‐based root studies. However, we found its versatility to be much broader, with an ability to be trained to recognize many different types of structures in a set of images.</p>
    <p>Although more root specific (Smith <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0058" ref-type="bibr">2020d</xref>; Gaggion <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0008" ref-type="bibr">2021</xref>; Narisetti <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0037" ref-type="bibr">2021</xref>) and more generalist segmentation tools such as F<sc>iji</sc> (Schindelin <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0050" ref-type="bibr">2012</xref>) via D<sc>eep</sc>I<sc>mage</sc>J (Gómez‐de Mariscal <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0009" ref-type="bibr">2021</xref>) make it possible to run trained deep‐learning models for segmentation, they do not provide easy‐to‐use model training functionality, which is the purpose of the R<sc>oot</sc>P<sc>ainter</sc> software presented.</p>
    <p>R<sc>oot</sc>P<sc>ainter</sc> allows a user to inspect model performance during the annotation process so they can make a more informed decision about how much and what data are necessary to label in order to train a model to an acceptable accuracy. It allows annotations to be targeted towards areas where the current model shows weakness (Fig. <xref rid="nph18387-fig-0001" ref-type="fig">1</xref>) in order to streamline the process of creating a dataset necessary to achieve a desired level of performance. R<sc>oot</sc>P<sc>ainter</sc> can operate in a semi‐automatic way, with a user assigning corrections to each segmented image, whilst the model learns from the assigned corrections, reducing the time‐requirements for each image as the process is continued. It can also operate in a fully automatic way by either using the model generated from the interactive procedure to process a larger dataset without required interaction, or in a more classical way by using a model trained from dense per‐pixel annotations which can also be created via the user interface.</p>
    <fig position="float" fig-type="Fig." id="nph18387-fig-0001">
      <label>Fig. 1</label>
      <caption>
        <p>R<sc>oot</sc>P<sc>ainter</sc> corrective annotation concept. (a) Roots in soil. (b) Artificial intelligence (AI) root predictions (segmentation) shown in bright blue overlaid on photograph. (c) Human corrections of the initial segmentation, with corrections of false negatives shown in red and corrections of false positives shown in green. (d) After a period of training the AI learns from the corrections provided. The updated segmentation is shown in bright blue.</p>
      </caption>
      <graphic xlink:href="NPH-236-774-g009" position="anchor" id="jats-graphic-1"/>
    </fig>
    <p>We evaluate the effectiveness of R<sc>oot</sc>P<sc>ainter</sc> by training models for three different types of data and tasks without dataset‐specific programming or hyperparameter tuning. We evaluate the effectiveness on a set of rhizotron root images and, in order to evaluate the versatility of the system, on two other types of data, both involving objects in the images quite different from roots: a biopores dataset and a legume root nodules dataset.</p>
    <p>For each dataset we compare the performance of models trained using the dense and corrective annotation strategies on images not used during the training procedure. If annotation is too time consuming, then R<sc>oot</sc>P<sc>ainter</sc> will be unfeasible for many projects. To investigate the possibility of rapid and convenient model training we use no prior knowledge and restrict annotation time to a maximum of 2 h for each model. We make two hypotheses. First, in a limited time period, R<sc>oot</sc>P<sc>ainter</sc> will be able to segment the objects of interest to an acceptable accuracy in three datasets including roots, biopores, and root nodules, demonstrated by a strong correlation between the measurements obtained from R<sc>oot</sc>P<sc>ainter</sc> and manual methods. Second, a corrective annotation strategy will result in a more accurate model compared with dense annotations, given the same time for annotation.</p>
    <p>Training with corrective annotation is a type of interactive machine learning, as it uses a human in the loop in the model training procedure. As opposed to active learning, which involves the learner automatically selecting which examples the user labels (Settles, <xref rid="nph18387-bib-0053" ref-type="bibr">2009</xref>), interactive machine learning involves a human deciding which examples should be added for future iterations of training (Amershi <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0001" ref-type="bibr">2014</xref>).</p>
    <p>Prior work for interactive training for segmentation includes Gonda <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0010" ref-type="bibr">2017</xref>) and Kontogianni <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0026" ref-type="bibr">2019</xref>). Gonda <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0010" ref-type="bibr">2017</xref>) evaluated their method using neuronal structures captured using electron microscopy and found the interactively trained model produced better segmentations than a model trained using exhaustive ground‐truth labels.</p>
    <p>Kontogianni <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0026" ref-type="bibr">2019</xref>) combined interactive segmentation with interactive training by using the user feedback in model updates. Their training approach requires an initial dataset with full ground‐truth segmentations, whereas our method requires no prior labelled data, which was a design choice we made to increase the applicability of our method to plant researchers looking to quantify new objects in a captured image dataset.</p>
    <p>As opposed to Gonda <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0010" ref-type="bibr">2017</xref>) we use a more modern, fully convolutional network model, which we expect to provide substantial efficiency benefits when dealing with larger images. Our work is novel, in that we evaluate an interactive corrective annotation procedure in terms of annotation time to reach a certain accuracy on real‐world plant‐image datasets. Synthetic data are often used to evaluate interactive segmentation methods (Benard &amp; Gygli, <xref rid="nph18387-bib-0002" ref-type="bibr">2017</xref>; Li <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0029" ref-type="bibr">2018</xref>; Mahadevan <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0033" ref-type="bibr">2018</xref>). To provide more realistic measurements of annotation time we use real human annotators for our experiments. As opposed to many competing deep‐learning methods for segmentation, we provide a graphical user interface that allows all operations to be completed using a user interface, an essential feature for ensuring uptake in the plant image analysis community.</p>
    <sec id="nph18387-sec-0002">
      <title>Roots in soil</title>
      <p>Plant roots are responsible for uptake of water and nutrients. This makes understanding root system development critical for the development of resource‐efficient crop production systems. For this purpose, we need to study roots under real‐life conditions in the field, studying the effects of crop genotypes and their management (Rasmussen <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0043" ref-type="bibr">2015</xref>; Rasmussen &amp; Thorup‐Kristensen, <xref rid="nph18387-bib-0044" ref-type="bibr">2016</xref>), cover crops (Thorup‐Kristensen, <xref rid="nph18387-bib-0067" ref-type="bibr">2001</xref>), crop rotation (Thorup‐Kristensen <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0069" ref-type="bibr">2012</xref>), and other factors. We need to study deep rooting, as this is critical for the use of agriculturally important resources, such as water and nitrogen (N) (Thorup‐Kristensen &amp; Kirkegaard, <xref rid="nph18387-bib-0072" ref-type="bibr">2016</xref>; Thorup‐Kristensen <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0070" ref-type="bibr">2020a</xref>).</p>
      <p>Rhizotron‐based root research is an important example of plant research. Acquisition of root images from rhizotrons is widely adopted (Rewald <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0046" ref-type="bibr">2012</xref>), as it allows repeated and nondestructive quantification of root growth and often to the full depth of the root systems. Traditionally, the method for root quantification in such studies involves a lengthy procedure to determine the root density on acquired images by counting intersections with grid lines (Thorup‐Kristensen, <xref rid="nph18387-bib-0068" ref-type="bibr">2006</xref>).</p>
      <p>Manual methods require substantial resources and can introduce undesired inter‐annotator variation on root density; therefore, a faster and more consistent method is required. More recently, fully automatic approaches using CNNs have been proposed (Smith <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0058" ref-type="bibr">2020d</xref>); although effective, such methods may be challenging to repurpose to different datasets for root scientists without the required programming expertise. A method that made the retraining process more accessible and convenient would accelerate the adoption of CNNs within the root research community.</p>
    </sec>
    <sec id="nph18387-sec-0003">
      <title>Biopores</title>
      <p>Biopores are tubular or round‐shaped continuous voids formed by root penetration and earthworm movement (Kautz, <xref rid="nph18387-bib-0023" ref-type="bibr">2015</xref>). They function as preferential pathways for root growth (Han <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0014" ref-type="bibr">2015b</xref>) and are therefore important for plant resource acquisition (Kopke <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0027" ref-type="bibr">2015</xref>; Han <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0012" ref-type="bibr">2017</xref>). Investigation of soil biopores is often done by manually drawing on transparent sheets on an excavated soil surface (Han <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0013" ref-type="bibr">2015a</xref>). This manual approach is time consuming and precludes a more in‐depth analysis of detailed information, including diameter, surface area, or distribution patterns such as clustering.</p>
    </sec>
    <sec id="nph18387-sec-0004">
      <title>Root nodules</title>
      <p>Growing legumes with N‐fixing capacity reduces the use of fertilizer (Kessel <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0025" ref-type="bibr">2000</xref>); hence, there is an increased demand for legume‐involved intercropping (Hauggaard‐Nielsen <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0015" ref-type="bibr">2001</xref>) and precropping for carryover effects. Roots of legumes form associations with rhizobia, forming nodules on the roots, where the N fixation occurs. Understanding the nodulation process is important to understand this symbiosis and the N fixation. However, counting nodules from the excavated roots is a cumbersome and time‐consuming procedure, especially for species with many small nodules, such as clovers (<italic toggle="yes">Trifolium</italic> spp.).</p>
    </sec>
  </sec>
  <sec sec-type="materials-and-methods" id="nph18387-sec-0005">
    <title>Materials and Methods</title>
    <sec id="nph18387-sec-0006">
      <title>Software implementation</title>
      <p>R<sc>oot</sc>P<sc>ainter</sc> uses a client–server architecture, allowing users with a typical laptop or desktop computer to utilize a graphics processing unit (GPU) on a more computationally powerful server. The client and server can be used on the same machine if it is equipped with suitable hardware, reducing network input/output overhead. Instructions are sent from the client to server using human‐readable JSON (JavaScript Object Notation) format. The client–server communication is facilitated entirely with files via a network drive or file synchronization application. This allows utilization of existing authentication, authorization and backup mechanisms whilst removing the need to setup a publicly accessible static Internet Protocol address. The graphical client is implemented using P<sc>y</sc>Q<sc>t</sc>5 which binds to the Q<sc>t</sc> cross‐platform widget toolkit. The client installers for M<sc>ac</sc>OS, W<sc>indows</sc>, and L<sc>inux</sc> are built using the P<sc>y</sc>I<sc>nstaller</sc> build system, which bundles all required dependencies. As opposed to the more generalist annotation software <sc>napari</sc> (Sofroniew <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0061" ref-type="bibr">2022</xref>), which is also built using Q<sc>t</sc>, the R<sc>oot</sc>P<sc>ainter</sc> client is designed to specifically facilitate our proposed corrective annotation protocol and to not require P<sc>ython</sc> familiarity. Image data can be provided as JPEG, PNG, or TIF and in either colour or greyscale. Image annotations and segmentations are stored as PNG files. Models produced during the training process are stored in the P<sc>ython</sc> pickle format and extracted measurements in comma‐separated value (CSV) text files.</p>
      <p>A folder referred to as the ‘sync directory’ is used to store all datasets, projects, and instructions that are shared between the server and client. The server setup (Supporting Information Notes <xref rid="nph18387-supitem-0001" ref-type="supplementary-material">S1</xref>) requires familiarity with the L<sc>inux</sc> command line, so should be completed by a system administrator. The server setup involves specification of a sync directory, which must then be shared with users. Users will be prompted to input the sync directory relative to their own file system when they open the client application for the first time and it will be automatically stored in their home folder in a file named <italic toggle="yes">root_painter_settings.json</italic>, which the user may delete or modify if required.</p>
      <sec id="nph18387-sec-0007">
        <title>Creating a dataset</title>
        <p>The ‘Create training dataset’ functionality is available as an option when opening the R<sc>oot</sc>P<sc>ainter</sc> client application. It is possible to specify a source image directory, which may be anywhere on the local file system and whether all images from the source directory should be used or a random sample of a specified number of images. It is also possible to specify the target width and height of one or more samples to take from each randomly selected image; this can provide two advantages in terms of training performance. First, R<sc>oot</sc>P<sc>ainter</sc> loads images from disk many times during training, which for larger images (&gt; 2000 × 2000 px<sup>2</sup>) can slow down training in proportion to image size and hardware capabilities. Second, recent results (Lin <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0030" ref-type="bibr">2020</xref>) indicate that capturing pixels from many images is more useful than capturing more pixels from each image when training models for semantic segmentation; thus, when working with datasets containing many large images, using only a part of each image will likely improve performance given a restricted time for annotation.</p>
        <p>When generating a dataset, each image to be processed is evaluated for whether it should be split into smaller pieces. If an image's dimensions are close to the target width and height, then the image will be added to the dataset without it being split. If an image is substantially bigger, then all possible ways to split the image into equally sized pieces above the minimum are evaluated. For each of the possible splits, the resultant piece dimensions are evaluated in terms of their ratio distance from a square and distance from the target width and height. The split that results in the smallest sum of these two distances is then applied. From the split image, up to the <italic toggle="yes">maximum tiles per image</italic> are selected at random and saved to the training dataset. The source images do not need to be the same size, and the images in the generated dataset will not necessarily be the same size, but all images provided must have a width and height of at least 572 px; we recommend at least 600 px, as this will allow random crop data augmentation. The dataset is created in the R<sc>oot</sc>P<sc>ainter</sc> sync directory in the datasets folder in a subdirectory that takes the user‐specified dataset name. To segment images in the original dimensions, the dataset creation routine can be bypassed by simply copying or moving a directory of images into a subdirectory in the R<sc>oot</sc>P<sc>ainter</sc> datasets directory.</p>
      </sec>
      <sec id="nph18387-sec-0008">
        <title>Working with projects</title>
        <p>Projects connect datasets with models, annotations, segmentations, and messages returned from the server. They are defined by a project file (.seg_proj), which specifies the details in JSON, and a project folder containing relevant data. The options to create a project or open an existing project are presented when opening the R<sc>oot</sc>P<sc>ainter</sc> client application. Creating projects requires specifying a dataset and, optionally, an initial model file. Alternatively, a user may select ‘random weights’, also known as training from scratch, which will use He initialization (He <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0017" ref-type="bibr">2015b</xref>) to assign a model's initial weights. A project can be used to inspect the performance of a model on a given dataset in the client or to train a model with new annotations, which can also be created using drawing tools in the client user interface.</p>
      </sec>
      <sec id="nph18387-sec-0009">
        <title>Model architecture</title>
        <p>We modified the network architecture from Smith <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0058" ref-type="bibr">2020d</xref>), which is a variant of U‐Net (Ronneberger <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0047" ref-type="bibr">2015</xref>) implemented in P<sc>y</sc>T<sc>orch</sc> (Paszke <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0039" ref-type="bibr">2017</xref>) using group normalization (Wu &amp; He, <xref rid="nph18387-bib-0079" ref-type="bibr">2018</xref>) layers. U‐Net is composed of a series of down‐blocks and up‐blocks joined by skip connections. The entire network learns a function that converts the input data into a desired output representation; for example, from an image of soil to a segmentation or binary map indicating which of the pixels in the image are part of a biopore. In the down‐blocks we added <mml:math id="jats-math-1" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math> convolution to halve the size of the feature maps. We modified both down‐blocks and up‐blocks to learn residual mappings, which have been found to ease optimization and improve accuracy in CNNs (He <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0016" ref-type="bibr">2015a</xref>) including U‐Net (Zhang <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0083" ref-type="bibr">2018</xref>). To speed up inference by increasing the size of the output segmentation, we added 1 px padding to the convolutions in the down‐blocks and modified the input dimensions from <mml:math id="jats-math-2" display="inline" overflow="scroll"><mml:mrow><mml:mn>512</mml:mn><mml:mo>×</mml:mo><mml:mn>512</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math> to <mml:math id="jats-math-3" display="inline" overflow="scroll"><mml:mrow><mml:mn>572</mml:mn><mml:mo>×</mml:mo><mml:mn>572</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math>, which resulted in a new respective output size of <mml:math id="jats-math-4" display="inline" overflow="scroll"><mml:mrow><mml:mn>500</mml:mn><mml:mo>×</mml:mo><mml:mn>500</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math>, containing a channel for the foreground and background predictions. The modified architecture has <italic toggle="yes">c</italic>. 1.3 million trainable parameters, whereas the original had 31 million. These alterations reduced the saved model size from 124.2 MB (Smith <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0059" ref-type="bibr">2019b</xref>) to 5.3 MB, making it small enough to be conveniently shared via email.</p>
      </sec>
      <sec id="nph18387-sec-0010">
        <title>Creating annotations</title>
        <p>Annotations can be added by drawing in the user interface with either the foreground or background brush tools. It is also possible to undo or redo brush strokes. Annotation can be removed with the eraser tool. If an image is only partially annotated, then only the regions with annotation assigned will be used in the training. Whilst annotating, it is possible to hide and show the annotation, image or segmentation. For convenience during use, a table of keyboard shortcuts is presented in Notes <xref rid="nph18387-supitem-0001" ref-type="supplementary-material">S2</xref>. When the user clicks ‘Save &amp; next’ in the interface, the current annotation will be saved and synced with the server, ready for use in training. The first and second annotations are added to the training and validation sets respectively (see the <xref rid="nph18387-sec-0011" ref-type="sec">Training procedure</xref> section). Afterwards, to maintain a typical ratio between training and validation sets, annotations will be added to the validation set when the training set is at least five times the size of the validation set, otherwise they will be added to the training set.</p>
      </sec>
      <sec id="nph18387-sec-0011">
        <title>Training procedure</title>
        <p>The training procedure can be started by selecting ‘Start training’ from the network menu, which will send a JSON instruction to the server to start training for the current project. The training will only start if the project has at least two saved annotations, as at least one is required for each of the training and validation sets. Based on Smith <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0058" ref-type="bibr">2020d</xref>) we use a learning rate of 0.01 and Nestorov momentum with a value of 0.99. We removed weight decay, as results have shown similar performance can be achieved with augmentation alone whilst reducing the coupling between hyperparameters and dataset (Hernández‐García &amp; König, <xref rid="nph18387-bib-0019" ref-type="bibr">2019</xref>). The removal of weight decay has also been suggested in practical advice (Bengio, <xref rid="nph18387-bib-0004" ref-type="bibr">2012</xref>) based on earlier results (Collobert &amp; Bengio, <xref rid="nph18387-bib-0007" ref-type="bibr">2004</xref>) indicating its superfluity when early stopping is used. We do not use a learning rate schedule in order to facilitate an indefinitely expanding dataset.</p>
        <p>An <italic toggle="yes">epoch</italic> typically refers to a training iteration over the entire dataset (Goodfellow <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0011" ref-type="bibr">2016</xref>). In this context, we initially define an epoch to be a training iteration over 612 image subregions corresponding to the network input size. The images to be used in the network training procedure are sampled randomly with replacement from all training set images. We found an iteration over this initial epoch size to take <italic toggle="yes">c</italic>. 30 s using two RTX 2080 Ti GPUs with an automatically selected batch size of 6. If the training dataset expands beyond 306 images, then the number of sampled subregions per epoch is set to twice the number of training images, to avoid validation overwhelming training time. The batch size is automatically selected based on total GPU memory, and all GPUs will be used by default using data parallelism.</p>
        <p>R<sc>oot</sc>P<sc>ainter</sc> utilizes a supervised learning procedure, which involves training a model to predict the same value (either foreground or background) for a pixel in the input data as the one found in the corresponding location of an annotation. The annotations are created by the user and include pixels annotated as foreground, pixels annotated as background, and pixels without annotation. Only the pixels annotated as foreground or background are used in training. This is achieved by setting the pixels without foreground or background annotation to 0 in both the network prediction and associated annotation. The distance between the annotated pixels and network predictions is computed using a loss function, which is a combination of dice‐loss and cross‐entropy taken from Smith <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0058" ref-type="bibr">2020d</xref>). This computed loss is then used to update the network weights, leading to a model with reduced error on subsequent images.</p>
        <p>After each epoch, the model predictions are computed on the validation set and <italic toggle="yes">F</italic>
<sub>1</sub> is calculated for the current and previously saved model. If the current model's <italic toggle="yes">F</italic>
<sub>1</sub> is higher than the previously saved model, then it is saved with its number and current time in the file name. If, for 60 epochs, no model improvements are observed and no annotations are saved or updated, then training will stop automatically.</p>
        <p>We designed the training procedure to have minimal RAM requirements that do not increase with dataset size, in order to facilitate training on larger datasets. We found the server application to use &lt; 8 GB of RAM during training and inference and would suggest at least 16 GB RAM for the machine running the server application. We found the client to use &lt; 1 GB RAM but have not yet tested on devices equipped with &lt; 8 GB of RAM.</p>
      </sec>
      <sec id="nph18387-sec-0012">
        <title>Augmentation</title>
        <p>We modified the augmentation procedure from Smith <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0058" ref-type="bibr">2020d</xref>) in three ways. We changed the order of the transforms from fixed to random in order to increase variation. We reduced the probability that each transform is applied to 80% in order to reduce the gap between clean and augmented data, which recent results indicate can decrease generalization performance (He <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0018" ref-type="bibr">2019</xref>). We also modified the elastic grid augmentation, as we found the creation of the deformation maps to be a performance bottleneck. To eliminate this bottleneck we created the deformation maps at an eighth of the image size and then interpolated them up to the correct size.</p>
      </sec>
      <sec id="nph18387-sec-0013">
        <title>Creating segmentations</title>
        <p>It is possible to view segmentations for each individual image in a dataset by creating an associated project and specifying a suitable model. The segmentations are generated automatically via an instruction sent to the server when viewing each image and saved in the segmentations folder in the corresponding project.</p>
        <p>When the server generates a segmentation, it first segments the original image and then a horizontally flipped version. The output segmentation is computed by taking the average of both and then thresholding at 0.5. This technique is a type of test time data augmentation, which is known to improve performance (Perez <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0040" ref-type="bibr">2018</xref>). The segmentation procedure involves first splitting the images into tiles with a width and height of 572 px, which are each passed through the network, and then an output corresponding to the original image is reconstructed.</p>
        <p>It is possible to segment a larger folder of images using the ‘Segment folder’ option available in the network menu. To do this, an input directory, output directory, and one or more models must be specified. The model with the highest number for any given project will have the highest accuracy in terms of <italic toggle="yes">F</italic>
<sub>1</sub> on the automatically selected validation set. Selecting more than one model will result in model averaging, an ensemble method that improves accuracy as different models do not usually make identical errors (Goodfellow <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0011" ref-type="bibr">2016</xref>). Selecting models from different projects representing different training runs on the same dataset will likely lead to a more diverse, and thus more accurate, ensemble, given they are of similar accuracy. It is also possible to use models saved at various points from a single training run, a method that can provide accuracy improvements without extending training time (Sennrich <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0052" ref-type="bibr">2016</xref>).</p>
        <p>Fig. <xref rid="nph18387-fig-0002" ref-type="fig">2</xref> shows the user interface with various stages of the interactive training procedure.</p>
        <fig position="float" fig-type="Fig." id="nph18387-fig-0002">
          <label>Fig. 2</label>
          <caption>
            <p>Screenshots of the R<sc>oot</sc>P<sc>ainter</sc> software, showing examples of the various stages of the interactive training procedure. (a) Initial annotation created at the start of interactive training, with foreground annotation shown in red and background annotation shown in green. (b) Corrective annotation whilst the model progresses to a suitable solution. The segmentation is shown in light blue. (c) Segmentation shown later in the model training process.</p>
          </caption>
          <graphic xlink:href="NPH-236-774-g001" position="anchor" id="jats-graphic-3"/>
        </fig>
      </sec>
      <sec id="nph18387-sec-0014">
        <title>Extracting measurements</title>
        <p>It is possible to extract measurements from the segmentations produced by selecting an option from the measurements menu. The ‘Extract length’ option extracts centrelines using the skeletonize method from <sc>scikit</sc>‐<sc>image</sc> (Walt <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0077" ref-type="bibr">2014</xref>) and then counts the centreline pixels for each image. The ‘Extract region properties’ uses the <sc>scikit</sc>‐<sc>image</sc> regionprops method to extract the coordinates, diameter, area, perimeter, and eccentricity for each detected region and stores this along with the associated filename. The ‘Extract count’ method gives the count of all regions per image. Each of the options requires the specification of an input segmentation folder and an output CSV.</p>
      </sec>
    </sec>
    <sec id="nph18387-sec-0015">
      <title>Datasets</title>
      <sec id="nph18387-sec-0016">
        <title>Biopore images</title>
        <p>Biopore images were collected near Meckenheim (50°37′9″N, 6°59′29″E) at a field trial of the University of Bonn in 2012; see Han <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0013" ref-type="bibr">2015a</xref>) for a detailed description. Within each plot, an area was excavated to a depth of 0.45 m. The exposed soil surface was carefully flattened to reveal biopores and then photographed in colour.</p>
        <p>B<sc>ersoft</sc> software (v.7.25; W<sc>indows</sc>) was used for biopore quantification. Using the eclipse function, the visible biopores were marked, and then the count number was generated as a CSV file. Pores &lt; 2 mm were excluded from biopore counting.</p>
        <p>We restricted the analysis to images with a suitable resolution and cropped to omit border areas. For each image, the number of pixels per millimetre was recorded using G<sc>imp</sc> (v.2.10, M<sc>ac</sc>OS) in order to calculate pore diameter. We split the images into two folders: BP_counted, which contained 39 images and was used for model validation after training, as these images had been counted by a biopore expert, and BP_uncounted, which contained 54 images and was used for training.</p>
      </sec>
      <sec id="nph18387-sec-0017">
        <title>Nodule images</title>
        <p>Root images of Persian clover (<italic toggle="yes">Trifolium resupinatum</italic>) were acquired at 800 DPI in colour using a water‐bed scanner after root extraction. We used a total of 113 images that all had a blue background but were taken with two different scanners. From the 113 images, 65 were captured using an Epson V700 scanner (Epson, Nagano, Japan) and appeared darker and underexposed, whereas 48 were captured using an Epson Expression 12000XL Photo Scanner and appeared well lit, showing the nodules more clearly. The blue background was obtained by blocking the scanner transparency unit with nontransparent blue paper to create a background that provided contrast to the root nodules. Blocking the scanner transparency unit meant that the only light source was from the document table (the lower part) of the scanner.</p>
        <p>They were counted manually using W<sc>in</sc>R<sc>hizo</sc> P<sc>ro</sc> (v.2016; Regent Instruments Inc., Sainte‐Foy, QC, Canada). Image sections were enlarged, and nodules were selected manually by clicking. Then, the total number of marked nodules were counted by the software. We manually cropped to remove the borders of the scanner using P<sc>review</sc> (v.10.0; M<sc>ac</sc>OS) and converted to JPEG to ease storage and sharing. Of these, 50 were selected at random to have subregions included in training, and the remaining 63 were used for validation.</p>
      </sec>
      <sec id="nph18387-sec-0018">
        <title>Roots dataset</title>
        <p>We downloaded the 867 grid‐counted images and manual root length measurements from Smith <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0057" ref-type="bibr">2019a</xref>), which were made available as part of the evaluation of U‐Net for segmenting roots in soil (Smith <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0058" ref-type="bibr">2020d</xref>) and originally captured as part of a study on chicory drought stress (Rasmussen <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0042" ref-type="bibr">2020</xref>) using a 4 m rhizobox laboratory described in Thorup‐Kristensen <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0071" ref-type="bibr">2020b</xref>). We removed the 10 test images from the grid‐counted images, leaving 857 images. The manual root length measurements are a root intensity measurement per image, which was obtained by counting root intersections with a grid as part of Rasmussen <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0042" ref-type="bibr">2020</xref>).</p>
      </sec>
    </sec>
    <sec id="nph18387-sec-0019">
      <title>Annotation and training</title>
      <p>For the roots, nodules, and biopores, we created training datasets using the ‘Create training dataset’ option. We used a random sample, with the details specified in Table <xref rid="nph18387-tbl-0001" ref-type="table">1</xref>. The two users (user a and user b) that we used to test the software were the two first authors. Each user trained two models for each dataset. For each model, the user had 2 h (with a 30 min break between them) to annotate 200 images. We first trained a model using the corrective annotation strategy whilst recording the finish time; we then repeated the process with the dense annotation strategy, using the recorded time from the corrective training as a time limit. This was done to ensure the same annotation time was used for both annotation strategies. With corrective annotations, the annotation and training processes are coupled, as there is a feedback loop between the user and model being trained that happens in real time, whereas with dense annotation the user annotated continuously, without regard to model performance. The protocol followed when using corrective annotations is outlined in Notes <xref rid="nph18387-supitem-0001" ref-type="supplementary-material">S3</xref>, and annotation advice given in Notes <xref rid="nph18387-supitem-0001" ref-type="supplementary-material">S4</xref>. For the first six annotations on each dataset, we added clear examples rather than corrections. This was because we observed divergence in the training process when using corrective annotation from the start in preliminary experiments. We suspect the divergence was caused by the user adding too many background classes compared with foreground or difficult examples. When creating dense annotations, we followed the procedure described in Notes <xref rid="nph18387-supitem-0001" ref-type="supplementary-material">S5</xref>.</p>
      <table-wrap position="float" id="nph18387-tbl-0001" content-type="Table">
        <label>Table 1</label>
        <caption>
          <p>Details for each of the datasets created for training.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="char" char="." span="1"/>
          <col align="char" char="." span="1"/>
          <col align="char" char="." span="1"/>
          <thead valign="bottom">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1">Object</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Name</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Source folder</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Reference</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">To sample</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Max. tiles</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Target size</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Biopores</td>
              <td align="left" valign="top" rowspan="1" colspan="1">BP_750_training</td>
              <td align="left" valign="top" rowspan="1" colspan="1">BP_uncounted</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Smith <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0055" ref-type="bibr">2020b</xref>)</td>
              <td align="char" char="." valign="top" rowspan="1" colspan="1">50</td>
              <td align="char" valign="top" rowspan="1" colspan="1">4</td>
              <td align="char" valign="top" rowspan="1" colspan="1">750</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Nodules</td>
              <td align="left" valign="top" rowspan="1" colspan="1">nodules_750_training</td>
              <td align="left" valign="top" rowspan="1" colspan="1">counted_nodules</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Smith <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0054" ref-type="bibr">2020a</xref>)</td>
              <td align="char" char="." valign="top" rowspan="1" colspan="1">50</td>
              <td align="char" valign="top" rowspan="1" colspan="1">4</td>
              <td align="char" valign="top" rowspan="1" colspan="1">750</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Roots</td>
              <td align="left" valign="top" rowspan="1" colspan="1">towers_750_training</td>
              <td align="left" valign="top" rowspan="1" colspan="1">grid_counted_roots</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Smith <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0057" ref-type="bibr">2019a</xref>)</td>
              <td align="char" char="." valign="top" rowspan="1" colspan="1">200</td>
              <td align="char" valign="top" rowspan="1" colspan="1">1</td>
              <td align="char" valign="top" rowspan="1" colspan="1">750</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot id="nph18387-ntgp-0001">
          <fn id="nph18387-note-0001">
            <p>The numbers of images and tiles were chosen to enable a consistent dataset size of 200 images. Only 50 images were sampled for the biopores and nodules, in order to ensure there were enough images left in the test set. The datasets created are available to download from Smith <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0054" ref-type="bibr">2020c</xref>).</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>When annotating roots, in the interests of efficiency, a small amount of soil covering the root would still be considered as root if it was very clear that root was still beneath. Larger gaps were not labelled as root. Occluded parts of nodules were still labelled as foreground (Fig. <xref rid="nph18387-fig-0003" ref-type="fig">3a</xref>). Only the centre part of a nodule was annotated, leaving the edge as undefined. This was to avoid nodules that were close together being joined into a single nodule. When annotating nodules that were touching, a green line (background labels) was drawn along the boundary to teach the network to separate them so that the segmentation would give the correct counts (Fig. <xref rid="nph18387-fig-0003" ref-type="fig">3b</xref>).</p>
      <fig position="float" fig-type="Fig." id="nph18387-fig-0003">
        <label>Fig. 3</label>
        <caption>
          <p>Nodule annotation. The red brush was used to mark the foreground (nodules) and the green brush to mark the background (not nodules). (a) We annotated nodules occluded by roots as though the roots were not there. (b) Adjacent nodules were separated using the background class.</p>
        </caption>
        <graphic xlink:href="NPH-236-774-g008" position="anchor" id="jats-graphic-5"/>
      </fig>
      <p>After completing the annotation, we left the models to finish training using the early stopping procedure and then used the final model to segment the respective datasets and produce the appropriate measurements.</p>
      <p>We also repeated this procedure for the projects, but using a restricted number of annotations by limiting to those that had been created in just 30, 60, 90, 120, and 150 min (including the 30 min break period) to give us an indication of model progression over time with the two different annotation strategies.</p>
    </sec>
    <sec id="nph18387-sec-0020">
      <title>Measurement, correlation, and segmentation metrics</title>
      <p>For each project we obtained correlations with manual measurements using the portion of the data not used during training to give a measure of generalization error, which is the expected value of the error on new input (Goodfellow <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0011" ref-type="bibr">2016</xref>). For the roots dataset, the manual measurements were compared with length estimates given by R<sc>oot</sc>P<sc>ainter</sc>, which are obtained from the segmentations using skeletonization and then pixel counting.</p>
      <p>For the biopores and nodules datasets we used the extract region properties functionality from R<sc>oot</sc>P<sc>ainter</sc>, which gives information on each connected region in an output segmentation. For the biopores, the regions &lt; 2 mm in diameter were excluded. The numbers of connected regions for each image were then compared with the manual counts.</p>
      <p>In order to obtain segmentation metrics, we used the extract segmentation metrics function available from the R<sc>oot</sc>P<sc>ainter</sc> extras menu. This function generates a CSV file containing dice score, recall, precision, and accuracy for each of the segmented images in a project. The ground truth used for evaluation is the model prediction with the corrections assigned (i.e. the corrected segmentation). This corrected segmentation is then used to evaluate the predicted segmentation, which is stored in the segmentations folder.</p>
    </sec>
    <sec id="nph18387-sec-0021">
      <title>Filtering nodules by size</title>
      <p>We investigated the effect of filtering out nodules less than a certain size by computing the correlation between the automated and manual nodule counts as a function of a size threshold. The size threshold meant that counted nodules would include only those above a specific area in pixels. We computed the correlation with each size threshold from 0 to 400 px. We did this for the model trained by user b only.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="nph18387-sec-0022">
    <title>Results</title>
    <p>We report the <italic toggle="yes">R</italic>
<sup>2</sup> for each annotation strategy for each user and dataset (Table <xref rid="nph18387-tbl-0002" ref-type="table">2</xref>). Training with corrective annotations resulted in strong correlation (<mml:math id="jats-math-5" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>≥</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math>) between the automated measurements and manual measurements five out of six times. The exception was the nodules dataset for user b with an <italic toggle="yes">R</italic>
<sup>2</sup> of 0.69 (Table <xref rid="nph18387-tbl-0002" ref-type="table">2</xref>). Training with dense annotations resulted in strong correlation three out of six times, with the lowest <italic toggle="yes">R</italic>
<sup>2</sup> being 0.55 also given by the nodules dataset for user b (Table <xref rid="nph18387-tbl-0002" ref-type="table">2</xref>).</p>
    <table-wrap position="float" id="nph18387-tbl-0002" content-type="Table">
      <label>Table 2</label>
      <caption>
        <p><italic toggle="no">R</italic><sup>2</sup> for each training run.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <col align="left" span="1"/>
        <col align="center" span="1"/>
        <col align="char" char="." span="1"/>
        <col align="char" char="." span="1"/>
        <thead valign="bottom">
          <tr style="border-bottom:solid 1px #000000">
            <th align="left" valign="bottom" rowspan="1" colspan="1">Dataset</th>
            <th align="center" valign="bottom" rowspan="1" colspan="1">User</th>
            <th align="center" valign="bottom" rowspan="1" colspan="1">Corrective <italic toggle="yes">R</italic>
<sup>2</sup>
</th>
            <th align="center" valign="bottom" rowspan="1" colspan="1">Dense <italic toggle="yes">R</italic>
<sup>2</sup>
</th>
          </tr>
        </thead>
        <tbody valign="top">
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">Biopores</td>
            <td align="center" valign="top" rowspan="1" colspan="1">a</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.78</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.58</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">Biopores</td>
            <td align="center" valign="top" rowspan="1" colspan="1">b</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.78</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.67</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">Nodules</td>
            <td align="center" valign="top" rowspan="1" colspan="1">a</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.73</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.89</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">Nodules</td>
            <td align="center" valign="top" rowspan="1" colspan="1">b</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.69</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.55</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">Roots</td>
            <td align="center" valign="top" rowspan="1" colspan="1">a</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.89</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.90</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">Roots</td>
            <td align="center" valign="top" rowspan="1" colspan="1">b</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.92</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.90</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot id="nph18387-ntgp-0002">
        <fn id="nph18387-note-0002">
          <p>These are computed by obtaining measurements from the segmentations from the final trained model and then correlating with manual measurements for the associated dataset.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <p>For each annotation strategy, we report both the mean and SE for the <italic toggle="yes">R</italic>
<sup>2</sup> values obtained from all datasets and both users (Table <xref rid="nph18387-tbl-0003" ref-type="table">3</xref>). The mean of the <italic toggle="yes">R</italic>
<sup>2</sup> values obtained when using corrective annotation shows they tended to be higher than with dense annotation, but the differences were not statistically significant (mixed‐effects model; <italic toggle="yes">P</italic> ≤ 0.05). We plot the mean and SE at each time point for which multiple <italic toggle="yes">R</italic>
<sup>2</sup> values were obtained (Fig. <xref rid="nph18387-fig-0004" ref-type="fig">4a</xref>). In general corrective improved over time, overtaking dense performance just after the break in annotation (Fig. <xref rid="nph18387-fig-0004" ref-type="fig">4a</xref>). The 30 min break period taken by the annotator after 1 h corresponds to a flat line in performance during that period (Fig. <xref rid="nph18387-fig-0004" ref-type="fig">4a</xref>). On average, dense annotations were more effective at the 30 min time period, whereas corrective annotations were more effective after 2 h (including the 30 min break) and at the end of the training (Table <xref rid="nph18387-tbl-0003" ref-type="table">3</xref>).</p>
    <table-wrap position="float" id="nph18387-tbl-0003" content-type="Table">
      <label>Table 3</label>
      <caption>
        <p>Mean and SE of the <italic toggle="no">R</italic>
<sup>2</sup> for each annotation strategy.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <col align="left" span="1"/>
        <col align="char" char="." span="1"/>
        <col align="char" char="." span="1"/>
        <thead valign="bottom">
          <tr style="border-bottom:solid 1px #000000">
            <th align="left" valign="bottom" rowspan="1" colspan="1">Strategy</th>
            <th align="left" valign="bottom" rowspan="1" colspan="1">Mean</th>
            <th align="left" valign="bottom" rowspan="1" colspan="1">SE</th>
          </tr>
        </thead>
        <tbody valign="top">
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">Corrective</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.80</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.04</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">Dense</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.75</td>
            <td align="char" valign="top" rowspan="1" colspan="1">0.07</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot id="nph18387-ntgp-0003">
        <fn id="nph18387-note-0003">
          <p>These are computed by obtaining measurements from the segmentations from the final trained model and then correlating with manual measurements. Using a mixed‐effects model with annotation strategy as a fixed factor and user and dataset as random factors, no significant effects were found (<mml:math id="jats-math-6" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math>).</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <fig position="float" fig-type="Fig." id="nph18387-fig-0004">
      <label>Fig. 4</label>
      <caption>
        <p>Annotation duration and accuracy. (a) Mean and SE for the <italic toggle="yes">R</italic>
<sup>2</sup> values over time. These include the 30 min break and are restricted to time points where multiple observations are available. The shaded area indicates the SE. (b) User‐reported duration in minutes for annotating each dataset, excluding the 30 min break taken after 1 h of annotation. The annotator would use the same amount of time for both corrective and dense annotation strategies. The time fell below the limit of 2 h (excluding break) when they ran out of images to annotate.</p>
      </caption>
      <graphic xlink:href="NPH-236-774-g006" position="anchor" id="jats-graphic-7"/>
    </fig>
    <p>We report the duration for each user and dataset (Fig. <xref rid="nph18387-fig-0004" ref-type="fig">4b</xref>). Five out of six times all 200 images were annotated in less than the 2 h time limit. The nodules dataset took the least time, with annotation completed in 66 min and 80 min for users a and b, respectively (Fig. <xref rid="nph18387-fig-0004" ref-type="fig">4b</xref>). The roots dataset for user a was the only project where the 2 h time limit was reached without running out of images (Fig. <xref rid="nph18387-fig-0004" ref-type="fig">4b</xref>).</p>
    <p>We show an example of errors found from the only model trained correctively that did not result in a strong correlation (Fig. <xref rid="nph18387-fig-0005" ref-type="fig">5</xref>). There were cases when the vast majority of pixels were labelled correctly, but a few small incorrect pixels could lead to substantial errors in count (Fig. <xref rid="nph18387-fig-0005" ref-type="fig">5</xref>).</p>
    <fig position="float" fig-type="Fig." id="nph18387-fig-0005">
      <label>Fig. 5</label>
      <caption>
        <p>Two correctly detected nodules shown with three false positives. Segmentation is shown overlaid in light blue on top of a subregion of one of the nodule images used for evaluation. The correct nodules are much larger and on the edge of the root. The three false positives are indicated by a red circle. They are much smaller and bunched together.</p>
      </caption>
      <graphic xlink:href="NPH-236-774-g004" position="anchor" id="jats-graphic-9"/>
    </fig>
    <p>We found filtering nodules less than a certain size to provide substantial reductions in error. There was an improvement in <italic toggle="yes">R</italic>
<sup>2</sup> from 0.69 to 0.75 when changing the area threshold from 0 to 5 px. The benefits increased up to an area threshold of 284 px, giving an <italic toggle="yes">R</italic>
<sup>2</sup> of 0.9 (Notes <xref rid="nph18387-supitem-0001" ref-type="supplementary-material">S6</xref>).</p>
    <p>We show examples of accurate segmentation results obtained with models trained using the corrective annotation strategy (Fig. <xref rid="nph18387-fig-0006" ref-type="fig">6a–c</xref>) along with the corresponding manual measurements plotted against the automatic measurements obtained using R<sc>oot</sc>P<sc>ainter</sc> (Fig. <xref rid="nph18387-fig-0007" ref-type="fig">7</xref>).</p>
    <fig position="float" fig-type="Fig." id="nph18387-fig-0006">
      <label>Fig. 6</label>
      <caption>
        <p>Example input and segmentation output from photographs not used in training. The segmentations are shown in light blue and are from models trained from scratch using no prior knowledge with annotations created using the corrective annotation protocol with R<sc>oot</sc>P<sc>ainter</sc>. (a) Biopores. Annotations created by user b in 1 h 45 min. (b) Nodules. Annotations created by user a in 1 h 6 min. (c) Roots. Annotations created by user a in 2 h.</p>
      </caption>
      <graphic xlink:href="NPH-236-774-g002" position="anchor" id="jats-graphic-11"/>
    </fig>
    <fig position="float" fig-type="Fig." id="nph18387-fig-0007">
      <label>Fig. 7</label>
      <caption>
        <p>Manual measurements plotted against automatic measurements attained using R<sc>oot</sc>P<sc>ainter</sc>. (a) Biopores using user b corrective model. (b) Nodules using user a corrective model. (c) Roots in soil using user a corrective model.</p>
      </caption>
      <graphic xlink:href="NPH-236-774-g003" position="anchor" id="jats-graphic-13"/>
    </fig>
    <p>The observed <italic toggle="yes">R</italic>
<sup>2</sup> values for corrective annotation had a significant positive correlation with annotation duration (<italic toggle="yes">P</italic> &lt; 0.001). There was no significant correlation between annotation time and <italic toggle="yes">R</italic>
<sup>2</sup> values for models trained using dense annotations.</p>
    <p>We plot the <italic toggle="yes">R</italic>
<sup>2</sup> for each project after training was completed along with the <italic toggle="yes">R</italic>
<sup>2</sup> obtained with training done only on annotations at restricted time limits, and refer to these as <italic toggle="yes">trained to completion</italic>, along with the models saved at that time point during the corrective annotation procedure as it happened, which we refer to as <italic toggle="yes">real time</italic> (Fig. <xref rid="nph18387-fig-0008" ref-type="fig">8</xref>). After only 60 min of annotation, all models trained for roots in soil gave a strong correlation with grid counts (Fig. <xref rid="nph18387-fig-0008" ref-type="fig">8</xref>, roots a and b). The performance of dense annotation for user b on the nodules dataset was anomalous, with a decrease in <italic toggle="yes">R</italic>
<sup>2</sup> as more annotated data were used in training (Fig. <xref rid="nph18387-fig-0008" ref-type="fig">8</xref>, nodules b). The corrective models obtained in real time were similar to those trained to completion, except nodules by user b, indicating that computing power was sufficient for real‐time corrective training (Fig. <xref rid="nph18387-fig-0008" ref-type="fig">8</xref>).</p>
    <fig position="float" fig-type="Fig." id="nph18387-fig-0008">
      <label>Fig. 8</label>
      <caption>
        <p><italic toggle="yes">R</italic><sup>2</sup> for the annotations attained after 30, 60, 90, and 120 min and the final time point for users a and b on the three datasets for dense and corrective annotation strategies. ‘Trained to completion’ refers to models that were trained until stopping without interaction, using the annotations created within the specified time period, whereas ‘real time’ refers to models saved during the corrective annotation procedure as it happened. For the corrective annotations we plot both the performance of the model saved during the training procedure and the same model if allowed to train to completion with the annotations available at that time.</p>
      </caption>
      <graphic xlink:href="NPH-236-774-g005" position="anchor" id="jats-graphic-15"/>
    </fig>
    <p>We plot the number of images viewed and annotated for the corrective and dense annotation strategies (Fig. <xref rid="nph18387-fig-0009" ref-type="fig">9a</xref>). For the corrective annotation strategy, only some of the viewed images required annotation. In all cases the annotator was able to progress through more images using corrective annotation (Fig. <xref rid="nph18387-fig-0009" ref-type="fig">9a</xref>).</p>
    <fig position="float" fig-type="Fig." id="nph18387-fig-0009">
      <label>Fig. 9</label>
      <caption>
        <p>Annotation progression. (a) Number of images viewed and annotated for the dense and corrective annotation strategies. For dense annotation, all images are both viewed and annotated, whereas corrective annotations are only added for images where the model predictions contain clear errors. (b) Total number of annotated pixels for dense and corrective annotation strategies over time during the annotation procedure. For dense annotation, almost all pixels in each image are annotated. Corrective annotations are only applied to areas of the image where the model being trained exhibits errors.</p>
      </caption>
      <graphic xlink:href="NPH-236-774-g007" position="anchor" id="jats-graphic-17"/>
    </fig>
    <p>For the roots and nodules datasets for user b for the first hour of training, progress through the images was faster when performing dense annotation (Fig. <xref rid="nph18387-fig-0009" ref-type="fig">9a</xref>, roots b and nodules b).</p>
    <p>We plot the number of labelled pixels for each training procedure over time for both corrective and dense annotations (Fig. <xref rid="nph18387-fig-0009" ref-type="fig">9b</xref>). With corrective annotation, fewer pixels were labelled in the same time period, and as the annotator progressed through the images the rate of label addition decreased (Fig. <xref rid="nph18387-fig-0009" ref-type="fig">9b</xref>). The dice score (Fig. <xref rid="nph18387-fig-0010" ref-type="fig">10a</xref>) and accuracy (Fig. <xref rid="nph18387-fig-0010" ref-type="fig">10b</xref>) are also plotted, along with running averages (<italic toggle="yes">n</italic> = 30) for the models trained using corrective annotation. For both dice and accuracy, the running average shows both large fluctuations and a trend of continuous improvement as more images are annotated. The performance of the biopores model being trained by user b is an outlier, as it appears to decrease in accuracy and to a lesser extent dice score towards the end of training. For the nodules and roots datasets, towards the end of corrective annotation, the dice score is approximately 0.9 but with larger fluctuations, whereas for the biopores the dice score appears to stay consistently above 0.9.</p>
    <fig position="float" fig-type="Fig." id="nph18387-fig-0010">
      <label>Fig. 10</label>
      <caption>
        <p>Segmentation metrics for each of the images segmented as part of the interactive segmentation procedure in order of annotation: (a) dice score; (b) accuracy. Metrics were computed using the segmentations and corrective annotations created as part of the interactive training procedure for user a and user b for each of the three datasets. In this case the ground truth used for evaluation is the model prediction with the corrections assigned (i.e. the corrected segmentation). Dice score ranges from 0 to 1 and is higher when the model prediction agrees with the ground truth. Accuracy ranges from 0 to 1 and is the ratio of pixels that were predicted correctly to the total pixels in the image. As accuracy is very high, to show the changes in the moving average, accuracy is only shown in the range of 0.99 to 1.0. There is a trend of continuous improvement as more images are annotated and interactive training continues.</p>
      </caption>
      <graphic xlink:href="NPH-236-774-g010" position="anchor" id="jats-graphic-19"/>
    </fig>
  </sec>
  <sec sec-type="discussion" id="nph18387-sec-0023">
    <title>Discussion</title>
    <p>In this study, we focused on annotation duration, as we consider the time requirements for annotation rather than the number of available images to be more relevant to the concerns of the majority of plant research groups looking to use deep learning for image analysis. Our results, for corrective training in particular, confirm our first hypothesis by showing that a deep‐learning model can be trained to a high accuracy for the three different datasets of varying target objects, background, and image quality in &lt; 2 h of annotation time.</p>
    <p>Our results demonstrate the feasibility of training an accurate model using annotations made in a short time period, which challenges the claims that tens of thousands of images (Ubbens <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0074" ref-type="bibr">2018</xref>) or substantial labelled data (Narisetti <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0038" ref-type="bibr">2019</xref>) are required to use CNNs. In practice, we also expect longer annotation periods to provide further improvement. The <italic toggle="yes">R</italic>
<sup>2</sup> for corrective training had a significant correlation with annotation duration, indicating that spending more time annotating would continue to improve performance.</p>
    <p>There was a trend for an increasing fraction of viewed images to be accepted without further annotation later in the corrective training (Fig. <xref rid="nph18387-fig-0009" ref-type="fig">9a</xref>), indicating fewer of the images required corrections as the model performance improved. This aligns with the reduction in the rate of growth for the total amount of corrections (Fig. <xref rid="nph18387-fig-0009" ref-type="fig">9b</xref>), indicating continuous improvement in the model accuracy over time during the corrective training.</p>
    <p>We suspect the cases where dense annotation had a comparatively faster speed in the beginning (Fig. <xref rid="nph18387-fig-0009" ref-type="fig">9a</xref>, roots b and nodules b) were due to three factors. First, switching through images has little overhead when using the dense annotation strategy as there is no delay caused by waiting for segmentations to be returned from the server. Second, corrective annotation will take a similar amount of time to dense annotation in the beginning as the annotator needs to assign a large amount of corrections for each image. And third, many of the nodule images did not contain nodules, meaning dense annotations could be added almost instantly.</p>
    <p>The average dice scores of approximately 0.9 for roots (Fig. <xref rid="nph18387-fig-0010" ref-type="fig">10a</xref>) are similar to previous results for root segmentation (Smith <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0058" ref-type="bibr">2020d</xref>), indicating our trained model was accurate (please see later note on the limitations of such comparisons). For comparison, in previous work, similar root segmentation accuracy was obtained using approximately 25 h of root annotation time (Smith <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0058" ref-type="bibr">2020d</xref>). We found that the dice scores for biopores were higher than those for roots or nodules (Fig. <xref rid="nph18387-fig-0010" ref-type="fig">10a</xref>), which we suspect was due to the biopore dataset being easier to segment, with higher contrast between the biopore and surrounding soil. Our reported dice‐scores plots (Fig. <xref rid="nph18387-fig-0010" ref-type="fig">10a</xref>) serve as a point of comparison for future work, and in combination with R<sc>oot</sc>P<sc>ainter</sc>'s functionality to extract segmentation metrics they provide a convenient way for users to confirm and report that their trained models have a suitable accuracy.</p>
    <p>Although a trend in improvement over time is shown in the segmentation metrics (Fig. <xref rid="nph18387-fig-0010" ref-type="fig">10a,b</xref>), there were large fluctuations and for user b on the biopores dataset, with there appearing to be a decrease in the model accuracy towards the end of corrective annotation (Fig. <xref rid="nph18387-fig-0010" ref-type="fig">10b</xref>). We expect that the variation in accuracy was caused by both intra‐annotator variation and by the large amount of variation in the quality and difficulty of the images for the network, as can be seen from all plots.</p>
    <p>It is important to note that metrics, such as those mentioned already, comparing corrected predictions with predictions are not comparable to completely independent annotations done while blinded to the model predictions. Completely independent annotations include to some degree an amount of irreducible error stemming from observer variation and uncertainty. Corrections, on the other hand, are expected to be of clear errors and may in some cases, therefore, be a more useful measure of the system's performance as assessed by the observer in question.</p>
    <p>Although corrective annotation tended to produce models with higher accuracy relative to dense (Table <xref rid="nph18387-tbl-0003" ref-type="table">3</xref>), the lack of a statistically significant difference prevents us from coming to a more substantive conclusion about the benefits of corrective over dense annotation. Despite being unable to confirm our second hypothesis, that corrective annotation provides improved accuracy over dense in a limited time period, it is still clear that it will provide many real‐world advantages. The feedback given to the annotator will allow them to better understand the characteristics of the model trained with the annotations applied. They will be able to make a more informed decision about how many images to annotate to train a model to sufficient accuracy for their use case.</p>
    <p>Although strong correlation was attained when using the models trained with corrective annotation, they in some cases overestimated (Fig. <xref rid="nph18387-fig-0007" ref-type="fig">7a</xref>) or underestimated (Fig. <xref rid="nph18387-fig-0007" ref-type="fig">7b</xref>) the objects of interest compared with the manual counts. For the biopores (Fig. <xref rid="nph18387-fig-0007" ref-type="fig">7a</xref>), this may be related to the calibration and threshold procedure, which results in biopores below a certain diameter being excluded from the dataset. We inspected the outlier in Fig. <xref rid="nph18387-fig-0007" ref-type="fig">7(b)</xref> where R<sc>oot</sc>P<sc>ainter</sc> had overestimated the number of nodules compared with the manual counts. We found that this image (043.jpg) contained many roots that were bunched together more closely than what was typical in the dataset. We suspect this had confused the trained network and could be mitigated by using a consistent and reduced amount of roots per scan, whilst using more of the images for training and annotating for longer to capture more of the variation in the dataset.</p>
    <p>In one case, training with corrective annotation failed to produce a model that gave a strong correlation with the manual measurements. This was for the nodules data for user b, where the <italic toggle="yes">R</italic>
<sup>2</sup> was 0.69. We suspect this was partially due to the limited number of nodules in the training data. Many of the images in the dataset created for training contained no nodules and only included the background. This also meant the annotation was able to finish in less time. We consider this a limitation of the experimental design, as we expect that a larger dataset that allowed for annotating nodules for the full 2 h time period would have provided better insights into the performance of the corrective training procedure.</p>
    <p>Fig. <xref rid="nph18387-fig-0005" ref-type="fig">5</xref> shows examples of some of the errors in the nodules dataset. In practice, the annotator would be able to view and correct such errors during training until they had abated. We noticed that many of the nodule errors were smaller false positives, so investigated the effect of filtering out nodules less than a certain size (Notes <xref rid="nph18387-supitem-0001" ref-type="supplementary-material">S6</xref>). The substantial improvements in nodule count correlation from 0.69 to 0.93 when using a nodule size threshold can be explained by the removal of smaller false‐positive artefacts. This indicates that the model was producing many small false‐positive predictions, which could also explain some of the overestimation of nodules (Fig. <xref rid="nph18387-fig-0007" ref-type="fig">7b</xref>).</p>
    <p>The problem with small false positives may have been mitigated with the dense annotations as a larger amount of background examples are added, suppressing more of the false‐positive predictions that arise in the limited training time.</p>
    <p>The improvement in <italic toggle="yes">R</italic>
<sup>2</sup> when removing small nodules may also be due to differences in subjective interpretation of what is a nodule, between the original counter and annotator training the model.</p>
    <p>The reduction in <italic toggle="yes">R</italic>
<sup>2</sup> as dense annotation time increased, shown in nodules b (Fig. <xref rid="nph18387-fig-0008" ref-type="fig">8</xref>), was highly unexpected. Although in some cases increasing training data can decrease performance when training CNNs (Nakkiran <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0035" ref-type="bibr">2019</xref>), it is usually the case that the opposite is observed. We suspect these anomalous results are due to the large amount of variation in the success of the dense training procedure, rather than revealing any general relationship between performance and the amount of data used.</p>
    <p>As the nodule images are captured in a controlled environment, further improvements to accuracy could be attained by reducing controllable sources of variation and increasing the technical quality of the images. The lighting was also varying for the nodules, with approximately half of the images underexposed. We expect that more consistent lighting conditions would further improve the nodule counting accuracy. Cropping the nodule images manually could also become a time‐consuming bottleneck, which could be avoided by ensuring all the roots and nodules were positioned inside the border and having the placement of the border be fixed in its position in the scanner such that the cropping could be done by removing a fixed amount from each image, which would be trivial to automate.</p>
    <p>Fig. <xref rid="nph18387-fig-0004" ref-type="fig">4(a)</xref> indicates corrective annotation leads to lower <italic toggle="yes">R</italic>
<sup>2</sup> in the earlier phases of annotation (e.g. within 60 min). We suspect this is due to dense annotation having an advantage at the start as the user is able to annotate more pixels in less time using dense annotation with no overhead caused by waiting for segmentations from the server. We suspect in many cases that corrective annotation will provide no benefits in terms of efficiency when the model is in the early stages of training, as the user will still have to apply large amounts of annotation to each image whilst being slowed down by the delay in waiting for segmentations. Later in training (e.g. after 1 h 40 min), corrective annotation overtakes dense annotation in terms of mean <italic toggle="yes">R</italic>
<sup>2</sup> performance (Fig. <xref rid="nph18387-fig-0004" ref-type="fig">4a</xref>). We suspect this is due to the advantages of corrective annotation increasing as the model converges, when more of the examples are segmented correctly and do not need adding to the training data as they would provide negligible utility beyond what has already been annotated. Our results show corrective annotation achieves competitive performance with a fraction of the labelled pixels compared with dense annotation (Fig. <xref rid="nph18387-fig-0009" ref-type="fig">9b</xref>). These results align with Toneva &amp; Sordoni (<xref rid="nph18387-bib-0073" ref-type="bibr">2019</xref>), who confirmed that a large portion of the training data could be discarded without hurting generalization performance. This view is further supported by theoretical work (Soudry <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0063" ref-type="bibr">2018</xref>) showing in certain cases that networks will learn a maximum‐margin classifier, with some data points being less relevant to the decision boundary.</p>
    <p>The corrective training procedure performance had lower SE after 1 h (Fig. <xref rid="nph18387-fig-0004" ref-type="fig">4a</xref>), and particularly at the end (Table <xref rid="nph18387-tbl-0003" ref-type="table">3</xref>). We conjecture that the corrective annotation strategy stabilized convergence and increased the robustness of the training procedure to the changes in dataset with the fixed hyperparameters by allowing the specific parts of the dataset used in training to be added based on the weaknesses that appear in each specific training run.</p>
    <p>In more heterogeneous datasets with many anomalies, we suspect corrective annotation to provide more advantages in comparison with dense annotation, as working through many images to find hard examples will capture more useful training data. A potential limitation of the corrective annotation procedure is the suitability of these annotations when used as a validation set for early stopping, as they are less likely to provide a representative sample, compared with a random selection. Our annotation protocol for corrective annotation involved initially focusing on clear examples (Notes <xref rid="nph18387-supitem-0001" ref-type="supplementary-material">S3</xref>), as in preliminary experiments we found corrective annotation did not work effectively at the very start of training. Training start‐up was also found to be a challenge for other systems utilizing interactive training procedures (Gonda <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0010" ref-type="bibr">2017</xref>), indicating future work in this area would be beneficial.</p>
    <p>Another possible limitation of corrective annotations is that they are based on the model's weaknesses at a specific point in time. This annotation will likely become less useful as the model drifts away to have different errors from those that were corrected.</p>
    <p>One explanation for the consistently strong correlation on the root data compared with biopores and nodules is that the correlation with counts will be more sensitive to small errors than correlation with length. A small pixel‐wise difference can make a large impact on the counts, whereas a pixel erroneously added to the width of a root may have no impact on the length and even pixels added to the end of the root will cause a small difference.</p>
    <p>A limitation of the R<sc>oot</sc>P<sc>ainter</sc> software is the hardware requirements for the server. We ran the experiments using two Nvidia RTX 2080 Ti GPUs connected with NVLink. Purchasing such GPUs may be prohibitively expensive for smaller projects, and hosted services such as Paperspace, Amazon Web Services, or Google Cloud may be more affordable. Although model training and data processing can be completed using the client user interface, specialist technical knowledge may still be required to set up the server component of the system. To mitigate the hardware requirements and technical knowledge required for the initial setup, we have prepared an open‐source J<sc>upyter</sc> notebook web application (see <xref rid="nph18387-sec-0027" ref-type="sec">Data availability</xref> section) which is made available via Google Colab and guides a user without specialist technical knowledge through setting up a R<sc>oot</sc>P<sc>ainter</sc> server using a freely available GPU. As part of the online supplementary material we also make a video available showing how to interactively train and use a biopores segmentation model with R<sc>oot</sc>P<sc>ainter</sc> (Video <xref rid="nph18387-supitem-0002" ref-type="supplementary-material">S1</xref>).</p>
    <p>Although R<sc>oot</sc>P<sc>ainter</sc> automates the annotation process by providing an initial model prediction, the correction process, which involves manually drawing, could still become laborious, especially for larger, more complex images. Interactive segmentation tools, such as G<sc>rabber</sc> (Bragantini <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0005" ref-type="bibr">2020</xref>), may be complementary and could be investigated in further work to accelerate R<sc>oot</sc>P<sc>ainter</sc>'s corrective‐annotation process.</p>
    <p>There are a limited number of root traits that can be exported from R<sc>oot</sc>P<sc>ainter</sc> in comparison with other root segmentation software applications, such as <sc>fa</sc>RIA (Narisetti <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0037" ref-type="bibr">2021</xref>). This limitation has been addressed by the addition of a conversion utility, available from the R<sc>oot</sc>P<sc>ainter</sc> extras menu, that enables R<sc>oot</sc>P<sc>ainter</sc> segmentations to be conveniently processed with R<sc>hizo</sc>V<sc>ision</sc> E<sc>xplorer</sc> (Seethepalli <italic toggle="yes">et al</italic>., <xref rid="nph18387-bib-0051" ref-type="bibr">2021</xref>), facilitating the extraction of many more traits from the R<sc>oot</sc>P<sc>ainter</sc> segmentations.</p>
    <p>In addition to the strong correlations with manual measurements when using corrective annotation, we found the accuracy of the segmentations obtained for biopores, nodules, and roots to indicate that the software would be useful for the intended counting and length measurement tasks (Fig. <xref rid="nph18387-fig-0006" ref-type="fig">6a–c</xref>).</p>
    <p>The performance of R<sc>oot</sc>P<sc>ainter</sc> on the images not used in the training procedure indicates that it would perform well as a fully automatic system on similar data. Our results are a demonstration that, for many datasets, using R<sc>oot</sc>P<sc>ainter</sc> will make it possible to complete the labelling, training, and data processing within one working day.</p>
  </sec>
  <sec id="nph18387-sec-0025">
    <title>Author contributions</title>
    <p>AGS implemented R<sc>oot</sc>P<sc>ainter</sc> and wrote the manuscript with assistance from all authors. AGS, EH, and JP designed the experiment. AGS and EH annotated and collaborated on the design of the study and the introduction. CG and MA captured and prepared the nodules data. NAFO tested the software and annotation protocol during development. DBD and KTK provided supervision and conceptual input. All authors read and approved the final manuscript.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material id="nph18387-supitem-0001" position="float" content-type="local-data">
      <caption>
        <p><bold>Notes S1</bold> Server software set‐up instructions. Instructions on how to set up the R<sc>oot</sc>P<sc>ainter</sc> server.</p>
        <p><bold>Notes S2</bold> Keyboard shortcuts. A table showing the keyboard shortcuts that can be used to speed up the annotation process when using the R<sc>oot</sc>P<sc>ainter</sc> client.</p>
        <p><bold>Notes S3</bold> Corrective training protocol. Instructions on how to train a model with R<sc>oot</sc>P<sc>ainter</sc> using the corrective annotation protocol, as was done in this study.</p>
        <p><bold>Notes S4</bold> Corrective annotation advice. Extra tips on how to execute the corrective training protocol effectively.</p>
        <p><bold>Notes S5</bold> Dense annotation advice. Tips on how to annotate densely, as was done in this study for comparison purposes.</p>
        <p><bold>Notes S6</bold> Nodule threshold plot. A plot showing correlation between automatic and manual nodules counts as a function of a nodule size threshold.</p>
      </caption>
      <media xlink:href="NPH-236-774-s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="nph18387-supitem-0002" position="float" content-type="local-data">
      <caption>
        <p><bold>Video S1</bold> R<sc>oot</sc>P<sc>ainter</sc> biopore model training video. A 42 min video showing the training of a biopore segmentation model using R<sc>oot</sc>P<sc>ainter</sc> with corrective annotation.</p>
        <p>Please note: Wiley Blackwell are not responsible for the content or functionality of any Supporting Information supplied by the authors. Any queries (other than missing material) should be directed to the <italic toggle="yes">New Phytologist</italic> Central Office.</p>
      </caption>
      <media xlink:href="NPH-236-774-s001.mp4" mimetype="video" mime-subtype="mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="nph18387-sec-0024">
    <title>Acknowledgements</title>
    <p>Martin Nenov for proof reading and conceptual development. Camilla Ruø Rasmussen for support with using the rhizotron images. Ivan Richter Vogelius and Sune Darkner for support during the final stage of the project. Prof. Dr Timo Kautz and Prof. Dr Ulrich Köpke for provision of the biopore dataset. Guanying Chen, Corentin Bonaventure L. R. Clement, and John Kirkegaard for helping test the software by being early adopters in using R<sc>oot</sc>P<sc>ainter</sc> for their root research. Simon Fiil Svane for providing insights into root phenotyping and image analysis.</p>
    <p>We thank Villum Foundation (DeepFrontier project, grant no. VKR023338) for financially supporting this study. Eusun Han is a Marie Curie Global Fellow working on a project SenseFuture (no. 884364) funded by European Union's Horizon 2020 Research and Innovation Programme. The biopore dataset was provided from a study supported by the German Research Foundation (Deutsche Forschungsgemeinschaft, DFG) within the framework of the research unit DFG FOR 1320.</p>
  </ack>
  <sec sec-type="data-availability" id="nph18387-sec-0027">
    <title>Data availability</title>
    <p>The nodules dataset is available from Smith <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0054" ref-type="bibr">2020a</xref>). The biopores dataset is available from Smith <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0055" ref-type="bibr">2020b</xref>). The roots dataset is available from Smith <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0057" ref-type="bibr">2019a</xref>). The client software installers are available from <ext-link xlink:href="https://github.com/Abe404/root_painter/releases" ext-link-type="uri" specific-use="software is-supplemented-by">https://github.com/Abe404/root_painter/releases</ext-link>. The source code for both client and server is available from <ext-link xlink:href="https://github.com/Abe404/root_painter" ext-link-type="uri" specific-use="software is-supplemented-by">https://github.com/Abe404/root_painter</ext-link>. The created training datasets and final trained models are available from Smith <italic toggle="yes">et al</italic>. (<xref rid="nph18387-bib-0056" ref-type="bibr">2020c</xref>). The Colab notebook is available at <ext-link xlink:href="https://colab.research.google.com/drive/104narYAvTBt-X4QEDrBSOZm_DRaAKHtA?usp=sharing" ext-link-type="uri" specific-use="software is-supplemented-by">https://colab.research.google.com/drive/104narYAvTBt‐X4QEDrBSOZm_DRaAKHtA?usp=sharing</ext-link>.</p>
  </sec>
  <ref-list content-type="cited-references" id="nph18387-bibl-0001">
    <title>References</title>
    <ref id="nph18387-bib-0001">
      <mixed-citation publication-type="journal" id="nph18387-cit-0001"><string-name><surname>Amershi</surname><given-names>S</given-names></string-name>, <string-name><surname>Cakmak</surname><given-names>M</given-names></string-name>, <string-name><surname>Knox</surname><given-names>WB</given-names></string-name>, <string-name><surname>Kulesza</surname><given-names>T</given-names></string-name>. <year>2014</year>. <article-title>Power to the people: the role of humans in interactive machine learning</article-title>. <source>AI Magazine</source><volume>35</volume>: <fpage>105</fpage>–<lpage>120</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0002">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0002"><string-name><surname>Benard</surname><given-names>A</given-names></string-name>, <string-name><surname>Gygli</surname><given-names>M</given-names></string-name>. <year>2017</year>. <article-title>Interactive video object segmentation in the wild</article-title>. <italic toggle="yes">arXiv</italic>: 1801.00269.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0003">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0003"><string-name><surname>Benenson</surname><given-names>R</given-names></string-name>, <string-name><surname>Popov</surname><given-names>S</given-names></string-name>, <string-name><surname>Ferrari</surname><given-names>V</given-names></string-name>. <year>2019</year>. <article-title>Large‐scale interactive object segmentation with human annotators</article-title>. <italic toggle="yes">arXiv</italic>: 1903.10830.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0004">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0004"><string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name><year>2012</year>. <article-title>Practical recommendations for gradient‐based training of deep architectures</article-title>. <italic toggle="yes">arXiv</italic>: 1206.5533.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0005">
      <mixed-citation publication-type="journal" id="nph18387-cit-0005"><string-name><surname>Bragantini</surname><given-names>J</given-names></string-name>, <string-name><surname>Moura</surname><given-names>B</given-names></string-name>, <string-name><surname>Falcão</surname><given-names>AX</given-names></string-name>, <string-name><surname>Cappabianco</surname><given-names>FAM</given-names></string-name>. <year>2020</year>. <article-title>G<sc>rabber</sc>: a tool to improve convergence in interactive image segmentation</article-title>. <source>Pattern Recognition Letters</source><volume>140</volume>: <fpage>267</fpage>–<lpage>273</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0006">
      <mixed-citation publication-type="journal" id="nph18387-cit-0006"><string-name><surname>Buda</surname><given-names>M</given-names></string-name>, <string-name><surname>Maki</surname><given-names>A</given-names></string-name>, <string-name><surname>Mazurowski</surname><given-names>MA</given-names></string-name>. <year>2018</year>. <article-title>A systematic study of the class imbalance problem in convolutional neural networks</article-title>. <source>Neural Networks</source><volume>106</volume>: <fpage>249</fpage>–<lpage>259</lpage>.<pub-id pub-id-type="pmid">30092410</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0007">
      <mixed-citation publication-type="book" id="nph18387-cit-0007"><string-name><surname>Collobert</surname><given-names>R</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>S</given-names></string-name>. <year>2004</year>. <part-title>Links between perceptrons, MLPs and SVMs</part-title>. In: <source>Twenty‐first international conference on machine learning – ICML '04 Banff, Alberta, Canada</source>. <publisher-name>ACM Press</publisher-name>, <fpage>23</fpage>. [WWW document] URL <ext-link xlink:href="http://portal.acm.org/citation.cfm?doid=1015330.1015415" ext-link-type="uri">http://portal.acm.org/citation.cfm?doid=1015330.1015415</ext-link> [accessed 3 August 2022].</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0008">
      <mixed-citation publication-type="journal" id="nph18387-cit-0008"><string-name><surname>Gaggion</surname><given-names>N</given-names></string-name>, <string-name><surname>Ariel</surname><given-names>F</given-names></string-name>, <string-name><surname>Daric</surname><given-names>V</given-names></string-name>, <string-name><surname>Lambert Legendre</surname><given-names>S</given-names></string-name>, <string-name><surname>Roulé</surname><given-names>T</given-names></string-name>, <string-name><surname>Camoirano</surname><given-names>A</given-names></string-name>, <string-name><surname>Milone</surname><given-names>DH</given-names></string-name>, <string-name><surname>Crespi</surname><given-names>M</given-names></string-name>, <string-name><surname>Blein</surname><given-names>T</given-names></string-name>, <string-name><surname>Ferrante</surname><given-names>E</given-names></string-name>. <year>2021</year>. <article-title>C<sc>hrono</sc>R<sc>oot</sc>: high‐throughput phenotyping by deep segmentation networks reveals novel temporal parameters of plant root system architecture</article-title>. <source>GigaScience</source><volume>10</volume>: <elocation-id>giab052</elocation-id>.<pub-id pub-id-type="pmid">34282452</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0009">
      <mixed-citation publication-type="journal" id="nph18387-cit-0009"><string-name><surname>Gómez‐de Mariscal</surname><given-names>E</given-names></string-name>, <string-name><surname>García‐López‐de Haro</surname><given-names>C</given-names></string-name>, <string-name><surname>Ouyang</surname><given-names>W</given-names></string-name>, <string-name><surname>Donati</surname><given-names>L</given-names></string-name>, <string-name><surname>Lundberg</surname><given-names>E</given-names></string-name>, <string-name><surname>Unser</surname><given-names>M</given-names></string-name>, <string-name><surname>Muñoz‐Barrutia</surname><given-names>A</given-names></string-name>, <string-name><surname>Sage</surname><given-names>D</given-names></string-name>. <year>2021</year>. <article-title>D<sc>eep</sc>I<sc>mage</sc>J: a user‐friendly environment to run deep learning models in I<sc>mage</sc>J</article-title>. <source>Nature Methods</source><volume>18</volume>: <fpage>1192</fpage>–<lpage>1195</lpage>.<pub-id pub-id-type="pmid">34594030</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0010">
      <mixed-citation publication-type="book" id="nph18387-cit-0010"><string-name><surname>Gonda</surname><given-names>F</given-names></string-name>, <string-name><surname>Kaynig</surname><given-names>V</given-names></string-name>, <string-name><surname>Jones</surname><given-names>TR</given-names></string-name>, <string-name><surname>Haehn</surname><given-names>D</given-names></string-name>, <string-name><surname>Lichtman</surname><given-names>JW</given-names></string-name>, <string-name><surname>Parag</surname><given-names>T</given-names></string-name>, <string-name><surname>Pfister</surname><given-names>H</given-names></string-name>. <year>2017</year>. <part-title>I<sc>con</sc>: an interactive approach to train deep neural networks for segmentation of neuronal structures</part-title>. In: <source>2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017) Melbourne, Australia</source>, <fpage>327</fpage>–<lpage>331</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0011">
      <mixed-citation publication-type="book" id="nph18387-cit-0011"><string-name><surname>Goodfellow</surname><given-names>I</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y</given-names></string-name>, <string-name><surname>Courville</surname><given-names>A</given-names></string-name>. <year>2016</year>. <part-title>Deep learning</part-title>. In: <person-group person-group-type="editor"><string-name><surname>Bach</surname><given-names>F</given-names></string-name></person-group>, ed. <source>Adaptive computation and machine learning series</source>. <publisher-loc>Cambridge, MA, USA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0012">
      <mixed-citation publication-type="journal" id="nph18387-cit-0012"><string-name><surname>Han</surname><given-names>E</given-names></string-name>, <string-name><surname>Kautz</surname><given-names>T</given-names></string-name>, <string-name><surname>Huang</surname><given-names>N</given-names></string-name>, <string-name><surname>Köpke</surname><given-names>U</given-names></string-name>. <year>2017</year>. <article-title>Dynamics of plant nutrient uptake as affected by bioporeassociated root growth in arable subsoil</article-title>. <source>Plant and Soil</source><volume>415</volume>: <fpage>145</fpage>–<lpage>160</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0013">
      <mixed-citation publication-type="journal" id="nph18387-cit-0013"><string-name><surname>Han</surname><given-names>E</given-names></string-name>, <string-name><surname>Kautz</surname><given-names>T</given-names></string-name>, <string-name><surname>Perkons</surname><given-names>U</given-names></string-name>, <string-name><surname>Lüsebrink</surname><given-names>M</given-names></string-name>, <string-name><surname>Pude</surname><given-names>R</given-names></string-name>, <string-name><surname>Köpke</surname><given-names>U</given-names></string-name>. <year>2015a</year>. <article-title>Quantification of soil biopore density after perennial fodder cropping</article-title>. <source>Plant and Soil</source><volume>394</volume>: <fpage>73</fpage>–<lpage>85</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0014">
      <mixed-citation publication-type="journal" id="nph18387-cit-0014"><string-name><surname>Han</surname><given-names>E</given-names></string-name>, <string-name><surname>Kautz</surname><given-names>T</given-names></string-name>, <string-name><surname>Perkons</surname><given-names>U</given-names></string-name>, <string-name><surname>Uteau</surname><given-names>D</given-names></string-name>, <string-name><surname>Peth</surname><given-names>S</given-names></string-name>, <string-name><surname>Huang</surname><given-names>N</given-names></string-name>, <string-name><surname>Horn</surname><given-names>R</given-names></string-name>, <string-name><surname>Köpke</surname><given-names>U</given-names></string-name>. <year>2015b</year>. <article-title>Root growth dynamics inside and outside of soil biopores as affected by crop sequence determined with the profile wall method</article-title>. <source>Biology and Fertility of Soils</source><volume>51</volume>: <fpage>847</fpage>–<lpage>856</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0015">
      <mixed-citation publication-type="journal" id="nph18387-cit-0015"><string-name><surname>Hauggaard‐Nielsen</surname><given-names>H</given-names></string-name>, <string-name><surname>Ambus</surname><given-names>P</given-names></string-name>, <string-name><surname>Jensen</surname><given-names>ES</given-names></string-name>. <year>2001</year>. <article-title>Temporal and spatial distribution of roots and competition for nitrogen in pea‐barley intercrops – a field study employing P‐32 technique</article-title>. <source>Plant and Soil</source><volume>236</volume>: <fpage>63</fpage>–<lpage>74</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0016">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0016"><string-name><surname>He</surname><given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>X</given-names></string-name>, <string-name><surname>Ren</surname><given-names>S</given-names></string-name>, <string-name><surname>Sun</surname><given-names>J</given-names></string-name>. <year>2015a</year>. <article-title>Deep residual learning for image recognition</article-title>. <italic toggle="yes">arXiv</italic>: 1512.03385.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0017">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0017"><string-name><surname>He</surname><given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>X</given-names></string-name>, <string-name><surname>Ren</surname><given-names>S</given-names></string-name>, <string-name><surname>Sun</surname><given-names>J</given-names></string-name>. <year>2015b</year>. <article-title>Delving deep into rectifiers: surpassing human‐level performance on I<sc>mage</sc>N<sc>et</sc> classification</article-title>. <italic toggle="yes">arXiv</italic>: 1502.01852.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0018">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0018"><string-name><surname>He</surname><given-names>Z</given-names></string-name>, <string-name><surname>Xie</surname><given-names>L</given-names></string-name>, <string-name><surname>Chen</surname><given-names>X</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Tian</surname><given-names>Q</given-names></string-name>. <year>2019</year>. <article-title>
Data augmentation revisited: rethinking the distribution gap between clean and augmented data
</article-title>. <italic toggle="yes">arXiv</italic>: 1909.09148.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0019">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0019"><string-name><surname>Hernández‐García</surname><given-names>A</given-names></string-name>, <string-name><surname>König</surname><given-names>P</given-names></string-name>. <year>2019</year>. <article-title>Data augmentation instead of explicit regularization</article-title>. <italic toggle="yes">arXiv</italic>: 1806.03852.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0020">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0020"><string-name><surname>Hu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Soltoggio</surname><given-names>A</given-names></string-name>, <string-name><surname>Lock</surname><given-names>R</given-names></string-name>, <string-name><surname>Carter</surname><given-names>S</given-names></string-name>. <year>2018</year>. <article-title>A fully convolutional two‐stream fusion network for interactive image segmentation</article-title>. <italic toggle="yes">arXiv</italic>: 1807.02480.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0021">
      <mixed-citation publication-type="journal" id="nph18387-cit-0021"><string-name><surname>Jiang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Li</surname><given-names>C</given-names></string-name>. <year>2020</year>. <article-title>Convolutional neural networks for image‐based high‐throughput plant phenotyping: a review</article-title>. <source>Plant Phenomics</source><volume>2020</volume>: <fpage>1</fpage>–<lpage>22</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0022">
      <mixed-citation publication-type="journal" id="nph18387-cit-0022"><string-name><surname>Kamilaris</surname><given-names>A</given-names></string-name>, <string-name><surname>Prenafeta‐Boldú</surname><given-names>FX</given-names></string-name>. <year>2018</year>. <article-title>A review of the use of convolutional neural networks in agriculture</article-title>. <source>The Journal of Agricultural Science</source><volume>156</volume>: <fpage>312</fpage>–<lpage>322</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0023">
      <mixed-citation publication-type="journal" id="nph18387-cit-0023"><string-name><surname>Kautz</surname><given-names>T</given-names></string-name>. <year>2015</year>. <article-title>Research on subsoil biopores and their functions in organically managed soils: a review</article-title>. <source>Renewable Agriculture and Food Systems</source><volume>30</volume>: <fpage>318</fpage>–<lpage>327</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0024">
      <mixed-citation publication-type="journal" id="nph18387-cit-0024"><string-name><surname>Kellenberger</surname><given-names>B</given-names></string-name>, <string-name><surname>Marcos</surname><given-names>D</given-names></string-name>, <string-name><surname>Lobry</surname><given-names>S</given-names></string-name>, <string-name><surname>Tuia</surname><given-names>D</given-names></string-name>. <year>2019</year>. <article-title>Half a percent of labels is enough: efficient animal detection in UAV imagery using deep CNNs and active learning</article-title>. <source>IEEE Transactions on Geoscience and Remote Sensing</source><volume>57</volume>: <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0025">
      <mixed-citation publication-type="journal" id="nph18387-cit-0025"><string-name><surname>Kessel</surname><given-names>CV</given-names></string-name>, <string-name><surname>Horwath</surname><given-names>WR</given-names></string-name>, <string-name><surname>Hartwig</surname><given-names>U</given-names></string-name>, <string-name><surname>Harris</surname><given-names>D</given-names></string-name>, <string-name><surname>LÜscher</surname><given-names>A</given-names></string-name>. <year>2000</year>. <article-title>Net soil carbon input under ambient and elevated CO<sub>2</sub> concentrations: isotopic evidence after 4 years</article-title>. <source>Global Change Biology</source><volume>6</volume>: <fpage>435</fpage>–<lpage>444</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0026">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0026"><string-name><surname>Kontogianni</surname><given-names>T</given-names></string-name>, <string-name><surname>Gygli</surname><given-names>M</given-names></string-name>, <string-name><surname>Uijlings</surname><given-names>J</given-names></string-name>, <string-name><surname>Ferrari</surname><given-names>V</given-names></string-name>. <year>2019</year>. <article-title>Continuous adaptation for interactive object segmentation by learning from corrections</article-title>. <italic toggle="yes">arXiv</italic>:1911.12709.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0027">
      <mixed-citation publication-type="journal" id="nph18387-cit-0027"><string-name><surname>Kopke</surname><given-names>U</given-names></string-name>, <string-name><surname>Athmann</surname><given-names>M</given-names></string-name>, <string-name><surname>Han</surname><given-names>E</given-names></string-name>, <string-name><surname>Kautz</surname><given-names>T</given-names></string-name>. <year>2015</year> jun. <article-title>Optimising cropping techniques for nutrient and environmental management in organic agriculture</article-title>. <source>Sustainable Agriculture Research</source>
<volume>4</volume>: <fpage>15</fpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0028">
      <mixed-citation publication-type="journal" id="nph18387-cit-0028"><string-name><surname>LeCun</surname><given-names>Y</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>G</given-names></string-name>. <year>2015</year>. <article-title>Deep learning</article-title>. <source>Nature</source><volume>521</volume>: <fpage>436</fpage>–<lpage>444</lpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0029">
      <mixed-citation publication-type="book" id="nph18387-cit-0029"><string-name><surname>Li</surname><given-names>Z</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Q</given-names></string-name>, <string-name><surname>Koltun</surname><given-names>V</given-names></string-name>. <year>2018</year>. <part-title>Interactive image segmentation with latent diversity</part-title>. In: <source>2018 IEEE/CVF conference on computer vision and pattern recognition Salt Lake City, UT</source>: <publisher-name>IEEE</publisher-name>, <fpage>577</fpage>–<lpage>585</lpage>. [WWW document] URL <ext-link xlink:href="https://ieeexplore.ieee.org/document/8578165/" ext-link-type="uri">https://ieeexplore.ieee.org/document/8578165/</ext-link> [accessed 3 August 2022].</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0030">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0030"><string-name><surname>Lin</surname><given-names>H</given-names></string-name>, <string-name><surname>Upchurch</surname><given-names>P</given-names></string-name>, <string-name><surname>Bala</surname><given-names>K</given-names></string-name>. <year>2020</year>. <article-title>
Block annotation: better image annotation for semantic segmentation with sub‐image decomposition
</article-title>. arXiv: 2002.06626.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0031">
      <mixed-citation publication-type="journal" id="nph18387-cit-0031"><string-name><surname>Lobet</surname><given-names>G</given-names></string-name>, <string-name><surname>Draye</surname><given-names>X</given-names></string-name>, <string-name><surname>Périlleux</surname><given-names>C</given-names></string-name>. <year>2013</year>. <article-title>An online database for plant image analysis software tools</article-title>. <source>Plant Methods</source><volume>9</volume>: <fpage>38</fpage>.<pub-id pub-id-type="pmid">24107223</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0032">
      <mixed-citation publication-type="journal" id="nph18387-cit-0032"><string-name><surname>Lynch</surname><given-names>JP</given-names></string-name>. <year>2007</year>. <article-title>TURNER REVIEW No. 14. Roots of the second Green Revolution</article-title>. <source>Australian Journal of Botany</source><volume>55</volume>: <fpage>493</fpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0033">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0033"><string-name><surname>Mahadevan</surname><given-names>S</given-names></string-name>, <string-name><surname>Voigtlaender</surname><given-names>P</given-names></string-name>, <string-name><surname>Leibe</surname><given-names>B</given-names></string-name>. <year>2018</year>. <article-title>Iteratively trained interactive segmentation</article-title>. <italic toggle="yes">arXiv</italic>: 1805.04398</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0034">
      <mixed-citation publication-type="journal" id="nph18387-cit-0034"><string-name><surname>Minervini</surname><given-names>M</given-names></string-name>, <string-name><surname>Scharr</surname><given-names>H</given-names></string-name>, <string-name><surname>Tsaftaris</surname><given-names>SA</given-names></string-name>. <year>2015</year>. <article-title>Image analysis: the new bottleneck in plant phenotyping (applications corner)</article-title>. <source>IEEE Signal Processing Magazine</source><volume>32</volume>: <fpage>126</fpage>–<lpage>131</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0035">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0035"><string-name><surname>Nakkiran</surname><given-names>P</given-names></string-name>, <string-name><surname>Kaplun</surname><given-names>G</given-names></string-name>, <string-name><surname>Bansal</surname><given-names>Y</given-names></string-name>, <string-name><surname>Yang</surname><given-names>T</given-names></string-name>, <string-name><surname>Barak</surname><given-names>B</given-names></string-name>, <string-name><surname>Sutskever</surname><given-names>I</given-names></string-name>. <year>2019</year>. <article-title>Deep double descent: where bigger models and more data hurt</article-title>. <italic toggle="yes">arXiv</italic>: 1912.02292.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0036">
      <mixed-citation publication-type="journal" id="nph18387-cit-0036"><string-name><surname>Nakkiran</surname><given-names>P</given-names></string-name>, <string-name><surname>Kaplun</surname><given-names>G</given-names></string-name>, <string-name><surname>Bansal</surname><given-names>Y</given-names></string-name>, <string-name><surname>Yang</surname><given-names>T</given-names></string-name>, <string-name><surname>Barak</surname><given-names>B</given-names></string-name>, <string-name><surname>Sutskever</surname><given-names>I</given-names></string-name>. <year>2021</year>. <article-title>Deep double descent: where bigger models and more data hurt</article-title>. <source>Journal of Statistical Mechanics: Theory and Experiment</source><volume>2021</volume>: <fpage>124003</fpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0037">
      <mixed-citation publication-type="journal" id="nph18387-cit-0037"><string-name><surname>Narisetti</surname><given-names>N</given-names></string-name>, <string-name><surname>Henke</surname><given-names>M</given-names></string-name>, <string-name><surname>Seiler</surname><given-names>C</given-names></string-name>, <string-name><surname>Junker</surname><given-names>A</given-names></string-name>, <string-name><surname>Ostermann</surname><given-names>J</given-names></string-name>, <string-name><surname>Altmann</surname><given-names>T</given-names></string-name>, <string-name><surname>Gladilin</surname><given-names>E</given-names></string-name>. <year>2021</year>. <article-title>Fully‐automated root image analysis (faRIA)</article-title>. <source>Scientific Reports</source><volume>11</volume>: <fpage>16047</fpage>.<pub-id pub-id-type="pmid">34362967</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0038">
      <mixed-citation publication-type="journal" id="nph18387-cit-0038"><string-name><surname>Narisetti</surname><given-names>N</given-names></string-name>, <string-name><surname>Henke</surname><given-names>M</given-names></string-name>, <string-name><surname>Seiler</surname><given-names>C</given-names></string-name>, <string-name><surname>Shi</surname><given-names>R</given-names></string-name>, <string-name><surname>Junker</surname><given-names>A</given-names></string-name>, <string-name><surname>Altmann</surname><given-names>T</given-names></string-name>, <string-name><surname>Gladilin</surname><given-names>E</given-names></string-name>. <year>2019</year>. <article-title>Semi‐automated root image analysis (saRIA)</article-title>. <source>Scientific Reports</source><volume>9</volume>: <fpage>1</fpage>–<lpage>10</lpage>.<pub-id pub-id-type="pmid">30626917</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0039">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0039"><string-name><surname>Paszke</surname><given-names>A</given-names></string-name>, <string-name><surname>Gross</surname><given-names>S</given-names></string-name>, <string-name><surname>Chintala</surname><given-names>S</given-names></string-name>, <string-name><surname>Chanan</surname><given-names>G</given-names></string-name>, <string-name><surname>Yang</surname><given-names>E</given-names></string-name>, <string-name><surname>DeVito</surname><given-names>Z</given-names></string-name>, <string-name><surname>Lin</surname><given-names>Z</given-names></string-name>, <string-name><surname>Desmaison</surname><given-names>A</given-names></string-name>, <string-name><surname>Antiga</surname><given-names>L</given-names></string-name>, <string-name><surname>Lerer</surname><given-names>A</given-names></string-name>. <year>2017</year>. <article-title>
Automatic differentiation in P<sc>y</sc>T<sc>orch</sc>
</article-title>. [WWW document] URL <ext-link xlink:href="https://openreview.net/forum?id=BJJsrmfCZ" ext-link-type="uri">https://openreview.net/forum?id=BJJsrmfCZ</ext-link> [accessed 3 August 2022].</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0040">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0040"><string-name><surname>Perez</surname><given-names>F</given-names></string-name>, <string-name><surname>Vasconcelos</surname><given-names>C</given-names></string-name>, <string-name><surname>Avila</surname><given-names>S</given-names></string-name>, <string-name><surname>Valle</surname><given-names>E</given-names></string-name>. <year>2018</year>. <article-title>Data augmentation for skin lesion analysis</article-title>. <italic toggle="yes">arXiv</italic>: 1809.01442.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0041">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0041"><string-name><surname>Pound</surname><given-names>MP</given-names></string-name>, <string-name><surname>Atkinson</surname><given-names>JA</given-names></string-name>, <string-name><surname>Wells</surname><given-names>DM</given-names></string-name>, <string-name><surname>Pridmore</surname><given-names>TP</given-names></string-name>, <string-name><surname>French</surname><given-names>AP</given-names></string-name>. <year>2017</year>. <article-title>Deep learning for multi‐task plant phenotyping</article-title>. <italic toggle="yes">2017 IEEE International Conference on Computer Vision (ICCV)</italic>, <fpage>2055</fpage>–<lpage>2063</lpage>. [WWW document] URL <ext-link xlink:href="https://openaccess.thecvf.com/content_ICCV_2017_workshops/w29/html/Pound_Deep_Learning_for_ICCV_2017_paper.html" ext-link-type="uri">https://openaccess.thecvf.com/content_ICCV_2017_workshops/w29/html/Pound_Deep_Learning_for_ICCV_2017_paper.html</ext-link> [accessed 3 August 2022].</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0042">
      <mixed-citation publication-type="journal" id="nph18387-cit-0042"><string-name><surname>Rasmussen</surname><given-names>CR</given-names></string-name>, <string-name><surname>Thorup‐Kristensen</surname><given-names>K</given-names></string-name>, <string-name><surname>Dresbøll</surname><given-names>DB</given-names></string-name>. <year>2020</year>. <article-title>Uptake of subsoil water below 2 m fails to alleviate drought response in deep‐rooted Chicory (<italic toggle="yes">Cichorium intybus</italic> L.)</article-title>. <source>Plant and Soil</source><volume>446</volume>: <fpage>275</fpage>–<lpage>290</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0043">
      <mixed-citation publication-type="journal" id="nph18387-cit-0043"><string-name><surname>Rasmussen</surname><given-names>IS</given-names></string-name>, <string-name><surname>Dresbøll</surname><given-names>DB</given-names></string-name>, <string-name><surname>Thorup‐Kristensen</surname><given-names>K</given-names></string-name>. <year>2015</year>. <article-title>Winter wheat cultivars and nitrogen (N) fertilization—effects on root growth, N uptake efficiency and N use efficiency</article-title>. <source>European Journal of Agronomy</source><volume>68</volume>: <fpage>38</fpage>–<lpage>49</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0044">
      <mixed-citation publication-type="journal" id="nph18387-cit-0044"><string-name><surname>Rasmussen</surname><given-names>IS</given-names></string-name>, <string-name><surname>Thorup‐Kristensen</surname><given-names>K</given-names></string-name>. <year>2016</year>. <article-title>Does earlier sowing of winter wheat improve root growth and N uptake?</article-title><source>Field Crops Research</source><volume>196</volume>: <fpage>10</fpage>–<lpage>21</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0045">
      <mixed-citation publication-type="journal" id="nph18387-cit-0045"><string-name><surname>Rebolledo</surname><given-names>MC</given-names></string-name>, <string-name><surname>Peña</surname><given-names>AL</given-names></string-name>, <string-name><surname>Duitama</surname><given-names>J</given-names></string-name>, <string-name><surname>Cruz</surname><given-names>DF</given-names></string-name>, <string-name><surname>Dingkuhn</surname><given-names>M</given-names></string-name>, <string-name><surname>Grenier</surname><given-names>C</given-names></string-name>, <string-name><surname>Tohme</surname><given-names>J</given-names></string-name>. <year>2016</year>. <article-title>Combining image analysis, genome wide association studies and different field trials to reveal stable genetic regions related to panicle architecture and the number of spikelets per panicle in rice</article-title>. <source>Frontiers in Plant Science</source><volume>7</volume>: <fpage>1384</fpage>.<pub-id pub-id-type="pmid">27703460</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0046">
      <mixed-citation publication-type="journal" id="nph18387-cit-0046"><string-name><surname>Rewald</surname><given-names>B</given-names></string-name>, <string-name><surname>Meinen</surname><given-names>C</given-names></string-name>, <string-name><surname>Trockenbrodt</surname><given-names>M</given-names></string-name>, <string-name><surname>Ephrath</surname><given-names>JE</given-names></string-name>, <string-name><surname>Rachmilevitch</surname><given-names>S</given-names></string-name>. <year>2012</year>. <article-title>Root taxa identification in plant mixtures ‐current techniques and future challenges</article-title>. <source>Plant and Soil</source><volume>359</volume>: <fpage>165</fpage>–<lpage>182</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0047">
      <mixed-citation publication-type="journal" id="nph18387-cit-0047"><string-name><surname>Ronneberger</surname><given-names>O</given-names></string-name>, <string-name><surname>Fischer</surname><given-names>P</given-names></string-name>, <string-name><surname>Brox</surname><given-names>T</given-names></string-name>. <year>2015</year>. <article-title>U‐Net: convolutional networks for biomedical image segmentation</article-title>. <source>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</source><volume>9351</volume>: <fpage>234</fpage>–<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0048">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0048"><string-name><surname>Sakinis</surname><given-names>T</given-names></string-name>, <string-name><surname>Milletari</surname><given-names>F</given-names></string-name>, <string-name><surname>Roth</surname><given-names>H</given-names></string-name>, <string-name><surname>Korfiatis</surname><given-names>P</given-names></string-name>, <string-name><surname>Kostandy</surname><given-names>P</given-names></string-name>, <string-name><surname>Philbrick</surname><given-names>K</given-names></string-name>, <string-name><surname>Akkus</surname><given-names>Z</given-names></string-name>, <string-name><surname>Xu</surname><given-names>Z</given-names></string-name>, <string-name><surname>Xu</surname><given-names>D</given-names></string-name>, <string-name><surname>Erickson</surname><given-names>BJ</given-names></string-name>. <year>2019</year>. <article-title>Interactive segmentation of medical images through fully convolutional neural networks</article-title>. <italic toggle="yes">arXiv</italic>: 1903.08205v1.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0049">
      <mixed-citation publication-type="book" id="nph18387-cit-0049"><string-name><surname>Santos</surname><given-names>L</given-names></string-name>, <string-name><surname>Santos</surname><given-names>FN</given-names></string-name>, <string-name><surname>Oliveira</surname><given-names>PM</given-names></string-name>, <string-name><surname>Shinde</surname><given-names>P</given-names></string-name>. <year>2020</year>. <part-title>Deep learning applications in agriculture: a short review</part-title>. In: <person-group person-group-type="editor"><string-name><surname>Silva</surname><given-names>MF</given-names></string-name></person-group>, <person-group person-group-type="editor"><string-name><surname>Luís Lima</surname><given-names>J</given-names></string-name></person-group>, <person-group person-group-type="editor"><string-name><surname>Reis</surname><given-names>LP</given-names></string-name></person-group>, <person-group person-group-type="editor"><string-name><surname>Sanfeliu</surname><given-names>A</given-names></string-name></person-group>, <person-group person-group-type="editor"><string-name><surname>Tardioli</surname><given-names>D</given-names></string-name></person-group>, eds. <source>Robot 2019: Fourth Iberian Robotics conference advances in intelligent systems and computing</source>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer International</publisher-name>, <fpage>139</fpage>–<lpage>151</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0050">
      <mixed-citation publication-type="journal" id="nph18387-cit-0050"><string-name><surname>Schindelin</surname><given-names>J</given-names></string-name>, <string-name><surname>Arganda‐Carreras</surname><given-names>I</given-names></string-name>, <string-name><surname>Frise</surname><given-names>E</given-names></string-name>, <string-name><surname>Kaynig</surname><given-names>V</given-names></string-name>, <string-name><surname>Longair</surname><given-names>M</given-names></string-name>, <string-name><surname>Pietzsch</surname><given-names>T</given-names></string-name>, <string-name><surname>Preibisch</surname><given-names>S</given-names></string-name>, <string-name><surname>Rueden</surname><given-names>C</given-names></string-name>, <string-name><surname>Saalfeld</surname><given-names>S</given-names></string-name>, <string-name><surname>Schmid</surname><given-names>B</given-names></string-name><italic toggle="yes">et al</italic>. <year>2012</year>. <article-title>F<sc>iji</sc>: an open‐source platform for biological‐image analysis</article-title>. <source>Nature Methods</source><volume>9</volume>: <fpage>676</fpage>–<lpage>682</lpage>.<pub-id pub-id-type="pmid">22743772</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0051">
      <mixed-citation publication-type="journal" id="nph18387-cit-0051"><string-name><surname>Seethepalli</surname><given-names>A</given-names></string-name>, <string-name><surname>Dhakal</surname><given-names>K</given-names></string-name>, <string-name><surname>Griffiths</surname><given-names>M</given-names></string-name>, <string-name><surname>Guo</surname><given-names>H</given-names></string-name>, <string-name><surname>Freschet</surname><given-names>GT</given-names></string-name>, <string-name><surname>York</surname><given-names>LM</given-names></string-name>. <year>2021</year>. <article-title>R<sc>hizo</sc>V<sc>ision</sc> E<sc>xplorer</sc>: opensource software for root image analysis and measurement standardization</article-title>. <source>AoB PLANTS</source><volume>13</volume>: <elocation-id>plab056</elocation-id>.<pub-id pub-id-type="pmid">34804466</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0052">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0052"><string-name><surname>Sennrich</surname><given-names>R</given-names></string-name>, <string-name><surname>Haddow</surname><given-names>B</given-names></string-name>, <string-name><surname>Birch</surname><given-names>A</given-names></string-name>. <year>2016</year>. <article-title>Edinburgh neural machine translation systems for WMT 16</article-title>. <italic toggle="yes">arXiv</italic>: 1606.02891.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0053">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0053"><string-name><surname>Settles</surname><given-names>B</given-names></string-name>. <year>2009</year>. <article-title>
Active learning literature survey
</article-title>. [WWW document] URL <ext-link xlink:href="https://minds.wisconsin.edu/handle/1793/60660" ext-link-type="uri">https://minds.wisconsin.edu/handle/1793/60660</ext-link> [accessed 3 August 2022].</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0054">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0054"><string-name><surname>Smith</surname><given-names>AG</given-names></string-name>, <string-name><surname>Han</surname><given-names>E</given-names></string-name>, <string-name><surname>Petersen</surname><given-names>J</given-names></string-name>, <string-name><surname>Olsen</surname><given-names>NAF</given-names></string-name>, <string-name><surname>Giese</surname><given-names>C</given-names></string-name>, <string-name><surname>Athmann</surname><given-names>M</given-names></string-name>, <string-name><surname>Dresbøll</surname><given-names>DB</given-names></string-name>, <string-name><surname>Thorup‐Kristensen</surname><given-names>K</given-names></string-name>. <year>2020a</year>. <article-title>Counted nodules dataset used in ‘R<sc>oot</sc>P<sc>ainter</sc>: deep learning segmentation of biological images with corrective annotation’</article-title>. <italic toggle="yes">Zenodo</italic>. doi: 10.5281/zenodo.3753602.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0055">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0055"><string-name><surname>Smith</surname><given-names>AG</given-names></string-name>, <string-name><surname>Han</surname><given-names>E</given-names></string-name>, <string-name><surname>Petersen</surname><given-names>J</given-names></string-name>, <string-name><surname>Olsen</surname><given-names>NAF</given-names></string-name>, <string-name><surname>Giese</surname><given-names>C</given-names></string-name>, <string-name><surname>Athmann</surname><given-names>M</given-names></string-name>, <string-name><surname>Dresbøll</surname><given-names>DB</given-names></string-name>, <string-name><surname>Thorup‐Kristensen</surname><given-names>K</given-names></string-name>. <year>2020b</year>. <article-title>Counted biopores dataset used in ‘R<sc>oot</sc>P<sc>ainter</sc>: deep learning segmentation of biological images with corrective annotation’</article-title>. <italic toggle="yes">Zenodo</italic>. doi: <pub-id pub-id-type="doi">10.5281/zenodo.3753969</pub-id>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0056">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0056"><string-name><surname>Smith</surname><given-names>AG</given-names></string-name>, <string-name><surname>Han</surname><given-names>E</given-names></string-name>, <string-name><surname>Petersen</surname><given-names>J</given-names></string-name>, <string-name><surname>Olsen</surname><given-names>NAF</given-names></string-name>, <string-name><surname>Giese</surname><given-names>C</given-names></string-name>, <string-name><surname>Athmann</surname><given-names>M</given-names></string-name>, <string-name><surname>Dresbøll</surname><given-names>DB</given-names></string-name>, <string-name><surname>Thorup‐Kristensen</surname><given-names>K</given-names></string-name>. <year>2020c</year>. <article-title>Training datasets and final models from paper ‘R<sc>oot</sc>P<sc>ainter</sc>: deep learning segmentation of biological images with corrective annotation’</article-title>. <italic toggle="yes">Zenodo</italic>. doi: <pub-id pub-id-type="doi">10.5281/zenodo.3754046</pub-id>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0057">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0057"><string-name><surname>Smith</surname><given-names>AG</given-names></string-name>, <string-name><surname>Petersen</surname><given-names>J</given-names></string-name>, <string-name><surname>Selvan</surname><given-names>R</given-names></string-name>, <string-name><surname>Rasmussen</surname><given-names>CR</given-names></string-name>. <year>2019a</year>. <article-title>Data for paper ‘Segmentation of roots in soil with U‐Net’</article-title>. <italic toggle="yes">Zenodo</italic>. doi: <pub-id pub-id-type="doi">10.5281/zenodo.3757713</pub-id>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0058">
      <mixed-citation publication-type="journal" id="nph18387-cit-0058"><string-name><surname>Smith</surname><given-names>AG</given-names></string-name>, <string-name><surname>Petersen</surname><given-names>J</given-names></string-name>, <string-name><surname>Selvan</surname><given-names>R</given-names></string-name>, <string-name><surname>Rasmussen</surname><given-names>CR</given-names></string-name>. <year>2020d</year>. <article-title>Segmentation of roots in soil with U‐Net</article-title>. <source>Plant Methods</source><volume>16</volume>: <fpage>13</fpage>.<pub-id pub-id-type="pmid">32055251</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0059">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0059"><string-name><surname>Smith</surname><given-names>AG</given-names></string-name>, <string-name><surname>Petersen</surname><given-names>J</given-names></string-name>, <string-name><surname>Selvan</surname><given-names>R</given-names></string-name>, <string-name><surname>Rasmussen</surname><given-names>CR</given-names></string-name>. <year>2019b</year>. <article-title>Trained U‐Net model for paper ‘Segmentation of roots in soil with U‐Net’</article-title>. <italic toggle="yes">Zenodo</italic>. doi: <pub-id pub-id-type="doi">10.5281/zenodo.3484015</pub-id>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0060">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0060"><string-name><surname>Smith</surname><given-names>LN</given-names></string-name>. <year>2018</year>. <article-title>A disciplined approach to neural network hyper‐parameters: part 1 – learning rate, batch size, momentum, and weight decay</article-title>. <italic toggle="yes">arXiv</italic>: 1803.09820.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0061">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0061"><string-name><surname>Sofroniew</surname><given-names>N</given-names></string-name>, <string-name><surname>Lambert</surname><given-names>T</given-names></string-name>, <string-name><surname>Evans</surname><given-names>K</given-names></string-name>, <string-name><surname>Nunez‐Iglesias</surname><given-names>J</given-names></string-name>, <string-name><surname>Bokota</surname><given-names>G</given-names></string-name>, <string-name><surname>Winston</surname><given-names>P</given-names></string-name>, <string-name><surname>Peña‐Castellanos</surname><given-names>G</given-names></string-name>, <string-name><surname>Yamauchi</surname><given-names>K</given-names></string-name>, <string-name><surname>Bussonnier</surname><given-names>M</given-names></string-name>, <string-name><surname>Doncila Pop</surname><given-names>D</given-names></string-name>, <italic toggle="yes">et al</italic>. <year>2022</year>. <article-title><sc>napari</sc>: a multi‐dimensional image viewer for P<sc>ython</sc></article-title>. <italic toggle="yes">Zenodo</italic>. doi: <pub-id pub-id-type="doi">10.5281/zenodo.6598542</pub-id>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0062">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0062"><string-name><surname>Soltaninejad</surname><given-names>M</given-names></string-name>, <string-name><surname>Sturrock</surname><given-names>CJ</given-names></string-name>, <string-name><surname>Griffiths</surname><given-names>M</given-names></string-name>, <string-name><surname>Pridmore</surname><given-names>TP</given-names></string-name>, <string-name><surname>Pound</surname><given-names>MP</given-names></string-name>. <year>2019</year>. <article-title>Three dimensional root CT segmentation using multi‐resolution encoder‐decoder networks</article-title>. <italic toggle="yes">bioRxiv</italic>: <fpage>713859v1</fpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0063">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0063"><string-name><surname>Soudry</surname><given-names>D</given-names></string-name>, <string-name><surname>Hoffer</surname><given-names>E</given-names></string-name>, <string-name><surname>Nacson</surname><given-names>MS</given-names></string-name>, <string-name><surname>Gunasekar</surname><given-names>S</given-names></string-name>, <string-name><surname>Srebro</surname><given-names>N</given-names></string-name>. <year>2018</year>. <article-title>The implicit bias of gradient descent on separable data</article-title>. <italic toggle="yes">arXiv</italic>: 1710.10345.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0064">
      <mixed-citation publication-type="journal" id="nph18387-cit-0064"><string-name><surname>Svane</surname><given-names>SF</given-names></string-name>, <string-name><surname>Jensen</surname><given-names>CS</given-names></string-name>, <string-name><surname>Thorup‐Kristensen</surname><given-names>K</given-names></string-name>. <year>2019</year>. <article-title>Construction of a large‐scale semi‐field facility to study genotypic differences in deep root growth and resources acquisition</article-title>. <source>Plant Methods</source><volume>15</volume>: <fpage>26</fpage>.<pub-id pub-id-type="pmid">30930953</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0065">
      <mixed-citation publication-type="journal" id="nph18387-cit-0065"><string-name><surname>Taghavi Namin</surname><given-names>S</given-names></string-name>, <string-name><surname>Esmaeilzadeh</surname><given-names>M</given-names></string-name>, <string-name><surname>Najafi</surname><given-names>M</given-names></string-name>, <string-name><surname>Brown</surname><given-names>TB</given-names></string-name>, <string-name><surname>Borevitz</surname><given-names>JO</given-names></string-name>. <year>2018</year>. <article-title>Deep phenotyping: deep learning for temporal phenotype/genotype classification</article-title>. <source>Plant Methods</source><volume>14</volume>: <fpage>66</fpage>.<pub-id pub-id-type="pmid">30087695</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0066">
      <mixed-citation publication-type="journal" id="nph18387-cit-0066"><string-name><surname>Tello</surname><given-names>J</given-names></string-name>, <string-name><surname>Montemayor</surname><given-names>MI</given-names></string-name>, <string-name><surname>Forneck</surname><given-names>A</given-names></string-name>, <string-name><surname>Ibáñez</surname><given-names>J</given-names></string-name>. <year>2018</year>. <article-title>A new image‐based tool for the high throughput phenotyping of pollen viability: evaluation of inter‐ and intra‐cultivar diversity in grapevine</article-title>. <source>Plant Methods</source><volume>14</volume>: <fpage>3</fpage>.<pub-id pub-id-type="pmid">29339970</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0067">
      <mixed-citation publication-type="journal" id="nph18387-cit-0067"><string-name><surname>Thorup‐Kristensen</surname><given-names>K</given-names></string-name>. <year>2001</year>. <article-title>Are differences in root growth of nitrogen catch crops important for their ability to reduce soil nitrate‐N content, and how can this be measured?</article-title><source>Plant and Soil</source><volume>230</volume>: <fpage>185</fpage>–<lpage>195</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0068">
      <mixed-citation publication-type="journal" id="nph18387-cit-0068"><string-name><surname>Thorup‐Kristensen</surname><given-names>K</given-names></string-name>. <year>2006</year>. <article-title>Effect of deep and shallow root systems on the dynamics of soil inorganic N during 3‐year crop rotations</article-title>. <source>Plant and Soil</source><volume>288</volume>: <fpage>233</fpage>–<lpage>248</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0069">
      <mixed-citation publication-type="journal" id="nph18387-cit-0069"><string-name><surname>Thorup‐Kristensen</surname><given-names>K</given-names></string-name>, <string-name><surname>Dresbøll</surname><given-names>DB</given-names></string-name>, <string-name><surname>Kristensen</surname><given-names>HL</given-names></string-name>. <year>2012</year>. <article-title>Crop yield, root growth, and nutrient dynamics in a conventional and three organic cropping systems with different levels of external inputs and N re‐cycling through fertility building crops</article-title>. <source>European Journal of Agronomy</source><volume>37</volume>: <fpage>66</fpage>–<lpage>82</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0070">
      <mixed-citation publication-type="journal" id="nph18387-cit-0070"><string-name><surname>Thorup‐Kristensen</surname><given-names>K</given-names></string-name>, <string-name><surname>Halberg</surname><given-names>N</given-names></string-name>, <string-name><surname>Nicolaisen</surname><given-names>M</given-names></string-name>, <string-name><surname>Olesen</surname><given-names>JE</given-names></string-name>, <string-name><surname>Crews</surname><given-names>TE</given-names></string-name>, <string-name><surname>Hinsinger</surname><given-names>P</given-names></string-name>, <string-name><surname>Kirkegaard</surname><given-names>J</given-names></string-name>, <string-name><surname>Pierret</surname><given-names>A</given-names></string-name>, <string-name><surname>Dresbøll</surname><given-names>DB</given-names></string-name>. <year>2020a</year>. <article-title>Digging deeper for agricultural resources, the value of deep rooting</article-title>. <source>Trends in Plant Science</source><volume>25</volume>: <fpage>406</fpage>–<lpage>417</lpage>.<pub-id pub-id-type="pmid">31964602</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0071">
      <mixed-citation publication-type="journal" id="nph18387-cit-0071"><string-name><surname>Thorup‐Kristensen</surname><given-names>K</given-names></string-name>, <string-name><surname>Halberg</surname><given-names>N</given-names></string-name>, <string-name><surname>Nicolaisen</surname><given-names>MH</given-names></string-name>, <string-name><surname>Olesen</surname><given-names>JE</given-names></string-name>, <string-name><surname>Dresbøll</surname><given-names>DB</given-names></string-name>. <year>2020b</year>. <article-title>Exposing deep roots: a rhizobox laboratory</article-title>. <source>Trends in Plant Science</source><volume>25</volume>: <fpage>418</fpage>–<lpage>419</lpage>.<pub-id pub-id-type="pmid">31974066</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0072">
      <mixed-citation publication-type="journal" id="nph18387-cit-0072"><string-name><surname>Thorup‐Kristensen</surname><given-names>K</given-names></string-name>, <string-name><surname>Kirkegaard</surname><given-names>J</given-names></string-name>. <year>2016</year>. <article-title>Root system‐based limits to agricultural productivity and efficiency: the farming systems context</article-title>. <source>Annals of Botany</source><volume>118</volume>: <fpage>573</fpage>–<lpage>592</lpage>.<pub-id pub-id-type="pmid">27411680</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0073">
      <mixed-citation publication-type="book" id="nph18387-cit-0073"><string-name><surname>Toneva</surname><given-names>M</given-names></string-name>, <string-name><surname>Sordoni</surname><given-names>A</given-names></string-name>. <year>2019</year>. <part-title>An empirical study of example forgetting during deep neural network learning</part-title>. In: <source>International conference on learning representations, New Orleans, Louisiana, United States</source>, <fpage>18</fpage>. [WWW document] URL <ext-link xlink:href="https://openreview.net/forum?id=BJlxm30cKm" ext-link-type="uri">https://openreview.net/forum?id=BJlxm30cKm</ext-link> [accessed 3 August 2022].</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0074">
      <mixed-citation publication-type="journal" id="nph18387-cit-0074"><string-name><surname>Ubbens</surname><given-names>J</given-names></string-name>, <string-name><surname>Cieslak</surname><given-names>M</given-names></string-name>, <string-name><surname>Prusinkiewicz</surname><given-names>P</given-names></string-name>, <string-name><surname>Stavness</surname><given-names>I</given-names></string-name>. <year>2018</year>. <article-title>The use of plant models in deep learning: an application to leaf counting in rosette plants</article-title>. <source>Plant Methods</source><volume>14</volume>: <fpage>1</fpage>–<lpage>10</lpage>.<pub-id pub-id-type="pmid">29321806</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0075">
      <mixed-citation publication-type="journal" id="nph18387-cit-0075"><string-name><surname>Ubbens</surname><given-names>JR</given-names></string-name>, <string-name><surname>Stavness</surname><given-names>I</given-names></string-name>. <year>2017</year>. <article-title>Deep plant phenomics: a deep learning platform for complex plant phenotyping tasks</article-title>. <source>Frontiers in Plant Science</source><volume>8</volume>: <fpage>1190</fpage>.<pub-id pub-id-type="pmid">28736569</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0076">
      <mixed-citation publication-type="book" id="nph18387-cit-0076"><string-name><surname>Vapnik</surname><given-names>VN</given-names></string-name>. <year>2000</year>. <part-title>Bounds on the rate of convergence of learning processes</part-title>. In: <source>The nature of statistical learning theory</source>. <publisher-loc>New York, NY, USA</publisher-loc>: <publisher-name>Springer</publisher-name>, <fpage>69</fpage>–<lpage>91</lpage>.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0077">
      <mixed-citation publication-type="journal" id="nph18387-cit-0077"><string-name><surname>Walt</surname><given-names>S</given-names></string-name>, <string-name><surname>Schönberger</surname><given-names>JL</given-names></string-name>, <string-name><surname>Nunez‐Iglesias</surname><given-names>J</given-names></string-name>, <string-name><surname>Boulogne</surname><given-names>F</given-names></string-name>, <string-name><surname>Warner</surname><given-names>JD</given-names></string-name>, <string-name><surname>Yager</surname><given-names>N</given-names></string-name>, <string-name><surname>Gouillart</surname><given-names>E</given-names></string-name>, <string-name><surname>Yu</surname><given-names>T</given-names></string-name>. <year>2014</year>. <article-title><sc>scikit</sc>‐<sc>image</sc>: image processing in P<sc>ython</sc></article-title>. <source>PeerJ</source><volume>2</volume>: <fpage>e453</fpage>.<pub-id pub-id-type="pmid">25024921</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0078">
      <mixed-citation publication-type="journal" id="nph18387-cit-0078"><string-name><surname>Walter</surname><given-names>J</given-names></string-name>, <string-name><surname>Edwards</surname><given-names>J</given-names></string-name>, <string-name><surname>Cai</surname><given-names>J</given-names></string-name>, <string-name><surname>McDonald</surname><given-names>G</given-names></string-name>, <string-name><surname>Miklavcic</surname><given-names>SJ</given-names></string-name>, <string-name><surname>Kuchel</surname><given-names>H</given-names></string-name>. <year>2019</year>. <article-title>High‐throughput field imaging and basic image analysis in a wheat breeding programme</article-title>. <source>Frontiers in Plant Science</source><volume>10</volume>: <fpage>449</fpage>.<pub-id pub-id-type="pmid">31105715</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0079">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0079"><string-name><surname>Wu</surname><given-names>Y</given-names></string-name>, <string-name><surname>He</surname><given-names>K</given-names></string-name>. <year>2018</year>. <article-title>Group normalization</article-title>. <italic toggle="yes">European Conference on Computer Vision (ECCV) 2018</italic>, <fpage>3</fpage>–<lpage>19</lpage>. [WWW document] URL <ext-link xlink:href="http://openaccess.thecvf.com/content_ECCV_2018/html/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.html" ext-link-type="uri">http://openaccess.thecvf.com/content_ECCV_2018/html/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.html</ext-link> [accessed 3 August 2022].</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0080">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0080"><string-name><surname>Xu</surname><given-names>W</given-names></string-name>, <string-name><surname>Yu</surname><given-names>G</given-names></string-name>, <string-name><surname>Zare</surname><given-names>A</given-names></string-name>, <string-name><surname>Zurweller</surname><given-names>B</given-names></string-name>, <string-name><surname>Rowland</surname><given-names>D</given-names></string-name>, <string-name><surname>Reyes‐Cabrera</surname><given-names>J</given-names></string-name>, <string-name><surname>Fritschi</surname><given-names>FB</given-names></string-name>, <string-name><surname>Matamala</surname><given-names>R</given-names></string-name>, <string-name><surname>Juenger</surname><given-names>TE</given-names></string-name>. <year>2019</year>. <article-title>Overcoming small minirhizotron datasets using transfer learning</article-title>. <italic toggle="yes">arXiv</italic>: 1903.09344.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0081">
      <mixed-citation publication-type="journal" id="nph18387-cit-0081"><string-name><surname>Yasrab</surname><given-names>R</given-names></string-name>, <string-name><surname>Atkinson</surname><given-names>JA</given-names></string-name>, <string-name><surname>Wells</surname><given-names>DM</given-names></string-name>, <string-name><surname>French</surname><given-names>AP</given-names></string-name>, <string-name><surname>Pridmore</surname><given-names>TP</given-names></string-name>, <string-name><surname>Pound</surname><given-names>MP</given-names></string-name>. <year>2019</year>. <article-title>R<sc>oot</sc>N<sc>av</sc> 2.0: deep learning for automatic navigation of complex plant root architectures</article-title>. <source>GigaScience</source><volume>8</volume>: <fpage>giz123</fpage>.<pub-id pub-id-type="pmid">31702012</pub-id></mixed-citation>
    </ref>
    <ref id="nph18387-bib-0082">
      <mixed-citation publication-type="miscellaneous" id="nph18387-cit-0082"><string-name><surname>Zhang</surname><given-names>C</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>S</given-names></string-name>, <string-name><surname>Hardt</surname><given-names>M</given-names></string-name>, <string-name><surname>Recht</surname><given-names>B</given-names></string-name>, <string-name><surname>Vinyals</surname><given-names>O</given-names></string-name>. <year>2017</year>. <article-title>Understanding deep learning requires rethinking generalization</article-title>. <italic toggle="yes">arXiv</italic>: 1611.03530.</mixed-citation>
    </ref>
    <ref id="nph18387-bib-0083">
      <mixed-citation publication-type="journal" id="nph18387-cit-0083"><string-name><surname>Zhang</surname><given-names>Z</given-names></string-name>, <string-name><surname>Liu</surname><given-names>Q</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Y</given-names></string-name>. <year>2018</year>. <article-title>Road extraction by deep residual U‐Net</article-title>. <source>IEEE Geoscience and Remote Sensing Letters</source><volume>15</volume>: <fpage>749</fpage>–<lpage>753</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
