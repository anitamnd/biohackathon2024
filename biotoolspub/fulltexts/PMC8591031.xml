<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neuroinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neuroinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5196</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8591031</article-id>
    <article-id pub-id-type="doi">10.3389/fninf.2021.715131</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Technology and Code</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PymoNNto: A Flexible Modular Toolbox for Designing Brain-Inspired Neural Networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Vieth</surname>
          <given-names>Marius</given-names>
        </name>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1344168/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Stöber</surname>
          <given-names>Tristan M.</given-names>
        </name>
        <uri xlink:href="http://loop.frontiersin.org/people/781459/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Triesch</surname>
          <given-names>Jochen</given-names>
        </name>
        <xref rid="c002" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1023/overview"/>
      </contrib>
    </contrib-group>
    <aff><institution>Frankfurt Institute for Advanced Studies</institution>, <addr-line>Frankfurt am Main</addr-line>, <country>Germany</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Gaute T. Einevoll, Norwegian University of Life Sciences, Norway</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Markus Diesmann, Helmholtz-Verband Deutscher Forschungszentren (HZ), Germany; Sam Neymotin, Nathan Kline Institute for Psychiatric Research, United States</p>
      </fn>
      <corresp id="c001">*Correspondence: Marius Vieth <email>vieth@fias.uni-frankfurt.de</email></corresp>
      <corresp id="c002">Jochen Triesch <email>triesch@fias.uni-frankfurt.de</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>01</day>
      <month>11</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>15</volume>
    <elocation-id>715131</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>5</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>07</day>
        <month>9</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2021 Vieth, Stöber and Triesch.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Vieth, Stöber and Triesch</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>The Python Modular Neural Network Toolbox (PymoNNto) provides a versatile and adaptable Python-based framework to develop and investigate brain-inspired neural networks. In contrast to other commonly used simulators such as Brian2 and NEST, PymoNNto imposes only minimal restrictions for implementation and execution. The basic structure of PymoNNto consists of one network class with several neuron- and synapse-groups. The behaviour of each group can be flexibly defined by exchangeable modules. The implementation of these modules is up to the user and only limited by Python itself. Behaviours can be implemented in Python, Numpy, Tensorflow, and other libraries to perform computations on CPUs and GPUs. PymoNNto comes with convenient high level behaviour modules, allowing differential equation-based implementations similar to Brian2, and an adaptable modular Graphical User Interface for real-time observation and modification of the simulated network and its parameters.</p>
    </abstract>
    <kwd-group>
      <kwd>neural network simulator</kwd>
      <kwd>software toolbox</kwd>
      <kwd>python library</kwd>
      <kwd>graphical user interface (GUI)</kwd>
      <kwd>simulator fusion</kwd>
      <kwd>evolutionary algorithm</kwd>
    </kwd-group>
    <counts>
      <fig-count count="7"/>
      <table-count count="1"/>
      <equation-count count="0"/>
      <ref-count count="48"/>
      <page-count count="16"/>
      <word-count count="8151"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Simulating neural networks has become an indispensable part of brain research, allowing neuroscientists to efficiently develop, explore, and evaluate hypotheses. Working with such models is facilitated by various simulation environments, which typically provide high level classes and functions for convenient model generation, simulation, and analysis.</p>
    <p>Each simulation environment has particular strengths and limitations. Neural network models can be formulated at different levels of detail/abstraction. Reflecting the various scales of investigation, several simulation environments exist, each with its own focus area (for review see Brette et al., <xref rid="B5" ref-type="bibr">2007</xref>; Brette and Goodman, <xref rid="B4" ref-type="bibr">2012</xref>; Tikidji-Hamburyan et al., <xref rid="B42" ref-type="bibr">2017</xref>). While for example, <italic toggle="yes">Neuron</italic> (Hines and Carnevale, <xref rid="B19" ref-type="bibr">1997</xref>) excels at simulating neurons with a high degree of biological detail, <italic toggle="yes">NEST</italic> (Fardet et al., <xref rid="B13" ref-type="bibr">2020</xref>) is optimized to simulate large networks of rather simplified spiking neurons on distributed computing clusters (Jordan et al., <xref rid="B24" ref-type="bibr">2018</xref>). Another simulator, <italic toggle="yes">Brian</italic>/<italic toggle="yes">Brian2</italic> (Goodman and Brette, <xref rid="B15" ref-type="bibr">2009</xref>; Stimberg et al., <xref rid="B38" ref-type="bibr">2019</xref>) prioritizes concise model definition over scaling to large computing environments.</p>
    <p>Typically, the convenience provided by a particular neural network simulation toolbox comes at the price of reduced flexibility. This can cause problems when researchers need to leave the “comfort zone” of a particular simulator. For example, when aiming to explore a novel plasticity rule, investigators may be confronted with a difficult choice: They either have to work their way around the constraints of the simulator or write their own simulation environment from scratch. While implementing a workaround may turn out to be arduous and complicated, writing a simulation environment from scratch is time consuming, error prone, hampering reproducibility, and sacrificing useful features of mature simulation environments (Pauli et al., <xref rid="B32" ref-type="bibr">2018</xref>).</p>
    <p>The scientific community has become increasingly aware of this dilemma. Several developments aim to increase the flexibility of existing simulators. For example, NEST has been extended with its own modeling language to allow for custom model definition without having to write C++ modules (Plotnikov et al., <xref rid="B34" ref-type="bibr">2016</xref>). Brian2 simulations, limited to a single core, can be accelerated by executing them on GPUs (Stimberg et al., <xref rid="B39" ref-type="bibr">2020</xref>) via automated code translation to GeNN (Yavuz et al., <xref rid="B47" ref-type="bibr">2016</xref>). However, in all cases, specific simulator-inherent restrictions remain.</p>
    <p>An alternative strategy to achieve both flexibility and reproducibility is to detach model definition from its execution. Simulator-independent model description interfaces, such as PyNN (Davison et al., <xref rid="B9" ref-type="bibr">2009</xref>) or general model description languages, such as NeuroML (Gleeson et al., <xref rid="B14" ref-type="bibr">2010</xref>), allow to first specify a model using a fixed set of vocabulary and syntax. In a second step, model definition is automatically translated to a selected simulation environment. In either approach flexibility remains bounded: The ability to express new mechanisms is limited by a finite number of language elements and the restrictions of the available simulation environments.</p>
    <p>To address the dilemma between flexibility and convenience with a novel approach, we designed PymoNNto as a modular low level Python (Van Rossum and Drake, <xref rid="B43" ref-type="bibr">1995</xref>) framework with minimal restrictions, while at the same time providing several high level modules for convenient analysis and interaction (see <xref rid="F1" ref-type="fig">Figure 1</xref> for an overview of PymoNNto's key features and core structure). Its lightweight structure comes with a number of advantages: (1) Dynamics of neurons and synapses can be freely designed by custom behaviour modules. (2) The content of behaviour modules is only limited by the expressive power of Python. (3) These modules can be optimized for speed, for example via Tensorflow (Abadi et al., <xref rid="B1" ref-type="bibr">2016</xref>) or Cython (Behnel et al., <xref rid="B2" ref-type="bibr">2010</xref>), and can even wrap around and combine established simulators, facilitating multi-scale approaches. Without sacrificing flexibility, PymoNNto allows for efficient implementation and analysis via a multitude of features, such as a powerful and extendable graphical user interface, a storage manager, and several pre-implemented neuronal/synaptic mechanisms and network models (compare <xref rid="T1" ref-type="table">Table 1</xref>).</p>
    <fig position="float" id="F1">
      <label>Figure 1</label>
      <caption>
        <p>Key features of PymoNNto <bold>(left)</bold> and its core structure <bold>(right)</bold>. The core structure consists of a <italic toggle="yes">Network</italic> class, <italic toggle="yes">NeuronGroups, SynapseGroups</italic>, and <italic toggle="yes">Behaviour</italic> modules. One <italic toggle="yes">Network</italic> can have many <italic toggle="yes">NeuronGroups</italic> and <italic toggle="yes">SynapseGroups</italic>. One or more <italic toggle="yes">Behaviour</italic> modules with custom code are attached to all three classes.</p>
      </caption>
      <graphic xlink:href="fninf-15-715131-g0001" position="float"/>
    </fig>
    <table-wrap position="float" id="T1">
      <label>Table 1</label>
      <caption>
        <p>Pre-implemented neuronal mechanisms and network models.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th valign="top" align="left" rowspan="1" colspan="1">
              <bold>Neuronal/synaptic mechanisms</bold>
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Spike-timing-dependent plasticity (STDP) (Lazar et al., <xref rid="B25" ref-type="bibr">2009</xref>)</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Synaptic weight normalization (Lazar et al., <xref rid="B25" ref-type="bibr">2009</xref>)</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Intrinsic plasticity (IP) (modified from Lazar et al., <xref rid="B25" ref-type="bibr">2009</xref>)</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Refractory period</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">NOX diffusion-based homeostasis (Sweeney et al., <xref rid="B40" ref-type="bibr">2015</xref>)</td>
          </tr>
          <tr style="border-top: thin solid #000000;">
            <td valign="top" align="left" rowspan="1" colspan="1">
              <bold>Network/Neuron Models</bold>
            </td>
          </tr>
          <tr style="border-top: thin solid #000000;">
            <td valign="top" align="left" rowspan="1" colspan="1">Hodgkin and Huxley (<xref rid="B20" ref-type="bibr">1952</xref>)</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Hopfield (<xref rid="B21" ref-type="bibr">1982</xref>)</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Hindmarsh and Rose (<xref rid="B18" ref-type="bibr">1984</xref>)</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Wang and Buzsáki (<xref rid="B45" ref-type="bibr">1996</xref>)</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Brunel and Hakim (<xref rid="B6" ref-type="bibr">1999</xref>)</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Diesmann et al. (<xref rid="B10" ref-type="bibr">1999</xref>)</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Izhikevich (<xref rid="B23" ref-type="bibr">2003</xref>)</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Brunel and Hakim (<xref rid="B6" ref-type="bibr">1999</xref>)</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
  <sec id="s2">
    <title>2. Architecture and Functionality</title>
    <p>To streamline the network development workflow, the core of PymoNNto forms a scaffold in which the user can embed his own code. In short, this scaffold consists of a network containing neurons and synapses. Interactions between these elements are defined by behaviour modules. The main purpose of this scaffold is to add structure to the model, to simplify the development process through communication functions and to make the development of additional tools more convenient.</p>
    <p>PymoNNto's architecture aims to represent neural circuits by reusable building blocks in an object-oriented fashion. The dynamics of each building block are described by a behaviour module—representing for example a specific synaptic receptor class. PymoNNto's modular design allows for efficient addition or removal of such building blocks, and thus facilitates the development and investigation of complex neural networks.</p>
    <sec>
      <title>2.1. Core Classes</title>
      <p>The low level core of PymoNNto consists of four main classes derived from the same <italic toggle="yes">NetworkObjectBase</italic> class. <xref rid="F2" ref-type="fig">Figure 2</xref> shows a detailed UML diagram explaining the inheritance relationships among the different classes. It also shows an example execution pipeline, where the behaviours have been sorted by their “keys” specifying the order of execution.</p>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p>A UML Diagram of PymoNNto's core with its most relevant variables and functions on the left as well as a visualization of an example execution pipeline composed of sorted behaviours (boxes) on the right. The white arrows indicate inheritance relationships and the black diamonds indicate compositions. The numbers in the execution pipeline represent the keys of corresponding behaviours in the dictionary of their parent object. The colors of these behaviours correspond to the colors of their parents on the left. The orange arrows connect both visualizations and indicate how the functions interact with the execution pipeline. npa, NumPy array.</p>
        </caption>
        <graphic xlink:href="fninf-15-715131-g0002" position="float"/>
      </fig>
      <p>
        <bold>NeuronGroup</bold>
      </p>
      <list list-type="simple">
        <list-item>
          <p><italic toggle="yes">NeuronGroup</italic> objects represent populations of neurons. PymoNNto neurons have no neuron-like behaviour, logic, or data by default. They can be seen as empty shells, which can be filled with custom code modules. A <italic toggle="yes">NeuronGroup</italic> object contains a list of behaviour modules which define what the neurons are doing, what variables they have, and how they communicate with other <italic toggle="yes">NeuronGroups</italic>. Further, <italic toggle="yes">NeuronGroup</italic> objects are equipped with functions to efficiently access afferent and efferent synapses, to initialize vectors for data storage, and to partition the group into subgroups.</p>
        </list-item>
      </list>
      <p>
        <bold>SynapseGroup</bold>
      </p>
      <list list-type="simple">
        <list-item>
          <p><italic toggle="yes">SynapseGroups</italic> are used to connect source and target <italic toggle="yes">NeuronGroups</italic>. As in <italic toggle="yes">NeuronGroups, SynapseGroups</italic> can be freely defined by their own behaviour modules.</p>
        </list-item>
        <list-item>
          <p>In contrast to <italic toggle="yes">NeuronGroups</italic>, which contain functions to initialize activity vectors, <italic toggle="yes">SynapseGroups</italic> contain helper functions to initialize synaptic weight matrices with specific connection densities and receptive fields.</p>
        </list-item>
      </list>
      <p>
        <bold>Network</bold>
      </p>
      <list list-type="simple">
        <list-item>
          <p>The <italic toggle="yes">Network</italic> object is the main object and contains all <italic toggle="yes">Neuron-</italic> and <italic toggle="yes">SynapseGroups</italic> of the simulation, as well as some optional global behaviour modules. It provides mechanisms for communication between the groups, functions to control the simulation and manages the order of execution of the custom code blocks.</p>
        </list-item>
      </list>
      <p>
        <bold>Behaviour</bold>
      </p>
      <list list-type="simple">
        <list-item>
          <p><italic toggle="yes">Behaviour</italic> modules are the core of the simulation and contain custom code. A <italic toggle="yes">Behaviour</italic> module is divided into an initialization- and an update-function called at every time step. Module-specific variables and functions can be stored inside, while shared functionality should be stored in the parent object.</p>
        </list-item>
        <list-item>
          <p>The <italic toggle="yes">Behaviour</italic> modules can be initialized in a very compact way with different helper functions to define their attributes. This allows to describe the full network and associated parameters in one file. Behaviour modules can be associated to the <italic toggle="yes">Network</italic> object, <italic toggle="yes">NeuronGroups</italic>, or <italic toggle="yes">SynapseGroups</italic>. However, in most cases, the <italic toggle="yes">NeuronGroup</italic> objects are the preferred objects to which <italic toggle="yes">Behaviour</italic> modules are assigned. This facilitates operations across different <italic toggle="yes">SynapseGroups</italic>, such as a synaptic normalization mechanism that scales the sum of all excitatory synapses onto a neuron to a specific value. Because <italic toggle="yes">Behaviour</italic> modules are classes, they can benefit from all the advantages of object oriented programming, such as inheritance.</p>
        </list-item>
      </list>
    </sec>
    <sec>
      <title>2.2. Internal Processing</title>
      <p>The internal workings of PymoNNto's core are simple. When <italic toggle="yes">Behaviour</italic> modules are assigned to different objects (compare Code block 2), each of these modules receives an individual number which determines the order of execution. These behaviour numbers are sorted across all objects of the network during initialization. The main loop repeatedly executes all the behaviours in the determined order (see <xref rid="F2" ref-type="fig">Figure 2</xref>), which only needs one dictionary access per behaviour.</p>
    </sec>
    <sec>
      <title>2.3. Additional High Level Functions</title>
      <p>In addition to the four core objects, PymoNNto contains a multitude of optional high level helper functions and tools to streamline network design and investigation. Here, we briefly summarize the most useful ones (see online documentation for more details):</p>
      <p>
        <bold>Graphical User Interface (GUI)</bold>
      </p>
      <list list-type="simple">
        <list-item>
          <p>PymoNNto's GUI is a powerful tool to interactively explore the behaviour of a network simulation. Parameters can be modified and statistics displayed in real time. For example, as parameters are varied or plasticity mechanisms switched on or off, the GUI allows to monitor ongoing network activity, the presence of activity oscillations, or emerging changes to the network connectivity (see <xref rid="F3" ref-type="fig">Figure 3</xref>). The GUI is organized into modular and customizable tabs. It is based on PyQt5 (Riverbank Computing, <xref rid="B35" ref-type="bibr">2020</xref>) and uses additional PyQtGraph (Campagnola, <xref rid="B7" ref-type="bibr">2020</xref>) elements for plotting.</p>
        </list-item>
      </list>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p>Design elements of PymoNNto's graphical user interface. The GUI layout is structured as follows: The left bar contains an activity monitor and control elements. The activity monitor displays one or several neuron groups in real time. Each neuron group can receive a distinctive base color. In addition, one can select one or several overlay colors to display ongoing activity, reflecting for example the current voltage or spikes. The activity monitor allows to select individual neurons (green) for further analysis. Control elements allow for example to start, pause, save, and load the simulation and can contain additional tab-specific elements. PymoNNto's GUI contains a large variety of tabs which can be used to analyze and monitor network properties. Three exemplary tabs are shown on the right: Spectral analysis of membrane potentials <bold>(top)</bold>, histograms of synaptic weight distributions <bold>(center)</bold>, and a three-dimensional animation of network activity with excitatory neurons in blue, inhibitory neurons in red <bold>(bottom)</bold>, recently active neurons in white, and a selected neuron in green. For a real GUI example, see <bold>Figure 6</bold>.</p>
        </caption>
        <graphic xlink:href="fninf-15-715131-g0003" position="float"/>
      </fig>
      <p>
        <bold>Tagging system</bold>
      </p>
      <list list-type="simple">
        <list-item>
          <p>To simultaneously access similar variables in multiple objects, the <italic toggle="yes">NetworkObjectBase</italic> class contains a tagging system. It can be used to find objects with the same tag, such as all <italic toggle="yes">SynapseGroups</italic> tagged with <italic toggle="yes">Glutamate</italic> receptors. The tagging systems helps to write simple, compact code by giving the programmer easy access to all tagged objects within an instance of a class. To use the tagging system the <italic toggle="yes">MyObject[“tag”]</italic> operator can be used. This removes the need to create variables for all kinds of objects and pass them to functions via multiple arguments. The only object that has to be passed is the root object, typically the network, and everything else can be accessed via the respective tag.</p>
        </list-item>
      </list>
      <p>
        <bold>Recorder</bold>
      </p>
      <list list-type="simple">
        <list-item>
          <p>The <italic toggle="yes">Recorder</italic> module records some custom variable of a <italic toggle="yes">NeuronGoup</italic> at a given interval. This allows PymoNNto to store activity traces for plotting and further analysis. The <italic toggle="yes">Recorder</italic> can not only record variables, but also results of custom functions. One can, for example, use the string “<italic toggle="yes">n.activity”</italic> to record the neurons' activity, but it is also possible to use “<italic toggle="yes">f(n.activity,…)”</italic>, where f can be the mean or the sum of the activity vector, for example. This is possible, because the string is compiled into code at runtime. Another useful feature is that this recording string can also be used as a tag for the previously described tagging system. For example, after adding a recorder with “<italic toggle="yes">n.activity,”</italic> calling <italic toggle="yes">MyNetwork[“n.activity”]</italic> will return a list of all recorded activity traces.</p>
        </list-item>
      </list>
      <p>
        <bold>Storage manager</bold>
      </p>
      <list list-type="simple">
        <list-item>
          <p>To store recording data, parameters, results and variables, a <italic toggle="yes">Storage-Manager</italic> is included in PymoNNto. It searches for a “Data” folder in the project directory and can create a directory with a custom name for a group of simulation runs. At every run, it creates a separate sub-folder to save and load vectors, matrices, images, videos, and parameters. Furthermore, the <italic toggle="yes">Storage-Manager</italic> allows to sort, compare, and analyse multiple runs with respect to different parameters of interest.</p>
        </list-item>
      </list>
      <p>
        <bold>Partitioning</bold>
      </p>
      <list list-type="simple">
        <list-item>
          <p>The partitioning function is helpful when designing locally-connected networks. When the implemented model is based on vector and matrix operations, the <italic toggle="yes">NeuronGroups</italic> can be divided into <italic toggle="yes">SubNeuronGroups</italic> with a mask. Such a <italic toggle="yes">SubNeuronGroup</italic> allows partial access to variables of the original <italic toggle="yes">NeuronGroup</italic>. The use of <italic toggle="yes">SubNeuronGroups</italic> can avoid slow computations due to large connection matrices by splitting one big sparse <italic toggle="yes">SynapseGroup</italic> into many smaller and denser ones.</p>
        </list-item>
        <list-item>
          <p>When adding the partitioning behaviour module to a <italic toggle="yes">SynapseGroup</italic> it will automatically detect the pre- and the post-synaptic <italic toggle="yes">NeuronGroup</italic> as well as the maximal distance in which a neuron can make connections. This information is then used to replace the big <italic toggle="yes">SynapseGroup</italic> with multiple smaller ones that are attached to <italic toggle="yes">SubNeuronGroups</italic>. With this, we can conveniently combine fast processing with small computational overhead and avoid the quadratic growth of synaptic weight matrices for increasing numbers of neurons when using networks of locally connected neurons.</p>
        </list-item>
      </list>
      <p>
        <bold>Evolution</bold>
      </p>
      <list list-type="simple">
        <list-item>
          <p>PymoNNto's <italic toggle="yes">Evolution</italic> package follows generic evolutionary principles to optimize parameters (Eiben et al., <xref rid="B12" ref-type="bibr">2003</xref>; Vikhar, <xref rid="B44" ref-type="bibr">2016</xref>): Multiple networks are initiated as <italic toggle="yes">individuals</italic>, differing in selected parameters, so called <italic toggle="yes">genes</italic>. In each round of simulation, the fitness of each individual is evaluated by a scoring function, a fraction of individuals with the best score survive and new individuals are generated with mutated parent genes. To use the <italic toggle="yes">Evolution</italic> package, the user may insert the two functions get_gene(key, default) and set_score(score) as interfaces to receive new parameters and to set the fitness in a given simulation file. During the optimization process, this file is executed multiple times, either on different cores or machines (accessed via ssh). This process can be controlled either by a master file or by the <italic toggle="yes">Evolution</italic> package's own graphical user interface. For more details and a code example, we refer the reader to the PymoNNto's online documentation. Note, the general design of the <italic toggle="yes">Evolution</italic> package allow its use beyond the context of network simulations.</p>
        </list-item>
      </list>
    </sec>
  </sec>
  <sec id="s3">
    <title>3. How To Use PymoNNto?</title>
    <p>PymoNNto is based on Python3 and can be installed with the pip package installer with the command: “<italic toggle="yes">pip install pymonnto”</italic> We also refer the reader to the online documentation and the GitHub repository for more detailed examples and descriptions.</p>
    <p>In the following, we demonstrate how to implement a minimal network with PymoNNto. The network consists of a group of simplified leaky-integrate and fire (LIF) neurons, communicating via excitatory synapses. To keep things simple, the resting and reset voltages of the simplified LIF neurons are defined to be zero. Membrane potential updates are calculated by numerically solving the differential equations with the Euler method for a fixed number of iterations. All code blocks in the section are compatible with each other. The relations between modules defined in Code blocks 1–3 are visualized in a flowchart, automatically generated via the function <italic toggle="yes">My_Neurons.visualize_module()</italic> (see <xref rid="F4" ref-type="fig">Figure 4</xref>).</p>
    <fig position="float" id="F4">
      <label>Figure 4</label>
      <caption>
        <p>Automatically generated flow chart of the PymoNNto model defined in Code blocks 1–3. Interactions between <italic toggle="yes">Basic_Behaviour</italic> and <italic toggle="yes">Input_Behaviour</italic> as well as the respective recorders are shown. Positions of behaviour modules reflect the order of internal execution from left to right.</p>
      </caption>
      <graphic xlink:href="fninf-15-715131-g0004" position="float"/>
    </fig>
    <sec>
      <title>3.1. Basic Structure</title>
      <p>The core of a PymoNNto simulation consists of three steps: (a) defining network, neurons, and synapses, (b) initializing, and (c) simulating them (see Code block 1). Both the <italic toggle="yes">NeuronGroup</italic> and the <italic toggle="yes">SynapseGroup</italic> receive as input the parent network and a name tag. Further, the <italic toggle="yes">NeuronGroup</italic> requires a size argument and the <italic toggle="yes">SynapseGroup</italic> its source and destination.</p>
      <p>
        <inline-graphic xlink:href="fninf-15-715131-i0001.jpg"/>
      </p>
    </sec>
    <sec>
      <title>3.2. Behaviour</title>
      <p><italic toggle="yes">Behaviour</italic> modules allow to define custom dynamics of neurons and synapses. Each <italic toggle="yes">Behaviour</italic> module typically consists of two functions: <italic toggle="yes">set_variables</italic> is called when the Network is initialized and <italic toggle="yes">new_iteration</italic> is called every time step. Both functions receive an additional attribute, in code block 2 it is named <italic toggle="yes">neurons</italic>, which points to the group the behaviour belongs to, in this case a <italic toggle="yes">NeuronGroup</italic>. This attribute allows to use parent group specific functions and to define and modify its variables. In this example, we initialize the <italic toggle="yes">NeuronGroup</italic> variable <italic toggle="yes">voltage</italic> with zero values via the <italic toggle="yes">get_neuron_vec</italic> function. At every timestep, we add random membrane noise to these voltages with <italic toggle="yes">get_neuron_vec(“uniform,”…)</italic>. Further, we define a local variable <italic toggle="yes">threshold</italic>, defining the voltage above which the neuron will create a spike before being reset, as well as the variable <italic toggle="yes">leak_factor</italic> for the <italic toggle="yes">voltage</italic> reduction at each iteration. Here, it is not relevant whether variables are stored in the neuron- or the behaviour-object. Though, in more complex simulations it can be advantageous to store variables only used by the behaviour in the <italic toggle="yes">Behaviour</italic> object and other variables in the parent object.</p>
      <p>
        <inline-graphic xlink:href="fninf-15-715131-i0002.jpg"/>
      </p>
      <p>We add the <italic toggle="yes">Basic_Behaviour</italic> to the <italic toggle="yes">NeuronGroup</italic> together with a pre-defined <italic toggle="yes">Recorder</italic> behaviour, to store the <italic toggle="yes">voltage</italic> variable over time. Behaviours are added to a <italic toggle="yes">NeuronGroup</italic> or <italic toggle="yes">SynapseGroup</italic> as a dictionary. The key in front of each <italic toggle="yes">behaviour</italic> has to be a positive number and determines the order of execution across the whole simulation. Note, that correct ordering is important. At the initialization step all behaviours across all neuron or synapse groups are ordered. In case behaviours have the same index, the order is randomly set (see <xref rid="F2" ref-type="fig">Figure 2</xref>). Here, we chose a higher number for the recorder, to store values at the end of each iteration.</p>
    </sec>
    <sec>
      <title>3.3. Synapses and Input</title>
      <p>Next, to couple the neurons via synapses, we add an additional <italic toggle="yes">Behaviour</italic> module, <italic toggle="yes">Input_Behaviour</italic> (see Code block 3). This module collects input at afferent synapses and updates the vector of neurons' voltages accordingly. In <italic toggle="yes">set_variables</italic> the synapse matrix W is created, which stores one weight-value for each connection. <italic toggle="yes">W</italic> has the dimension of <italic toggle="yes">D</italic> × <italic toggle="yes">S</italic>, where <italic toggle="yes">D</italic> is the size of the destination <italic toggle="yes">NeuronGroup</italic> and <italic toggle="yes">S</italic> is the size of the source <italic toggle="yes">NeuronGroup</italic> group. Such synapse matrices can be conveniently created with the function <italic toggle="yes">get_synapse_mat</italic> with equal or random values. The function <italic toggle="yes">new_iteration</italic> defines how the information is propagated through the synapses (dot product). Here, the for-loops are not necessary, because we only have one <italic toggle="yes">SynapseGroup</italic>. However, they would be required for multiple <italic toggle="yes">Neuron</italic>- and <italic toggle="yes">SynapseGroups</italic>. With <italic toggle="yes">synapse.src</italic> and <italic toggle="yes">synapse.dst</italic> you can access the source and destination <italic toggle="yes">NeuronGroups</italic> assigned to a <italic toggle="yes">SynapseGroup</italic>.</p>
      <p>In this example, the membrane voltage is mainly driven by random input, which avoids network instability due to runaway excitation. Mechanisms for stabilizing network activity, like a refractory period, intrinsic plasticity, or interneurons can be added with further modules and neuron groups.</p>
      <p>
        <inline-graphic xlink:href="fninf-15-715131-i0003.jpg"/>
      </p>
    </sec>
    <sec>
      <title>3.4. Tagging System and Plotting</title>
      <p>PymoNNto's tagging system makes access to the <italic toggle="yes">NeuronGroups, SynapseGroups, Behaviours</italic>, and recorded variables inside the network more convenient. To access the tagged objects we can use the <italic toggle="yes">[]</italic> operator. <italic toggle="yes">['my_tag']</italic> returns a list of all objects tagged with <italic toggle="yes">my_tag</italic>. It basically searches the whole tree structure defined by the object and its children recursively. Because of an internal caching mechanism, the search is only performed once. After the first search, the execution is as fast as a dictionary access when the same tag is requested repeatedly. Therefore it can also be used in <italic toggle="yes">Behaviour</italic> modules where speed is critical.</p>
      <p>In the following Code block 4 we see an example of how the tagging system can be used to plot data. Here we access the variables stored in the recorder from the previous example after the simulation. An example output of this code is shown in <xref rid="F5" ref-type="fig">Figure 5</xref>. Internally, the recording strings <italic toggle="yes">"n.voltage"</italic> and <italic toggle="yes">"np.mean(n.voltage)"</italic> are converted into Python code and executed at every time step during recording. These strings also act as tags for the tagging system to access the recorded data series. In the code block, we also show some general examples and their output to illustrate how the tagging system can be used.</p>
      <fig position="float" id="F5">
        <label>Figure 5</label>
        <caption>
          <p>The output of the plotting code block 4 (with additional axis labels), where the neurons receive random input from the <italic toggle="yes">Basic_Behaviour</italic> and additional input from other neurons through the <italic toggle="yes">Input_Behaviour</italic>. <bold>(Left)</bold> The individual voltage traces of the first 10 neurons are plotted with different colors, the mean voltage in black and the (constant) firing threshold with dashed lines. <bold>(Right)</bold> Raster plot showing spikes (black dots) of all neurons during the same period.</p>
        </caption>
        <graphic xlink:href="fninf-15-715131-g0005" position="float"/>
      </fig>
      <p>
        <inline-graphic xlink:href="fninf-15-715131-i0004.jpg"/>
      </p>
    </sec>
    <sec>
      <title>3.5. Diversification and Initialization</title>
      <p>The <italic toggle="yes">Basic_Behaviour</italic> code example can also be extended with another useful feature of PymoNNto. <italic toggle="yes">Behaviour</italic> modules provide additional functions for compact behaviour initialization. Because it can be useful to access the parent object during the initialization, <italic toggle="yes">Behaviour</italic> modules do not use the typical Python class constructor <italic toggle="yes">init</italic>. The problem with the default constructor is that the parent neuron group is not yet created when the behaviour is constructed. To solve this, variables are initialized in <italic toggle="yes">set_variables</italic>, which is called at the end of the network description. Here, the <italic toggle="yes">get_init_attr</italic> function allows to access the original initialization attributes. Further, the <italic toggle="yes">get_init_attr</italic> function adds functionality for neuron diversification. In the code example <italic toggle="yes">leak_factor</italic> is a number. However, when we, for example, change the initialization to <italic toggle="yes">Basic_Behaviour[leak_factor='normal(0.9,0.1);plot']</italic>, the variable becomes a vector with different values for each neuron, without changing the rest of the code. In this example we use a normal distribution, which can be displayed in a histogram with the optional ";plot" string at the end. We can use all distributions in the numpy.random package, like lognormal, uniform, or poisson, as well as custom functions.</p>
    </sec>
    <sec>
      <title>3.6. Graphical User Interface</title>
      <p>To control and evaluate our model with PymoNNto's interactive graphical user interface we can replace the <italic toggle="yes">pyplot</italic> functions (Hunter, <xref rid="B22" ref-type="bibr">2007</xref>), the <italic toggle="yes">recorder</italic> and the <italic toggle="yes">simulate_iterations</italic> with code to launch the <italic toggle="yes">Network_UI</italic> (Code block 5 and <xref rid="F6" ref-type="fig">Figure 6</xref>). Like other parts of PymoNNto, the <italic toggle="yes">Network_UI</italic> is modular. It consists of multiple <italic toggle="yes">UI_modules</italic>, which can be freely chosen. Here, we use the function <italic toggle="yes">get_default_UI_modules</italic> to get a list of standard modules applicable to most networks. To correctly render the output, some <italic toggle="yes">UI_modules</italic> require additional specifications or adjustment of the code. In this example, the <italic toggle="yes">sidebar_activity_module</italic> displays the activity of the neurons on a grid and allows to select individual neurons (blue rectangle, <xref rid="F5" ref-type="fig">Figure 5</xref>). The size is specified via a <italic toggle="yes">NeuronDimension</italic> behaviour, which receives the width, height and depth of the grid and creates spatial coordinates for each neuron stored in the vectors <italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>, and <italic toggle="yes">z</italic>.</p>
      <fig position="float" id="F6">
        <label>Figure 6</label>
        <caption>
          <p>An image of the graphical user interface executed with the code from the User Interface section. The neuron grid on the left displays the activity of each neuron by increasing levels of brightness. The control panel below includes controls to start, pause, save, and load the simulation. The <italic toggle="yes">Multi Group</italic> tab that has been selected on the right displays the mean activity (blue trace) of the whole neuron group as well as the activity (green trace) of a selected neuron (green pixel in neuron grid) across time. The other (non-selected) tabs listed at the top provide additional forms of live visualizations when selected.</p>
        </caption>
        <graphic xlink:href="fninf-15-715131-g0006" position="float"/>
      </fig>
      <p>
        <inline-graphic xlink:href="fninf-15-715131-i0005.jpg"/>
      </p>
    </sec>
  </sec>
  <sec id="s4">
    <title>4. Flexibility</title>
    <p>So far, the presented examples relied on NumPy (Harris et al., <xref rid="B16" ref-type="bibr">2020</xref>) routines for data storage and computation. However, the minimal design of PymoNNto allows to freely define and optimize data types and computations to any specific problem. Any Python-based data representation or computation library can be employed, such as PyTorch matrices or SciPy sparse matrices.</p>
    <sec>
      <title>4.1. Increase of Simulation Speed With Tensorflow</title>
      <p>To demonstrate PymoNNto's versatility, we re-implement the examples of section 3 with Tensorflow 2 (see Code block 6). Commonly used for deep learning, Tensorflow efficiently operates with tensor graphs, which are multidimensional arrays, connected by mathematical operations. These operations are not restricted to deep learning approaches and rather resemble NumPy's functionality, with only few exceptions.</p>
      <p>The use of Tensorflow can substantially increase simulation speed for large networks (Mohanta and Assisi, <xref rid="B29" ref-type="bibr">2019</xref>). Tensorflow is highly optimized and natively runs on CPUs, GPUs or even specialized Tensor Processing Units.</p>
      <p>To compare the performance, we simulated the neural network, defined as NumPy version in section 3, and its Tensorflow counterpart with different sizes (~10<sup>2</sup>−10<sup>4</sup> neurons in steps of 100 * 1.2s; 1,000 iterations; computed on a Dell XPS 15 with i7-8750H CPU and Nvidia-GeForce-GTX-1050-Ti GPU). We find that Tensorflow is slower compared to NumPy for small networks (below around 2,000 neurons), likely due to its larger computational overhead. However, for larger networks, Tensorflow is consistently faster on both, the CPU and GPU (see <xref rid="F7" ref-type="fig">Figure 7</xref>). Note, the speed of the Tensorflow network may be further optimized. Especially, the creation and conversion of a new random vector at every time step is not optimal, but it makes the comparison to the NumPy implementation easier.</p>
      <fig position="float" id="F7">
        <label>Figure 7</label>
        <caption>
          <p>Comparison of processing time (y axis, log scale, seconds) for the described network comprised of different numbers of neurons (x axis, log scale, number of neurons) implemented either with NumPy (blue) or Tensorflow (orange and red) modules. Plotted are the means over 10 runs and their standard deviations.</p>
        </caption>
        <graphic xlink:href="fninf-15-715131-g0007" position="float"/>
      </fig>
      <p>The mixing of NumPy and Tensorflow modules is also possible but requires conversions with the <italic toggle="yes">tensor.numpy()</italic> command. This, however, only makes sense when only small vectors are moved from GPU to CPU memory and back. One potentially useful option would be to shift the computationally expensive weight matrix and its operations to the GPU via Tensorflow, while only the result vectors are moved to the CPU for further processing.</p>
      <p>
        <inline-graphic xlink:href="fninf-15-715131-i0006.jpg"/>
      </p>
    </sec>
    <sec>
      <title>4.2. PymoNNto Supports Brian2-Like Model Definition</title>
      <p>A major advantage of Brian2 is its concise model definition. Dynamics are defined as a string of differential equations with physical units handled by the SymPy package (Meurer et al., <xref rid="B28" ref-type="bibr">2017</xref>). In Code blocks 7 and 8 we show how similar features can be added to PymoNNto with few additional modules: The <italic toggle="yes">Clock</italic> module keeps track of time across iterations, the <italic toggle="yes">Variable</italic> module initializes the neuron parameters and the <italic toggle="yes">Equation</italic> module handles differential equations in string format. While these modules are still in development, they already allow to write PymoNNto programs which resemble Brian2's concise style and produce similar results with similar processing speed.</p>
      <p>
        <inline-graphic xlink:href="fninf-15-715131-i0007.jpg"/>
      </p>
      <p>
        <inline-graphic xlink:href="fninf-15-715131-i0008.jpg"/>
      </p>
    </sec>
    <sec>
      <title>4.3. Simulator Fusion With PymoNNto</title>
      <p>The flexible and modular nature of PymoNNto allows to embed other simulators into PymoNNto. This unique feature allows to combine the functionality of other simulators with PymoNNto modules and its user interface. For clarity we only show two minimal examples, integrating Brian2 and NEST into PymoNNto (see Code block 9 and 10).</p>
      <p>
        <inline-graphic xlink:href="fninf-15-715131-i0009.jpg"/>
      </p>
      <p>
        <inline-graphic xlink:href="fninf-15-715131-i0010.jpg"/>
      </p>
    </sec>
    <sec>
      <title>4.4. Custom UI Module</title>
      <p>In this last example (Code block 11) we define a custom tab for the graphical user interface to plot the mean voltage of the neuron group (compare blue trace in <xref rid="F6" ref-type="fig">Figure 6</xref>). UI modules are derived from the TabBase class, which typically consists of the following four functions: <italic toggle="yes">__init__, add_recording_variables, initialize</italic> and <italic toggle="yes">update</italic>. These modules have a similar layout as <italic toggle="yes">Behaviour</italic> modules. The update function is called at every timestep. To access the parent user interface, we use an additional initialization function. Here, the <italic toggle="yes">__init__</italic> function is only defined to give the tab a name which can be done before the parent user interface is initialized.</p>
      <p>We specify the tab and its user interface elements in the <italic toggle="yes">initialize</italic> function. First, we add a new tab by calling <italic toggle="yes">Network_UI.Next_Tab</italic> which creates a new tab element and a corresponding layout for the internal components. This layout is arranged in rows and we can attach Qt widgets (QLabel, QPushButton, QSlider, …) to the current row with the <italic toggle="yes">Add_Element</italic> function. <italic toggle="yes">Next_H_Block</italic> can be called to jump to the next row. In this example we want to add a PyQtGraph plot to the tab, which is also a Qt widget compatible with the rest of the Qt framework. Because plotting is relatively common, there is a convenience function <italic toggle="yes">Add_plot_curve</italic> which creates a plot with a curve and adds them to the current row automatically.</p>
      <p>Next, we define the recording variables in the <italic toggle="yes">add_recording_variables</italic> function. To this end, we call the <italic toggle="yes">Network_UI</italic> function <italic toggle="yes">add_recording_variable</italic>, specifying what we want to record and for how many time steps. This function checks whether there are redundant recorders and, if so, replaces them with one recorder covering the full recording time to improve memory efficiency. The access through the tagging system is not affected by this and is still the same as in the previous plotting example. Alternatively, one could directly add a recorder to the neuron group similar to the previous examples. However, this could be inefficient if multiple tabs use partially redundant recorders.</p>
      <p>The last step is to define the <italic toggle="yes">update</italic> function which refreshes the plotted voltage trace. To save resources we check whether the tab is visible in the first place. If so, we access the recorded data via the tagging system. Like in the previous plotting example we can use the same string for variable evaluation and tagging. Therefore, <italic toggle="yes">[“np.mean(n.voltage),” 0, “np”]</italic> gives us the recorded mean of the voltage, selects the first and only element in the list of the tagged objects and directly converts it to a numpy array with the “<italic toggle="yes">np”</italic> attribute. The <italic toggle="yes">[</italic>−<italic toggle="yes">1000:]</italic> at the end is optional and ensures that the plotted trace is not longer than 1,000 elements, which could be the case when merging recorders of different length. This, however, only gives us the y-axis data. If we want to get the corresponding time steps on the x-axis, we can access the <italic toggle="yes">n.iteration</italic> trace in the same way as the y-data. This is possible, because the <italic toggle="yes">Network_UI</italic> adds this recorder automatically. To display the custom tab, we can add it to the list of <italic toggle="yes">ui_modules</italic> from the first examples, which is shown at the bottom of the code block.</p>
      <p>
        <inline-graphic xlink:href="fninf-15-715131-i0011.jpg"/>
      </p>
    </sec>
    <sec>
      <title>4.5. Cython</title>
      <p>Performance of Python code can be drastically improved by using Cython, which compiles Python into faster C code. In contrast to the PymoNNto's lightweight core, <italic toggle="yes">Behaviour</italic> modules with their heavy computations may strongly benefit from the use of Cython. The flexibility of PymoNNto allows to speed up only selected <italic toggle="yes">Behaviour</italic> modules. This leaves the rest of the code unaffected and avoids extensive re-compilation at every run. PymoNNto's online documentation contains detailed instructions on how to use Cython with PymoNNto.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s5">
    <title>5. Discussion</title>
    <p>We presented PymoNNto, a flexible modular neural network toolbox, which provides a low level core together with several high level features. This design aims to impose only minimal restrictions for model definition, while at the same time simplifying the network development via support functions. The flexibility of PymoNNto allows for any Python-based data representation and computation, opening the way to seamless interactions with external neuronal network libraries, such as Tensorflow or Brian2. Featuring a versatile user interface, a storage manager and an evolution package for hyperparameter tuning, Pymonnto facilitates an efficient workflow.</p>
    <p>PymoNNto's emphasis on flexibility defines its niche in the vibrant ecosystem of neural network simulators. In recent years, the research community has been witnessing intensive developments of many established simulators. For example, NEST Desktop allows to design, control and analyse NEST simulations without the need to write code (Spreizer et al., <xref rid="B37" ref-type="bibr">2021</xref>). Going even further, NetPyNE, a simulation manager for Neuron, provides both a programmatic and graphical interface for model definition, standardized import and export, parallel execution, parameter optimization, visualization, and analysis (Dura-Bernal et al., <xref rid="B11" ref-type="bibr">2019</xref>). Making use of the highly-optimized deep learning library PyTorch, BindsNET can efficiently simulate spiking neural networks both on CPUs and GPUs (Hazan et al., <xref rid="B17" ref-type="bibr">2018</xref>). And, the simulator Nengo (Bekolay et al., <xref rid="B3" ref-type="bibr">2014</xref>) recently received a backend for Intel's neuromorphic chip Loihi (Davies et al., <xref rid="B8" ref-type="bibr">2018</xref>). Together, these developments illustrate a common trend: The number of options increases for how to define a model and which hardware to use for execution. However, in most cases, these additions do not extend the expressive power of the respective core. PymoNNto strives not only to allow for flexible core control but also to keep the core itself as flexible as possible.</p>
    <p>Due to its simple core design, PymoNNto is easy to learn. Furthermore, transferring existing custom vector based models to PymoNNto is straightforward. Hence, PymoNNto may be of great interest to researchers that, until now, do not use existing simulators for their custom models, because of described restrictions of these simulators. With PymoNNto they get access to a powerful GUI and many useful support and analysis functions with minimal changes to their code.</p>
    <p>While we see PymoNNto primarily as a stand-alone neural network simulator, it can also be used in combination with one or several external simulation environments. While in our minimal example PymoNNto was only combined with Brian2 and NEST, more complex interactions can be conceived. For example, a Brian2 NeuronGroup could use a native PymoNNto plasticity module, while it interacts with a deep neural network implemented in Tensorflow. Note, as of now PymoNNto provides only a scaffold for interactions, but does not possess any built-in optimization for such processes. Thus, potential pitfalls remain and users need to assert caution when integrating external libraries. In a related approach, real-time interactions between a robotic simulator and the NEST simulation environment have been achieved by bridging between the Multi-Simulator Coordinator (MUSIC) and the Robotic Operating System (ROS) (Weidel et al., <xref rid="B46" ref-type="bibr">2016</xref>).</p>
    <p>When using differential equation-based model definitions, users may choose between PymoNNto's own differential equation module or integrating Brian2 into PymoNNto code. While integrating Brian2 directly allows access to its extensive functionality, PymoNNto's differential equation module has a lower computational overhead and allows for additional flexibility.</p>
    <p>The features demonstrated in this manuscript are considered stable except otherwise noted. In the future, we aim to include additional features, such as pre-processing functions for video and audio data, advanced high level modules for convenient model definition, and multicore processing of single networks. Because a single Python instance can execute operations only on a single core, multiprocessing or distributed computing is currently limited to specific cases: Computations can be executed on multiple cores via Tensorflow; and the <italic toggle="yes">Evolution</italic> module uses a “pleasingly parallel” computation scheme to test different parameter configurations on multiple cores and machines. To enable true multiprocessing, we intend to explore data exchange between multiple Python instances or a PymoNNto C++ backend with a Python interface. Another goal is to expand the public repository for behaviour and GUI modules. This would facilitate the incorporation of, e.g., plasticity models or useful network visualization tools developed by other research groups.</p>
    <p>PymoNNto facilitates interactions between spiking neural networks and deep learning. The efficiency of training deep non-spiking convolutional networks has led to remarkable progress in artificial intelligence research (LeCun et al., <xref rid="B26" ref-type="bibr">2015</xref>; Schmidhuber, <xref rid="B36" ref-type="bibr">2015</xref>). In contrast, as of now, spiking neural networks are mostly used in the context of brain research. Reflecting this divide, largely different tools are used in each of the two domains. Boosted by the prospect of energy-efficient neuromorphic hardware, efforts have been started to translate deep-learning based training algorithms to spiking neural networks (Pfeiffer and Pfeil, <xref rid="B33" ref-type="bibr">2018</xref>; Zenke and Ganguli, <xref rid="B48" ref-type="bibr">2018</xref>; Neftci et al., <xref rid="B31" ref-type="bibr">2019</xref>, for a review see Tavanaei et al., <xref rid="B41" ref-type="bibr">2019</xref>). In addition, deep learning frameworks are extended to simulate spiking neural networks (Hazan et al., <xref rid="B17" ref-type="bibr">2018</xref>; Mozafari et al., <xref rid="B30" ref-type="bibr">2019</xref>). Thus, with increasing interactions between these two fields, it will become important to have tools like PymoNNto, which can be used in both contexts and can flexibly combine the strengths of existing libraries.</p>
  </sec>
  <sec id="s6">
    <title>6. Development and Availability</title>
    <p>PymoNNto is released under the free and open MIT licence (Massachusetts Institute of Technology, <xref rid="B27" ref-type="bibr">1988</xref>). The development is public and code is available at: <ext-link xlink:href="https://github.com/trieschlab/PymoNNto" ext-link-type="uri">https://github.com/trieschlab/PymoNNto</ext-link> Tutorials and documentation can be found at: <ext-link xlink:href="https://pymonnto.readthedocs.io" ext-link-type="uri">https://pymonnto.readthedocs.io</ext-link>. All code examples can be found in the GitHub repository and were executed with PymoNNto's release version 1 and the library versions from the release description. We invite the community to contribute to PymoNNto's development and to extend the ecosystem with additional behaviour and UI modules.</p>
  </sec>
  <sec sec-type="data-availability" id="s7">
    <title>Data Availability Statement</title>
    <p>Publicly available datasets were analyzed in this study. This data can be found here: <ext-link xlink:href="https://github.com/trieschlab/PymoNNto" ext-link-type="uri">https://github.com/trieschlab/PymoNNto</ext-link>.</p>
  </sec>
  <sec id="s8">
    <title>Author Contributions</title>
    <p>MV is the main software developer of PymoNNto. MV and TS created the figures. MV, TS, and JT wrote the article. JT supervised the project. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-information" id="s9">
    <title>Funding</title>
    <p>This work was supported by the European Union, Horizon 2020 Research and Innovation Program, G.A. no. 713010, Project GOAL-Robots—Goal-based Open-ended Autonomous Learning Robots (MV), The German Research Foundation, DFG, SPP 2041, Project number 347573108: The dynamic connectome: keeping the balance (TS) and The dynamic connectome: dynamics of learning (MV), the LOEWE Center for Personalized Translational Epilepsy Research (CePTER) (TS), and the Johanna Quandt foundation (JT).</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s10">
    <title>Publisher's Note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
</body>
<back>
  <ack>
    <p>We thank the two reviewers, who greatly helped us to improve the initial version of this manuscript. We also thank Jan Marker for feedback on the manuscript.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M.</given-names></name><name><surname>Agarwal</surname><given-names>A.</given-names></name><name><surname>Barham</surname><given-names>P.</given-names></name><name><surname>Brevdo</surname><given-names>E.</given-names></name><name><surname>Chen</surname><given-names>Z.</given-names></name><name><surname>Citro</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Tensorflow: large-scale machine learning on heterogeneous distributed systems</article-title>. <source>arXiv preprint arXiv:1603.04467</source>.</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behnel</surname><given-names>S.</given-names></name><name><surname>Bradshaw</surname><given-names>R.</given-names></name><name><surname>Citro</surname><given-names>C.</given-names></name><name><surname>Dalcin</surname><given-names>L.</given-names></name><name><surname>Seljebotn</surname><given-names>D. S.</given-names></name><name><surname>Smith</surname><given-names>K.</given-names></name></person-group> (<year>2010</year>). <article-title>Cython: the best of both worlds</article-title>. <source>Comput. Sci. Eng</source>. <volume>13</volume>, <fpage>31</fpage>–<lpage>39</lpage>. <pub-id pub-id-type="doi">10.1109/MCSE.2010.118</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bekolay</surname><given-names>T.</given-names></name><name><surname>Bergstra</surname><given-names>J.</given-names></name><name><surname>Hunsberger</surname><given-names>E.</given-names></name><name><surname>DeWolf</surname><given-names>T.</given-names></name><name><surname>Stewart</surname><given-names>T. C.</given-names></name><name><surname>Rasmussen</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Nengo: a python tool for building large-scale functional brain models</article-title>. <source>Front. Neuroinformatics</source><volume>7</volume>:<fpage>48</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2013.00048</pub-id><?supplied-pmid 24431999?><pub-id pub-id-type="pmid">24431999</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname><given-names>R.</given-names></name><name><surname>Goodman</surname><given-names>D. F.</given-names></name></person-group> (<year>2012</year>). <article-title>Simulating spiking neural networks on GPU</article-title>. <source>Network</source>
<volume>23</volume>, <fpage>167</fpage>–<lpage>182</lpage>. <pub-id pub-id-type="doi">10.3109/0954898X.2012.730170</pub-id><?supplied-pmid 23067314?><pub-id pub-id-type="pmid">23067314</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname><given-names>R.</given-names></name><name><surname>Rudolph</surname><given-names>M.</given-names></name><name><surname>Carnevale</surname><given-names>T.</given-names></name><name><surname>Hines</surname><given-names>M.</given-names></name><name><surname>Beeman</surname><given-names>D.</given-names></name><name><surname>Bower</surname><given-names>J. M.</given-names></name><etal/></person-group>. (<year>2007</year>). <article-title>Simulation of networks of spiking neurons: a review of tools and strategies</article-title>. <source>J. Comput. Neurosci</source>. <volume>23</volume>, <fpage>349</fpage>–<lpage>398</lpage>. <pub-id pub-id-type="doi">10.1007/s10827-007-0038-6</pub-id><?supplied-pmid 17629781?><pub-id pub-id-type="pmid">17629781</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N.</given-names></name><name><surname>Hakim</surname><given-names>V.</given-names></name></person-group> (<year>1999</year>). <article-title>Fast global oscillations in networks of integrate-and-fire neurons with low firing rates</article-title>. <source>Neural Comput</source>. <volume>11</volume>, <fpage>1621</fpage>–<lpage>1671</lpage>. <pub-id pub-id-type="doi">10.1162/089976699300016179</pub-id><?supplied-pmid 10490941?><pub-id pub-id-type="pmid">10490941</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Campagnola</surname><given-names>L.</given-names></name></person-group> (<year>2020</year>). <source>PyQtGraph</source>. <publisher-loc>Chapel Hill</publisher-loc>: <publisher-name>University of North Carolina</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davies</surname><given-names>M.</given-names></name><name><surname>Srinivasa</surname><given-names>N.</given-names></name><name><surname>Lin</surname><given-names>T.-H.</given-names></name><name><surname>Chinya</surname><given-names>G.</given-names></name><name><surname>Cao</surname><given-names>Y.</given-names></name><name><surname>Choday</surname><given-names>S. H.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Loihi: A neuromorphic manycore processor with on-chip learning</article-title>. <source>IEEE Micro</source><volume>38</volume>, <fpage>82</fpage>–<lpage>99</lpage>. <pub-id pub-id-type="doi">10.1109/MM.2018.112130359</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davison</surname><given-names>A. P.</given-names></name><name><surname>Brüderle</surname><given-names>D.</given-names></name><name><surname>Eppler</surname><given-names>J. M.</given-names></name><name><surname>Kremkow</surname><given-names>J.</given-names></name><name><surname>Muller</surname><given-names>E.</given-names></name><name><surname>Pecevski</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2009</year>). <article-title>PyNN: a common interface for neuronal network simulators</article-title>. <source>Front. Neuroinformatics</source><volume>2</volume>:<fpage>11</fpage>. <pub-id pub-id-type="doi">10.3389/neuro.11.011.2008</pub-id><?supplied-pmid 19194529?><pub-id pub-id-type="pmid">19194529</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diesmann</surname><given-names>M.</given-names></name><name><surname>Gewaltig</surname><given-names>M.-O.</given-names></name><name><surname>Aertsen</surname><given-names>A.</given-names></name></person-group> (<year>1999</year>). <article-title>Stable propagation of synchronous spiking in cortical neural networks</article-title>. <source>Nature</source>
<volume>402</volume>, <fpage>529</fpage>–<lpage>533</lpage>. <pub-id pub-id-type="doi">10.1038/990101</pub-id><?supplied-pmid 10591212?><pub-id pub-id-type="pmid">10591212</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dura-Bernal</surname><given-names>S.</given-names></name><name><surname>Suter</surname><given-names>B. A.</given-names></name><name><surname>Gleeson</surname><given-names>P.</given-names></name><name><surname>Cantarelli</surname><given-names>M.</given-names></name><name><surname>Quintana</surname><given-names>A.</given-names></name><name><surname>Rodriguez</surname><given-names>F.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>NetPyNE, a tool for data-driven multiscale modeling of brain circuits</article-title>. <source>eLife</source><volume>8</volume>:<fpage>e44494</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.44494</pub-id><?supplied-pmid 31025934?><pub-id pub-id-type="pmid">31025934</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eiben</surname><given-names>A. E.</given-names></name><name><surname>Smith</surname><given-names>J. E.</given-names></name></person-group> (<year>2003</year>). <source>Introduction to Evolutionary Computing, Vol. 53</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>. <pub-id pub-id-type="doi">10.1007/978-3-662-05094-1</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Fardet</surname><given-names>T.</given-names></name><name><surname>Vennemo</surname><given-names>S. B.</given-names></name><name><surname>Mitchell</surname><given-names>J.</given-names></name><name><surname>Mørk</surname><given-names>H.</given-names></name><name><surname>Graber</surname><given-names>S.</given-names></name><name><surname>Hahne</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2020</year>). <source>NEST 2.20.1</source>. Available online at: <ext-link xlink:href="https://zenodo.org/record/4018718" ext-link-type="uri">https://zenodo.org/record/4018718</ext-link></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gleeson</surname><given-names>P.</given-names></name><name><surname>Crook</surname><given-names>S.</given-names></name><name><surname>Cannon</surname><given-names>R. C.</given-names></name><name><surname>Hines</surname><given-names>M. L.</given-names></name><name><surname>Billings</surname><given-names>G. O.</given-names></name><name><surname>Farinella</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>NeuroML: a language for describing data driven models of neurons and networks with a high degree of biological detail</article-title>. <source>PLoS Comput. Biol</source>. <volume>6</volume>:<fpage>e1000815</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000815</pub-id><?supplied-pmid 20585541?><pub-id pub-id-type="pmid">20585541</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodman</surname><given-names>D. F.</given-names></name><name><surname>Brette</surname><given-names>R.</given-names></name></person-group> (<year>2009</year>). <article-title>The Brian simulator</article-title>. <source>Front. Neurosci</source>. <volume>3</volume>:<fpage>26</fpage>. <pub-id pub-id-type="doi">10.3389/neuro.01.026.2009</pub-id><?supplied-pmid 20011141?><pub-id pub-id-type="pmid">20011141</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>C. R.</given-names></name><name><surname>Millman</surname><given-names>K. J.</given-names></name><name><surname>van der Walt</surname><given-names>S. J.</given-names></name><name><surname>Gommers</surname><given-names>R.</given-names></name><name><surname>Virtanen</surname><given-names>P.</given-names></name><name><surname>Cournapeau</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Array programming with NumPy</article-title>. <source>Nature</source><volume>585</volume>, <fpage>357</fpage>–<lpage>362</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><?supplied-pmid 32939066?><pub-id pub-id-type="pmid">32939066</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hazan</surname><given-names>H.</given-names></name><name><surname>Saunders</surname><given-names>D. J.</given-names></name><name><surname>Khan</surname><given-names>H.</given-names></name><name><surname>Patel</surname><given-names>D.</given-names></name><name><surname>Sanghavi</surname><given-names>D. T.</given-names></name><name><surname>Siegelmann</surname><given-names>H. T.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Bindsnet: a machine learning-oriented spiking neural networks library in python</article-title>. <source>Front. Neuroinformatics</source><volume>12</volume>:<fpage>89</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2018.00089</pub-id><?supplied-pmid 30631269?><pub-id pub-id-type="pmid">30631269</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hindmarsh</surname><given-names>J. L.</given-names></name><name><surname>Rose</surname><given-names>R.</given-names></name></person-group> (<year>1984</year>). <article-title>A model of neuronal bursting using three coupled first order differential equations</article-title>. <source>Proc. R. Soc. Lond. Ser. B Biol. Sci</source>. <volume>221</volume>, <fpage>87</fpage>–<lpage>102</lpage>. <pub-id pub-id-type="doi">10.1098/rspb.1984.0024</pub-id><?supplied-pmid 6144106?><pub-id pub-id-type="pmid">6144106</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname><given-names>M. L.</given-names></name><name><surname>Carnevale</surname><given-names>N. T.</given-names></name></person-group> (<year>1997</year>). <article-title>The NEURON simulation environment</article-title>. <source>Neural Comput</source>. <volume>9</volume>, <fpage>1179</fpage>–<lpage>1209</lpage>. <pub-id pub-id-type="doi">10.1162/neco.1997.9.6.1179</pub-id><?supplied-pmid 9248061?><pub-id pub-id-type="pmid">9248061</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hodgkin</surname><given-names>A. L.</given-names></name><name><surname>Huxley</surname><given-names>A. F.</given-names></name></person-group> (<year>1952</year>). <article-title>A quantitative description of membrane current and its application to conduction and excitation in nerve</article-title>. <source>J. Physiol</source>. <volume>117</volume>, <fpage>500</fpage>–<lpage>544</lpage>. <pub-id pub-id-type="doi">10.1113/jphysiol.1952.sp004764</pub-id><?supplied-pmid 2185861?><pub-id pub-id-type="pmid">12991237</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>J. J.</given-names></name></person-group> (<year>1982</year>). <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>79</volume>, <fpage>2554</fpage>–<lpage>2558</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.79.8.2554</pub-id><?supplied-pmid 6953413?><pub-id pub-id-type="pmid">6953413</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>J. D.</given-names></name></person-group> (<year>2007</year>). <article-title>Matplotlib: a 2d graphics environment</article-title>. <source>Comput. Sci. Eng</source>. <volume>9</volume>, <fpage>90</fpage>–<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izhikevich</surname><given-names>E. M.</given-names></name></person-group> (<year>2003</year>). <article-title>Simple model of spiking neurons</article-title>. <source>IEEE Trans. Neural Netw</source>. <volume>14</volume>, <fpage>1569</fpage>–<lpage>1572</lpage>. <pub-id pub-id-type="doi">10.1109/TNN.2003.820440</pub-id><?supplied-pmid 18244602?><pub-id pub-id-type="pmid">18244602</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jordan</surname><given-names>J.</given-names></name><name><surname>Ippen</surname><given-names>T.</given-names></name><name><surname>Helias</surname><given-names>M.</given-names></name><name><surname>Kitayama</surname><given-names>I.</given-names></name><name><surname>Sato</surname><given-names>M.</given-names></name><name><surname>Igarashi</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Extremely scalable spiking neuronal network simulation code: from laptops to exascale computers</article-title>. <source>Front. Neuroinformatics</source><volume>12</volume>:<fpage>2</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2018.00034</pub-id><?supplied-pmid 30008668?><pub-id pub-id-type="pmid">30008668</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lazar</surname><given-names>A.</given-names></name><name><surname>Pipa</surname><given-names>G.</given-names></name><name><surname>Triesch</surname><given-names>J.</given-names></name></person-group> (<year>2009</year>). <article-title>Sorn: a self-organizing recurrent neural network</article-title>. <source>Front. Comput. Neurosci</source>. <volume>3</volume>:<fpage>23</fpage>. <pub-id pub-id-type="doi">10.3389/neuro.10.019.2009</pub-id><?supplied-pmid 25852533?><pub-id pub-id-type="pmid">19893759</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group> (<year>2015</year>). <article-title>Deep learning</article-title>. <source>Nature</source>
<volume>521</volume>, <fpage>436</fpage>–<lpage>444</lpage>. <pub-id pub-id-type="doi">10.1038/nature14539</pub-id><?supplied-pmid 26017442?><pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><collab>Massachusetts Institute of Technology</collab></person-group> (<year>1988</year>). <source>MIT License</source>. <publisher-name>Massachusetts Institute of Technology</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meurer</surname><given-names>A.</given-names></name><name><surname>Smith</surname><given-names>C. P.</given-names></name><name><surname>Paprocki</surname><given-names>M.</given-names></name><name><surname>Čertík</surname><given-names>O.</given-names></name><name><surname>Kirpichev</surname><given-names>S. B.</given-names></name><name><surname>Rocklin</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>SymPy: symbolic computing in Python</article-title>. <source>PeerJ Comput. Sci</source>. <volume>3</volume>:<fpage>e103</fpage>. <pub-id pub-id-type="doi">10.7717/peerj-cs.103</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Mohanta</surname><given-names>S. S.</given-names></name><name><surname>Assisi</surname><given-names>C.</given-names></name></person-group> (<year>2019</year>). <source>Parallel scalable simulations of biological neural networks using TensorFlow: a beginner's guide. arXiv preprint arXiv:1906.03958</source>. Available online at: <ext-link xlink:href="https://arxiv.org/abs/1906.03958" ext-link-type="uri">https://arxiv.org/abs/1906.03958</ext-link></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mozafari</surname><given-names>M.</given-names></name><name><surname>Ganjtabesh</surname><given-names>M.</given-names></name><name><surname>Nowzari-Dalini</surname><given-names>A.</given-names></name><name><surname>Masquelier</surname><given-names>T.</given-names></name></person-group> (<year>2019</year>). <article-title>Spyketorch: efficient simulation of convolutional spiking neural networks with at most one spike per neuron</article-title>. <source>Front. Neurosci</source>. <volume>13</volume>:<fpage>625</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2019.00625</pub-id><?supplied-pmid 31354403?><pub-id pub-id-type="pmid">31354403</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neftci</surname><given-names>E. O.</given-names></name><name><surname>Mostafa</surname><given-names>H.</given-names></name><name><surname>Zenke</surname><given-names>F.</given-names></name></person-group> (<year>2019</year>). <article-title>Surrogate gradient learning in spiking neural networks: bringing the power of gradient-based optimization to spiking neural networks</article-title>. <source>IEEE Signal Process. Mag</source>. <volume>36</volume>, <fpage>51</fpage>–<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1109/MSP.2019.2931595</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pauli</surname><given-names>R.</given-names></name><name><surname>Weidel</surname><given-names>P.</given-names></name><name><surname>Kunkel</surname><given-names>S.</given-names></name><name><surname>Morrison</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>). <article-title>Reproducing polychronization: a guide to maximizing the reproducibility of spiking network models</article-title>. <source>Front. Neuroinformatics</source>
<volume>12</volume>:<fpage>46</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2018.00046</pub-id><?supplied-pmid 30123121?><pub-id pub-id-type="pmid">30123121</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname><given-names>M.</given-names></name><name><surname>Pfeil</surname><given-names>T.</given-names></name></person-group> (<year>2018</year>). <article-title>Deep learning with spiking neurons: opportunities and challenges</article-title>. <source>Front. Neurosci</source>. <volume>12</volume>:<fpage>774</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2018.00774</pub-id><?supplied-pmid 30410432?><pub-id pub-id-type="pmid">30410432</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Plotnikov</surname><given-names>D.</given-names></name><name><surname>Rumpe</surname><given-names>B.</given-names></name><name><surname>Blundell</surname><given-names>I.</given-names></name><name><surname>Ippen</surname><given-names>T.</given-names></name><name><surname>Eppler</surname><given-names>J. M.</given-names></name><name><surname>Morrison</surname><given-names>A.</given-names></name></person-group> (<year>2016</year>). <article-title>“NESTML: a modeling language for spiking neurons,”</article-title> in <source>Modellierung 2016</source> (<publisher-loc>Karlsruhe</publisher-loc>).</mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><collab>Riverbank Computing</collab></person-group> (<year>2020</year>). <source>PyQt5</source>. <publisher-name>Riverbank Computing</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidhuber</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>Deep learning in neural networks: an overview</article-title>. <source>Neural Netw</source>. <volume>61</volume>, <fpage>85</fpage>–<lpage>117</lpage>. <pub-id pub-id-type="doi">10.1016/j.neunet.2014.09.003</pub-id><?supplied-pmid 25462637?><pub-id pub-id-type="pmid">25462637</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spreizer</surname><given-names>S.</given-names></name><name><surname>Senk</surname><given-names>J.</given-names></name><name><surname>Rotter</surname><given-names>S.</given-names></name><name><surname>Diesmann</surname><given-names>M.</given-names></name><name><surname>Weyers</surname><given-names>B.</given-names></name></person-group> (<year>2021</year>). <article-title>NEST desktop-an educational application for neuroscience</article-title>. <source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2021.06.15.444791</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stimberg</surname><given-names>M.</given-names></name><name><surname>Brette</surname><given-names>R.</given-names></name><name><surname>Goodman</surname><given-names>D. F.</given-names></name></person-group> (<year>2019</year>). <article-title>Brian 2, an intuitive and efficient neural simulator</article-title>. <source>Elife</source>
<volume>8</volume>:<fpage>e47314</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.47314</pub-id><?supplied-pmid 31429824?><pub-id pub-id-type="pmid">31429824</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stimberg</surname><given-names>M.</given-names></name><name><surname>Goodman</surname><given-names>D. F.</given-names></name><name><surname>Nowotny</surname><given-names>T.</given-names></name></person-group> (<year>2020</year>). <article-title>Brian2GeNN: accelerating spiking neural network simulations with graphics hardware</article-title>. <source>Sci. Rep</source>. <volume>10</volume>, <fpage>1</fpage>–<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1038/s41598-019-54957-7</pub-id><?supplied-pmid 31941893?><pub-id pub-id-type="pmid">31913322</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sweeney</surname><given-names>Y.</given-names></name><name><surname>Hellgren Kotaleski</surname><given-names>J.</given-names></name><name><surname>Hennig</surname><given-names>M. H.</given-names></name></person-group> (<year>2015</year>). <article-title>A diffusive homeostatic signal maintains neural heterogeneity and responsiveness in cortical networks</article-title>. <source>PLoS Comput. Biol</source>. <volume>11</volume>:<fpage>e1004389</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004389</pub-id><?supplied-pmid 26158556?><pub-id pub-id-type="pmid">26158556</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavanaei</surname><given-names>A.</given-names></name><name><surname>Ghodrati</surname><given-names>M.</given-names></name><name><surname>Kheradpisheh</surname><given-names>S. R.</given-names></name><name><surname>Masquelier</surname><given-names>T.</given-names></name><name><surname>Maida</surname><given-names>A.</given-names></name></person-group> (<year>2019</year>). <article-title>Deep learning in spiking neural networks</article-title>. <source>Neural Netw</source>. <volume>111</volume>, <fpage>47</fpage>–<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1016/j.neunet.2018.12.002</pub-id><?supplied-pmid 30682710?><pub-id pub-id-type="pmid">30682710</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tikidji-Hamburyan</surname><given-names>R. A.</given-names></name><name><surname>Narayana</surname><given-names>V.</given-names></name><name><surname>Bozkus</surname><given-names>Z.</given-names></name><name><surname>El-Ghazawi</surname><given-names>T. A.</given-names></name></person-group> (<year>2017</year>). <article-title>Software for brain network simulations: a comparative study</article-title>. <source>Front. Neuroinformatics</source>
<volume>11</volume>:<fpage>46</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2017.00046</pub-id><?supplied-pmid 29743871?><pub-id pub-id-type="pmid">29743871</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Van Rossum</surname><given-names>G.</given-names></name><name><surname>Drake</surname><given-names>F. L.</given-names><suffix>Jr.</suffix></name></person-group> (<year>1995</year>). <source>Python Reference Manual</source>. <publisher-loc>Amsterdam</publisher-loc>: <publisher-name>Centrum voor Wiskunde en Informatica Amsterdam</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vikhar</surname><given-names>P. A.</given-names></name></person-group> (<year>2016</year>). <article-title>“Evolutionary algorithms: a critical review and its future prospects,”</article-title> in <source>2016 International Conference on Global Trends in Signal Processing, Information Computing and Communication (ICGTSPICC)</source>, 261–265. <pub-id pub-id-type="doi">10.1109/ICGTSPICC.2016.7955308</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X.-J.</given-names></name><name><surname>Buzsáki</surname><given-names>G.</given-names></name></person-group> (<year>1996</year>). <article-title>Gamma oscillation by synaptic inhibition in a hippocampal interneuronal network model</article-title>. <source>J. Neurosci</source>. <volume>16</volume>, <fpage>6402</fpage>–<lpage>6413</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-20-06402.1996</pub-id><?supplied-pmid 8815919?><pub-id pub-id-type="pmid">8815919</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weidel</surname><given-names>P.</given-names></name><name><surname>Djurfeldt</surname><given-names>M.</given-names></name><name><surname>Duarte</surname><given-names>R. C.</given-names></name><name><surname>Morrison</surname><given-names>A.</given-names></name></person-group> (<year>2016</year>). <article-title>Closed loop interactions between spiking neural network and robotic simulators based on MUSIC and ROS</article-title>. <source>Front. Neuroinformatics</source>
<volume>10</volume>:<fpage>31</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2016.00031</pub-id><?supplied-pmid 27536234?><pub-id pub-id-type="pmid">27536234</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yavuz</surname><given-names>E.</given-names></name><name><surname>Turner</surname><given-names>J.</given-names></name><name><surname>Nowotny</surname><given-names>T.</given-names></name></person-group> (<year>2016</year>). <article-title>GeNN: a code generation framework for accelerated brain simulations</article-title>. <source>Sci. Rep</source>. <volume>6</volume>, <fpage>1</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1038/srep18854</pub-id><?supplied-pmid 26740369?><pub-id pub-id-type="pmid">28442746</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zenke</surname><given-names>F.</given-names></name><name><surname>Ganguli</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>). <article-title>Superspike: supervised learning in multilayer spiking neural networks</article-title>. <source>Neural Comput</source>. <volume>30</volume>, <fpage>1514</fpage>–<lpage>1541</lpage>. <pub-id pub-id-type="doi">10.1162/neco_a_01086</pub-id><?supplied-pmid 29652587?><pub-id pub-id-type="pmid">29652587</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
