<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7014657</article-id>
    <article-id pub-id-type="publisher-id">3393</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-020-3393-1</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DTranNER: biomedical named entity recognition with deep learning-based label-label transition model</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2756-7053</contrib-id>
        <name>
          <surname>Hong</surname>
          <given-names>S. K.</given-names>
        </name>
        <address>
          <email>skhong831@kaist.ac.kr</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8711-7732</contrib-id>
        <name>
          <surname>Lee</surname>
          <given-names>Jae-Gil</given-names>
        </name>
        <address>
          <email>jaegil@kaist.ac.kr</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2292 0500</institution-id><institution-id institution-id-type="GRID">grid.37172.30</institution-id><institution>Graduate School of Knowledge Service Engineering, KAIST, </institution></institution-wrap>291 Daehak-ro, Yuseong-gu, Daejeon, 34141 South Korea </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2292 0500</institution-id><institution-id institution-id-type="GRID">grid.37172.30</institution-id><institution>Department of Industrial &amp; Systems Engineering, KAIST, </institution></institution-wrap>291 Daehak-ro, Yuseong-gu, Daejeon, 34141 South Korea </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>2</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>11</day>
      <month>2</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>21</volume>
    <elocation-id>53</elocation-id>
    <history>
      <date date-type="received">
        <day>2</day>
        <month>6</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>31</day>
        <month>1</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Biomedical named-entity recognition (BioNER) is widely modeled with conditional random fields (CRF) by regarding it as a sequence labeling problem. The CRF-based methods yield structured outputs of labels by imposing connectivity between the labels. Recent studies for BioNER have reported state-of-the-art performance by combining deep learning-based models (e.g., bidirectional Long Short-Term Memory) and CRF. The deep learning-based models in the CRF-based methods are dedicated to estimating individual labels, whereas the relationships between connected labels are described as static numbers; thereby, it is not allowed to timely reflect the context in generating the most plausible label-label transitions for a given input sentence. Regardless, correctly segmenting entity mentions in biomedical texts is challenging because the biomedical terms are often descriptive and long compared with general terms. Therefore, limiting the label-label transitions as static numbers is a bottleneck in the performance improvement of BioNER.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We introduce DTranNER, a novel CRF-based framework incorporating a deep learning-based label-label transition model into BioNER. DTranNER uses two separate deep learning-based networks: Unary-Network and Pairwise-Network. The former is to model the input for determining individual labels, and the latter is to explore the context of the input for describing the label-label transitions. We performed experiments on five benchmark BioNER corpora. Compared with current state-of-the-art methods, DTranNER achieves the best F1-score of 84.56% beyond 84.40% on the BioCreative II gene mention (BC2GM) corpus, the best F1-score of 91.99% beyond 91.41% on the BioCreative IV chemical and drug (BC4CHEMD) corpus, the best F1-score of 94.16% beyond 93.44% on the chemical NER, the best F1-score of 87.22% beyond 86.56% on the disease NER of the BioCreative V chemical disease relation (BC5CDR) corpus, and a near-best F1-score of 88.62% on the NCBI-Disease corpus.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">Our results indicate that the incorporation of the deep learning-based label-label transition model provides distinctive contextual clues to enhance BioNER over the static transition model. We demonstrate that the proposed framework enables the dynamic transition model to adaptively explore the contextual relations between adjacent labels in a fine-grained way. We expect that our study can be a stepping stone for further prosperity of biomedical literature mining.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Bioinformatics</kwd>
      <kwd>Data mining</kwd>
      <kwd>Named entity recognition</kwd>
      <kwd>Neural network</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>National Research Foundation of Korea</institution>
        </funding-source>
        <award-id>2017R1E1A1A01075927</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p>Biomedical named-entity recognition (BioNER) automatically identifies specific mentions of interest such as chemicals, diseases, drugs, genes, DNAs, proteins, viruses etc. in biomedical literature. As the fundamental step for various downstream linguistic tasks, e.g., adverse drug event extraction [<xref ref-type="bibr" rid="CR1">1</xref>], bacteria biotope task [<xref ref-type="bibr" rid="CR2">2</xref>], drug-drug interaction [<xref ref-type="bibr" rid="CR3">3</xref>], and protein-protein interaction detection [<xref ref-type="bibr" rid="CR4">4</xref>], the performance of BioNER is crucial in the overall biomedical knowledge discovery process [<xref ref-type="bibr" rid="CR2">2</xref>].</p>
    <p>BioNER operates by predicting a class label for each token across biomedical literature. It is typically considered as a sequence labeling problem and is thus widely modeled by a first-order linear-chain conditional random field (CRF) [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>]. CRF yields chain-structured label sequences by collectively assessing possible label-label transition relations between words over the entire input sequence.</p>
    <p>In recent years, deep learning (briefly, DL) has become prevalent across various machine learning-based natural language processing (NLP) tasks since neural network-based learning systems can effectively identify prominent features in a data-driven way, replacing task-specific feature engineering based on high-level domain knowledge [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>]. For NER tasks, recent methods [<xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR14">14</xref>] have reported state-of-the-art performance by introducing a bidirectional long short-term memory (BiLSTM) into CRF. Accordingly, the combination of BiLSTM and CRF has been widely considered as a standard architecture for various sequence labeling problems.</p>
    <p>The combined models (i.e., BiLSTM-CRFs) for NER typically consist of two major components: a token-level BiLSTM and a real-valued transition matrix. The BiLSTM is dedicated to estimate the best-suited label on each token, while the transition matrix is solely responsible for describing the transition compatibility between all possible pairs of labels on neighboring tokens; in detail, the numerical score at the <italic>i</italic>th row and <italic>j</italic>th column of a transition matrix represents the transition compatibility from the <italic>i</italic>th label to the <italic>j</italic>th label. Note that the transition matrix is once established by being suited to the statistics of given training data via its parameter learning and is frozen afterward. As a result, the transition matrix cannot provide the contextualized compatibility for the relationship of neighboring labels in a fine-grained way.</p>
    <p>Accordingly, we contend that solely relying on the static transition matrix is not enough to explain the ever-changing label-label transition relations in BioNER, since biomedical entities are frequently descriptive, long or even contain conjunctions [<xref ref-type="bibr" rid="CR15">15</xref>], e.g., “normal thymic epithelial cells,” “peripheral sensor neuropathy,” and “central nervous system and cardiac toxicity.” As a result, the boundaries of entity-mentions in biomedical texts are often too ambiguous to accurately segment them. Therefore, we argue that exploiting contextual information to describe label-label transition relations is important to facilitate the accurate identification of biomedical entities. Recently, Lin et al. [<xref ref-type="bibr" rid="CR16">16</xref>] studied that explicitly modeling relations between parts in a structured model is applicable to semantic image segmentation, whereas it has been rarely studied in recent DL-based NLP methods.</p>
    <p>To this end, we propose a novel framework, called <italic><underline>D</underline></italic><italic>ynamic </italic><italic><underline>Tran</underline></italic><italic>sition for </italic><italic><underline>NER</underline></italic><italic> (DTranNER)</italic>, to incorporate a DL-based model, which adaptively identify label-label transition relations to further improve the accuracy of BioNER. Overall, DTranNER makes use of two separate DL-based models: Unary-Network and Pairwise-Network. The addition of Pairwise-Network makes it possible to assess the transition compatibility between adjacent labels by exploring the context of an input sentence. Meanwhile, as another DL-based model, Unary-Network is used for individual labeling as in previous works. After all, Unary-Network and Pairwise-Network are arranged to yield agreed label sequences via this novel framework.</p>
    <p>Because DTranNER is orthogonal to a DL-based model, any type of DL-based models such as attention [<xref ref-type="bibr" rid="CR17">17</xref>] or transformer [<xref ref-type="bibr" rid="CR18">18</xref>] can be employed to play the role of Unary-Network or Pairwise-Network. In this study, we conduct experiments using a BiLSTM as the underlying DL networks since it has been widely adopted in various sequence labeling problems so far.</p>
    <p>We evaluated DTranNER by comparing with current state-of-the-art NER methods on five benchmark BioNER corpora to investigate the effectiveness of the DL-based label-label transition model. The results show that DTranNER outperformed the existing best performer on four out of five corpora and showed comparable accuracy to the existing best performer on one remaining corpus, thereby demonstrating the excellent performance of DTranNER.</p>
  </sec>
  <sec id="Sec2">
    <title>Background</title>
    <sec id="Sec3">
      <title>Problem definition: biomedical named entity recognition (BioNER)</title>
      <p>An instance of a BioNER corpus consists of an input token sequence <italic>x</italic>=<italic>x</italic><sub>1</sub>,…,<italic>x</italic><sub><italic>N</italic></sub> and its associated output label sequence <italic>y</italic>=<italic>y</italic><sub>1</sub>,…,<italic>y</italic><sub><italic>N</italic></sub>. We use the IOBES tagging scheme by which tokens are annotated with one of “I,” “O,” “B,” “E,” or “S” labels. In the case of an entity spanning over multiple tokens, “B” is tagged to the token to indicate the beginning of the entity, “I” stands for “Inside,” and “E” indicates the ending token of the entity. For the case of an entity of a single token, the “S” label is tagged to it. The “O” label stands for “Outside,” which means that the token is not part of any named entity. To indicate the type of entities, one of the type tags, such as “Chemical,” “Disease,” “Gene,” or “Protein,” is additionally concatenated to each IOBES tag.</p>
    </sec>
    <sec id="Sec4">
      <title>Linear-chain conditional random field (CRF)</title>
      <p>As a class of discriminative probabilistic graphical models, a linear-chain conditional random field (CRF) describes the joint probability <italic>P</italic>(<bold>y</bold>|<bold>x</bold>) of the entire structured labels <bold>y</bold> with respect to the structure of an undirected graph, given a set of inputs <bold>x</bold>. CRF is widely used in various sequence labeling problems as well as BioNER by imposing the first-order Markov property on the output sequence labeling. There are two types—unary and pairwise—of elementary feature functions to organize an output label sequence. The unary feature functions are dedicated to estimating the suitability of candidate labels at each individual position, whereas the pairwise feature functions are designed to assess possible pairwise labels on two connected positions. Summing up, when an input sequence <bold>x</bold> of length <italic>N</italic> is given, the conditional distribution <italic>P</italic>(<bold>y</bold>|<bold>x</bold>) is represented as a product of position-dependent unary and pairwise feature functions; thus, it is formulated as in the following equation:
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} P(\mathbf{y}|\mathbf{x}) = \frac{1}{Z(\mathbf{x})}\exp(\sum\limits_{i=1}^{N}{\sum\limits_{j}^{J}{\lambda_{j}^{s}s_{j}(y_{i},\mathbf{x},i)}} \\ +\sum\limits_{i=2}^{N}\sum\limits_{k}^{K}\lambda_{k}^{t}t_{k}(y_{i-1},y_{i},\mathbf{x},i)), \end{array} $$ \end{document}</tex-math><mml:math id="M2"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>exp</mml:mo><mml:mo>(</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="align-1"><mml:mo>+</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2020_3393_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <italic>s</italic><sub><italic>k</italic></sub>(<italic>y</italic><sub><italic>i</italic></sub>,<bold>x</bold>,<italic>i</italic>) denotes a member of the unary feature functions (i.e., <italic>s</italic>∈<italic>S</italic>) at the position <italic>i</italic>, and <italic>t</italic>(<italic>y</italic><sub><italic>i</italic>−1</sub>,<italic>y</italic><sub><italic>i</italic></sub>,<bold>x</bold>,<italic>i</italic>) indicates a member of the pairwise feature functions (i.e., <italic>t</italic>∈<italic>T</italic>) at two consecutive positions <italic>i-1</italic> and <italic>i</italic>. Traditionally, the unary and pairwise feature functions are manually designed to facilitate accurate sequence labeling, and they are usually real-valued binary indicators representing either true or false. The weights (i.e., <italic>λ</italic><sup><italic>s</italic></sup>∈<italic>θ</italic><sub><italic>s</italic></sub> and <italic>λ</italic><sup><italic>t</italic></sup>∈<italic>θ</italic><sub><italic>t</italic></sub>) associated to the feature functions are trainable parameters. <italic>Z</italic>(<bold>x</bold>) is the partition function as a normalization constant over all possible label assignments.</p>
    </sec>
    <sec id="Sec5">
      <title>Bidirectional long short-term memory (BiLSTM)</title>
      <p>Long short-term memory (LSTM) [<xref ref-type="bibr" rid="CR19">19</xref>] is a specific variant of recurrent neural networks to mitigate the problem of vanishing and exploding gradients in modeling long-term dependencies of a sequence. LSTM is suited for modeling sequential data with recurrent connections of hidden states <italic>H</italic>={<italic>h</italic><sub>1</sub>,<italic>h</italic><sub>2</sub>,…,<italic>h</italic><sub><italic>N</italic></sub>} and have become ubiquitous in a wide range of NLP tasks. At every time step, LSTM yields a current hidden state <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overrightarrow {h_{t}}$\end{document}</tex-math><mml:math id="M4"><mml:mover class="overrightarrow"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⃗</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq1.gif"/></alternatives></inline-formula> and internally updates a current cell state <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overrightarrow {c_{t}}$\end{document}</tex-math><mml:math id="M6"><mml:mover class="overrightarrow"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⃗</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq2.gif"/></alternatives></inline-formula> based on <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overrightarrow {h_{t-1}}$\end{document}</tex-math><mml:math id="M8"><mml:mover class="overrightarrow"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>⃗</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq3.gif"/></alternatives></inline-formula> and <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overrightarrow {c_{t-1}}$\end{document}</tex-math><mml:math id="M10"><mml:mover class="overrightarrow"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>⃗</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq4.gif"/></alternatives></inline-formula> calculated in the previous time step.</p>
      <p>Given that LSTM is limited to using past context in the forward direction, a bidirectional LSTM (BiLSTM) is employed to exploit future context as well as past context. BiLSTM processes an input sequence in both forward and backward directions with two separate LSTMs. That is, the hidden states from both directional LSTMs are concatenated to make final output vectors <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$h_{t}=\{\overrightarrow {h_{t}}\oplus \overleftarrow {h_{t}}\}$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mover class="overrightarrow"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⃗</mml:mo></mml:mover><mml:mo>⊕</mml:mo><mml:mover class="overleftarrow"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⃖</mml:mo></mml:mover><mml:mo>}</mml:mo></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq5.gif"/></alternatives></inline-formula>.</p>
    </sec>
    <sec id="Sec6">
      <title>Merger of BiLSTM and CRF: BiLSTM-CRF</title>
      <p>BiLSTM-CRF has been widely employed in recent neural network-based NER studies [<xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR21">21</xref>] for sequence labeling. The architecture of BiLSTM-CRF is typically comprised of four layers: a token-embedding layer, a token-level BiLSTM layer, a binding layer, and a CRF layer. We denote an input token sequence of length <italic>N</italic> by <bold>x</bold>={<italic>x</italic><sub>1</sub>,⋯,<italic>x</italic><sub><italic>N</italic></sub>} and the corresponding output label sequence by <bold>y</bold>={<italic>y</italic><sub>1</sub>,⋯,<italic>y</italic><sub><italic>N</italic></sub>}. First, the token-embedding layer encodes input tokens into its fixed-dimensional vectors as <italic>e</italic><sub>1</sub>,<italic>e</italic><sub>2</sub>,…,<italic>e</italic><sub><italic>N</italic></sub>. Next, the BiLSTM layer takes the token-embedding vectors as the inputs to generate the hidden-state vectors <italic>h</italic><sub>1</sub>,<italic>h</italic><sub>2</sub>,…,<italic>h</italic><sub><italic>N</italic></sub>. Before being fed to the CRF layer, the hidden-state vectors are transformed to the score vectors <italic>U</italic><sub>1</sub>,<italic>U</italic><sub>2</sub>,…,<italic>U</italic><sub><italic>N</italic></sub> with <italic>L</italic>-dimensionality, where <italic>L</italic> denotes the number of labels, via the binding layer so as to match the number of labels. The score vector contains the confidence values for possible labels on its corresponding token position. Namely, the stack from the token-embedding layer to the binding layer can be considered to play the role of the unary feature functions (i.e., <italic>s</italic>∈<italic>S</italic>) in Eq. <xref rid="Equ1" ref-type="">1</xref>. Besides, a real-valued transition matrix, denoted as <italic>A</italic>, accounts for all the label-label transition relations; it is likewise regarded to play the role of the pairwise feature functions (i.e., <italic>t</italic>∈<italic>T</italic>) in Eq. <xref rid="Equ1" ref-type="">1</xref>. Eventually, BiLSTM-CRF calculates the likelihood for a label sequence <bold>y</bold> given an input token sequence <bold>x</bold> via the following equation:
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ P_{u}(\mathbf{y}|\mathbf{x}) = \frac{1}{Z}\exp{(\sum\limits_{i=1}^{N}U_{i}(y_{i})+\sum\limits_{i=2}^{N}A_{i-1,i})},   $$ \end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mfrac><mml:mo>exp</mml:mo><mml:mo>(</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2020_3393_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <italic>U</italic><sub><italic>i</italic></sub>(<italic>y</italic><sub><italic>i</italic></sub>) denotes the unary score for assigning the label <italic>y</italic><sub><italic>i</italic></sub> on the <italic>i</italic>th token, <italic>A</italic><sub><italic>i,j</italic></sub> corresponds to the real-valued pairwise transition compatibility from <italic>i</italic>th label to <italic>j</italic>th label, and <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$Z\,=\,\sum \nolimits _{\mathbf {y}}{\exp {(\sum \nolimits _{i=1}^{N}U_{i}(y_{i})+\sum \nolimits _{i=2}^{N}A_{i-1,i})}}$\end{document}</tex-math><mml:math id="M16"><mml:mi>Z</mml:mi><mml:mspace width="0.3em"/><mml:mo>=</mml:mo><mml:mspace width="0.3em"/><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:munder><mml:mo>exp</mml:mo><mml:mo>(</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq6.gif"/></alternatives></inline-formula>.</p>
    </sec>
  </sec>
  <sec id="Sec7">
    <title>Related work</title>
    <p>Recent state-of-the-art CRF-based NER studies [<xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR20">20</xref>–<xref ref-type="bibr" rid="CR22">22</xref>] have demonstrated the effectiveness of data-driven representation learning (i.e., DL) under CRF. We discuss several CRF-based methods for NER in terms of two kinds of feature functions: unary and pairwise feature functions. We also introduce BioBERT that showed the state-of-the-art performance in BioNER.
<list list-type="bullet"><list-item><p>Lample et al. [<xref ref-type="bibr" rid="CR9">9</xref>] proposed to bring BiLSTM into CRF for NER in general news domain. The model uses two BiLSTMs: one for token-level representation learning and the another for character-level representation learning. The BiLSTMs work as unary feature functions, whereas a static transition matrix comes in for pairwise feature functions. Afterward, Habibi et al. [<xref ref-type="bibr" rid="CR10">10</xref>] adopted the model of Lample et al. [<xref ref-type="bibr" rid="CR9">9</xref>] for BioNER.</p></list-item><list-item><p>Luo et al. [<xref ref-type="bibr" rid="CR22">22</xref>] adopted BiLSTM-CRF for NER in chemistry domain and applied an attention mechanism to leverage document-level context information. They employ abbreviation embeddings using a specific external library to handle abbreviations that frequently appear in chemical entities’ naming. Their model also relies on a static matrix to retrieve all the label-label transition relations in CRF.</p></list-item><list-item><p>Dang et al. [<xref ref-type="bibr" rid="CR12">12</xref>] developed D3NER to utilize various linguistic information under BiLSTM-CRF. D3NER creates a token embedding by aggregating several embeddings: a pre-trained word embedding, an abbreviation embedding, a POS embedding, and a character-level token embedding. Similarly, a transition matrix solely plays the role of pairwise feature functions.</p></list-item><list-item><p>Wang et al. [<xref ref-type="bibr" rid="CR11">11</xref>] introduced a multi-task learning framework for BioNER. They trained a model using several biomedical corpora together to overcome a limited amount of annotated biomedical corpora. Their model also adopts BiLSTM-CRF with a transition matrix.</p></list-item><list-item><p>Yoon et al. [<xref ref-type="bibr" rid="CR14">14</xref>] proposed aggregation of multiple expert models. They named it CollaboNet, where each expert model is mapped to a BiLSTM-CRF and is trained with each distinct corpus. Likewise, each BiLSTM-CRF has a transition matrix, corresponding to pairwise feature functions.</p></list-item><list-item><p>Peters et al. [<xref ref-type="bibr" rid="CR13">13</xref>] introduced ELMo as a pre-trained model. ELMo provides contextualized word embeddings for various downstream tasks. They also trained the ELMo-enhanced BiLSTM-CRF for NER.</p></list-item><list-item><p>Lee et al. [<xref ref-type="bibr" rid="CR23">23</xref>] released BioBERT by training BERT [<xref ref-type="bibr" rid="CR24">24</xref>] for the use in the <italic>Bioinformatics</italic> domain. Similarly to ELMo, as a pre-trained model, BioBERT provides contextualized word embeddings and thus can be applied to downstream tasks. BioBERT achieved the state-of-the-art performance in several BioNER corpora.</p></list-item></list></p>
  </sec>
  <sec id="Sec8">
    <title>DTranNER: architecture and method</title>
    <p>In this section, we present the proposed framework DTranNER as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. For parameter learning, the components (i.e., Unary-Network and Pairwise-Network) of DTranNER are systematically trained via two separate CRFs (i.e., Unary-CRF and Pairwise-CRF). Once trained, Unary-Network and Pairwise-Network are combined into a CRF for BioNER label sequence prediction. First of all, we describe how to build the token embeddings in our models. Although DTranNER is not limited to a specific DL architecture in the places of the underlying networks, from now on, we evaluate our framework using BiLSTM, which has been typically adopted in a majority of NER studies.
<fig id="Fig1"><label>Fig. 1</label><caption><p>The overall architectures of the proposed framework DTranNER. <bold>a</bold> As a CRF-based framework, DTranNER is comprised of two separate, underlying deep learning-based networks: Unary-Network and Pairwise-Network are arranged to yield agreed label sequences in the prediction stage. The underlying DL-based networks of DTranNER are trained via two separate CRFs: Unary-CRF and Pairwise-CRF. <bold>b</bold> The architecture of Unary-CRF. It is dedicated to train Unary-Network. <bold>c</bold> The architecture of Pairwise-CRF. It is also committed to train Pairwise-Network. A token embedding layer is shared by Unary-Network and Pairwise-Network. A token-embedding is built upon by concatenating its traditional word embedding (denoted as “W2V”) and its contextualized token embedding (denoted as “ELMo”)</p></caption><graphic xlink:href="12859_2020_3393_Fig1_HTML" id="MO1"/></fig></p>
    <sec id="Sec9">
      <title>Token-embedding layer</title>
      <p>Given a sequence of <italic>N</italic> tokens (<italic>x</italic><sub>1</sub>,<italic>x</italic><sub>2</sub>,..., <italic>x</italic><sub><italic>N</italic></sub>), they are converted token-by-token into a series of fixed-dimensional vectors (<italic>e</italic><sub>1</sub>,<italic>e</italic><sub>2</sub>,..., <italic>e</italic><sub><italic>N</italic></sub>) via the token-embedding layer. Each token embedding is designed to encode several linguistic information of the corresponding token in the sentence. Each token embedding is thus built up by concatenating the traditional context-independent token embedding and its contextualized token embedding. These token embeddings are subsequently fed to Unary-Network and Pairwise-Network as the inputs. We do not consider additional character-level token embeddings unlike several models [<xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR21">21</xref>], because ELMo [<xref ref-type="bibr" rid="CR13">13</xref>] as our contextualized token embedding provider basically includes a character-level CNN model.</p>
      <sec id="Sec10">
        <title>Context-independent token embedding</title>
        <p>We use the pre-trained token vectors, <italic>Wiki-PubMed-PMC</italic>, created by Pyysalo et al. [<xref ref-type="bibr" rid="CR25">25</xref>] to initialize the traditional token-embedding vectors. The pre-trained token vectors were made up by being trained on three different datasets: the abstracts of the PubMed database, the full-text articles of the PubMed Central (PMC) database, and the texts of a recent Wikipedia dump. It is available at [<xref ref-type="bibr" rid="CR26">26</xref>]. We replace every out-of-vocabulary (OOV) token with a special <italic>&lt;UNK&gt;</italic> vector.</p>
      </sec>
      <sec id="Sec11">
        <title>Contextualized token embedding</title>
        <p>We employ ELMo [<xref ref-type="bibr" rid="CR13">13</xref>] for the contextualized token embeddings. Unlike context-independent token embeddings based on GloVe [<xref ref-type="bibr" rid="CR27">27</xref>] or Word2Vec [<xref ref-type="bibr" rid="CR28">28</xref>], ELMo creates context-dependent token embeddings by reconsidering the syntax and semantics of each token under its sentence-level context. In particular, we adopt the in-domain ELMo model pre-trained on the PubMed corpus, which is available at [<xref ref-type="bibr" rid="CR29">29</xref>].</p>
      </sec>
    </sec>
    <sec id="Sec12">
      <title>Unary-Network</title>
      <p>As shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>b, Unary-Network takes token embeddings as inputs, put them into its own BiLSTM layer to extract task-specific contextual information in an ordered token-level sequence, and finally produces the <italic>L</italic>-dimensional score vectors as many as the number of tokens via its binding layer. The binding layer consists of two linear transformations with an activation function and a skip connection between them. That is, the binding layer is formulated as follows:
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  {U_{i} = W_{2}^{u}(\sigma(W_{1}^{u}h_{i}^{u} + b_{1}^{u})+h_{i}^{u})+b_{2}^{u}},  $$ \end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2020_3393_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <italic>U</italic><sub><italic>i</italic></sub> denotes the <italic>L</italic>-dimensional score vector exhibiting the suitability over all possible labels on the <italic>i</italic>th token, <inline-formula id="IEq7"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$h_{i}^{u}$\end{document}</tex-math><mml:math id="M20"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq7.gif"/></alternatives></inline-formula> is the <italic>i</italic>-th hidden state from the BiLSTM layer, <inline-formula id="IEq8"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$W_{1}^{u}\in \mathbb {R}^{d\times d}$\end{document}</tex-math><mml:math id="M22"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq8.gif"/></alternatives></inline-formula> and <inline-formula id="IEq9"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$W_{2}^{u}\in \mathbb {R}^{L\times d}$\end{document}</tex-math><mml:math id="M24"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq9.gif"/></alternatives></inline-formula> are trainable weight matrices, and <inline-formula id="IEq10"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$b_{1}^{u}$\end{document}</tex-math><mml:math id="M26"><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq10.gif"/></alternatives></inline-formula> and <inline-formula id="IEq11"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$b_{2}^{u}$\end{document}</tex-math><mml:math id="M28"><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq11.gif"/></alternatives></inline-formula> are the bias vectors. Here, <inline-formula id="IEq12"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$W_{2}^{u}$\end{document}</tex-math><mml:math id="M30"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq12.gif"/></alternatives></inline-formula> projects the <italic>d</italic>-dimensional vector obtained by both the feed-forward network and the skip connection to the <italic>L</italic>-dimensional output vector. We use an ELU as the activation function <italic>σ</italic>(·). As will be explained in the following section, Unary-Network is trained via the purpose-built CRF (i.e., Unary-CRF) for the parameter learning.</p>
    </sec>
    <sec id="Sec13">
      <title>Pairwise-Network</title>
      <p>Pairwise-Network aims to extract contextual information related to pairwise labeling. This design explains why two consecutive hidden state vectors of the BiLSTM are involved in describing an edge connection in the CRF layer as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>c. Pairwise-Network therefore generates <italic>L</italic><sup>2</sup>-dimensional score vectors to match the number of possible label pairs on two tokens. We employ a bilinear model-based method [<xref ref-type="bibr" rid="CR30">30</xref>] to exploit interactive features of two neighboring hidden state vectors. This method approximates a classical three-dimensional tensor with three two-dimensional tensors, significantly reducing the number of parameters. It is shown in the following equation:
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  f_{i-1,i} = H(Q_{1}h_{i-1}^{p}\circ Q_{2}h_{i}^{p}),  $$ \end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo>∘</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2020_3393_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <italic>f</italic><sub><italic>i</italic>−1,<italic>i</italic></sub> denotes the <italic>m</italic>-dimensional vector via the bilinear model of two neighboring hidden state vectors (i.e., <inline-formula id="IEq13"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$h_{i-1}^{p}$\end{document}</tex-math><mml:math id="M34"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq13.gif"/></alternatives></inline-formula> and <inline-formula id="IEq14"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$h_{i}^{p}$\end{document}</tex-math><mml:math id="M36"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq14.gif"/></alternatives></inline-formula>) of the underlying BiLSTM layer; <inline-formula id="IEq15"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$Q_{1}\in \mathbb {R}^{c\times d}, Q_{2}\in \mathbb {R}^{c\times d}$\end{document}</tex-math><mml:math id="M38"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq15.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq16"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$H\in \mathbb {R}^{m\times c}$\end{document}</tex-math><mml:math id="M40"><mml:mi>H</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq16.gif"/></alternatives></inline-formula> are trainable matrices; and ∘ denotes Hadamard product (i.e., element-wise product of two vectors). The binding layer has a skip connection as in Unary-Network. It is thus formulated as the following equation:
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  V_{i-1,i} = W_{2}^{p}(\sigma(W_{1}^{p}f_{i-1,i} + b_{1}^{p}) + f_{i-1,i}) + b_{2}^{p},  $$ \end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2020_3393_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <inline-formula id="IEq17"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$V_{i-1,i}\in \mathbb {R}^{L^{2}}$\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq17.gif"/></alternatives></inline-formula> denotes the score vector indicating the confidence values over all label combinations on the neighboring (<italic>i</italic>−1)th and <italic>i</italic>th tokens, <inline-formula id="IEq18"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$W_{1}^{p}\in \mathbb {R}^{m\times m}$\end{document}</tex-math><mml:math id="M46"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq18.gif"/></alternatives></inline-formula> and <inline-formula id="IEq19"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$W_{2}^{p}\in \mathbb {R}^{L^{2}\times m}$\end{document}</tex-math><mml:math id="M48"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq19.gif"/></alternatives></inline-formula> are trainable weight matrices, <inline-formula id="IEq20"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$b_{1}^{p}$\end{document}</tex-math><mml:math id="M50"><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq20.gif"/></alternatives></inline-formula> and <inline-formula id="IEq21"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$b_{2}^{p}$\end{document}</tex-math><mml:math id="M52"><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq21.gif"/></alternatives></inline-formula> are the bias terms, and <italic>σ</italic>(·) is an ELU activation. Similarly to Unary-Network, Pairwise-Network is also trained via the purpose-built CRF (i.e., Pairwise-CRF) for the parameter learning.</p>
    </sec>
    <sec id="Sec14">
      <title>Model training</title>
      <p>Here, we explain how to train DTranNER. In order to facilitate the parameter learning of the two underlying networks (i.e., Unary-Network and Pairwise-Network), we establish two separate linear-chain CRFs, which are referred as Unary-CRF (Fig. <xref rid="Fig1" ref-type="fig">1</xref>b) and Pairwise-CRF (Fig. <xref rid="Fig1" ref-type="fig">1</xref>c), by allocating the two types of DL-based networks (i.e., BiLSTMs in our case) to the two purpose-built CRFs, respectively. The reason is that, when both Unary-Network and Pairwise-Network coexist in a single CRF, as Smith et al. [<xref ref-type="bibr" rid="CR31">31</xref>] and Sutton et al. [<xref ref-type="bibr" rid="CR32">32</xref>] claimed that the existence of a few indicative features can swamp the parameter learning of other weaker features, either one of the two networks starts to hold a dominant position, causing the other network to deviate from its optimal parameter learning. Our solution enables each network to notice own prediction error during the parameter learning. We explain in detail the effect of our training strategy in the Additional file <xref rid="MOESM1" ref-type="media">1</xref>.</p>
      <p>In this study, note that each of Unary- and Pairwise-CRFs is a sufficient label sequence predictor or learner; in the sense, the conditional likelihood <italic>P</italic><sub><italic>u</italic></sub> of Unary-CRF is formulated as in Eq. <xref rid="Equ2" ref-type="">2</xref>, and the conditional likelihood <italic>P</italic><sub><italic>p</italic></sub> of Pairwise-CRF given the input sequence <bold>x</bold> with the length <italic>N</italic> is formulated as the following equation:
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  P_{p}(\mathbf{y}|\mathbf{x}) = \frac{1}{Z}\exp{(\sum\limits_{i=2}^{N}V_{i-1,i}(y_{i-1},y_{i}))},  $$ \end{document}</tex-math><mml:math id="M54"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>Z</mml:mi></mml:mrow></mml:mfrac><mml:mo>exp</mml:mo><mml:mo>(</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2020_3393_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <inline-formula id="IEq22"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$Z\,=\,\sum \nolimits _{\mathbf {y}}{\exp {(\sum \nolimits _{i=2}^{N}V_{i-1,i}(y_{i-1},y_{i})}}$\end{document}</tex-math><mml:math id="M56"><mml:mi>Z</mml:mi><mml:mspace width="0.3em"/><mml:mo>=</mml:mo><mml:mspace width="0.3em"/><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:munder><mml:mo>exp</mml:mo><mml:mo>(</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq22.gif"/></alternatives></inline-formula> is the normalization constant.</p>
      <p>Rather than individually training multiple CRFs offline as in [<xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR32">32</xref>], Unary-CRF and Pairwise-CRF are jointly trained in our training strategy by maximizing their product—i.e., <inline-formula id="IEq23"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\prod {P_{\mathit {v} \in \{\mathit {u},\mathit {p}\}}(\mathbf {y}_{v}|\mathbf {x})}$\end{document}</tex-math><mml:math id="M58"><mml:mo>∏</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">v</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mi mathvariant="italic">u</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="italic">p</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2020_3393_Article_IEq23.gif"/></alternatives></inline-formula>—of the two likelihoods of Unary-CRF and Pairwise-CRF. By equivalently converting the objective function into the negative log likelihood, the optimization problem is written as the following equation:
<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \min_{\theta_{u}, \theta_{p}} \sum\limits_{e}{-\log(P_{u}(\mathbf{y}^{e}|\mathbf{x}^{e};\theta_{u})) -\log(P_{p}(\mathbf{y}^{e}|\mathbf{x}^{e};\theta_{p}))},  $$ \end{document}</tex-math><mml:math id="M60"><mml:munder><mml:mrow><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:munder><mml:mo>−</mml:mo><mml:mo>log</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mo>log</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2020_3393_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <bold>x</bold><sup><italic>e</italic></sup> and <bold>y</bold><sup><italic>e</italic></sup> denote the <italic>e</italic>th training sentence example and its ground-truth label sequence, and <italic>θ</italic><sub><italic>u</italic></sub> and <italic>θ</italic><sub><italic>p</italic></sub> denote the model parameters of Unary-CRF and Pairwise-CRF respectively.</p>
    </sec>
    <sec id="Sec15">
      <title>Prediction</title>
      <p>We explain the detail on how to infer label sequences with the trained DTranNER. Once trained via the two separate CRFs, Unary-Network and Pairwise-Network are arranged into a CRF to yield an agreed label sequence in the prediction stage. Note that Unary-Network and Pairwise-Network have distinct focuses derived by different roles, leading to learn their own specific representations. We combine them by multiplying them as a product of models [<xref ref-type="bibr" rid="CR33">33</xref>]. More specifically, all the components obtained through the aforementioned training process—Unary-Network, Pairwise-Network, and the transition matrix—are organized in a CRF, as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a. The combined model is formulated in terms of the probability for a label sequence <bold>y</bold> given an input sequence <bold>x</bold> via the following equation:
<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} P(\mathbf{y}|\mathbf{x})&amp;=P_{u}(\mathbf{y}|\mathbf{x})\cdot P_{p}(\mathbf{y}|\mathbf{x})\\ &amp;\propto\exp{(\sum\limits_{i=1}^{N}U_{i}(y_{i})+\sum\limits_{i=2}^{N}A_{i-1,i})}\cdot\exp{(\sum\limits_{i=2}^{N}V_{i-1,i}(y_{i-1},y_{i}))}\\ &amp;=\exp{(\sum\limits_{i=1}^{N}U_{i}(y_{i})+\sum\limits_{i=2}^{N}V_{i-1,i}(y_{i-1},y_{i})+\sum\limits_{i=2}^{N}A_{i-1,i})}. \end{aligned}  $$ \end{document}</tex-math><mml:math id="M62"><mml:mtable><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>∝</mml:mo><mml:mo>exp</mml:mo><mml:mo>(</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>·</mml:mo><mml:mo>exp</mml:mo><mml:mo>(</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mo>exp</mml:mo><mml:mo>(</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2020_3393_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>As a result, we obtain the most likely label sequence using the Viterbi decoding.</p>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Experimental setup</title>
    <sec id="Sec17">
      <title>Datasets</title>
      <p>We conducted our experiments with five BioNER benchmark corpora: BC2GM, BC4CHEMD, BC5CDR-chemical, BC5CDR-disease, and NCBI-Disease, which are commonly used in the existing literature [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR23">23</xref>].</p>
      <p>Table <xref rid="Tab1" ref-type="table">1</xref> shows the overall description of the five benchmark BioNER corpora. They are publicly available and can be downloaded from [<xref ref-type="bibr" rid="CR34">34</xref>]. The BioCreative II Gene Mention (<bold>BC2GM</bold>) task corpus [<xref ref-type="bibr" rid="CR35">35</xref>] consists of 20,128 sentences from biomedical publication abstracts and is annotated for mentions of the names of proteins, genes, and related entities. The BioCreative IV Chemical and Drug (<bold>BC4CHEMD</bold>) task corpus [<xref ref-type="bibr" rid="CR36">36</xref>] contains the annotations for chemical and drug mentions in 10,000 biomedical abstracts. The BioCreative V Chemical Disease Relation (<bold>BC5CDR</bold>) corpus [<xref ref-type="bibr" rid="CR37">37</xref>] is composed of mentions of chemicals and diseases that appeared in 1,500 PubMed articles. The NCBI-Disease corpus (<bold>NCBI-Disease</bold>) [<xref ref-type="bibr" rid="CR38">38</xref>] is composed of 793 PubMed abstracts annotated for disease mentions. The aforementioned corpora cover four major biomedical entity types: gene, protein, chemical, and disease.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>BioNER corpora in experiments</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Datasets</th><th align="left">Number of Sentences</th><th align="left">Entity Types</th><th align="left">Entity Counts</th><th align="left">Max Entity Length</th><th align="left">Average Entity Length</th></tr></thead><tbody><tr><td align="left">BC2GM [<xref ref-type="bibr" rid="CR35">35</xref>]</td><td align="left">20128</td><td align="left">Gene/Protein</td><td align="left">24583</td><td align="left">26 tokens</td><td align="left">2.44 tokens</td></tr><tr><td align="left">BC4CHEMD [<xref ref-type="bibr" rid="CR36">36</xref>]</td><td align="left">87682</td><td align="left">Chemical/Drug</td><td align="left">84310</td><td align="left">137 tokens</td><td align="left">2.19 tokens</td></tr><tr><td align="left">BC5CDR-Chemical [<xref ref-type="bibr" rid="CR37">37</xref>]</td><td align="left">13935</td><td align="left">Chemical/Drug</td><td align="left">15935</td><td align="left">56 tokens</td><td align="left">1.33 tokens</td></tr><tr><td align="left">BC5CDR-Disease [<xref ref-type="bibr" rid="CR37">37</xref>]</td><td align="left">13935</td><td align="left">Disease</td><td align="left">12852</td><td align="left">19 tokens</td><td align="left">1.65 tokens</td></tr><tr><td align="left">NCBI-Disease [<xref ref-type="bibr" rid="CR38">38</xref>]</td><td align="left">7284</td><td align="left">Disease</td><td align="left">6881</td><td align="left">22 tokens</td><td align="left">2.21 tokens</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec18">
      <title>Training setup</title>
      <p>In model training, we added L2 regularization penalty to the loss (i.e., Eq. <xref rid="Equ7" ref-type="">7</xref>) with the decay factor of 1×10<sup>−5</sup>. The <italic>Glorot</italic> uniform initializer of Glorot and Bengio [<xref ref-type="bibr" rid="CR39">39</xref>] is used for initializing our weight matrices, and the biases are initialized with 0. All the activation functions are ELU (exponential linear unit). We set the minibatch size of model training to ten examples across all experiments. Our models are differentiable; thereby, the CRF and its underlying neural networks can be jointly trained end-to-end by backpropagation. We use the <italic>Adam</italic> optimizer of [<xref ref-type="bibr" rid="CR40">40</xref>] with the learning rate of 0.001. In the training process, we renormalize all gradients whenever the L2 norm of the gradients exceeds 5 in every minibatch update. We applied layer normalization [<xref ref-type="bibr" rid="CR41">41</xref>] to the outputs of the token embedding layer, and also applied weight normalization [<xref ref-type="bibr" rid="CR42">42</xref>] to all the weight matrices of the binding layers of Unary-Network and Pairwise-Network. We used Dropout [<xref ref-type="bibr" rid="CR43">43</xref>] with keep probability 0.5 in both the binding layers. We established our models within at most 50 epochs for all the corpora.</p>
    </sec>
    <sec id="Sec19">
      <title>Evaluation metrics</title>
      <p>We evaluated all the methods using the precision, recall, and F1 score on the test sets of all corpora. We defined each predicted entity as correct if and only if both the entity type and the boundary were exactly matched to the ground-truth annotation. We used the python version of the evaluation script designed for CoNLL-2000 Benchmark Task, which can be downloaded from [<xref ref-type="bibr" rid="CR44">44</xref>]. To get reliable results, we repeated every test <italic>five times</italic> with different random initialization and report the arithmetic mean.</p>
    </sec>
  </sec>
  <sec id="Sec20" sec-type="results">
    <title>Results</title>
    <sec id="Sec21">
      <title>Overall performance comparison</title>
      <p>We compared DTranNER with five state-of-the-art methods: <bold>(1)</bold> Att-BiLSTM-CRF [<xref ref-type="bibr" rid="CR22">22</xref>], <bold>(2)</bold> D3NER [<xref ref-type="bibr" rid="CR12">12</xref>], <bold>(3)</bold> Collabonet [<xref ref-type="bibr" rid="CR14">14</xref>], <bold>(4)</bold> the multi-task learning-based model of Wang et al. [<xref ref-type="bibr" rid="CR11">11</xref>], and <bold>(5)</bold> BioBERT [<xref ref-type="bibr" rid="CR23">23</xref>]. Note that all the models except BioBERT employ a CRF as their top layer and rely on a static transition matrix. The performance values in terms of the <italic>precision</italic>, <italic>recall</italic>, and <italic>F1</italic>-score over all the corpora are presented in Table <xref rid="Tab2" ref-type="table">2</xref>. DTranNER outperformed the current state-of-the-art models on four of five corpora—BC2GM, BC4CHEMD, BC5CDR-Disease, and BC5CDR-Chemical—in terms of F1 scores.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Performance values in terms of the <italic>precision</italic> (%), <italic>recall</italic> (%) and <italic>F1</italic>-score (%) for the state-of-the-art methods and the proposed model <bold>DTranNER</bold></p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Corpus</th><th align="left" colspan="3">BC2GM</th><th align="left" colspan="3">BC4CHEMD</th><th align="left" colspan="3">BC5CDR-Chemical</th><th align="left" colspan="3">BC5CDR-Disease</th><th align="left" colspan="3">NCBI-Disease</th></tr><tr><th align="left"/><th align="left">P</th><th align="left">R</th><th align="left">F1</th><th align="left">P</th><th align="left">R</th><th align="left">F1</th><th align="left">P</th><th align="left">R</th><th align="left">F1</th><th align="left">P</th><th align="left">R</th><th align="left">F1</th><th align="left">P</th><th align="left">R</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">Att-BiLSTM-CRF (2017)</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left"><bold>9</bold><bold>2</bold><bold>.</bold><bold>2</bold><bold>9</bold></td><td align="left">90.01</td><td align="left">91.14</td><td align="left">93.49</td><td align="left">91.68</td><td align="left">92.57</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">-</td></tr><tr><td align="left">D3NER (2018)</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">93.73</td><td align="left">92.56</td><td align="left">93.14</td><td align="left">83.98</td><td align="left">85.40</td><td align="left">84.68</td><td align="left">85.03</td><td align="left">83.80</td><td align="left">84.41</td></tr><tr><td align="left">Collabonet (2018)</td><td align="left">80.49</td><td align="left">78.99</td><td align="left">79.73</td><td align="left">90.78</td><td align="left">87.01</td><td align="left">88.85</td><td align="left">94.26</td><td align="left">92.38</td><td align="left">93.31</td><td align="left">85.61</td><td align="left">82.61</td><td align="left">84.08</td><td align="left">85.48</td><td align="left">87.27</td><td align="left">86.36</td></tr><tr><td align="left">Wang et al. (2018)</td><td align="left">82.10</td><td align="left">79.42</td><td align="left">80.74</td><td align="left">91.30</td><td align="left">87.53</td><td align="left">89.37</td><td align="left">93.56</td><td align="left">92.48</td><td align="left">93.03</td><td align="left">84.14</td><td align="left">85.76</td><td align="left">84.95</td><td align="left">85.86</td><td align="left">86.42</td><td align="left">86.14</td></tr><tr><td align="left">BioBERT (2019)</td><td align="left"><bold>8</bold><bold>5</bold><bold>.</bold><bold>1</bold><bold>6</bold></td><td align="left">83.65</td><td align="left">84.40</td><td align="left">92.23</td><td align="left">90.61</td><td align="left">91.41</td><td align="left">93.27</td><td align="left">93.61</td><td align="left">93.44</td><td align="left">85.86</td><td align="left">87.27</td><td align="left">86.56</td><td align="left"><bold>8</bold><bold>9</bold><bold>.</bold><bold>0</bold><bold>4</bold></td><td align="left"><bold>8</bold><bold>9</bold><bold>.</bold><bold>6</bold><bold>9</bold></td><td align="left"><bold>8</bold><bold>9</bold><bold>.</bold><bold>3</bold><bold>6</bold></td></tr><tr><td align="left"><bold>DTranNER</bold></td><td align="left">84.21</td><td align="left"><bold>8</bold><bold>4</bold><bold>.</bold><bold>8</bold><bold>4</bold></td><td align="left"><bold>8</bold><bold>4</bold><bold>.</bold><bold>5</bold><bold>6</bold></td><td align="left">91.94</td><td align="left"><bold>9</bold><bold>2</bold><bold>.</bold><bold>0</bold><bold>4</bold></td><td align="left"><bold>9</bold><bold>1</bold><bold>.</bold><bold>9</bold><bold>9</bold></td><td align="left"><bold>9</bold><bold>4</bold><bold>.</bold><bold>2</bold><bold>8</bold></td><td align="left"><bold>9</bold><bold>4</bold><bold>.</bold><bold>0</bold><bold>4</bold></td><td align="left"><bold>9</bold><bold>4</bold><bold>.</bold><bold>1</bold><bold>6</bold></td><td align="left"><bold>8</bold><bold>6</bold><bold>.</bold><bold>7</bold><bold>5</bold></td><td align="left"><bold>8</bold><bold>7</bold><bold>.</bold><bold>7</bold><bold>0</bold></td><td align="left"><bold>8</bold><bold>7</bold><bold>.</bold><bold>2</bold><bold>2</bold></td><td align="left">88.21</td><td align="left">89.04</td><td align="left">88.62</td></tr></tbody></table><table-wrap-foot><p><italic>Note:</italic> The highest performance in each corpus is highlighted in <bold>Bold</bold>. We quoted the published scores for the other models. For Wang et al. [<xref ref-type="bibr" rid="CR11">11</xref>], we conducted additional experiments to obtain the performance scores for two corpora (i.e., BC5CDR-Chemical and BC5CDR-Disease) using the software on their open source repository [<xref ref-type="bibr" rid="CR45">45</xref>]</p></table-wrap-foot></table-wrap></p>
      <p>DTranNER achieved a much higher F1 score with higher precision than the current best performer (94.16% vs. 93.44%) for BC5CDR-Chemical, where its NER process was confused owing to many abbreviations despite its shorter average entity length as shown in Table <xref rid="Tab1" ref-type="table">1</xref>. Thus, the pairwise transition network of DTranNER is shown to be advantageous in discovering abbreviation-formed entities.</p>
    </sec>
    <sec id="Sec22">
      <title>Ablation studies</title>
      <p>We investigated the effectiveness of main components of our proposed method DTranNER through ablation studies.</p>
      <sec id="Sec23">
        <title>Impact of unary- and pairwise-Networks</title>
        <p>To investigate the contribution of Unary- and Pairwise-Networks to DTranNER, we trained experimental models by deactivating each component (i.e., either Unary-Network or Pairwise-Network) in turn from DTranNER and then measured the performance of the variant models on three benchmark corpora: BC5CDR-Chemical, BC5CDR-Disease, and NCBI-Disease. The results are shown in Table <xref rid="Tab3" ref-type="table">3</xref>.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Impact of Unary-Network and Pairwise-Network in terms of the F1-score (%)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Settings</th><th align="left">BC5CDR-Chemical</th><th align="left">BC5CDR-Disease</th><th align="left">NCBI-Disease</th></tr></thead><tbody><tr><td align="left">Unary-CRF</td><td align="left">93.01</td><td align="left">86.14</td><td align="left">86.94</td></tr><tr><td align="left">Pairwise-CRF</td><td align="left">93.27</td><td align="left">86.05</td><td align="left">86.71</td></tr><tr><td align="left">Unary+Pairwise ensemble</td><td align="left">93.25</td><td align="left">86.78</td><td align="left">87.09</td></tr><tr><td align="left">DTranNER</td><td align="left">94.16</td><td align="left">87.22</td><td align="left">88.62</td></tr></tbody></table><table-wrap-foot><p><italic>Note:</italic> “Unary-CRF” denotes a variant model excluding Pairwise-Network from DTranNER, “Pairwise-CRF” denotes a variant model excluding Unary-Network from DTranNER, and “Unary+Pairwise ensemble” is an ensemble model of “Unary-CRF” and “Pairwise-CRF.” In the ensemble model, “Unary-CRF” and “Pairwise-CRF” were independently trained, and they voted over the sequence predictions by their prediction scores</p></table-wrap-foot></table-wrap></p>
        <p>The removal of either Unary-Network or Pairwise-Network from DTranNER caused the overall performance degradation in all the corpora by up to 1.91 percent points. That is, this ablation study presents that the performance achievement of DTranNER is attributed to not only an individual component but also the mutual collaboration of Unary-Network and Pairwise-Network. The relative importance between the two networks was not very clear.</p>
        <p>We also compared DTranNER with an ensemble model of Unary-CRF and Pairwise-CRF, denoted as “Unary+Pairwise ensemble,” which were separately trained. The sequence prediction of the ensemble model was decided by voting with their sequence output scores. As shown in Table <xref rid="Tab3" ref-type="table">3</xref>, the performance improvement of the ensemble model was marginal in BC5CDR-Chemical and NCBI-Disease. More important, the ensemble model was much worse than DTranNER in all corpora. This result indicates that yielding agreed label sequences between the two networks, which have separate views, as in DTranNER is more effective than their ensemble via simple voting.</p>
      </sec>
      <sec id="Sec24">
        <title>Impact of separate BiLSTM layers of Unary- and Pairwise networks</title>
        <p>Unary-Network and Pairwise-Network have an independent underlying layer which learns its role-specific representations. We investigate the impact of the separate underlying layers in the peer networks. For this purpose, we additionally built a variant model of DTranNER, denoted as “DTranNER-shared,” that forced Unary-Network and Pairwise-Network to share the parameters of their BiLSTM layers. As shown in Table <xref rid="Tab4" ref-type="table">4</xref> for the comparison result, it turned out that Unary-Network and Pairwise-Network benefit from the exclusive underlying layer.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Impact of separate BiLSTM layers in terms of the F1-score (%)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Settings</th><th align="left">BC2GM</th><th align="left">BC5CDR-Chemical</th><th align="left">BC5CDR-Disease</th><th align="left">NCBI-Disease</th></tr></thead><tbody><tr><td align="left">DTranNER-shared</td><td align="left">83.69</td><td align="left">93.57</td><td align="left">86.75</td><td align="left">88.01</td></tr><tr><td align="left">DTranNER</td><td align="left">84.56</td><td align="left">94.16</td><td align="left">87.22</td><td align="left">88.62</td></tr></tbody></table><table-wrap-foot><p><italic>Note:</italic> “DTranNER-shared” is a variant model that shares the BiLSTM layer in “Unary-Network” and “Pairwise-Network.”</p></table-wrap-foot></table-wrap></p>
      </sec>
      <sec id="Sec25">
        <title>Embedding layer</title>
        <p>We here investigate the impact of each element in the token embedding layer of DTranNER. For this purpose, we built two variants of DTranNER: (1) a model (denoted as “W2V”) whose token embedding consists of only 200-dimensional pre-trained token embedding [<xref ref-type="bibr" rid="CR26">26</xref>] and (2) another model (denoted as “ELMo”) whose token embedding is solely comprised of 1024-dimensional ELMo embedding, which is obtained from the ELMo model [<xref ref-type="bibr" rid="CR29">29</xref>] pre-trained on the PubMed corpus. The comparison results are presented in Table <xref rid="Tab5" ref-type="table">5</xref>. The context-dependent token embeddings via the ELMo model bring significant performance improvement on the four benchmark corpora, especially on NCBI-Disease. Nevertheless, the best performance is consistently achieved by the combination of the context-dependent ELMo embedding and the traditional context-independent embedding.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Impact of each component in the token embedding composition in terms of the F1-score (%)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Settings</th><th align="left">BC2GM</th><th align="left">BC5CDR-Chemical</th><th align="left">BC5CDR-Disease</th><th align="left">NCBI-Disease</th></tr></thead><tbody><tr><td align="left">W2V</td><td align="left">82.03</td><td align="left">92.64</td><td align="left">85.17</td><td align="left">84.88</td></tr><tr><td align="left">ELMo</td><td align="left">83.41</td><td align="left">93.78</td><td align="left">86.76</td><td align="left">88.27</td></tr><tr><td align="left">ELMo + W2V(=DTranNER)</td><td align="left">84.56</td><td align="left">94.16</td><td align="left">87.22</td><td align="left">88.62</td></tr></tbody></table><table-wrap-foot><p><italic>Note:</italic> “W2V” is a variant model of DTranNER whose embedding layer uses only traditional context-independent token vectors (i.e., <italic>Wiki-PubMed-PMC</italic> [<xref ref-type="bibr" rid="CR25">25</xref>]), “ELMo” is another variant model of DTranNER whose embedding layer uses only ELMo, and “ELMo + W2V” is equivalent to DTranNER</p></table-wrap-foot></table-wrap></p>
      </sec>
    </sec>
    <sec id="Sec26">
      <title>Case studies</title>
      <p>To demonstrate the advantage of the DL-based label-label transition model, which is the main feature of DTranNER, we compared several example outcomes yielded by DTranNER and Unary-CRF as shown in Table <xref rid="Tab6" ref-type="table">6</xref>. Note that Unary-CRF is not equipped with this main feature. In addition, the label sequence predictions of DTranNER in Table <xref rid="Tab6" ref-type="table">6</xref> coincide with the ground-truth annotations.
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Case study of the label sequence prediction performed by DTranNER and Unary-CRF</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" colspan="3">Diseases/Chemicals</td></tr><tr><td align="left">Case 1</td><td align="left">Unary-CRF</td><td align="left">to enable diagnosis of <underline>ureteric stones</underline> or obstruction in patients with <underline>HIV infection</underline> who receive <underline>indinavir</underline> theraphy</td></tr><tr><td align="left"/><td align="left">DTranNER</td><td align="left">to enable diagnosis of <underline>ureteric stones or obstruction</underline> in patients with <underline>HIV infection</underline> who receive <underline>indinavir</underline> theraphy</td></tr><tr><td align="left">Case 2</td><td align="left">Unary-CRF</td><td align="left">The present study was designed to investigate whether nociceptin / <underline>orphanin</underline> FQ and <underline>nocistatin</underline> could modulate</td></tr><tr><td align="left"/><td align="left"/><td align="left"><underline>impairment of learning and memory</underline> induced by <underline>scopolamine</underline></td></tr><tr><td align="left"/><td align="left">DTranNER</td><td align="left">The present study was designed to investigate whether <underline>nociceptin</underline> / <underline>orphanin FQ</underline> and <underline>nocistatin</underline> could modulate</td></tr><tr><td align="left"/><td align="left"/><td align="left"><underline>impairment of learning and memory</underline> induced by <underline>scopolamine</underline></td></tr><tr><td align="left">Case 3</td><td align="left">Unary-CRF</td><td align="left">We report the case of a female patient with <underline>rheumatoid arthritis</underline> who developed <underline>acute cytolytic hepatitis</underline> due to <underline>meloxicam</underline></td></tr><tr><td align="left"/><td align="left">DTranNER</td><td align="left">We report the case of a female patient with <underline>rheumatoid arthritis</underline> who developed acute cytolytic <underline>hepatitis</underline> due to <underline>meloxicam</underline></td></tr><tr><td align="left">Case 4</td><td align="left">Unary-CRF</td><td align="left">Reduced <underline>nicotinamide adenine</underline> dinucleotide phosphate - diaphorase (NADPH - d) histochemistry was also employed to</td></tr><tr><td align="left"/><td align="left">DTranNER</td><td align="left">Reduced <underline>nicotinamide adenine dinucleotide phosphate</underline> - diaphorase (<underline>NADPH</underline> - d) histochemistry was also employed to</td></tr><tr><td align="left" colspan="3">Genes/Proteins</td></tr><tr><td align="left">Case 5</td><td align="left">Unary-CRF</td><td align="left">The MIC90 of ABK against coagulase type IV strains was rather high, 12.5 micrograms/ml</td></tr><tr><td align="left"/><td align="left">DTranNER</td><td align="left">The MIC90 of ABK against <underline>coagulase type IV</underline> strains was rather high, 12.5 micrograms/ml</td></tr><tr><td align="left">Case 6</td><td align="left">Unary-CRF</td><td align="left">subtle differences between individual subunits that lead to species - specific properties of <underline>RNA polymerase</underline> I transcription</td></tr><tr><td align="left"/><td align="left">DTranNER</td><td align="left">subtle differences between individual subunits that lead to species - specific properties of <underline>RNA polymerase I</underline> transcription</td></tr><tr><td align="left">Case 7</td><td align="left">Unary-CRF</td><td align="left"><underline>The S. typhimurium aspartyl / asparaginyl beta - hydroxylase</underline> homologue (designated <underline>lpxO</underline>) was cloned into</td></tr><tr><td align="left"/><td align="left">DTranNER</td><td align="left"><underline>The S. typhimurium aspartyl / asparaginyl beta - hydroxylase homologue</underline> (designated <underline>lpxO</underline>) was cloned into</td></tr></tbody></table><table-wrap-foot><p><italic>Note</italic>: Unary-CRF is the purpose-built model excluding Pairwise-Network from DTranNER. The named entities inferred by each model are underlined in sentences</p></table-wrap-foot></table-wrap></p>
      <p>For Case 1, Unary-CRF failed to detect one of the boundaries of the disease-type entity “ureteric stones or obstruction” because of the intervention of the inner conjunction “or,” while DTranNER precisely determined both boundaries. For Case 2, Unary-CRF failed to identify the chemical-type entities enumerated via the conjunctions “/” and “and,” whereas DTranNER exactly identified all the separate terms. For Case 3, Unary-CRF failed to determine the left boundary of the single-token entity “hepatitis” by mistakenly regarding “acute” and “cytolytic” as its constituent elements, whereas DTranNER exactly distinguished them from this entity by understanding the contextual relations. For Case 4, DTranNER correctly identified the two entities, where the latter is the abbreviation of the former, but Unary-CRF failed. For Case 5, Unary-CRF ignored the gene-type entity “coagulase type IV” by mistakenly regarding “type” and “IV” as generic terms, whereas DTranNER correctly identified it by reflecting the contextual correlations between its constituent elements. For Case 6, DTranNER correctly identified both boundaries of the gene-type entity “RNA polymerase I” by exploiting the contextual clues on the consecutive pairs, 〈“polymerase” and “I” 〉 and 〈“I” and “transcription” 〉, though “I” solely looks ambiguous; in contrast, Unary-CRF failed to determine the right boundary because it classified “I” as a generic term. For Case 7, DTranNER correctly extracted the lengthy entity by grasping the correlation between the neighboring tokens (i.e., “hydroxylase” and “homologue”), whereas Unary-CRF failed to handle this lengthy entity.</p>
      <p>Summing up, DTranNER successfully supports various cases which would be very difficult without the contextual information, and these cases indeed show the benefit of DTranNER for BioNER.</p>
    </sec>
  </sec>
  <sec id="Sec27" sec-type="conclusion">
    <title>Conclusion</title>
    <p>In this paper, we proposed a novel framework for BioNER, for which we call <bold>DTranNER</bold>. The main novelty lies in that DTranNER learns the label-label transition relations with deep learning in consideration of the context in an input sequence. DTranNER possesses two separate DL-based networks: Unary-Network and Pairwise-Network; the former focuses on individual labeling, while the latter is dedicated to assess the transition suitability between labels. Once established via our training strategy, these networks are integrated into the CRF of DTranNER to yield agreed label sequences in the prediction step. In other words, DTranNER creates the synergy leveraging different knowledge obtained from the two underlying DL-based networks. As a result, DTranNER outperformed the best existing model in terms of the F1-score on four of five popular benchmark corpora. We are extending DTranNER to utilize unlabeled biomedical data. This extension is meaningful in several aspects: (1) building a more-generalized model using a wide range of biomedical literature, (2) rapidly incorporating up-to-date biomedical literature by skipping time-consuming annotation, and (3) reducing annotation cost.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec28">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2020_3393_MOESM1_ESM.pdf">
            <caption>
              <p><bold>Additional file 1</bold> Model training strategy in the proposed framework. The effect of the proposed model training strategy is not shown in the main manuscript. The learning curves during model training demonstrate the reason why the training strategy facilitates the parameter learning of the underlying deep learning-based models under our proposed framework.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>BiLSTM</term>
        <def>
          <p>Bidirectional long short-term memory</p>
        </def>
      </def-item>
      <def-item>
        <term>BioNER</term>
        <def>
          <p>Biomedical named entity recognition</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p>Convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>CRF</term>
        <def>
          <p>Conditional random field</p>
        </def>
      </def-item>
      <def-item>
        <term>DL</term>
        <def>
          <p>Deep learning</p>
        </def>
      </def-item>
      <def-item>
        <term>NER</term>
        <def>
          <p>Named entity recognition</p>
        </def>
      </def-item>
      <def-item>
        <term>NLP</term>
        <def>
          <p>Natural language processing</p>
        </def>
      </def-item>
      <def-item>
        <term>POS</term>
        <def>
          <p>Part-of-speech</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> accompanies this paper at 10.1186/s12859-020-3393-1.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>SK and JG designed the study. SK implemented the model, performed experiments and analyses. SK drafted the manuscript. JG revised it. All authors have read and approved the final version of this manuscript. All data generated or analysed during this study are included in this published article (and its Additional file <xref rid="MOESM1" ref-type="media">1</xref>).</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (Ministry of Science and ICT) (No. 2017R1E1A1A01075927).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The code for our models and instructions for the use can be found on GitHub <ext-link ext-link-type="uri" xlink:href="https://github.com/kaist-dmlab/BioNER">https://github.com/kaist-dmlab/BioNER</ext-link>. The datasets used for performance evaluation and analysis during the current study are available in the MTL-Bioinformatics-2016 repository, <ext-link ext-link-type="uri" xlink:href="https://github.com/cambridgeltl/MTL-Bioinformatics-2016">https://github.com/cambridgeltl/MTL-Bioinformatics-2016</ext-link>.</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gurulingappa</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Mateen-Rajpu</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Toldo</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Extraction of potential adverse drug events from medical case reports</article-title>
        <source>J Biomed Semant</source>
        <year>2012</year>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>15</fpage>
        <pub-id pub-id-type="doi">10.1186/2041-1480-3-15</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bossy</surname>
            <given-names>Robert</given-names>
          </name>
          <name>
            <surname>Jourde</surname>
            <given-names>Julien</given-names>
          </name>
          <name>
            <surname>Manine</surname>
            <given-names>Alain-Pierre</given-names>
          </name>
          <name>
            <surname>Veber</surname>
            <given-names>Philippe</given-names>
          </name>
          <name>
            <surname>Alphonse</surname>
            <given-names>Erick</given-names>
          </name>
          <name>
            <surname>van de Guchte</surname>
            <given-names>Maarten</given-names>
          </name>
          <name>
            <surname>Bessi.res</surname>
            <given-names>Philippe</given-names>
          </name>
          <name>
            <surname>N.dellec</surname>
            <given-names>Claire</given-names>
          </name>
        </person-group>
        <article-title>BioNLP Shared Task - The Bacteria Track</article-title>
        <source>BMC Bioinformatics</source>
        <year>2012</year>
        <volume>13</volume>
        <issue>Suppl 11</issue>
        <fpage>S3</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-13-S11-S3</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Predicting potential drug-drug interactions by integrating chemical, biological, phenotypic and network data</article-title>
        <source>BMC Bioinformatics</source>
        <year>2017</year>
        <volume>18</volume>
        <issue>1</issue>
        <fpage>18</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-016-1415-9</pub-id>
        <pub-id pub-id-type="pmid">28056782</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Szklarczyk</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Franceschini</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wyder</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Forslund</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Heller</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Huerta-Cepas</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Simonovic</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Roth</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Santos</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tsafou</surname>
            <given-names>KP</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>STRING v10: protein–protein interaction networks, integrated over the tree of life</article-title>
        <source>Nucleic Acids Res</source>
        <year>2014</year>
        <volume>43</volume>
        <issue>D1</issue>
        <fpage>447</fpage>
        <lpage>52</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gku1003</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <mixed-citation publication-type="other">Lafferty J, McCallum A, Pereira FC. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In: Proceedings of the 18th International Conference on Machine Learning. ACM: 2001. p. 282–9. <ext-link ext-link-type="uri" xlink:href="http://portal.acm.org/citation.cfm?id=655813">http://portal.acm.org/citation.cfm?id=655813</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sutton</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>McCallum</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>An introduction to conditional random fields</article-title>
        <source>Found Trends® Mach Learn</source>
        <year>2012</year>
        <volume>4</volume>
        <issue>4</issue>
        <fpage>267</fpage>
        <lpage>373</lpage>
        <pub-id pub-id-type="doi">10.1561/2200000013</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <mixed-citation publication-type="other">Settles B. Biomedical named entity recognition using conditional random fields and rich feature sets. In: Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and Its Applications: 2004. p. 104–7. 10.3115/1567594.1567618.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>C-H</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>tmchem: a high performance approach for chemical named entity recognition and normalization</article-title>
        <source>J Cheminformatics</source>
        <year>2015</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>3</fpage>
        <pub-id pub-id-type="doi">10.1186/1758-2946-7-S1-S3</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <mixed-citation publication-type="other">Lample G, Ballesteros M, Subramanian S, Kawakami K, Dyer C. Neural architectures for named entity recognition. In: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: 2016. p. 260–70. 10.18653/v1/n16-1030.</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Habibi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Weber</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Neves</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wiegandt</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Leser</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>Deep learning with word embeddings improves biomedical named entity recognition</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <issue>14</issue>
        <fpage>37</fpage>
        <lpage>48</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx228</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Xuan</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Yu</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>Xiang</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Yuhao</given-names>
          </name>
          <name>
            <surname>Zitnik</surname>
            <given-names>Marinka</given-names>
          </name>
          <name>
            <surname>Shang</surname>
            <given-names>Jingbo</given-names>
          </name>
          <name>
            <surname>Langlotz</surname>
            <given-names>Curtis</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>Jiawei</given-names>
          </name>
        </person-group>
        <article-title>Cross-type biomedical named entity recognition with deep multi-task learning</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>35</volume>
        <issue>10</issue>
        <fpage>1745</fpage>
        <lpage>1752</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty869</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dang</surname>
            <given-names>TH</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>H. -Q.</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>TM</given-names>
          </name>
          <name>
            <surname>Vu</surname>
            <given-names>ST</given-names>
          </name>
        </person-group>
        <article-title>D3NER: biomedical named entity recognition using CRF-biLSTM improved with fine-tuned embeddings of various linguistic information</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>20</issue>
        <fpage>3539</fpage>
        <lpage>46</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty356</pub-id>
        <pub-id pub-id-type="pmid">29718118</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <mixed-citation publication-type="other">Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, Zettlemoyer L. Deep contextualized word representations. In: Proc. of NAACL. Association for Computational Linguistics (ACL): 2018. <ext-link ext-link-type="uri" xlink:href="https://www.aclweb.org/anthology/N18-1202/">https://www.aclweb.org/anthology/N18-1202/</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <mixed-citation publication-type="other">Yoon W, So CH, Lee1 J, Kang J. Collabonet: collaboration of deep neural networks for biomedical named entity recognition. BMC Bioinformatics. 2019; 20(10):249.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Recognizing names in biomedical texts: a machine learning approach</article-title>
        <source>Bioinformatics</source>
        <year>2004</year>
        <volume>20</volume>
        <issue>7</issue>
        <fpage>1178</fpage>
        <lpage>90</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bth060</pub-id>
        <pub-id pub-id-type="pmid">14871877</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>van den Hengel</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Reid</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Exploring context with deep structured models for semantic segmentation</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>2018</year>
        <volume>40</volume>
        <issue>6</issue>
        <fpage>1352</fpage>
        <lpage>66</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2708714</pub-id>
        <pub-id pub-id-type="pmid">28574343</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <mixed-citation publication-type="other">Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate: 2014. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. In: Advances in Neural Information Processing Systems. Neural Information Processing Systems Foundation: 2017. p. 5998–6008. <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>80</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chiu</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Nichols</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Named entity recognition with bidirectional LSTM-CNNs</article-title>
        <source>Trans Assoc Comput Linguist</source>
        <year>2016</year>
        <volume>4</volume>
        <fpage>357</fpage>
        <lpage>70</lpage>
        <pub-id pub-id-type="doi">10.1162/tacl_a_00104</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <mixed-citation publication-type="other">Ma X, Hovy E. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics: 2016. p. 1064–74. 10.18653/v1/p16-1101.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Luo</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>An attention-based BiLSTM-CRF approach to document-level chemical named entity recognition</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>34</volume>
        <issue>8</issue>
        <fpage>1381</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx761</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <mixed-citation publication-type="other">Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, Kang J. Biobert: a pre-trained biomedical language representation model for biomedical text mining. arXiv preprint. 2019. arXiv:1901.08746.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <mixed-citation publication-type="other">Devlin J, Chang M. -W., Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint. 2018. arXiv:1810.04805.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <mixed-citation publication-type="other">Pyysalo S, Ginter F, Moen H, Salakoski T, Ananiadou S. Distributional semantics resources for biomedical text processing. In: Proceedings of the Fifth International Symposium on Languages in Biology and Medicine. Database Center for Life Science: 2013. p. 39–44. <ext-link ext-link-type="uri" xlink:href="https://pdfs.semanticscholar.org/e2f2/8568031e1902d4f8ee818261f0f2c20de6dd.pdf">https://pdfs.semanticscholar.org/e2f2/8568031e1902d4f8ee818261f0f2c20de6dd.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <mixed-citation publication-type="other">Sampo Pyysalo FilipGinter Hans Moen. Word vectors for biomedical natural language processing. 2013. <ext-link ext-link-type="uri" xlink:href="http://evexdb.org/pmresources/vec-space-models/">http://evexdb.org/pmresources/vec-space-models/</ext-link>. Accessed 22 Aug 2019.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <mixed-citation publication-type="other">Pennington J, Socher R, Manning CD. Glove: Global vectors for word representation. In: Empirical Methods in Natural Language Processing (EMNLP): 2014. p. 1532–43. <ext-link ext-link-type="uri" xlink:href="http://www.aclweb.org/anthology/D14-1162">http://www.aclweb.org/anthology/D14-1162</ext-link>. https://doi.org/10.3115/v1/d14-1162.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <mixed-citation publication-type="other">Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their compositionality. In: Advances in Neural Information Processing Systems. Neural Information Processing Systems Foundation: 2013. p. 3111–9. <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <mixed-citation publication-type="other">Matthew E. Peters. PubMed-based ELMo Model. 2018. https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/contributed/pubmed/elmo_2x4096_512_2048cnn_ 2xhighway_weights_PubMed_only.hdf5. Accessed 22 Aug 2019.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <mixed-citation publication-type="other">Kim J-H, On KW, Lim W, Kim J, Ha J-W, Zhang B-T. Hadamard Product for Low-rank Bilinear Pooling. In: The 5th International Conference on Learning Representations. International Conference on Learning Representations (ICLR): 2017. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1610.04325">https://arxiv.org/abs/1610.04325</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <mixed-citation publication-type="other">Smith A, Cohn T, Osborne M. Logarithmic opinion pools for conditional random fields. In: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics: 2005. p. 18–25. 10.3115/1219840.1219843.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <mixed-citation publication-type="other">Sutton C, Sindelar M, McCallum A. Reducing weight undertraining in structured discriminative learning. In: Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics. Association for Computational Linguistics: 2006. p. 89–95. 10.3115/1220835.1220847.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hinton</surname>
            <given-names>GE</given-names>
          </name>
        </person-group>
        <article-title>Training products of experts by minimizing contrastive divergence</article-title>
        <source>Neural Comput</source>
        <year>2002</year>
        <volume>14</volume>
        <issue>8</issue>
        <fpage>1771</fpage>
        <lpage>800</lpage>
        <pub-id pub-id-type="doi">10.1162/089976602760128018</pub-id>
        <pub-id pub-id-type="pmid">12180402</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <mixed-citation publication-type="other">Gamal Crichton SampoPyysalo Billy Chiu and Anna Korhonen. MTL-Bioinformatics-2016. 2016. <ext-link ext-link-type="uri" xlink:href="https://github.com/cambridgeltl/MTL-Bioinformatics-2016">https://github.com/cambridgeltl/MTL-Bioinformatics-2016</ext-link>. Accessed 22 Aug 2019.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Smith</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Tanabe</surname>
            <given-names>LK</given-names>
          </name>
          <name>
            <surname>nee Ando</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Kuo</surname>
            <given-names>C-J</given-names>
          </name>
          <name>
            <surname>Chung</surname>
            <given-names>I-F</given-names>
          </name>
          <name>
            <surname>Hsu</surname>
            <given-names>C-N</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Y-S</given-names>
          </name>
          <name>
            <surname>Klinger</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Friedrich</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Ganchev</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Overview of biocreative ii gene mention recognition</article-title>
        <source>Genome Biol</source>
        <year>2008</year>
        <volume>9</volume>
        <issue>2</issue>
        <fpage>2</fpage>
        <pub-id pub-id-type="doi">10.1186/gb-2008-9-s2-s2</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krallinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rabal</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Leitner</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Vazquez</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Salgado</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lowe</surname>
            <given-names>DM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The chemdner corpus of chemicals and drugs and its annotation principles</article-title>
        <source>J Cheminformatics</source>
        <year>2015</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>2</fpage>
        <pub-id pub-id-type="doi">10.1186/1758-2946-7-S1-S2</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37</label>
      <mixed-citation publication-type="other">Li J, Sun Y, Johnson R, Sciaky D, Wei C. -H., Leaman R, Davis AP, Mattingly CJ, Wiegers TC, Lu Z. Annotating chemicals, diseases, and their interactions in biomedical literature. In: Proceedings of the Fifth BioCreative Challenge Evaluation Workshop: 2015. p. 173–82. <ext-link ext-link-type="uri" xlink:href="https://pdfs.semanticscholar.org/eb1c/c140b14d0a8f5f789ba26e5e497a9776dd7e.pdf">https://pdfs.semanticscholar.org/eb1c/c140b14d0a8f5f789ba26e5e497a9776dd7e.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Doğan</surname>
            <given-names>RI</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Ncbi disease corpus: a resource for disease name recognition and concept normalization</article-title>
        <source>J Biomed Informa</source>
        <year>2014</year>
        <volume>47</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2013.12.006</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39</label>
      <mixed-citation publication-type="other">Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. International Conference on Artificial Intelligence and Statistics (AISTATS): 2010. p. 249–56. <ext-link ext-link-type="uri" xlink:href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J. Adam: A method for stochastic optimization. In: Proceedings of the 3rd International Conference on Learning Representations. International Conference on Learning Representations (ICLR): 2015. <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41</label>
      <mixed-citation publication-type="other">Lei Ba J, Kiros JR, Hinton GE. Layer normalization. arXiv preprint. 2016. arXiv:1607.06450.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42</label>
      <mixed-citation publication-type="other">Salimans T, Kingma DP. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In: Advances in Neural Information Processing Systems. Neural Information Processing Systems Foundation: 2016. p. 901–9. http://papers.nips.cc/paper/ 6113-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dropout: A simple way to prevent neural networks from overfitting</article-title>
        <source>J Mach Learn Res</source>
        <year>2014</year>
        <volume>15</volume>
        <fpage>1929</fpage>
        <lpage>58</lpage>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44</label>
      <mixed-citation publication-type="other">Sampo Pyysalo. Python version of the evaluation script from CoNLL 2000. 2016. <ext-link ext-link-type="uri" xlink:href="https://github.com/spyysalo/conlleval.py">https://github.com/spyysalo/conlleval.py</ext-link>. Accessed 22 Aug 2019.</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Xuan</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Yu</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>Xiang</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Yuhao</given-names>
          </name>
          <name>
            <surname>Zitnik</surname>
            <given-names>Marinka</given-names>
          </name>
          <name>
            <surname>Shang</surname>
            <given-names>Jingbo</given-names>
          </name>
          <name>
            <surname>Langlotz</surname>
            <given-names>Curtis</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>Jiawei</given-names>
          </name>
        </person-group>
        <article-title>Cross-type biomedical named entity recognition with deep multi-task learning</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>35</volume>
        <issue>10</issue>
        <fpage>1745</fpage>
        <lpage>1752</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty869</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
