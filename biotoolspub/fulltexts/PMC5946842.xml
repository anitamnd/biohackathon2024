<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5946842</article-id>
    <article-id pub-id-type="pmid">29280997</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btx818</article-id>
    <article-id pub-id-type="publisher-id">btx818</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Sequence Analysis</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepSig: deep learning improves signal peptide detection in proteins</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-7359-0633</contrib-id>
        <name>
          <surname>Savojardo</surname>
          <given-names>Castrense</given-names>
        </name>
        <xref ref-type="aff" rid="btx818-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Martelli</surname>
          <given-names>Pier Luigi</given-names>
        </name>
        <xref ref-type="aff" rid="btx818-aff1">1</xref>
        <xref ref-type="corresp" rid="btx818-cor1"/>
        <!--<email>pierluigi.martelli@unibo.it</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fariselli</surname>
          <given-names>Piero</given-names>
        </name>
        <xref ref-type="aff" rid="btx818-aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Casadio</surname>
          <given-names>Rita</given-names>
        </name>
        <xref ref-type="aff" rid="btx818-aff1">1</xref>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Valencia</surname>
          <given-names>Alfonso</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <aff id="btx818-aff1"><label>1</label>Biocomputing Group, Department of Pharmacy and Biotechnology - Interdepartmental Centre ‘L. Galvani’ for Integrated Studies of Bioinformatics, Biophysics and Biocomplexity, University of Bologna, Bologna, Italy</aff>
    <aff id="btx818-aff2"><label>2</label>Department of Comparative Biomedicine and Food Science (BCA), University of Padova, Padova, Italy</aff>
    <author-notes>
      <corresp id="btx818-cor1">To whom correspondence should be addressed. Email: <email>pierluigi.martelli@unibo.it</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>15</day>
      <month>5</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2017-12-21">
      <day>21</day>
      <month>12</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>21</day>
      <month>12</month>
      <year>2017</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>34</volume>
    <issue>10</issue>
    <fpage>1690</fpage>
    <lpage>1696</lpage>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>6</month>
        <year>2017</year>
      </date>
      <date date-type="rev-recd">
        <day>22</day>
        <month>11</month>
        <year>2017</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>12</month>
        <year>2017</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2017. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2017</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btx818.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The identification of signal peptides in protein sequences is an important step toward protein localization and function characterization.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Here, we present DeepSig, an improved approach for signal peptide detection and cleavage-site prediction based on deep learning methods. Comparative benchmarks performed on an updated independent dataset of proteins show that DeepSig is the current best performing method, scoring better than other available state-of-the-art approaches on both signal peptide detection and precise cleavage-site identification.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>DeepSig is available as both standalone program and web server at <ext-link ext-link-type="uri" xlink:href="https://deepsig.biocomp.unibo.it">https://deepsig.biocomp.unibo.it</ext-link>. All datasets used in this study can be obtained from the same website.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <counts>
      <page-count count="7"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Protein sorting and compartmentalization are complex biological mechanisms, often guided by specific sequence signals present in the nascent protein. Signal peptides are short sequence segments located at the N-termini of newly synthesized proteins that are sorted toward the secretory pathway (<xref rid="btx818-B23" ref-type="bibr">von Heijne, 1990</xref>). Proteins endowed with a signal peptide include proteins resident in endoplasmic reticulum and Golgi apparatus, secreted proteins and proteins inserted in the plasma membrane. Identifying signal peptides in the protein sequence is a prerequisite to unveil protein destination and function.</p>
    <p>Several computational methods have been trained on available experimental data to detect the signal sequence in the N-terminus of a query protein. The most successful methods are based on machine learning models. Artificial Neural Networks and Support Vector Machines learn directly from the available experimental data the signal sequence features (<xref rid="btx818-B14" ref-type="bibr">Nugent and Jones, 2009</xref>; <xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>). Other methods (<xref rid="btx818-B3" ref-type="bibr">Bagos <italic>et al.</italic>, 2010</xref>; <xref rid="btx818-B10" ref-type="bibr">Käll <italic>et al.</italic>, 2005</xref>; <xref rid="btx818-B16" ref-type="bibr">Reynolds <italic>et al.</italic>, 2008</xref>; <xref rid="btx818-B21" ref-type="bibr">Tsirigos <italic>et al.</italic>, 2015</xref>; <xref rid="btx818-B22" ref-type="bibr">Viklund <italic>et al.</italic>, 2008</xref>) adopt Hidden Markov Models to define regular grammars. They explicitly model the modular architecture of the signal sequence, consisting of three regions: the positively charged N-region, the central hydrophobic H-region and the polar uncharged C-region containing the cleavage site (<xref rid="btx818-B12" ref-type="bibr">Martoglio and Dobberstein, 1998</xref>).</p>
    <p>A major challenge in signal peptide prediction is discriminating between true signal sequences and other hydrophobic regions, and, in particular, N-terminal transmembrane helices. The accurate prediction of the cleavage site is also challenging, mainly due to the high variability of the signal sequence length and the absence of sequence motifs that unambiguously mark the position of the cutting site.</p>
    <p>In this paper, we present DeepSig, a new method that takes advantage of Deep Learning advancement and improves the state-of-the-art performance. DeepSig is designed for both detecting signal peptides and finding their cleavage sites in protein sequences. The predictor consists of two consecutive building blocks: a deep neural network architecture and a probabilistic method that incorporates the current biological knowledge of the signal peptide structure.</p>
    <p>In the first stage, the N-terminus of a query protein sequence is analysed to assess the presence of a signal peptide. For this purpose, we designed a Deep Convolutional Neural Network (DCNN) (<xref rid="btx818-B11" ref-type="bibr">LeCun <italic>et al.</italic>, 2015</xref>) architecture (<xref ref-type="fig" rid="btx818-F1">Fig. 1</xref>), specifically tuned to recognize signal peptide sequences. DCNNs are very powerful deep learning architectures that achieve very high performance in several applications (Alipanahai <italic>et al.</italic>, 2015; <xref rid="btx818-B25" ref-type="bibr">Krizhevsky <italic>et al.</italic>, 2012</xref>; <xref rid="btx818-B26" ref-type="bibr">Zhou and Troyanskaya, 2015</xref>). Here, we devise a DCNN comprising three cascading convolution-pooling stages that process the N-terminus of the query protein, sorting out three classes: signal peptides, transmembrane regions and ‘anything else.’
</p>
    <fig id="btx818-F1" orientation="portrait" position="float">
      <label>Fig. 1.</label>
      <caption>
        <p>The architecture of the DCNN processing an input protein sequence to detect signal peptides. Feature extraction involves the application of three convolution-pooling (conv-pool) stages. The final classification is performed by a standard fully-connected neural network </p>
      </caption>
      <graphic xlink:href="btx818f1"/>
    </fig>
    <fig id="btx818-F2" orientation="portrait" position="float">
      <label>Fig. 2.</label>
      <caption>
        <p>The signal-peptide GRHCRF model capturing the modular structure of the signal peptide. States labeled with N, H, and C represents the positively charged N-region, the hydrophobic H-region and the cleavage C-region, respectively (see Section 2.4 for further details)</p>
      </caption>
      <graphic xlink:href="btx818f2"/>
    </fig>
    <p>If a signal peptide is detected, the protein is passed to the next prediction stage where the precise position of the cleavage site is identified (<xref ref-type="fig" rid="btx818-F2">Fig. 2</xref>). This task is tackled in DeepSig as a sequence labelling problem, where each residue is labelled as signal-peptide (S) or not (N). In particular, we adopted a probabilistic sequence labelling model (<xref rid="btx818-B6" ref-type="bibr">Fariselli <italic>et al.</italic>, 2009</xref>) similar to the regular grammars adopted by other HMM-based approaches (<xref rid="btx818-B10" ref-type="bibr">Käll <italic>et al.</italic>, 2004</xref>).</p>
    <p>For improving cleavage site detection, we also applied the Deep Taylor Decomposition (<xref rid="btx818-B13" ref-type="bibr">Montavon <italic>et al</italic>., 2017</xref>) to compute how relevant each residue at the N-terminus is for the recognition of the signal sequence. This score is used as additional feature for the sequence labelling model to improve cleavage-site prediction.</p>
    <p>We trained the DeepSig predictor on the dataset of proteins adopted by SignalP, one of the best performing methods developed so far (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>). It comprises 10 303 non-redundant proteins extracted from UniprotKB and belonging to three different organism classes: Eukaryotes, Gram-positive and Gram-negative bacteria.</p>
    <p>Comparative benchmarks were performed on a new independent validation dataset comprising 1707 sequences with experimental annotations in UniprotKB and not included in the training set. In all experiments, DeepSig outperforms other state-of-the-art approaches in both signal peptide detection and cleavage site prediction. Interestingly, when restricting the negative dataset to the most challenging cases (N-terminal transmembrane regions), DeepSig outperforms state-of-the-art predictors, specifically in the case of Eukaryotic proteins.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Datasets</title>
      <sec>
        <title>2.1.1 The SignalP4.0 dataset</title>
        <p>The first dataset used in this work was generated to train and test the well-known SignalP method (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>). Data were extracted from UniProtKB/SwissProt release 2010_05 including proteins from Eukaryotes, Gram-positive and Gram-negative bacteria. Only proteins with experimentally annotated signal peptide cleavage sites were retained. Negative sets (i.e. proteins lacking a signal peptide) were chosen from two different subsets: (i) proteins experimentally annotated as cytosolic and/or nuclear (ii) proteins experimentally annotated as single- or multi-pass membrane proteins, with a transmembrane segment annotated in the first 70 positions. All data were homology-reduced in order to obtain non-redundant datasets for each of the three organism classes. Two eukaryotic proteins were considered as similar if a local alignment between them included more than 17 identical residues out of 70 N-terminal residues. A threshold of 21 residues was instead used for bacterial proteins. See <xref rid="btx818-T1" ref-type="table">Table 1</xref> for a summary of the SignalP4.0 dataset.
<table-wrap id="btx818-T1" orientation="portrait" position="float"><label>Table 1.</label><caption><p>Statistics of the three datasets adopted in this study</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="left" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Dataset</th><th align="left" rowspan="1" colspan="1">Organism</th><th align="left" rowspan="1" colspan="1">SP</th><th align="left" rowspan="1" colspan="1">T</th><th align="left" rowspan="1" colspan="1">N/C</th><th align="left" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td rowspan="3" colspan="1">SignalP4.0</td><td rowspan="1" colspan="1">Eukaryotes</td><td rowspan="1" colspan="1">1640</td><td rowspan="1" colspan="1">987</td><td rowspan="1" colspan="1">5133</td><td rowspan="1" colspan="1">7760</td></tr><tr><td rowspan="1" colspan="1">Gram-positive</td><td rowspan="1" colspan="1">208</td><td rowspan="1" colspan="1">117</td><td rowspan="1" colspan="1">360</td><td rowspan="1" colspan="1">685</td></tr><tr><td rowspan="1" colspan="1">Gram-negative</td><td rowspan="1" colspan="1">423</td><td rowspan="1" colspan="1">523</td><td rowspan="1" colspan="1">912</td><td rowspan="1" colspan="1">1858</td></tr><tr><td rowspan="3" colspan="1">SPDS17</td><td rowspan="1" colspan="1">Eukaryotes</td><td rowspan="1" colspan="1">46</td><td rowspan="1" colspan="1">323</td><td rowspan="1" colspan="1">689</td><td rowspan="1" colspan="1">1058</td></tr><tr><td rowspan="1" colspan="1">Gram-positive</td><td rowspan="1" colspan="1">9</td><td rowspan="1" colspan="1">189</td><td rowspan="1" colspan="1">240</td><td rowspan="1" colspan="1">438</td></tr><tr><td rowspan="1" colspan="1">Gram-negative</td><td rowspan="1" colspan="1">23</td><td rowspan="1" colspan="1">89</td><td rowspan="1" colspan="1">99</td><td rowspan="1" colspan="1">211</td></tr><tr><td rowspan="1" colspan="1"><italic>E.coli</italic></td><td rowspan="1" colspan="1">–</td><td rowspan="1" colspan="1">573</td><td rowspan="1" colspan="1">1024</td><td rowspan="1" colspan="1">4375</td><td rowspan="1" colspan="1">5972</td></tr></tbody></table><table-wrap-foot><fn id="tblfn1"><p><italic>Note</italic>: SP, signal-peptide proteins; T, transmembrane proteins (with a single alpha helix in the N-terminal region); N/C, Nuclear and/or Cytosolic proteins (proteins without signal peptide); Total, total sum.</p></fn></table-wrap-foot></table-wrap></p>
      </sec>
      <sec>
        <title>2.1.2 The SPDS17 blind dataset</title>
        <p>We generated a new benchmark dataset to compare different approaches on signal peptide detection and cleavage-site prediction. We selected proteins from UniprotKB (rel. 04_2017) released after June 2015. This allowed to exclude any protein already included in the SignalP dataset used for training.</p>
        <p>Positive data were separately collected for Eukaryotes, Gram-negative and Gram-positive (in constructing this set we considered only proteins from <italic>Actinobacteria</italic> and <italic>Firmicutes</italic> phyla) by extracting proteins endowed with an experimentally annotated cleavage site for the signal peptide.</p>
        <p>Next, analogously to the SignalP4.0 dataset, for each organism class, two negative sets were generated: (i) proteins with a membrane-spanning segment in the first 70 residues and (ii) proteins localized into the nucleus and/or the cytoplasm. To generate these sets, we retained only proteins with experimental or manually curated annotation (corresponding to the UniProtKB evidence codes ECO: 0000269 and ECO: 0000305, respectively).</p>
        <p>The set redundancy was reduced to 25% sequence identity by running the blastclust algorithm and retaining a representative sequence from each cluster. Furthermore, we excluded all proteins sharing more than 25% sequence identity with any protein in the SignalP dataset. The blastp program with e-value threshold set to 1e-3 was adopted to search for similar proteins. <xref rid="btx818-T1" ref-type="table">Table 1</xref> contains a summary of the SPDS17 dataset.</p>
      </sec>
      <sec>
        <title>2.1.3 The Escherichia coli proteome</title>
        <p>We assessed DeepSig proteome-wide performance using the entire proteome of <italic>Escherichia coli (strain K12)</italic>. From release 11_2017 UniprotKB we downloaded all the 5972 reviewed entries. The sequences endowed with signal peptide are 573; 1024 have a transmembrane segment annotated in the first 70 residues.</p>
      </sec>
    </sec>
    <sec>
      <title>2.2 Deep convolutional neural networks for signal peptide prediction</title>
      <p>Deep Convolutional Neural Networks (DCNNs) (<xref rid="btx818-B11" ref-type="bibr">LeCun <italic>et al.</italic>, 2015</xref>) are powerful deep learning models devised to process multi-channel input data. Several data types fall in this category. The main application domain of DCNN is image processing (e.g. image object recognition or segmentation), where each pixel of a 2-Dimensional image is encoded by a vector of three intensity channels.</p>
      <p>Here, we apply DCNNs to protein sequence analysis. In this case, the input domain is a 1-dimensional signal, where each position in a sequence is represented by a multi-channel (i.e. multi-dimensional) vector encoding the residue type at each position of a protein, one channel for each residue type.</p>
      <p>Signal-peptide prediction is a special task of protein classification where the goal is to detect the presence/absence of the signal sequence in the N-terminus of the protein. <xref ref-type="fig" rid="btx818-F1">Figure 1</xref> summarizes the architecture of the DCNN defined in this paper for signal peptide prediction, comprising two basic modules: the feature extraction and the classification.</p>
      <sec>
        <title>2.2.1 Feature extraction module</title>
        <p>The feature extraction module consists of several hierarchical convolution (conv) and pooling (pool) layers which collectively compute a feature representation of the input protein sequence. Convolutional layers can be seen as sequence motif detectors used to scan the input sequence. A convolutional layer is mainly characterized by the number of motifs (or filters) it applies and by the motif length. Each motif detector slides along the input sequence, and computes the positional score for the motif at any sequence position. The scores are stored in the convolution neurons. Motif parameters are learnt during training and, routinely, parameter sharing is enforced (i.e. the same motif weights are applied to all positions during sequence scanning). After convolution, pooling layers are applied to aggregate neighbour convolution neurons into a single output neuron, with a consequent reduction of dimensionality. Typical pooling operations include max or average functions, computed over short non-overlapping slices of convolution neurons. The main parameter of a pooling layer is the width of the slice adopted. Iterative applications of convolution-pooling (conv-pool) operations are performed to extract a complex feature representation of the input sequence. In fact, a hierarchical feature extraction protocol is adopted where low-level motifs are progressively aggregated to model higher level inter-motif interactions. Adding conv-pool layers to the network allows extracting complex patterns of interaction through motifs, though increasing the complexity of the network.</p>
        <p>More formally, an input protein sequence is defined as a <inline-formula id="IE1"><mml:math id="IM1"><mml:mi>l</mml:mi><mml:mo>×</mml:mo><mml:mn>20</mml:mn></mml:math></inline-formula> matrix <inline-formula id="IE2"><mml:math id="IM2"><mml:mi>X</mml:mi></mml:math></inline-formula> where <inline-formula id="IE3"><mml:math id="IM3"><mml:mi>l</mml:mi></mml:math></inline-formula> is the sequence length and <inline-formula id="IE4"><mml:math id="IM4"><mml:mn>20</mml:mn></mml:math></inline-formula> is the number of different residue types. Here, protein sequences are shortened to the 96 N-terminal residues, hence <inline-formula id="IE5"><mml:math id="IM5"><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>96</mml:mn></mml:math></inline-formula>.</p>
        <p>A motif detector of odd-sized width <inline-formula id="IE6"><mml:math id="IM6"><mml:mi>w</mml:mi></mml:math></inline-formula> in the first convolution layer is defined as a weight matrix <inline-formula id="IE7"><mml:math id="IM7"><mml:mi>F</mml:mi></mml:math></inline-formula> of dimension <inline-formula id="IE8"><mml:math id="IM8"><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mn>20</mml:mn></mml:math></inline-formula>. If <inline-formula id="IE9"><mml:math id="IM9"><mml:mi>f</mml:mi></mml:math></inline-formula> different motif detectors are applied, the output of the convolution layer is a <inline-formula id="IE10"><mml:math id="IM10"><mml:mi>l</mml:mi><mml:mo>×</mml:mo><mml:mi>f</mml:mi></mml:math></inline-formula> matrix <inline-formula id="IE11"><mml:math id="IM11"><mml:mi>C</mml:mi></mml:math></inline-formula>, where the element <inline-formula id="IE12"><mml:math id="IM12"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is computed as:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mtext>(</mml:mtext><mml:mi>w</mml:mi><mml:mo>−</mml:mo><mml:mtext>1)</mml:mtext><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mtext>(</mml:mtext><mml:mi>w</mml:mi><mml:mo>−</mml:mo><mml:mtext>1)</mml:mtext><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mtext>(</mml:mtext><mml:mi>w</mml:mi><mml:mo>−</mml:mo><mml:mtext>1)</mml:mtext><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE13"><mml:math id="IM13"><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>is <inline-formula id="IE14"><mml:math id="IM14"><mml:mi>j</mml:mi></mml:math></inline-formula>-th motif weight matrix and <inline-formula id="IE15"><mml:math id="IM15"><mml:mtext mathvariant="italic">max</mml:mtext><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the <italic>rectified linear unit</italic> (ReLU) activation function. Using ReLUs instead of other activation functions (such as tanh or sigmoid) speeds up the training process, particularly in networks with many layers (<xref rid="btx818-B11" ref-type="bibr">LeCun <italic>et al.</italic>, 2015</xref>).</p>
        <p>The role of the pooling layer with pool size equal to <inline-formula id="IE18"><mml:math id="IM16"><mml:mi>s</mml:mi></mml:math></inline-formula>, applied to each motif of matrix <inline-formula id="IE19"><mml:math id="IM17"><mml:mi>C</mml:mi></mml:math></inline-formula>, is to reduce its dimensionality by merging together <inline-formula id="IE20"><mml:math id="IM18"><mml:mi>s</mml:mi></mml:math></inline-formula> neighbour convolutional neurons into one. Although other schemes are possible, here average pooling is applied to adjacent pairs of convolution neurons, leading to dimensionality reduction from <inline-formula id="IE21"><mml:math id="IM19"><mml:mi>l</mml:mi></mml:math></inline-formula> to <inline-formula id="IE22"><mml:math id="IM20"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The pooling layer computes a <inline-formula id="IE23"><mml:math id="IM21"><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>f</mml:mi></mml:math></inline-formula> matrix <inline-formula id="IE24"><mml:math id="IM22"><mml:mi>P</mml:mi></mml:math></inline-formula> defined as follows:
<disp-formula id="E2"><label>(2)</label><graphic xlink:href="btx818m2.jpg" position="float" orientation="portrait"/></disp-formula>
where <italic>i</italic> ranges between 1 and m=l/2. Overall, a single conv-pool application transforms an input sequence of dimension <inline-formula id="IE25"><mml:math id="IM23"><mml:mi>l</mml:mi><mml:mo>×</mml:mo><mml:mn>20</mml:mn></mml:math></inline-formula> to a non-linear feature representation of dimension <inline-formula id="IE26"><mml:math id="IM24"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:mi>f</mml:mi></mml:math></inline-formula>. Hence, a series of <inline-formula id="IE27"><mml:math id="IM25"><mml:mi>r</mml:mi></mml:math></inline-formula> conv-pool stages, stacked together, extracts a non-linear feature space representation of dimension <inline-formula id="IE28"><mml:math id="IM26"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, with <inline-formula id="IE29"><mml:math id="IM27"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo></mml:math></inline-formula>being the number of motif detectors in the last conv layer.</p>
        <p>In our final network, we apply three cascading conv-pool stages. Different architectures were tried and selected through cross-validation, varying the number of motif detectors and motif width on each conv layer.</p>
      </sec>
      <sec>
        <title>2.2.2 Classification module</title>
        <p>The output classification is performed by means of a module implementing a conventional fully connected feed-forward neural network, comprising a single hidden-layer with <inline-formula id="IE30"><mml:math id="IM28"><mml:mi>h</mml:mi></mml:math></inline-formula> neurons. The number of neurons in the hidden layer was varied and optimized through cross-validation, separately for each organism class. Firstly, the computed feature representation is flattened into a column vector <inline-formula id="IE31"><mml:math id="IM29"><mml:mi>v</mml:mi></mml:math></inline-formula> that encodes the input of the feed-forward network.</p>
        <p>Each neuron <inline-formula id="IE32"><mml:math id="IM30"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in the hidden layer computes a non-linear transformation, defined as follows:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mo>α</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE33"><mml:math id="IM31"><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE34"><mml:math id="IM32"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are, respectively, the weight vector and bias of the hidden neuron <inline-formula id="IE35"><mml:math id="IM33"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (again, the ReLU activation is used).</p>
        <p>Finally, the hidden layer output vector <inline-formula id="IE36"><mml:math id="IM34"><mml:mi>h</mml:mi></mml:math></inline-formula> is mapped to the <inline-formula id="IE37"><mml:math id="IM35"><mml:mi>i</mml:mi></mml:math></inline-formula>-th output neuron as follows:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>h</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mo>β</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE38"><mml:math id="IM36"><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE39"><mml:math id="IM37"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are, respectively, the weight vector and the bias of the output neuron <inline-formula id="IE40"><mml:math id="IM38"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and the function <inline-formula id="IE41"><mml:math id="IM39"><mml:mi>t</mml:mi></mml:math></inline-formula> is the <inline-formula id="IE42"><mml:math id="IM40"><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:math></inline-formula> function, allowing a probabilistic interpretation of the network output.</p>
        <p>The final output of our DCNN comprises three output neurons accounting for three different output classes: signal peptide (S), transmembrane segment (T) or other (N). This three-class schema allows to reduce the misclassification between transmembrane regions and signal peptides (Section Results). An input protein sequence is classified into the class <inline-formula id="IE43"><mml:math id="IM41"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> with the highest predicted probability, namely:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>argmax</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Given a training set <inline-formula id="IE44"><mml:math id="IM42"><mml:mo>θ</mml:mo><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></inline-formula> of <inline-formula id="IE45"><mml:math id="IM43"><mml:mi>N</mml:mi></mml:math></inline-formula> protein sequences with true output targets, network parameters are optimized by minimizing the average cross-entropy loss function on the training set, defined as:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6"><mml:mi>L</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>θ</mml:mo></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE46"><mml:math id="IM44"><mml:msubsup><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math></inline-formula> is the <inline-formula id="IE47"><mml:math id="IM45"><mml:mi>j</mml:mi></mml:math></inline-formula>-th network output when the <inline-formula id="IE48"><mml:math id="IM46"><mml:mi>i</mml:mi></mml:math></inline-formula>-th sequence is provided in input.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 Evaluating residue positional relevance with deep Taylor decomposition</title>
      <p>The DCNN described in the previous section is designed to provide a prediction of the presence/absence of the signal peptide sequence in the N-terminus of an input protein. In general, when such predictions are performed with DCNN, some of the elements of an input sequence (i.e. individual residues) may be more determinant than others in driving the model classification toward one specific class or another. An important question is then how this piece of information can be extracted from the analysis of the internal neuronal activity of DCNN.</p>
      <p>Many methods are available to analyse the complex behaviour of non-linear classifiers in the attempt of quantifying the importance of basic elements in the input data with respect to the task at hand (<xref rid="btx818-B2" ref-type="bibr">Bach <italic>et al.</italic>, 2015</xref>; <xref rid="btx818-B13" ref-type="bibr">Montavon <italic>et al</italic>., 2017</xref>; <xref rid="btx818-B19" ref-type="bibr">Simonyan <italic>et al.</italic>, 2013</xref>). For instance, in image classification, one wants to identify a subset of relevant pixels that are responsible for the recognition of an object in the image (<xref rid="btx818-B2" ref-type="bibr">Bach <italic>et al.</italic>, 2015</xref>; <xref rid="btx818-B13" ref-type="bibr">Montavon <italic>et al</italic>., 2017</xref>; <xref rid="btx818-B19" ref-type="bibr">Simonyan <italic>et al.</italic>, 2013</xref>; <xref rid="btx818-B20" ref-type="bibr">Szegedy <italic>et al.</italic>, 2013</xref>). In the context of signal peptide detection, given an input protein sequence in which a signal peptide has been recognized, we want to identify residue positions along the sequence that are more relevant for the global recognition of the signal.</p>
      <p>Available methods can be roughly classified into two different categories: functional approaches look at networks as function approximators and highlight the most relevant input features by analysing the prediction function (<xref rid="btx818-B19" ref-type="bibr">Simonyan <italic>et al.</italic>, 2013</xref>); message passing approaches exploit the network as a computational graph and propagate prediction values throughout the different layers back to input variables (<xref rid="btx818-B2" ref-type="bibr">Bach <italic>et al.</italic>, 2015</xref>).</p>
      <p>Here, we adopt the deep Taylor decomposition (<xref rid="btx818-B13" ref-type="bibr">Montavon <italic>et al.</italic>, 2017</xref>), a hybrid functional/message passing approach that has been recently introduced for the analysis of deep neural networks. The method focuses on image classification, but it can be easily extended to other types of prediction scenarios, such as protein sequence classification. We briefly describe here its main aspects and refer to the original paper for a comprehensive mathematical description of the method (<xref rid="btx818-B13" ref-type="bibr">Montavon <italic>et al.</italic>, 2017</xref>) and to our <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref> for a description of how this method can be applied to our signal-peptide DCNN.</p>
      <p>Let be <inline-formula id="IE49"><mml:math id="IM47"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> an input protein sequence of length <inline-formula id="IE50"><mml:math id="IM48"><mml:mi>l</mml:mi></mml:math></inline-formula> where each <inline-formula id="IE51"><mml:math id="IM49"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> is a <inline-formula id="IE52"><mml:math id="IM50"><mml:mn>20</mml:mn></mml:math></inline-formula>-channel vector representing a residue in the sequence. <inline-formula id="IE53"><mml:math id="IM51"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:math></inline-formula> is the scalar function implemented with a DCNN and evaluated on the input <inline-formula id="IE54"><mml:math id="IM52"><mml:mi mathvariant="bold">x</mml:mi></mml:math></inline-formula>. The function <inline-formula id="IE55"><mml:math id="IM53"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> quantifies the evidence (or score) that a signal peptide is present in the N-terminus of the sequence <inline-formula id="IE56"><mml:math id="IM54"><mml:mi mathvariant="bold">x</mml:mi></mml:math></inline-formula>. We want to assign to each residue <inline-formula id="IE57"><mml:math id="IM55"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> a <italic>relevance score</italic><inline-formula id="IE58"><mml:math id="IM56"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> that quantifies the individual contribution of that residue to the total predicted evidence function <inline-formula id="IE59"><mml:math id="IM57"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>.</p>
      <p>Operatively, deep Taylor decomposition proceeds by assigning to each neuron in a deep network a relevance score which is a measure of the contribution of the neuron to the total predicted score <inline-formula id="IE60"><mml:math id="IM58"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. Neuron relevance scores are computed by establishing local, connectivity-dependent functional mappings between neuron activation values and propagated relevance values from upper-layers. Taylor expansions of these local mappings at neuron-specific root points are then computed. Depending on the functional form of the mappings and on the nature of the input domain, different relevance propagation rules are defined (for details, see <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>).</p>
      <p>We apply this procedure to our signal peptide DCNN to evaluate the contribution of each residue position to the detection of the signal sequence. The result for a sequence in input of length <inline-formula id="IE61"><mml:math id="IM59"><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>96</mml:mn></mml:math></inline-formula> is a vector:
<disp-formula id="E7"><label>(7)</label><mml:math id="M7"><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where the component <inline-formula id="IE62"><mml:math id="IM60"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> is the relevance of the residue in position <inline-formula id="IE63"><mml:math id="IM61"><mml:mi>i</mml:mi></mml:math></inline-formula>.</p>
    </sec>
    <sec>
      <title>2.4 Prediction of the signal peptide cleavage site</title>
      <p>When a signal peptide is detected with the DCNN, the protein sequence passes to the second prediction stage which identifies the location of the cleavage site. In particular, each residue of a positively-predicted sequence is assigned to one of two classes: signal peptide (S) or non-signal region (N).</p>
      <p>Here, we adopt a Grammar-Restrained Hidden Conditional Random Field (GRHCRF) (<xref rid="btx818-B6" ref-type="bibr">Fariselli <italic>et al.</italic>, 2009</xref>; <xref rid="btx818-B8" ref-type="bibr">Indio <italic>et al</italic>., 2013</xref>; <xref rid="btx818-B17" ref-type="bibr">Savojardo <italic>et al</italic>., 2013</xref>; <xref rid="btx818-B18" ref-type="bibr">Savojardo <italic>et al</italic>., 2017</xref>). Like HMMs, a GRHCRF can be represented as a finite state automaton whose state structure and transitions reflect a regular grammar describing the problem at hand (<xref rid="btx818-B6" ref-type="bibr">Fariselli <italic>et al.</italic>, 2009</xref>). Each state of the model is associated to a label that can be assigned to each element of a sequence. Model parameters are weights that score the compatibility between input sequences included in the training set and their true labelling. Once the model has been trained, sequence labelling is performed by assigning labels corresponding to the most probable state path in the model. The optimal state path is computed by means of Posterior-Viterbi decoding (<xref rid="btx818-B6" ref-type="bibr">Fariselli <italic>et al.</italic>, 2009</xref>).</p>
      <p>The GHRCRF model is defined on top of the grammar depicted in <xref ref-type="fig" rid="btx818-F2">Figure 2</xref> as a finite-state automaton. The model defines different states organized to capture the modular structure of a typical signal peptide: 7 states to model the initial positively charged N-region (states N1–N7), 11 states for the hydrophobic H-region (states H1–H11) and 13 states for the cleavage C-region (states C1–C13). State transitions are defined such that minimal and maximal lengths for each sub-region are enforced. In particular, N-regions can be from two up to seven residues long. In contrast, the H-region has a minimal length of four residues with no upper bound. Finally, C-regions comprise between 3 and 13 residues. The remaining mature protein portion is modelled through a single recursive state (G0). The cleavage site corresponds to the position of the residue assigned to state C13.</p>
      <p>Training of the GRHCRF is performed on a training set of protein sequences endowed with signal peptides. Also in this case, sequences are reduced to the first 96 N-terminal residues. Each residue is encoded using a 21-dimensional feature vector consisting of:
<list list-type="bullet"><list-item><p>20 positions of the vector correspond to the usual residue encoding described above;</p></list-item><list-item><p>the relevance score of the residue computed from the DCNN and deep Taylor decomposition as described in Section 2.3.</p></list-item></list></p>
    </sec>
    <sec>
      <title>2.5 Model optimization and implementation</title>
      <p>All the models are trained on the SignalP4.0 dataset using a nested 5-fold cross-validation procedure as done in <xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic> (2011)</xref>. Three different optimization runs are performed on Eukaryotic, Gram-positive and Gram-negative, respectively.</p>
      <p>Firstly, the entire dataset is randomly split into five subsets containing broadly the same number of proteins. Random splits are computed so that the balancing between signal-peptide, transmembrane and other proteins is maintained on each subset and it is similar to the one observed in the whole dataset.</p>
      <p>Secondly, a nested cross-validation procedure is performed as follows: one subset is kept out and used for testing while a full inner 4-fold cross-validation is performed on the remaining four subsets. In each run of this inner procedure, three subsets are used for training and one for validation. The inner cross-validation is used to optimize the network parameters and architecture. In fact, we retain the top-performing network as evaluated on the inner validation sets. The procedure is repeated leaving out each time a different subset for testing. In summary, 20 different networks are obtained (four optimal networks that are identical in parameters and architecture but have been trained on different inner training sets for each one of the five main subsets). When performance is evaluated on the testing set, outputs of the four inner networks are averaged to give the final score. In the final version of our DeepSig predictor, we average the output of all the 20 optimal networks.</p>
      <p>The same procedure and data split was applied to train/test the cleavage site predictor based on the GRHCRF model.</p>
      <p>The DCNN is implemented using the Keras Python package (<ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link>) (<xref rid="btx818-B5" ref-type="bibr">Chollet <italic>et al.</italic>, 2015</xref>) with the Tensorflow (<ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org">https://www.tensorflow.org</ext-link>) (<xref rid="btx818-B1" ref-type="bibr">Abadi <italic>et al.</italic>, 2015</xref>) backend. The categorical cross-entropy loss minimization is carried-out with the standard error back-propagation procedure and the stochastic gradient descent algorithm. Default hyper-parameters were used for training the networks (Default hyper-parameters for network training were set to 0.01 for learning rate and to 0 for momentum and weight decay).</p>
    </sec>
    <sec>
      <title>2.6 Scoring measures</title>
      <p>Signal-peptide detection is scored using the following measures:
<list list-type="bullet"><list-item><p>Matthews Correlation Coefficient (MCC), defined as:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8"><mml:mrow><mml:mtext>MCC</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>×</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>-</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>×</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where TP and TN are the correct predictions in the positive and negative classes, respectively, and FN and FP are the number of under- and over-predictions in the signal peptide class</p></list-item><list-item><p>False positive rate computed on transmembrane proteins, defined as:
<disp-formula id="E9"><label>(9)</label><mml:math id="M9"><mml:mrow><mml:msub><mml:mrow><mml:mtext>FPR</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mtext>FP</mml:mtext><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where FP<sub><italic>T</italic></sub> is the number of transmembrane proteins misclassified as having a signal peptide and N<sub><italic>T</italic></sub> is the total number of transmembrane proteins.</p></list-item><list-item><p>Cleavage-site prediction is scored by the cleavage-site F1 measure defined as:
<disp-formula id="E10"><label>(10)</label><mml:math id="M10"><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>CS</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula>
namely, the harmonic mean between Cleavage Site Sensitivity, <inline-formula id="IE64"><mml:math id="IM62"><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> and Cleavage Site Precision, <inline-formula id="IE65"><mml:math id="IM63"><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></inline-formula>, where <italic>N</italic><sub>corr</sub> is the number of correctly identified cleavage sites and <italic>N</italic> and <italic>N<sub>P</sub></italic> are, respectively, the true number of signal peptides and the number of predicted signal peptides.</p></list-item></list></p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Performance on the SignalP4.0 dataset</title>
      <p>We firstly evaluate the performance of our DeepSig predictor on the dataset adopted to train and test SignalP4.0 (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>) with the same nested cross-validation procedure described in Section 2.5.</p>
      <p>This allows a direct and accurate comparison of DeepSig with SignalP4.0. In particular, three different versions of SignaP are scored: SignalP-TM, the version of the method optimized to distinguish signal peptides from transmembrane regions; SignalP-noTM which is not optimized and SignalP4.0 which is a combination of the two methods above.</p>
      <p>Our DeepSig predictor is also evaluated in two versions, either using the relevance profile as feature for cleavage-site prediction (Section 2.4) or not (‘no relevance’ in <xref rid="btx818-T2" ref-type="table">Table 2</xref>).
<table-wrap id="btx818-T2" orientation="portrait" position="float"><label>Table 2.</label><caption><p>Performance of different versions of SignalP and DeepSig on signal peptide detection and cleavage site prediction in 5-fold cross-validation on the SignalP4.0 dataset (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>)</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/></colgroup><thead><tr><th rowspan="2" colspan="1">Method</th><th colspan="3" rowspan="1">Eukaryotes<hr/></th><th colspan="3" rowspan="1">Gram-positive<hr/></th><th colspan="3" rowspan="1">Gram-negative<hr/></th></tr><tr><th rowspan="1" colspan="1">MCC</th><th rowspan="1" colspan="1">FPR<sub>T</sub></th><th rowspan="1" colspan="1">F1<sub>cs</sub></th><th rowspan="1" colspan="1">MCC</th><th rowspan="1" colspan="1">FPR<sub>T</sub></th><th rowspan="1" colspan="1">F1<sub>cs</sub></th><th rowspan="1" colspan="1">MCC</th><th rowspan="1" colspan="1">FPR<sub>T</sub></th><th rowspan="1" colspan="1">F1<sub>cs</sub></th></tr></thead><tbody><tr><td rowspan="1" colspan="1">SignalP 4.0<xref ref-type="table-fn" rid="tblfn3"><sup>a</sup></xref></td><td rowspan="1" colspan="1">0.874</td><td rowspan="1" colspan="1">6.1</td><td rowspan="1" colspan="1">67.1</td><td rowspan="1" colspan="1">0.851</td><td rowspan="1" colspan="1">2.6</td><td rowspan="1" colspan="1">77.8</td><td rowspan="1" colspan="1">0.848</td><td rowspan="1" colspan="1">1.5</td><td rowspan="1" colspan="1">68.0</td></tr><tr><td rowspan="1" colspan="1">SignalP-TM<xref ref-type="table-fn" rid="tblfn3"><sup>a</sup></xref></td><td rowspan="1" colspan="1">0.871</td><td rowspan="1" colspan="1">3.3</td><td rowspan="1" colspan="1">67.2</td><td rowspan="1" colspan="1">0.851</td><td rowspan="1" colspan="1">2.6</td><td rowspan="1" colspan="1">77.8</td><td rowspan="1" colspan="1">0.815</td><td rowspan="1" colspan="1">1.1</td><td rowspan="1" colspan="1">67.7</td></tr><tr><td rowspan="1" colspan="1">SignalP-noTM<xref ref-type="table-fn" rid="tblfn3"><sup>a</sup></xref></td><td rowspan="1" colspan="1">0.674</td><td rowspan="1" colspan="1">38.1</td><td rowspan="1" colspan="1">54.6</td><td rowspan="1" colspan="1">0.556</td><td rowspan="1" colspan="1">47.9</td><td rowspan="1" colspan="1">49.4</td><td rowspan="1" colspan="1">0.497</td><td rowspan="1" colspan="1">35.8</td><td rowspan="1" colspan="1">67.7</td></tr><tr><td rowspan="1" colspan="1">DeepSig (no relevance)</td><td rowspan="1" colspan="1">0.910</td><td rowspan="1" colspan="1">2.6</td><td rowspan="1" colspan="1">71.1</td><td rowspan="1" colspan="1">0.878</td><td rowspan="1" colspan="1">5.9</td><td rowspan="1" colspan="1">69.7</td><td rowspan="1" colspan="1">0.900</td><td rowspan="1" colspan="1">1.5</td><td rowspan="1" colspan="1">83.5</td></tr><tr><td rowspan="1" colspan="1">DeepSig</td><td rowspan="1" colspan="1">0.910</td><td rowspan="1" colspan="1">2.6</td><td rowspan="1" colspan="1">73.3</td><td rowspan="1" colspan="1">0.878</td><td rowspan="1" colspan="1">5.9</td><td rowspan="1" colspan="1">72.3</td><td rowspan="1" colspan="1">0.900</td><td rowspan="1" colspan="1">1.5</td><td rowspan="1" colspan="1">86.2</td></tr></tbody></table><table-wrap-foot><fn id="tblfn2"><p><italic>Note</italic>: MCC, Matthews Correlation Coefficient; FPR<sub>T</sub>, False Positive Rate on transmembrane proteins; F1<sub>cs</sub>, The harmonic mean between precision and recall on cleavage-site detection. No relevance = without relevance profile as feature for cleavage-site prediction (Section 2.4).</p></fn><fn id="tblfn3"><label>a</label><p>Data taken from <xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic> (2011)</xref>.</p></fn></table-wrap-foot></table-wrap></p>
      <p>Comparative results of both signal peptide detection and cleavage site prediction are reported in <xref rid="btx818-T2" ref-type="table">Table 2</xref>. Methods are trained and scored separately on each organism class: Eukaryotes, Gram-positive and Gram-negative. Results for SignalP are derived from the original paper (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>).</p>
      <p>The first aspect evaluated is the detection of the signal peptide (with the MCC and FPR<sub>T</sub> scoring indexes). DeepSig outperforms SignalP (considering the MCC values on <xref rid="btx818-T2" ref-type="table">Table 2</xref>) on all the three datasets (Eukaryotes, Gram-positive and Gram-negative proteins).</p>
      <p>Specifically, on the Eukaryote dataset our method shows a lower false positive rate on proteins with a transmembrane segment annotated in the first 70 residues (<xref rid="btx818-T2" ref-type="table">Table 2</xref>, FPR<sub>T</sub>). It is well known that the ability to distinguish true signal peptides from N-terminal transmembrane regions is one of the main challenges for signal-peptide detection methods, due the similar physical-chemical profiles (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>). In this respect, DeepSig scores with a false positive rate of 2.6%, lower than that of SignalP-TM (3.3%) and a higher MCC value. In absolute terms, DeepSig and SignalP-TM produce 20 and 27 false positive predictions out of 787 transmembrane proteins, respectively.</p>
      <p>On the two other datasets (Gram-positive and Gram-negative bacteria) DeepSig scores on transmembrane proteins with a false positive rate of 5.9% and 1.5%, respectively. On Gram-positive bacteria, the DeepSig false positive rate is higher compared to that reported SignalP-TM (<xref rid="btx818-T2" ref-type="table">Table 2</xref>). It is possible that low number of transmembrane proteins of Gram-positives hampers the ability of the DCNN to discriminate true signal sequence from transmembrane regions.</p>
      <p>The second aspect evaluated is the ability to identify the correct location of the cleavage site. As described in Section 2.4, our method is based on a probabilistic sequence labelling approach which makes use of the relevance profile computed by means of deep Taylor decomposition. For this reason, we are interested in quantifying the impact of this additional feature on the cleavage-site prediction performance. As highlighted in <xref rid="btx818-T2" ref-type="table">Table 2</xref>, considering the F1cs values of all the three protein sets, the inclusion of the relevance profile leads to a better F1 score in cleavage-site prediction of DeepSig. This demonstrates that the relevance profile, when incorporated into the probabilistic sequence labelling method, provides additional information that, in conjunction with primary sequence, helps in identifying the correct extent of the signal sequence.</p>
      <p>Comparing results in <xref rid="btx818-T2" ref-type="table">Table 2</xref>, we can conclude that the cleavage site position is better predicted by DeepSig than SignalP, with the exception of Gram positive bacteria. The improvement ranges from 2% to 4%.</p>
    </sec>
    <sec>
      <title>3.2 Performance on the SPDS17 independent dataset</title>
      <p>Five state-of-the-art predictors are benchmarked toghether with DeepSig on an independent and blind SPDS17 validation set. The predictors are: SignalP4.1 (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>), TOPCONS2.0 (<xref rid="btx818-B21" ref-type="bibr">Tsirigos <italic>et al.</italic>, 2015</xref>), SPOCTOPUS (<xref rid="btx818-B22" ref-type="bibr">Viklund <italic>et al.</italic>, 2008</xref>), PolyPhobius (<xref rid="btx818-B10" ref-type="bibr">Käll <italic>et al.</italic>, 2005</xref>), Philius (<xref rid="btx818-B16" ref-type="bibr">Reynolds <italic>et al.</italic>, 2008</xref>) and PRED-TAT (<xref rid="btx818-B3" ref-type="bibr">Bagos <italic>et al.</italic>, 2010</xref>), all based on different and well established methods. Again, predictions were generated separately on Eukaryote, Gram-positive and Gram-negative data, either launching the sequences on the respective web-servers or running in-house the standalone versions. Three complementary aspects are compared: the efficiency of the signal peptide detection evaluated with the Matthews correlation coefficient (MCC), the precision of the discrimination between signal peptides and N-terminal transmembrane regions, and the performance on the prediction of the cleavage-site, measured with the F1 score.</p>
      <p>For all the organism classes and for all the considered aspects, DeepSig reports the best performances (<xref rid="btx818-T3" ref-type="table">Table 3</xref>). The MCCs of the signal peptide detection are 2 to 4 percentage points higher than the state-of-the art SignalP4.1. When restricting the negative dataset to the most challenging cases (N-terminal transmembrane regions), DeepSig reports the best false positive rate, outperforming SignalP4.1 by 1.5% in the case of eukaryotic proteins. Moreover, DeepSig gives a more exact prediction of the cleavage site in all the three organisms, as highlighted by the cleavage-site F1 values.
<table-wrap id="btx818-T3" orientation="portrait" position="float"><label>Table 3.</label><caption><p>Comparative benchmark of different methods in signal peptide detection and cleavage site prediction on the SPDS17 independent dataset</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/></colgroup><thead><tr><th rowspan="2" colspan="1">Method</th><th colspan="3" rowspan="1">Eukaryotes<hr/></th><th colspan="3" rowspan="1">Gram-positive<hr/></th><th colspan="3" rowspan="1">Gram-negative<hr/></th></tr><tr><th rowspan="1" colspan="1">MCC</th><th rowspan="1" colspan="1">FPR<sub>T</sub></th><th rowspan="1" colspan="1">F1<sub>cs</sub></th><th rowspan="1" colspan="1">MCC</th><th rowspan="1" colspan="1">FPR<sub>T</sub></th><th rowspan="1" colspan="1">F1<sub>cs</sub></th><th rowspan="1" colspan="1">MCC</th><th rowspan="1" colspan="1">FPR<sub>T</sub></th><th rowspan="1" colspan="1">F1<sub>cs</sub></th></tr></thead><tbody><tr><td rowspan="1" colspan="1">SPOCTOPUS</td><td rowspan="1" colspan="1">0.54</td><td rowspan="1" colspan="1">16.7</td><td rowspan="1" colspan="1">0.20</td><td rowspan="1" colspan="1">0.28</td><td rowspan="1" colspan="1">20.2</td><td rowspan="1" colspan="1">0.37</td><td rowspan="1" colspan="1">0.63</td><td rowspan="1" colspan="1">14.3</td><td rowspan="1" colspan="1">0.12</td></tr><tr><td rowspan="1" colspan="1">PRED-TAT</td><td rowspan="1" colspan="1">0.55</td><td rowspan="1" colspan="1">9.3</td><td rowspan="1" colspan="1">0.33</td><td rowspan="1" colspan="1">0.26</td><td rowspan="1" colspan="1">2.2</td><td rowspan="1" colspan="1">0.72</td><td rowspan="1" colspan="1">0.82</td><td rowspan="1" colspan="1">9.9</td><td rowspan="1" colspan="1">0.14</td></tr><tr><td rowspan="1" colspan="1">Philius</td><td rowspan="1" colspan="1">0.62</td><td rowspan="1" colspan="1">6.5</td><td rowspan="1" colspan="1">0.46</td><td rowspan="1" colspan="1">0.31</td><td rowspan="1" colspan="1">3.4</td><td rowspan="1" colspan="1">0.72</td><td rowspan="1" colspan="1">0.87</td><td rowspan="1" colspan="1">7.4</td><td rowspan="1" colspan="1">0.22</td></tr><tr><td rowspan="1" colspan="1">PolyPhobius</td><td rowspan="1" colspan="1">0.73</td><td rowspan="1" colspan="1">7.4</td><td rowspan="1" colspan="1">0.42</td><td rowspan="1" colspan="1">0.44</td><td rowspan="1" colspan="1">11.2</td><td rowspan="1" colspan="1">0.53</td><td rowspan="1" colspan="1">0.80</td><td rowspan="1" colspan="1">7.9</td><td rowspan="1" colspan="1">0.06</td></tr><tr><td rowspan="1" colspan="1">TOPCONS2.0</td><td rowspan="1" colspan="1">0.74</td><td rowspan="1" colspan="1">5.3</td><td rowspan="1" colspan="1">0.27</td><td rowspan="1" colspan="1">0.49</td><td rowspan="1" colspan="1">4.5</td><td rowspan="1" colspan="1">0.60</td><td rowspan="1" colspan="1">0.91</td><td rowspan="1" colspan="1">2.6</td><td rowspan="1" colspan="1">0.08</td></tr><tr><td rowspan="1" colspan="1">SignalP4.1</td><td rowspan="1" colspan="1">0.82</td><td rowspan="1" colspan="1">4.0</td><td rowspan="1" colspan="1">0.69</td><td rowspan="1" colspan="1">0.50</td><td rowspan="1" colspan="1">0.0</td><td rowspan="1" colspan="1">0.79</td><td rowspan="1" colspan="1">0.93</td><td rowspan="1" colspan="1">4.2</td><td rowspan="1" colspan="1">0.33</td></tr><tr><td rowspan="1" colspan="1">DeepSig</td><td rowspan="1" colspan="1">0.86</td><td rowspan="1" colspan="1">2.5</td><td rowspan="1" colspan="1">0.72</td><td rowspan="1" colspan="1">0.54</td><td rowspan="1" colspan="1">0.0</td><td rowspan="1" colspan="1">0.82</td><td rowspan="1" colspan="1">0.95</td><td rowspan="1" colspan="1">2.6</td><td rowspan="1" colspan="1">0.36</td></tr></tbody></table><table-wrap-foot><fn id="tblfn4"><p><italic>Note</italic>: MCC, Matthews Correlation Coefficient; FPR<sub>T</sub>, False Positive Rate on transmembrane proteins; F1<sub>cs</sub>, The harmonic mean between precision and recall on cleavage-site detection.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec>
      <title>3.3 Proteome-wide scanning and detection of TAT-type signal peptides</title>
      <p>As a final benchmark, we assessed the performance of DeepSig on the entire proteome of <italic>E.coli (strain K12)</italic>.</p>
      <p>DeepSig scores with a MCC value of 0.81, which is in line with the results obtained on other benchmarks. A very low false positive rate on transmembrane proteins was also registered: only 4 out of 1024 transmembrane proteins were incorrectly classified as signal peptides, corresponding to a FPR<sub>T</sub> of 0.39%. Furthermore, the method was also able to recover the correct cleavage site for 340 signal peptides, corresponding to a F1<sub>cs</sub> value of 69%. Specifically, the set contains 138 sequences with an experimentally detected signal peptide: DeepSig correctly identifies 126 sequences and correctly places cleavage sites of 116.</p>
      <p>Interestingly, even if DeepSig has not been trained to explicitly recognize Twin-Arginine Translocation (TAT-type) signal sequences (<xref rid="btx818-B4" ref-type="bibr">Berks, 2015</xref>), the method correctly detects 18 out of 32 Tat-type signals that were annotated on <italic>E.coli</italic> sequences (sensitivity is 56%).</p>
      <p>To further investigate the performance of DeepSig on detecting TAT-type signal sequences, we downloaded from UniprotKB/SwissProt all reviewed sequences carrying this kind of signal. We ended up with 553 bacterial proteins, 466 of which were from Gram-negative and 71 from Gram-positive bacteria. Running DeepSig on these sequences, we were able to recover 330 out of 553 TAT signals, corresponding to a sensitivity of about 60%.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>In this paper, we present DeepSig, a novel approach to predict signal peptides in proteins based on deep learning and sequence labelling methods. The proposed approach was evaluated and compared with other available predictors, including the top-performing SignalP (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>). In all the benchmarks, DeepSig reported performances that were comparable and even superior to other state-of-the-art methods.</p>
    <p>The method is available as web server and as a standalone program (<ext-link ext-link-type="uri" xlink:href="https://deepsig.biocomp.unibo.it">https://deepsig.biocomp.unibo.it</ext-link>). The standalone version of the program is very fast and easy to install. It takes only 40 min to process the entire human proteome containing some 70 000 protein sequences (test executed by running DeepSig in parallel using four CPU cores). All this suggests that DeepSig is a premier candidate for proteome-scale assessment of protein sub-cellular localization (where high precision is crucial) as well as for single-protein analyses where one is interested in the accurate identification of the signal sequence and cleavage site.</p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>Supplementary Data</label>
      <media xlink:href="btx818_supplementary.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="btx818-B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Abadi</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) Tensorflow: Large-Scale Machine Learning on Heterogeneous Systems. Software available online: <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org">https://www.tensorflow.org</ext-link>.</mixed-citation>
    </ref>
    <ref id="btx818-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alipanahi</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Predicting the sequence specificities of DNA- and RN-binding proteins by deep learning</article-title>. <source>Nat. Biotechnol</source>., <volume>33</volume>, <fpage>831</fpage>–<lpage>838</lpage>.<pub-id pub-id-type="pmid">26213851</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bach</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>On pixel-wise explanations for non-linear classifier decision by layer-wise relevance propagation</article-title>. <source>PLoS One</source>, <volume>10</volume>, <fpage>e0130140.</fpage><pub-id pub-id-type="pmid">26161953</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bagos</surname><given-names>P.G.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Combined prediction of Tat and Sec signal peptides with hidden Markov models</article-title>. <source>Bioinformatics</source>, <volume>26</volume>, <fpage>2811</fpage>–<lpage>2817</lpage>.<pub-id pub-id-type="pmid">20847219</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Berks</surname><given-names>B.C.</given-names></name></person-group> (<year>2015</year>) 
<article-title>The twin-arginine protein translocation pathway</article-title>. <source>Annu. Rev. Biochem</source>., <volume>84</volume>, <fpage>843</fpage>–<lpage>864</lpage>.<pub-id pub-id-type="pmid">25494301</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Chollet</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) Keras. Software available online: <ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link>.</mixed-citation>
    </ref>
    <ref id="btx818-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fariselli</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>Grammatical-restrained hidden conditional random fields for bioinformatics applications</article-title>. <source>Algorithms Mol. Biol</source>., <volume>4</volume>, <fpage>13</fpage>.<pub-id pub-id-type="pmid">19849839</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Indio</surname><given-names>V.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>The prediction of organelle targeting peptides in eukaryotic proteins with Grammatical Restrained Hidden Conditional Random Fields</article-title>. <source>Bioinformatics</source>, <volume>29</volume>, <fpage>981</fpage>–<lpage>988</lpage>.<pub-id pub-id-type="pmid">23428638</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Käll</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2004</year>) 
<article-title>A combined transmembrane topology and signal peptide prediction method</article-title>. <source>J. Mol. Biol</source>., <volume>338</volume>, <fpage>1027</fpage>–<lpage>1036</lpage>.<pub-id pub-id-type="pmid">15111065</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Käll</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2005</year>) 
<article-title>An HMM posterior decoder for sequence feature prediction that includes homology information</article-title>. <source>Bioinformatics</source>, <volume>21</volume>, <fpage>i251</fpage>–<lpage>i257</lpage>.<pub-id pub-id-type="pmid">15961464</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Krizhevsky</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Imagenet classification with deep convolutional neural networks</article-title>. In: <person-group person-group-type="author"><name name-style="western"><surname>Pereira</surname><given-names>F.</given-names></name></person-group> (ed.), <source>Advances in Neural Information Processing Systems</source>, pp. <fpage>1097</fpage>–<lpage>1105</lpage>.</mixed-citation>
    </ref>
    <ref id="btx818-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Deep learning</article-title>. <source>Nature</source>, <volume>521</volume>, <fpage>436</fpage>–<lpage>444</lpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Martoglio</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Dobberstein</surname><given-names>B.</given-names></name></person-group> (<year>1998</year>) 
<article-title>Signal sequences: more than just greasy peptides</article-title>. <source>Trends Cell Biol</source>., <volume>8</volume>, <fpage>410</fpage>–<lpage>415</lpage>.<pub-id pub-id-type="pmid">9789330</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Montavon</surname><given-names>G.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Explaining nonlinear classification decisions with deep Taylor decomposition</article-title>. <source>Pattern Recogn</source>., <volume>65</volume>, <fpage>211</fpage>–<lpage>222</lpage>.</mixed-citation>
    </ref>
    <ref id="btx818-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nugent</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>D.T.</given-names></name></person-group> (<year>2009</year>) 
<article-title>Transmembrane protein topology prediction using support vector machines</article-title>. <source>BMC Bioinformatics</source>, <volume>10</volume>, <fpage>159.</fpage><pub-id pub-id-type="pmid">19470175</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Petersen</surname><given-names>T.N.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>SignalP 4.0: discriminating signal peptides from transmembrane regions</article-title>. <source>Nat. Methods</source>, <volume>8</volume>, <fpage>785</fpage>–<lpage>786</lpage>.<pub-id pub-id-type="pmid">21959131</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Reynolds</surname><given-names>S.M.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>Transmembrane topology and signal peptide prediction using dynamic Bayesian networks</article-title>. <source>PLoS Comput. Biol</source>., <volume>4</volume>, <fpage>e1000213</fpage>.<pub-id pub-id-type="pmid">18989393</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Savojardo</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>BETAWARE: a machine-learning tool to detect and predict transmembrane beta-barrel proteins in Prokaryotes</article-title>. <source>Bioinformatics</source>, <volume>29</volume>, <fpage>504</fpage>–<lpage>505</lpage>.<pub-id pub-id-type="pmid">23297037</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Savojardo</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>ISPRED4: interaction site PREDiction in protein structures with a refining grammar model</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>1656</fpage>–<lpage>1663</lpage>.<pub-id pub-id-type="pmid">28130235</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Simonyan</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Deep inside convolutional networks: visualizing image classification models and saliency maps</article-title>. <source>Comput. Res. Repository</source>, 1312.6034.</mixed-citation>
    </ref>
    <ref id="btx818-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Szegedy</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Intriguing properties of neural networks</article-title>. <source>Comput. Res. Repository</source>, 1312.6199.</mixed-citation>
    </ref>
    <ref id="btx818-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tsirigos</surname><given-names>K.D.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>The TOPCONS web server for consensus prediction of membrane protein topology and signal peptides</article-title>. <source>Nucleic Acids Res</source>., <volume>43</volume>, <fpage>W401</fpage>–<lpage>W407</lpage>.<pub-id pub-id-type="pmid">25969446</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Viklund</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>SPOCTOPUS: a combined predictor of signal peptides and membrane protein topology</article-title>. <source>Bioinformatics</source>, <volume>24</volume>, <fpage>2928</fpage>–<lpage>2929</lpage>.<pub-id pub-id-type="pmid">18945683</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>von Heijne</surname><given-names>G.</given-names></name></person-group> (<year>1990</year>) 
<article-title>The signal peptide</article-title>. <source>J. Membr. Biol</source>., <volume>115</volume>, <fpage>195</fpage>–<lpage>201</lpage>.<pub-id pub-id-type="pmid">2197415</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Troyanskaya</surname><given-names>O.G.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Predicting effects of noncoding variants with deep learning-based sequence model</article-title>. <source>Nature Methods</source>, <volume>12</volume>, <fpage>931</fpage>–<lpage>934</lpage>.<pub-id pub-id-type="pmid">26301843</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5946842</article-id>
    <article-id pub-id-type="pmid">29280997</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btx818</article-id>
    <article-id pub-id-type="publisher-id">btx818</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Sequence Analysis</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepSig: deep learning improves signal peptide detection in proteins</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-7359-0633</contrib-id>
        <name>
          <surname>Savojardo</surname>
          <given-names>Castrense</given-names>
        </name>
        <xref ref-type="aff" rid="btx818-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Martelli</surname>
          <given-names>Pier Luigi</given-names>
        </name>
        <xref ref-type="aff" rid="btx818-aff1">1</xref>
        <xref ref-type="corresp" rid="btx818-cor1"/>
        <!--<email>pierluigi.martelli@unibo.it</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fariselli</surname>
          <given-names>Piero</given-names>
        </name>
        <xref ref-type="aff" rid="btx818-aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Casadio</surname>
          <given-names>Rita</given-names>
        </name>
        <xref ref-type="aff" rid="btx818-aff1">1</xref>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Valencia</surname>
          <given-names>Alfonso</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <aff id="btx818-aff1"><label>1</label>Biocomputing Group, Department of Pharmacy and Biotechnology - Interdepartmental Centre ‘L. Galvani’ for Integrated Studies of Bioinformatics, Biophysics and Biocomplexity, University of Bologna, Bologna, Italy</aff>
    <aff id="btx818-aff2"><label>2</label>Department of Comparative Biomedicine and Food Science (BCA), University of Padova, Padova, Italy</aff>
    <author-notes>
      <corresp id="btx818-cor1">To whom correspondence should be addressed. Email: <email>pierluigi.martelli@unibo.it</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>15</day>
      <month>5</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2017-12-21">
      <day>21</day>
      <month>12</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>21</day>
      <month>12</month>
      <year>2017</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>34</volume>
    <issue>10</issue>
    <fpage>1690</fpage>
    <lpage>1696</lpage>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>6</month>
        <year>2017</year>
      </date>
      <date date-type="rev-recd">
        <day>22</day>
        <month>11</month>
        <year>2017</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>12</month>
        <year>2017</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2017. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2017</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btx818.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The identification of signal peptides in protein sequences is an important step toward protein localization and function characterization.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Here, we present DeepSig, an improved approach for signal peptide detection and cleavage-site prediction based on deep learning methods. Comparative benchmarks performed on an updated independent dataset of proteins show that DeepSig is the current best performing method, scoring better than other available state-of-the-art approaches on both signal peptide detection and precise cleavage-site identification.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>DeepSig is available as both standalone program and web server at <ext-link ext-link-type="uri" xlink:href="https://deepsig.biocomp.unibo.it">https://deepsig.biocomp.unibo.it</ext-link>. All datasets used in this study can be obtained from the same website.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <counts>
      <page-count count="7"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Protein sorting and compartmentalization are complex biological mechanisms, often guided by specific sequence signals present in the nascent protein. Signal peptides are short sequence segments located at the N-termini of newly synthesized proteins that are sorted toward the secretory pathway (<xref rid="btx818-B23" ref-type="bibr">von Heijne, 1990</xref>). Proteins endowed with a signal peptide include proteins resident in endoplasmic reticulum and Golgi apparatus, secreted proteins and proteins inserted in the plasma membrane. Identifying signal peptides in the protein sequence is a prerequisite to unveil protein destination and function.</p>
    <p>Several computational methods have been trained on available experimental data to detect the signal sequence in the N-terminus of a query protein. The most successful methods are based on machine learning models. Artificial Neural Networks and Support Vector Machines learn directly from the available experimental data the signal sequence features (<xref rid="btx818-B14" ref-type="bibr">Nugent and Jones, 2009</xref>; <xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>). Other methods (<xref rid="btx818-B3" ref-type="bibr">Bagos <italic>et al.</italic>, 2010</xref>; <xref rid="btx818-B10" ref-type="bibr">Käll <italic>et al.</italic>, 2005</xref>; <xref rid="btx818-B16" ref-type="bibr">Reynolds <italic>et al.</italic>, 2008</xref>; <xref rid="btx818-B21" ref-type="bibr">Tsirigos <italic>et al.</italic>, 2015</xref>; <xref rid="btx818-B22" ref-type="bibr">Viklund <italic>et al.</italic>, 2008</xref>) adopt Hidden Markov Models to define regular grammars. They explicitly model the modular architecture of the signal sequence, consisting of three regions: the positively charged N-region, the central hydrophobic H-region and the polar uncharged C-region containing the cleavage site (<xref rid="btx818-B12" ref-type="bibr">Martoglio and Dobberstein, 1998</xref>).</p>
    <p>A major challenge in signal peptide prediction is discriminating between true signal sequences and other hydrophobic regions, and, in particular, N-terminal transmembrane helices. The accurate prediction of the cleavage site is also challenging, mainly due to the high variability of the signal sequence length and the absence of sequence motifs that unambiguously mark the position of the cutting site.</p>
    <p>In this paper, we present DeepSig, a new method that takes advantage of Deep Learning advancement and improves the state-of-the-art performance. DeepSig is designed for both detecting signal peptides and finding their cleavage sites in protein sequences. The predictor consists of two consecutive building blocks: a deep neural network architecture and a probabilistic method that incorporates the current biological knowledge of the signal peptide structure.</p>
    <p>In the first stage, the N-terminus of a query protein sequence is analysed to assess the presence of a signal peptide. For this purpose, we designed a Deep Convolutional Neural Network (DCNN) (<xref rid="btx818-B11" ref-type="bibr">LeCun <italic>et al.</italic>, 2015</xref>) architecture (<xref ref-type="fig" rid="btx818-F1">Fig. 1</xref>), specifically tuned to recognize signal peptide sequences. DCNNs are very powerful deep learning architectures that achieve very high performance in several applications (Alipanahai <italic>et al.</italic>, 2015; <xref rid="btx818-B25" ref-type="bibr">Krizhevsky <italic>et al.</italic>, 2012</xref>; <xref rid="btx818-B26" ref-type="bibr">Zhou and Troyanskaya, 2015</xref>). Here, we devise a DCNN comprising three cascading convolution-pooling stages that process the N-terminus of the query protein, sorting out three classes: signal peptides, transmembrane regions and ‘anything else.’
</p>
    <fig id="btx818-F1" orientation="portrait" position="float">
      <label>Fig. 1.</label>
      <caption>
        <p>The architecture of the DCNN processing an input protein sequence to detect signal peptides. Feature extraction involves the application of three convolution-pooling (conv-pool) stages. The final classification is performed by a standard fully-connected neural network </p>
      </caption>
      <graphic xlink:href="btx818f1"/>
    </fig>
    <fig id="btx818-F2" orientation="portrait" position="float">
      <label>Fig. 2.</label>
      <caption>
        <p>The signal-peptide GRHCRF model capturing the modular structure of the signal peptide. States labeled with N, H, and C represents the positively charged N-region, the hydrophobic H-region and the cleavage C-region, respectively (see Section 2.4 for further details)</p>
      </caption>
      <graphic xlink:href="btx818f2"/>
    </fig>
    <p>If a signal peptide is detected, the protein is passed to the next prediction stage where the precise position of the cleavage site is identified (<xref ref-type="fig" rid="btx818-F2">Fig. 2</xref>). This task is tackled in DeepSig as a sequence labelling problem, where each residue is labelled as signal-peptide (S) or not (N). In particular, we adopted a probabilistic sequence labelling model (<xref rid="btx818-B6" ref-type="bibr">Fariselli <italic>et al.</italic>, 2009</xref>) similar to the regular grammars adopted by other HMM-based approaches (<xref rid="btx818-B10" ref-type="bibr">Käll <italic>et al.</italic>, 2004</xref>).</p>
    <p>For improving cleavage site detection, we also applied the Deep Taylor Decomposition (<xref rid="btx818-B13" ref-type="bibr">Montavon <italic>et al</italic>., 2017</xref>) to compute how relevant each residue at the N-terminus is for the recognition of the signal sequence. This score is used as additional feature for the sequence labelling model to improve cleavage-site prediction.</p>
    <p>We trained the DeepSig predictor on the dataset of proteins adopted by SignalP, one of the best performing methods developed so far (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>). It comprises 10 303 non-redundant proteins extracted from UniprotKB and belonging to three different organism classes: Eukaryotes, Gram-positive and Gram-negative bacteria.</p>
    <p>Comparative benchmarks were performed on a new independent validation dataset comprising 1707 sequences with experimental annotations in UniprotKB and not included in the training set. In all experiments, DeepSig outperforms other state-of-the-art approaches in both signal peptide detection and cleavage site prediction. Interestingly, when restricting the negative dataset to the most challenging cases (N-terminal transmembrane regions), DeepSig outperforms state-of-the-art predictors, specifically in the case of Eukaryotic proteins.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Datasets</title>
      <sec>
        <title>2.1.1 The SignalP4.0 dataset</title>
        <p>The first dataset used in this work was generated to train and test the well-known SignalP method (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>). Data were extracted from UniProtKB/SwissProt release 2010_05 including proteins from Eukaryotes, Gram-positive and Gram-negative bacteria. Only proteins with experimentally annotated signal peptide cleavage sites were retained. Negative sets (i.e. proteins lacking a signal peptide) were chosen from two different subsets: (i) proteins experimentally annotated as cytosolic and/or nuclear (ii) proteins experimentally annotated as single- or multi-pass membrane proteins, with a transmembrane segment annotated in the first 70 positions. All data were homology-reduced in order to obtain non-redundant datasets for each of the three organism classes. Two eukaryotic proteins were considered as similar if a local alignment between them included more than 17 identical residues out of 70 N-terminal residues. A threshold of 21 residues was instead used for bacterial proteins. See <xref rid="btx818-T1" ref-type="table">Table 1</xref> for a summary of the SignalP4.0 dataset.
<table-wrap id="btx818-T1" orientation="portrait" position="float"><label>Table 1.</label><caption><p>Statistics of the three datasets adopted in this study</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="left" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Dataset</th><th align="left" rowspan="1" colspan="1">Organism</th><th align="left" rowspan="1" colspan="1">SP</th><th align="left" rowspan="1" colspan="1">T</th><th align="left" rowspan="1" colspan="1">N/C</th><th align="left" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td rowspan="3" colspan="1">SignalP4.0</td><td rowspan="1" colspan="1">Eukaryotes</td><td rowspan="1" colspan="1">1640</td><td rowspan="1" colspan="1">987</td><td rowspan="1" colspan="1">5133</td><td rowspan="1" colspan="1">7760</td></tr><tr><td rowspan="1" colspan="1">Gram-positive</td><td rowspan="1" colspan="1">208</td><td rowspan="1" colspan="1">117</td><td rowspan="1" colspan="1">360</td><td rowspan="1" colspan="1">685</td></tr><tr><td rowspan="1" colspan="1">Gram-negative</td><td rowspan="1" colspan="1">423</td><td rowspan="1" colspan="1">523</td><td rowspan="1" colspan="1">912</td><td rowspan="1" colspan="1">1858</td></tr><tr><td rowspan="3" colspan="1">SPDS17</td><td rowspan="1" colspan="1">Eukaryotes</td><td rowspan="1" colspan="1">46</td><td rowspan="1" colspan="1">323</td><td rowspan="1" colspan="1">689</td><td rowspan="1" colspan="1">1058</td></tr><tr><td rowspan="1" colspan="1">Gram-positive</td><td rowspan="1" colspan="1">9</td><td rowspan="1" colspan="1">189</td><td rowspan="1" colspan="1">240</td><td rowspan="1" colspan="1">438</td></tr><tr><td rowspan="1" colspan="1">Gram-negative</td><td rowspan="1" colspan="1">23</td><td rowspan="1" colspan="1">89</td><td rowspan="1" colspan="1">99</td><td rowspan="1" colspan="1">211</td></tr><tr><td rowspan="1" colspan="1"><italic>E.coli</italic></td><td rowspan="1" colspan="1">–</td><td rowspan="1" colspan="1">573</td><td rowspan="1" colspan="1">1024</td><td rowspan="1" colspan="1">4375</td><td rowspan="1" colspan="1">5972</td></tr></tbody></table><table-wrap-foot><fn id="tblfn1"><p><italic>Note</italic>: SP, signal-peptide proteins; T, transmembrane proteins (with a single alpha helix in the N-terminal region); N/C, Nuclear and/or Cytosolic proteins (proteins without signal peptide); Total, total sum.</p></fn></table-wrap-foot></table-wrap></p>
      </sec>
      <sec>
        <title>2.1.2 The SPDS17 blind dataset</title>
        <p>We generated a new benchmark dataset to compare different approaches on signal peptide detection and cleavage-site prediction. We selected proteins from UniprotKB (rel. 04_2017) released after June 2015. This allowed to exclude any protein already included in the SignalP dataset used for training.</p>
        <p>Positive data were separately collected for Eukaryotes, Gram-negative and Gram-positive (in constructing this set we considered only proteins from <italic>Actinobacteria</italic> and <italic>Firmicutes</italic> phyla) by extracting proteins endowed with an experimentally annotated cleavage site for the signal peptide.</p>
        <p>Next, analogously to the SignalP4.0 dataset, for each organism class, two negative sets were generated: (i) proteins with a membrane-spanning segment in the first 70 residues and (ii) proteins localized into the nucleus and/or the cytoplasm. To generate these sets, we retained only proteins with experimental or manually curated annotation (corresponding to the UniProtKB evidence codes ECO: 0000269 and ECO: 0000305, respectively).</p>
        <p>The set redundancy was reduced to 25% sequence identity by running the blastclust algorithm and retaining a representative sequence from each cluster. Furthermore, we excluded all proteins sharing more than 25% sequence identity with any protein in the SignalP dataset. The blastp program with e-value threshold set to 1e-3 was adopted to search for similar proteins. <xref rid="btx818-T1" ref-type="table">Table 1</xref> contains a summary of the SPDS17 dataset.</p>
      </sec>
      <sec>
        <title>2.1.3 The Escherichia coli proteome</title>
        <p>We assessed DeepSig proteome-wide performance using the entire proteome of <italic>Escherichia coli (strain K12)</italic>. From release 11_2017 UniprotKB we downloaded all the 5972 reviewed entries. The sequences endowed with signal peptide are 573; 1024 have a transmembrane segment annotated in the first 70 residues.</p>
      </sec>
    </sec>
    <sec>
      <title>2.2 Deep convolutional neural networks for signal peptide prediction</title>
      <p>Deep Convolutional Neural Networks (DCNNs) (<xref rid="btx818-B11" ref-type="bibr">LeCun <italic>et al.</italic>, 2015</xref>) are powerful deep learning models devised to process multi-channel input data. Several data types fall in this category. The main application domain of DCNN is image processing (e.g. image object recognition or segmentation), where each pixel of a 2-Dimensional image is encoded by a vector of three intensity channels.</p>
      <p>Here, we apply DCNNs to protein sequence analysis. In this case, the input domain is a 1-dimensional signal, where each position in a sequence is represented by a multi-channel (i.e. multi-dimensional) vector encoding the residue type at each position of a protein, one channel for each residue type.</p>
      <p>Signal-peptide prediction is a special task of protein classification where the goal is to detect the presence/absence of the signal sequence in the N-terminus of the protein. <xref ref-type="fig" rid="btx818-F1">Figure 1</xref> summarizes the architecture of the DCNN defined in this paper for signal peptide prediction, comprising two basic modules: the feature extraction and the classification.</p>
      <sec>
        <title>2.2.1 Feature extraction module</title>
        <p>The feature extraction module consists of several hierarchical convolution (conv) and pooling (pool) layers which collectively compute a feature representation of the input protein sequence. Convolutional layers can be seen as sequence motif detectors used to scan the input sequence. A convolutional layer is mainly characterized by the number of motifs (or filters) it applies and by the motif length. Each motif detector slides along the input sequence, and computes the positional score for the motif at any sequence position. The scores are stored in the convolution neurons. Motif parameters are learnt during training and, routinely, parameter sharing is enforced (i.e. the same motif weights are applied to all positions during sequence scanning). After convolution, pooling layers are applied to aggregate neighbour convolution neurons into a single output neuron, with a consequent reduction of dimensionality. Typical pooling operations include max or average functions, computed over short non-overlapping slices of convolution neurons. The main parameter of a pooling layer is the width of the slice adopted. Iterative applications of convolution-pooling (conv-pool) operations are performed to extract a complex feature representation of the input sequence. In fact, a hierarchical feature extraction protocol is adopted where low-level motifs are progressively aggregated to model higher level inter-motif interactions. Adding conv-pool layers to the network allows extracting complex patterns of interaction through motifs, though increasing the complexity of the network.</p>
        <p>More formally, an input protein sequence is defined as a <inline-formula id="IE1"><mml:math id="IM1"><mml:mi>l</mml:mi><mml:mo>×</mml:mo><mml:mn>20</mml:mn></mml:math></inline-formula> matrix <inline-formula id="IE2"><mml:math id="IM2"><mml:mi>X</mml:mi></mml:math></inline-formula> where <inline-formula id="IE3"><mml:math id="IM3"><mml:mi>l</mml:mi></mml:math></inline-formula> is the sequence length and <inline-formula id="IE4"><mml:math id="IM4"><mml:mn>20</mml:mn></mml:math></inline-formula> is the number of different residue types. Here, protein sequences are shortened to the 96 N-terminal residues, hence <inline-formula id="IE5"><mml:math id="IM5"><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>96</mml:mn></mml:math></inline-formula>.</p>
        <p>A motif detector of odd-sized width <inline-formula id="IE6"><mml:math id="IM6"><mml:mi>w</mml:mi></mml:math></inline-formula> in the first convolution layer is defined as a weight matrix <inline-formula id="IE7"><mml:math id="IM7"><mml:mi>F</mml:mi></mml:math></inline-formula> of dimension <inline-formula id="IE8"><mml:math id="IM8"><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mn>20</mml:mn></mml:math></inline-formula>. If <inline-formula id="IE9"><mml:math id="IM9"><mml:mi>f</mml:mi></mml:math></inline-formula> different motif detectors are applied, the output of the convolution layer is a <inline-formula id="IE10"><mml:math id="IM10"><mml:mi>l</mml:mi><mml:mo>×</mml:mo><mml:mi>f</mml:mi></mml:math></inline-formula> matrix <inline-formula id="IE11"><mml:math id="IM11"><mml:mi>C</mml:mi></mml:math></inline-formula>, where the element <inline-formula id="IE12"><mml:math id="IM12"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is computed as:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mtext>(</mml:mtext><mml:mi>w</mml:mi><mml:mo>−</mml:mo><mml:mtext>1)</mml:mtext><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mtext>(</mml:mtext><mml:mi>w</mml:mi><mml:mo>−</mml:mo><mml:mtext>1)</mml:mtext><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>F</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mtext>(</mml:mtext><mml:mi>w</mml:mi><mml:mo>−</mml:mo><mml:mtext>1)</mml:mtext><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE13"><mml:math id="IM13"><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>is <inline-formula id="IE14"><mml:math id="IM14"><mml:mi>j</mml:mi></mml:math></inline-formula>-th motif weight matrix and <inline-formula id="IE15"><mml:math id="IM15"><mml:mtext mathvariant="italic">max</mml:mtext><mml:mfenced open="(" close=")" separators="|"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula> is the <italic>rectified linear unit</italic> (ReLU) activation function. Using ReLUs instead of other activation functions (such as tanh or sigmoid) speeds up the training process, particularly in networks with many layers (<xref rid="btx818-B11" ref-type="bibr">LeCun <italic>et al.</italic>, 2015</xref>).</p>
        <p>The role of the pooling layer with pool size equal to <inline-formula id="IE18"><mml:math id="IM16"><mml:mi>s</mml:mi></mml:math></inline-formula>, applied to each motif of matrix <inline-formula id="IE19"><mml:math id="IM17"><mml:mi>C</mml:mi></mml:math></inline-formula>, is to reduce its dimensionality by merging together <inline-formula id="IE20"><mml:math id="IM18"><mml:mi>s</mml:mi></mml:math></inline-formula> neighbour convolutional neurons into one. Although other schemes are possible, here average pooling is applied to adjacent pairs of convolution neurons, leading to dimensionality reduction from <inline-formula id="IE21"><mml:math id="IM19"><mml:mi>l</mml:mi></mml:math></inline-formula> to <inline-formula id="IE22"><mml:math id="IM20"><mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The pooling layer computes a <inline-formula id="IE23"><mml:math id="IM21"><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>f</mml:mi></mml:math></inline-formula> matrix <inline-formula id="IE24"><mml:math id="IM22"><mml:mi>P</mml:mi></mml:math></inline-formula> defined as follows:
<disp-formula id="E2"><label>(2)</label><graphic xlink:href="btx818m2.jpg" position="float" orientation="portrait"/></disp-formula>
where <italic>i</italic> ranges between 1 and m=l/2. Overall, a single conv-pool application transforms an input sequence of dimension <inline-formula id="IE25"><mml:math id="IM23"><mml:mi>l</mml:mi><mml:mo>×</mml:mo><mml:mn>20</mml:mn></mml:math></inline-formula> to a non-linear feature representation of dimension <inline-formula id="IE26"><mml:math id="IM24"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:mi>f</mml:mi></mml:math></inline-formula>. Hence, a series of <inline-formula id="IE27"><mml:math id="IM25"><mml:mi>r</mml:mi></mml:math></inline-formula> conv-pool stages, stacked together, extracts a non-linear feature space representation of dimension <inline-formula id="IE28"><mml:math id="IM26"><mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, with <inline-formula id="IE29"><mml:math id="IM27"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo></mml:math></inline-formula>being the number of motif detectors in the last conv layer.</p>
        <p>In our final network, we apply three cascading conv-pool stages. Different architectures were tried and selected through cross-validation, varying the number of motif detectors and motif width on each conv layer.</p>
      </sec>
      <sec>
        <title>2.2.2 Classification module</title>
        <p>The output classification is performed by means of a module implementing a conventional fully connected feed-forward neural network, comprising a single hidden-layer with <inline-formula id="IE30"><mml:math id="IM28"><mml:mi>h</mml:mi></mml:math></inline-formula> neurons. The number of neurons in the hidden layer was varied and optimized through cross-validation, separately for each organism class. Firstly, the computed feature representation is flattened into a column vector <inline-formula id="IE31"><mml:math id="IM29"><mml:mi>v</mml:mi></mml:math></inline-formula> that encodes the input of the feed-forward network.</p>
        <p>Each neuron <inline-formula id="IE32"><mml:math id="IM30"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in the hidden layer computes a non-linear transformation, defined as follows:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mo>α</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE33"><mml:math id="IM31"><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE34"><mml:math id="IM32"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are, respectively, the weight vector and bias of the hidden neuron <inline-formula id="IE35"><mml:math id="IM33"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (again, the ReLU activation is used).</p>
        <p>Finally, the hidden layer output vector <inline-formula id="IE36"><mml:math id="IM34"><mml:mi>h</mml:mi></mml:math></inline-formula> is mapped to the <inline-formula id="IE37"><mml:math id="IM35"><mml:mi>i</mml:mi></mml:math></inline-formula>-th output neuron as follows:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi>h</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mo>β</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE38"><mml:math id="IM36"><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE39"><mml:math id="IM37"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are, respectively, the weight vector and the bias of the output neuron <inline-formula id="IE40"><mml:math id="IM38"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and the function <inline-formula id="IE41"><mml:math id="IM39"><mml:mi>t</mml:mi></mml:math></inline-formula> is the <inline-formula id="IE42"><mml:math id="IM40"><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:math></inline-formula> function, allowing a probabilistic interpretation of the network output.</p>
        <p>The final output of our DCNN comprises three output neurons accounting for three different output classes: signal peptide (S), transmembrane segment (T) or other (N). This three-class schema allows to reduce the misclassification between transmembrane regions and signal peptides (Section Results). An input protein sequence is classified into the class <inline-formula id="IE43"><mml:math id="IM41"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> with the highest predicted probability, namely:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mrow><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>argmax</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>o</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Given a training set <inline-formula id="IE44"><mml:math id="IM42"><mml:mo>θ</mml:mo><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math></inline-formula> of <inline-formula id="IE45"><mml:math id="IM43"><mml:mi>N</mml:mi></mml:math></inline-formula> protein sequences with true output targets, network parameters are optimized by minimizing the average cross-entropy loss function on the training set, defined as:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6"><mml:mi>L</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>θ</mml:mo></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE46"><mml:math id="IM44"><mml:msubsup><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msubsup></mml:math></inline-formula> is the <inline-formula id="IE47"><mml:math id="IM45"><mml:mi>j</mml:mi></mml:math></inline-formula>-th network output when the <inline-formula id="IE48"><mml:math id="IM46"><mml:mi>i</mml:mi></mml:math></inline-formula>-th sequence is provided in input.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 Evaluating residue positional relevance with deep Taylor decomposition</title>
      <p>The DCNN described in the previous section is designed to provide a prediction of the presence/absence of the signal peptide sequence in the N-terminus of an input protein. In general, when such predictions are performed with DCNN, some of the elements of an input sequence (i.e. individual residues) may be more determinant than others in driving the model classification toward one specific class or another. An important question is then how this piece of information can be extracted from the analysis of the internal neuronal activity of DCNN.</p>
      <p>Many methods are available to analyse the complex behaviour of non-linear classifiers in the attempt of quantifying the importance of basic elements in the input data with respect to the task at hand (<xref rid="btx818-B2" ref-type="bibr">Bach <italic>et al.</italic>, 2015</xref>; <xref rid="btx818-B13" ref-type="bibr">Montavon <italic>et al</italic>., 2017</xref>; <xref rid="btx818-B19" ref-type="bibr">Simonyan <italic>et al.</italic>, 2013</xref>). For instance, in image classification, one wants to identify a subset of relevant pixels that are responsible for the recognition of an object in the image (<xref rid="btx818-B2" ref-type="bibr">Bach <italic>et al.</italic>, 2015</xref>; <xref rid="btx818-B13" ref-type="bibr">Montavon <italic>et al</italic>., 2017</xref>; <xref rid="btx818-B19" ref-type="bibr">Simonyan <italic>et al.</italic>, 2013</xref>; <xref rid="btx818-B20" ref-type="bibr">Szegedy <italic>et al.</italic>, 2013</xref>). In the context of signal peptide detection, given an input protein sequence in which a signal peptide has been recognized, we want to identify residue positions along the sequence that are more relevant for the global recognition of the signal.</p>
      <p>Available methods can be roughly classified into two different categories: functional approaches look at networks as function approximators and highlight the most relevant input features by analysing the prediction function (<xref rid="btx818-B19" ref-type="bibr">Simonyan <italic>et al.</italic>, 2013</xref>); message passing approaches exploit the network as a computational graph and propagate prediction values throughout the different layers back to input variables (<xref rid="btx818-B2" ref-type="bibr">Bach <italic>et al.</italic>, 2015</xref>).</p>
      <p>Here, we adopt the deep Taylor decomposition (<xref rid="btx818-B13" ref-type="bibr">Montavon <italic>et al.</italic>, 2017</xref>), a hybrid functional/message passing approach that has been recently introduced for the analysis of deep neural networks. The method focuses on image classification, but it can be easily extended to other types of prediction scenarios, such as protein sequence classification. We briefly describe here its main aspects and refer to the original paper for a comprehensive mathematical description of the method (<xref rid="btx818-B13" ref-type="bibr">Montavon <italic>et al.</italic>, 2017</xref>) and to our <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref> for a description of how this method can be applied to our signal-peptide DCNN.</p>
      <p>Let be <inline-formula id="IE49"><mml:math id="IM47"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> an input protein sequence of length <inline-formula id="IE50"><mml:math id="IM48"><mml:mi>l</mml:mi></mml:math></inline-formula> where each <inline-formula id="IE51"><mml:math id="IM49"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>20</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> is a <inline-formula id="IE52"><mml:math id="IM50"><mml:mn>20</mml:mn></mml:math></inline-formula>-channel vector representing a residue in the sequence. <inline-formula id="IE53"><mml:math id="IM51"><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:math></inline-formula> is the scalar function implemented with a DCNN and evaluated on the input <inline-formula id="IE54"><mml:math id="IM52"><mml:mi mathvariant="bold">x</mml:mi></mml:math></inline-formula>. The function <inline-formula id="IE55"><mml:math id="IM53"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> quantifies the evidence (or score) that a signal peptide is present in the N-terminus of the sequence <inline-formula id="IE56"><mml:math id="IM54"><mml:mi mathvariant="bold">x</mml:mi></mml:math></inline-formula>. We want to assign to each residue <inline-formula id="IE57"><mml:math id="IM55"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> a <italic>relevance score</italic><inline-formula id="IE58"><mml:math id="IM56"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> that quantifies the individual contribution of that residue to the total predicted evidence function <inline-formula id="IE59"><mml:math id="IM57"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>.</p>
      <p>Operatively, deep Taylor decomposition proceeds by assigning to each neuron in a deep network a relevance score which is a measure of the contribution of the neuron to the total predicted score <inline-formula id="IE60"><mml:math id="IM58"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>. Neuron relevance scores are computed by establishing local, connectivity-dependent functional mappings between neuron activation values and propagated relevance values from upper-layers. Taylor expansions of these local mappings at neuron-specific root points are then computed. Depending on the functional form of the mappings and on the nature of the input domain, different relevance propagation rules are defined (for details, see <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>).</p>
      <p>We apply this procedure to our signal peptide DCNN to evaluate the contribution of each residue position to the detection of the signal sequence. The result for a sequence in input of length <inline-formula id="IE61"><mml:math id="IM59"><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>96</mml:mn></mml:math></inline-formula> is a vector:
<disp-formula id="E7"><label>(7)</label><mml:math id="M7"><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where the component <inline-formula id="IE62"><mml:math id="IM60"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> is the relevance of the residue in position <inline-formula id="IE63"><mml:math id="IM61"><mml:mi>i</mml:mi></mml:math></inline-formula>.</p>
    </sec>
    <sec>
      <title>2.4 Prediction of the signal peptide cleavage site</title>
      <p>When a signal peptide is detected with the DCNN, the protein sequence passes to the second prediction stage which identifies the location of the cleavage site. In particular, each residue of a positively-predicted sequence is assigned to one of two classes: signal peptide (S) or non-signal region (N).</p>
      <p>Here, we adopt a Grammar-Restrained Hidden Conditional Random Field (GRHCRF) (<xref rid="btx818-B6" ref-type="bibr">Fariselli <italic>et al.</italic>, 2009</xref>; <xref rid="btx818-B8" ref-type="bibr">Indio <italic>et al</italic>., 2013</xref>; <xref rid="btx818-B17" ref-type="bibr">Savojardo <italic>et al</italic>., 2013</xref>; <xref rid="btx818-B18" ref-type="bibr">Savojardo <italic>et al</italic>., 2017</xref>). Like HMMs, a GRHCRF can be represented as a finite state automaton whose state structure and transitions reflect a regular grammar describing the problem at hand (<xref rid="btx818-B6" ref-type="bibr">Fariselli <italic>et al.</italic>, 2009</xref>). Each state of the model is associated to a label that can be assigned to each element of a sequence. Model parameters are weights that score the compatibility between input sequences included in the training set and their true labelling. Once the model has been trained, sequence labelling is performed by assigning labels corresponding to the most probable state path in the model. The optimal state path is computed by means of Posterior-Viterbi decoding (<xref rid="btx818-B6" ref-type="bibr">Fariselli <italic>et al.</italic>, 2009</xref>).</p>
      <p>The GHRCRF model is defined on top of the grammar depicted in <xref ref-type="fig" rid="btx818-F2">Figure 2</xref> as a finite-state automaton. The model defines different states organized to capture the modular structure of a typical signal peptide: 7 states to model the initial positively charged N-region (states N1–N7), 11 states for the hydrophobic H-region (states H1–H11) and 13 states for the cleavage C-region (states C1–C13). State transitions are defined such that minimal and maximal lengths for each sub-region are enforced. In particular, N-regions can be from two up to seven residues long. In contrast, the H-region has a minimal length of four residues with no upper bound. Finally, C-regions comprise between 3 and 13 residues. The remaining mature protein portion is modelled through a single recursive state (G0). The cleavage site corresponds to the position of the residue assigned to state C13.</p>
      <p>Training of the GRHCRF is performed on a training set of protein sequences endowed with signal peptides. Also in this case, sequences are reduced to the first 96 N-terminal residues. Each residue is encoded using a 21-dimensional feature vector consisting of:
<list list-type="bullet"><list-item><p>20 positions of the vector correspond to the usual residue encoding described above;</p></list-item><list-item><p>the relevance score of the residue computed from the DCNN and deep Taylor decomposition as described in Section 2.3.</p></list-item></list></p>
    </sec>
    <sec>
      <title>2.5 Model optimization and implementation</title>
      <p>All the models are trained on the SignalP4.0 dataset using a nested 5-fold cross-validation procedure as done in <xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic> (2011)</xref>. Three different optimization runs are performed on Eukaryotic, Gram-positive and Gram-negative, respectively.</p>
      <p>Firstly, the entire dataset is randomly split into five subsets containing broadly the same number of proteins. Random splits are computed so that the balancing between signal-peptide, transmembrane and other proteins is maintained on each subset and it is similar to the one observed in the whole dataset.</p>
      <p>Secondly, a nested cross-validation procedure is performed as follows: one subset is kept out and used for testing while a full inner 4-fold cross-validation is performed on the remaining four subsets. In each run of this inner procedure, three subsets are used for training and one for validation. The inner cross-validation is used to optimize the network parameters and architecture. In fact, we retain the top-performing network as evaluated on the inner validation sets. The procedure is repeated leaving out each time a different subset for testing. In summary, 20 different networks are obtained (four optimal networks that are identical in parameters and architecture but have been trained on different inner training sets for each one of the five main subsets). When performance is evaluated on the testing set, outputs of the four inner networks are averaged to give the final score. In the final version of our DeepSig predictor, we average the output of all the 20 optimal networks.</p>
      <p>The same procedure and data split was applied to train/test the cleavage site predictor based on the GRHCRF model.</p>
      <p>The DCNN is implemented using the Keras Python package (<ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link>) (<xref rid="btx818-B5" ref-type="bibr">Chollet <italic>et al.</italic>, 2015</xref>) with the Tensorflow (<ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org">https://www.tensorflow.org</ext-link>) (<xref rid="btx818-B1" ref-type="bibr">Abadi <italic>et al.</italic>, 2015</xref>) backend. The categorical cross-entropy loss minimization is carried-out with the standard error back-propagation procedure and the stochastic gradient descent algorithm. Default hyper-parameters were used for training the networks (Default hyper-parameters for network training were set to 0.01 for learning rate and to 0 for momentum and weight decay).</p>
    </sec>
    <sec>
      <title>2.6 Scoring measures</title>
      <p>Signal-peptide detection is scored using the following measures:
<list list-type="bullet"><list-item><p>Matthews Correlation Coefficient (MCC), defined as:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8"><mml:mrow><mml:mtext>MCC</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>×</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>-</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>×</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where TP and TN are the correct predictions in the positive and negative classes, respectively, and FN and FP are the number of under- and over-predictions in the signal peptide class</p></list-item><list-item><p>False positive rate computed on transmembrane proteins, defined as:
<disp-formula id="E9"><label>(9)</label><mml:math id="M9"><mml:mrow><mml:msub><mml:mrow><mml:mtext>FPR</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mtext>FP</mml:mtext><mml:mi>T</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where FP<sub><italic>T</italic></sub> is the number of transmembrane proteins misclassified as having a signal peptide and N<sub><italic>T</italic></sub> is the total number of transmembrane proteins.</p></list-item><list-item><p>Cleavage-site prediction is scored by the cleavage-site F1 measure defined as:
<disp-formula id="E10"><label>(10)</label><mml:math id="M10"><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>CS</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula>
namely, the harmonic mean between Cleavage Site Sensitivity, <inline-formula id="IE64"><mml:math id="IM62"><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> and Cleavage Site Precision, <inline-formula id="IE65"><mml:math id="IM63"><mml:msub><mml:mrow><mml:mi mathvariant="normal">C</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></inline-formula>, where <italic>N</italic><sub>corr</sub> is the number of correctly identified cleavage sites and <italic>N</italic> and <italic>N<sub>P</sub></italic> are, respectively, the true number of signal peptides and the number of predicted signal peptides.</p></list-item></list></p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Performance on the SignalP4.0 dataset</title>
      <p>We firstly evaluate the performance of our DeepSig predictor on the dataset adopted to train and test SignalP4.0 (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>) with the same nested cross-validation procedure described in Section 2.5.</p>
      <p>This allows a direct and accurate comparison of DeepSig with SignalP4.0. In particular, three different versions of SignaP are scored: SignalP-TM, the version of the method optimized to distinguish signal peptides from transmembrane regions; SignalP-noTM which is not optimized and SignalP4.0 which is a combination of the two methods above.</p>
      <p>Our DeepSig predictor is also evaluated in two versions, either using the relevance profile as feature for cleavage-site prediction (Section 2.4) or not (‘no relevance’ in <xref rid="btx818-T2" ref-type="table">Table 2</xref>).
<table-wrap id="btx818-T2" orientation="portrait" position="float"><label>Table 2.</label><caption><p>Performance of different versions of SignalP and DeepSig on signal peptide detection and cleavage site prediction in 5-fold cross-validation on the SignalP4.0 dataset (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>)</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/></colgroup><thead><tr><th rowspan="2" colspan="1">Method</th><th colspan="3" rowspan="1">Eukaryotes<hr/></th><th colspan="3" rowspan="1">Gram-positive<hr/></th><th colspan="3" rowspan="1">Gram-negative<hr/></th></tr><tr><th rowspan="1" colspan="1">MCC</th><th rowspan="1" colspan="1">FPR<sub>T</sub></th><th rowspan="1" colspan="1">F1<sub>cs</sub></th><th rowspan="1" colspan="1">MCC</th><th rowspan="1" colspan="1">FPR<sub>T</sub></th><th rowspan="1" colspan="1">F1<sub>cs</sub></th><th rowspan="1" colspan="1">MCC</th><th rowspan="1" colspan="1">FPR<sub>T</sub></th><th rowspan="1" colspan="1">F1<sub>cs</sub></th></tr></thead><tbody><tr><td rowspan="1" colspan="1">SignalP 4.0<xref ref-type="table-fn" rid="tblfn3"><sup>a</sup></xref></td><td rowspan="1" colspan="1">0.874</td><td rowspan="1" colspan="1">6.1</td><td rowspan="1" colspan="1">67.1</td><td rowspan="1" colspan="1">0.851</td><td rowspan="1" colspan="1">2.6</td><td rowspan="1" colspan="1">77.8</td><td rowspan="1" colspan="1">0.848</td><td rowspan="1" colspan="1">1.5</td><td rowspan="1" colspan="1">68.0</td></tr><tr><td rowspan="1" colspan="1">SignalP-TM<xref ref-type="table-fn" rid="tblfn3"><sup>a</sup></xref></td><td rowspan="1" colspan="1">0.871</td><td rowspan="1" colspan="1">3.3</td><td rowspan="1" colspan="1">67.2</td><td rowspan="1" colspan="1">0.851</td><td rowspan="1" colspan="1">2.6</td><td rowspan="1" colspan="1">77.8</td><td rowspan="1" colspan="1">0.815</td><td rowspan="1" colspan="1">1.1</td><td rowspan="1" colspan="1">67.7</td></tr><tr><td rowspan="1" colspan="1">SignalP-noTM<xref ref-type="table-fn" rid="tblfn3"><sup>a</sup></xref></td><td rowspan="1" colspan="1">0.674</td><td rowspan="1" colspan="1">38.1</td><td rowspan="1" colspan="1">54.6</td><td rowspan="1" colspan="1">0.556</td><td rowspan="1" colspan="1">47.9</td><td rowspan="1" colspan="1">49.4</td><td rowspan="1" colspan="1">0.497</td><td rowspan="1" colspan="1">35.8</td><td rowspan="1" colspan="1">67.7</td></tr><tr><td rowspan="1" colspan="1">DeepSig (no relevance)</td><td rowspan="1" colspan="1">0.910</td><td rowspan="1" colspan="1">2.6</td><td rowspan="1" colspan="1">71.1</td><td rowspan="1" colspan="1">0.878</td><td rowspan="1" colspan="1">5.9</td><td rowspan="1" colspan="1">69.7</td><td rowspan="1" colspan="1">0.900</td><td rowspan="1" colspan="1">1.5</td><td rowspan="1" colspan="1">83.5</td></tr><tr><td rowspan="1" colspan="1">DeepSig</td><td rowspan="1" colspan="1">0.910</td><td rowspan="1" colspan="1">2.6</td><td rowspan="1" colspan="1">73.3</td><td rowspan="1" colspan="1">0.878</td><td rowspan="1" colspan="1">5.9</td><td rowspan="1" colspan="1">72.3</td><td rowspan="1" colspan="1">0.900</td><td rowspan="1" colspan="1">1.5</td><td rowspan="1" colspan="1">86.2</td></tr></tbody></table><table-wrap-foot><fn id="tblfn2"><p><italic>Note</italic>: MCC, Matthews Correlation Coefficient; FPR<sub>T</sub>, False Positive Rate on transmembrane proteins; F1<sub>cs</sub>, The harmonic mean between precision and recall on cleavage-site detection. No relevance = without relevance profile as feature for cleavage-site prediction (Section 2.4).</p></fn><fn id="tblfn3"><label>a</label><p>Data taken from <xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic> (2011)</xref>.</p></fn></table-wrap-foot></table-wrap></p>
      <p>Comparative results of both signal peptide detection and cleavage site prediction are reported in <xref rid="btx818-T2" ref-type="table">Table 2</xref>. Methods are trained and scored separately on each organism class: Eukaryotes, Gram-positive and Gram-negative. Results for SignalP are derived from the original paper (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>).</p>
      <p>The first aspect evaluated is the detection of the signal peptide (with the MCC and FPR<sub>T</sub> scoring indexes). DeepSig outperforms SignalP (considering the MCC values on <xref rid="btx818-T2" ref-type="table">Table 2</xref>) on all the three datasets (Eukaryotes, Gram-positive and Gram-negative proteins).</p>
      <p>Specifically, on the Eukaryote dataset our method shows a lower false positive rate on proteins with a transmembrane segment annotated in the first 70 residues (<xref rid="btx818-T2" ref-type="table">Table 2</xref>, FPR<sub>T</sub>). It is well known that the ability to distinguish true signal peptides from N-terminal transmembrane regions is one of the main challenges for signal-peptide detection methods, due the similar physical-chemical profiles (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>). In this respect, DeepSig scores with a false positive rate of 2.6%, lower than that of SignalP-TM (3.3%) and a higher MCC value. In absolute terms, DeepSig and SignalP-TM produce 20 and 27 false positive predictions out of 787 transmembrane proteins, respectively.</p>
      <p>On the two other datasets (Gram-positive and Gram-negative bacteria) DeepSig scores on transmembrane proteins with a false positive rate of 5.9% and 1.5%, respectively. On Gram-positive bacteria, the DeepSig false positive rate is higher compared to that reported SignalP-TM (<xref rid="btx818-T2" ref-type="table">Table 2</xref>). It is possible that low number of transmembrane proteins of Gram-positives hampers the ability of the DCNN to discriminate true signal sequence from transmembrane regions.</p>
      <p>The second aspect evaluated is the ability to identify the correct location of the cleavage site. As described in Section 2.4, our method is based on a probabilistic sequence labelling approach which makes use of the relevance profile computed by means of deep Taylor decomposition. For this reason, we are interested in quantifying the impact of this additional feature on the cleavage-site prediction performance. As highlighted in <xref rid="btx818-T2" ref-type="table">Table 2</xref>, considering the F1cs values of all the three protein sets, the inclusion of the relevance profile leads to a better F1 score in cleavage-site prediction of DeepSig. This demonstrates that the relevance profile, when incorporated into the probabilistic sequence labelling method, provides additional information that, in conjunction with primary sequence, helps in identifying the correct extent of the signal sequence.</p>
      <p>Comparing results in <xref rid="btx818-T2" ref-type="table">Table 2</xref>, we can conclude that the cleavage site position is better predicted by DeepSig than SignalP, with the exception of Gram positive bacteria. The improvement ranges from 2% to 4%.</p>
    </sec>
    <sec>
      <title>3.2 Performance on the SPDS17 independent dataset</title>
      <p>Five state-of-the-art predictors are benchmarked toghether with DeepSig on an independent and blind SPDS17 validation set. The predictors are: SignalP4.1 (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>), TOPCONS2.0 (<xref rid="btx818-B21" ref-type="bibr">Tsirigos <italic>et al.</italic>, 2015</xref>), SPOCTOPUS (<xref rid="btx818-B22" ref-type="bibr">Viklund <italic>et al.</italic>, 2008</xref>), PolyPhobius (<xref rid="btx818-B10" ref-type="bibr">Käll <italic>et al.</italic>, 2005</xref>), Philius (<xref rid="btx818-B16" ref-type="bibr">Reynolds <italic>et al.</italic>, 2008</xref>) and PRED-TAT (<xref rid="btx818-B3" ref-type="bibr">Bagos <italic>et al.</italic>, 2010</xref>), all based on different and well established methods. Again, predictions were generated separately on Eukaryote, Gram-positive and Gram-negative data, either launching the sequences on the respective web-servers or running in-house the standalone versions. Three complementary aspects are compared: the efficiency of the signal peptide detection evaluated with the Matthews correlation coefficient (MCC), the precision of the discrimination between signal peptides and N-terminal transmembrane regions, and the performance on the prediction of the cleavage-site, measured with the F1 score.</p>
      <p>For all the organism classes and for all the considered aspects, DeepSig reports the best performances (<xref rid="btx818-T3" ref-type="table">Table 3</xref>). The MCCs of the signal peptide detection are 2 to 4 percentage points higher than the state-of-the art SignalP4.1. When restricting the negative dataset to the most challenging cases (N-terminal transmembrane regions), DeepSig reports the best false positive rate, outperforming SignalP4.1 by 1.5% in the case of eukaryotic proteins. Moreover, DeepSig gives a more exact prediction of the cleavage site in all the three organisms, as highlighted by the cleavage-site F1 values.
<table-wrap id="btx818-T3" orientation="portrait" position="float"><label>Table 3.</label><caption><p>Comparative benchmark of different methods in signal peptide detection and cleavage site prediction on the SPDS17 independent dataset</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/><col valign="top" align="char" char="." span="1"/></colgroup><thead><tr><th rowspan="2" colspan="1">Method</th><th colspan="3" rowspan="1">Eukaryotes<hr/></th><th colspan="3" rowspan="1">Gram-positive<hr/></th><th colspan="3" rowspan="1">Gram-negative<hr/></th></tr><tr><th rowspan="1" colspan="1">MCC</th><th rowspan="1" colspan="1">FPR<sub>T</sub></th><th rowspan="1" colspan="1">F1<sub>cs</sub></th><th rowspan="1" colspan="1">MCC</th><th rowspan="1" colspan="1">FPR<sub>T</sub></th><th rowspan="1" colspan="1">F1<sub>cs</sub></th><th rowspan="1" colspan="1">MCC</th><th rowspan="1" colspan="1">FPR<sub>T</sub></th><th rowspan="1" colspan="1">F1<sub>cs</sub></th></tr></thead><tbody><tr><td rowspan="1" colspan="1">SPOCTOPUS</td><td rowspan="1" colspan="1">0.54</td><td rowspan="1" colspan="1">16.7</td><td rowspan="1" colspan="1">0.20</td><td rowspan="1" colspan="1">0.28</td><td rowspan="1" colspan="1">20.2</td><td rowspan="1" colspan="1">0.37</td><td rowspan="1" colspan="1">0.63</td><td rowspan="1" colspan="1">14.3</td><td rowspan="1" colspan="1">0.12</td></tr><tr><td rowspan="1" colspan="1">PRED-TAT</td><td rowspan="1" colspan="1">0.55</td><td rowspan="1" colspan="1">9.3</td><td rowspan="1" colspan="1">0.33</td><td rowspan="1" colspan="1">0.26</td><td rowspan="1" colspan="1">2.2</td><td rowspan="1" colspan="1">0.72</td><td rowspan="1" colspan="1">0.82</td><td rowspan="1" colspan="1">9.9</td><td rowspan="1" colspan="1">0.14</td></tr><tr><td rowspan="1" colspan="1">Philius</td><td rowspan="1" colspan="1">0.62</td><td rowspan="1" colspan="1">6.5</td><td rowspan="1" colspan="1">0.46</td><td rowspan="1" colspan="1">0.31</td><td rowspan="1" colspan="1">3.4</td><td rowspan="1" colspan="1">0.72</td><td rowspan="1" colspan="1">0.87</td><td rowspan="1" colspan="1">7.4</td><td rowspan="1" colspan="1">0.22</td></tr><tr><td rowspan="1" colspan="1">PolyPhobius</td><td rowspan="1" colspan="1">0.73</td><td rowspan="1" colspan="1">7.4</td><td rowspan="1" colspan="1">0.42</td><td rowspan="1" colspan="1">0.44</td><td rowspan="1" colspan="1">11.2</td><td rowspan="1" colspan="1">0.53</td><td rowspan="1" colspan="1">0.80</td><td rowspan="1" colspan="1">7.9</td><td rowspan="1" colspan="1">0.06</td></tr><tr><td rowspan="1" colspan="1">TOPCONS2.0</td><td rowspan="1" colspan="1">0.74</td><td rowspan="1" colspan="1">5.3</td><td rowspan="1" colspan="1">0.27</td><td rowspan="1" colspan="1">0.49</td><td rowspan="1" colspan="1">4.5</td><td rowspan="1" colspan="1">0.60</td><td rowspan="1" colspan="1">0.91</td><td rowspan="1" colspan="1">2.6</td><td rowspan="1" colspan="1">0.08</td></tr><tr><td rowspan="1" colspan="1">SignalP4.1</td><td rowspan="1" colspan="1">0.82</td><td rowspan="1" colspan="1">4.0</td><td rowspan="1" colspan="1">0.69</td><td rowspan="1" colspan="1">0.50</td><td rowspan="1" colspan="1">0.0</td><td rowspan="1" colspan="1">0.79</td><td rowspan="1" colspan="1">0.93</td><td rowspan="1" colspan="1">4.2</td><td rowspan="1" colspan="1">0.33</td></tr><tr><td rowspan="1" colspan="1">DeepSig</td><td rowspan="1" colspan="1">0.86</td><td rowspan="1" colspan="1">2.5</td><td rowspan="1" colspan="1">0.72</td><td rowspan="1" colspan="1">0.54</td><td rowspan="1" colspan="1">0.0</td><td rowspan="1" colspan="1">0.82</td><td rowspan="1" colspan="1">0.95</td><td rowspan="1" colspan="1">2.6</td><td rowspan="1" colspan="1">0.36</td></tr></tbody></table><table-wrap-foot><fn id="tblfn4"><p><italic>Note</italic>: MCC, Matthews Correlation Coefficient; FPR<sub>T</sub>, False Positive Rate on transmembrane proteins; F1<sub>cs</sub>, The harmonic mean between precision and recall on cleavage-site detection.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec>
      <title>3.3 Proteome-wide scanning and detection of TAT-type signal peptides</title>
      <p>As a final benchmark, we assessed the performance of DeepSig on the entire proteome of <italic>E.coli (strain K12)</italic>.</p>
      <p>DeepSig scores with a MCC value of 0.81, which is in line with the results obtained on other benchmarks. A very low false positive rate on transmembrane proteins was also registered: only 4 out of 1024 transmembrane proteins were incorrectly classified as signal peptides, corresponding to a FPR<sub>T</sub> of 0.39%. Furthermore, the method was also able to recover the correct cleavage site for 340 signal peptides, corresponding to a F1<sub>cs</sub> value of 69%. Specifically, the set contains 138 sequences with an experimentally detected signal peptide: DeepSig correctly identifies 126 sequences and correctly places cleavage sites of 116.</p>
      <p>Interestingly, even if DeepSig has not been trained to explicitly recognize Twin-Arginine Translocation (TAT-type) signal sequences (<xref rid="btx818-B4" ref-type="bibr">Berks, 2015</xref>), the method correctly detects 18 out of 32 Tat-type signals that were annotated on <italic>E.coli</italic> sequences (sensitivity is 56%).</p>
      <p>To further investigate the performance of DeepSig on detecting TAT-type signal sequences, we downloaded from UniprotKB/SwissProt all reviewed sequences carrying this kind of signal. We ended up with 553 bacterial proteins, 466 of which were from Gram-negative and 71 from Gram-positive bacteria. Running DeepSig on these sequences, we were able to recover 330 out of 553 TAT signals, corresponding to a sensitivity of about 60%.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>In this paper, we present DeepSig, a novel approach to predict signal peptides in proteins based on deep learning and sequence labelling methods. The proposed approach was evaluated and compared with other available predictors, including the top-performing SignalP (<xref rid="btx818-B15" ref-type="bibr">Petersen <italic>et al.</italic>, 2011</xref>). In all the benchmarks, DeepSig reported performances that were comparable and even superior to other state-of-the-art methods.</p>
    <p>The method is available as web server and as a standalone program (<ext-link ext-link-type="uri" xlink:href="https://deepsig.biocomp.unibo.it">https://deepsig.biocomp.unibo.it</ext-link>). The standalone version of the program is very fast and easy to install. It takes only 40 min to process the entire human proteome containing some 70 000 protein sequences (test executed by running DeepSig in parallel using four CPU cores). All this suggests that DeepSig is a premier candidate for proteome-scale assessment of protein sub-cellular localization (where high precision is crucial) as well as for single-protein analyses where one is interested in the accurate identification of the signal sequence and cleavage site.</p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>Supplementary Data</label>
      <media xlink:href="btx818_supplementary.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="btx818-B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Abadi</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) Tensorflow: Large-Scale Machine Learning on Heterogeneous Systems. Software available online: <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org">https://www.tensorflow.org</ext-link>.</mixed-citation>
    </ref>
    <ref id="btx818-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alipanahi</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Predicting the sequence specificities of DNA- and RN-binding proteins by deep learning</article-title>. <source>Nat. Biotechnol</source>., <volume>33</volume>, <fpage>831</fpage>–<lpage>838</lpage>.<pub-id pub-id-type="pmid">26213851</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bach</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>On pixel-wise explanations for non-linear classifier decision by layer-wise relevance propagation</article-title>. <source>PLoS One</source>, <volume>10</volume>, <fpage>e0130140.</fpage><pub-id pub-id-type="pmid">26161953</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bagos</surname><given-names>P.G.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Combined prediction of Tat and Sec signal peptides with hidden Markov models</article-title>. <source>Bioinformatics</source>, <volume>26</volume>, <fpage>2811</fpage>–<lpage>2817</lpage>.<pub-id pub-id-type="pmid">20847219</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Berks</surname><given-names>B.C.</given-names></name></person-group> (<year>2015</year>) 
<article-title>The twin-arginine protein translocation pathway</article-title>. <source>Annu. Rev. Biochem</source>., <volume>84</volume>, <fpage>843</fpage>–<lpage>864</lpage>.<pub-id pub-id-type="pmid">25494301</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B5">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Chollet</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) Keras. Software available online: <ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link>.</mixed-citation>
    </ref>
    <ref id="btx818-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fariselli</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>Grammatical-restrained hidden conditional random fields for bioinformatics applications</article-title>. <source>Algorithms Mol. Biol</source>., <volume>4</volume>, <fpage>13</fpage>.<pub-id pub-id-type="pmid">19849839</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Indio</surname><given-names>V.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>The prediction of organelle targeting peptides in eukaryotic proteins with Grammatical Restrained Hidden Conditional Random Fields</article-title>. <source>Bioinformatics</source>, <volume>29</volume>, <fpage>981</fpage>–<lpage>988</lpage>.<pub-id pub-id-type="pmid">23428638</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Käll</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2004</year>) 
<article-title>A combined transmembrane topology and signal peptide prediction method</article-title>. <source>J. Mol. Biol</source>., <volume>338</volume>, <fpage>1027</fpage>–<lpage>1036</lpage>.<pub-id pub-id-type="pmid">15111065</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Käll</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2005</year>) 
<article-title>An HMM posterior decoder for sequence feature prediction that includes homology information</article-title>. <source>Bioinformatics</source>, <volume>21</volume>, <fpage>i251</fpage>–<lpage>i257</lpage>.<pub-id pub-id-type="pmid">15961464</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Krizhevsky</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Imagenet classification with deep convolutional neural networks</article-title>. In: <person-group person-group-type="author"><name name-style="western"><surname>Pereira</surname><given-names>F.</given-names></name></person-group> (ed.), <source>Advances in Neural Information Processing Systems</source>, pp. <fpage>1097</fpage>–<lpage>1105</lpage>.</mixed-citation>
    </ref>
    <ref id="btx818-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Deep learning</article-title>. <source>Nature</source>, <volume>521</volume>, <fpage>436</fpage>–<lpage>444</lpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Martoglio</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Dobberstein</surname><given-names>B.</given-names></name></person-group> (<year>1998</year>) 
<article-title>Signal sequences: more than just greasy peptides</article-title>. <source>Trends Cell Biol</source>., <volume>8</volume>, <fpage>410</fpage>–<lpage>415</lpage>.<pub-id pub-id-type="pmid">9789330</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Montavon</surname><given-names>G.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Explaining nonlinear classification decisions with deep Taylor decomposition</article-title>. <source>Pattern Recogn</source>., <volume>65</volume>, <fpage>211</fpage>–<lpage>222</lpage>.</mixed-citation>
    </ref>
    <ref id="btx818-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nugent</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>D.T.</given-names></name></person-group> (<year>2009</year>) 
<article-title>Transmembrane protein topology prediction using support vector machines</article-title>. <source>BMC Bioinformatics</source>, <volume>10</volume>, <fpage>159.</fpage><pub-id pub-id-type="pmid">19470175</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Petersen</surname><given-names>T.N.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>SignalP 4.0: discriminating signal peptides from transmembrane regions</article-title>. <source>Nat. Methods</source>, <volume>8</volume>, <fpage>785</fpage>–<lpage>786</lpage>.<pub-id pub-id-type="pmid">21959131</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Reynolds</surname><given-names>S.M.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>Transmembrane topology and signal peptide prediction using dynamic Bayesian networks</article-title>. <source>PLoS Comput. Biol</source>., <volume>4</volume>, <fpage>e1000213</fpage>.<pub-id pub-id-type="pmid">18989393</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Savojardo</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>BETAWARE: a machine-learning tool to detect and predict transmembrane beta-barrel proteins in Prokaryotes</article-title>. <source>Bioinformatics</source>, <volume>29</volume>, <fpage>504</fpage>–<lpage>505</lpage>.<pub-id pub-id-type="pmid">23297037</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Savojardo</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>ISPRED4: interaction site PREDiction in protein structures with a refining grammar model</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>1656</fpage>–<lpage>1663</lpage>.<pub-id pub-id-type="pmid">28130235</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Simonyan</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Deep inside convolutional networks: visualizing image classification models and saliency maps</article-title>. <source>Comput. Res. Repository</source>, 1312.6034.</mixed-citation>
    </ref>
    <ref id="btx818-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Szegedy</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Intriguing properties of neural networks</article-title>. <source>Comput. Res. Repository</source>, 1312.6199.</mixed-citation>
    </ref>
    <ref id="btx818-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tsirigos</surname><given-names>K.D.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>The TOPCONS web server for consensus prediction of membrane protein topology and signal peptides</article-title>. <source>Nucleic Acids Res</source>., <volume>43</volume>, <fpage>W401</fpage>–<lpage>W407</lpage>.<pub-id pub-id-type="pmid">25969446</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Viklund</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>SPOCTOPUS: a combined predictor of signal peptides and membrane protein topology</article-title>. <source>Bioinformatics</source>, <volume>24</volume>, <fpage>2928</fpage>–<lpage>2929</lpage>.<pub-id pub-id-type="pmid">18945683</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>von Heijne</surname><given-names>G.</given-names></name></person-group> (<year>1990</year>) 
<article-title>The signal peptide</article-title>. <source>J. Membr. Biol</source>., <volume>115</volume>, <fpage>195</fpage>–<lpage>201</lpage>.<pub-id pub-id-type="pmid">2197415</pub-id></mixed-citation>
    </ref>
    <ref id="btx818-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Troyanskaya</surname><given-names>O.G.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Predicting effects of noncoding variants with deep learning-based sequence model</article-title>. <source>Nature Methods</source>, <volume>12</volume>, <fpage>931</fpage>–<lpage>934</lpage>.<pub-id pub-id-type="pmid">26301843</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
