<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6444607</article-id>
    <article-id pub-id-type="publisher-id">2743</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-2743-3</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>AUTALASSO: an automatic adaptive LASSO for genome-wide prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2390-6609</contrib-id>
        <name>
          <surname>Waldmann</surname>
          <given-names>Patrik</given-names>
        </name>
        <address>
          <email>Patrik.Waldmann@slu.se</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ferenčaković</surname>
          <given-names>Maja</given-names>
        </name>
        <address>
          <email>mferencakovic@agr.hr</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mészáros</surname>
          <given-names>Gábor</given-names>
        </name>
        <address>
          <email>gabor.meszaros@boku.ac.at</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Khayatzadeh</surname>
          <given-names>Negar</given-names>
        </name>
        <address>
          <email>kh.negar@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Curik</surname>
          <given-names>Ino</given-names>
        </name>
        <address>
          <email>icurik@agr.hr</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sölkner</surname>
          <given-names>Johann</given-names>
        </name>
        <address>
          <email>johann.soelkner@boku.ac.at</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 8578 2742</institution-id><institution-id institution-id-type="GRID">grid.6341.0</institution-id><institution>Department of Animal Breeding and Genetics, Swedish University of Agricultural Sciences, </institution></institution-wrap>Box 7023, Uppsala, 750 07 Sweden </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0657 4636</institution-id><institution-id institution-id-type="GRID">grid.4808.4</institution-id><institution>Department of Animal Science, Faculty of Agriculture, University of Zagreb, </institution></institution-wrap>Svetosimunska 25, Zagreb, 10000 Croatia </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2298 5320</institution-id><institution-id institution-id-type="GRID">grid.5173.0</institution-id><institution>Division of Livestock Sciences,Department of Sustainable Agricultural Systems,University of Natural Resources and Life Sciences Vienna, </institution></institution-wrap>Gregor Mendel Str. 33, Vienna, A-1180 Austria </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>2</day>
      <month>4</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>2</day>
      <month>4</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>167</elocation-id>
    <history>
      <date date-type="received">
        <day>1</day>
        <month>8</month>
        <year>2018</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>3</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>Genome-wide prediction has become the method of choice in animal and plant breeding. Prediction of breeding values and phenotypes are routinely performed using large genomic data sets with number of markers on the order of several thousands to millions. The number of evaluated individuals is usually smaller which results in problems where model sparsity is of major concern. The LASSO technique has proven to be very well-suited for sparse problems often providing excellent prediction accuracy. Several computationally efficient LASSO algorithms have been developed, but optimization of hyper-parameters can be demanding.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>We have developed a novel automatic adaptive LASSO (AUTALASSO) based on the alternating direction method of multipliers (ADMM) optimization algorithm. The two major hyper-parameters of ADMM are the learning rate and the regularization factor. The learning rate is automatically tuned with line search and the regularization factor optimized using Golden section search. Results show that AUTALASSO provides superior prediction accuracy when evaluated on simulated and real bull data compared to the adaptive LASSO, LASSO and ridge regression implemented in the popular glmnet software.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>The AUTALASSO provides a very flexible and computationally efficient approach to GWP, especially when it is important to obtain high prediction accuracy and genetic gain. The AUTALASSO also has the capability to perform GWAS of both additive and dominance effects with smaller prediction error than the ordinary LASSO.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Genomic selection</kwd>
      <kwd>GWAS</kwd>
      <kwd>Regularization</kwd>
      <kwd>Mathematical optimization</kwd>
      <kwd>Proximal algorithms</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100006353</institution-id>
            <institution>Kjell och Märta Beijers Stiftelse</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100006589</institution-id>
            <institution>Ministry of Science and Technology, Croatia</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100005012</institution-id>
            <institution>Österreichische Agentur für Internationale Mobilität und Kooperation in Bildung, Wissenschaft und Forschung</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Genome-wide prediction (GWP) refers to the idea that regression coefficients, obtained by regressing genomic markers on phenotypic measurements in some training data, can be used to predict phenotypic values without the need to measure the phenotypes in some test data [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. Genome-wide data often consist of several thousands, sometimes millions of markers. Since the number of individuals is usually smaller, in the range of some hundreds to a few thousands, the result is a multivariate high- dimensional statistical issue that is often referred to as the <italic>p</italic>&gt;&gt;<italic>n</italic> problem [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>]. The outer product of the design matrix in the ordinary least squares (OLS) estimator is not invertible in these situations and will result in a matrix of rank one. As a consequence, different regularization techniques have been proposed to facilitate in obtaining well-conditioned and unique solutions [<xref ref-type="bibr" rid="CR6">6</xref>].</p>
    <p>Regularization is a mathematical technique to impose prior information on the structure of the solution to an optimization problem. It closely resembles the task of using priors in Bayesian statistics. By formulating regression as a minimization of loss plus penalty function problem it is easy to enforce certain restrictions on the regression coefficients. Ridge regression [<xref ref-type="bibr" rid="CR7">7</xref>] and the LASSO [<xref ref-type="bibr" rid="CR8">8</xref>] are two popular regularization methods that only differ in the norms of their penalty functions, i.e. the <italic>ℓ</italic><sub>2</sub> and <italic>ℓ</italic><sub>1</sub> norms, respectively. It is well established that the LASSO usually results in better prediction accuracy than ridge regression if the predictors display low to moderate correlation between each other [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>].</p>
    <p>The <italic>ℓ</italic><sub>1</sub> norm of the LASSO is special because it results in a convex minimization problem with sparse solutions (many regression coefficients are set to zero) which facilitate computations in large scale situations. A huge number of studies have been devoted to extensions of the lasso in various directions [<xref ref-type="bibr" rid="CR11">11</xref>]. Generalized Linear Model (GLM), survival and support vector machine versions can be obtained by alternation of the loss function. Modifications of the penalty function have resulted in the elastic net, group and fused LASSO that can handle correlated, structured and time dependent predictor variables. Even more elaborate extensions include sparse additive models and the graphical LASSO [<xref ref-type="bibr" rid="CR12">12</xref>].</p>
    <p>Several optimization algorithms have been developed for the LASSO. The least-angle regression (LAR) algorithm borrows from forward-stepwise regression by updating an active set of regressors via a regularization path [<xref ref-type="bibr" rid="CR13">13</xref>]. LAR provides relatively fast fitting for sparse data but requires one OLS solution per iteration and is not suitable for all loss functions. Another algorithm that has turned out to be effective is cyclic coordinate descent (CCD). CCD is also an iterative algorithm that updates the regression coefficients by choosing a single coordinate to update, and then perform a univariate minimization over this coordinate. The idea is to compute the OLS coefficients on the partial residuals and apply soft-thresholding to take care of the LASSO contribution to the penalty. CCD efficiently exploits sparsity and can handle large data sets [<xref ref-type="bibr" rid="CR14">14</xref>].</p>
    <p>The rise of big data problems has resulted in a resurgence of first order gradient descent based methods in mathematical optimization [<xref ref-type="bibr" rid="CR15">15</xref>]. One problem with the LASSO is that the penalty function is nondifferentiable at zero. However, it turns out that the rediscovery of proximal operator techniques has solved this issue. The idea behind the proximal gradient is to decompose the objective function <italic>f</italic> into the sum of one convex, differentiable function <italic>g</italic> and one convex, nondifferentiable function <italic>h</italic>, and then use a proximal map of <italic>h</italic> along with the gradient of <italic>g</italic> to update in each iteration [<xref ref-type="bibr" rid="CR16">16</xref>]. Proximal versions of the LASSO include the fast iterative soft-thresholding algorithm (FISTA) [<xref ref-type="bibr" rid="CR17">17</xref>] and the alternating diretion method of multipliers (ADMM) [<xref ref-type="bibr" rid="CR18">18</xref>]. These methods are fast and can handle large data, but the tuning of the learning rate and the regularization parameter can be tedious.</p>
    <p>The purpose of this study is to introduce proximal algorithms, with a special focus on ADMM, into a GWP framework, and then to develop a general approach that automatically finds the optimal values of the learning rate and the regularization parameters of an adaptive LASSO. The learning rate determines how large steps the iterative algorithm takes towards the minimum. Too large steps leads to fewer iterations, but with a high risk to miss the minimum. Too small steps may lead to very many iterations to convergence and therefore also more computing time. The learning rate is optimized using line search with Armijo rule. The level of the regularization parameter in the LASSO regulates how many regression coefficients that are set to zero and is optimized with golden section search. Finally, the method is evaluated on simulated and real GWP data from cattle.</p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>Simulated data</title>
      <p>For the QTLMAS2010 data we set <italic>ε</italic><sub>ADMM</sub>=10<sup>−4</sup> and <italic>ε</italic><sub>GSS</sub>=10<sup>−2</sup>. Moreover, the initial bracketing points in the GSS procedure were set to <italic>λ</italic><sub><italic>a</italic></sub>=0.0001<italic>λ</italic><sub><italic>max</italic></sub> and <italic>λ</italic><sub><italic>b</italic></sub>=<italic>λ</italic><sub><italic>max</italic></sub>. In this data set the last generation is used as test data and therefore only one evaluation is needed. The AUTALASSO completed in 190 s and resulted in a MSE<sub><italic>test</italic></sub> of 64.34 and <italic>r</italic><sub><italic>test</italic></sub> of 0.676. The number of <italic>λ</italic>-values tested to convergence in the GSS algorithm was 21. The additive effects were calculated as <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$a=-\theta _{\lambda _{opt,gen=0}}+\theta _{\lambda _{opt,gen=2}}$\end{document}</tex-math><mml:math id="M2"><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">opt</mml:mtext><mml:mo>,</mml:mo><mml:mtext mathvariant="italic">gen</mml:mtext><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">opt</mml:mtext><mml:mo>,</mml:mo><mml:mtext mathvariant="italic">gen</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq1.gif"/></alternatives></inline-formula> and the non-zero SNPs are plotted in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. It can be seen that the major coefficients corresponds well with the anticipated QTLs, i.e. the two major additive loci and the additive part of the dominance locus. The heterozygote effects equals the dominance effects <italic>d</italic> and selected variables are plotted in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. The three most important variables are the dominance, over-dominance and under-domiance loci, respectively. The numbers of selected variables are 196 and 97 for the additive and dominance effects, respectively, in Figs. <xref rid="Fig1" ref-type="fig">1</xref> and <xref rid="Fig2" ref-type="fig">2</xref>.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Selected additive genetic effects of the SNPs for the simulated QTLMAS2010 data produced with the AUTALASSO. The two largest estimated effects corresponds well with the simulated effects of the two major controlled QTLs (□) and the third largest effect is close to the additive part of the dominance QTL (△). The number of non-zero variables is 196</p></caption><graphic xlink:href="12859_2019_2743_Fig1_HTML" id="MO1"/></fig>
<fig id="Fig2"><label>Fig. 2</label><caption><p>Selected dominance genetic effects of the SNPs for the simulated QTLMAS2010 data produced with the AUTALASSO. The three largest effects corresponds well with the simulated effects of the dominance (△), over-dominance (♢) and under-dominance <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$(\triangledown)$\end{document}</tex-math><mml:math id="M4"><mml:mo>(</mml:mo><mml:mo>▿</mml:mo><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq2.gif"/></alternatives></inline-formula> QTLs. The number of non-zero variables is 97</p></caption><graphic xlink:href="12859_2019_2743_Fig2_HTML" id="MO2"/></fig></p>
      <p>The computing times of the glmnet ALASSO were strongly influenced by the number of <italic>λ</italic>-values, with 7, 56 and 546 s for 100, 1000 and 10,000 values, respectively. However, the influence of varying the number of <italic>λ</italic> on the MSE<sub><italic>test</italic></sub> was small, 64.52, 64.53 and 64.48, respectively. Hence, we decided to use 100 <italic>λ</italic>-values for all analyses. The glmnet LASSO and RR resulted in MSE<sub><italic>test</italic></sub> of 65.73 and 83.07, respectively. The corresponding <italic>r</italic><sub><italic>test</italic></sub> were for the ALASSO, LASSO and RR 0.675, 0.679 and 0.551, respectively. Hence, the AUTALASSO provides lower MSE<sub><italic>test</italic></sub> regardless of method in glmnet. That the LASSO provides slightly higher <italic>r</italic><sub><italic>test</italic></sub> than both the AUTALASSO and ALASSO should be interpreted with care (see Discussion).</p>
    </sec>
    <sec id="Sec4">
      <title>Real data</title>
      <p>We used the same <italic>ε</italic>-values as for the QTLMAS2010 data in the analyses of the bull data. Moreover, we first ran one analysis of one fold with <italic>λ</italic><sub><italic>a</italic></sub>=0.0001<italic>λ</italic><sub><italic>max</italic></sub> and <italic>λ</italic><sub><italic>b</italic></sub>=<italic>λ</italic><sub><italic>max</italic></sub> to get estimates that could guide to a more narrow <italic>λ</italic> interval. The consecutive folds were analysed with <italic>λ</italic><sub><italic>a</italic></sub>=0.0001<italic>λ</italic><sub><italic>max</italic></sub> and <italic>λ</italic><sub><italic>b</italic></sub>=0.002<italic>λ</italic><sub><italic>max</italic></sub>. The mean timing over the folds was 3020 s and the mean MSE<sub><italic>test</italic></sub> was 11.801 for AUTALASSO. The <italic>r</italic><sub><italic>test</italic></sub> was estimated to 0.615 for the AUTALASSO. With 100 <italic>λ</italic>-values the mean timing of the glmnet ALASSO was 170 s and mean MSE<sub><italic>test</italic></sub> equal to 13.17. The MSE<sub><italic>test</italic></sub> for the LASSO and RR was 12.65 and 13.20, respectively. The corresponding estimates for <italic>r</italic><sub><italic>test</italic></sub> were for the ALASSO, LASSO and RR 0.554, 0.579 and 0.554. Hence, the AUTALASSO produces the lowest prediction error and highest prediction accuracy, but is somewhat slower than glmnet. Though it should be noted that glmnet is based on the CCD algorithm and implemented in compiled Fortran which should be faster than Julia.</p>
      <p>Of the original 716 environmental variables were 28 selected by the AUTALASSO as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. None of the 617 repeated measurement indicators were selected. The three highest positive effects were obtained for variables 672 (age), 692 (year 2007) and 691 (year 2006), respectively. Two negative effects also deviate from the others, variable 711 (semen coll. numb.15) and 689 (year 2004).
<fig id="Fig3"><label>Fig. 3</label><caption><p>Selected environmental effects for the real Fleckvieh bull data produced with the AUTALASSO. The three largest positive effects on the phenotype are due to age, year 2007 and year 2006, whereas the two most negative are semen collector number 15 and year 2004. The number of non-zero variables is 28</p></caption><graphic xlink:href="12859_2019_2743_Fig3_HTML" id="MO3"/></fig></p>
      <p>The plots of the selected regression coefficients <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\theta _{\lambda _{opt}}$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">opt</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq3.gif"/></alternatives></inline-formula> for the two SNP homozygotes are provided from the authors. The additive and dominance effects were calculated in the same way as for the simulated data. Figure <xref rid="Fig4" ref-type="fig">4</xref> provides a plot of the additive effects of the Fleckvieh bull data. The number of selected additive effects was 1116. The maximum additive effect was 0.223 for SNP 23194, whereas the minimum of -0.230 was found for SNP 28087. Figure <xref rid="Fig5" ref-type="fig">5</xref> provides a plot of the dominance effects of the bull data. The number of selected dominance effects was 1468. The maximum dominance effect was 0.279 for SNP 17125, whereas the minimum of -0.254 was found for SNP 26154.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Selected additive genetic SNP effects for the real Fleckvieh bull data produced with the AUTALASSO. The largest positive and negative effects are for SNP 23194 (0.223) and 28087(-0.230), respectively. The number of non-zero variables is 1116</p></caption><graphic xlink:href="12859_2019_2743_Fig4_HTML" id="MO4"/></fig>
<fig id="Fig5"><label>Fig. 5</label><caption><p>Selected dominance genetic SNP effects for the real Fleckvieh bull data produced with the AUTALASSO. The largest positive and negative effects are for SNP 17125(0.279) and 26154(-0.254), respectively. The number of non-zero variables is 1468</p></caption><graphic xlink:href="12859_2019_2743_Fig5_HTML" id="MO5"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec5" sec-type="discussion">
    <title>Discussion</title>
    <p>One of the major obstacles in using high density genomic marker data for prediction purposes is that the number of individuals that can be scored usually is considerably smaller than the number of markers. This situation introduces several problems. For example, when <italic>p</italic>&gt;<italic>n</italic> the ordinary least squares estimates are not unique and will considerably over-fit the data resulting in a low prediction accuracy [<xref ref-type="bibr" rid="CR4">4</xref>]. Other problems with big genome-wide data sets include spurious random correlations, incidental endogeneity, noise accumulation, and measurement error [<xref ref-type="bibr" rid="CR19">19</xref>]. Regularization provides an efficient technique to constrain parameters which will lead to unique solutions and often better prediction accuracy. The most successful regularization method is the LASSO because it provides a convex optimization problem with sparse solutions [<xref ref-type="bibr" rid="CR12">12</xref>]. Here, we have proposed an automatic adaptive LASSO that uses the ADMM algorithm for computing solutions to the objective function, line search for tuning of the learning rate and golden section search for optimization of the regularization parameter.</p>
    <p>A large number of methods have been proposed for genome-wide prediction and several papers have evaluated their predictive properties [<xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR21">21</xref>]. Regularization approaches include ridge regression, the LASSO and mixture modeling. Their Bayesian counterparts are often referred to as the “Bayesian alphabet” in the genetics literature [<xref ref-type="bibr" rid="CR22">22</xref>]. The least angle regression version of the LASSO was exploited for GWP in [<xref ref-type="bibr" rid="CR23">23</xref>] and found to yield better prediction accuracy than genomic BLUP and BayesA. In another study based on glmnet [<xref ref-type="bibr" rid="CR14">14</xref>], the LASSO, the elastic net, adaptive LASSO and the adaptive elastic net all had similar accuracies but outperformed ridge regression and ridge regression BLUP [<xref ref-type="bibr" rid="CR10">10</xref>]. In [<xref ref-type="bibr" rid="CR9">9</xref>], an overview of both the frequentist and Bayesian LASSO is provided. They found the LASSO and adaptive LASSO to be competitive compared to several other methods in evaluations on both simulated and real data. In conclusion, the LASSO seems to be a computationally efficient method that yields good prediction accuracy on GWP data under many situations, but it would be valuable to compile all evaluations into a meta-analysis.</p>
    <p>We have calculated both MSE<sub><italic>test</italic></sub> and <italic>r</italic><sub><italic>test</italic></sub> as measures of model fit and found that they give somewhat contradictory results, especially for the glmnet implementations where the ordinary LASSO had both higher <italic>r</italic><sub><italic>test</italic></sub> and MSE<sub><italic>test</italic></sub> than the ALASSO. The standard Pearson correlation coefficient is <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$r_{test} = {\sigma _{\hat {y}_{test}y_{test}}}/\left ({\sigma _{\hat {y}_{test}}\sigma _{y_{test}}}\right)$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ŷ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ŷ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq4.gif"/></alternatives></inline-formula>. When we decompose <italic>r</italic><sub><italic>test</italic></sub> for the QTLMAS2010 data into its parts, we see that <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\sigma _{y_{test}}$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq5.gif"/></alternatives></inline-formula> is the same at 10.83 for both methods. However, <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\sigma _{\hat {y}_{test}}$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ŷ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq6.gif"/></alternatives></inline-formula> is increased from 5.93 in the LASSO to 6.82 in the ALASSO, and <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\sigma _{\hat {y}_{test}y_{test}}$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>ŷ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq7.gif"/></alternatives></inline-formula> is increased from 43.556 in the LASSO to 49.86 in the ALASSO. The introduction of the weight factor in the ALASSO increases model complexity which results in decreased model bias, on the expense of an increased variance. However, most importantly, the MSE<sub><italic>test</italic></sub> is reduced. This is an example of the Bias-Variance trade-off that is fundamental in statistical learning [<xref ref-type="bibr" rid="CR4">4</xref>]. Unfortunately, this phenomenon can complicate the interpretation of <italic>r</italic><sub><italic>test</italic></sub> as a measure of model fit and it should be used with care.</p>
    <p>ADMM is a form of a proximal operator algorithm that has become very popular in mathematical optimization [<xref ref-type="bibr" rid="CR15">15</xref>]. As described in the “<xref rid="Sec7" ref-type="sec">Methods and data</xref>” section, we can see that the ADMM is closely related to the augmented Lagrangian method, but does not require a joint minimization which facilitate computations in special applications, for example the LASSO. The ADMM LASSO was first outlined in [<xref ref-type="bibr" rid="CR18">18</xref>] and since then there have been extensions to for example the clustering LASSO [<xref ref-type="bibr" rid="CR24">24</xref>] and the generalized LASSO [<xref ref-type="bibr" rid="CR25">25</xref>]. We have used the least squares (Euclidean) loss function since we perform regression in this study, but it is straightforward to change to a classification loss function (for example logit or hinge loss) in the case of a binary trait.</p>
  </sec>
  <sec id="Sec6" sec-type="conclusion">
    <title>Conclusions</title>
    <p>The AUTALASSO provides a very flexible and computationally efficient approach to GWP, especially in situations were it is important to obtain high prediction accuracy and genetic gain. The algorithm automatically tunes the learning rate with line search and the regularization factor using Golden section search. The results show that the prediction error is smaller for the AUTALASSO when compared with the adaptive LASSO, the ordinary LASSO and ridge regression implemented in the popular glmnet package, albeit at the expense of some increased computing time. It is also shown that important SNP markers can be identified, both with underlying additive and dominance effects. The implementation in Julia is easy to modify, and we have hope that future versions of the AUTALASSO will fully capitalize on the distributed computing facilities of the ADMM algorithm.</p>
  </sec>
  <sec id="Sec7">
    <title>Methods and data</title>
    <sec id="Sec8">
      <title>ADMM adaptive LASSO</title>
      <p>The LASSO was introduced by [<xref ref-type="bibr" rid="CR8">8</xref>] and the objective function of the optimization problem consists of an ordinary squared <italic>ℓ</italic><sub>2</sub> (Euclidean) norm loss function and an <italic>ℓ</italic><sub>1</sub> norm penalty function 
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ {\text{minimize }}f(\beta)+g(\beta)  $$ \end{document}</tex-math><mml:math id="M16"><mml:mtext>minimize</mml:mtext><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$f(\beta)=(1/2)\left \| X\beta -y \right \|_{2}^{2}$\end{document}</tex-math><mml:math id="M18"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:mi>Xβ</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq8.gif"/></alternatives></inline-formula> and <italic>g</italic>(<italic>β</italic>)=<italic>λ</italic>∥<italic>β</italic>∥<sub>1</sub>. In ADMM form (see Appendix), the LASSO becomes 
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} &amp;{\text{minimize }}f(\beta)+g(\theta)\\ &amp;{\text{subject to }}\beta-\theta=0. \end{aligned}  $$ \end{document}</tex-math><mml:math id="M20"><mml:mtable><mml:mtr><mml:mtd><mml:mtext>minimize</mml:mtext><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>subject to</mml:mtext><mml:mi>β</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>The ADMM algorithm then is the following iterative procedure 
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} &amp;\beta^{t+1}=\left(X^{T}X+{\rho}I\right)^{-1}+\left(X^{T}y+{\rho}\left(\theta^{t}-u^{t}\right)\right)\\[-2pt] &amp;\theta^{t+1}=S_{{\lambda\rho}}\left(\beta^{t+1}+u^{t}\right)\\[-2pt] &amp;u^{t+1}=u^{t}+\beta^{t+1}-\theta^{t+1} \end{aligned}  $$ \end{document}</tex-math><mml:math id="M22"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mi>ρI</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>λρ</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <italic>ρ</italic>&gt;0 is the learning rate, <italic>λ</italic>&gt;0 is the regularization parameter and <italic>S</italic><sub><italic>λ</italic><italic>ρ</italic></sub>(<italic>ν</italic>)=(<italic>ν</italic>−<italic>λ</italic><italic>ρ</italic>)<sub>+</sub>−(−<italic>ν</italic>−<italic>λ</italic><italic>ρ</italic>)<sub>+</sub> is the soft-thresholding operator that depends on both <italic>ρ</italic> and <italic>λ</italic>. It is now possible to rewrite this algorithm using proximal operators (see Appendix) 
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} &amp;\beta^{t+1}=\operatorname{prox}_{\rho f}\left(\theta^{t}-u^{t}\right)\\[-2pt] &amp;\theta^{t+1}=\operatorname{prox}_{{\lambda}{\rho}g}\left(\beta^{t+1}+u^{t}\right)\\[-2pt] &amp;u^{t+1}=u^{t}+\beta^{t+1}-\theta^{t+1}, \end{aligned}  $$ \end{document}</tex-math><mml:math id="M24"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>prox</mml:mo></mml:mrow><mml:mrow><mml:mi>ρf</mml:mi></mml:mrow></mml:munder><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>prox</mml:mo></mml:mrow><mml:mrow><mml:mi>λρg</mml:mi></mml:mrow></mml:munder><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>which is iterated to convergence determined by 
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} &amp;\left\| \beta^{t+1}- \theta^{t+1}\right\|_{\infty}\leq \epsilon_{\text{ADMM}}\left(1+\left\| \mu^{t+1}\right\|_{\infty}\right) \end{aligned}  $$ \end{document}</tex-math><mml:math id="M26"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>∞</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mtext>ADMM</mml:mtext></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>∞</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>for tolerance parameter <italic>ε</italic><sub>ADMM</sub>. The choice of <italic>ε</italic><sub>ADMM</sub> can have a dramatic influence on the number of iterations to convergence and we recommend to set it between 10<sup>−3</sup> and 10<sup>−4</sup>.</p>
      <p>In [<xref ref-type="bibr" rid="CR26">26</xref>], it was shown that the LASSO can be inconsistent for variable selection under some situations and the adaptive LASSO was suggested in order to reduce bias and full-fill oracle properties (i.e that the estimator satisfies support and <inline-formula id="IEq9"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\sqrt {n}$\end{document}</tex-math><mml:math id="M28"><mml:msqrt><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msqrt></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq9.gif"/></alternatives></inline-formula>-estimation consistency). The idea behind the adaptive LASSO is to use variable specific weights <inline-formula id="IEq10"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {w}=1/|{\hat {\beta }}|^{\gamma }$\end{document}</tex-math><mml:math id="M30"><mml:mi>ŵ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mo>|</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq10.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq11"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${\hat {\beta }}$\end{document}</tex-math><mml:math id="M32"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq11.gif"/></alternatives></inline-formula> is estimated based on some pilot run and <italic>γ</italic>&gt;0 a tuning parameter. The OLS estimates of <inline-formula id="IEq12"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${\hat {\beta }}$\end{document}</tex-math><mml:math id="M34"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq12.gif"/></alternatives></inline-formula> are not defined when <italic>p</italic>&gt;<italic>n</italic>, but univariate marginal regression (or covariance) coefficients calculated as <inline-formula id="IEq13"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${\hat {\beta } = X^{T}y/n}$\end{document}</tex-math><mml:math id="M36"><mml:mover accent="true"><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>y</mml:mi><mml:mo>/</mml:mo><mml:mi>n</mml:mi></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq13.gif"/></alternatives></inline-formula> can be used and yield good recovery properties under most situations [<xref ref-type="bibr" rid="CR27">27</xref>]. Moreover, the adaptive lasso penalty can be seen as an approximation to the <italic>ℓ</italic><sub><italic>q</italic></sub> penalties with <italic>q</italic>=1−<italic>γ</italic>. Hence, <italic>γ</italic>=1 approximates the <italic>ℓ</italic><sub>0</sub> penalty [<xref ref-type="bibr" rid="CR12">12</xref>]. This formulation is a great advantage since the penalty function is still convex and the only part of the objective function that needs to be changed in the adaptive LASSO is <inline-formula id="IEq14"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$g^{*}(\beta)={\lambda }\left \|\hat {w}\beta \right \|_{1}$\end{document}</tex-math><mml:math id="M38"><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:mi>ŵβ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq14.gif"/></alternatives></inline-formula>.</p>
    </sec>
    <sec id="Sec9">
      <title>Automatic tuning of the learning rate and the regularization factor</title>
      <p>Many procedures have been proposed for tuning of the learning rate <italic>ρ</italic> in gradient decent algorithms. Since the proximal gradient is a special case of projected gradients methods we can use the fact that line search with Armijo rule can be used for optimization of the learning rate in this form of constrained optimization [<xref ref-type="bibr" rid="CR15">15</xref>]. Given that the projected gradient starts from the current iterate of <italic>β</italic><sup><italic>t</italic></sup>, moves in the direction of the negative gradient of the loss function <italic>β</italic><sup><italic>t</italic></sup>−<italic>ρ</italic>∇<italic>f</italic>(<italic>β</italic><sup><italic>t</italic></sup>) and then performs a projection back to the convex constraint set that holds <italic>θ</italic><sup><italic>t</italic></sup>, a line search can be formulated by iteratively decreasing <italic>ρ</italic><sup><italic>k</italic></sup> until 
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} &amp;f\left(\theta^{t}\right) &gt; f\left(\beta^{t}\right)-{\nabla}f\left(\beta^{t}\right)^{T}\beta+\left(1/2{\rho}^{k}\right)\left\| \beta^{t}\right\|_{2}^{2} \\ \end{aligned}  $$ \end{document}</tex-math><mml:math id="M40"><mml:mtable><mml:mtr><mml:mtd><mml:mi>f</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>&gt;</mml:mo><mml:mi>f</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr/></mml:mtable></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>no longer holds. The gradient is calculated as ∇<italic>f</italic>(<italic>β</italic><sup><italic>t</italic></sup>)=<italic>X</italic><sup><italic>T</italic></sup>(<italic>X</italic><italic>β</italic><sup><italic>t</italic></sup>−<italic>y</italic>). A suitable choice can be to start with <italic>ρ</italic><sup>0</sup>=1 and decrease <italic>ρ</italic><sup><italic>k</italic></sup> by a factor of 0.5 each <italic>k</italic>th iteration until <italic>ρ</italic><sup><italic>K</italic></sup> is found. A new <italic>ρ</italic><sup><italic>K</italic></sup> is then used in each <italic>t</italic>th ADMM update.</p>
      <p>The bias-variance tradeoff in statistical learning makes it necessary to use test (or validation) data to avoid under- and overfitting [<xref ref-type="bibr" rid="CR4">4</xref>]. A common approach to find the minimum test MSE of a LASSO evaluation is to calculate the value of the regularization factor where all regression coefficients of the training data are zero, i.e. <italic>λ</italic><sub><italic>max</italic></sub>=∥<italic>X</italic><sup><italic>T</italic></sup><italic>y</italic>∥<sub><italic>∞</italic></sub>, and then to evaluate the MSE<sub>test</sub> along a path of equally spaced and decreasing <italic>λ</italic>-values until an arbitrarily choosen <italic>λ</italic><sub><italic>min</italic></sub> is reached [<xref ref-type="bibr" rid="CR14">14</xref>]. Often, this amounts to fitting the LASSO for at least 100 <italic>λ</italic>-values, and the precision in finding the optimum depends on how well the path covers the shape of the test error function around the minimum.</p>
      <p>Another option is to optimize the <italic>λ</italic>-value using a formal optimization algorithm that minimize the convex test error function. Here we propose to use the Golden section search (GSS) which is a technique for finding the minimum of a function by successively narrowing the range of values inside which the minimum is known to exist [<xref ref-type="bibr" rid="CR28">28</xref>]. The GSS algorithm is described as follows: 
<list list-type="order"><list-item><p>Set two bracketing points, for example <italic>λ</italic><sub><italic>a</italic></sub>=0.001<italic>λ</italic><sub><italic>max</italic></sub> and <italic>λ</italic><sub><italic>b</italic></sub>=<italic>λ</italic><sub><italic>max</italic></sub>.</p></list-item><list-item><p>Fit the ADMM LASSO with line search to the training data for <italic>λ</italic><sub><italic>a</italic></sub> and <italic>λ</italic><sub><italic>b</italic></sub> to obtain <inline-formula id="IEq15"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\theta _{\lambda _{a}}$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq15.gif"/></alternatives></inline-formula> and <inline-formula id="IEq16"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\theta _{\lambda _{b}}$\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq16.gif"/></alternatives></inline-formula> which are used to predict the squared test errors 
<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} &amp;\text{SE}_{test,\lambda_{a}}=1/2\left\|X_{test}\theta_{\lambda_{a}}-y_{test}\right\|_{2}^{2}\\ &amp;\text{SE}_{test,\lambda_{b}}=1/2\left\|X_{test}\theta_{\lambda_{b}}-y_{test}\right\|_{2}^{2}.\\ \end{aligned}  $$ \end{document}</tex-math><mml:math id="M46"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mtext>SE</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mtext>SE</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>.</mml:mi></mml:mtd></mml:mtr><mml:mtr/></mml:mtable></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p></list-item><list-item><p>Set the golden section ratio to <inline-formula id="IEq17"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\varphi = \left (1+\sqrt {5}\right)/2$\end{document}</tex-math><mml:math id="M48"><mml:mi>φ</mml:mi><mml:mo>=</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:mfenced><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq17.gif"/></alternatives></inline-formula>.</p></list-item><list-item><p>Let <italic>λ</italic><sub><italic>c</italic></sub>=<italic>λ</italic><sub><italic>b</italic></sub>−(<italic>λ</italic><sub><italic>b</italic></sub>−<italic>λ</italic><sub><italic>a</italic></sub>)/<italic>φ</italic> and <italic>λ</italic><sub><italic>d</italic></sub>=<italic>λ</italic><sub><italic>a</italic></sub>+(<italic>λ</italic><sub><italic>b</italic></sub>−<italic>λ</italic><sub><italic>a</italic></sub>)/<italic>φ</italic>.</p></list-item><list-item><p>Compute both <inline-formula id="IEq18"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\text {SE}_{test,\lambda _{c}}$\end{document}</tex-math><mml:math id="M50"><mml:msub><mml:mrow><mml:mtext>SE</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq18.gif"/></alternatives></inline-formula> and <inline-formula id="IEq19"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\text {SE}_{test,\lambda _{d}}$\end{document}</tex-math><mml:math id="M52"><mml:msub><mml:mrow><mml:mtext>SE</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq19.gif"/></alternatives></inline-formula> following (7) in the first iteration. For iterations &gt;1 calculate <inline-formula id="IEq20"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\text {SE}_{test,\lambda _{c}}$\end{document}</tex-math><mml:math id="M54"><mml:msub><mml:mrow><mml:mtext>SE</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq20.gif"/></alternatives></inline-formula> if <italic>f</italic><italic>l</italic><italic>a</italic><italic>g</italic>=1 and <inline-formula id="IEq21"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\text {SE}_{test,\lambda _{d}}$\end{document}</tex-math><mml:math id="M56"><mml:msub><mml:mrow><mml:mtext>SE</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq21.gif"/></alternatives></inline-formula> if <italic>f</italic><italic>l</italic><italic>a</italic><italic>g</italic>=0.</p></list-item><list-item><p>If <inline-formula id="IEq22"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\text {SE}_{test,\lambda _{c}}&lt;\text {SE}_{test,\lambda _{d}}$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mrow><mml:mtext>SE</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mtext>SE</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq22.gif"/></alternatives></inline-formula> then move <italic>λ</italic><sub><italic>d</italic></sub>→<italic>λ</italic><sub><italic>b</italic></sub>, <inline-formula id="IEq23"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\text {SE}_{test,\lambda _{c}} \rightarrow \text {SE}_{test,\lambda _{d}}$\end{document}</tex-math><mml:math id="M60"><mml:msub><mml:mrow><mml:mtext>SE</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mtext>SE</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq23.gif"/></alternatives></inline-formula>, <inline-formula id="IEq24"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\theta _{\lambda _{c}} \rightarrow \theta _{\lambda _{d}}$\end{document}</tex-math><mml:math id="M62"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq24.gif"/></alternatives></inline-formula>. Set <italic>f</italic><italic>l</italic><italic>a</italic><italic>g</italic>=1. Compute new <italic>λ</italic><sub><italic>c</italic></sub>=<italic>λ</italic><sub><italic>b</italic></sub>−(<italic>λ</italic><sub><italic>a</italic></sub>−<italic>λ</italic><sub><italic>b</italic></sub>)/<italic>φ</italic> and <italic>λ</italic><sub><italic>d</italic></sub>=<italic>λ</italic><sub><italic>a</italic></sub>+(<italic>λ</italic><sub><italic>b</italic></sub>−<italic>λ</italic><sub><italic>a</italic></sub>)/<italic>φ</italic>.</p></list-item><list-item><p>Otherwise move <italic>λ</italic><sub><italic>c</italic></sub>→<italic>λ</italic><sub><italic>a</italic></sub>, <inline-formula id="IEq25"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\text {SE}_{test,\lambda _{d}} \rightarrow \text {SE}_{test,\lambda _{c}}$\end{document}</tex-math><mml:math id="M64"><mml:msub><mml:mrow><mml:mtext>SE</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mtext>SE</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq25.gif"/></alternatives></inline-formula>, <inline-formula id="IEq26"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\theta _{\lambda _{d}} \rightarrow \theta _{\lambda _{c}}$\end{document}</tex-math><mml:math id="M66"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq26.gif"/></alternatives></inline-formula>. Set <italic>f</italic><italic>l</italic><italic>a</italic><italic>g</italic>=0.. Compute new <italic>λ</italic><sub><italic>c</italic></sub>=<italic>λ</italic><sub><italic>b</italic></sub>−(<italic>λ</italic><sub><italic>a</italic></sub>−<italic>λ</italic><sub><italic>b</italic></sub>)/<italic>φ</italic> and <italic>λ</italic><sub><italic>d</italic></sub>=<italic>λ</italic><sub><italic>a</italic></sub>+(<italic>λ</italic><sub><italic>b</italic></sub>−<italic>λ</italic><sub><italic>a</italic></sub>)/<italic>φ</italic>.</p></list-item><list-item><p>Repeat 5-7 until (|<italic>λ</italic><sub><italic>d</italic></sub>−<italic>λ</italic><sub><italic>c</italic></sub>|/((<italic>λ</italic><sub><italic>c</italic></sub>+<italic>λ</italic><sub><italic>d</italic></sub>)/2))&lt;<italic>ε</italic><sub>GSS</sub>.</p></list-item><list-item><p>Calculate <italic>λ</italic><sub><italic>opt</italic></sub>=(<italic>λ</italic><sub><italic>c</italic></sub>+<italic>λ</italic><sub><italic>d</italic></sub>)/2 and perform a final ADMM run to get <inline-formula id="IEq27"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\theta _{\lambda _{opt}}$\end{document}</tex-math><mml:math id="M68"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">opt</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq27.gif"/></alternatives></inline-formula> and <inline-formula id="IEq28"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\text {SE}_{test,\lambda _{opt}}$\end{document}</tex-math><mml:math id="M70"><mml:msub><mml:mrow><mml:mtext>SE</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">test</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">opt</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2743_Article_IEq28.gif"/></alternatives></inline-formula>.</p></list-item></list></p>
      <p>The value to choose for the convergence tolerance <italic>ε</italic><sub>GSS</sub> can have a large effect on computing time. For a relatively good balance between precision and computing time, <italic>ε</italic><sub>GSS</sub> can be set between 0.01 and 0.001. The start values of <italic>θ</italic> and <italic>β</italic> also influence computing time in each ADMM run. Hence, the <italic>θ</italic> and <italic>β</italic> fits in step 5 of the GSS algorithm are re-used in next iteration to obtain warm-starts, but the <italic>u</italic>-vector is set to 0. Warm-starts have been shown to reduce the number of iterations to convergence in ADMM considerably [<xref ref-type="bibr" rid="CR29">29</xref>].</p>
    </sec>
    <sec id="Sec10">
      <title>Simulated data</title>
      <p>The original data was produced for the QTLMAS2010 work-shop [<xref ref-type="bibr" rid="CR30">30</xref>]. The total number of individuals is 3226 and they are structured in a pedigree with 5 generations. The pedigree is founded by 20 individuals (5 males and 15 females), and created assuming that each female mates once and gives birth to approximately 30 progeny. Five autosomal chromosomes of length 100Mbp were simulated. A neutral coalescent model was used to simulate the SNP data. The procedure created 10,031 markers, including 263 monomorphic and 9768 biallelic SNPs.</p>
      <p>The continuous quantitative trait was created from 37 QTLs, including 9 controlled genes and 28 random genes. The controlled QTLs included two pairs of epistatic genes with no individual effects, 3 maternally imprinted genes and two additive major genes with effects of -3 and 3. The random genes were selected from the simulated SNPs and then their effects were sampled from a truncated normal distribution, and accepted if the absolute value of the additive effect was smaller than 2. The resulting additive effects of the random genes varied between -1.98 and 1.93. Each simulated QTL was surrounded by 19-47 polymorphic SNPs (MAF &gt;0.05) positioned within 1 Mb distance from the QTL. 364 SNPs were in moderate to high LD with the QTLs.</p>
      <p>In addition to the original data, one dominance locus was positioned at SNP number 9212 on chromosome 5 by giving the heterozygote (1) an effect of 5.00 and the upper homozygote (2) a value of 5.01. One over-dominance locus was produced at SNP 9404 by assigning the heterozygote an effect of 5.00, and lower homozygote (0) the effect of -0.01 and upper homozygote (2) the effect of 0.01. Finally, one under-dominance loci was created at SNP id 9602 by assigning a value of -5.00 to the heterozygote, and lower homozygote (0) the effect of -0.01 and upper homozygote (2) the effect of 0.01. The values of the genotypes of these new QTLs were added to the original <italic>y</italic>-values. MAF cleaning was performed at the 0.01 level so the final sample of SNPs with 0,1,2 coding was 9723. The SNPs were converted into one-hot encoding, i.e. indicator variables for each genotype. Hence, the final number of genomic markers was 29169. Data from individual 1 to 2326 were used as training data and from individual 2327 to 3226 (the 5th generation) as test data.</p>
    </sec>
    <sec id="Sec11">
      <title>Real data</title>
      <p>The SNP genotype data comes from the bovine 50k Beadchip and the phenotypes measured were total number of spermatozoa in ejaculates during routine artificial insemination programs, both obtained from Austrian Fleckvieh bulls [<xref ref-type="bibr" rid="CR31">31</xref>]. Sperm quality data were obtained from 3 Austrian AI stations: Gleisdorf station in Styria (7704 ejaculates, 301 bulls sampled from 2000 to 2010), Hohenzell station in Upper Austria (16671 ejaculates, 309 bulls sampled from 2000 to 2009), and Wieselburg station in Lower Austria (15514 ejaculates, 293 bulls sampled from 2000 to 2009). In addition to year and station were also age of bull (13 - 161 months), month of year (January - December), period between 2 successive ejaculates (0 - 60 days) and collector (1 - 17) used as input variables. All variables, except age of bull and period between successive ejaculates, were transformed into 0,1 indicator variables (one-hot encoding). Age of bull and period between successive ejaculates were standardized to mean 0 and SD 1, whereas the phenotype total number of spermatozoa was mean centered. The final number of environmental variables was 716, where the first 617 corresponds to bull repeated measurements indicators.</p>
      <p>1799 Austrian Fleckvieh bulls were genotyped using the bovine SNP50 Beadchip v1 (Illumina) which contains 54001 SNPs (50k). Only autosomal SNPs that were assigned to a chromosome were used. Missing SNP data were imputed using FImpute [<xref ref-type="bibr" rid="CR32">32</xref>] and converted to 0,1 and 2 code. SNPs with MAF smaller than 0.01 were removed resulting in a total of 38871 SNPs. These SNPs were also converted into indicator variables which produced 116613 markers. Bulls without phenotype measures were also removed. The final number of bulls was 671 with a total of 21567 phenotype measures. The data was randomly divided into 10 repeats (corresponding to 10-fold cross-validation) of training data with 15097 observations and test data with 6470 observations.</p>
    </sec>
    <sec id="Sec12">
      <title>Implementation</title>
      <p>The full automatic adaptive LASSO (AUTALASSO) was implemented in Julia version 0.6 [<xref ref-type="bibr" rid="CR33">33</xref>] and uses the ProximalOperators package for the proximal steps in the ADMM algorithm. The Julia code used for the QTLMAS2010 data is provided at: <ext-link ext-link-type="uri" xlink:href="https://github.com/patwa67/AUTALASSO">https://github.com/patwa67/AUTALASSO</ext-link>.</p>
      <p>The prediction accuracy of the AUTALASSO was compared with the R implementation of an adaptive LASSO (ALASSO), the ordinary LASSO (LASSO) and ridge regression (RR) using the glmnet package with default settings [<xref ref-type="bibr" rid="CR14">14</xref>]. For comparative purposes were both MSE<sub><italic>test</italic></sub> and Pearson correlation coefficient (<italic>r</italic><sub><italic>test</italic></sub>) calculated. The evaluations were performed on an Intel Xeon E5 4-Core 3.7GHz with 256GB RAM.</p>
    </sec>
  </sec>
  <sec id="Sec13">
    <title>Appendix</title>
    <sec id="Sec14">
      <title>Proximal operators</title>
      <p>A proximal operator prox<italic>f</italic> is used to evaluate a closed and proper convex function <italic>f</italic>(<italic>β</italic>) of a specific optimization subproblem that is assumed to be easier to solve than the original problem. By iteratively evaluating proximal operators on subproblems, a proximal algorithm converges to the solution of the original problem [<xref ref-type="bibr" rid="CR34">34</xref>]. The proximal operator is defined as 
<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \operatorname{prox}_{f}(\theta) = \underset{\beta}{\operatorname{argmin}} \left(f(\beta)+(1/2)\left\| \beta -\theta \right\|_{2}^{2}\right)   $$ \end{document}</tex-math><mml:math id="M72"><mml:munder><mml:mrow><mml:mo>prox</mml:mo></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:munder><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>argmin</mml:mo></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:munder><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:mi>β</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where ∥·∥<sub>2</sub> is the Euclidean norm, and <italic>β</italic> and <italic>θ</italic> are vectors of length <italic>p</italic>. The right hand side of the argument is strongly convex so it has a unique minimizer for every <italic>θ</italic>∈<italic>R</italic><sup><italic>p</italic></sup>. A scaled version of (8) is obtained by introducing parameter <italic>λ</italic>&gt;0 resulting in <italic>λ</italic><italic>f</italic> where (1/2) is replaced by (1/2<italic>λ</italic>). This definition indicates that prox<italic>f</italic>(<italic>θ</italic>) is a point that compromises between minimizing <italic>f</italic> and being close to <italic>θ</italic>. <italic>λ</italic> can be seen as a trade-off parameter between these two terms. Another key property is for separable sum functions <italic>f</italic>(<italic>β</italic>,<italic>θ</italic>)=<italic>g</italic>(<italic>β</italic>)+<italic>h</italic>(<italic>θ</italic>) where splitting leads to 
<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \operatorname{prox}_{f}(\beta,\theta) = \operatorname{prox}_{g}(\beta)+\operatorname{prox}_{h}(\theta).   $$ \end{document}</tex-math><mml:math id="M74"><mml:munder><mml:mrow><mml:mo>prox</mml:mo></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:munder><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>prox</mml:mo></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:munder><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:mo>prox</mml:mo></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:munder><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mi>.</mml:mi></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Finally we note that there is a near relationship between proximal operators and gradient methods where 
<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \operatorname{prox}_{\lambda f}(\beta) \approx \beta-\lambda \nabla f(\beta)   $$ \end{document}</tex-math><mml:math id="M76"><mml:munder><mml:mrow><mml:mo>prox</mml:mo></mml:mrow><mml:mrow><mml:mi>λf</mml:mi></mml:mrow></mml:munder><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>≈</mml:mo><mml:mi>β</mml:mi><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>when <italic>λ</italic> is small and <italic>f</italic>(<italic>β</italic>) is differentiable. In this formulation, ∇ denotes the gradient and <italic>λ</italic> is an equivalent to the learning rate of a gradient optimizer [<xref ref-type="bibr" rid="CR16">16</xref>].</p>
    </sec>
    <sec id="Sec15">
      <title>Alternating direction method of multipliers (ADMM)</title>
      <p>The alternating direction method of multipliers (ADMM) is a simple but powerful algorithm that is appropriate for distributed convex optimization, and specifically to problems in statistics and machine learning. It is based on decomposition of a large global problem into small local coordinated subproblems to find a solution [<xref ref-type="bibr" rid="CR18">18</xref>].</p>
      <p>In order to understand the ADMM, it is useful to introduce the methods of dual ascent and augmented Lagrangians. For the convex optimization problem 
<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} &amp;\underset{\beta}{\text{minimize }}f(\beta)\\ &amp;{\text{subject to }}X\beta = y \end{aligned}  $$ \end{document}</tex-math><mml:math id="M78"><mml:mtable><mml:mtr><mml:mtd><mml:munder><mml:mrow><mml:mtext>minimize</mml:mtext></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>subject to</mml:mtext><mml:mi>Xβ</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>the Langrangian will be 
<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ L(\beta,\theta)=f(\beta)+\theta^{T}(X\beta - y)  $$ \end{document}</tex-math><mml:math id="M80"><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>Xβ</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>and the dual function is 
<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ g(\theta)=\underset{\beta}{\text{inf }}L(\beta,\theta)=-f^{*}\left(-X^{T}\theta\right)-y^{T}\theta  $$ \end{document}</tex-math><mml:math id="M82"><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>inf</mml:mtext></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:munder><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>θ</mml:mi></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <italic>X</italic> is a matrix of size <italic>n</italic>×<italic>p</italic>, <italic>y</italic> is a vector of length <italic>n</italic>, <italic>θ</italic> is the dual variable and <italic>f</italic><sup>∗</sup> is the convex conjugate of <italic>f</italic>. The dual problem is to maximize <italic>g</italic>(<italic>θ</italic>). In dual ascent the dual problem is solved using gradient ascent. If the dual function is differentiable the following iterating scheme describes dual ascent 
<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} &amp;\beta^{t+1}=\underset{\beta}{\operatorname{argmin}}L\left(\beta,\theta^{t}\right)\\ &amp;\theta^{t+1}=\theta^{t}+\rho\left(X\beta^{t+1}-y\right) \end{aligned}  $$ \end{document}</tex-math><mml:math id="M84"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>argmin</mml:mo></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:munder><mml:mi>L</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>X</mml:mi><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <italic>ρ</italic>&gt;0 is the learning rate and <italic>t</italic> is the iteration number. The first step of (<xref rid="Equ14" ref-type="">14</xref>) is a <italic>β</italic>-minimization step and the second step is a dual variable update. If <italic>ρ</italic>&gt;0 is adequately chosen and several assumptions hold, then <italic>β</italic><sup><italic>t</italic></sup> converges to an optimal point and <italic>θ</italic><sup><italic>t</italic></sup> converges to an optimal dual point.</p>
      <p>Augmented Lagrangian methods were partly developed to make the dual ascent method more robust in situations where the objective function lacks strict convexity. The augmented Lagrangian for (<xref rid="Equ11" ref-type="">11</xref>) is 
<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ L_{\lambda}(\beta,\theta)=f(\beta)+\theta^{T}(X\beta - y)+(\lambda/2)\left\| X\beta - y \right\|_{2}^{2}  $$ \end{document}</tex-math><mml:math id="M86"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>Xβ</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mi>λ</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:mi>Xβ</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <italic>λ</italic> is the penalty parameter. The augmented Lagrangian can be used in an iterative procedure denoted the method of multipliers 
<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} &amp;\beta^{t+1}=\underset{\beta}{\operatorname{argmin}}L_{\lambda}\left(\beta,\theta^{t}\right)\\ &amp;\theta^{t+1}=\theta^{t}+\lambda\left(X\beta^{t+1}-y\right) \end{aligned}  $$ \end{document}</tex-math><mml:math id="M88"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>argmin</mml:mo></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>X</mml:mi><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>which is the same as dual ascent, except for the augmented part of the Lagrangian and the use of <italic>λ</italic> as step size. The method of multipliers has better convergence properties than dual ascent.</p>
      <p>By extending the parameter space into <italic>β</italic> and <italic>θ</italic>, the optimization problem for the ADMM algorithm can now be rewritten as 
<disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} &amp;\underset{\beta}{\text{minimize }}f(\beta)+g(\theta)\\ &amp;{\text{subject to }}X\beta +Z\theta= y \end{aligned}  $$ \end{document}</tex-math><mml:math id="M90"><mml:mtable><mml:mtr><mml:mtd><mml:munder><mml:mrow><mml:mtext>minimize</mml:mtext></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>subject to</mml:mtext><mml:mi>Xβ</mml:mi><mml:mo>+</mml:mo><mml:mi>Zθ</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where the augented Lagrangian is 
<disp-formula id="Equ18"><label>18</label><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} L_{\lambda}(\beta,\theta,\mu)&amp;=f(\beta)+g(\theta)+\mu^{T}(X\beta+Z\theta- y)\\ &amp;+(\lambda/2)\left\| X\beta+Z\theta-y \right\|_{2}^{2} \end{aligned}  $$ \end{document}</tex-math><mml:math id="M92"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mi>Xβ</mml:mi><mml:mo>+</mml:mo><mml:mi>Zθ</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mi>λ</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:mi>Xβ</mml:mi><mml:mo>+</mml:mo><mml:mi>Zθ</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ18.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>and the iterative ADMM algorithm becomes 
<disp-formula id="Equ19"><label>19</label><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} &amp;\beta^{t+1}=\underset{\beta}{\operatorname{argmin}}L_{\lambda}\left(\beta,\theta^{t},\mu^{t}\right)\\ &amp;\theta^{t+1}=\underset{\theta}{\operatorname{argmin}}L_{\lambda}\left(\beta^{t+1},\theta,\mu^{t}\right)\\ &amp;\mu^{t+1}=\mu^{t}+\lambda(X\beta^{t+1}+Z\theta^{t+1}-y) \end{aligned}  $$ \end{document}</tex-math><mml:math id="M94"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>argmin</mml:mo></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>argmin</mml:mo></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>Z</mml:mi><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ19.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Noting that the primal residual is <italic>e</italic>=<italic>X</italic><italic>β</italic>+<italic>Z</italic><italic>θ</italic>−<italic>y</italic> and the scaled dual variable is <italic>u</italic>=(1/<italic>λ</italic>)/<italic>μ</italic> a scaled version of (<xref rid="Equ19" ref-type="">19</xref>) can be obtained as 
<disp-formula id="Equ20"><label>20</label><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ {{} \begin{aligned} &amp;\beta^{t+1}=\underset{\beta}{\operatorname{argmin}}\left(f(\beta)+(\lambda/2)\left\| X\beta+Z\theta^{t}-y+u^{t} \right\|_{2}^{2}\right)\\ &amp;\theta^{t+1}=\underset{\theta}{\operatorname{argmin}}\left(g(\theta)+(\lambda/2)\left\| X\beta^{t+1}+Z\theta-y+u^{t} \right\|_{2}^{2}\right)\\ &amp;u^{t+1}=u^{t}+X\beta^{t+1}+Z\theta^{t+1}-y \end{aligned}}  $$ \end{document}</tex-math><mml:math id="M96"><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>argmin</mml:mo></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:munder><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mi>λ</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:mi>Xβ</mml:mi><mml:mo>+</mml:mo><mml:mi>Z</mml:mi><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>argmin</mml:mo></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:munder><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mi>λ</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:msubsup><mml:mrow><mml:mfenced close="∥" open="∥" separators=""><mml:mrow><mml:mi>X</mml:mi><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>Zθ</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>X</mml:mi><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>Z</mml:mi><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_2743_Article_Equ20.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>and <italic>r</italic><sup><italic>t</italic>+1</sup>=<italic>λ</italic><italic>X</italic><sup><italic>T</italic></sup><italic>Z</italic>(<italic>θ</italic><sup><italic>t</italic>+1</sup>−<italic>θ</italic><sup><italic>t</italic></sup>) can be interpreted as a dual residual. It can be shown that both <italic>r</italic><sup><italic>t</italic></sup> and <italic>e</italic><sup><italic>t</italic></sup> approach zero at convergence [<xref ref-type="bibr" rid="CR16">16</xref>].</p>
    </sec>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgements</title>
    <p>Thanks to Christopher Rackauckas for useful suggestions regarding the Julia code.</p>
    <sec id="d29e4551">
      <title>Funding</title>
      <p>The work was supported by the Beijer foundation, Sweden and by a bilateral collaborative project involving the Croatian Ministry of Science, Education and Sports, and the Austrian Agency for International Cooperation in Education and Research (OeAD GmbH; Runs of homozygosity: Detecting selection in livestock populations).</p>
    </sec>
    <sec id="d29e4556" sec-type="data-availability">
      <title>Availability of data and materials</title>
      <p>The data that support the findings of this study are available from ZuchtData EDV-Dienstleistungen GmbH but restrictions apply to the availability of these data, which were used under license for the current study, and so are not publicly available. Data are however available from the authors upon reasonable request and with permission of ZuchtData EDV-Dienstleistungen GmbH.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>All the authors contributed to the method design. MF, GM, NK, IC and JS acquired and edited the Fleckvieh bull data. PW implemented the algorithm, carried out the experiments and wrote the paper. All authors have read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="COI-statement">
    <sec>
      <title>Ethics approval and consent to participate</title>
      <p>Not applicable.</p>
    </sec>
    <sec>
      <title>Consent for publication</title>
      <p>Not applicable.</p>
    </sec>
    <sec>
      <title>Competing interests</title>
      <p>The authors declare that they have no competing interests.</p>
    </sec>
    <sec>
      <title>Publisher’s Note</title>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </sec>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meuwissen</surname>
            <given-names>THE</given-names>
          </name>
          <name>
            <surname>Hayes</surname>
            <given-names>BJ</given-names>
          </name>
          <name>
            <surname>Goddard</surname>
            <given-names>ME</given-names>
          </name>
        </person-group>
        <article-title>Prediction of total genetic value using genome-wide dense marker maps</article-title>
        <source>Genetics</source>
        <year>2001</year>
        <volume>157</volume>
        <fpage>1819</fpage>
        <lpage>29</lpage>
        <?supplied-pmid 11290733?>
        <pub-id pub-id-type="pmid">11290733</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>de los Campos</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gianola</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Allison</surname>
            <given-names>DB</given-names>
          </name>
        </person-group>
        <article-title>Predicting genetic predisposition in humans: the promise of whole-genome markers</article-title>
        <source>Nat Rev Genet</source>
        <year>2010</year>
        <volume>11</volume>
        <fpage>880</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1038/nrg2898</pub-id>
        <pub-id pub-id-type="pmid">21045869</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <collab>Gondro C</collab>
          <collab>van der Werf J</collab>
          <collab>Hayes B</collab>
        </person-group>
        <source>Genome-Wide Association Studies and Genomic Prediction. 1st edn</source>
        <year>2013</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Humana Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <collab>Hastie T</collab>
          <collab>Tibshirani R</collab>
          <collab>Friedman J</collab>
        </person-group>
        <source>The Elements of Statistical Learning. 2nd edn</source>
        <year>2009</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Springer Verlag</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lv</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A selective overview of variable selection in high dimensional feature space</article-title>
        <source>Stat Sin</source>
        <year>2010</year>
        <volume>20</volume>
        <fpage>101</fpage>
        <lpage>48</lpage>
        <?supplied-pmid 21572976?>
        <pub-id pub-id-type="pmid">21572976</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <collab>Theodoridis S</collab>
        </person-group>
        <source>Machine Learning: A Bayesian and Optimization Perspective. 1st edn</source>
        <year>2015</year>
        <publisher-loc>London</publisher-loc>
        <publisher-name>Academic Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hoerl</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Kennard</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>Ridge regression: biased estimation for nonorthogonal problems</article-title>
        <source>Technometrics</source>
        <year>1970</year>
        <volume>12</volume>
        <fpage>55</fpage>
        <lpage>67</lpage>
        <pub-id pub-id-type="doi">10.1080/00401706.1970.10488634</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Regression shrinkage and selection via the Lasso</article-title>
        <source>J Roy Stat Soc Ser B</source>
        <year>1996</year>
        <volume>58</volume>
        <fpage>267</fpage>
        <lpage>88</lpage>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Sillanpää</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>Overview of LASSO-related penalized regression methods for quantitative trait mapping and genomic selection</article-title>
        <source>Theor Appl Genet</source>
        <year>2012</year>
        <volume>125</volume>
        <fpage>419</fpage>
        <lpage>35</lpage>
        <pub-id pub-id-type="doi">10.1007/s00122-012-1892-9</pub-id>
        <pub-id pub-id-type="pmid">22622521</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ogutu</surname>
            <given-names>JO</given-names>
          </name>
          <name>
            <surname>Schulz-Streeck</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Piepho</surname>
            <given-names>HP</given-names>
          </name>
        </person-group>
        <article-title>Genomic selection using regularized linear regression models: ridge regression, lasso, elastic net and their extensions</article-title>
        <source>BMC Proc</source>
        <year>2012</year>
        <volume>6</volume>
        <issue>Suppl 2</issue>
        <fpage>S10</fpage>
        <pub-id pub-id-type="doi">10.1186/1753-6561-6-S2-S10</pub-id>
        <pub-id pub-id-type="pmid">22640436</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vidaurre</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bielza</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Larrañaga</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>A survey of L1 regression</article-title>
        <source>Int Stat Rev</source>
        <year>2013</year>
        <volume>81</volume>
        <fpage>361</fpage>
        <lpage>87</lpage>
        <pub-id pub-id-type="doi">10.1111/insr.12023</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <collab>Hastie T</collab>
          <collab>Tibshirani R</collab>
          <collab>Wainwright M</collab>
        </person-group>
        <source>Statistical Learning with Sparsity: The Lasso and Generalizations. 1st edn</source>
        <year>2015</year>
        <publisher-loc>Boca Raton</publisher-loc>
        <publisher-name>CRC Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Efron</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Johnstone</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Least angle regression</article-title>
        <source>Ann Stat</source>
        <year>2004</year>
        <volume>32</volume>
        <fpage>407</fpage>
        <lpage>99</lpage>
        <pub-id pub-id-type="doi">10.1214/009053604000000067</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Friedman</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Regularization paths for generalized linear models via coordinate descent</article-title>
        <source>J Stat Soft</source>
        <year>2010</year>
        <volume>33</volume>
        <fpage>1</fpage>
        <lpage>22</lpage>
        <pub-id pub-id-type="doi">10.18637/jss.v033.i01</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <collab>Bertsekas DP</collab>
        </person-group>
        <source>Convex Optimization Algorithms. 1st edn</source>
        <year>2015</year>
        <publisher-loc>Belmont</publisher-loc>
        <publisher-name>Athena Scientific</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Parikh</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Boyd</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Proximal algorithms</article-title>
        <source>Found Trend Opt</source>
        <year>2013</year>
        <volume>1</volume>
        <fpage>123</fpage>
        <lpage>231</lpage>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Beck</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Teboulle</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>A fast iterative shrinkage-thresholding algorithm for linear inverse problems</article-title>
        <source>SIAM J Imag Sci</source>
        <year>2009</year>
        <volume>2</volume>
        <fpage>183</fpage>
        <lpage>202</lpage>
        <pub-id pub-id-type="doi">10.1137/080716542</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boyd</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Parikh</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Chu</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Peleato</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Eckstein</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Distributed optimization and statistical learning via the alternating direction method of multipliers</article-title>
        <source>Found Trend Mach Learn</source>
        <year>2011</year>
        <volume>3</volume>
        <fpage>1</fpage>
        <lpage>122</lpage>
        <pub-id pub-id-type="doi">10.1561/2200000016</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Challenges of big data analysis</article-title>
        <source>Nat Sci Rev</source>
        <year>2014</year>
        <volume>1</volume>
        <fpage>293</fpage>
        <lpage>314</lpage>
        <pub-id pub-id-type="doi">10.1093/nsr/nwt032</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>de los Campos</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Hickey</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Pong-Wong</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Daetwyler</surname>
            <given-names>HD</given-names>
          </name>
          <name>
            <surname>Calus</surname>
            <given-names>MPL</given-names>
          </name>
        </person-group>
        <article-title>Whole genome regression and prediction methods applied to plant and animal breeding</article-title>
        <source>Genetics</source>
        <year>2013</year>
        <volume>193</volume>
        <fpage>327</fpage>
        <lpage>45</lpage>
        <pub-id pub-id-type="doi">10.1534/genetics.112.143313</pub-id>
        <pub-id pub-id-type="pmid">22745228</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <collab>Okser S</collab>
          <collab>Pahikkala T</collab>
          <collab>Airola A</collab>
          <collab>Salakoski T</collab>
          <collab>Ripatti S</collab>
          <collab>Aittokallio T</collab>
        </person-group>
        <article-title>Regularized machine learning in the genetic prediction of complex traits</article-title>
        <source>PLoS Genet</source>
        <year>2014</year>
        <volume>10</volume>
        <fpage>e1004754</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pgen.1004754</pub-id>
        <pub-id pub-id-type="pmid">25393026</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gianola</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Priors in whole-genome regression: The Bayesian alphabet returns</article-title>
        <source>Genetics</source>
        <year>2013</year>
        <volume>194</volume>
        <fpage>573</fpage>
        <lpage>96</lpage>
        <pub-id pub-id-type="doi">10.1534/genetics.113.151753</pub-id>
        <pub-id pub-id-type="pmid">23636739</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Usai</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Goddard</surname>
            <given-names>ME</given-names>
          </name>
          <name>
            <surname>Hayes</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>LASSO with cross-validation for genomic selection</article-title>
        <source>Genet Res</source>
        <year>2009</year>
        <volume>91</volume>
        <fpage>427</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="doi">10.1017/S0016672309990334</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chi</surname>
            <given-names>EC</given-names>
          </name>
          <name>
            <surname>Lange</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>(2015) Splitting methods for convex clustering</article-title>
        <source>J Comp Graph Stat</source>
        <year>2015</year>
        <volume>24</volume>
        <fpage>994</fpage>
        <lpage>1013</lpage>
        <pub-id pub-id-type="doi">10.1080/10618600.2014.948181</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>An augmented ADMM algorithm with application to the generalized lasso problem</article-title>
        <source>J Comp Graph Stat</source>
        <year>2017</year>
        <volume>26</volume>
        <fpage>195</fpage>
        <lpage>204</lpage>
        <pub-id pub-id-type="doi">10.1080/10618600.2015.1114491</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>The adaptive lasso and its oracle properties</article-title>
        <source>J Am Stat Ass</source>
        <year>2006</year>
        <volume>101</volume>
        <fpage>1418</fpage>
        <lpage>29</lpage>
        <pub-id pub-id-type="doi">10.1198/016214506000000735</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>CH</given-names>
          </name>
        </person-group>
        <article-title>Adaptive lasso for sparse high-dimensional regression models</article-title>
        <source>Stat Sin</source>
        <year>2008</year>
        <volume>18</volume>
        <fpage>1603</fpage>
        <lpage>18</lpage>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <collab>Press WH</collab>
          <collab>Teukolsky SA</collab>
          <collab>Vetterling WT</collab>
          <collab>Flannery BP</collab>
        </person-group>
        <source>Numerical Recipes: The Art of Scientific Computing. 3rd edn</source>
        <year>2007</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Cambridge University Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ramdas</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>RJ</given-names>
          </name>
        </person-group>
        <article-title>Fast and flexible ADMM algorithms for trend filtering</article-title>
        <source>J Compu Graph Stat</source>
        <year>2016</year>
        <volume>25</volume>
        <fpage>839</fpage>
        <lpage>58</lpage>
        <pub-id pub-id-type="doi">10.1080/10618600.2015.1054033</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <collab>Szydłowski M</collab>
          <collab>Paczyńska P</collab>
        </person-group>
        <article-title>QTLMAS 2010: simulated dataset</article-title>
        <source>BMC Proc</source>
        <year>2011</year>
        <volume>5</volume>
        <issue>Suppl 3</issue>
        <fpage>S3</fpage>
        <pub-id pub-id-type="doi">10.1186/1753-6561-5-S3-S3</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ferenčaković</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sölkner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kapš</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Curik</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Genome-wide mapping and estimation of inbreeding depression of semen quality traits in a cattle population</article-title>
        <source>J Dairy Sci</source>
        <year>2017</year>
        <volume>100</volume>
        <fpage>4721</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.3168/jds.2016-12164</pub-id>
        <pub-id pub-id-type="pmid">28434751</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sargolzaei</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Chesnais</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Schenkel</surname>
            <given-names>FS</given-names>
          </name>
        </person-group>
        <article-title>A new approach for efficient genotype imputation using information from relatives</article-title>
        <source>BMC Genom</source>
        <year>2014</year>
        <volume>15</volume>
        <fpage>478</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2164-15-478</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bezanson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Edelman</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Karpinski</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>VB</given-names>
          </name>
        </person-group>
        <article-title>Julia: A fresh approach to numerical computing</article-title>
        <source>SIAM Rev</source>
        <year>2017</year>
        <volume>59</volume>
        <fpage>65</fpage>
        <lpage>98</lpage>
        <pub-id pub-id-type="doi">10.1137/141000671</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Polson</surname>
            <given-names>NG</given-names>
          </name>
          <name>
            <surname>Scott</surname>
            <given-names>JG</given-names>
          </name>
          <name>
            <surname>Willard</surname>
            <given-names>BT</given-names>
          </name>
        </person-group>
        <article-title>Proximal algorithms in statistics and machine learning</article-title>
        <source>Stat Sci</source>
        <year>2015</year>
        <volume>30</volume>
        <fpage>559</fpage>
        <lpage>81</lpage>
        <pub-id pub-id-type="doi">10.1214/15-STS530</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
