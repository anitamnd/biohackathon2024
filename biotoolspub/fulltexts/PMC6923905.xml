<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Genomics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Genomics</journal-id>
    <journal-title-group>
      <journal-title>BMC Genomics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2164</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6923905</article-id>
    <article-id pub-id-type="publisher-id">6286</article-id>
    <article-id pub-id-type="doi">10.1186/s12864-019-6286-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>A hybrid and scalable error correction algorithm for indel and substitution errors of long reads</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Das</surname>
          <given-names>Arghya Kusum</given-names>
        </name>
        <address>
          <email>dasa@uwplatt.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Goswami</surname>
          <given-names>Sayan</given-names>
        </name>
        <address>
          <email>sgoswa1@lsu.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lee</surname>
          <given-names>Kisung</given-names>
        </name>
        <address>
          <email>klee@lsu.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Park</surname>
          <given-names>Seung-Jong</given-names>
        </name>
        <address>
          <email>sjpark@cct.lsu.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label>Department of Computer Science and Software Engineering, University of Wisconsin at Platteville, Platteville, WI USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0662 7451</institution-id><institution-id institution-id-type="GRID">grid.64337.35</institution-id><institution>School of Electrical Engineering and Computer Science, Center for Computation and Technology, Louisiana State University, Baton Rouge, </institution></institution-wrap>Baton Rouge, LA USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>20</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <issue>Suppl 11</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. The Supplement Editors declare that they have no competing interests.</issue-sponsor>
    <elocation-id>948</elocation-id>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Long-read sequencing has shown the promises to overcome the short length limitations of second-generation sequencing by providing more complete assembly. However, the computation of the long sequencing reads is challenged by their higher error rates (e.g., 13% vs. 1%) and higher cost ($0.3 vs. $0.03 per Mbp) compared to the short reads.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p id="Par2">In this paper, we present a new hybrid error correction tool, called ParLECH (Parallel Long-read Error Correction using Hybrid methodology). The error correction algorithm of ParLECH is distributed in nature and efficiently utilizes the <italic>k</italic>-mer coverage information of high throughput Illumina short-read sequences to rectify the PacBio long-read sequences.ParLECH first constructs a de Bruijn graph from the short reads, and then replaces the indel error regions of the long reads with their corresponding widest path (or maximum min-coverage path) in the short read-based de Bruijn graph. ParLECH then utilizes the <italic>k</italic>-mer coverage information of the short reads to divide each long read into a sequence of low and high coverage regions, followed by a majority voting to rectify each substituted error base.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par3">ParLECH outperforms latest state-of-the-art hybrid error correction methods on real PacBio datasets. Our experimental evaluation results demonstrate that ParLECH can correct large-scale real-world datasets in an accurate and scalable manner. ParLECH can correct the indel errors of human genome PacBio long reads (312 GB) with Illumina short reads (452 GB) in less than 29 h using 128 compute nodes. ParLECH can align more than 92% bases of an <italic>E. coli</italic> PacBio dataset with the reference genome, proving its accuracy.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par4">ParLECH can scale to over terabytes of sequencing data using hundreds of computing nodes. The proposed hybrid error correction methodology is novel and rectifies both indel and substitution errors present in the original long reads or newly introduced by the short reads.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Hybrid error correction</kwd>
      <kwd>PacBio</kwd>
      <kwd>Illumina</kwd>
      <kwd>Hadoop</kwd>
      <kwd>NoSQL</kwd>
    </kwd-group>
    <conference xlink:href="http://orienta.ugr.es/bibm2018/">
      <conf-name>IEEE International Conference on Bioinformatics and Biomedicine 2018</conf-name>
      <conf-loc>Madrid, Spain</conf-loc>
      <conf-date>3-6 December 2018</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>The rapid development of genome sequencing technologies has become the major driving force for genomic discoveries. The second-generation sequencing technologies (e.g., Illumina, Ion Torrent) have been providing researchers with the required throughput at significantly low cost ($0.03/million-bases), which enabled the discovery of many new species and variants. Although they are being widely utilized for understanding the complex phenotypes, they are typically incapable of resolving long repetitive elements, common in various genomes (e.g., eukaryotic genomes), because of the short read lengths [<xref ref-type="bibr" rid="CR1">1</xref>].</p>
    <p>To address the issues with the short read lengths, third-generation sequencing technologies (e.g., PacBio, Oxford Nanopore) have started emerging recently. By producing long reads greater than 10 kbp, these third-generation sequencing platforms provide researchers with significantly less fragmented assembly and the promise of a much better downstream analysis. However, the production costs of these long sequences are almost 10 times more expensive than those of the short reads, and the analysis of these long reads is severely constrained by their higher error rate.</p>
    <p>Motivated by this, we develop ParLECH (Parallel Long-read Error Correction using Hybrid methodology). ParLECH uses the power of MapReduce and distributed NoSQL to scale with terabytes of sequencing data [<xref ref-type="bibr" rid="CR2">2</xref>]. Utilizing the power of these big data programming models, we develop fully distributed algorithms to replace both the indel and substitution errors of long reads. To rectify the indel errors, we first create a de Bruijn graph from the Illumina short reads. The indel errors of the long reads are then replaced with the widest path algorithm that maximizes the minimum <italic>k</italic>-mer coverage between two vertices in the de Bruijn graph. To correct the substitution errors, we divide the long read into a series of low and high coverage regions by utilizing the median statistics of the <italic>k</italic>-mer coverage information of the Illumina short reads. The substituted error bases are then replaced separately in those low and high coverage regions.</p>
    <p>ParLECH can achieve higher accuracy and scalability over existing error correction tools. For example, ParLECH successfully aligns 95% of <italic>E. Coli</italic> long reads, maintaining larger N50 compared to the existing tools. We demonstrate the scalability of ParLECH by correcting a 312GB human genome PacBio dataset, with leveraging a 452 GB Illumina dataset (64x coverage), on 128 nodes in less than 29 h.</p>
    <sec id="Sec2">
      <title>Related work</title>
      <p>The second-generation sequencing platforms produce short reads at an error rate of 1-2% [<xref ref-type="bibr" rid="CR3">3</xref>] in which most of the errors are substitution errors. However, the low cost of production results in high coverage of data, which enables self-correction of the errors without using any reference genome. Utilizing the basic fact that the <italic>k</italic>-mers resulting from an error base will have significantly lower coverage compared to the actual <italic>k</italic>-mers, many error correction tools have been proposed such as Quake [<xref ref-type="bibr" rid="CR4">4</xref>], Reptile [<xref ref-type="bibr" rid="CR5">5</xref>], Hammer [<xref ref-type="bibr" rid="CR6">6</xref>], RACER [<xref ref-type="bibr" rid="CR7">7</xref>], Coral [<xref ref-type="bibr" rid="CR8">8</xref>], Lighter [<xref ref-type="bibr" rid="CR9">9</xref>], Musket [<xref ref-type="bibr" rid="CR10">10</xref>], Shrec [<xref ref-type="bibr" rid="CR11">11</xref>], DecGPU [<xref ref-type="bibr" rid="CR12">12</xref>], Echo [<xref ref-type="bibr" rid="CR13">13</xref>], and ParSECH [<xref ref-type="bibr" rid="CR14">14</xref>].</p>
      <p>Unlike second-generation sequencing platforms, the third-generation sequencing platforms, such as PacBio and Oxford Nanopore sequencers, produce long reads where indel (insertion/deletion) errors are dominant [<xref ref-type="bibr" rid="CR1">1</xref>]. Therefore, the error correction tools designed for substitution errors in short reads cannot produce accurate results for long reads. However, it is common to leverage the relatively lower error rate of the short-read sequences to improve the quality of long reads.</p>
      <p>While improving the quality of long reads, these hybrid error correction tools also reduce the cost of the pipeline by utilizing the complementary low-cost and high-quality short reads. LoRDEC [<xref ref-type="bibr" rid="CR15">15</xref>], Jabba [<xref ref-type="bibr" rid="CR16">16</xref>], Proovread [<xref ref-type="bibr" rid="CR17">17</xref>], PacBioToCA [<xref ref-type="bibr" rid="CR18">18</xref>], LSC [<xref ref-type="bibr" rid="CR19">19</xref>], and ColorMap [<xref ref-type="bibr" rid="CR20">20</xref>] are a few examples of hybrid error correction tools. LoRDEC [<xref ref-type="bibr" rid="CR15">15</xref>] and Jabba [<xref ref-type="bibr" rid="CR16">16</xref>] use a de Bruijn graph (DBG)-based methodology for error correction. Both the tools build the DBG from Illumina short reads. LoRDEC then corrects the error regions in long reads through the local assembly on the DBG while Jabba uses different sizes of <italic>k</italic>-mer iteratively to polish the unaligned regions of the long reads. Some hybrid error correction tools use alignment-based approaches for correcting the long reads. For example, PacBioToCA [<xref ref-type="bibr" rid="CR18">18</xref>] and LSC [<xref ref-type="bibr" rid="CR19">19</xref>] first map the short reads to the long reads to create an overlap graph. The long reads are then corrected through a consensus-based algorithm. Proovread [<xref ref-type="bibr" rid="CR17">17</xref>] reaches the consensus through the iterative alignment procedures that increase the sensitivity of the long reads incrementally in each iteration. ColorMap [<xref ref-type="bibr" rid="CR20">20</xref>] keeps information of consensual dissimilarity on each edge of the overlap graph and then utilizes the Dijkstra’s shortest path algorithm to rectify the indel errors. Although these tools produce accurate results in terms of successful alignments, their error correction process is lossy in nature, which reduces the coverage of the resultant data set. For example, Jabba, PacBioToCA, and Proovread use aggressive trimming of the error regions of the long reads instead of correcting them, losing a huge number of bases after the correction [<xref ref-type="bibr" rid="CR21">21</xref>] and thereby limiting the practical use of the resultant data sets. Furthermore, these tools use a stand-alone methodology to improve the base quality of the long reads, which suffers from scalability issues that limit their practical adoption for large-scale genomes.</p>
      <p>On the contrary, ParLECH is distributed in nature, and it can scale to terabytes of sequencing data on hundreds of compute nodes. ParLECH utilizes the DBG for error correction like LoRDEC. However, to improve the error correction accuracy, we propose a widest path algorithm that maximizes the minimum <italic>k</italic>-mer coverage between two vertices of the DBG. By utilizing the <italic>k</italic>-mer coverage information during the local assembly on the DBG, ParLECH is capable to produce more accurate results than LoRDEC. Unlike Jabba, PacBioToCA, and Proovread, ParLECH does not use aggressive trimming to avoid lossy correction. ParLECH further improves the base quality instead by correcting the substitution errors either present in the original long reads or newly introduced by the short reads during the hybrid correction of the indel errors. Although there are several tools to rectify substitution errors for second-generation sequences (e.g., [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR13">13</xref>]), this phase is often overlooked in the error correction tools developed for long reads. However, this phase is important for hybrid error correction because a significant number of substitution errors are introduced by the Illumina reads. Existing pipelines depend on polishing tools, such as Pilon [<xref ref-type="bibr" rid="CR22">22</xref>] and Quiver [<xref ref-type="bibr" rid="CR23">23</xref>], to further improve the quality of the corrected long reads. Unlike the distributed error correction pipeline of ParLECH, these polishing tools are stand-alone and cannot scale with large genomes.</p>
      <p>LorMA [<xref ref-type="bibr" rid="CR24">24</xref>], CONSENT [<xref ref-type="bibr" rid="CR25">25</xref>], and Canu [<xref ref-type="bibr" rid="CR26">26</xref>] are a few self-error correction tools that utilize long reads only to rectify the errors in them. These tools can automatically bypass the substitution errors of the short reads and are capable to produce accurate results. However, the sequencing cost per base for long reads is extremely high, and so it would be prohibitive to get long reads with high coverage that is essential for error correction without reference genomes. Although Canu reduces the coverage requirement to half of that of LorMA and CONSENT by using the tf-idf weighting scheme for long reads, almost 10 times more expensive cost of PacBio sequences is still a major obstacle to utilizing it for large genomes. Because of this practical limitation, we do not report the accuracy of the these self-error correction tools in this paper.</p>
    </sec>
  </sec>
  <sec id="Sec3">
    <title>Methods</title>
    <sec id="Sec4">
      <title>Rationale behind the indel error correction</title>
      <p>Since we leverage the lower error rate of Illumina reads to correct the PacBio indel errors, let us first describe an error model for Illumina sequences and its consequence on the DBG constructed from these reads. We first observe that <italic>k</italic>-mers, DNA words of a fixed length <italic>k</italic>, tend to have similar abundances within a read. This is a well-known property of <italic>k</italic>-mers that stem from each read originating from a single source molecule of DNA [<xref ref-type="bibr" rid="CR27">27</xref>]. Let us consider two reads <italic>R</italic><sub>1</sub> and <italic>R</italic><sub>2</sub> representing the same region of the genome, and <italic>R</italic>1 has one error base. Assuming that the <italic>k</italic>-mers between the position <italic>p</italic><italic>o</italic><italic>s</italic><sub><italic>begin</italic></sub> and <italic>p</italic><italic>o</italic><italic>s</italic><sub><italic>end</italic></sub> represent an error region in <italic>R</italic><sub>1</sub> where error base is at position <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${pos}_{error} = \frac {pos_{end}+{pos}_{begin}}{2}$\end{document}</tex-math><mml:math id="M2"><mml:msub><mml:mrow><mml:mtext mathvariant="italic">pos</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">error</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">po</mml:mtext><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">end</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="italic">pos</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">begin</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12864_2019_6286_Article_IEq1.gif"/></alternatives></inline-formula>, we can make the following claim.</p>
      <p><italic>Claim 1:</italic> The coverage of at least one <italic>k</italic>-mer of <italic>R</italic><sub>1</sub> in the region between <italic>p</italic><italic>o</italic><italic>s</italic><sub><italic>begin</italic></sub> and <italic>p</italic><italic>o</italic><italic>s</italic><sub><italic>end</italic></sub> is lower than the coverage of any <italic>k</italic>-mer in the same region of <italic>R</italic><sub>2</sub>. A brief theoretical rationale of the claim can be found in Additional file <xref rid="MOESM1" ref-type="media">1</xref>. Figure <xref rid="Fig1" ref-type="fig">1</xref> shows the rationale behind the claim.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Widest Path Example: Select correct path for high coverage error <italic>k</italic>-mers</p></caption><graphic xlink:href="12864_2019_6286_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec5">
      <title>Rationale behind the substitution error correction</title>
      <p>After correcting the indel errors with the Illumina reads, a substantial number of substitution errors are introduced in the PacBio reads as they dominate in the Illumina short-read sequences. To rectify those errors, we first divide each PacBio long read into smaller subregions like short reads. Next, we classify only those subregions as errors where most of the <italic>k</italic>-mers have high coverage, and only a few low-coverage <italic>k</italic>-mers exist as outliers.</p>
      <p>Specifically, we use Pearson’s skew coefficient (or median skew coefficient) to classify the true and error subregions. Figure <xref rid="Fig2" ref-type="fig">2</xref> shows the histogram of three different types of subregions in a genomic dataset. Figure <xref rid="Fig2" ref-type="fig">2</xref>a has similar numbers of low- and high-coverage <italic>k</italic>-mers, making the skewness of this subregion almost zero. Hence, it is not considered as error. Figure <xref rid="Fig2" ref-type="fig">2</xref>b is also classified as true because the subregion is mostly populated with the low-coverage <italic>k</italic>-mers. Figure <xref rid="Fig2" ref-type="fig">2</xref>c is classified as error because the subregion is largely skewed towards the high-coverage <italic>k</italic>-mers, and only a few low-coverage <italic>k</italic>-mers exist as outliers. Existing substitution error correction tools do not analyze the coverage of neighboring <italic>k</italic>-mers and often classify the true yet low-coverage <italic>k</italic>-mers (e.g., Fig. <xref rid="Fig2" ref-type="fig">2</xref>b as errors.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Skewness in <italic>k</italic>-mer coverage statistics</p></caption><graphic xlink:href="12864_2019_6286_Fig2_HTML" id="MO2"/></fig></p>
      <p>Another major advantage of our median-based methodology is that the accuracy of the method has a lower dependency on the value of <italic>k</italic>. Median values are robust because, for a relatively small value of <italic>k</italic>, a few substitution errors will not alter the median <italic>k</italic>-mer abundance of the read [<xref ref-type="bibr" rid="CR28">28</xref>]. However, these errors will increase the skewness of the read. The robustness of the median values in the presence of sequencing errors is shown mathematically in the Additional file <xref rid="MOESM1" ref-type="media">1</xref>.</p>
    </sec>
    <sec id="Sec6">
      <title>Big data framework in the context of genomic error correction</title>
      <p>Error correction for sequencing data is not only data- and compute-intensive but also search-intensive because the size of the <italic>k</italic>-mer spectrum increases almost exponentially with the increasing value of <italic>k</italic> (i.e., up to 4<sup><italic>k</italic></sup> unique <italic>k</italic>-mers), and we need to search in the huge search space. For example, a large genome with 1 million reads of length 5000 bp involves more than 5 billion searches in a set of almost 10 billion unique <italic>k</italic>-mers. Since existing hybrid error correction tools are not designed for large-scale genome sequence data such as human genomes, we design ParLECH as a scalable and distributed framework equipped with Hadoop and Hazelcast.</p>
      <p>Hadoop is an open-source abstraction of Google’s MapReduce, which is a fully parallel and distributed framework for large-scale computation. It reads the data from a distributed file system called Hadoop Distributed File System (HDFS) in small subsets. In the Map phase, a Map function executes on each subset, producing the output in the form of key-value pairs. These intermediate key-value pairs are then grouped based on the unique keys. Finally, a Reduce function executes on each group, producing the final output on HDFS.</p>
      <p>Hazelcast [<xref ref-type="bibr" rid="CR29">29</xref>] is a NoSQL database, which stores large-scale data in the distributed memory using a key-value format. Hazelcast uses MummurHash to distribute the data evenly over multiple nodes and to reduce the collision. The data can be stored and retrieved from Hazelcast using hash table functions (such as <italic>get</italic> and <italic>put</italic>) in <italic>O</italic>(1) time. Multiple Map and Reduce functions can access this hash table simultaneously and independently, improving the search performance of ParLECH.</p>
    </sec>
    <sec id="Sec7">
      <title>Error correction pipeline</title>
      <p>Figure <xref rid="Fig3" ref-type="fig">3</xref> shows the indel error correction pipeline of ParLECH. It consists of three phases: 1) constructing a de Bruijn graph, 2) locating errors in long reads, and 3) correcting the errors. We store the raw sequencing reads in the HDFS while Hazelcast is used to store the de Bruijn graph created from the Illumina short reads. We develop the graph construction algorithm following the MapReduce programming model and use Hadoop for this purpose. In the subsequent phases, we use both Hadoop and Hazelcast to locate and correct the indel errors. Finally, we write the indel error-corrected reads into HDFS. We describe each phase in detail in the subsequent sections.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Indel error correction</p></caption><graphic xlink:href="12864_2019_6286_Fig3_HTML" id="MO3"/></fig></p>
      <p>ParLECH has three major steps for hybrid correction of indel errors as shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. In the first step, we construct a DBG from the Illumina short reads with the coverage information of each <italic>k</italic>-mer stored in each vertex. In the second step, we partition each PacBio long read into a sequence of strong and weak regions (alternatively, correct and error regions respectively) based on the <italic>k</italic>-mer coverage information stored in the DBG. We select the right and left boundary <italic>k</italic>-mers of two consecutive strong regions as source and destination vertices respectively in the DBG. Finally, in the third step, we replace each weak region (i.e., indel error region) of the long read between those two boundary <italic>k</italic>-mers with the corresponding widest path in the DBG, which maximizes the minimum <italic>k</italic>-mer coverage between those two vertices.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Error correction steps</p></caption><graphic xlink:href="12864_2019_6286_Fig4_HTML" id="MO4"/></fig></p>
      <p>Figure <xref rid="Fig5" ref-type="fig">5</xref> shows the substitution error correction pipeline of ParLECH. It has two different phases: 1) locating errors and 2) correcting errors. Like the indel error correction, the computation of phase is fully distributed with Hadoop. These Hadoop-based algorithms work on top of the indel error-corrected reads that were generated in the last phase and stored in HDFS. The same <italic>k</italic>-mer spectrum that was generated from the Illumina short reads and stored in Hazelcast is used to correct the substitution errors as well.
<fig id="Fig5"><label>Fig. 5</label><caption><p>Substitution error correction</p></caption><graphic xlink:href="12864_2019_6286_Fig5_HTML" id="MO5"/></fig></p>
    </sec>
    <sec id="Sec8">
      <title>De bruijn graph construction and counting <italic>k</italic>-mer</title>
      <p>
        <graphic position="anchor" xlink:href="12864_2019_6286_Figa_HTML" id="MO10"/>
      </p>
      <p>Algorithm 1 explains the MapReduce algorithm for de Bruijn graph construction, and Fig. <xref rid="Fig6" ref-type="fig">6</xref> shows the working of the algorithm. The map function scans each read of the data set and emits each <italic>k</italic>-mer as an intermediate key and its previous and next <italic>k</italic>-mer as the value. The intermediate key represents a vertex in the de Bruijn graph whereas the previous and the next <italic>k</italic>-mers in the intermediate value represent an incoming edge and an outgoing edge respectively. An associated count of occurrence (1) is also emitted as a part of the intermediate value. After the map function completes, the shuffle phase partitions these intermediate key-value pairs on the basis of the intermediate key (the <italic>k</italic>-mer). Finally, the reduce function accumulates all the previous <italic>k</italic>-mers and next <italic>k</italic>-mers corresponding to the key as the incoming and outgoing edges respectively. The same reduce function also sums together all the intermediate counts (i.e., 1) emitted for that particular <italic>k</italic>-mer. In the end of the reduce function, the entire graph structure and the count for each <italic>k</italic>-mer is stored in the NoSQL database of Hazelcast using Hazelcast’s <italic>put</italic> method. For improved performance, we emit only a single nucleotide character (i.e., <italic>A</italic>, <italic>T</italic>, <italic>G</italic>, or <italic>C</italic> instead of the entire <italic>k</italic>-mer) to store the incoming and outgoing edges. The actual <italic>k</italic>-mer can be obtained by prepending/appending that character with the <italic>k</italic>−1 prefix/suffix of the vertex <italic>k</italic>-mer.
<fig id="Fig6"><label>Fig. 6</label><caption><p>De Bruijn graph construction and <italic>k</italic>-mer count</p></caption><graphic xlink:href="12864_2019_6286_Fig6_HTML" id="MO6"/></fig></p>
    </sec>
    <sec id="Sec9">
      <title>Locating the indel errors of long read</title>
      <p>To locate the errors in the PacBio long reads, ParLECH uses the <italic>k</italic>-mer coverage information from the de Bruijn graph stored in Hazelcast. The entire process is designed in an embarrassingly parallel fashion and developed as a Hadoop Map-only job. Each of the map tasks scans through each of the PacBio reads and generates the <italic>k</italic>-mers with the same value of <italic>k</italic> as in the de Bruijn graph. Then, for each of those <italic>k</italic>-mers, we search the coverage in the graph. If the coverage falls below a predefined threshold, we mark it as weak indicating an indel error in the long read. It is possible to find more than one consecutive errors in a long read. In that case, we mark the entire region as weak. If the coverage is above the predefined threshold, we denote the region as strong or correct. To rectify the weak region, ParLECH uses the widest path algorithm described in the next subsection.</p>
    </sec>
    <sec id="Sec10">
      <title>Correcting the indel errors</title>
      <p>Like locating the errors, our correction algorithm is also embarrassingly parallel and developed as a Hadoop Map-only job. Like LoRDEC, we use the pair of strong <italic>k</italic>-mers that enclose a weak region of a long read as the source and destination vertices in the DBG. Any path in the DBG between those two vertices denotes a sequence that can be assembled from the short reads. We implement the widest path algorithm for this local assembly. The widest path algorithm maximizes the minimum <italic>k</italic>-mer coverage of a path in the DBG. We use the widest path based on our assumption that the probability of having the <italic>k</italic>-mer with the minimum coverage is higher in a path generated from a read with sequencing errors than a path generated from a read without sequencing errors for the same region in a genome. In other words, even if there are some <italic>k</italic>-mers with high coverage in a path, it is highly likely that the path includes some <italic>k</italic>-mer with low coverage that will be an obstacle to being selected as the widest path, as illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>.</p>
      <p>Therefore, ParLECH is equipped with the widest path technique to find a more accurate sequence to correct the weak region in the long read. Algorithm 2 shows our widest path algorithm implemented in ParLECH, a slight modification of the Dijkstra’s shortest path algorithm using a priority queue that leads to the time complexity of <italic>O</italic>(<italic>E</italic> log<italic>V</italic>). Instead of computing the shortest paths, ParLECH traverses the graph and updates the width of each path from the source vertex as the minimum width of any edge on the path (line 15).</p>
    </sec>
    <sec id="Sec11">
      <title>Locating the substitution error</title>
      <p>
        <graphic position="anchor" xlink:href="12864_2019_6286_Figb_HTML" id="MO11"/>
      </p>
      <p>
        <graphic position="anchor" xlink:href="12864_2019_6286_Figc_HTML" id="MO9"/>
      </p>
      <p>Algorithm 3 shows the process to locate substitution base errors. To locate the substitution errors in the long reads, we first divided the long reads into shorter fragments. As the <italic>k</italic>-mers in a smaller subregion tend to have similar abundances [<xref ref-type="bibr" rid="CR27">27</xref>], this will divide the longer reads into a sequence of high- and low-coverage fragments. If a fragment belongs to a low-coverage area of the genome, most of the k-mers in that fragment are expected to have low coverage. Otherwise, the <italic>k</italic>-mers are expected to have high coverage. This methodology enables ParLECH to better distinguish between true-yet-low-coverage and error-yet-high-coverage <italic>k</italic>-mers. By default, ParLECH uses the length of the short reads as the length of the shorter fragments. However, it can be easily modified with a user-defined length. The last fragment of the long reads can have a length shorter than default (or user-defined) length. This fragment is always ignored for correcting the substitution error as it is considered insufficient to gather any statistics.</p>
      <p>After dividing the long reads into shorter fragments, we calculate the Pearson’s skew coefficient (mentioned as <italic>skewThreshold</italic> in Algorithm 3) of the <italic>k</italic>-mer coverage of each fragment as a threshold to classify those fragments as true or error. If the skew coefficient of the fragment lies in a certain interval, the fragment is classified as a true fragment without any error. Furthermore, the fragments with mostly low-coverage <italic>k</italic>-mers are also ignored. All the other fragments (i.e., the fragments with highly skewed towards high-coverage <italic>k</italic>-mers) are classified as erroneous. Through this classification, all the low-coverage areas of the genome will be considered as correct even if they have low-coverage <italic>k</italic>-mers but almost similar coverage as that of the neighboring <italic>k</italic>-mers.</p>
      <p>After classifying the fragments as true and error, we divide all the error fragments as high and low coverage. If the median <italic>k</italic>-mer coverage of a fragment is greater than the median coverage of the entire <italic>k</italic>-mer spectrum, the fragment is classified as high coverage. Otherwise, the fragment belongs to a low-coverage area. ParLECH uses a pattern of true and error k-mers to localize the errors and searches for the set of corrections with a maximum likelihood that make all k-mers true.</p>
    </sec>
    <sec id="Sec12">
      <title>Correcting the substitution error</title>
      <p>To rectify the substitution errors, ParLECH uses a majority voting algorithm similar to that of Quake [<xref ref-type="bibr" rid="CR4">4</xref>]. However, we have two major differences. First, ParLECH’s majority voting algorithm is fully distributed and can scale over hundreds of nodes. Second, unlike Quake, ParLECH uses different thresholds for the low and high coverage area of the genome to improve the accuracy. For each error base detected in the previous phase, ParLECH substitutes the base with all the different nucleotide characters (i.e., <italic>A</italic>, <italic>T</italic>, <italic>G</italic>, and <italic>C</italic>) and calculates the coverage of all the <italic>k</italic>-mers with that base. Finally, the error base is replaced with the one such that all those <italic>k</italic>-mers with that base exceeds or equals the specified threshold for that area.</p>
    </sec>
  </sec>
  <sec id="Sec13" sec-type="results">
    <title>Results</title>
    <p>In this section, we show the experimental results of ParLECH using various real-world sequence datasets.</p>
    <sec id="Sec14">
      <title>Datasets</title>
      <p>We evaluate ParLECH with respect to four real data sets including <italic>E. coli</italic>, yeast, fruit fly, and human genome. The details of the data set are summarized in Table <xref rid="Tab1" ref-type="table">1</xref>. The first three of them are relatively small-sized genomes. We use them to compare the accuracy of ParLECH with the existing hybrid error correction tools such as LoRDEC, Jabba, and Proovread. These data sets are also used to analyze the scalability and compare other resource consumption statistics such as memory requirement and CPU-Hour.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Data</th><th align="left" colspan="2">Accn. #</th><th align="left" colspan="2">#Reads</th><th align="left" colspan="2">Data size (GB)</th><th align="left" colspan="2">Read length</th><th align="left" colspan="2">%Reads aligned</th></tr><tr><th align="left"/><th align="left">PacBio</th><th align="left">Illumina</th><th align="left">PacBio</th><th align="left">Illumina</th><th align="left">PacBio</th><th align="left">Illumina</th><th align="left">PacBio (Avg)</th><th align="left">Illumina</th><th align="left">PacBio</th><th align="left">Illumina</th></tr></thead><tbody><tr><td align="left">E. coli</td><td align="left">DevNet</td><td align="left">ERR022075</td><td align="left">282394</td><td align="left">45440200</td><td align="left">1.032</td><td align="left">13.50</td><td align="left">1120</td><td align="left">101</td><td align="left">78.97</td><td align="left">99.44</td></tr><tr><td align="left">Yeast</td><td align="left">DevNet</td><td align="left">SRR567755</td><td align="left">2315594</td><td align="left">4503422</td><td align="left">0.53</td><td align="left">1.20</td><td align="left">5874</td><td align="left">101</td><td align="left">82.12</td><td align="left">93.75</td></tr><tr><td align="left">Fruit fly</td><td align="left">BergmanLab</td><td align="left">ERX645969</td><td align="left">6701498</td><td align="left">179363706</td><td align="left">55</td><td align="left">59</td><td align="left">4328</td><td align="left">101</td><td align="left">51.14</td><td align="left">95.56</td></tr><tr><td align="left">Human</td><td align="left">DevNet</td><td align="left">SRX016231</td><td align="left">23897260</td><td align="left">1420689270</td><td align="left">312</td><td align="left">452</td><td align="left">6587</td><td align="left">101</td><td align="left">72.3</td><td align="left">79.60</td></tr></tbody></table></table-wrap></p>
      <p>The fourth one is the largest among all. It is a large human genome data set that consists of almost 764 GB of sequencing reads including both Illumina and PacBio sequences. We use it to showcase the scaling capability of ParLECH with hundreds of GBs of sequencing reads over hundreds of compute nodes. In our experiments, other existing tools could not produce the result for the data set.</p>
    </sec>
    <sec id="Sec15">
      <title>Computing environment</title>
      <p>To evaluate ParLECH, we use <italic>SuperMic</italic> [<xref ref-type="bibr" rid="CR30">30</xref>] HPC cluster, and Table <xref rid="Tab2" ref-type="table">2</xref> summarizes its configuration. The maximum number of compute nodes we can use for a single job is 128. Each node has 20 cores, 64 GB main memory, and one 250 GB hard disk drive (HDD). Note that the main bottleneck for our Hadoop jobs running on top of disk-based HDFS is the I/O throughput because each node is equipped with only one HDD. We expect that the performance of ParLECH can be significantly improved by using multiple HDDs per node and/or SSD. Our previous work [<xref ref-type="bibr" rid="CR31">31</xref>–<xref ref-type="bibr" rid="CR33">33</xref>] demonstrates the effects of various computing environments for large-scale data processing.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Experimental environment</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Maximum #nodes</th><th align="left">128</th></tr></thead><tbody><tr><td align="left">Processor</td><td align="left">Intel IvyBridge Xeon</td></tr><tr><td align="left">#cores per node</td><td align="left">20</td></tr><tr><td align="left">DRAM per node</td><td align="left">64 GB</td></tr><tr><td align="left">Disk per node</td><td align="left">250 GB hard disk drive</td></tr><tr><td align="left">Network</td><td align="left">56 Gbps InfiniBand</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec16">
      <title>Accuracy metrics</title>
      <p>We evaluate the accuracy of ParLECH with respect to three different metrics as follows: 1) <italic>% Aligned reads</italic> and 2) <italic>% Aligned bases</italic>: These accuracy metrics indicate how well the corrected long reads are aligned to the reference genome. We report the %alignment both in terms of the total number of reads as well as the total bases present in the data set. For all the data sets other than the human genome, we use BLASR [<xref ref-type="bibr" rid="CR34">34</xref>] to align the long reads to the reference genome as it reports longer alignments by bridging the long indel error. However, for the large human genome, we use BWA-mem [<xref ref-type="bibr" rid="CR35">35</xref>] to get the alignment results quickly.</p>
      <p>2) <italic>N50 statistics:</italic> It is also important to preserve input read depth in the corrected data set. Shorter reads and/or reduced depth may show better alignment but may have a negative impact on downstream analyses. Hence, we measure the N50 statistics of the data sets to indicate the discard or trimming of errors in the long reads instead of rectifying them.</p>
      <p>3) <italic>Gain:</italic> We also use the <bold>gain</bold> metric [<xref ref-type="bibr" rid="CR5">5</xref>] to measure the fraction of effectively corrected errors by ParLECH. The gain is defined as
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ Gain = \frac{TP-FP}{TP+FN}  $$ \end{document}</tex-math><mml:math id="M4"><mml:mtext mathvariant="italic">Gain</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12864_2019_6286_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <italic>TP</italic> (true-positive) is the number of error bases that are successfully corrected, <italic>FP</italic> (false-positive) is the number of true bases that are wrongly changed, and <italic>FN</italic> (false-negative) is the number of error bases that are falsely detected as correct.</p>
      <p>To measure <italic>TP</italic>, <italic>FP</italic>, and <italic>FN</italic>, we follow the procedure described in [<xref ref-type="bibr" rid="CR36">36</xref>]. Let <italic>r</italic> be an original read and <italic>r</italic><sub><italic>c</italic></sub> be the read after correction. We derive the set of real sequencing errors <italic>E</italic><sub><italic>m</italic></sub> by mapping <italic>r</italic> to the reference genome and recording differences. Then, we measure <italic>E</italic><sub><italic>r</italic></sub>, the set of errors remaining in <italic>r</italic><sub><italic>c</italic></sub>, by applying global alignment between <italic>r</italic><sub><italic>c</italic></sub> and the genomic region where <italic>r</italic> was mapped to and recording the differences in the alignment. Finally, we calculate <italic>T</italic><italic>P</italic>=|<italic>E</italic><sub><italic>m</italic></sub>∖<italic>E</italic><sub><italic>r</italic></sub>|, <italic>F</italic><italic>P</italic>=|<italic>E</italic><sub><italic>r</italic></sub>∖<italic>E</italic><sub><italic>m</italic></sub>|, and <italic>F</italic><italic>N</italic>=|<italic>E</italic><italic>r</italic>∩<italic>E</italic><italic>m</italic>|.</p>
    </sec>
    <sec id="Sec17">
      <title>Comparison with existing tools</title>
      <p>Table <xref rid="Tab3" ref-type="table">3</xref> compares the accuracy of ParLECH with that of LoRDEC, Jabba, and Proovread in terms of the percentage of aligned reads and aligned bases. Table <xref rid="Tab4" ref-type="table">4</xref>, on the other hand, compares the accuracy in terms of gain. We measure the accuracy metrics using BLASR by running multiple instances of BLASR in parallel for efficiently processing large datasets.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Accuracy comparison (Alignments)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Data</th><th align="left">Methodology</th><th align="left">#Reads</th><th align="left">#Bases</th><th align="left">N50</th><th align="left">#Aligned Reads</th><th align="left">#Aligned bases</th><th align="left">%Aligned reads</th><th align="left">%Aligned bases</th></tr></thead><tbody><tr><td align="left">E. coli</td><td align="left">Original</td><td align="left">282394</td><td align="left">316367409</td><td align="left">3414</td><td align="left">223017</td><td align="left">237497013</td><td align="left">78.97</td><td align="left">75.07</td></tr><tr><td align="left"/><td align="left">LoRDEC</td><td align="left">282394</td><td align="left">307987923</td><td align="left">3422</td><td align="left">247227</td><td align="left">266373078</td><td align="left">87.55</td><td align="left">86.49</td></tr><tr><td align="left"/><td align="left">Jabba</td><td align="left">149836</td><td align="left">149322524</td><td align="left">2517</td><td align="left">148293</td><td align="left">141563938</td><td align="left">98.97</td><td align="left">94.80</td></tr><tr><td align="left"/><td align="left">Proovread</td><td align="left">263206</td><td align="left">284871906</td><td align="left">1222</td><td align="left">241948</td><td align="left">246138387</td><td align="left">91.92</td><td align="left">86.40</td></tr><tr><td align="left"/><td align="left">ParLECH (Indel)</td><td align="left">282394</td><td align="left">309367145</td><td align="left">3394</td><td align="left">264574</td><td align="left">285070391</td><td align="left">93.69</td><td align="left">92.15</td></tr><tr><td align="left"/><td align="left">ParLECH (Indel+Subst)</td><td align="left">282394</td><td align="left">309367145</td><td align="left">3394</td><td align="left">264720</td><td align="left">295438268</td><td align="left"><bold>93.74</bold></td><td align="left"><bold>95.50</bold></td></tr><tr><td align="left">Yeast</td><td align="left">Original</td><td align="left">231594</td><td align="left">1360457697</td><td align="left">2990</td><td align="left">190184</td><td align="left">1206524663</td><td align="left">82.12</td><td align="left">88.69</td></tr><tr><td align="left"/><td align="left">LoRDEC</td><td align="left">231594</td><td align="left">1345253694</td><td align="left">2982</td><td align="left">196669</td><td align="left">1171490123</td><td align="left">84.92</td><td align="left">87.08</td></tr><tr><td align="left"/><td align="left">Jabba</td><td align="left">152882</td><td align="left">634947441</td><td align="left">2173</td><td align="left">151359</td><td align="left">634732955</td><td align="left">99.02</td><td align="left">99.09</td></tr><tr><td align="left"/><td align="left">Proovread</td><td align="left">225032</td><td align="left">1307137185</td><td align="left">1693</td><td align="left">211323</td><td align="left">1100350212</td><td align="left">93.90</td><td align="left">84.18</td></tr><tr><td align="left"/><td align="left">ParLECH (Indel)</td><td align="left">231594</td><td align="left">1389446261</td><td align="left">2994</td><td align="left">199332</td><td align="left">1240945939</td><td align="left">86.07</td><td align="left">89.31</td></tr><tr><td align="left"/><td align="left">ParLECH (Indel+Subst)</td><td align="left">231594</td><td align="left">1389446261</td><td align="left">2994</td><td align="left">201857</td><td align="left">1254987596</td><td align="left"><bold>87.16</bold></td><td align="left"><bold>90.32</bold></td></tr><tr><td align="left">Fruit fly</td><td align="left">Original</td><td align="left">6701498</td><td align="left">29007475325</td><td align="left">15154</td><td align="left">3427146</td><td align="left">13355041639</td><td align="left">51.14</td><td align="left">46.04</td></tr><tr><td align="left"/><td align="left">LoRDEC</td><td align="left">6701498</td><td align="left">30025673204</td><td align="left">15154</td><td align="left">3654326</td><td align="left">14919815143</td><td align="left">54.53</td><td align="left">49.69</td></tr><tr><td align="left"/><td align="left">Jabba</td><td align="left">4423855</td><td align="left">10820828565</td><td align="left">14302</td><td align="left">3921032</td><td align="left">9455816742</td><td align="left">88.63</td><td align="left">87.38</td></tr><tr><td align="left"/><td align="left">Proovread</td><td align="left">6511617</td><td align="left">20174923756</td><td align="left">8603</td><td align="left">5450784</td><td align="left">14497076095</td><td align="left">83.70</td><td align="left">71.86</td></tr><tr><td align="left"/><td align="left">ParLECH (Indel)</td><td align="left">6701498</td><td align="left">30117416348</td><td align="left">15154</td><td align="left">4417627</td><td align="left">18799138439</td><td align="left">65.92</td><td align="left">62.42</td></tr><tr><td align="left"/><td align="left">ParLECH (Indel+Subst)</td><td align="left">6701498</td><td align="left">30117416348</td><td align="left">15154</td><td align="left">4557627</td><td align="left">19983756932</td><td align="left"><bold>68.01</bold></td><td align="left"><bold>66.35</bold></td></tr></tbody></table><table-wrap-foot><p>The best results are shown in bold faces</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Accuracy comparison (Gain)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"/><th align="left">TP</th><th align="left">FP</th><th align="left">FN</th><th align="left">%Gain</th></tr></thead><tbody><tr><td align="left">E. coli</td><td align="left">LoRDEC</td><td align="left">31264830</td><td align="left">330659</td><td align="left">4230385</td><td align="left">87.15</td></tr><tr><td align="left"/><td align="left">Jabba</td><td align="left">10386868</td><td align="left">105445</td><td align="left">244608</td><td align="left">96.7</td></tr><tr><td align="left"/><td align="left">Proovread</td><td align="left">23541209</td><td align="left">318191</td><td align="left">3942940</td><td align="left">84.49</td></tr><tr><td align="left"/><td align="left">ParLECH (Indel)</td><td align="left">33229635</td><td align="left">355464</td><td align="left">3275190</td><td align="left">90.05</td></tr><tr><td align="left"/><td align="left">ParLECH (Indel+Subst)</td><td align="left">34521649</td><td align="left">250129</td><td align="left">2088511</td><td align="left"><bold>93.61</bold></td></tr><tr><td align="left">Yeast</td><td align="left">LoRDEC</td><td align="left">322660270</td><td align="left">8989628</td><td align="left">62594234</td><td align="left">81.42</td></tr><tr><td align="left"/><td align="left">Jabba</td><td align="left">171200961</td><td align="left">3004132</td><td align="left">9543906</td><td align="left">93.06</td></tr><tr><td align="left"/><td align="left">Proovread</td><td align="left">313517992</td><td align="left">8734915</td><td align="left">60820684</td><td align="left">83.21</td></tr><tr><td align="left"/><td align="left">ParLECH (Indel)</td><td align="left">355708411</td><td align="left">20037769</td><td align="left">51642375</td><td align="left">82.40</td></tr><tr><td align="left"/><td align="left">ParLECH (Indel+Subst)</td><td align="left">368206322</td><td align="left">19556218</td><td align="left">39626015</td><td align="left"><bold>85.49</bold></td></tr><tr><td align="left">Fruit fly</td><td align="left">LoRDEC</td><td align="left">732799376</td><td align="left">34190591</td><td align="left">84891209</td><td align="left">85.43</td></tr><tr><td align="left"/><td align="left">Jabba</td><td align="left">188817493</td><td align="left">18141254</td><td align="left">45042597</td><td align="left">93.2</td></tr><tr><td align="left"/><td align="left">Proovread</td><td align="left">613007402</td><td align="left">30867421</td><td align="left">72123053</td><td align="left">84.96</td></tr><tr><td align="left"/><td align="left">ParLECH (Indel)</td><td align="left">785735162</td><td align="left">37126377</td><td align="left">97826995</td><td align="left">84.73</td></tr><tr><td align="left"/><td align="left">ParLECH (Indel+Subst)</td><td align="left">799834035</td><td align="left">34065158</td><td align="left">86789341</td><td align="left"><bold>86.37</bold></td></tr></tbody></table><table-wrap-foot><p>The best results are shown in bold faces</p></table-wrap-foot></table-wrap></p>
      <p>The results demonstrate that ParLECH can rectify the indel errors with significantly more accuracy comparing to LoRDEC both in terms of the aligned bases and gain. Like LoRDEC, ParLECH does not correct the long reads in which there is no strong <italic>k</italic>-mer. However, ParLECH searches strong <italic>k</italic>-mers in all reads regardless of their length while LoRDEC filters out reads whose length is less than a threshold.</p>
      <p>Although Jabba attains significantly higher alignment accuracy compared to ParLECH, this high alignment accuracy is attained at the cost of producing reduced depths. This is because, unlike ParLECH, Jabba chooses to discard several of the uncorrected reads instead of rectifying them. As shown in Table <xref rid="Tab3" ref-type="table">3</xref>, the total number of reads in the resulting error-corrected dataset is significantly higher in ParLECH comparing to Jabba.</p>
      <p>Proovread attains almost similar alignment accuracy comparing to ParLECH. However, it trims many of the error regions in each read and breaks an erroneous longer read at the error region, producing multiple shorter reads. Consequently, Proovread produces significantly lower N50 compared to ParLECH.</p>
      <p>We have further improved the accuracy by correcting the substitution errors of the long reads. This phase is not present in LoRDEC. However, it has a substantial impact on improving the quality of the data. As shown in Tables <xref rid="Tab3" ref-type="table">3</xref> and <xref rid="Tab4" ref-type="table">4</xref>, by correcting the substitution errors, ParLECH improve the quality of the dataset by 1 to 3% from the indel error-corrected output both in terms of alignment and gain.</p>
    </sec>
    <sec id="Sec18">
      <title>Scalability</title>
      <p>Figure <xref rid="Fig7" ref-type="fig">7</xref> demonstrates the scalability of different phases of ParLECH. Figure <xref rid="Fig7" ref-type="fig">7</xref>a demonstrates the scalability of each phase of ParLECH’s indel error correction pipeline for the fruit fly dataset. The results show that the processing time of all three phases (i.e., constructing a de Bruijn graph, locating errors in long reads, and correcting errors in long reads) improves almost linearly with the increasing number of compute nodes. Therefore, the overall execution time of ParLECH also shows the almost linear scalability as we add more compute nodes.
<fig id="Fig7"><label>Fig. 7</label><caption><p>Scalability of ParLECH. <bold>a</bold> Time to correct indel error of fruit fly dataset. <bold>b</bold> Time to correct subst. error of fruit fly dataset</p></caption><graphic xlink:href="12864_2019_6286_Fig7_HTML" id="MO7"/></fig></p>
      <p>Figure <xref rid="Fig7" ref-type="fig">7</xref>b demonstrates the scalability of different phases of ParLECH’s substitution error correction pipeline for the same fruit fly dataset. Like the indel error correction phases, these phases are also linearly scalable with the increasing number of nodes.</p>
      <p>Figure <xref rid="Fig8" ref-type="fig">8</xref> compares ParLECH with existing error correction tools. As shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>a, on a single node for the same <italic>E. coli</italic> data, ParLECH performs almost 1.5 times faster than Jabba and almost 7.5 times faster than Proovread. On a single node, LoRDEC shows slightly better (1.2 times faster) performance than ParLECH because both the tools have similar asymptotic complexity (<italic>O</italic>(<italic>E</italic> log<italic>v</italic>)) whereas ParLECH has some distributed computing overhead. However, utilizing the power of Hadoop and Hazelcast, the embarrassingly parallel algorithm of ParLECH can be easily distributed over multiple nodes and eventually outperform LoRDEC by several magnitudes, which is not designed for distributed computing. Even though the correction algorithm of LoRDEC can work independently on each of the long reads, the computation cannot be distributed because of the absence of a proper scheduler.
<fig id="Fig8"><label>Fig. 8</label><caption><p>Comparing execution time of ParLECH with existing error correction tools. <bold>a</bold> Time for hybrid correction of indel errors in <italic>E.coli</italic> long reads (1.032 GB). <bold>b</bold> Time for correction of substitution errors in <italic>E.coli</italic> short reads (13.50 GB)</p></caption><graphic xlink:href="12864_2019_6286_Fig8_HTML" id="MO8"/></fig></p>
      <p>Figure <xref rid="Fig8" ref-type="fig">8</xref>b compares the substitution error correction pipeline with Quake [<xref ref-type="bibr" rid="CR4">4</xref>], an existing tool to correct the substitution errors of Illumina short read sequences. For the similar reason mentioned above, ParLECH outperforms Quake by several magnitudes when distributed over multiple nodes. For a fair comparison with Quake, we use the <italic>E. coli</italic> Illumina dataset only for this experiment. Since the major motivation of ParLECH is to correct the long-read errors, we did not report the results of accuracy comparison between ParLECH and Quake in this paper.</p>
    </sec>
  </sec>
  <sec id="Sec19" sec-type="discussion">
    <title>Discussion</title>
    <sec id="Sec20">
      <title>Effects of different traversal algorithms on indel error correction</title>
      <p>To better understand the benefit of our widest path algorithm (ParLECH <sub><italic>WP</italic></sub>), we compare its accuracy with that of two other graph traversal algorithms, which are popular in this domain. The first one is the Dijkstra’s shortest path algorithm (ParLECH <sub><italic>SP</italic></sub>), and the other one is a greedy traversal algorithm (ParLECH <sub><italic>Greedy</italic></sub>). Table <xref rid="Tab5" ref-type="table">5</xref> reports the accuracy results of all the three algorithms over the real PacBio data sets.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Effects of different traversal algorithms</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Data</th><th align="left">Methodology</th><th align="left">#Reads</th><th align="left">#Bases</th><th align="left">#Aligned Reads</th><th align="left">#Aligned bases</th><th align="left">%Aligned reads</th><th align="left">%Aligned bases</th></tr></thead><tbody><tr><td align="left">E. coli</td><td align="left">ParLECH <sub><italic>WP</italic></sub></td><td align="left">282394</td><td align="left">309367145</td><td align="left">264574</td><td align="left">285070391</td><td align="left">93.69</td><td align="left">92.15</td></tr><tr><td align="left"/><td align="left">ParLECH <sub><italic>SP</italic></sub></td><td align="left">282394</td><td align="left">307987923</td><td align="left">247227</td><td align="left">266373078</td><td align="left">87.55</td><td align="left">86.49</td></tr><tr><td align="left"/><td align="left">ParLECH <sub><italic>Greedy</italic></sub></td><td align="left">282394</td><td align="left">328966341</td><td align="left">216543</td><td align="left">233312807</td><td align="left">76.68</td><td align="left">70.92</td></tr><tr><td align="left">Yeast</td><td align="left">ParLECH <sub><italic>WP</italic></sub></td><td align="left">231594</td><td align="left">1389446261</td><td align="left">199332</td><td align="left">1240945939</td><td align="left">86.07</td><td align="left">89.31</td></tr><tr><td align="left"/><td align="left">ParLECH <sub><italic>SP</italic></sub></td><td align="left">231594</td><td align="left">1355153783</td><td align="left">196669</td><td align="left">1171490123</td><td align="left">84.92</td><td align="left">86.44</td></tr><tr><td align="left"/><td align="left">ParLECH <sub><italic>Greedy</italic></sub></td><td align="left">231594</td><td align="left">1399628927</td><td align="left">175478</td><td align="left">1045262567</td><td align="left">75.77</td><td align="left">74.68</td></tr><tr><td align="left">Fruit fly</td><td align="left">ParLECH <sub><italic>WP</italic></sub></td><td align="left">6701498</td><td align="left">30117416348</td><td align="left">4417627</td><td align="left">18799138439</td><td align="left">65.92</td><td align="left">62.42</td></tr><tr><td align="left"/><td align="left">ParLECH <sub><italic>SP</italic></sub></td><td align="left">6701498</td><td align="left">30193752318</td><td align="left">3654326</td><td align="left">14919815143</td><td align="left">54.53</td><td align="left">49.41</td></tr><tr><td align="left"/><td align="left">ParLECH <sub><italic>Greedy</italic></sub></td><td align="left">6701498</td><td align="left">32131749687</td><td align="left">2946734</td><td align="left">12030871508</td><td align="left">43.97</td><td align="left">37.44</td></tr></tbody></table></table-wrap></p>
      <p>ParLECH <sub><italic>SP</italic></sub> replaces the weak region in the long read with the sequence corresponding to the shortest path in the DBG. ParLECH <sub><italic>Greedy</italic></sub> always selects the vertex with the maximum coverage among all neighboring vertices during its traversal. For ParLECH <sub><italic>Greedy</italic></sub>, the traversal often ends up in a tip of a dead-end path. So, we use a branching factor <italic>b</italic> (100 by default) such that, after traversing <italic>b</italic> successive vertices from the source vertex, the algorithm backtracks if it cannot meet the destination vertex. The algorithm aborts when all successors from the source vertex are visited using this branching factor.</p>
      <p>Although ParLECH <sub><italic>SP</italic></sub> has the similar performance as ParLECH <sub><italic>WP</italic></sub>, because of the counter intuitive nature of shortest paths and the strong (high coverage) <italic>k</italic>-mers desired for the correction, it cannot take the advantage of the <italic>k</italic>-mer coverage information in a straight forward way, adversely impacting the accuracy. ParLECH <sub><italic>Greedy</italic></sub>, on the other hand, can take the advantage of the <italic>k</italic>-mer coverage information, but its accuracy depends highly on the higher value of the branching factor that poses a severe limitation on its performance.</p>
      <p>Our widest path algorithm not only optimizes the performance but also makes better use of <italic>k</italic>-mer coverage information. The algorithm maximizes the minimum coverage of the <italic>k</italic>-mer in a path. Compared to both ParLECH <sub><italic>SP</italic></sub> and ParLECH <sub><italic>Greedy</italic></sub>, ParLECH <sub><italic>WP</italic></sub> better balances the coverage of all the <italic>k</italic>-mers in a particular path of the DBG, which improves the accuracy of the resultant data set.</p>
      <p>As shown in Table <xref rid="Tab5" ref-type="table">5</xref>, the widest path shows almost 15 to 25% better alignment accuracy compared to the greedy algorithm, which is found to perform worst among all. Comparing to the shortest path algorithm, the widest path shows almost 6 to 13% improvement for the dataset.</p>
    </sec>
    <sec id="Sec21">
      <title>Resource consumption statistics</title>
      <p>Using the power of Hadoop and Hazelcast, ParLECH is capable to tradeoff between CPU-Hour and DRAM utilization. That is, based on the data size and the available resources, ParLECH can be tuned to utilize the disk space at the cost of higher execution time.</p>
      <p>Table <xref rid="Tab6" ref-type="table">6</xref> compares the CPU-Hour and DRAM resource consumption of ParLECH with existing error correction tools with respect to the <italic>E. coli</italic> data set. For the best (lowest) execution time, ParLECH consumes almost similar CPU-Hour as LoRDEC, which is significantly less comparing to Jabba and Proovread. For this performance, ParLECH needs the entire <italic>k</italic>-mer spectrum in DRAM. Consequently, it utilizes almost 32GB of DRAM. However, ParLECH can process the same E. coli data consuming significantly less amount (only 5GB) of DRAM if configured properly. However, the process takes more time to finish because of context switching between the DRAM and the hard disk.
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Comparing resource consumption of ParLECH with existing error correction tools with respect to E. coli dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Error correction tool</th><th align="left">CPU-Hour (single node)</th><th align="left">Peak memory usage</th></tr></thead><tbody><tr><td align="left">LoRDEC</td><td align="justify">10</td><td align="justify">20.65</td></tr><tr><td align="left">Jabba</td><td align="justify">18</td><td align="justify">11.16</td></tr><tr><td align="left">Proovread</td><td align="justify">89</td><td align="justify">31.77</td></tr><tr><td align="left">ParLECH (configured for least execution time)</td><td align="justify">11.67</td><td align="justify">23.80</td></tr><tr><td align="left">ParLECH (configured to use lower DRAM)</td><td align="justify">29.37</td><td align="justify">5</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec22">
      <title>Processing large-scale human genomes</title>
      <p>To showcase the data handling capability of ParLECH with hundreds of GBs of sequencing data and its scaling capability with hundreds of computing nodes, we analyze a large human genome data set. This 312 GB of PacBio data set includes more than 23 million long reads with the average length of 6,587 base pairs. The corresponding Illumina data set is 452 GB in size and contains more than 1.4 billion reads with the read length of 101 base pairs. To analyze this large data set (764 GB cumulative), we use 128 nodes of SuperMic cluster. We tuned ParLECH for the maximum performance. That means we distributed the entire de Bruijn graph in the memory available across the cluster.</p>
      <p>The indel error correction process takes about 28.6 h as shown in Table <xref rid="Tab7" ref-type="table">7</xref>. After this indel error correction, 78.3% of reads and 75.4% of bases are successfully aligned to the reference genome. The substitution error correction process took another 26.5 h, successfully aligning 79.73% of the reads and 80.24% of the bases to the reference genome.
<table-wrap id="Tab7"><label>Table 7</label><caption><p>Correcting a human genome</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left">PacBio data size</td><td align="left">312GB</td></tr><tr><td align="left">Illumina data size</td><td align="left">452GB</td></tr><tr><td align="left">#nodes used</td><td align="left">128</td></tr><tr><td align="left">Time</td><td align="left">28.6 h</td></tr><tr><td align="left">%Aligned reads (Indel)</td><td align="left">78.3</td></tr><tr><td align="left">%Aligned bases (Indel)</td><td align="left">75.43</td></tr><tr><td align="left">%Gain (Indel)</td><td align="left">82.38</td></tr><tr><td align="left">Time (Indel + Subst)</td><td align="left">3.4 h</td></tr><tr><td align="left">%Aligned reads (Indel + Subst)</td><td align="left">79.73</td></tr><tr><td align="left">%Aligned bases (Indel + Subst)</td><td align="left">80.24</td></tr><tr><td align="left">%Gain (Indel + Subst)</td><td align="left">84.51</td></tr></tbody></table></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec23" sec-type="conclusion">
    <title>Conclusion</title>
    <p>In this paper, we present a distributed hybrid error correction framework for PacBio long reads, called ParLECH. For efficient and scalable analysis of large-scale sequence data, ParLECH makes use of Hadoop and Hazelcast. ParLECH uses the de Bruijn graph and <italic>k</italic>-mer coverage information from the short reads to rectify the errors of the long reads. We develop a distributed version of the widest path algorithm to maximize the minimum <italic>k</italic>-mer coverage in a path of the de Bruijn graph constructed from the Illumina short reads. We replace the indel error regions in a long read with their corresponding widest path. To improve the substitution accuracy, we develop a median statistics-based strategy that considers relative <italic>k</italic>-mer abundance in a specific area of a genome to take care of high- and low-coverage areas separately. Our experimental results show that ParLECH can scale with hundreds of compute nodes and can improve the quality of large-scale sequencing data sets in an accurate manner. While correcting the errors, ParLECH takes care of high- and low-coverage regions of the sequencing reads separately and is better capable to balance the <italic>k</italic>-mer coverage based on the neighborhood. Hence, we believe that it is a good starting point for detecting and correcting errors in RNA and metagenome sequences.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec24">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12864_2019_6286_MOESM1_ESM.pdf">
            <caption>
              <p><bold>Additional file 1</bold> This file provides a brief of the theoretical rationale for using widest path algorithm (claim 1), and a theoretical justification for why median statistics has lower dependency on the value of <italic>k</italic>.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>CCT</term>
        <def>
          <p>Center for computation and technology</p>
        </def>
      </def-item>
      <def-item>
        <term>DBG</term>
        <def>
          <p>De bruijn graph</p>
        </def>
      </def-item>
      <def-item>
        <term>DNA</term>
        <def>
          <p>Deoxyribonucleic acid</p>
        </def>
      </def-item>
      <def-item>
        <term>DRAM</term>
        <def>
          <p>Dynamic random access memory</p>
        </def>
      </def-item>
      <def-item>
        <term>GB</term>
        <def>
          <p>Giga bytes</p>
        </def>
      </def-item>
      <def-item>
        <term>HDD</term>
        <def>
          <p>Hard disk drive</p>
        </def>
      </def-item>
      <def-item>
        <term>HDFS</term>
        <def>
          <p>Hadoop distributed file system</p>
        </def>
      </def-item>
      <def-item>
        <term>HPC</term>
        <def>
          <p>High performance computing</p>
        </def>
      </def-item>
      <def-item>
        <term>LSU</term>
        <def>
          <p>Louisiana State University</p>
        </def>
      </def-item>
      <def-item>
        <term>NoSQL</term>
        <def>
          <p>Not only SQL</p>
        </def>
      </def-item>
      <def-item>
        <term>ParLECH</term>
        <def>
          <p>Parallel long-read error correction using hybrid methodology</p>
        </def>
      </def-item>
      <def-item>
        <term>RNA</term>
        <def>
          <p>Ribonucleic acid</p>
        </def>
      </def-item>
      <def-item>
        <term>SSD</term>
        <def>
          <p>Solid state drive</p>
        </def>
      </def-item>
      <def-item>
        <term>UW</term>
        <def>
          <p>University of Wisconsin</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> accompanies this paper at 10.1186/s12864-019-6286-9.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>We would like to thank the Information Technology and Service (ITS) department of both UW Platteville and LSU for providing the testing infrastructure required in different phases of the project.</p>
    <sec id="d29e3118">
      <title>About this supplement</title>
      <p>This article has been published as part of <italic>BMC Genomics Volume 20 Supplement 11, 2019: Selected articles from the IEEE BIBM International Conference on Bioinformatics &amp; Biomedicine (BIBM) 2018: genomics</italic>. The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="https://bmcgenomics.biomedcentral.com/articles/supplements/volume-20-supplement-11">https://bmcgenomics.biomedcentral.com/articles/supplements/volume-20-supplement-11</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>AKD and KL developed the algorithms of long read error correction. SG and SJP evaluated and tested the tool. All the authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Publication costs were funded by NSF grants (MRI-1338051, IBSS-L-1620451, SCC-1737557, RAPID-1762600), NIH grants (P20GM103458-10, P30GM110760-03, P20GM103424), LA Board of Regents grants (LEQSF(2016-19)-RD-A-08 and ITRS), and IBM faculty awards.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The source code for ParLECH is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/arghyakusumdas/GenomicErrorCorrection">https://github.com/arghyakusumdas/GenomicErrorCorrection</ext-link>.</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goodwin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>McPherson</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>McCombie</surname>
            <given-names>WR</given-names>
          </name>
        </person-group>
        <article-title>Coming of age: ten years of next-generation sequencing technologies</article-title>
        <source>Nat Rev Genet</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>6</issue>
        <fpage>333</fpage>
        <lpage>51</lpage>
        <pub-id pub-id-type="doi">10.1038/nrg.2016.49</pub-id>
        <pub-id pub-id-type="pmid">27184599</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <mixed-citation publication-type="other">Das AK, Lee K, Park S-J. Parlech: Parallel long-read error correction with hadoop. In: 2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE: 2018. p. 341–8. 10.1109/bibm.2018.8621549.</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lou</surname>
            <given-names>D. I.</given-names>
          </name>
          <name>
            <surname>Hussmann</surname>
            <given-names>J. A.</given-names>
          </name>
          <name>
            <surname>McBee</surname>
            <given-names>R. M.</given-names>
          </name>
          <name>
            <surname>Acevedo</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Andino</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Press</surname>
            <given-names>W. H.</given-names>
          </name>
          <name>
            <surname>Sawyer</surname>
            <given-names>S. L.</given-names>
          </name>
        </person-group>
        <article-title>High-throughput DNA sequencing errors are reduced by orders of magnitude using circle sequencing</article-title>
        <source>Proceedings of the National Academy of Sciences</source>
        <year>2013</year>
        <volume>110</volume>
        <issue>49</issue>
        <fpage>19872</fpage>
        <lpage>19877</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1319590110</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kelley</surname>
            <given-names>David R</given-names>
          </name>
          <name>
            <surname>Schatz</surname>
            <given-names>Michael C</given-names>
          </name>
          <name>
            <surname>Salzberg</surname>
            <given-names>Steven L</given-names>
          </name>
        </person-group>
        <article-title>Quake: quality-aware detection and correction of sequencing errors</article-title>
        <source>Genome Biology</source>
        <year>2010</year>
        <volume>11</volume>
        <issue>11</issue>
        <fpage>R116</fpage>
        <pub-id pub-id-type="doi">10.1186/gb-2010-11-11-r116</pub-id>
        <pub-id pub-id-type="pmid">21114842</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Dorman</surname>
            <given-names>K. S.</given-names>
          </name>
          <name>
            <surname>Aluru</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Reptile: representative tiling for short read error correction</article-title>
        <source>Bioinformatics</source>
        <year>2010</year>
        <volume>26</volume>
        <issue>20</issue>
        <fpage>2526</fpage>
        <lpage>2533</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btq468</pub-id>
        <pub-id pub-id-type="pmid">20834037</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Medvedev</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Scott</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Kakaradov</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Pevzner</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Error correction of high-throughput sequencing datasets with non-uniform coverage</article-title>
        <source>Bioinformatics</source>
        <year>2011</year>
        <volume>27</volume>
        <issue>13</issue>
        <fpage>i137</fpage>
        <lpage>i141</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btr208</pub-id>
        <pub-id pub-id-type="pmid">21685062</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ilie</surname>
            <given-names>Lucian</given-names>
          </name>
          <name>
            <surname>Molnar</surname>
            <given-names>Michael</given-names>
          </name>
        </person-group>
        <article-title>RACER: Rapid and accurate correction of errors in reads</article-title>
        <source>Bioinformatics</source>
        <year>2013</year>
        <volume>29</volume>
        <issue>19</issue>
        <fpage>2490</fpage>
        <lpage>2493</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btt407</pub-id>
        <pub-id pub-id-type="pmid">23853064</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Salmela</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Schroder</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Correcting errors in short reads by multiple alignments</article-title>
        <source>Bioinformatics</source>
        <year>2011</year>
        <volume>27</volume>
        <issue>11</issue>
        <fpage>1455</fpage>
        <lpage>1461</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btr170</pub-id>
        <pub-id pub-id-type="pmid">21471014</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <mixed-citation publication-type="other">Song L, Florea L, Langmead B. Lighter: fast and memory-efficient sequencing error correction without counting. Genome Biol. 2014; 15(11). 10.1186/s13059-014-0509-9.</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Yongchao</given-names>
          </name>
          <name>
            <surname>Schröder</surname>
            <given-names>Jan</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>Bertil</given-names>
          </name>
        </person-group>
        <article-title>Musket: a multistage k-mer spectrum-based error corrector for Illumina sequence data</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>29</volume>
        <issue>3</issue>
        <fpage>308</fpage>
        <lpage>315</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts690</pub-id>
        <pub-id pub-id-type="pmid">23202746</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schroder</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Schroder</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Puglisi</surname>
            <given-names>S. J.</given-names>
          </name>
          <name>
            <surname>Sinha</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>SHREC: a short-read error correction method</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <issue>17</issue>
        <fpage>2157</fpage>
        <lpage>2163</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp379</pub-id>
        <pub-id pub-id-type="pmid">19542152</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <mixed-citation publication-type="other">Liu Y, Schmidt B, Maskell DL. Decgpu: distributed error correction on massively parallel graphics processing units using cuda and mpi. BMC Bioinformatics. 2011; 12(1). 10.1186/1471-2105-12-85.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kao</surname>
            <given-names>W.-C.</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>A. H.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>Y. S.</given-names>
          </name>
        </person-group>
        <article-title>ECHO: A reference-free short-read error correction algorithm</article-title>
        <source>Genome Research</source>
        <year>2011</year>
        <volume>21</volume>
        <issue>7</issue>
        <fpage>1181</fpage>
        <lpage>1192</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.111351.110</pub-id>
        <pub-id pub-id-type="pmid">21482625</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <mixed-citation publication-type="other">Das AK, Shams S, Goswami S, Platania R, Lee K, Park S-J. Parsech: Parallel sequencing error correction with hadoop for large-scale genome. In: Proceedings of the 9th International BICob Conference. ISCA: 2017. <ext-link ext-link-type="uri" xlink:href="https://www.searchdl.org/PagesPublic/ConfPaper.aspx?ConfPprID=26C12DF8-87DB-E711-A40B-E4B3180586B9">https://www.searchdl.org/PagesPublic/ConfPaper.aspx?ConfPprID=26C12DF8-87DB-E711-A40B-E4B3180586B9</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Salmela</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Rivals</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Lordec: accurate and efficient long read error correction</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>24</issue>
        <fpage>3506</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu538</pub-id>
        <pub-id pub-id-type="pmid">25165095</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Miclotte</surname>
            <given-names>Giles</given-names>
          </name>
          <name>
            <surname>Heydari</surname>
            <given-names>Mahdi</given-names>
          </name>
          <name>
            <surname>Demeester</surname>
            <given-names>Piet</given-names>
          </name>
          <name>
            <surname>Audenaert</surname>
            <given-names>Pieter</given-names>
          </name>
          <name>
            <surname>Fostier</surname>
            <given-names>Jan</given-names>
          </name>
        </person-group>
        <article-title>Jabba: Hybrid Error Correction for Long Sequencing Reads Using Maximal Exact Matches</article-title>
        <source>Lecture Notes in Computer Science</source>
        <year>2015</year>
        <publisher-loc>Berlin, Heidelberg</publisher-loc>
        <publisher-name>Springer Berlin Heidelberg</publisher-name>
        <fpage>175</fpage>
        <lpage>188</lpage>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hackl</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hedrich</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Schultz</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Förster</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>proovread: large-scale high-accuracy pacbio correction through iterative short read consensus</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>21</issue>
        <fpage>3004</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu392</pub-id>
        <pub-id pub-id-type="pmid">25015988</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koren</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schatz</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Walenz</surname>
            <given-names>BP</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Howard</surname>
            <given-names>JT</given-names>
          </name>
          <name>
            <surname>Ganapathy</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Rasko</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>McCombie</surname>
            <given-names>WR</given-names>
          </name>
          <name>
            <surname>Jarvis</surname>
            <given-names>ED</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Hybrid error correction and de novo assembly of single-molecule sequencing reads</article-title>
        <source>Nat Biotechnol</source>
        <year>2012</year>
        <volume>30</volume>
        <issue>7</issue>
        <fpage>693</fpage>
        <lpage>700</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.2280</pub-id>
        <pub-id pub-id-type="pmid">22750884</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Au</surname>
            <given-names>KF</given-names>
          </name>
          <name>
            <surname>Underwood</surname>
            <given-names>JG</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>WH</given-names>
          </name>
        </person-group>
        <article-title>Improving pacbio long read accuracy by short read alignment</article-title>
        <source>PLoS ONE</source>
        <year>2012</year>
        <volume>7</volume>
        <issue>10</issue>
        <fpage>46679</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0046679</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Haghshenas</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Hach</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Sahinalp</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Chauve</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Colormap: Correcting long reads by mapping short reads</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <issue>17</issue>
        <fpage>545</fpage>
        <lpage>51</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btw463</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <mixed-citation publication-type="other">Zhang H, Jain C, Aluru S. A comprehensive evaluation of long read error correction methods. BioRxiv. 2019:519330. 10.1101/519330.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Walker</surname>
            <given-names>BJ</given-names>
          </name>
          <name>
            <surname>Abeel</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Shea</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Priest</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Abouelliel</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sakthikumar</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Cuomo</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Wortman</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Young</surname>
            <given-names>SK</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Pilon: an integrated tool for comprehensive microbial variant detection and genome assembly improvement</article-title>
        <source>PLoS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <issue>11</issue>
        <fpage>112963</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0112963</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <mixed-citation publication-type="other">Hsu J. PacBio <italic>Ⓡ</italic> variant and consensus caller. <ext-link ext-link-type="uri" xlink:href="https://github.com/PacificBiosciences/GenomicConsensus">https://github.com/PacificBiosciences/GenomicConsensus</ext-link>. Last accessed on 03 Feb 2018.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Salmela</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Walve</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Rivals</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Ukkonen</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Accurate self-correction of errors in long reads using de bruijn graphs</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>33</volume>
        <issue>6</issue>
        <fpage>799</fpage>
        <lpage>806</lpage>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <mixed-citation publication-type="other">Morisse P, Marchet C, Limasset A, Lecroq T, Lefebvre A. Consent: Scalable self-correction of long reads with multiple sequence alignment. BioRxiv. 2019:546630. 10.1101/546630.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koren</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Walenz</surname>
            <given-names>BP</given-names>
          </name>
          <name>
            <surname>Berlin</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Bergman</surname>
            <given-names>NH</given-names>
          </name>
          <name>
            <surname>Phillippy</surname>
            <given-names>AM</given-names>
          </name>
        </person-group>
        <article-title>Canu: scalable and accurate long-read assembly via adaptive k-mer weighting and repeat separation</article-title>
        <source>Genome Res</source>
        <year>2017</year>
        <volume>27</volume>
        <issue>5</issue>
        <fpage>722</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.215087.116</pub-id>
        <pub-id pub-id-type="pmid">28298431</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Crusoe</surname>
            <given-names>Michael R.</given-names>
          </name>
          <name>
            <surname>Alameldin</surname>
            <given-names>Hussien F.</given-names>
          </name>
          <name>
            <surname>Awad</surname>
            <given-names>Sherine</given-names>
          </name>
          <name>
            <surname>Boucher</surname>
            <given-names>Elmar</given-names>
          </name>
          <name>
            <surname>Caldwell</surname>
            <given-names>Adam</given-names>
          </name>
          <name>
            <surname>Cartwright</surname>
            <given-names>Reed</given-names>
          </name>
          <name>
            <surname>Charbonneau</surname>
            <given-names>Amanda</given-names>
          </name>
          <name>
            <surname>Constantinides</surname>
            <given-names>Bede</given-names>
          </name>
          <name>
            <surname>Edvenson</surname>
            <given-names>Greg</given-names>
          </name>
          <name>
            <surname>Fay</surname>
            <given-names>Scott</given-names>
          </name>
          <name>
            <surname>Fenton</surname>
            <given-names>Jacob</given-names>
          </name>
          <name>
            <surname>Fenzl</surname>
            <given-names>Thomas</given-names>
          </name>
          <name>
            <surname>Fish</surname>
            <given-names>Jordan</given-names>
          </name>
          <name>
            <surname>Garcia-Gutierrez</surname>
            <given-names>Leonor</given-names>
          </name>
          <name>
            <surname>Garland</surname>
            <given-names>Phillip</given-names>
          </name>
          <name>
            <surname>Gluck</surname>
            <given-names>Jonathan</given-names>
          </name>
          <name>
            <surname>González</surname>
            <given-names>Iván</given-names>
          </name>
          <name>
            <surname>Guermond</surname>
            <given-names>Sarah</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>Jiarong</given-names>
          </name>
          <name>
            <surname>Gupta</surname>
            <given-names>Aditi</given-names>
          </name>
          <name>
            <surname>Herr</surname>
            <given-names>Joshua R.</given-names>
          </name>
          <name>
            <surname>Howe</surname>
            <given-names>Adina</given-names>
          </name>
          <name>
            <surname>Hyer</surname>
            <given-names>Alex</given-names>
          </name>
          <name>
            <surname>Härpfer</surname>
            <given-names>Andreas</given-names>
          </name>
          <name>
            <surname>Irber</surname>
            <given-names>Luiz</given-names>
          </name>
          <name>
            <surname>Kidd</surname>
            <given-names>Rhys</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>David</given-names>
          </name>
          <name>
            <surname>Lippi</surname>
            <given-names>Justin</given-names>
          </name>
          <name>
            <surname>Mansour</surname>
            <given-names>Tamer</given-names>
          </name>
          <name>
            <surname>McA'Nulty</surname>
            <given-names>Pamela</given-names>
          </name>
          <name>
            <surname>McDonald</surname>
            <given-names>Eric</given-names>
          </name>
          <name>
            <surname>Mizzi</surname>
            <given-names>Jessica</given-names>
          </name>
          <name>
            <surname>Murray</surname>
            <given-names>Kevin D.</given-names>
          </name>
          <name>
            <surname>Nahum</surname>
            <given-names>Joshua R.</given-names>
          </name>
          <name>
            <surname>Nanlohy</surname>
            <given-names>Kaben</given-names>
          </name>
          <name>
            <surname>Nederbragt</surname>
            <given-names>Alexander Johan</given-names>
          </name>
          <name>
            <surname>Ortiz-Zuazaga</surname>
            <given-names>Humberto</given-names>
          </name>
          <name>
            <surname>Ory</surname>
            <given-names>Jeramia</given-names>
          </name>
          <name>
            <surname>Pell</surname>
            <given-names>Jason</given-names>
          </name>
          <name>
            <surname>Pepe-Ranney</surname>
            <given-names>Charles</given-names>
          </name>
          <name>
            <surname>Russ</surname>
            <given-names>Zachary N.</given-names>
          </name>
          <name>
            <surname>Schwarz</surname>
            <given-names>Erich</given-names>
          </name>
          <name>
            <surname>Scott</surname>
            <given-names>Camille</given-names>
          </name>
          <name>
            <surname>Seaman</surname>
            <given-names>Josiah</given-names>
          </name>
          <name>
            <surname>Sievert</surname>
            <given-names>Scott</given-names>
          </name>
          <name>
            <surname>Simpson</surname>
            <given-names>Jared</given-names>
          </name>
          <name>
            <surname>Skennerton</surname>
            <given-names>Connor T.</given-names>
          </name>
          <name>
            <surname>Spencer</surname>
            <given-names>James</given-names>
          </name>
          <name>
            <surname>Srinivasan</surname>
            <given-names>Ramakrishnan</given-names>
          </name>
          <name>
            <surname>Standage</surname>
            <given-names>Daniel</given-names>
          </name>
          <name>
            <surname>Stapleton</surname>
            <given-names>James A.</given-names>
          </name>
          <name>
            <surname>Steinman</surname>
            <given-names>Susan R.</given-names>
          </name>
          <name>
            <surname>Stein</surname>
            <given-names>Joe</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>Benjamin</given-names>
          </name>
          <name>
            <surname>Trimble</surname>
            <given-names>Will</given-names>
          </name>
          <name>
            <surname>Wiencko</surname>
            <given-names>Heather L.</given-names>
          </name>
          <name>
            <surname>Wright</surname>
            <given-names>Michael</given-names>
          </name>
          <name>
            <surname>Wyss</surname>
            <given-names>Brian</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Qingpeng</given-names>
          </name>
          <name>
            <surname>zyme</surname>
            <given-names>en</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>C. Titus</given-names>
          </name>
        </person-group>
        <article-title>The khmer software package: enabling efficient nucleotide sequence analysis</article-title>
        <source>F1000Research</source>
        <year>2015</year>
        <volume>4</volume>
        <fpage>900</fpage>
        <pub-id pub-id-type="doi">10.12688/f1000research.6924.1</pub-id>
        <pub-id pub-id-type="pmid">26535114</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <mixed-citation publication-type="other">Brown CT, Howe A, Zhang Q, Pyrkosz AB, Brom TH. A reference-free algorithm for computational normalization of shotgun sequencing data. 2012. arXiv preprint arXiv:1203.4802.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <mixed-citation publication-type="other">Johns M. Getting Started with Hazelcast: Packt Publishing Ltd; 2015. <ext-link ext-link-type="uri" xlink:href="https://www.packtpub.com/big-data-and-business-intelligence/getting-started-hazelcast">https://www.packtpub.com/big-data-and-business-intelligence/getting-started-hazelcast</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <mixed-citation publication-type="other">High Performance Computing Louisiana State University. <ext-link ext-link-type="uri" xlink:href="http://www.hpc.lsu.edu/resources/hpc/system.php?system=SuperMIC">http://www.hpc.lsu.edu/resources/hpc/system.php?system=SuperMIC</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Das</surname>
            <given-names>Arghya Kusum</given-names>
          </name>
          <name>
            <surname>Koppa</surname>
            <given-names>Praveen Kumar</given-names>
          </name>
          <name>
            <surname>Goswami</surname>
            <given-names>Sayan</given-names>
          </name>
          <name>
            <surname>Platania</surname>
            <given-names>Richard</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>Seung-Jong</given-names>
          </name>
        </person-group>
        <article-title>Large-scale parallel genome assembler over cloud computing environment</article-title>
        <source>Journal of Bioinformatics and Computational Biology</source>
        <year>2017</year>
        <volume>15</volume>
        <issue>03</issue>
        <fpage>1740003</fpage>
        <pub-id pub-id-type="doi">10.1142/S0219720017400030</pub-id>
        <pub-id pub-id-type="pmid">28610458</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <mixed-citation publication-type="other">Das AK, Park S-J, Hong J, Chang W. Evaluating different distributed-cyber-infrastructure for data and compute intensive scientific application. In: IEEE International Conference on Big Data: 2015. 10.1109/bigdata.2015.7363750.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <mixed-citation publication-type="other">Das AK, Hong J, Goswami S, Platania R, Lee K, Chang W, Park S-J, Liu L. Augmenting amdahl’s second law: A theoretical model to build cost-effective balanced hpc infrastructure for data-driven science. In: Cloud Computing (CLOUD), 2017 IEEE 10th International Conference On. IEEE: 2017. p. 147–54. 10.1109/cloud.2017.27.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chaisson</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Tesler</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Mapping single molecule sequencing reads using basic local alignment with successive refinement (blasr): application and theory</article-title>
        <source>BMC Bioinformatics</source>
        <year>2012</year>
        <volume>13</volume>
        <issue>1</issue>
        <fpage>238</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-13-238</pub-id>
        <pub-id pub-id-type="pmid">22988817</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Durbin</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Fast and accurate short read alignment with burrows–wheeler transform</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <issue>14</issue>
        <fpage>1754</fpage>
        <lpage>60</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp324</pub-id>
        <pub-id pub-id-type="pmid">19451168</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chockalingam</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Aluru</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>A survey of error-correction methods for next-generation sequencing</article-title>
        <source>Brief Bioinform</source>
        <year>2012</year>
        <volume>14</volume>
        <issue>1</issue>
        <fpage>56</fpage>
        <lpage>66</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbs015</pub-id>
        <pub-id pub-id-type="pmid">22492192</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
