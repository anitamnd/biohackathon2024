<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS Comput Biol</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLOS Computational Biology</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1553-734X</issn>
    <issn pub-type="epub">1553-7358</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9836277</article-id>
    <article-id pub-id-type="pmid">36520922</article-id>
    <article-id pub-id-type="publisher-id">PCOMPBIOL-D-22-00822</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1010779</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Database and Informatics Methods</subject>
          <subj-group>
            <subject>Bioinformatics</subject>
            <subj-group>
              <subject>Sequence Analysis</subject>
              <subj-group>
                <subject>Sequence Motif Analysis</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied Mathematics</subject>
            <subj-group>
              <subject>Algorithms</subject>
              <subj-group>
                <subject>Machine Learning Algorithms</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
          <subj-group>
            <subject>Algorithms</subject>
            <subj-group>
              <subject>Machine Learning Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Machine Learning Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Deep Learning</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Genetics</subject>
          <subj-group>
            <subject>Gene Expression</subject>
            <subj-group>
              <subject>Gene Regulation</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Biochemistry</subject>
          <subj-group>
            <subject>Nucleotides</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Molecular Biology</subject>
          <subj-group>
            <subject>Molecular Biology Techniques</subject>
            <subj-group>
              <subject>Sequencing Techniques</subject>
              <subj-group>
                <subject>Nucleotide Sequencing</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Molecular Biology Techniques</subject>
          <subj-group>
            <subject>Sequencing Techniques</subject>
            <subj-group>
              <subject>Nucleotide Sequencing</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Computational Biology</subject>
          <subj-group>
            <subject>Genome Analysis</subject>
            <subj-group>
              <subject>Gene Ontologies</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Genetics</subject>
          <subj-group>
            <subject>Genomics</subject>
            <subj-group>
              <subject>Genome Analysis</subject>
              <subj-group>
                <subject>Gene Ontologies</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Genome-wide identification and characterization of DNA enhancers with a stacked multivariate fusion framework</article-title>
      <alt-title alt-title-type="running-head">SMFM</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Yansong</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hou</surname>
          <given-names>Zilong</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4556-139X</contrib-id>
        <name>
          <surname>Yang</surname>
          <given-names>Yuning</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wong</surname>
          <given-names>Ka-chun</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <xref rid="aff003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8716-9823</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Xiangtao</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>School of Artificial Intelligence, Jilin University, Changchun, China</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Donnelly Centre for Cellular and Biomolecular Research, University of Toronto, Toronto, Canada</addr-line>
    </aff>
    <aff id="aff003">
      <label>3</label>
      <addr-line>Department of Computer science, City University of Hong Kong, Hong Kong, Special Administrative Region</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Sinha</surname>
          <given-names>Saurabh</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>University of Illinois at Urbana-Champaign, UNITED STATES</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>lixt314@jlu.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <volume>18</volume>
    <issue>12</issue>
    <elocation-id>e1010779</elocation-id>
    <history>
      <date date-type="received">
        <day>27</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>29</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Wang et al</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Wang et al</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pcbi.1010779.pdf"/>
    <abstract>
      <p>Enhancers are short non-coding DNA sequences outside of the target promoter regions that can be bound by specific proteins to increase a gene’s transcriptional activity, which has a crucial role in the spatiotemporal and quantitative regulation of gene expression. However, enhancers do not have a specific sequence motifs or structures, and their scattered distribution in the genome makes the identification of enhancers from human cell lines particularly challenging. Here we present a novel, stacked multivariate fusion framework called SMFM, which enables a comprehensive identification and analysis of enhancers from regulatory DNA sequences as well as their interpretation. Specifically, to characterize the hierarchical relationships of enhancer sequences, multi-source biological information and dynamic semantic information are fused to represent regulatory DNA enhancer sequences. Then, we implement a deep learning–based sequence network to learn the feature representation of the enhancer sequences comprehensively and to extract the implicit relationships in the dynamic semantic information. Ultimately, an ensemble machine learning classifier is trained based on the refined multi-source features and dynamic implicit relations obtained from the deep learning-based sequence network. Benchmarking experiments demonstrated that SMFM significantly outperforms other existing methods using several evaluation metrics. In addition, an independent test set was used to validate the generalization performance of SMFM by comparing it to other state-of-the-art enhancer identification methods. Moreover, we performed motif analysis based on the contribution scores of different bases of enhancer sequences to the final identification results. Besides, we conducted interpretability analysis of the identified enhancer sequences based on attention weights of EnhancerBERT, a fine-tuned BERT model that provides new insights into exploring the gene semantic information likely to underlie the discovered enhancers in an interpretable manner. Finally, in a human placenta study with 4,562 active distal gene regulatory enhancers, SMFM successfully exposed tissue-related placental development and the differential mechanism, demonstrating the generalizability and stability of our proposed framework.</p>
    </abstract>
    <abstract abstract-type="summary">
      <title>Author summary</title>
      <p>Numerous evidence suggest that genes regulated by enhancers located in non-coding DNA regions are involved in a myriad of biological activities. To fully understand the regulatory role and mechanisms of enhancers on genes, the localization and identification of enhancers is essential. Several experimental biological methods are capable of localizing enhancers, however, these methods are resource intensive. To address this limitation, we developed a stacked multivariate fusion framework, called SMFM to identify and analyze enhancers with high accuracy and efficiency based on enhancer-specific dynamic semantic information and multi-source biological properties. The performance of the model is verified by experiments comparing different feature algorithms and classification algorithms. The superiority of our method is demonstrated by comparing it with several state-of-the-art algorithms. In addition, several analytical experiments demonstrate that SMFM is capable of recognizing enhancers in different tissues and detecting motifs in enhancers. To the best of our knowledge, this is the first computational approach that uses enhancer-specific dynamic semantic information to identify enhancers from regulatory DNA sequences and interpret them. It is expected that the SMFM model will effectively target enhancers and provide valid candidates for further biological experiments.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001809</institution-id>
            <institution>National Natural Science Foundation of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>62076109</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8716-9823</contrib-id>
          <name>
            <surname>Li</surname>
            <given-names>Xiangtao</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This study was supported by the National Natural Science Foundation of China under (Grant No. 62076109 to XL). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="12"/>
      <table-count count="4"/>
      <page-count count="33"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>PLOS Publication Stage</meta-name>
        <meta-value>vor-update-to-uncorrected-proof</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>Publication Update</meta-name>
        <meta-value>2023-01-12</meta-value>
      </custom-meta>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>All supporting source codes and data can be downloaded from <ext-link xlink:href="https://github.com/no-banana/SMFM-master" ext-link-type="uri">https://github.com/no-banana/SMFM-master</ext-link> and <ext-link xlink:href="https://figshare.com/articles/software/IDEAL/19398338" ext-link-type="uri">https://figshare.com/articles/software/IDEAL/19398338</ext-link>. And the webserver of SMFM is publicly accessible at <ext-link xlink:href="http://39.104.69.176:5010/" ext-link-type="uri">http://39.104.69.176:5010/</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>All supporting source codes and data can be downloaded from <ext-link xlink:href="https://github.com/no-banana/SMFM-master" ext-link-type="uri">https://github.com/no-banana/SMFM-master</ext-link> and <ext-link xlink:href="https://figshare.com/articles/software/IDEAL/19398338" ext-link-type="uri">https://figshare.com/articles/software/IDEAL/19398338</ext-link>. And the webserver of SMFM is publicly accessible at <ext-link xlink:href="http://39.104.69.176:5010/" ext-link-type="uri">http://39.104.69.176:5010/</ext-link>.</p>
  </notes>
</front>
<body>
  <disp-quote>
    <p>This is a <italic toggle="yes">PLOS Computational Biology</italic> Methods paper.</p>
  </disp-quote>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>Enhancers are a series of DNA segments in the non-coding DNA sequences that can significantly increase the transcription rate of their target genes after being bound by transcriptional factors and other co-regulators that control the promoters of the associated genes [<xref rid="pcbi.1010779.ref001" ref-type="bibr">1</xref>]. Recent studies have shown that different enhancers have distinct sets of subregions (or motifs) that bind specific transcription factors, and exhibit diverse activities and regulatory roles on multiple biological genes [<xref rid="pcbi.1010779.ref002" ref-type="bibr">2</xref>]. Enhancers are typically in the intergenic and intronic regions and often include binding sites for multiple transcription factors. Intriguingly, active enhancers undergo transcription by RNA polymerase II to generate enhancer RNAs (eRNAs) [<xref rid="pcbi.1010779.ref003" ref-type="bibr">3</xref>, <xref rid="pcbi.1010779.ref004" ref-type="bibr">4</xref>]. Moreover, genetic variants in cell-type-specific enhancer sequences are associated with a risk for common diseases in humans [<xref rid="pcbi.1010779.ref005" ref-type="bibr">5</xref>]. Therefore, it is of great interest to identify enhancers in regulatory DNA sequences with the potential to provide new opportunities for understanding physiological and pathological processes.</p>
    <p>In the early days, researchers identified enhancers primarily by conducting biological experiments with vitro and vivo functional assays, such as gel-shift assays in [<xref rid="pcbi.1010779.ref006" ref-type="bibr">6</xref>]. More recent approaches use publicly available comparative sequence datasets for comparative genomics [<xref rid="pcbi.1010779.ref007" ref-type="bibr">7</xref>], for example. However, the heavy cost and tedious processing times of high-throughput experiments severely restrict their practical application [<xref rid="pcbi.1010779.ref008" ref-type="bibr">8</xref>]for effective enhancer identification, due to the lack of sample diversity [<xref rid="pcbi.1010779.ref007" ref-type="bibr">7</xref>] and the difficulty in simulating different cellular conditions [<xref rid="pcbi.1010779.ref009" ref-type="bibr">9</xref>]. Currently, a series of computational methods have been developed to address enhancer identification, which can be divided into three categories: <italic toggle="yes">1) Chromatin-based methods</italic>: these algorithms typically employ chromatin information to characterize enhancer sequences, and then most identify enhancers using various machine learning classifiers, including ChromaGenSVM [<xref rid="pcbi.1010779.ref010" ref-type="bibr">10</xref>], RFECS [<xref rid="pcbi.1010779.ref011" ref-type="bibr">11</xref>], EnhancerFinder [<xref rid="pcbi.1010779.ref012" ref-type="bibr">12</xref>], GKM-SVM [<xref rid="pcbi.1010779.ref013" ref-type="bibr">13</xref>]. <italic toggle="yes">2) Physicochemical-based methods</italic>: such algorithms are implemented using various physicochemical features that encode enhancer subsequences, including iEnhancer-2L [<xref rid="pcbi.1010779.ref014" ref-type="bibr">14</xref>], EnhancerPred [<xref rid="pcbi.1010779.ref015" ref-type="bibr">15</xref>], iEnhancer-EL [<xref rid="pcbi.1010779.ref016" ref-type="bibr">16</xref>], iEnhancer-RF [<xref rid="pcbi.1010779.ref017" ref-type="bibr">17</xref>], iEnhancer-XG [<xref rid="pcbi.1010779.ref018" ref-type="bibr">18</xref>], iEnhancer-ECNN [<xref rid="pcbi.1010779.ref019" ref-type="bibr">19</xref>], CSI-ANN [<xref rid="pcbi.1010779.ref020" ref-type="bibr">20</xref>] and Enhancer-IF [<xref rid="pcbi.1010779.ref021" ref-type="bibr">21</xref>], where iEnhancer-ECNN [<xref rid="pcbi.1010779.ref019" ref-type="bibr">19</xref>] and CSI-ANN [<xref rid="pcbi.1010779.ref020" ref-type="bibr">20</xref>] utilize deep learning techniques to learn the implicit information in the features, and the other methods use traditional machine learning classifiers to accomplish the identification task. <italic toggle="yes">3) Contextual-based methods</italic>:iEnhancer-EBLSTM [<xref rid="pcbi.1010779.ref022" ref-type="bibr">22</xref>], iEnhancer-5Step [<xref rid="pcbi.1010779.ref023" ref-type="bibr">23</xref>] and BERT-2DCNNs [<xref rid="pcbi.1010779.ref024" ref-type="bibr">24</xref>] consider the contextual information in enhancer sequences, and use different natural language processing technologies to form the embedding matrix of enhancer sequences. However, most of these computational models use only a single feature type to characterize enhancer sequences, making it difficult to describe distribution and the representations between nucleotides and their contexts, leaving adequate room for improving performance.</p>
    <p>In our study, we designed a novel stacked multivariate fusion model, called SMFM. In SMFM, multi-source biological features and EnhancerBERT are proposed to represent the enhancer sequences, where EnhancerBERT can maximize the characterization power of the dynamic semantic information of enhancer sequences. Then, we designed a deep learning-based sequence network to learn the dynamic implicit relations and long-distance dependencies in the dynamic semantic information. Finally, we merged the two types of processed features and feed them into an ensemble machine learning classifier to derive the final prediction results. To validate the effectiveness and good performance of SMFM, we conducted several experiments performing a rigorous 10-fold cross-validation on the training set. The experimental results showed that SMFM significantly outperforms currently available methods. In addition, to verify the stability and generalization ability of SMFM, we tested and compared and compared the conduct of SMFM on a completely independent test set and results indicated that SMFM generally outperforms existing methods. Furthermore, to explore the ability of characterization of SMFM for tissue-specific enhancers, we designed a stepwise experiment on 4,562 placental enhancers: identifying placental enhancers in the first step and distinguishing placental enhancers from enhancers in other tissues in the second step. In order to validate the effectiveness of placental enhancers identified by SMFM, we then performed gene ontology (GO) and kyoto encyclopedia of genes and genomes (KEGG) enrichment analysis based on results of stepwise experiments. Finally, we carried out motif analysis and interpretability analysis of the identified enhancer sequences based on attention weights in EnhancerBERT and provide here an online web server that can predict enhancers in DNA sequences online, which is available at <ext-link xlink:href="http://39.104.69.176:5010/" ext-link-type="uri">http://39.104.69.176:5010/</ext-link>.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec002">
    <title>Materials and methods</title>
    <sec id="sec003">
      <title>A. Data sources</title>
      <p>We collected the dataset from nine different cell lines, including H1ES, K562, GM12878, HepG2, HUVEC, HSMM, NHLF, NHEK and HMEC [<xref rid="pcbi.1010779.ref014" ref-type="bibr">14</xref>]. The samples in the dataset were selected based on chromatin state information, which was annotated by ChromHMM [<xref rid="pcbi.1010779.ref025" ref-type="bibr">25</xref>], and divided into 200bp fragments to match linker and nucleosome length DNA. A sample was discarded if its length was less than 200bp. The CD-HIT tool was utilized for reducing the similarity between fragments with a threshold value of 0.8. From this, we obtained the dataset including three classes: strong enhancers (<inline-formula id="pcbi.1010779.e001"><alternatives><graphic xlink:href="pcbi.1010779.e001.jpg" id="pcbi.1010779.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:msub><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>), weak enhancers (<inline-formula id="pcbi.1010779.e002"><alternatives><graphic xlink:href="pcbi.1010779.e002.jpg" id="pcbi.1010779.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:msub><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>) and non-enhancers (<inline-formula id="pcbi.1010779.e003"><alternatives><graphic xlink:href="pcbi.1010779.e003.jpg" id="pcbi.1010779.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></alternatives></inline-formula>). In our work, we merged the <inline-formula id="pcbi.1010779.e004"><alternatives><graphic xlink:href="pcbi.1010779.e004.jpg" id="pcbi.1010779.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:msub><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1010779.e005"><alternatives><graphic xlink:href="pcbi.1010779.e005.jpg" id="pcbi.1010779.e005g" position="anchor"/><mml:math id="M5" display="inline" overflow="scroll"><mml:msub><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> as the positive samples (<inline-formula id="pcbi.1010779.e006"><alternatives><graphic xlink:href="pcbi.1010779.e006.jpg" id="pcbi.1010779.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></alternatives></inline-formula>), while <inline-formula id="pcbi.1010779.e007"><alternatives><graphic xlink:href="pcbi.1010779.e007.jpg" id="pcbi.1010779.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></alternatives></inline-formula> were the negative samples following reference [<xref rid="pcbi.1010779.ref014" ref-type="bibr">14</xref>]. The structure of the dataset can be described as follows:
<disp-formula id="pcbi.1010779.e008"><alternatives><graphic xlink:href="pcbi.1010779.e008.jpg" id="pcbi.1010779.e008g" position="anchor"/><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="double-struck">S</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>∪</mml:mo><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mo>−</mml:mo></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:msub><mml:mrow/><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∪</mml:mo><mml:msup><mml:mi mathvariant="double-struck">S</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:msub><mml:mrow/><mml:mrow><mml:mi>w</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo/></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>The dataset includes 2968 samples, of which 1484 are enhancers and the others non-enhancers. We evaluated the performance using 10-fold cross-validation, which divides the training set into 10 subsets, where one subset is the validation set, and the other 9 subsets constitute the training set. Each subset needs to be performed once as a validation set. In addition, we employed an independent test set including 200 enhancers and 200 non-enhancers to test the stability and generalization ability of SMFM compared with other existing methods.</p>
      <p>To visualize the enhancer dataset, we applied a series of dimensionality reduction methods to project the sequence feature representation based on the one-hot encoding approach to the two-dimensional space, as envisioned in <xref rid="pcbi.1010779.g001" ref-type="fig">Fig 1</xref>. Unfortunately, it can be observed that the enhancer dataset cannot be classified linearly. Therefore, the development of effective sequence representation models and nonlinear-based modeling including deep neural networks is imperative to identify these sequences in human cell lines.</p>
      <fig position="float" id="pcbi.1010779.g001">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>Dataset visualization for DNA Enhancers based on the one-hot encoding approach.</title>
          <p>All figures are drawn using dimensionality reduction methods including ICA, PCA, FA and t-SNE with Python scikit-learn package as default setting. From the figures it can be concluded that linear classification cannot be utilized in enhancer dataset.</p>
        </caption>
        <graphic xlink:href="pcbi.1010779.g001" position="float"/>
      </fig>
    </sec>
    <sec id="sec004">
      <title>B. Feature representation schemes</title>
      <p>To characterize enhancer sequences as efficiently as possible, two types of quantifiable features including multi-source biological information and dynamic semantic information are usually adopted in research to represent regulatory DNA enhancer sequences.</p>
      <p><italic toggle="yes">1) Positional gapped k-m-tuple pairs (PGKM)</italic>: To capture interactions between non-adjacent residues, gapped k-mer feature generation method is often employed to represent the enhancer sequence for the classification tasks [<xref rid="pcbi.1010779.ref026" ref-type="bibr">26</xref>]. However, such a method discards information about the positions of the different functional subsequences (motifs), which have an important role in recording the distinction between the particular functional sequences (e.g. enhancers). To overcome this limitation, we introduce the positional gap k-m-tuple pair (PGKM) as one of the feature descriptors. PGKM contains three parts: k-tuple ({<italic toggle="yes">Nu</italic> × <italic toggle="yes">k</italic>}), m-tuple ({<italic toggle="yes">Nu</italic> × <italic toggle="yes">m</italic>}), and gap (<italic toggle="yes">G</italic>). The feature generation procedure can be characterized as follows:
<disp-formula id="pcbi.1010779.e009"><alternatives><graphic xlink:href="pcbi.1010779.e009.jpg" id="pcbi.1010779.e009g" position="anchor"/><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mi>G</mml:mi><mml:mi>K</mml:mi><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>N</mml:mi><mml:mi>u</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi><mml:mo>}</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:mo>{</mml:mo><mml:mi>N</mml:mi><mml:mi>u</mml:mi><mml:mo>×</mml:mo><mml:mi>m</mml:mi><mml:mo>}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic toggle="yes">Nu</italic> ∈ {<italic toggle="yes">A</italic>, <italic toggle="yes">C</italic>, <italic toggle="yes">G</italic>, <italic toggle="yes">T</italic>}, <italic toggle="yes">Gap</italic> represents the number of nucleotide intervals between tuples, for <italic toggle="yes">Gap</italic> = <italic toggle="yes">n</italic>, PGKM will calculate the nucleotides between two tuples less than or equal to n, with a lower bound of 1. <italic toggle="yes">k</italic> denotes the number of nucleotides in the first tuple, and <italic toggle="yes">m</italic> denotes the number of nucleotides in the second tuple, respectively. Therefore, in general, when <italic toggle="yes">Gap</italic> = <italic toggle="yes">n</italic>, PGKM can generate 4<sup><italic toggle="yes">k</italic></sup> × 4<sup><italic toggle="yes">m</italic></sup> × <italic toggle="yes">n</italic> features for an enhancer sequence.</p>
      <p>Considering the sequence ‘ACCGTA’ as an example, PGKM counts the number of times each tuple pair appears in the sequence and uses this number as the value of the corresponding feature, when <italic toggle="yes">Gap</italic> = 3, <italic toggle="yes">k</italic> = 1, <italic toggle="yes">m</italic> = 1, 48 tuple-pairs (features) can be generated, including three cases: 1) when <italic toggle="yes">Gap</italic> = 1, the following features are calculated: A_A, A_C, A_G, A_T, C_A, C_C, C_G, C_T, G_A, G_C, G_G, G_T, T_A, T_C, T_G, T_T; 2) when <italic toggle="yes">Gap</italic> = 2, A_ _A, A_ _C, A_ _G, A_ _T, C_ _A, C_ _C, C_ _G, C_ _T, G_ _A, G_ _C, G_ _G, G_ _T, T_ _A, T_ _C, T_ _G, T_ _T are calculated; 3) when <italic toggle="yes">Gap</italic> = 3, there are 16 features as follows: A_ _ _A, A_ _ _C, A_ _ _G, A_ _ _T, C_ _ _A, C_ _ _C, C_ _ _G, C_ _ _T, G_ _ _A, G_ _ _C, G_ _ _G, G_ _ _T, T_ _ _A, T_ _ _C, T_ _ _G and T_ _ _T. On this basis, the given sequence has: ∑<italic toggle="yes">A</italic>_<italic toggle="yes">C</italic> = 1, ∑<italic toggle="yes">C</italic>_<italic toggle="yes">G</italic> = 1, ∑<italic toggle="yes">C</italic>_<italic toggle="yes">T</italic> = 1, ∑<italic toggle="yes">G</italic>_<italic toggle="yes">A</italic> = 1, ∑<italic toggle="yes">A</italic>_ _<italic toggle="yes">G</italic> = 1, ∑<italic toggle="yes">C</italic>_ _<italic toggle="yes">T</italic> = 1, ∑<italic toggle="yes">C</italic>_ _<italic toggle="yes">A</italic> = 1, ∑<italic toggle="yes">A</italic>_ _ _<italic toggle="yes">T</italic> = 1, ∑<italic toggle="yes">C</italic>_ _ _<italic toggle="yes">A</italic> = 1. In addition, the value is set to 0 for the remaining features as they do not appear in the sequence ‘ACCGTA’.</p>
      <p><italic toggle="yes">2) Pseudo K-tuple nucleotide composition (PseKNC)</italic>: To extract local contextual features from the enhancer sequences, PseKNC is employed to encode the nucleotide sequences, which can embrace the adjacent information of each nucleotide in the sequences [<xref rid="pcbi.1010779.ref027" ref-type="bibr">27</xref>]. Specifically, the regular k-tuple is a vector that represents a nucleotide sequence with size of 4<sup><italic toggle="yes">k</italic></sup>. The PseKNC can be applied by aggregating the set of k-tuples that contains all tuples consisting of less than or equal to <italic toggle="yes">k</italic> nucleotides. It can be defined as follows:
<disp-formula id="pcbi.1010779.e010"><alternatives><graphic xlink:href="pcbi.1010779.e010.jpg" id="pcbi.1010779.e010g" position="anchor"/><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="0.277778em"/><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:msubsup><mml:mi>f</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="0.277778em"/><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:msubsup><mml:mi>f</mml:mi><mml:mn>3</mml:mn><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="0.277778em"/><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="0.277778em"/><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:msubsup><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="0.277778em"/><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="0.277778em"/><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:msubsup><mml:mi>f</mml:mi><mml:mrow><mml:msup><mml:mn>4</mml:mn><mml:mi>i</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="0.277778em"/><mml:mo>]</mml:mo></mml:mrow><mml:mspace width="4pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic toggle="yes">V</italic><sub><italic toggle="yes">i</italic></sub> represents the vector generated by i-tuple and <inline-formula id="pcbi.1010779.e011"><alternatives><graphic xlink:href="pcbi.1010779.e011.jpg" id="pcbi.1010779.e011g" position="anchor"/><mml:math id="M11" display="inline" overflow="scroll"><mml:msubsup><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> denotes the frequency of t-th i-tuple in a sequence. We set k = 3, which yields vectors corresponding to mononucleotide tuples, dinucleotide tuples and trinucleotide tuples. On this basis, each enhancer sequence would be depicted as a one-dimensional vector with size <italic toggle="yes">V</italic><sub>1</sub> + <italic toggle="yes">V</italic><sub>2</sub> + <italic toggle="yes">V</italic><sub>3</sub>.</p>
      <p><italic toggle="yes">3) Nucleotide physicochemical properties (NPCP)</italic>: Apart from the nucleotide distribution representation, the physicochemical property is a fundamental property of a nucleotide that provides a unique contribution to characterize the sequences. Here four different physicochemical properties including Zcurve [<xref rid="pcbi.1010779.ref028" ref-type="bibr">28</xref>], GC-content [<xref rid="pcbi.1010779.ref029" ref-type="bibr">29</xref>], (A+T)/(C+G) ratio [<xref rid="pcbi.1010779.ref030" ref-type="bibr">30</xref>], and GC/AT skew [<xref rid="pcbi.1010779.ref031" ref-type="bibr">31</xref>], are employed to represent the enhancer sequence, which can generate 3-, 1-, 1- and 2-dimensional vectors, respectively. Therefore, the NPCP for the <italic toggle="yes">t</italic>th sequence <italic toggle="yes">s</italic><sub><italic toggle="yes">t</italic></sub> can be formulated as follows:
<disp-formula id="pcbi.1010779.e012"><alternatives><graphic xlink:href="pcbi.1010779.e012.jpg" id="pcbi.1010779.e012g" position="anchor"/><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="26pt"/><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic toggle="yes">f</italic><sub><italic toggle="yes">i</italic></sub> indicates the <italic toggle="yes">i</italic>-th property in NPCP.</p>
      <p><italic toggle="yes">4) Multi-source feature selection</italic>: Since multi-source biological information yields excessive features, this leads to a very laborious training process of the model and also prevents the model from capturing the most critical information that distinguishes the different enhancer subsequences. To address these limitations, we propose employing an AdaBoost model to identify the best subset of features from these high-dimensional features. Specifically, the selector in the AdaBoost model scores the different features by partitioning each feature into all the trees trained on instances with different weight distributions, and calculating the average impurity reduction for each feature. After obtaining the scores of all features, the 472 refined features with an average impurity curtailment over zero are selected as the final streamlined feature set.</p>
      <p><italic toggle="yes">5) Enhancer dynamic semantic information (EnhancerBERT)</italic>: BERT (bidirectional encoder representations from transformers) can learn powerful representations of language to encode information about syntax and semantics, and which is typically pre-trained on a large corpus in a self-supervised fashion [<xref rid="pcbi.1010779.ref032" ref-type="bibr">32</xref>]. In this context, it is natural to consider enhancer sequences as texts and to explore the semantic information between them by considering nucleic acids as words in a biological language, and structural and regulatory functions as syntactic and semantic information in the enhancer sequence. Inspired by reference [<xref rid="pcbi.1010779.ref033" ref-type="bibr">33</xref>], we developed EnhancerBERT to maximize the characterization power of the dynamic semantic information of enhancer sequences. In our EnhancerBERT model, we tokenize the enhancer sequences to make them more syntactic, while the prediction task of the BERT-based model shifts to make predictions on how many continuous tokens in an enhancer ‘sentence’ match the possible realistic cases. Indeed, considering that the use of a single acid as a token is too rare, we use <italic toggle="yes">k</italic>-mer (<italic toggle="yes">k</italic> is an integer greater than zero) to process the enhancer sequences. For the sequence ‘ATCGGGCTA’, when <italic toggle="yes">k</italic> = 3, the tokens {ATC, TCG, CGG, GGG, GGC, GCT, CTA} will be generated after 3-mer processing. Note that we have added two special tokens: [CLS] to represent the beginning of the sequence and [SEP] to represent the end of the sequence following reference [<xref rid="pcbi.1010779.ref033" ref-type="bibr">33</xref>]. Therefore, 4<sup><italic toggle="yes">k</italic></sup> + 2 tokens can be obtained in the vocabulary of kmer. After that, the EnhancerBERT model is pre-trained on a set of masked enhancer sequences that are processed as a series of <italic toggle="yes">k</italic>-mer tokens, each of which can be represented as a unique numerical vector. That is, each sequence can be represented as a matrix <italic toggle="yes">M</italic>. On this basis, EnhancerBERT captures contextual information using a multi-headed self-focus mechanism on <italic toggle="yes">M</italic>, which is described as follows:
<disp-formula id="pcbi.1010779.e013"><alternatives><graphic xlink:href="pcbi.1010779.e013.jpg" id="pcbi.1010779.e013g" position="anchor"/><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mi>O</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></disp-formula>
<disp-formula id="pcbi.1010779.e014"><alternatives><graphic xlink:href="pcbi.1010779.e014.jpg" id="pcbi.1010779.e014g" position="anchor"/><mml:math id="M14" display="block" overflow="scroll"><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula>
<disp-formula id="pcbi.1010779.e015"><alternatives><graphic xlink:href="pcbi.1010779.e015.jpg" id="pcbi.1010779.e015g" position="anchor"/><mml:math id="M15" display="block" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mi>t</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mi>K</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac><mml:mo>)</mml:mo><mml:mo>·</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where
<disp-formula id="pcbi.1010779.e016"><alternatives><graphic xlink:href="pcbi.1010779.e016.jpg" id="pcbi.1010779.e016g" position="anchor"/><mml:math id="M16" display="block" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>Q</mml:mi></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>K</mml:mi></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>V</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo/></mml:mrow></mml:math></alternatives></disp-formula>
<italic toggle="yes">Q</italic>, <italic toggle="yes">K</italic>, <italic toggle="yes">V</italic> represents query, key and value respectively, which are projected by <italic toggle="yes">n</italic> diverse linear conversions. <inline-formula id="pcbi.1010779.e017"><alternatives><graphic xlink:href="pcbi.1010779.e017.jpg" id="pcbi.1010779.e017g" position="anchor"/><mml:math id="M17" display="inline" overflow="scroll"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>Q</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>K</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>V</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> are the learnable parameter matrices of the linear projection, respectively. Each <italic toggle="yes">head</italic><sub><italic toggle="yes">i</italic></sub> is utilized to compute the next hidden state of the matrix <italic toggle="yes">M</italic>, first calculating the attention fraction between every two tokens and then appending rows in <inline-formula id="pcbi.1010779.e018"><alternatives><graphic xlink:href="pcbi.1010779.e018.jpg" id="pcbi.1010779.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>V</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> using them as weights. After that, <italic toggle="yes">MultiHead</italic> concatenates <italic toggle="yes">head</italic><sub>1∼<italic toggle="yes">n</italic></sub> with a distinct set of <inline-formula id="pcbi.1010779.e019"><alternatives><graphic xlink:href="pcbi.1010779.e019.jpg" id="pcbi.1010779.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>Q</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>K</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>V</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The whole process is conducted <italic toggle="yes">T</italic> times and <italic toggle="yes">T</italic> is the number of layers.</p>
      <p>In the process of fine-tuning the model, we remove the head of the pre-trained model and replace it with a random initialization. Regarding the hyperparameters used for fine-tuning, we fine-tune EnhancerBERT for five epochs on the enhancer training set and apply an early stopping mechanism with a patience of two to prevent overfitting phenomena. A roll back mechanism of the model parameters is utilized after an early stop mechanism and the Adam, without weight decay, is chosen as the optimizer. Remarkably, the aforementioned hyperparameters are consistent for all EnhancerBERT (including 4 models, from 3 to 6mers, respectively), which also use 12 Transformer encoder layers, each consisting of 12 self-attentive heads, to extract semantic information using a multi-headed self-attentive mechanism. Moreover, in our study, to capture sufficient multilayer fusion enhancer information, we simply extract the hidden states from the last layer of the model and drop the vector representation obtained from the special tokens [CLS] and [SEP] added before and after each enhancer sequence to generate the (200-<italic toggle="yes">k</italic>+1, 768) matrix, where <italic toggle="yes">k</italic> is the value of kmer used to process the enhancer sequences, and 768 is the dimension of the vector generated by EnhancerBERT for each token.</p>
    </sec>
    <sec id="sec005">
      <title>C. Stacked multivariate fusion model (SMFM)</title>
      <p>To capture efficiently the information contained in multiple feature scenarios that are critical for enhancer characterization, we designed a novel stacked multivariate fusion model, called SMFM including three important components, as shown in <xref rid="pcbi.1010779.g002" ref-type="fig">Fig 2</xref>. As depicted in this figure, rather than traditional machine learning or deep learning approaches, SMFM synergizes the two in a stacked fashion. First, the dynamic semantic information obtained by EnahncerBERT is directly fed into the deep learning-based sequence network to learn the implicit semantic information and long-range dependencies. Then, refined features are obtained by scoring multi-source biological information using a multi-source feature selection model. Afterwards, based on the integration of the both features mentioned above, we propose an ensemble machine learning classifier to predict enhancers in human cell lines, where SVM [<xref rid="pcbi.1010779.ref034" ref-type="bibr">34</xref>], Deep Forest [<xref rid="pcbi.1010779.ref035" ref-type="bibr">35</xref>] and Random Forest [<xref rid="pcbi.1010779.ref036" ref-type="bibr">36</xref>] are adopted as the individual classifiers of the ensemble model.</p>
      <fig position="float" id="pcbi.1010779.g002">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <p>(a) The overall framework of SMFM. First, enhancer sequences are generated as multi-source biological features and dynamic semantic information utilizing multi-source feature generation and EnhancerBERT, which are then fed into a multi-source feature selector and deep learning-based sequence network, respectively. Finally, the streamlined information is combined as input for the ensemble machine learning classifier to produce the final prediction results. (b) The motif analysis for EnhancerBERT and corresponding interpretation. We extract the attention heads of EnhancerBERT to calculate attention scores of each token, and motifs are found by using filter conditions of attention scores. Corresponding interpretations are performed to analyse attention process of EnhancerBERT and the regions that EnhancerBERT concentrated on. (c) The workflow and function display of the SMFM web server. The web server has three functions: predicting enhancer sequences, motif analysis pipeline and downloading EnhancerBERT models and source codes of SMFM.</p>
        </caption>
        <graphic xlink:href="pcbi.1010779.g002" position="float"/>
      </fig>
      <p><italic toggle="yes">1) Deep learning-based sequence network</italic>: In this section, SMFM first feeds the dynamic semantic vectors into the one dimensional convolutional neural network (1D CNN) to learn the implicit relationships in the enhancer sequences as it has previously shown potential and significance in relation to the local feature extraction and sequence data prediction [<xref rid="pcbi.1010779.ref037" ref-type="bibr">37</xref>]. Then, each layer of SMFM performs a linear transformation of the output of the previous layer by multiplying by a weight matrix. Indeed, each filter in a kernel has different weight parameter matrices, <italic toggle="yes">M</italic>, as well as bias vectors <italic toggle="yes">b</italic>. For each convolution kernel, it scans the original semantic vectors <italic toggle="yes">R</italic><sub><italic toggle="yes">k</italic></sub> with stride size and does matrix multiplication on the scanned area of features according to the perceptual field, then it superimposes the results of the above operations to obtain the bias vector. Mathematically, a convolutional layer is computed as follows:
<disp-formula id="pcbi.1010779.e020"><alternatives><graphic xlink:href="pcbi.1010779.e020.jpg" id="pcbi.1010779.e020g" position="anchor"/><mml:math id="M20" display="block" overflow="scroll"><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo>(</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>·</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic toggle="yes">n</italic> is the number of matrices obtained from the convolution kernel, each <italic toggle="yes">vector</italic> calculated by the above equation characterizes the value of the element at the corresponding position in the matrix <italic toggle="yes">M</italic>. In addition, <italic toggle="yes">ReLU</italic> is an activation function that enables the network to learn complex forms in input data, which can be defined as follows:
<disp-formula id="pcbi.1010779.e021"><alternatives><graphic xlink:href="pcbi.1010779.e021.jpg" id="pcbi.1010779.e021g" position="anchor"/><mml:math id="M21" display="block" overflow="scroll"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd columnalign="right"><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="4pt"/><mml:mi>x</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="4pt"/><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo/></mml:mrow></mml:math></alternatives></disp-formula>
From these, the output of the two layers of the deep convolution network is enriched with implicit semantic relations, which are significant for the representation of the enhancers.</p>
      <p>Simultaneously, to address the long-distance dependencies available in the enhancer sequences, SMFM uses a Bidirectional long-short-term memory network that includes a conditional random field layer in conjunction with attention-based feature modeling to identify DNA enhancers in human cell lines. Compared to traditional recurrent neural networks (RNNs), our model is advantageous in resolving gradient disappearance or explosion, while allowing capturing long-term dependencies. Intuitively, the implicit semantic vectors are presented forwards and backwards in two separate networks available for the enhancer sequences and then connected to the same output layer. The forward LSTM reads an input implicit semantic vector from beginning to end and the backward LSTM reads the same input vector from back to front. Specifically, for the <italic toggle="yes">t</italic>-th time step, the current forgetting factor (<italic toggle="yes">f</italic><sub><italic toggle="yes">t</italic></sub>) can be calculated using the hidden state of the last time step <italic toggle="yes">H</italic><sub><italic toggle="yes">t</italic>−1</sub> and the implicit semantic vector learned from the enhancer sequences of the current time step <italic toggle="yes">I</italic><sub><italic toggle="yes">t</italic></sub>:
<disp-formula id="pcbi.1010779.e022"><alternatives><graphic xlink:href="pcbi.1010779.e022.jpg" id="pcbi.1010779.e022g" position="anchor"/><mml:math id="M22" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic toggle="yes">σ</italic> is the logistic sigmoid function, and <italic toggle="yes">W</italic><sub><italic toggle="yes">f</italic></sub> is a trainable weight of the forget gate in BiLSTM. After that, the model regulates the percentage of the implicit semantic vector <italic toggle="yes">I</italic><sub><italic toggle="yes">t</italic></sub> flowing into the memory cell by using two functional modules. One module controls the inflow percentage by generating a control signal <italic toggle="yes">s</italic><sub><italic toggle="yes">t</italic></sub>, and the other module calculates the candidate memory cell <inline-formula id="pcbi.1010779.e023"><alternatives><graphic xlink:href="pcbi.1010779.e023.jpg" id="pcbi.1010779.e023g" position="anchor"/><mml:math id="M23" display="inline" overflow="scroll"><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> based on the <italic toggle="yes">tanh</italic> layer and <italic toggle="yes">s</italic><sub><italic toggle="yes">t</italic></sub>.
<disp-formula id="pcbi.1010779.e024"><alternatives><graphic xlink:href="pcbi.1010779.e024.jpg" id="pcbi.1010779.e024g" position="anchor"/><mml:math id="M24" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
<disp-formula id="pcbi.1010779.e025"><alternatives><graphic xlink:href="pcbi.1010779.e025.jpg" id="pcbi.1010779.e025g" position="anchor"/><mml:math id="M25" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where the <italic toggle="yes">W</italic><sub><italic toggle="yes">i</italic></sub> and <italic toggle="yes">W</italic><sub><italic toggle="yes">M</italic></sub> represent the trainable weight of the input gate and <italic toggle="yes">M</italic><sub><italic toggle="yes">t</italic></sub> of the model, respectively. Then the new memory cell of the current time step, <italic toggle="yes">M</italic><sub><italic toggle="yes">t</italic></sub> can be obtained, which retains a portion of the dependent information from the previous time step:
<disp-formula id="pcbi.1010779.e026"><alternatives><graphic xlink:href="pcbi.1010779.e026.jpg" id="pcbi.1010779.e026g" position="anchor"/><mml:math id="M26" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
Finally, SMFM can filters the <italic toggle="yes">M</italic><sub><italic toggle="yes">t</italic></sub> by generating a control factor <italic toggle="yes">o</italic><sub><italic toggle="yes">t</italic></sub> to obtain the new output <italic toggle="yes">output</italic><sub><italic toggle="yes">t</italic></sub> of BiLSTM:
<disp-formula id="pcbi.1010779.e027"><alternatives><graphic xlink:href="pcbi.1010779.e027.jpg" id="pcbi.1010779.e027g" position="anchor"/><mml:math id="M27" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.277778em"/><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
<disp-formula id="pcbi.1010779.e028"><alternatives><graphic xlink:href="pcbi.1010779.e028.jpg" id="pcbi.1010779.e028g" position="anchor"/><mml:math id="M28" display="block" overflow="scroll"><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
The loop is repeated and the long-range dependencies of the original semantic features can be learned and aggregated by our SMFM model, resulting in extra significant features and representations of the enhancers.</p>
      <p><italic toggle="yes">2) Ensemble machine learning classifier</italic>: To further boost the performance of enhancer prediction, we established a feature-based ensemble learning classifier to identify DNA enhancers in human cell lines by using fully the interplay between different machine learning algorithms and feature spaces. To demonstrate why we chose these classifiers, we applied different machine learning algorithms to identify DNA enhancers in human cell lines. In a preliminary experiment, we selected the base classifiers from a number of machine learning classifiers including Deep Forest [<xref rid="pcbi.1010779.ref035" ref-type="bibr">35</xref>], XGBoost [<xref rid="pcbi.1010779.ref038" ref-type="bibr">38</xref>], LightGBM [<xref rid="pcbi.1010779.ref039" ref-type="bibr">39</xref>], SVM [<xref rid="pcbi.1010779.ref034" ref-type="bibr">34</xref>], Random Forest [<xref rid="pcbi.1010779.ref036" ref-type="bibr">36</xref>], Logistic Regression [<xref rid="pcbi.1010779.ref040" ref-type="bibr">40</xref>], KNN [<xref rid="pcbi.1010779.ref041" ref-type="bibr">41</xref>] and GBDT [<xref rid="pcbi.1010779.ref042" ref-type="bibr">42</xref>]. In particular, we trained the different base classifiers to predict DNA enhancers, and the performance results of the base classifiers are summarized in <xref rid="pcbi.1010779.t001" ref-type="table">Table 1</xref>. From the results, SVM, Random Forest, and Deep Forest were the top three classifiers in terms of performance, and there is a performance gap between each two classifiers with diversity, which is more suitable for forming the ensemble. Therefore, we finally chose Deep Forest, Random Forest and SVM as the base classifiers of the ensemble classifier. Then, the hard voting scheme is employed to reach the final decision, and it outputs the category with the highest majority of votes in the base classifier:
<disp-formula id="pcbi.1010779.e029"><alternatives><graphic xlink:href="pcbi.1010779.e029.jpg" id="pcbi.1010779.e029g" position="anchor"/><mml:math id="M29" display="block" overflow="scroll"><mml:mrow><mml:mi>v</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:mi>B</mml:mi><mml:mi>M</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mspace width="0.277778em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic toggle="yes">BML</italic><sub><italic toggle="yes">i</italic></sub> represents the label generated by the <italic toggle="yes">i</italic>-th base classifier in the ensemble classifier, and then “1” in each generated label indicates that the sample is an enhancer and a 0 indicates that it is not. <italic toggle="yes">v</italic><sub><italic toggle="yes">t</italic></sub> is the vector that characterizes the <italic toggle="yes">t</italic>-th sequences in dataset. <italic toggle="yes">n</italic> denotes the number of base classifiers in the ensemble model. The classification of the <italic toggle="yes">t</italic>th sequence is judged by the value of <italic toggle="yes">vote</italic>. The <italic toggle="yes">t</italic>th sequence is classified as an enhancer with a <italic toggle="yes">vote</italic> &gt; 0.5, otherwise it is classified as a non-enhancer.</p>
      <table-wrap position="float" id="pcbi.1010779.t001">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Results for each base classifier on the training set assessed by four metrics.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pcbi.1010779.t001" id="pcbi.1010779.t001g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Classifier</th>
                <th align="left" rowspan="1" colspan="1">ACC (%)</th>
                <th align="left" rowspan="1" colspan="1">MCC</th>
                <th align="left" rowspan="1" colspan="1">SN (%)</th>
                <th align="left" rowspan="1" colspan="1">SP (%)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">Deep Forest</td>
                <td align="left" rowspan="1" colspan="1">82</td>
                <td align="char" char="." rowspan="1" colspan="1">0.651</td>
                <td align="char" char="." rowspan="1" colspan="1">83.25</td>
                <td align="char" char="." rowspan="1" colspan="1">81.5</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">SVM</td>
                <td align="left" rowspan="1" colspan="1">66.91</td>
                <td align="char" char="." rowspan="1" colspan="1">0.348</td>
                <td align="char" char="." rowspan="1" colspan="1">78.56</td>
                <td align="char" char="." rowspan="1" colspan="1">55.27</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Random Forest</td>
                <td align="left" rowspan="1" colspan="1">69.89</td>
                <td align="char" char="." rowspan="1" colspan="1">0.408</td>
                <td align="char" char="." rowspan="1" colspan="1">68.75</td>
                <td align="char" char="." rowspan="1" colspan="1">70.92</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">GDBT</td>
                <td align="left" rowspan="1" colspan="1">66.72</td>
                <td align="char" char="." rowspan="1" colspan="1">0.338</td>
                <td align="char" char="." rowspan="1" colspan="1">74.16</td>
                <td align="char" char="." rowspan="1" colspan="1">59.28</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Logistic Regression</td>
                <td align="left" rowspan="1" colspan="1">65.93</td>
                <td align="char" char="." rowspan="1" colspan="1">0.327</td>
                <td align="char" char="." rowspan="1" colspan="1">72.41</td>
                <td align="char" char="." rowspan="1" colspan="1">59.47</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">KNN</td>
                <td align="left" rowspan="1" colspan="1">62.14</td>
                <td align="char" char="." rowspan="1" colspan="1">0.243</td>
                <td align="char" char="." rowspan="1" colspan="1">64.65</td>
                <td align="char" char="." rowspan="1" colspan="1">59.64</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">LightGBM</td>
                <td align="left" rowspan="1" colspan="1">66.76</td>
                <td align="char" char="." rowspan="1" colspan="1">0.339</td>
                <td align="char" char="." rowspan="1" colspan="1">74.19</td>
                <td align="char" char="." rowspan="1" colspan="1">59.37</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">XGBoost</td>
                <td align="left" rowspan="1" colspan="1">65.46</td>
                <td align="char" char="." rowspan="1" colspan="1">0.311</td>
                <td align="char" char="." rowspan="1" colspan="1">70.96</td>
                <td align="char" char="." rowspan="1" colspan="1">59.97</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
    </sec>
    <sec id="sec006">
      <title>D. Parameter settings</title>
      <p>The details of the parameter settings for SMFM and the other machine learning algorithms are described below.</p>
      <p><italic toggle="yes">1) Parameters of SMFM</italic>:SMFM contains a number of tunable hyperparameters, which can be specifically divided into the hyperparameters of deep learning-based sequence network and ensemble machine learning classifier. During the optimization of these parameters, we assign the search space for each parameter and explore their optimal combination using 10-fold cross-validation and grid search. After that, the average MCC values (see below) from ten rounds of cross-validation are calculated as the criterion for selecting the parameter combinations. The hyperparameters of SMFM contain mainly the size of the convolution kernel, the number of filters in the convolution layer and the units of BiLSTM. We assign their search spaces as {{1,3}, {3,3}, {3,5}, {5,5}}, {16, 32, 64, 128} and {16, 32, 64, 128}. After optimization, we eventually choose the parameter combinations of 3, 3, 32, 16 and 16, representing the kernel sizes of the first and second convolutional layers, the number of filters in the first and second convolutional layers and the untis of the BiLSTM network, respectively. Indeed, the deep learning-based sequence network is trained using the tensorflow version 2.5.1, and the parameter distribution of each hidden layer in the model adopts the default version of tensorflow. To prevent overfitting, we apply the early stopping method in the training. The hyperparameters of the ensemble machine learning classifier are divided into three components: the first are the hyperparameters of Deep Forest(DF) [<xref rid="pcbi.1010779.ref035" ref-type="bibr">35</xref>], where we mainly tune the number of estimators in each cascade layer, the number of trees in each estimator, the maximum depth of the cascade forest, decide whether to connect additional predictors at the end, and the type of predictors. Second, we optimize the kernel function in the SVM [<xref rid="pcbi.1010779.ref034" ref-type="bibr">34</xref>] as well as the values of gamma and C. Third, there are the hyperparameters of random forest [<xref rid="pcbi.1010779.ref036" ref-type="bibr">36</xref>], which consists of a function measuring the quality of the split and the number of estimators in each tree. <xref rid="pcbi.1010779.t002" ref-type="table">Table 2</xref> summarizes the search space of the ensemble learning classifier and the optimal combination for each hyperparameter.</p>
      <table-wrap position="float" id="pcbi.1010779.t002">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Best combination of hyperparameters for each classifier.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pcbi.1010779.t002" id="pcbi.1010779.t002g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Base classifier</th>
                <th align="left" rowspan="1" colspan="1">Search space</th>
                <th align="left" rowspan="1" colspan="1">Best combination</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="5" colspan="1">Deep Forest</td>
                <td align="left" rowspan="1" colspan="1">‘n_estimators’: {50, 55, 60, 65};</td>
                <td align="left" rowspan="5" colspan="1">{65, ‘True’, ‘lightgbm’, 30, 25}</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘use_predictor’: {‘True’, ‘False’};</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘predictor’: {‘xgboost’, ‘lightgbm’, ‘forest’};</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘max_layers’: {10, 20, 30, 40};</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘n_trees’: {20, 25, 30}</td>
              </tr>
              <tr>
                <td align="left" rowspan="3" colspan="1">SVM</td>
                <td align="left" rowspan="1" colspan="1">‘C’: {5, 10, 15, 20};</td>
                <td align="left" rowspan="3" colspan="1">{5, 1e-3, ‘poly’}</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘gamma’: {1e-3, 5e-3, 1e-4};</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘kernel’: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’};</td>
              </tr>
              <tr>
                <td align="left" rowspan="2" colspan="1">Random Forest</td>
                <td align="left" rowspan="1" colspan="1">‘n_estimators’: {60, 65, 70, 75};</td>
                <td align="left" rowspan="2" colspan="1">{75, ‘gini’}</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘criterion’: {‘gini’, ‘entropy’}</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p><italic toggle="yes">2) Deep learning algorithms</italic>: To elucidate the effectiveness of our proposed model, we compare SMFM with several deep learning models including CNN, RNN and ResNet-1D, a residual network with 1D convolution layers. For CNN, we mainly adjust the number of hidden layers, the number of filters in each layer, the size of the convolution kernel and the learning rate. For RNN, the learning rate and the number of units in hidden layers are selected to optimize RNN. For ResNet-1D, the number of convolution blocks and the activation function are tuned to achieve the best performance. Hyperparameters tuning of these models is performed by grid search.</p>
      <p><italic toggle="yes">3) Machine learning algorithms</italic>: In terms of machine learning algorithms, XGBoost [<xref rid="pcbi.1010779.ref038" ref-type="bibr">38</xref>], LightGBM [<xref rid="pcbi.1010779.ref039" ref-type="bibr">39</xref>], SVM [<xref rid="pcbi.1010779.ref034" ref-type="bibr">34</xref>], Random Forest [<xref rid="pcbi.1010779.ref036" ref-type="bibr">36</xref>], Logistic Regression [<xref rid="pcbi.1010779.ref040" ref-type="bibr">40</xref>], KNN [<xref rid="pcbi.1010779.ref041" ref-type="bibr">41</xref>] and GBDT [<xref rid="pcbi.1010779.ref042" ref-type="bibr">42</xref>] are employed to compare performance to SMFM. The version of XGBoost [<xref rid="pcbi.1010779.ref038" ref-type="bibr">38</xref>] is 1.5.1, the version of LightGBM [<xref rid="pcbi.1010779.ref039" ref-type="bibr">39</xref>] is 3.3.1, and the rest of the model is implemented under the scikit-learn package [<xref rid="pcbi.1010779.ref043" ref-type="bibr">43</xref>]. In our experiments, we utilize the grid search method to find the optimal parameters for each model.</p>
    </sec>
    <sec id="sec007">
      <title>E. Evaluation metrics</title>
      <p>We use accuracy (ACC), Matthews correlation coefficient (MCC), sensitivity (SN), and specificity (SP) to evaluate the enhancer identification performance of our models.</p>
      <p>For DNA enhancer identification, the prediction results can be divided into four categories: true positive (TP), false positive (FP), true negative (TN) and false negative (FN). ACC is the ratio of the number of correctly classified samples to the number of all samples, which most intuitively represents how well a model performs in correctly classifying samples as follows:
<disp-formula id="pcbi.1010779.e030"><alternatives><graphic xlink:href="pcbi.1010779.e030.jpg" id="pcbi.1010779.e030g" position="anchor"/><mml:math id="M30" display="block" overflow="scroll"><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
<italic toggle="yes">SN</italic> is the proportion of true positive samples classified as positive, which characterizes the sensitivity of the model to positive samples.
<disp-formula id="pcbi.1010779.e031"><alternatives><graphic xlink:href="pcbi.1010779.e031.jpg" id="pcbi.1010779.e031g" position="anchor"/><mml:math id="M31" display="block" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
As opposed to <italic toggle="yes">SN</italic>, <italic toggle="yes">SP</italic> represents the sensitivity of the model to negative samples, i.e., the proportion of true negative samples among those classified as negative.
<disp-formula id="pcbi.1010779.e032"><alternatives><graphic xlink:href="pcbi.1010779.e032.jpg" id="pcbi.1010779.e032g" position="anchor"/><mml:math id="M32" display="block" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
MCC is a metric applied to measure the balanced performance of a binary classification model that considers simultaneously TP, TN, FP and FN to obtain a fair result when an imbalance exists in the dataset:
<disp-formula id="pcbi.1010779.e033"><alternatives><graphic xlink:href="pcbi.1010779.e033.jpg" id="pcbi.1010779.e033g" position="anchor"/><mml:math id="M33" display="block" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo><mml:mo>·</mml:mo><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo><mml:mo>·</mml:mo><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>)</mml:mo><mml:mo>·</mml:mo><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac><mml:mspace width="0.277778em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
Indeed, MCC characterizes the correlation coefficient between the actual and the predicted classifications, with a value of 1 indicating that the model achieves a perfect performance of the problem, and a value of -1, indicating that the classifier performs even worse than a random prediction.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec008">
    <title>Results and discussion</title>
    <p>We carried out several experiments to elaborate the effectiveness of our proposed algorithm. At first, we performed ablation experiments on a variety of biological features using the training set to demonstrate the superiority of the features we use. On this basis, we also compared SMFM with some classic deep learning networks and machine learning models. In addition, to elucidate the importance of the dynamic semantic information in the model, we compared the performance of EnhancerBERT models based on different k-mers used to tokenize the enhancer sequences. Moreover, we used an independent test set to compare the performance of SMFM to already existing enhancer prediction models to further investigate the superior performance of SMFM. Finally, we performed motif and interpretability analysis based on the EnhancerBERT attention in SMFM.</p>
    <sec id="sec009">
      <title>A. Multi-source feature descriptors importance analysis</title>
      <p>To begin with, we compared the performance of different types of features including multi-source biological feature-encoding and EnhancerBERT on SMFM, and the results are presented in <xref rid="pcbi.1010779.g003" ref-type="fig">Fig 3A</xref>, where we see that the fusion of the two feature types is better than the individual feature alone. The values of the evaluation metrics of the model after the fusion of the two features were 84.93% ± 0.017, 0.698 ± 0.034, 84.35% ± 0.027, and 85.62% ± 0.026, respectively.</p>
      <fig position="float" id="pcbi.1010779.g003">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <p>(a) shows the experimental results of ablation of two groups of feature encodings on SMFM, where the fusion of the two feature types achieves best performance; (b) Ablation experiment of multi-source biological features in SMFM, showing percentage of variance of each ablation experiment; (c) illustrates performance of different feature selection methods, where multi-source feature selection can select feature set better than other feature selection methods; (d) compares the specific effects of gap values of PGKM features on the final performance; as the gap value increases, the performance increases.</p>
        </caption>
        <graphic xlink:href="pcbi.1010779.g003" position="float"/>
      </fig>
      <p>To further verify the effectiveness of the components including Positional gapped k-m-tuple pairs (PGKM), Pseudo K-tuple nucleotide composition (PseKNC), Nucleotide physicochemical properties (NPCP), and their combinations in the multi-source biological feature encoding methods, we performed ablation experiments on them. Specifically, four different experiments were conducted to compare the result of removing each of the four encoding methods from the feature set. The experimental results are shown in <xref rid="pcbi.1010779.g003" ref-type="fig">Fig 3B</xref>. Each individual feature in the figure represents the performance obtained after removing the particular feature. Having all features gets the highest metric values for all four metrics. It is worth mentioning that the stability of the ablated model (with ACC standard deviation value of 0.041) is lower than that of the complete model (with ACC standard deviation value of 0.017) in the cross-validation, indicating that using multi-source biological features, SMFM is able to capture divergent aspects of sequences to support prediction.</p>
      <p>In addition, to demonstrate the effectiveness of our proposed multi-source feature selection, we compared our model with different feature selection methods that replace the multi-source feature selection of SMFM to conduct a fair gcomparison. The experimental results are presented in <xref rid="pcbi.1010779.g003" ref-type="fig">Fig 3C</xref>, and confirm that the refined features generated by our method brings a significantly better performance than the other feature selection methods. After training, the features encoded by our method reached an ACC of 84.93% ± 0.017 and MCC of 0.698 ± 0.034, which is about 4.37% and 6.79% higher, respectively than the best performance of any of the other feature selection methods.</p>
      <p>In addition, we analysed the effect of different <italic toggle="yes">gap</italic> values in the positional gapped k-m-tuple pairs (PGKM), by testing the performance of the PGKM features with <italic toggle="yes">gap</italic> values of 1,2,3,4 and 5, respectively. The results generated for each gap value are illustrated in <xref rid="pcbi.1010779.g003" ref-type="fig">Fig 3D</xref> that indicates that the performance obtained for <italic toggle="yes">gap</italic> = 5 is optimal since the features with higher <italic toggle="yes">gap</italic> values encapsulate features with lower <italic toggle="yes">gap</italic> values, i.e., features generated for <italic toggle="yes">gap</italic> &lt; 5 are a subset of <italic toggle="yes">gap</italic> = 5, which assists in retaining a portion of the short-distance dependencies in enhancer sequences.</p>
    </sec>
    <sec id="sec010">
      <title>B. The impact of different natural language processing techniques</title>
      <p>To evaluate the effect of <italic toggle="yes">k</italic> values on the identification performance of our model, we tokenized sequences into 3mers, 4mers, 5mers and 6mers to fine-tune different EnhancerBERT models, and separately tested the performance of the dynamic semantic information generated by these different EnhancerBERT models for comparison. The results are summarized in <xref rid="pcbi.1010779.g004" ref-type="fig">Fig 4A</xref>. Through cross validation, we observe that the performance of the EnhancerBERT models are 75.75% ± 0.023, 75.00% ± 0.027, 73.50% ± 0.028, and 74.50% ± 0.021, respectively. To further explore the reason why the 3mer-model achieves the best performance, we calculated the Pearson correlation coefficient between every two features in the k-mer EnhancerBERT and clustered the features based on this to obtain the corresponding correlation heat map. <xref rid="pcbi.1010779.g004" ref-type="fig">Fig 4C</xref> shows the correlation heatmaps with different dynamic semantic information, where it can be observed that 3mers provides the best correlation compared to the other groups, both in terms of degree of correlation and aggregation. From the point of view of performance and correlations, we choose 3mers for the fine-tuning and performed dynamic semantic information extraction of the enhancer sequences.</p>
      <fig position="float" id="pcbi.1010779.g004">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <p>(a) ACC performance of different k-mer EnhancerBERT models, where the 3mer-model achieves the best performance over all models; (b) values of four metrics for assessing performance of EnhancerBERT versus the different NLP technologies, showing that dynamic semantic information in EnhancerBERT has the best characterization capability; (c) compares degree of correlation of different k-mer dynamic semantic information using Pearson correlation coefficient, 3mer-model has the clearest correlationship between features, which support SMFM in identifying enhancers.</p>
        </caption>
        <graphic xlink:href="pcbi.1010779.g004" position="float"/>
      </fig>
      <p>To investigate the advantage of applying a dynamic semantic information to SMFM, we conducted an experiment comparing EnhancerBERT to existing several static NLP methods, including Word2Vec, FastText, GloVe and Doc2Vec. The results are summarized in <xref rid="pcbi.1010779.g004" ref-type="fig">Fig 4B</xref>. Dynamic semantic information from EnhancerBERT obtains the highest values for all four metrics (ACC of 84.93% ± 0.017 and MCC of 0.698 ± 0.034), well above the metric values of Word2Vec (78.11% ± 0.025 and 0.566 ± 0.046), GloVe (78.37% ± 0.025 and 0.571 ± 0.049), Doc2Vec (78.23% ± 0.026 and 0.568 ± 0.047) and FastText (77.96% ± 0.021 and 0.562 ± 0.056). Further, we compared the performance results of EnhancerBERT with other static NLP methods using the t-test, with <italic toggle="yes">p</italic>-values of 1.9e-2 (Word2Vec), 2.1e-2 (Doc2Vec), 2.2e-2 (GloVe) and 1.8e-2 (Fasttext), respectively, indicating that improvements were significant with EnhancerBERT. We can observe that there is some difference in the sensitivity of the static NLP features to positive and negative samples, and dynamic semantic information can eliminate the difference. Benefit from fine-tune process and multi-head self-attention mechanism, dynamic semantic information contain more relationships about the token position and the dependencies between each nucleotide and its context, resulting in better performance than static NLP technologies. Based on the results, EnhancerBERT model can fully capture the general global contextual characteristics of enhancer sequences.</p>
    </sec>
    <sec id="sec011">
      <title>C. The Ablation Analysis of the SMFM Model</title>
      <p>To illustrate the necessity of each module in the SMFM, we performed an ablation analysis for each of its modules. Specifically, we ablated each component of SMFM, including the deep learning-based sequence network fusing CNN and RNN, the stack-based ensemble learning classifier, and each base classifier inside its stack, resulting in the following six scenarios: 1) Remove multi-source feature selection from SMFM and the original multi-source biological features are fed directly into the model, called SMFM<sub><italic toggle="yes">NFS</italic></sub>; 2) Remove the ensemble machine learning classifier from SMFM and directly use the deep learning-based sequence network for prediction, called SMFM<sub><italic toggle="yes">Nensemble</italic></sub>; 3) Remove the deep learning-based sequence network from SMFM, called SMFM<sub><italic toggle="yes">NDL</italic></sub>; 4) Remove SVM from the ensemble machine learning classifier, called SMFM<sub><italic toggle="yes">NSVM</italic></sub>; 5) Remove deep forest from the ensemble machine learning classifier, called SMFM<sub><italic toggle="yes">NDF</italic></sub>; 6) Remove random forest from ensemble machine learning classifier, called SMFM<sub><italic toggle="yes">NRF</italic></sub>. The experimental results are summarized in <xref rid="pcbi.1010779.g005" ref-type="fig">Fig 5</xref> of assessment by four evaluation metrics. We can observe in <xref rid="pcbi.1010779.g005" ref-type="fig">Fig 5</xref>, that SMFM outperforms all the altered cases (highest ACC value of 84.93% ± 0.017, MCC value of 0.698 ± 0.034, SN value of 84.35% ± 0.027 and SP value of 85.62% ± 0.026). By comparing SMFM<sub><italic toggle="yes">NFS</italic></sub> to SMFM, we see that the feature selection module in SMFM not only improves the prediction performance of the model, but also reduces the number of features of multi-source biological information from 14,891 to 472, thereby significantly reducing the computational time of the model. Comparing SMFM<sub><italic toggle="yes">NDL</italic></sub> to SMFM, we see SMFM shows better performance, also indicating that the deep learning-based sequence network can learn potential features more effectively and capture the implicit relationships and long-distance dependencies, which has a positive impact on the overall performance of the algorithm. Moreover, from the results of SMFM<sub><italic toggle="yes">Nensemble</italic></sub>, SMFM<sub><italic toggle="yes">NSVM</italic></sub>, SMFM<sub><italic toggle="yes">NDL</italic></sub> and SMFM<sub><italic toggle="yes">NDF</italic></sub>, it can be seen that the ensemble of these three machine learning classifiers has a significant impact on the final identification results. In addition, the sensitivity (SN) and specificity (SP) performance analyses in <xref rid="pcbi.1010779.g005" ref-type="fig">Fig 5</xref> demonstrates that SMFM<sub><italic toggle="yes">NRF</italic></sub> and SMFM<sub><italic toggle="yes">NDF</italic></sub> are comparable; nevertheless, the bias for positive and negative samples is notably different. This phenomenon can be removed when RF, DF and SVM are combined for identification, justifying the combining of these three classifiers. In summary, each module of SMFM is reasonable and valid.</p>
      <fig position="float" id="pcbi.1010779.g005">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>Performance of the different SMFM ablated architectures of SMFM, with ACC, SN and SP values as percent units.</title>
        </caption>
        <graphic xlink:href="pcbi.1010779.g005" position="float"/>
      </fig>
    </sec>
    <sec id="sec012">
      <title>D. SMFM is superior to other deep learning architectures</title>
      <p>To demonstrate the effectiveness of our proposed SMFM, we compared our proposed model with several deep learning architectures including CNN architectures, BiLSTM networks with attention mechanism and ResNet-1D, on the same dataset. <xref rid="pcbi.1010779.g006" ref-type="fig">Fig 6A</xref> displays the results of the different architectures, showing SMFM obtains ACC and MCC values of 84.93% ± 0.017 and 0.698 ± 0.034, respectively, which is the best performance of all four models. For the other models, CNN, BiLSTM and ResNet-1D obtained ACCs of 80.93% ± 0.028, 80.15% ± 0.021 and 81.87% ± 0.016 and MCCs of 0.61 ± 0.056, 0.60 ± 0.044 and 0.64 ± 0.031, respectively, indicating that the learning capability of SMFM is much stronger than a single deep learning model as it synergizes deep learning and machine learning. Moreover, we also note that the results of SMFM are 84.35% ± 0.027 and 85.62% ± 0.026 for SN and SP, respectively, while the results of the other three deep learning models are 80.51% ± 0.075, 77.88% ± 0.024, and 82.82% ± 0.027 for SN and 79.41% ± 0.054, 82.43% ± 0.030, and 80.93% ± 0.042 for SP, revealing that SMFM better addresses the large variability between sequences compared to the other three single deep learning models.</p>
      <fig position="float" id="pcbi.1010779.g006">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.g006</object-id>
        <label>Fig 6</label>
        <caption>
          <p>(a) exhibits performance of different deep learning architectures in comparison with SMFM, each box represents four metric values of different architectures; (b) shows performance of different machine learning algorithms in contrast to SMFM, red line shows average value of four metric values of each method; (c) indicating performance of SMFM compared to current state-of-the-art models of enhancer identification on an independent test set, each sub-figure represents comparison result of one of four metrics used in experiments.</p>
        </caption>
        <graphic xlink:href="pcbi.1010779.g006" position="float"/>
      </fig>
    </sec>
    <sec id="sec013">
      <title>E. SMFM can provide better performance than several machine learning models</title>
      <p>To verify further the effectiveness of SMFM in enhancer identification, we compared our proposed model to seven machine learning models, including XGBoost, LightGBM, SVM, Random Forest (RF), Logistic Regression (LR), KNN, and GBDT. We performed a grid search for each algorithm to achieve the best performance on the dataset, and the detailed information on the tuning parameters of each algorithm can be found in <xref rid="pcbi.1010779.t003" ref-type="table">Table 3</xref>. As can be seen in <xref rid="pcbi.1010779.g006" ref-type="fig">Fig 6B</xref>, SMFM achieved the best results in all four metrics. SMFM achieved 4.41%, 8.69%, 5.73%, and 2.27% higher values respectively than the other machine learning models for the four metrics. The significant improvement in MCC demonstrates the higher stability of SMFM compared to general machine learning models. Notably, after SMFM, the GDBT classifier obtained better results than the rest of the models, further revealing the effectiveness of ensemble learning in enhancer classification.</p>
      <table-wrap position="float" id="pcbi.1010779.t003">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.t003</object-id>
        <label>Table 3</label>
        <caption>
          <title>Reference parameters for each of the machine learning algorithms.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pcbi.1010779.t003" id="pcbi.1010779.t003g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">ML algorithm</th>
                <th align="left" rowspan="1" colspan="1">Search space</th>
                <th align="left" rowspan="1" colspan="1">Best combination</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="4" colspan="1">XGBoost</td>
                <td align="left" rowspan="1" colspan="1">‘max_depth’: {4, 6, 8, 10};</td>
                <td align="left" rowspan="4" colspan="1">{6, 0.9, 0.5, 0.1}</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘subsample’: {0.5, 0.7, 0.9, 1.0};</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘colsample_bytree’: {0.5, 0.7, 0.9, 1.0};</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘learning_rate’: {0.05, 0.1, 0.15, 0.2};</td>
              </tr>
              <tr>
                <td align="left" rowspan="3" colspan="1">SVM</td>
                <td align="left" rowspan="1" colspan="1">‘C’: {5, 10, 15, 20};</td>
                <td align="left" rowspan="3" colspan="1">{5, 1e-3, ‘poly’}</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘gamma’: {1e-3, 5e-3, 1e-4};</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘kernel’: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’};</td>
              </tr>
              <tr>
                <td align="left" rowspan="3" colspan="1">LR</td>
                <td align="left" rowspan="1" colspan="1">‘penalty’: {‘l1’, ‘l2’, ‘elasticnet’, ‘none’};</td>
                <td align="left" rowspan="3" colspan="1">{‘l2’, ‘liblinear’, 100}</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘solver’: {‘liblinear’, ‘lbfgs’, ‘sag’, ‘newton-cg’};</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘max_iter’: {50, 100, 150, 200};</td>
              </tr>
              <tr>
                <td align="left" rowspan="3" colspan="1">KNN</td>
                <td align="left" rowspan="1" colspan="1">‘weights’: {‘uniform’, ‘distance’};</td>
                <td align="left" rowspan="3" colspan="1">{‘distance’, 35, ‘euclidean’}</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘leaf_size’: {25, 30, 35, 40};</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘metric’: {‘euclidean’, ‘manhattan’, ‘chebyshev’};</td>
              </tr>
              <tr>
                <td align="left" rowspan="3" colspan="1">GDBT</td>
                <td align="left" rowspan="1" colspan="1">‘n_estimators’: {50, 75, 100, 125}</td>
                <td align="left" rowspan="3" colspan="1">100, 0.8, 0.6</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘learning_rate’: {0.2, 0.4, 0.6, 0.8}</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘subsample’: {0.5, 0.6, 0.7, 0.8}</td>
              </tr>
              <tr>
                <td align="left" rowspan="2" colspan="1">RF</td>
                <td align="left" rowspan="1" colspan="1">‘n_estimators’: {60, 65, 70, 75}</td>
                <td align="left" rowspan="2" colspan="1">{60, ‘gini’}</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘criterion’: {‘gini’, ‘entropy’}</td>
              </tr>
              <tr>
                <td align="left" rowspan="4" colspan="1">LightGBM</td>
                <td align="left" rowspan="1" colspan="1">‘learning_rate’: {0.05, 0.07, 0.09, 0.1}</td>
                <td align="left" rowspan="4" colspan="1">{0.1, 100, 4, 0.9}</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘n_estimators’: {50, 75, 100, 125}</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘max_depth’: {3, 4, 5, 6}</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">‘subsample’: {0.8, 0.9, 1.0}</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
    </sec>
    <sec id="sec014">
      <title>F. Comparison with existing enhancer identification methods</title>
      <p>To further demonstrate the generalization performance and stability of SMFM, we compared SMFM with a number of existing enhancer identification models including iEnhancer-2L [<xref rid="pcbi.1010779.ref014" ref-type="bibr">14</xref>], EnhancerPred [<xref rid="pcbi.1010779.ref015" ref-type="bibr">15</xref>], iEnhancer-EL [<xref rid="pcbi.1010779.ref016" ref-type="bibr">16</xref>], iEnhancer-XG [<xref rid="pcbi.1010779.ref018" ref-type="bibr">18</xref>], iEnhancer-EBLSTM [<xref rid="pcbi.1010779.ref022" ref-type="bibr">22</xref>], iEnhancer-ECNN [<xref rid="pcbi.1010779.ref019" ref-type="bibr">19</xref>], BERT-2DCNNs [<xref rid="pcbi.1010779.ref024" ref-type="bibr">24</xref>], and Enhancer-IF [<xref rid="pcbi.1010779.ref021" ref-type="bibr">21</xref>] on an independent test set. iEnhancer-2L [<xref rid="pcbi.1010779.ref014" ref-type="bibr">14</xref>] is a two-layer classifier built on an SVM model, where the first layer is used to identify whether the sequence is an enhancer and the second layer classifies the strength of the enhancer sequence. EnhancerPred [<xref rid="pcbi.1010779.ref015" ref-type="bibr">15</xref>] also uses an SVM model to build the corresponding prediction model. iEnhancer-EL [<xref rid="pcbi.1010779.ref016" ref-type="bibr">16</xref>] applies the ensemble learning idea to obtain a two-layer ensemble classifier. iEnhancer-XG [<xref rid="pcbi.1010779.ref018" ref-type="bibr">18</xref>] is a two-layer enhancer identification model built using XGBoost [<xref rid="pcbi.1010779.ref038" ref-type="bibr">38</xref>] and five classical physicochemical features. iEnhancer-EBLSTM [<xref rid="pcbi.1010779.ref022" ref-type="bibr">22</xref>] and iEnhancer-ECNN [<xref rid="pcbi.1010779.ref019" ref-type="bibr">19</xref>] bring deep learning to the enhancer identification problem by building ensemble deep learning networks. BERT-2DCNNs [<xref rid="pcbi.1010779.ref024" ref-type="bibr">24</xref>] construct a 2D CNN network using sequence features extracted from the pre-trained BERT models. Enhancer-IF [<xref rid="pcbi.1010779.ref021" ref-type="bibr">21</xref>] is an approach for investigating the cell specificity of enhancers using five base classifiers to construct the enhancer identification model on eight different cell lines. The results of the comparative analysis are shown in <xref rid="pcbi.1010779.g006" ref-type="fig">Fig 6C</xref> and <xref rid="pcbi.1010779.t004" ref-type="table">Table 4</xref>. The performance results of each method on the training set are shown in <xref rid="pcbi.1010779.s001" ref-type="supplementary-material">S1 Table</xref>.</p>
      <table-wrap position="float" id="pcbi.1010779.t004">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.t004</object-id>
        <label>Table 4</label>
        <caption>
          <title>Results of each model on the independent test set using four metrics.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pcbi.1010779.t004" id="pcbi.1010779.t004g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Methods</th>
                <th align="left" rowspan="1" colspan="1">ACC (%)</th>
                <th align="left" rowspan="1" colspan="1">MCC</th>
                <th align="left" rowspan="1" colspan="1">SN (%)</th>
                <th align="left" rowspan="1" colspan="1">SP (%)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">SMFM</td>
                <td align="left" rowspan="1" colspan="1">
                  <bold>82</bold>
                </td>
                <td align="char" char="." rowspan="1" colspan="1">
                  <bold>0.651</bold>
                </td>
                <td align="left" rowspan="1" colspan="1">
                  <bold>83.25</bold>
                </td>
                <td align="left" rowspan="1" colspan="1">
                  <bold>81.5</bold>
                </td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">iEnhancer-2L</td>
                <td align="left" rowspan="1" colspan="1">73</td>
                <td align="char" char="." rowspan="1" colspan="1">0.460</td>
                <td align="left" rowspan="1" colspan="1">71</td>
                <td align="left" rowspan="1" colspan="1">75</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">EnhancerPred</td>
                <td align="left" rowspan="1" colspan="1">74</td>
                <td align="char" char="." rowspan="1" colspan="1">0.480</td>
                <td align="left" rowspan="1" colspan="1">73.5</td>
                <td align="left" rowspan="1" colspan="1">74.5</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">iEnhancer-EL</td>
                <td align="left" rowspan="1" colspan="1">74.75</td>
                <td align="char" char="." rowspan="1" colspan="1">0.496</td>
                <td align="left" rowspan="1" colspan="1">71</td>
                <td align="left" rowspan="1" colspan="1">78.5</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">iEnhancer-XG</td>
                <td align="left" rowspan="1" colspan="1">75.75</td>
                <td align="char" char="." rowspan="1" colspan="1">0.515</td>
                <td align="left" rowspan="1" colspan="1">74</td>
                <td align="left" rowspan="1" colspan="1">77.5</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">BERT-2DCNNs</td>
                <td align="left" rowspan="1" colspan="1">75.6</td>
                <td align="char" char="." rowspan="1" colspan="1">0.514</td>
                <td align="left" rowspan="1" colspan="1">80</td>
                <td align="left" rowspan="1" colspan="1">71.2</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">iEnhancer-EBLSTM</td>
                <td align="left" rowspan="1" colspan="1">77.2</td>
                <td align="char" char="." rowspan="1" colspan="1">0.534</td>
                <td align="left" rowspan="1" colspan="1">75.5</td>
                <td align="left" rowspan="1" colspan="1">79.5</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">iEnhancer-ECNN</td>
                <td align="left" rowspan="1" colspan="1">76.9</td>
                <td align="char" char="." rowspan="1" colspan="1">0.537</td>
                <td align="left" rowspan="1" colspan="1">78.5</td>
                <td align="left" rowspan="1" colspan="1">75.2</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Enhancer-IF</td>
                <td align="left" rowspan="1" colspan="1">79.3</td>
                <td align="char" char="." rowspan="1" colspan="1">0.585</td>
                <td align="left" rowspan="1" colspan="1">80.5</td>
                <td align="left" rowspan="1" colspan="1">78.7</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>SMFM achieved the highest performance in all four metrics with values of 82% (ACC), 0.651 (MCC), 83.25% (SN) and 81.5% (SP) on the test set, which proves that SMFM has a superior ability to identify DNA enhancers. Compared to BERT-2DCNNs [<xref rid="pcbi.1010779.ref024" ref-type="bibr">24</xref>], EnhancerBERT in SMFM exhibits a better representation capability. In contrast to several machine learning-based algorithms, SMFM can extract implicit relationships and long-distance dependencies from the original features, which makes the effective information more aggregated. As opposed to the various deep learning-based algorithms, SMFM makes predictions based on ensemble machine learning, which incorporates the diverse perspectives of features. Moreover, it is worth mentioning that the performance results of SMFM on the training and test sets are the closest, while the other methods have a larger gap [<xref rid="pcbi.1010779.ref014" ref-type="bibr">14</xref>–<xref rid="pcbi.1010779.ref016" ref-type="bibr">16</xref>, <xref rid="pcbi.1010779.ref018" ref-type="bibr">18</xref>], which proves that SMFM is able to maintain some stability between datasets containing different information. Based on all the above, SMFM is better tailored to enhancer identification than the existing methods, and has great potential for exploration of enhancer sequences.</p>
    </sec>
    <sec id="sec015">
      <title>G. Motif Analysis learned from SMFM</title>
      <p>To elucidate the ability of SMFM to extract enhancer motifs, we compared our proposed SMFM with BPNet [<xref rid="pcbi.1010779.ref044" ref-type="bibr">44</xref>] on this enhancer dataset. Indeed, BPNet is a general and interpretable deep learning model for learning transcription factor (TF) binding motifs in DNA sequences, and then the learned parameters of BPNet are fed into DeepLIFT and TF-MoDISco to detect the motifs. To conduct a fair comparison, similar to BPNet, we also first input the learned parameters of SMFM to DeepLIFT [<xref rid="pcbi.1010779.ref045" ref-type="bibr">45</xref>] to backtrack signals from the last layer of the two models to calculate the contribution scores of different bases in a sequence to the final identification result, respectively, thus identifying DNA fragments with high contribution scores from the complete sequence. After that, the TF-MoDISco tool [<xref rid="pcbi.1010779.ref046" ref-type="bibr">46</xref>] was used to scan and cluster the obtained fragments and highlight the significant regions within the sequences by the feature importance scores, and motifs are then aggregated by aligning fragments from each cluster. On this basis, we finally identified 56 motifs with widths ranging from 15 to 62 for SMFM while 47 motifs with widths ranging 11 to 69 for BPNet.</p>
      <p>To further verify the validity of the motifs captured by the two algorithms, we extracted the corresponding position weight matrices (PWM) from fragments clusters identified by SMFM and BPNet, respectively and visualized the motifs according to the sequence background of enhancer dataset (0.284 for A and T and 0.216 for C and G). Then, we input the PWMs of the two sets of motifs obtained by SMFM and BPNet into the TOMTOM algorithm [<xref rid="pcbi.1010779.ref047" ref-type="bibr">47</xref>] separately for comparison with experimentally verified motifs in the transcription factor motif database, JASPAR CORE [<xref rid="pcbi.1010779.ref048" ref-type="bibr">48</xref>] with a significant E-value threshold of 0.05. <xref rid="pcbi.1010779.s002" ref-type="supplementary-material">S2 Table</xref>. summarized the comparison of the meaningful motifs detected by SMFM and BPNet, SMFM finally obtained 45 sets of comparison results corresponding to 28 motifs with different IDs in the database, while BPNet obtained 28 sets corresponding to 17 motifs with different IDs. From the table, we observe that the meaningful motifs obtained by SMFM captured the majority of the motifs obtained by BPNet. Moreover, SMFM is able to detect more normal and reverse complementary motifs compared to BPNet (e.g., MA1274.1, MA1403.1, MA0528.1, MA0538.1, etc.). In summary, BPNet is a motif detection tool for a wide range of gene sequences, while SMFM integrates dynamic semantic information for enhancer sequences and multi-source biological properties, thus providing a more comprehensive performance for detecting motifs in enhancers than BPNet.</p>
      <p>For easy reference, we put the results of the comparison of the motifs obtained from SMFM and BPNet with those from the JASPAR database in <xref rid="pcbi.1010779.s003" ref-type="supplementary-material">S3</xref> and <xref rid="pcbi.1010779.s004" ref-type="supplementary-material">S4</xref> Tables, respectively, which are also available in the SMFM web server <ext-link xlink:href="http://39.104.69.176:5010/" ext-link-type="uri">http://39.104.69.176:5010/</ext-link>. In addition, the codes of different computational algorithms for detecting motifs are available at <ext-link xlink:href="https://github.com/no-banana/SMFM-master" ext-link-type="uri">https://github.com/no-banana/SMFM-master</ext-link>.</p>
    </sec>
    <sec id="sec016">
      <title>H. Interpretability analysis of SMFM</title>
      <p>To verify the effectiveness of extracting dynamic semantic information from EnhancerBERT, as shown in Figs <xref rid="pcbi.1010779.g007" ref-type="fig">7</xref> and <xref rid="pcbi.1010779.g008" ref-type="fig">8</xref>, we explored different aspects of attention weights in EnhancerBERT. In the top half of <xref rid="pcbi.1010779.g007" ref-type="fig">Fig 7</xref>, we provide all the attention heads corresponding to a given sequence in the first attention layer (shown in blue) and the fourth attention layer (shown in red) of EnhancerBERT. It can be seen that after two iterations of the layers, the attention scores of the attention heads in each layer gradually accumulate in some key regions of the sequence that have a large influence on the identification decision.</p>
      <fig position="float" id="pcbi.1010779.g007">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.g007</object-id>
        <label>Fig 7</label>
        <caption>
          <title>Top half shows a bird’s eye view of the attention distribution of the different attention heads in two different layers of EnhancerBERT, the columns represent attention heads of each EnhancerBERT layer, and rows represent layers of EnhancerBERT.</title>
          <p>With iterating of the EnhancerBERT layer, the attention scores of each attention head gradually concentrate in some key regions of input enhancer sequence; Bottom half visualizes the process of attention score calculation, where first and second columns represent <italic toggle="yes">Query</italic> vector and <italic toggle="yes">Key</italic> vector, respectively. The framed up vectors show the two most relevant tokens in the enhancer sequence.</p>
        </caption>
        <graphic xlink:href="pcbi.1010779.g007" position="float"/>
      </fig>
      <fig position="float" id="pcbi.1010779.g008">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.g008</object-id>
        <label>Fig 8</label>
        <caption>
          <p>(a) shows the t-SNE results of deep learning-based sequence network for different hidden layers of the dynamic implicit relation and long-distance dependency process in dynamic contextual features; (b) The top 20 features of dynamic semantic information, the higher the SHAP value, the greater the influence of the feature in the classification; (c) Correlations among the top 20 features of dynamic semantic information; (d) The top 20 features of the multi-source biological features having the highest impact on classification; (e) Feature rankings for enhancer identification, where the two sub-figures above are ranked based on F1 and ACC metrics, both rankings are measured based on random forest classifier building under scikit-learn package. The two sub-figures at the bottom are ranked using RFE and UFS feature selection methods.</p>
        </caption>
        <graphic xlink:href="pcbi.1010779.g008" position="float"/>
      </fig>
      <p>The bottom half of <xref rid="pcbi.1010779.g007" ref-type="fig">Fig 7</xref> demonstrates how the attention head of each layer of EnhancerBERT generated the corresponding attention scores for a given sequence; where <italic toggle="yes">Query q</italic> and <italic toggle="yes">Key k</italic> represent the <italic toggle="yes">Query</italic> vector and <italic toggle="yes">Key</italic> vector in the model. Based on these, the attention scores between different tokens can be calculated according to the formula described in the EnhancerBERT section. In this figure, the positive values are displayed in blue, with higher values becoming darker, while negative values are displayed in orange, with lower values becoming darker. Here, we choose the attention scores of the sequence token ‘ATG’ calculated by the first attention head of the last layer of EnhancerBERT as an example, and observe that the attention values between token ‘ATG’ and other tokens in the selected attention head do not decay noticeably with increasing distance, indicating that EnhancerBERT preserves the long-distance dependence information and short-distance information in the sequence successfully.</p>
      <p>In addition, we explored the contribution of refined multi-source biological features and implicit dynamic semantic information to enhancer identification. The analysis results are shown in <xref rid="pcbi.1010779.g008" ref-type="fig">Fig 8</xref>. To better explain the learning process of dynamic semantic information in SMFM, we extracted the output of each hidden layer in the deep learning-based sequence network during the training process and projected each hidden vector onto a two-dimensional view using t-SNE. As shown in <xref rid="pcbi.1010779.g008" ref-type="fig">Fig 8A</xref>, the first subplot represents the t-SNE results of the original dynamic semantic information that can be understood as the entire sample points not showing any representative clusters. The second subplot displays the t-SNE result after two layers of CNN processing in the deep learning-based sequence network, where the hidden vectors have a regular distribution. The third subplot reveals the t-SNE results after processing by BiLSTM, where we observed a more obvious clustering distribution, indicating that the implicit relationships and information between features have been adequately captured. Finally, we feed the implicit vector into a softmax classifier and yielded the fourth t-SNE subplot with clear classification results. <xref rid="pcbi.1010779.g008" ref-type="fig">Fig 8B</xref> reflects the impact of each feature of the top 20 features of the implicit dynamic semantic information on identification of different DNA enhancer sequences, where higher SHAP values indicate that the particular feature plays a more positive role in the final prediction decision. <xref rid="pcbi.1010779.g008" ref-type="fig">Fig 8C</xref> shows relationships between the top 20 features, where red indicates positive correlation between the features of the row and column, while purple indicates negative correlation. It can be seen that after learning of deep learning-based sequence network in SMFM, the correlation between features is further amplified; for instance, feature 6 has significant positive correlation with features 29, 26 and 10, which indicates these features are synergistic. Similar conclusions can be drawn between features 29, 26 and 31. <xref rid="pcbi.1010779.g008" ref-type="fig">Fig 8D</xref> reflects the influence of each feature of the top refined biological features, where red color indicates a positive effect: the higher the feature value of the feature, the more likely the sequence is predicted to be an enhancer, and the blue color indicates a negative effect: where the higher the feature value, the more likely the sequence is predicted to be a non-enhancer. We see that different features may have various contributions to final output. Therefore, for the best characterization of enhancer sequences it is better to fuse features together. In the next step, we performed single feature ranking analysis on the top 20 features of the refined multi-source biological features by using the random forest model. <xref rid="pcbi.1010779.g008" ref-type="fig">Fig 8E</xref> summarizes the results, where the two histograms at the top show features of the top 20 SHAP value ranking based on ACC and F1 metrics, respectively. Besides, to obtain the ranking of feature importance for different views, we ranked the feature importance of the refined multi-source biological features using recursive feature elimination [<xref rid="pcbi.1010779.ref049" ref-type="bibr">49</xref>] and univariate feature selection [<xref rid="pcbi.1010779.ref050" ref-type="bibr">50</xref>] (as shown in the two histograms at the bottom of <xref rid="pcbi.1010779.g008" ref-type="fig">Fig 8E</xref>). It can be seen that the feature ranking based on SHAP values is totally different from feature ranking based on metrics and feature importance, indicating that there is a large differential expression when characterizing enhancer sequences using only the physicochemical and sequential features. This, on the other hand, reflects the necessity and validity of extracting dynamic semantic information from EnhancerBERT to alleviate this differential expression.</p>
    </sec>
    <sec id="sec017">
      <title>I. SMFM enables efficient characterization of placental-specific enhancers</title>
      <p>The placenta is an essential organ for a successful pregnancy and has a variety of basic functions, including the delivery of nutrients to the developing fetus and the protection of the fetus from infectious diseases [<xref rid="pcbi.1010779.ref051" ref-type="bibr">51</xref>]. Research also indicates that placenta dysfunction is related to pregnancy complications—preeclampsia and preterm birth (PTB), etc [<xref rid="pcbi.1010779.ref052" ref-type="bibr">52</xref>–<xref rid="pcbi.1010779.ref055" ref-type="bibr">55</xref>]. Precise control of gene expression is critical for fetal development during pregnancy, and gene regulatory enhancers play a mediating role in controlling gene expression and contribute significantly to development and disease [<xref rid="pcbi.1010779.ref056" ref-type="bibr">56</xref>–<xref rid="pcbi.1010779.ref058" ref-type="bibr">58</xref>]. Therefore, the identification of active enhancers in placental tissue is extremely crucial. Here we designed an experiment for 4,562 placental enhancers [<xref rid="pcbi.1010779.ref059" ref-type="bibr">59</xref>] and then compared the experimental results of SMFM with other existing enhancer methods. To conduct a fair experiment in the placental enhancers task, we did not perform targeted parameter tuning for all methods used for comparison. We directly used the best hyperparameters of each method obtained from the previous experimental analysis, which can better illustrate the robustness of our algorithm. In our study, we first use 4,562 non-enhancers as negative samples, and utilized different methods to identify placental enhancers. In a second step, we replaced negative samples with the same number of enhancers from the human embryonic kidney cell line (HEK293) to test the ability of different methods to distinguish enhancers in placental tissue. We tested the performance of SMFM, iEnhancer-XG [<xref rid="pcbi.1010779.ref018" ref-type="bibr">18</xref>], iEnhancer-ECNN [<xref rid="pcbi.1010779.ref019" ref-type="bibr">19</xref>], and BERT-2DCNNs [<xref rid="pcbi.1010779.ref024" ref-type="bibr">24</xref>] in this experiment, and the experimental results are summarized in <xref rid="pcbi.1010779.g009" ref-type="fig">Fig 9</xref>. In the first experiment, SMFM identified enhancers very well and achieves the highest values for the five metrics, which are 0.985 of AUC, 0.962 of ACC, 0.923 of MCC, 0.97 of SN and 0.953 of SP. In the second experiment, SMFM also showed strong performance in distinguishing enhancers from different tissues with 0.903 of AUC, 0.827 of ACC, 0.655 of MCC, 0.846 of SN and 0.808 of SP. Although the performance of each method decreased in the task of distinguishing placental enhancers from those in other tissues, SMFM remained the most stable and highest performing method.</p>
      <fig position="float" id="pcbi.1010779.g009">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.g009</object-id>
        <label>Fig 9</label>
        <caption>
          <p>(a) Performance of first step of different enhancer identifying methods compared to SMFM, where the left sub-figure illustrates the AUC performance of SMFM, iEnhancer-XG, iEnhancerECNN and BERT-2DCNNs; (b) shows performance of the second step experiment using different methods.</p>
        </caption>
        <graphic xlink:href="pcbi.1010779.g009" position="float"/>
      </fig>
      <p>After obtaining results of SMFM in the first experiment, we visualized the samples classified as placental enhancer by SMFM on 22 human autosomes and compared them with the distribution of known placental enhancers on these chromosomes from the FANTOM5 atlas [<xref rid="pcbi.1010779.ref060" ref-type="bibr">60</xref>]. The visualization and comparison results are demonstrated in <xref rid="pcbi.1010779.g010" ref-type="fig">Fig 10</xref>. It can be observed that the distribution of positive samples obtained by SMFM (shown in black) is generally consistent with the distribution of placental enhancers on chromosomes in the FANTOM5 atlas (shown in red), indicating SMFM brings accurate and efficient characterization of placental enhancers.</p>
      <fig position="float" id="pcbi.1010779.g010">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.g010</object-id>
        <label>Fig 10</label>
        <caption>
          <title>Visualization of placental enhancers identified by SMFM and FANTOM5 placental enhancers on hg19 autosomes, where red lines indicates placental enhancers from FANTOM5 atlas, the identified placental enhancers are shown using black lines.</title>
          <p>The red regions in autosomes are centromeres, and white regions and regions colored from gray to black represents Giemsa negative and positive regions, respectively. The highly variable and tightly constricted regions on the p-arms of 13, 14, 15, 21, 22 chromosomes that cannot be predicted are shown as blue and gray.</p>
        </caption>
        <graphic xlink:href="pcbi.1010779.g010" position="float"/>
      </fig>
      <p>We then carried out several experiments to validate the relevance of the placental enhancers identified by SMFM from a gene regulation perspective. We first conducted the enrichment analysis including gene ontology (GO) and kyoto encyclopedia of genes and genomes (KEGG) enrichments for genes that are regulated by the placental enhancers identified by SMFM.</p>
      <p><xref rid="pcbi.1010779.g011" ref-type="fig">Fig 11A</xref> shows the top 20 types of GO enrichment ordered by <italic toggle="yes">p</italic>-values. It is worth noting that the top five enriched biological processes of GO are gland development (GO:0048732), wnt signaling pathway (GO:0016055), cell-cell signaling by wnt (GO:0198738), wound healing (GO:0042060) and muscle tissue development (GO:0060537). It can be seen that the majority of enriched biological processes are associated with various tissue development pathways, therefore highly related to development of placenta, successful pregnancy and embryonic development [<xref rid="pcbi.1010779.ref061" ref-type="bibr">61</xref>–<xref rid="pcbi.1010779.ref064" ref-type="bibr">64</xref>]. In addition, the top five enriched cellular components are cell-cell junction (GO:0005911), cell leading edge (GO:0031252), cell-substrate junction (GO:0030055), focal adhesion (GO:0005925) and transcription regulator complex (GO:0005667). The top five enriched molecular functions are GTPase regulator activity (GO:0030695), nucleoside-triphosphatase regulator activity (GO:0060589), GTPase activator activity (GO:0005096), DNA-binding transcription activator activity (GO:0001228) and RNA polymerase II-specific DNA-binding transcription factor binding (GO:0061629). In addition, the result of the KEGG enrichment analysis is summarized in <xref rid="pcbi.1010779.g011" ref-type="fig">Fig 11B</xref>. The left sub-figure displays the top 20 of KEGG enrichments ordered by <italic toggle="yes">p</italic>-values. The pathways can also be annotated and classified as functional categories of KEGG at three different levels, as shown in the right sub-figure, where we learn that the pathways can be divided into four categories for level one, including cellular processes, environmental information processing, human diseases and organismal systems, and different functional categories for level two. Among the pathways, most are critical for early embryonic development. For example, Rap1 signaling pathway, which controls important processes such as cell adhesion, cell-cell junction formation and cell polarity. In addition, the regulation of actin cytoskeleton is responsible for regulating the formation of new individuals from embryonic cells [<xref rid="pcbi.1010779.ref064" ref-type="bibr">64</xref>, <xref rid="pcbi.1010779.ref065" ref-type="bibr">65</xref>]. Based on the above analysis, we can conclude that the genes regulated by the placental enhancers identified by SMFM are highly associated with embryonic development and successful pregnancy, which further validates the effectiveness of SMFM for discerning placental enhancers.</p>
      <fig position="float" id="pcbi.1010779.g011">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.g011</object-id>
        <label>Fig 11</label>
        <caption>
          <title>Genomic enrichment analysis of placental enhancers identified by SMFM.</title>
          <p>(a) The top 20 categories of gene ontology (GO) analysis ordered by <italic toggle="yes">p</italic>-values, including biological process, cellular component, molecular function. (b) shows the top 20 KEGG enrichment pathways ordered by <italic toggle="yes">p</italic>-values and KEGG classified by different functional category levels of KEGG.</p>
        </caption>
        <graphic xlink:href="pcbi.1010779.g011" position="float"/>
      </fig>
    </sec>
    <sec id="sec018">
      <title>J. SMFM outperforms existing methods on a large-scale dataset</title>
      <p>To further validate the predictive ability of SMFM, we investigated its ability on a large-scale dataset from the candidate cis-Regulatory elements (cCREs) in BENGI [<xref rid="pcbi.1010779.ref066" ref-type="bibr">66</xref>]. To construct this dataset, we collected 30,000 enhancer-like sequences from human cCREs that were longer than 400 bp but shorter than 600 bp and truncated them to 400 bp. Then, we excluded the homologous sequences using the CD-HIT tool with a sequence similarity threshold of 60%. Finally, we obtained 26160 sequences as positive samples in our dataset. Of note, due to the shortage of publicly available high-confidence datasets of non-enhancer sequences, inspired by Dao et al [<xref rid="pcbi.1010779.ref067" ref-type="bibr">67</xref>], we sampled each pair of human cCREs more than 400 bp apart as negative samples. Then, we removed homologous sequences that shared &gt;60% of their bases with other non-redundant negative and positive samples. Finally, the dataset contained 26160 positive samples and 26160 negative samples with a sample length of 400bp.</p>
      <p>For comparison on this large dataset, we compared the SMFM algorithm with other baseline methods, including iEnhancer-XG, iEnhancer-ECNN, BERT-2DCNNs, several deep learning and machine learning methods on this dataset. Note that we did not tune the hyperparameters of each method for this dataset in order to better validate the robustness of each method. The experimental results are summarized in <xref rid="pcbi.1010779.g012" ref-type="fig">Fig 12</xref>. It can be seen that SMFM achieved the best performance of all the methods on this large-scale dataset (0.808 for AUC, 0.822 for ACC, 0.655 for MCC, 0.834 for SN, and 0.810 for SP), indicating that SMFM has stronger generalization ability compared with other comparative methods. Moreover, in terms of sensitivity and specificity, SMFM is more balanced for identifying positive and negative samples in the dataset, while the rest of the methods are biased towards the classification of positive samples. In addition, we observed that the method ranking second was iEnhancer-ECNN [<xref rid="pcbi.1010779.ref019" ref-type="bibr">19</xref>], a method that also uses ensemble learning, indicating that ensemble machine learning classifiers are more accurate at predicting regulatory DNA enhancer sequences. In summary, the experiment also validates the powerful predictive ability of our model.</p>
      <fig position="float" id="pcbi.1010779.g012">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1010779.g012</object-id>
        <label>Fig 12</label>
        <caption>
          <title>Performance of other enhancer identification methods compared to SMFM on a large-scale dataset, where the left panel illustrates the AUC performance of SMFM, baseline methods, classical deep learning networks, and multiple machine learning classifiers.</title>
          <p>The right panel shows the four performance measure metrics (ACC, MCC, SN and SP) for each methods.</p>
        </caption>
        <graphic xlink:href="pcbi.1010779.g012" position="float"/>
      </fig>
    </sec>
    <sec id="sec019">
      <title>K. The SMFM web server</title>
      <p>To facilitate use by researchers, we developed a web server for SMFM that allows identifying whether a sequence is an enhancer or not, and this prediction webserver link is available at <ext-link xlink:href="http://39.104.69.176:5010/" ext-link-type="uri">http://39.104.69.176:5010/</ext-link>. The web server guides users in generating dynamic semantic information and multi-source biological features corresponding to their dataset, and then the user receives the user-generated files to make predictions on the dataset. In addition, the successfully submitted jobs and prediction results are sent to the contact address of the users, including the results of each base classifier and the final results using SMFM. Furthermore, we provide the datasets used in this study, including the training set and independent test set, which can be downloaded directly from the web server. Finally, if users are interested, they can also download the corresponding original EnhancerBERT models from the web server.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec020">
    <title>Discussion</title>
    <p>In this study, we propose SMFM, a novel method for identifying and characterizing DNA enhancers using a stacked multivariate fusion model. To gather all the useful information from enhancer sequences, the multi-source biological features and dynamic semantic information are extracted and fused to construct feature schemes with excellent representation. After that, a deep learning-based sequence network synergized by CNN and BiLSTM networks is proposed to retrieve the implicit relations and long-distance dependencies. Then, an ensemble machine learning classifier was developed for training based on the refined multi-source features and dynamic implicit relations obtained from the deep learning-based sequence network to predict DNA enhancers in human cell lines. We evaluated SMFM on a benchmark set including 1484 enhancers and 1484 non-enhancers, and then demonstrated the advantages of SMFM over existing methods on an independent test set. In addition, by conducting motif and interpretable analyses, we explain what SMFM has learned to achieve better performance, while revealing how SMFM focus to key functional fragments of the enhancer sequences. Meanwhile, we also designed an experiment to explore characterization ability of SMFM for tissue-specific enhancers, and the analysis indicated that placental enhancers identified by SMFM are effectively associated with embryo development and normal placental functions such as nutrient transport.</p>
    <p>However, there is still much room to improve. For example, the current graph neural network achieved remarkable results in multiple fields, and we will attempt to model the DNA sequence structure based on the obtained graph data. On the other hand, it will be interesting to define the notions between language model and enhancer sequences to provide more biological interpretability, subject to data availability in the future.</p>
  </sec>
  <sec id="sec021" sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material id="pcbi.1010779.s001" position="float" content-type="local-data">
      <label>S1 Table</label>
      <caption>
        <title>Results of each model on the training set measured by four metrics.</title>
        <p>(XLSX)</p>
      </caption>
      <media xlink:href="pcbi.1010779.s001.xlsx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010779.s002" position="float" content-type="local-data">
      <label>S2 Table</label>
      <caption>
        <title>Visualization of meaningful motifs detected by SMFM and BPNet.</title>
        <p>(XLSX)</p>
      </caption>
      <media xlink:href="pcbi.1010779.s002.xlsx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010779.s003" position="float" content-type="local-data">
      <label>S3 Table</label>
      <caption>
        <title>Visualization of 56 motifs detected by SMFM.</title>
        <p>(XLSX)</p>
      </caption>
      <media xlink:href="pcbi.1010779.s003.xlsx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pcbi.1010779.s004" position="float" content-type="local-data">
      <label>S4 Table</label>
      <caption>
        <title>Visualization of 47 motifs detected by BPNet.</title>
        <p>(XLSX)</p>
      </caption>
      <media xlink:href="pcbi.1010779.s004.xlsx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pcbi.1010779.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Maston</surname><given-names>Glenn A</given-names></name>, <name><surname>Evans</surname><given-names>Sara K</given-names></name>, <name><surname>Green</surname><given-names>Michael R</given-names></name>. <article-title>Transcriptional regulatory elements in the human genome[J]</article-title>. <source>Annu. Rev. Genomics Hum. Genet</source>., <year>2006</year>, <volume>7</volume>: <fpage>29</fpage>–<lpage>59</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1146/annurev.genom.7.080505.115623</pub-id><?supplied-pmid 16719718?><pub-id pub-id-type="pmid">16719718</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Uebbing</surname><given-names>S</given-names></name>, <name><surname>Gockley</surname><given-names>J</given-names></name>, <name><surname>Reilly</surname><given-names>S K</given-names></name>, <name><surname>Kocher</surname><given-names>A A</given-names></name>, <name><surname>Geller</surname><given-names>E</given-names></name>, <name><surname>Gandotra</surname><given-names>N</given-names></name>, <etal>et al</etal>. <article-title>Massively parallel discovery of human-specific substitutions that alter enhancer activity[J]</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <year>2021</year>, <volume>118</volume>(<issue>2</issue>): <fpage>e2007049118</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.2007049118</pub-id><?supplied-pmid 33372131?><pub-id pub-id-type="pmid">33372131</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Smith</surname><given-names>E</given-names></name>, <name><surname>Shilatifard</surname><given-names>A</given-names></name>. <article-title>Enhancer biology and enhanceropathies[J]</article-title>. <source>Nature structural &amp; molecular biology</source>, <year>2014</year>, <volume>21</volume>(<issue>3</issue>): <fpage>210</fpage>–<lpage>219</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nsmb.2784</pub-id><?supplied-pmid 24599251?><pub-id pub-id-type="pmid">24599251</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>W</given-names></name>, <name><surname>Notani</surname><given-names>D</given-names></name>, <name><surname>Rosenfeld</surname><given-names>M G</given-names></name>. <article-title>Enhancers as non-coding RNA transcription units: recent insights and future perspectives[J]</article-title>. <source>Nature Reviews Genetics</source>, <year>2016</year>, <volume>17</volume>(<issue>4</issue>): <fpage>207</fpage>–<lpage>223</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nrg.2016.4</pub-id><?supplied-pmid 26948815?><pub-id pub-id-type="pmid">26948815</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Corradin</surname><given-names>Olivia S</given-names></name>, <name><surname>Peter</surname><given-names>C</given-names></name>. <article-title>Enhancer variants: evaluating functions in common disease[J]</article-title>. <source>Genome medicine</source>, <year>2014</year>, <volume>6</volume>(<issue>10</issue>): <fpage>1</fpage>–<lpage>14</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/s13073-014-0085-3</pub-id><?supplied-pmid 25473424?><pub-id pub-id-type="pmid">24433494</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Matsumura</surname><given-names>K</given-names></name>, <name><surname>Saito</surname><given-names>T</given-names></name>, <name><surname>Takahashi</surname><given-names>Y</given-names></name>, <name><surname>Ozeki</surname><given-names>T</given-names></name>, <name><surname>Kiyotani</surname><given-names>K</given-names></name>, <name><surname>Fujieda</surname><given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Identification of a novel polymorphic enhancer of the human CYP3A4 gene[J]</article-title>. <source>Molecular pharmacology</source>, <year>2004</year>, <volume>65</volume>(<issue>2</issue>): <fpage>326</fpage>–<lpage>334</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1124/mol.65.2.326</pub-id><?supplied-pmid 14742674?><pub-id pub-id-type="pmid">14742674</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Visel</surname><given-names>A</given-names></name>, <name><surname>Bristow</surname><given-names>J</given-names></name>, <name><surname>Pennacchio</surname><given-names>L A</given-names></name>. <article-title>Enhancer identification through comparative genomics[C]</article-title>//<source>Seminars in cell &amp; developmental biology. Academic Press</source>, <year>2007</year>, <volume>18</volume>(<issue>1</issue>): <fpage>140</fpage>–<lpage>152</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.semcdb.2006.12.014</pub-id><?supplied-pmid 17276707?><pub-id pub-id-type="pmid">17276707</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Kleftogiannis</surname><given-names>D</given-names></name>, <name><surname>Kalnis</surname><given-names>P</given-names></name>, <name><surname>Bajic</surname><given-names>V B</given-names></name>. <article-title>Progress and challenges in bioinformatics approaches for enhancer identification[J]</article-title>. <source>Briefings in bioinformatics</source>, <year>2016</year>, <volume>17</volume>(<issue>6</issue>): <fpage>967</fpage>–<lpage>979</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bib/bbv101</pub-id><?supplied-pmid 26634919?><pub-id pub-id-type="pmid">26634919</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Shlyueva</surname><given-names>D</given-names></name>, <name><surname>Stampfel</surname><given-names>G</given-names></name>, <name><surname>Stark</surname><given-names>A</given-names></name>. <article-title>Transcriptional enhancers: from properties to genome-wide predictions[J]</article-title>. <source>Nature Reviews Genetics</source>, <year>2014</year>, <volume>15</volume>(<issue>4</issue>): <fpage>272</fpage>–<lpage>286</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nrg3682</pub-id><?supplied-pmid 24614317?><pub-id pub-id-type="pmid">24614317</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Fernandez</surname><given-names>M</given-names></name>, <name><surname>Miranda-Saavedra</surname><given-names>D</given-names></name>. <article-title>Genome-wide enhancer prediction from epigenetic signatures using genetic algorithm-optimized support vector machines[J]</article-title>. <source>Nucleic acids research</source>, <year>2012</year>, <volume>40</volume>(<issue>10</issue>): <fpage>e77</fpage>–<lpage>e77</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/gks149</pub-id><?supplied-pmid 22328731?><pub-id pub-id-type="pmid">22328731</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Rajagopal</surname><given-names>N</given-names></name>, <name><surname>Xie</surname><given-names>W</given-names></name>, <name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Wagner</surname><given-names>U</given-names></name>, <name><surname>Wang</surname><given-names>W</given-names></name>, <name><surname>Stamatoyannopoulos</surname><given-names>J</given-names></name>, <etal>et al</etal>. <article-title>RFECS: a random-forest based algorithm for enhancer identification from chromatin state[J]</article-title>. <source>PLoS computational biology</source>, <year>2013</year>, <volume>9</volume>(<issue>3</issue>): <fpage>e1002968</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002968</pub-id><?supplied-pmid 23526891?><pub-id pub-id-type="pmid">23526891</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Erwin</surname><given-names>G D</given-names></name>, <name><surname>Oksenberg</surname><given-names>N</given-names></name>, <name><surname>Truty</surname><given-names>R M</given-names></name>, <name><surname>Kostka</surname><given-names>D</given-names></name>, <name><surname>Murphy</surname><given-names>K K</given-names></name>, <name><surname>Ahituv</surname><given-names>N</given-names></name>, <etal>et al</etal>. <article-title>Integrating diverse datasets improves developmental enhancer prediction[J]</article-title>. <source>PLoS computational biology</source>, <year>2014</year>, <volume>10</volume>(<issue>6</issue>): <fpage>e1003677</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003677</pub-id><?supplied-pmid 24967590?><pub-id pub-id-type="pmid">24967590</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Beer</surname><given-names>M A</given-names></name>. <article-title>Predicting enhancer activity and variant impact using gkm-SVM[J]</article-title>. <source>Human Mutation</source>, <year>2017</year>, <volume>38</volume>(<issue>9</issue>): <fpage>1251</fpage>–<lpage>1258</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/humu.23185</pub-id><?supplied-pmid 28120510?><pub-id pub-id-type="pmid">28120510</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>B</given-names></name>, <name><surname>Fang</surname><given-names>L</given-names></name>, <name><surname>Long</surname><given-names>R</given-names></name>, <name><surname>Lan</surname><given-names>X</given-names></name>, <name><surname>Chou</surname><given-names>K C</given-names></name>. <article-title>iEnhancer-2L: a two-layer predictor for identifying enhancers and their strength by pseudo k-tuple nucleotide composition[J]</article-title>. <source>Bioinformatics</source>, <year>2016</year>, <volume>32</volume>(<issue>3</issue>): <fpage>362</fpage>–<lpage>369</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btv604</pub-id><?supplied-pmid 26476782?><pub-id pub-id-type="pmid">26476782</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Jia</surname><given-names>C</given-names></name>, <name><surname>He</surname><given-names>W</given-names></name>. <article-title>EnhancerPred: a predictor for discovering enhancers based on the combination and selection of multiple features[J]</article-title>. <source>Scientific reports</source>, <year>2016</year>, <volume>6</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>7</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/srep38741</pub-id><?supplied-pmid 27941893?><pub-id pub-id-type="pmid">28442746</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>B</given-names></name>, <name><surname>Li</surname><given-names>K</given-names></name>, <name><surname>Huang</surname><given-names>D S</given-names></name>, <name><surname>Chou</surname><given-names>K C</given-names></name>. <article-title>iEnhancer-EL: identifying enhancers and their strength with ensemble learning approach[J]</article-title>. <source>Bioinformatics</source>, <year>2018</year>, <volume>34</volume>(<issue>22</issue>): <fpage>3835</fpage>–<lpage>3842</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/bty458</pub-id><?supplied-pmid 29878118?><pub-id pub-id-type="pmid">29878118</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Lim</surname><given-names>D Y</given-names></name>, <name><surname>Khanal</surname><given-names>J</given-names></name>, <name><surname>Tayara</surname><given-names>H</given-names></name>, <name><surname>Chong</surname><given-names>K T</given-names></name>. <article-title>iEnhancer-RF: Identifying enhancers and their strength by enhanced feature representation using random forest[J]</article-title>. <source>Chemometrics and Intelligent Laboratory Systems</source>, <year>2021</year>, <volume>212</volume>: <fpage>104284</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.chemolab.2021.104284</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Cai</surname><given-names>L</given-names></name>, <name><surname>Ren</surname><given-names>X</given-names></name>, <name><surname>Fu</surname><given-names>X</given-names></name>, <name><surname>Peng</surname><given-names>L</given-names></name>, <name><surname>Gao</surname><given-names>M</given-names></name>, <name><surname>Zeng</surname><given-names>X</given-names></name>. <article-title>iEnhancer-XG: interpretable sequence-based enhancers and their strength predictor[J]</article-title>. <source>Bioinformatics</source>, <year>2021</year>, <volume>37</volume>(<issue>8</issue>): <fpage>1060</fpage>–<lpage>1067</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa914</pub-id><?supplied-pmid 33119044?><pub-id pub-id-type="pmid">33119044</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Nguyen</surname><given-names>Q H</given-names></name>, <name><surname>Nguyen-Vo</surname><given-names>T H</given-names></name>, <name><surname>Le</surname><given-names>N Q K</given-names></name>, <name><surname>Do</surname><given-names>T T</given-names></name>, <name><surname>Rahardja</surname><given-names>S</given-names></name>, <name><surname>Nguyen</surname><given-names>B P</given-names></name>. <article-title>iEnhancer-ECNN: identifying enhancers and their strength using ensembles of convolutional neural networks[J]</article-title>. <source>BMC genomics</source>, <year>2019</year>, <volume>20</volume>(<issue>9</issue>): <fpage>1</fpage>–<lpage>10</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/s12864-019-6336-3</pub-id><?supplied-pmid 31874637?><pub-id pub-id-type="pmid">30606130</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Firpi</surname><given-names>H A</given-names></name>, <name><surname>Ucar</surname><given-names>D</given-names></name>, <name><surname>Tan</surname><given-names>K</given-names></name>. <article-title>Discover regulatory DNA elements using chromatin signatures and artificial neural network[J]</article-title>. <source>Bioinformatics</source>, <year>2010</year>, <volume>26</volume>(<issue>13</issue>): <fpage>1579</fpage>–<lpage>1586</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btq248</pub-id><?supplied-pmid 20453004?><pub-id pub-id-type="pmid">20453004</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Basith</surname><given-names>S</given-names></name>, <name><surname>Hasan</surname><given-names>M M</given-names></name>, <name><surname>Lee</surname><given-names>G</given-names></name>, <name><surname>Wei</surname><given-names>L</given-names></name>, <name><surname>Manavalan</surname><given-names>B</given-names></name>. <article-title>Integrative machine learning framework for the identification of cell-specific enhancers from the human genome[J]</article-title>. <source>Briefings in Bioinformatics</source>, <year>2021</year>, <volume>22</volume>(<issue>6</issue>): <fpage>bbab252</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bib/bbab252</pub-id><?supplied-pmid 34226917?><pub-id pub-id-type="pmid">34226917</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref022">
      <label>22</label>
      <mixed-citation publication-type="journal"><name><surname>Niu</surname><given-names>K</given-names></name>, <name><surname>Luo</surname><given-names>X</given-names></name>, <name><surname>Zhang</surname><given-names>S</given-names></name>, <name><surname>Teng</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>T</given-names></name>, <name><surname>Zhao</surname><given-names>Y</given-names></name>. <article-title>iEnhancer-EBLSTM: identifying enhancers and strengths by ensembles of bidirectional long short-term memory[J]</article-title>. <source>Frontiers in Genetics</source>, <year>2021</year>, <volume>12</volume>: <fpage>665498</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fgene.2021.665498</pub-id><?supplied-pmid 33833783?><pub-id pub-id-type="pmid">33833783</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Le</surname><given-names>N Q K</given-names></name>, <name><surname>Yapp</surname><given-names>E K Y</given-names></name>, <name><surname>Ho</surname><given-names>Q T</given-names></name>, <name><surname>Nagasundaram</surname><given-names>N</given-names></name>, <name><surname>Ou</surname><given-names>Y Y</given-names></name>, <name><surname>Yeh</surname><given-names>H Y</given-names></name>. <article-title>iEnhancer-5Step: identifying enhancers using hidden information of DNA sequences via Chou’s 5-step rule and word embedding[J]</article-title>. <source>Analytical biochemistry</source>, <year>2019</year>, <volume>571</volume>: <fpage>53</fpage>–<lpage>61</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.ab.2019.02.017</pub-id><?supplied-pmid 30822398?><pub-id pub-id-type="pmid">30822398</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Le</surname><given-names>N Q K</given-names></name>, <name><surname>Ho</surname><given-names>Q T</given-names></name>, <name><surname>Nguyen</surname><given-names>T T D</given-names></name>, <name><surname>Ou</surname><given-names>Y Y</given-names></name>. <article-title>A transformer architecture based on BERT and 2D convolutional neural network to identify DNA enhancers from sequence information[J]</article-title>. <source>Briefings in bioinformatics</source>, <year>2021</year>, <volume>22</volume>(<issue>5</issue>): <fpage>bbab005</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bib/bbab005</pub-id><?supplied-pmid 33539511?><pub-id pub-id-type="pmid">33539511</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Ernst</surname><given-names>J</given-names></name>, <name><surname>Kellis</surname><given-names>M</given-names></name>. <article-title>ChromHMM: automating chromatin-state discovery and characterization[J]</article-title>. <source>Nature methods</source>, <year>2012</year>, <volume>9</volume>(<issue>3</issue>): <fpage>215</fpage>–<lpage>216</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nmeth.1906</pub-id><?supplied-pmid 22373907?><pub-id pub-id-type="pmid">22373907</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Ghandi</surname><given-names>M</given-names></name>, <name><surname>Lee</surname><given-names>D</given-names></name>, <name><surname>Mohammad-Noori</surname><given-names>M</given-names></name>, <name><surname>Beer</surname><given-names>M A</given-names></name>. <article-title>Enhanced regulatory sequence prediction using gapped k-mer features[J]</article-title>. <source>PLoS computational biology</source>, <year>2014</year>, <volume>10</volume>(<issue>7</issue>): <fpage>e1003711</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003711</pub-id><?supplied-pmid 25033408?><pub-id pub-id-type="pmid">25033408</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>W</given-names></name>, <name><surname>Lin</surname><given-names>H</given-names></name>, <name><surname>Chou</surname><given-names>K C</given-names></name>. <article-title>Pseudo nucleotide composition or PseKNC: an effective formulation for analyzing genomic sequences[J]</article-title>. <source>Molecular BioSystems</source>, <year>2015</year>, <volume>11</volume>(<issue>10</issue>): <fpage>2620</fpage>–<lpage>2634</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1039/C5MB00155B</pub-id><?supplied-pmid 26099739?><pub-id pub-id-type="pmid">26099739</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>R</given-names></name>, <name><surname>Zhang</surname><given-names>C T</given-names></name>. <article-title>A brief review: The z-curve theory and its application in genome analysis[J]</article-title>. <source>Current genomics</source>, <year>2014</year>, <volume>15</volume>(<issue>2</issue>): <fpage>78</fpage>–<lpage>94</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.2174/1389202915999140328162433</pub-id><?supplied-pmid 24822026?><pub-id pub-id-type="pmid">24822026</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref029">
      <label>29</label>
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>C T</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Zhang</surname><given-names>R</given-names></name>. <article-title>A novel method to calculate the G+ C content of genomic DNA sequences[J]</article-title>. <source>Journal of Biomolecular Structure and Dynamics</source>, <year>2001</year>, <volume>19</volume>(<issue>2</issue>): <fpage>333</fpage>–<lpage>341</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/07391102.2001.10506743</pub-id><?supplied-pmid 11697737?><pub-id pub-id-type="pmid">11697737</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref030">
      <label>30</label>
      <mixed-citation publication-type="journal"><name><surname>Doležel</surname><given-names>J</given-names></name>, <name><surname>Sgorbati</surname><given-names>S</given-names></name>, <name><surname>Lucretti</surname><given-names>S</given-names></name>. <article-title>Comparison of three DNA fluorochromes for flow cytometric estimation of nuclear DNA content in plants[J]</article-title>. <source>Physiologia plantarum</source>, <year>1992</year>, <volume>85</volume>(<issue>4</issue>): <fpage>625</fpage>–<lpage>631</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1111/j.1399-3054.1992.tb04764.x</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref031">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>Grigoriev</surname><given-names>A</given-names></name>. <article-title>Analyzing genomes with cumulative skew diagrams[J]</article-title>. <source>Nucleic acids research</source>, <year>1998</year>, <volume>26</volume>(<issue>10</issue>): <fpage>2286</fpage>–<lpage>2290</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/26.10.2286</pub-id><?supplied-pmid 9580676?><pub-id pub-id-type="pmid">9580676</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref032">
      <label>32</label>
      <mixed-citation publication-type="other">Devlin J, Chang M W, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.</mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref033">
      <label>33</label>
      <mixed-citation publication-type="journal"><name><surname>Ji</surname><given-names>Y</given-names></name>, <name><surname>Zhou</surname><given-names>Z</given-names></name>, <name><surname>Liu</surname><given-names>H</given-names></name>, <name><surname>Davuluri</surname><given-names>R V</given-names></name>. <article-title>DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome[J]</article-title>. <source>Bioinformatics</source>, <year>2021</year>, <volume>37</volume>(<issue>15</issue>): <fpage>2112</fpage>–<lpage>2120</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bioinformatics/btab083</pub-id><?supplied-pmid 33538820?><pub-id pub-id-type="pmid">33538820</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>Cortes</surname><given-names>C</given-names></name>, <name><surname>Vapnik</surname><given-names>V</given-names></name>. <article-title>Support-vector networks[J]</article-title>. <source>Machine learning</source>, <year>1995</year>, <volume>20</volume>(<issue>3</issue>): <fpage>273</fpage>–<lpage>297</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/BF00994018</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref035">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>Z H</given-names></name>, <name><surname>Feng</surname><given-names>J</given-names></name>. <article-title>Deep Forest: Towards An Alternative to Deep Neural Networks[C]</article-title>//<source>IJCAI</source>. <year>2017</year>: <fpage>3553</fpage>–<lpage>3559</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref036">
      <label>36</label>
      <mixed-citation publication-type="journal"><name><surname>Breiman</surname><given-names>L</given-names></name>. <article-title>Random forests[J]</article-title>. <source>Machine learning</source>, <year>2001</year>, <volume>45</volume>(<issue>1</issue>): <fpage>5</fpage>–<lpage>32</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref037">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Koo</surname><given-names>P K</given-names></name>, <name><surname>Eddy</surname><given-names>S R</given-names></name>. <article-title>Representation learning of genomic sequence motifs with convolutional neural networks[J]</article-title>. <source>PLoS computational biology</source>, <year>2019</year>, <volume>15</volume>(<issue>12</issue>): <fpage>e1007560</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007560</pub-id><?supplied-pmid 31856220?><pub-id pub-id-type="pmid">31856220</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref038">
      <label>38</label>
      <mixed-citation publication-type="other">Chen T, Guestrin C. Xgboost: A scalable tree boosting system[C]//Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. 2016: 785–794.</mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref039">
      <label>39</label>
      <mixed-citation publication-type="journal"><name><surname>Ke</surname><given-names>G</given-names></name>, <name><surname>Meng</surname><given-names>Q</given-names></name>, <name><surname>Finley</surname><given-names>T</given-names></name>, <name><surname>Wang</surname><given-names>T</given-names></name>, <name><surname>Chen</surname><given-names>W</given-names></name>, <name><surname>Ma</surname><given-names>W</given-names></name>, <etal>et al</etal>. <article-title>Lightgbm: A highly efficient gradient boosting decision tree[J]</article-title>. <source>Advances in neural information processing systems</source>, <year>2017</year>, <volume>30</volume>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref040">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Wright</surname><given-names>R E</given-names></name>. <source>Logistic regression[J]</source>. <year>1995</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref041">
      <label>41</label>
      <mixed-citation publication-type="other">Thavareesan S, Mahesan S. K-NearestNeighbor[J].</mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref042">
      <label>42</label>
      <mixed-citation publication-type="journal"><name><surname>Friedman</surname><given-names>J H</given-names></name>. <article-title>Greedy function approximation: a gradient boosting machine[J]</article-title>. <source>Annals of statistics</source>, <year>2001</year>: <fpage>1189</fpage>–<lpage>1232</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref043">
      <label>43</label>
      <mixed-citation publication-type="journal"><name><surname>Pedregosa</surname><given-names>F</given-names></name>, <name><surname>Varoquaux</surname><given-names>G</given-names></name>, <name><surname>Gramfort</surname><given-names>A</given-names></name>, <name><surname>Michel</surname><given-names>V</given-names></name>, <name><surname>Thirion</surname><given-names>B</given-names></name>, <name><surname>Grisel</surname><given-names>O</given-names></name>, <etal>et al</etal>. <article-title>Scikit-learn: Machine learning in Python[J]</article-title>. <source>the Journal of machine Learning research</source>, <year>2011</year>, <volume>12</volume>: <fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref044">
      <label>44</label>
      <mixed-citation publication-type="journal"><name><surname>Avsec</surname><given-names>Ž</given-names></name>, <name><surname>Weilert</surname><given-names>M</given-names></name>, <name><surname>Shrikumar</surname><given-names>A</given-names></name>, <name><surname>Krueger</surname><given-names>S</given-names></name>, <name><surname>Alexandari</surname><given-names>A</given-names></name>, <name><surname>Dalal</surname><given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Base-resolution models of transcription-factor binding reveal soft motif syntax[J]</article-title>. <source>Nature Genetics</source>, <year>2021</year>, <volume>53</volume>(<issue>3</issue>): <fpage>354</fpage>–<lpage>366</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41588-021-00782-6</pub-id><?supplied-pmid 33603233?><pub-id pub-id-type="pmid">33603233</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref045">
      <label>45</label>
      <mixed-citation publication-type="journal"><name><surname>Shrikumar</surname><given-names>A</given-names></name>, <name><surname>Greenside</surname><given-names>P</given-names></name>, <name><surname>Kundaje</surname><given-names>A</given-names></name>. <article-title>Learning important features through propagating activation differences[C]//International conference on machine learning</article-title>. <source>PMLR</source>, <year>2017</year>: <fpage>3145</fpage>–<lpage>3153</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref046">
      <label>46</label>
      <mixed-citation publication-type="other">Shrikumar A, Tian K, Avsec Ž, Shcherbina A, Banerjee A, Sharmin M, et al. Technical note on transcription factor motif discovery from importance scores (TF-MoDISco) version 0.5. 6.5[J]. arXiv preprint arXiv:1811.00416, 2018.</mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref047">
      <label>47</label>
      <mixed-citation publication-type="journal"><name><surname>Gupta</surname><given-names>S</given-names></name>, <name><surname>Stamatoyannopoulos</surname><given-names>J A</given-names></name>, <name><surname>Bailey</surname><given-names>T L</given-names></name>, <name><surname>Noble</surname><given-names>W S</given-names></name>. <article-title>Quantifying similarity between motifs[J]</article-title>. <source>Genome biology</source>, <year>2007</year>, <volume>8</volume>(<issue>2</issue>): <fpage>1</fpage>–<lpage>9</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/gb-2007-8-2-r24</pub-id><?supplied-pmid 17324271?><pub-id pub-id-type="pmid">17324271</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref048">
      <label>48</label>
      <mixed-citation publication-type="journal"><name><surname>Castro-Mondragon</surname><given-names>J A</given-names></name>, <name><surname>Riudavets-Puig</surname><given-names>R</given-names></name>, <name><surname>Rauluseviciute</surname><given-names>I</given-names></name>, <name><surname>Berhanu</surname><given-names>L R</given-names></name>, <name><surname>Turchi</surname><given-names>L</given-names></name>, <name><surname>Blanc-Mathieu</surname><given-names>R</given-names></name>, <etal>et al</etal>. <article-title>JASPAR 2022: the 9th release of the open-access database of transcription factor binding profiles[J]</article-title>. <source>Nucleic acids research</source>, <year>2022</year>, <volume>50</volume>(<issue>D1</issue>): <fpage>D165</fpage>–<lpage>D173</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/nar/gkab1113</pub-id><?supplied-pmid 34850907?><pub-id pub-id-type="pmid">34850907</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref049">
      <label>49</label>
      <mixed-citation publication-type="journal"><name><surname>Guyon</surname><given-names>I</given-names></name>, <name><surname>Weston</surname><given-names>J</given-names></name>, <name><surname>Barnhill</surname><given-names>S</given-names></name>, <name><surname>Vapnik</surname><given-names>V</given-names></name>. <article-title>Gene selection for cancer classification using support vector machines[J]</article-title>. <source>Machine learning</source>, <year>2002</year>, <volume>46</volume>(<issue>1</issue>): <fpage>389</fpage>–<lpage>422</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1023/A:1012487302797</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref050">
      <label>50</label>
      <mixed-citation publication-type="other">Jović A, Brkić K, Bogunović N. A review of feature selection methods with applications[C]//2015 38th international convention on information and communication technology, electronics and microelectronics (MIPRO). Ieee, 2015: 1200–1205.</mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref051">
      <label>51</label>
      <mixed-citation publication-type="journal"><name><surname>Cross</surname><given-names>J C</given-names></name>, <name><surname>Werb</surname><given-names>Z</given-names></name>, <name><surname>Fisher</surname><given-names>S J</given-names></name>. <article-title>Implantation and the placenta: key pieces of the development puzzle[J]</article-title>. <source>Science</source>, <year>1994</year>, <volume>266</volume>(<issue>5190</issue>): <fpage>1508</fpage>–<lpage>1518</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.7985020</pub-id><?supplied-pmid 7985020?><pub-id pub-id-type="pmid">7985020</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref052">
      <label>52</label>
      <mixed-citation publication-type="journal"><name><surname>Morgan</surname><given-names>T K</given-names></name>. <article-title>Placental insufficiency is a leading cause of preterm labor[J]</article-title>. <source>NeoReviews</source>, <year>2014</year>, <volume>15</volume>(<issue>12</issue>): <fpage>e518</fpage>–<lpage>e525</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1542/neo.15-12-e518</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref053">
      <label>53</label>
      <mixed-citation publication-type="journal"><name><surname>Kovo</surname><given-names>M</given-names></name>, <name><surname>Schreiber</surname><given-names>L</given-names></name>, <name><surname>Ben-Haroush</surname><given-names>A</given-names></name>, <name><surname>Asalee</surname><given-names>L</given-names></name>, <name><surname>Seadia</surname><given-names>S</given-names></name>, <name><surname>Golan</surname><given-names>A</given-names></name>, <etal>et al</etal>. <source>The placental factor in spontaneous preterm labor with and without premature rupture of membranes[J]</source>. <year>2011</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref054">
      <label>54</label>
      <mixed-citation publication-type="journal"><name><surname>Faye-Petersen</surname><given-names>O M</given-names></name>. <article-title>The placenta in preterm birth[J]</article-title>. <source>Journal of Clinical Pathology</source>, <year>2008</year>, <volume>61</volume>(<issue>12</issue>): <fpage>1261</fpage>–<lpage>1275</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1136/jcp.2008.055244</pub-id><?supplied-pmid 19074631?><pub-id pub-id-type="pmid">19074631</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref055">
      <label>55</label>
      <mixed-citation publication-type="journal"><name><surname>Williams</surname><given-names>P J</given-names></name>, <name><surname>Pipkin</surname><given-names>F B</given-names></name>. <article-title>The genetics of pre-eclampsia and other hypertensive disorders of pregnancy[J]</article-title>. <source>Best practice &amp; research Clinical obstetrics &amp; gynaecology</source>, <year>2011</year>, <volume>25</volume>(<issue>4</issue>): <fpage>405</fpage>–<lpage>417</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.bpobgyn.2011.02.007</pub-id><?supplied-pmid 21429808?><pub-id pub-id-type="pmid">21429808</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref056">
      <label>56</label>
      <mixed-citation publication-type="journal"><name><surname>Lettice</surname><given-names>L A</given-names></name>, <name><surname>Heaney</surname><given-names>S J</given-names></name>, <name><surname>Purdie</surname><given-names>L A</given-names></name>, <name><surname>Li</surname><given-names>L</given-names></name>, <name><surname>de Beer</surname><given-names>P</given-names></name>, <name><surname>Oostra</surname><given-names>B A</given-names></name>, <etal>et al</etal>. <article-title>A long-range Shh enhancer regulates expression in the developing limb and fin and is associated with preaxial polydactyly[J]</article-title>. <source>Human molecular genetics</source>, <year>2003</year>, <volume>12</volume>(<issue>14</issue>): <fpage>1725</fpage>–<lpage>1735</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/hmg/ddg180</pub-id><?supplied-pmid 12837695?><pub-id pub-id-type="pmid">12837695</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref057">
      <label>57</label>
      <mixed-citation publication-type="journal"><name><surname>Maurano</surname><given-names>M T</given-names></name>, <name><surname>Humbert</surname><given-names>R</given-names></name>, <name><surname>Rynes</surname><given-names>E</given-names></name>, <name><surname>Thurman</surname><given-names>R E</given-names></name>, <name><surname>Haugen</surname><given-names>E</given-names></name>, <name><surname>Wang</surname><given-names>H</given-names></name>, <etal>et al</etal>. <article-title>Systematic localization of common disease-associated variation in regulatory DNA[J]</article-title>. <source>Science</source>, <year>2012</year>, <volume>337</volume>(<issue>6099</issue>): <fpage>1190</fpage>–<lpage>1195</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.1222794</pub-id><?supplied-pmid 22955828?><pub-id pub-id-type="pmid">22955828</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref058">
      <label>58</label>
      <mixed-citation publication-type="journal"><name><surname>Bauer</surname><given-names>D E</given-names></name>, <name><surname>Kamran</surname><given-names>S C</given-names></name>, <name><surname>Lessard</surname><given-names>S</given-names></name>, <name><surname>Xu</surname><given-names>J</given-names></name>, <name><surname>Fujiwara</surname><given-names>Y</given-names></name>, <name><surname>Lin</surname><given-names>C</given-names></name>, <etal>et al</etal>. <article-title>An erythroid enhancer of BCL11A subject to genetic variation determines fetal hemoglobin level[J]</article-title>. <source>Science</source>, <year>2013</year>, <volume>342</volume>(<issue>6155</issue>): <fpage>253</fpage>–<lpage>257</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.1242088</pub-id><?supplied-pmid 24115442?><pub-id pub-id-type="pmid">24115442</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref059">
      <label>59</label>
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>J</given-names></name>, <name><surname>Simonti</surname><given-names>C N</given-names></name>, <name><surname>Capra</surname><given-names>J A</given-names></name>. <article-title>Genome-wide maps of distal gene regulatory enhancers active in the human placenta[J]</article-title>. <source>PLoS One</source>, <year>2018</year>, <volume>13</volume>(<issue>12</issue>): <fpage>e0209611</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0209611</pub-id><?supplied-pmid 30589856?><pub-id pub-id-type="pmid">30589856</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref060">
      <label>60</label>
      <mixed-citation publication-type="journal"><name><surname>Andersson</surname><given-names>R</given-names></name>, <name><surname>Gebhard</surname><given-names>C</given-names></name>, <name><surname>Miguel-Escalada</surname><given-names>I</given-names></name>, <name><surname>Hoof</surname><given-names>I</given-names></name>, <name><surname>Bornholdt</surname><given-names>J</given-names></name>, <name><surname>Boyd</surname><given-names>M</given-names></name>, <etal>et al</etal>. <article-title>An atlas of active enhancers across human cell types and tissues[J]</article-title>. <source>Nature</source>, <year>2014</year>, <volume>507</volume>(<issue>7493</issue>): <fpage>455</fpage>–<lpage>461</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nature12787</pub-id><?supplied-pmid 24670763?><pub-id pub-id-type="pmid">24670763</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref061">
      <label>61</label>
      <mixed-citation publication-type="journal"><name><surname>Macias</surname><given-names>H</given-names></name>, <name><surname>Hinck</surname><given-names>L</given-names></name>. <article-title>Mammary gland development[J]</article-title>. <source>Wiley Interdisciplinary Reviews: Developmental Biology</source>, <year>2012</year>, <volume>1</volume>(<issue>4</issue>): <fpage>533</fpage>–<lpage>557</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/wdev.35</pub-id><?supplied-pmid 22844349?><pub-id pub-id-type="pmid">22844349</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref062">
      <label>62</label>
      <mixed-citation publication-type="journal"><name><surname>Nayeem</surname><given-names>S B</given-names></name>, <name><surname>Arfuso</surname><given-names>F</given-names></name>, <name><surname>Dharmarajan</surname><given-names>A</given-names></name>, <name><surname>Keelan</surname><given-names>J A</given-names></name>. <article-title>Role of Wnt signalling in early pregnancy[J]</article-title>. <source>Reproduction, Fertility and Development</source>, <year>2016</year>, <volume>28</volume>(<issue>5</issue>): <fpage>525</fpage>–<lpage>544</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1071/RD14079</pub-id><?supplied-pmid 25190280?><pub-id pub-id-type="pmid">25190280</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref063">
      <label>63</label>
      <mixed-citation publication-type="journal"><name><surname>Zhinkin</surname><given-names>L N</given-names></name>, <name><surname>Andreeva</surname><given-names>L F</given-names></name>. <source>DNA synthesis and nuclear reproduction during embryonic development and regeneration of muscle tissue[J]</source>. <year>1963</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref064">
      <label>64</label>
      <mixed-citation publication-type="journal"><name><surname>Boettner</surname><given-names>B</given-names></name>, <name><surname>Van Aelst</surname><given-names>L</given-names></name>. <article-title>Control of cell adhesion dynamics by Rap1 signaling[J]</article-title>. <source>Current opinion in cell biology</source>, <year>2009</year>, <volume>21</volume>(<issue>5</issue>): <fpage>684</fpage>–<lpage>693</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.ceb.2009.06.004</pub-id><?supplied-pmid 19615876?><pub-id pub-id-type="pmid">19615876</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref065">
      <label>65</label>
      <mixed-citation publication-type="journal"><name><surname>Gallicano</surname><given-names>G I</given-names></name>. <article-title>Composition, regulation, and function of the cytoskeleton in mammalian eggs and embryos[J]</article-title>. <source>Front Biosci</source>, <year>2001</year>, <volume>6</volume>: <fpage>D1089</fpage>–<lpage>D1108</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.2741/A672</pub-id><?supplied-pmid 11532603?><pub-id pub-id-type="pmid">11532603</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref066">
      <label>66</label>
      <mixed-citation publication-type="journal"><name><surname>Moore</surname><given-names>J E</given-names></name>, <name><surname>Pratt</surname><given-names>H E</given-names></name>, <name><surname>Purcaro</surname><given-names>M J</given-names></name>, <name><surname>Weng</surname><given-names>Zhiping</given-names></name>. <article-title>A curated benchmark of enhancer-gene interactions for evaluating enhancer-target gene prediction methods[J]</article-title>. <source>Genome biology</source>, <year>2020</year>, <volume>21</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>16</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/s13059-019-1924-8</pub-id><?supplied-pmid 31969180?><pub-id pub-id-type="pmid">31969180</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1010779.ref067">
      <label>67</label>
      <mixed-citation publication-type="journal"><name><surname>Dao</surname><given-names>F</given-names></name>, <name><surname>Lv</surname><given-names>H</given-names></name>, <name><surname>Su</surname><given-names>W</given-names></name>, <name><surname>Sun</surname><given-names>Z</given-names></name>, <name><surname>Huang</surname><given-names>Q</given-names></name>, <name><surname>Lin</surname><given-names>H</given-names></name>. <article-title>iDHS-deep: an integrated tool for predicting DNase I hypersensitive sites by deep neural network[J]</article-title>. <source>Briefings in Bioinformatics</source>, <year>2021</year>, <volume>22</volume>(<issue>5</issue>): <fpage>bbab047</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1093/bib/bbab047</pub-id><?supplied-pmid 33751027?><pub-id pub-id-type="pmid">33751027</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
