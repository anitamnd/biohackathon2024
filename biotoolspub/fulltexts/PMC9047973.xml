<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with OASIS Tables with MathML3 v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archive-oasis-article1-mathml3.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats-oasis2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Ecol Evol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Ecol Evol</journal-id>
    <journal-id journal-id-type="doi">10.1002/(ISSN)2045-7758</journal-id>
    <journal-id journal-id-type="publisher-id">ECE3</journal-id>
    <journal-title-group>
      <journal-title>Ecology and Evolution</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-7758</issn>
    <publisher>
      <publisher-name>John Wiley and Sons Inc.</publisher-name>
      <publisher-loc>Hoboken</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9047973</article-id>
    <article-id pub-id-type="doi">10.1002/ece3.8851</article-id>
    <article-id pub-id-type="publisher-id">ECE38851</article-id>
    <article-categories>
      <subj-group subj-group-type="article-subject-classification">
        <subject>Behavioural Ecology</subject>
      </subj-group>
      <subj-group subj-group-type="overline">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Research Articles</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SEALNET: Facial recognition software for ecological studies of harbor seals</article-title>
      <alt-title alt-title-type="left-running-head">BIRENBAUM et al.</alt-title>
    </title-group>
    <contrib-group>
      <contrib id="ece38851-cr-0001" contrib-type="author">
        <name>
          <surname>Birenbaum</surname>
          <given-names>Zach</given-names>
        </name>
        <xref rid="ece38851-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib id="ece38851-cr-0002" contrib-type="author">
        <name>
          <surname>Do</surname>
          <given-names>Hieu</given-names>
        </name>
        <xref rid="ece38851-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="ece38851-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib id="ece38851-cr-0003" contrib-type="author">
        <name>
          <surname>Horstmyer</surname>
          <given-names>Lauren</given-names>
        </name>
        <xref rid="ece38851-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib id="ece38851-cr-0004" contrib-type="author">
        <name>
          <surname>Orff</surname>
          <given-names>Hailey</given-names>
        </name>
        <xref rid="ece38851-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib id="ece38851-cr-0005" contrib-type="author" corresp="yes">
        <name>
          <surname>Ingram</surname>
          <given-names>Krista</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-4512-1549</contrib-id>
        <xref rid="ece38851-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
        <address>
          <email>kingram@colgate.edu</email>
        </address>
      </contrib>
      <contrib id="ece38851-cr-0006" contrib-type="author" corresp="yes">
        <name>
          <surname>Ay</surname>
          <given-names>Ahmet</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0726-5461</contrib-id>
        <xref rid="ece38851-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="ece38851-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
        <address>
          <email>kingram@colgate.edu</email>
          <email>aay@colgate.edu</email>
        </address>
      </contrib>
    </contrib-group>
    <aff id="ece38851-aff-0001">
      <label>
        <sup>1</sup>
      </label>
      <institution-wrap>
        <institution-id institution-id-type="ringgold">3719</institution-id>
      </institution-wrap>
      <named-content content-type="organisation-division">Department of Computer Science</named-content>
      <institution>Colgate University</institution>
      <city>Hamilton</city>
      <named-content content-type="country-part">New York</named-content>
      <country country="US">USA</country>
    </aff>
    <aff id="ece38851-aff-0002">
      <label>
        <sup>2</sup>
      </label>
      <institution-wrap>
        <institution-id institution-id-type="ringgold">3719</institution-id>
      </institution-wrap>
      <named-content content-type="organisation-division">Department of Mathematics</named-content>
      <institution>Colgate University</institution>
      <city>Hamilton</city>
      <named-content content-type="country-part">New York</named-content>
      <country country="US">USA</country>
    </aff>
    <aff id="ece38851-aff-0003">
      <label>
        <sup>3</sup>
      </label>
      <institution-wrap>
        <institution-id institution-id-type="ringgold">3719</institution-id>
      </institution-wrap>
      <named-content content-type="organisation-division">Department of Biology</named-content>
      <institution>Colgate University</institution>
      <city>Hamilton</city>
      <named-content content-type="country-part">New York</named-content>
      <country country="US">USA</country>
    </aff>
    <author-notes>
      <corresp id="correspondenceTo"><label>*</label><bold>Correspondence</bold><break/>
Krista Ingram and Ahmet Ay, Department of Biology, Colgate University, Hamilton, NY, USA.<break/>
Emails: <email>kingram@colgate.edu</email>; <email>aay@colgate.edu</email>
<break/>
</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>28</day>
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <volume>12</volume>
    <issue seq="30">5</issue>
    <issue-id pub-id-type="doi">10.1002/ece3.v12.5</issue-id>
    <elocation-id>e8851</elocation-id>
    <history>
      <date date-type="rev-recd">
        <day>25</day>
        <month>3</month>
        <year>2022</year>
      </date>
      <date date-type="received">
        <day>03</day>
        <month>1</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>30</day>
        <month>3</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <!--&#x000a9; 2022 Published by John Wiley & Sons Ltd.-->
      <copyright-statement content-type="article-copyright">© 2022 The Authors. <italic toggle="yes">Ecology and Evolution</italic> published by John Wiley &amp; Sons Ltd.</copyright-statement>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link> License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="file:ECE3-12-e8851.pdf"/>
    <abstract id="ece38851-abs-0001">
      <title>Abstract</title>
      <p>Methods for long‐term monitoring of coastal species such as harbor seals (<italic toggle="yes">Phoca vitulina</italic>) are often costly, time‐consuming, and highly invasive, underscoring the need for improved techniques for data collection and analysis. Here, we propose the use of automated facial recognition technology for identification of individual seals and demonstrate its utility in ecological and population studies. We created a software package, SealNet, that automates photo identification of seals, using a graphical user interface (GUI) software to detect, align, and chip seal faces from photographs and a deep convolutional neural network (CNN) suitable for small datasets (e.g., 100 seals with five photos per seal) to classify individual seals. We piloted the SealNet technology with a population of harbor seals located within Casco Bay on the coast of Maine, USA. Across two years of sampling, 2019 and 2020, at seven haul‐out sites in Middle Bay, we obtained a dataset optimized for the development and testing of SealNet. We processed 1752 images representing 408 individual seals and achieved 88% Rank‐1 and 96% Rank‐5 accuracy in closed set seal identification. In identifying individual seals, SealNet software outperformed a similar face recognition method, PrimNet, developed for primates but retrained on seals. The ease and wealth of image data that can be processed using SealNet software contributes a vital tool for ecological and behavioral studies of marine mammals in the developing field of conservation technology.</p>
    </abstract>
    <abstract abstract-type="graphical" id="ece38851-abs-0002">
      <p>In this paper, we describe the successful application of our newly developed automated facial recognition software as a tool for ecological analysis of harbor seals (<italic toggle="yes">Phoca vitulina</italic>). We outline an emerging method of data collection and analysis that facilitates rapid interpretation of large photo datasets over wide temporal and geographic scales. In addition, we use this machine learning‐based technology in a preliminary ecological study in a wild population of seals in the Casco Bay region of Maine to demonstrate the effectiveness of this non‐invasive method for use in mark‐recapture and site fidelity studies in the field.<boxed-text position="anchor" content-type="graphic" id="ece38851-blkfxd-0001"><graphic xlink:href="ECE3-12-e8851-g001.jpg" position="anchor" id="jats-graphic-1"/></boxed-text>
</p>
    </abstract>
    <counts>
      <fig-count count="7"/>
      <table-count count="3"/>
      <page-count count="11"/>
      <word-count count="7702"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>source-schema-version-number</meta-name>
        <meta-value>2.0</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>cover-date</meta-name>
        <meta-value>May 2022</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>details-of-publishers-convertor</meta-name>
        <meta-value>Converter:WILEY_ML3GV2_TO_JATSPMC version:6.1.4 mode:remove_FC converted:28.04.2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <p content-type="self-citation">
      <mixed-citation publication-type="journal" id="ece38851-cit-1001"><string-name><surname>Birenbaum</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Do</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Horstmyer</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Orff</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ingram</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Ay</surname>, <given-names>A.</given-names></string-name> (<year>2022</year>). <article-title>SEALNET: Facial recognition software for ecological studies of harbor seals</article-title>. <source>Ecology and Evolution</source>, <volume>12</volume>, <elocation-id>e8851</elocation-id>. <pub-id pub-id-type="doi">10.1002/ece3.8851</pub-id>
</mixed-citation>
    </p>
    <fn-group id="ece38851-ntgp-0001">
      <fn fn-type="equal" id="ece38851-note-0001">
        <p>Zach Birenbaum and Hieu Do equal Contribution.</p>
      </fn>
    </fn-group>
  </notes>
</front>
<body id="ece38851-body-0001">
  <sec id="ece38851-sec-0001">
    <label>1</label>
    <title>INTRODUCTION</title>
    <p>The dynamic nature of marine ecosystems requires the monitoring of populations across a range of temporal and geographic scales to inform conservation efforts (Hindell et al., <xref rid="ece38851-bib-0016" ref-type="bibr">2003</xref>). However, methods for long‐term monitoring of coastal species, including marine mammals, are often invasive, costly, and time‐consuming (Cunningham, <xref rid="ece38851-bib-0008" ref-type="bibr">2009</xref>), underscoring the need for new techniques for systematic data collection and analysis. The automation of population survey tools can also improve the efficiency of long‐term monitoring by increasing reproducibility while decreasing cost and labor (Weinstein, <xref rid="ece38851-bib-0031" ref-type="bibr">2018</xref>).</p>
    <p>Due to the ecological and economical importance of marine mammals as predators (Aarts et al., <xref rid="ece38851-bib-0001" ref-type="bibr">2019</xref>), systemic monitoring of these highly mobile species is critical for understanding their population dynamics across a large geographic range. Tagging methods to track marine mammals have been widely used in the past. However, these GPS‐monitoring devices are expensive, ranging from $1000 to $3000 for one device (<italic toggle="yes">GPS and VHF Tracking Collars Used for Wildlife Monitoring</italic>, <xref rid="ece38851-bib-0014" ref-type="bibr">2017</xref>). In addition, the attachment of external devices may interfere with behaviors such as swimming speed, oxygen consumption, and metabolic rate, potentially corrupting the data collected or harming or disturbing the individual (Rosen et al., <xref rid="ece38851-bib-0026" ref-type="bibr">2017</xref>). Aerial observation methods limit interference with marine mammal behavior, but this technique is also time consuming and expensive (Cunningham, <xref rid="ece38851-bib-0008" ref-type="bibr">2009</xref>). Photo‐based identification techniques have been widely used in cetacean species and other marine mammals (Balmer et al., <xref rid="ece38851-bib-0003" ref-type="bibr">2008</xref>; Cunningham, <xref rid="ece38851-bib-0008" ref-type="bibr">2009</xref>; Elwen et al., <xref rid="ece38851-bib-0011" ref-type="bibr">2009</xref>; Glennie et al., <xref rid="ece38851-bib-0013" ref-type="bibr">2021</xref>; Rayment et al., <xref rid="ece38851-bib-0025" ref-type="bibr">2009</xref>), and have the advantage of being non‐invasive, but manual interpretation of photographs is time‐intensive and often limited to small‐scale projects. In addition, diagnostic features may be difficult to photograph reliably in some species, like harbor seals, where pelage color and/or patterning changes over time and across seasons.</p>
    <p>Harbor seals (<italic toggle="yes">Phoca vitulina</italic>) are important indicators of ecosystem health given their extensive overlap with human activities both in and out of the water, and these marine mammals are particularly vulnerable to increased anthropogenic activity (Allen et al., <xref rid="ece38851-bib-0002" ref-type="bibr">1984</xref>). As top predators, seal populations affect ecosystem dynamics, with healthy populations in the Northern Atlantic likely decreasing competition among species such as flounder and dab (suborder <italic toggle="yes">Pleuronectoidei</italic>), and sole (family <italic toggle="yes">Aciridae)</italic>, and, in turn, influencing the balance of both ecologically and economically critical fish populations (Aarts et al., <xref rid="ece38851-bib-0001" ref-type="bibr">2019</xref>). Increases in seal populations along the Atlantic coast of the United States have also increased the numbers of sharks that inhabit coastal waters, potentially affecting tourism revenue in addition to local ecosystems (O’Toole et al., <xref rid="ece38851-bib-0024" ref-type="bibr">2020</xref>). Over the last century, the Atlantic coast populations of harbor seals in northeastern North America were heavily exploited. Following their protection under the Marine Mammal Protection Act of 1972, populations of harbor seals off the Northeast coast of the United States successfully rebounded to healthy population numbers. However, the steep decline in abundance prior to any legislation is evidence of the potential vulnerability of the population to acute or chronic ecological challenges.</p>
    <p>As key regulators and indicators of ecosystem health (Heithaus et al., <xref rid="ece38851-bib-0015" ref-type="bibr">2008</xref>), accurate monitoring of harbor seal populations and movement patterns is essential. Photographic identification of individual harbor seals will facilitate population measures, including measures of site fidelity and estimates of population size based on mark‐recapture methods. Harbor seals are relatively easy to monitor via photographic analysis as large numbers of seals can be observed non‐invasively as they congregate at “haul‐out” sites—areas where seals come out of the water to rest on rocky islets, allowing them to thermoregulate and avoid predation—which make them easily visible to researchers from afar (Honeywell &amp; Maher, <xref rid="ece38851-bib-0017" ref-type="bibr">2017</xref>). Some promising progress in photo ID techniques has been made using analysis of pelage markings, i.e., spots on the seal's coat that can be reliably used as diagnostic tools (Cunningham, <xref rid="ece38851-bib-0008" ref-type="bibr">2009</xref>). However, the identification of individuals harbor seals based on pelage patterns is difficult due to the density of individuals at haul‐out sites and to changing coat patterns as seals mature or during annual molting. These difficulties highlight the need for a novel photographic identification technique that does not depend on whole‐body photography and that can be automated for the inexpensive, efficient classification of individuals.</p>
    <p>Here, we propose the use of automated facial recognition technology as a system for the identification of marine mammals for ecological and population studies. We used deep learning methods and convolutional neural networks to develop SealNet, a redesign of the PrimNet software developed for primates. SealNet contributes the first marine mammal face recognition software to automate the process of seal identification for use by researchers in the field.</p>
    <p>In this paper, we outline the creation of a graphical user interface (GUI), that allows the user to automatically select, align, and chip seal faces to facilitate the processing of raw data. Then, we develop a seal face recognition software to identify individual seals. We train and test this software on a wild population of Atlantic harbor seals in Casco Bay, Maine, U.S.A. We compare the performance of SealNet with its predecessor PrimNet and show that SealNet outperforms this software in the classification of harbor seals. SealNet provides a new, non‐invasive tool for tracking individual seals in ecological and behavioral studies.</p>
  </sec>
  <sec sec-type="methods" id="ece38851-sec-0002">
    <label>2</label>
    <title>METHODS</title>
    <sec id="ece38851-sec-0003">
      <label>2.1</label>
      <title>Photographic data collection</title>
      <p>In the summers of 2019 and 2020, we captured 2267 photos across seven haul‐out sites around Casco Bay, Maine, U.S.A. (Seal Rock, Wilson Cove, Brandt Ledges, Mitchell Fields, Branning Ledge, Whaleboat, and Bustin's Ledge; see Figure <xref rid="ece38851-fig-0001" ref-type="fig">1</xref>). As we were optimizing the photographic data collection for developing and training SealNet, sites were visited only once, and there was no overlap in sites between 2019 and 2020 (Table <xref rid="ece38851-tbl-0001" ref-type="table">1</xref>). During a single visit to each site, we took photos for 30 min to one hour from a 22‐foot Eastern motorboat equipped with a 90‐horsepower engine, an open deck, and a low‐profile console. All site visits occurred in the summer (molting season) of each year, with exact dates dependent on weather and tides, as some of the sites are inaccessible at high tide. We used a Nikon COOLPIX P1000 digital camera with a 125x optical zoom. We photographed at a minimum distance of 54.9 m (60 yards) from haul‐out sites with the engine in low throttle or off to create minimal disturbance to the seals. We took multiple photographs of each individual seal as the boat drifted past the site. Below, we describe the steps of the pipeline for the processing of photographic data and the development of SealNet and the database of individual seal IDs; these steps are outlined in Figure <xref rid="ece38851-fig-0002" ref-type="fig">2</xref>.</p>
      <fig position="float" fig-type="FIGURE" id="ece38851-fig-0001">
        <label>FIGURE 1</label>
        <caption>
          <p>Harbor seal haul‐out sites photographed in preliminary study. In 2020, seal ‘015_Armani’ (photographed on Brandt ledges originally in 2019) was rephotographed on Branning Ledges; the seals ‘198_Petal’ and ‘211_Clove’ (both photographed on Brandt ledges in 2019) were rephotographed on Mitchell Field ledges; and the seal, ‘393_Cystine’, (photographed on Brandt ledges in 2019) was rephotographed on Whaleboat ledges</p>
        </caption>
        <graphic xlink:href="ECE3-12-e8851-g002" position="anchor" id="jats-graphic-3"/>
      </fig>
      <table-wrap position="float" id="ece38851-tbl-0001" content-type="TABLE">
        <label>TABLE 1</label>
        <caption>
          <p>Dataset summary</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <thead valign="top">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="top" rowspan="1" colspan="1">Year</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Date</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Location</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Total # Seals (# Chips)</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Unique Seals (# IDs)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="5" colspan="1">2019</td>
              <td align="left" rowspan="1" colspan="1">7/16</td>
              <td align="left" rowspan="1" colspan="1">Brandt Ledges</td>
              <td align="char" char="." rowspan="1" colspan="1">50</td>
              <td align="char" char="." rowspan="1" colspan="1">11</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">7/20</td>
              <td align="left" rowspan="1" colspan="1">Brandt Ledges</td>
              <td align="char" char="." rowspan="1" colspan="1">56</td>
              <td align="char" char="." rowspan="1" colspan="1">17</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">7/24</td>
              <td align="left" rowspan="1" colspan="1">Seal Rock</td>
              <td align="char" char="." rowspan="1" colspan="1">45</td>
              <td align="char" char="." rowspan="1" colspan="1">8</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">7/27</td>
              <td align="left" rowspan="1" colspan="1">Wilson Cove</td>
              <td align="char" char="." rowspan="1" colspan="1">19</td>
              <td align="char" char="." rowspan="1" colspan="1">4</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">7/30</td>
              <td align="left" rowspan="1" colspan="1">Bustin's Ledge</td>
              <td align="char" char="." rowspan="1" colspan="1">15</td>
              <td align="char" char="." rowspan="1" colspan="1">3</td>
            </tr>
            <tr>
              <td align="left" rowspan="5" colspan="1">2020</td>
              <td align="left" rowspan="1" colspan="1">7/01</td>
              <td align="left" rowspan="1" colspan="1">Whaleboat</td>
              <td align="char" char="." rowspan="1" colspan="1">39</td>
              <td align="char" char="." rowspan="1" colspan="1">8</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">7/10</td>
              <td align="left" rowspan="1" colspan="1">Branning Ledges</td>
              <td align="char" char="." rowspan="1" colspan="1">820</td>
              <td align="char" char="." rowspan="1" colspan="1">197</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">7/28</td>
              <td align="left" rowspan="1" colspan="1">Whaleboat</td>
              <td align="char" char="." rowspan="1" colspan="1">33</td>
              <td align="char" char="." rowspan="1" colspan="1">8</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">7/29</td>
              <td align="left" rowspan="1" colspan="1">Branning Ledges</td>
              <td align="char" char="." rowspan="1" colspan="1">254</td>
              <td align="char" char="." rowspan="1" colspan="1">65</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">7/31</td>
              <td align="left" rowspan="1" colspan="1">Mitchell Fields</td>
              <td align="char" char="." rowspan="1" colspan="1">434</td>
              <td align="char" char="." rowspan="1" colspan="1">127</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot id="ece38851-ntgp-0002">
          <title>Note</title>
          <fn id="ece38851-note-0002">
            <p>The number of chips is the number of faces recognized by the facial recognition software. The number of IDs represents the number of individuals identified after grouping the chips according to individual.</p>
          </fn>
        </table-wrap-foot>
        <permissions>
          <copyright-holder>John Wiley &amp; Sons, Ltd</copyright-holder>
        </permissions>
      </table-wrap>
      <fig position="float" fig-type="FIGURE" id="ece38851-fig-0002">
        <label>FIGURE 2</label>
        <caption>
          <p>Summary of steps to create the final photo‐ID database using SEALNET</p>
        </caption>
        <graphic xlink:href="ECE3-12-e8851-g007" position="anchor" id="jats-graphic-5"/>
      </fig>
    </sec>
    <sec id="ece38851-sec-0004">
      <label>2.2</label>
      <title>Processing of photographic data</title>
      <sec id="ece38851-sec-0005">
        <label>2.2.1</label>
        <title>Raw data cleaning</title>
        <p>We manually processed the total number of photos in the database (&gt;5000 images) to remove blurry photos, shots of sky or water, for a total of 2267 raw images. We then removed photo duplicates—images that were very similar to each other. We cropped each photo in the condensed dataset (<italic toggle="yes">n</italic> = 1752) to focus on the seal faces to minimize the amount of time the software takes to select faces. Once photos have been cropped, they are ready to be viewed in the graphical user interface and analyzed with the face detection software (steps outlined in Figure <xref rid="ece38851-fig-0003" ref-type="fig">3</xref>). For this preliminary study, we processed images across four haul‐out sites in 2019 and across three additional haul‐out sites in 2020 for a total of seven locations in Casco Bay (Table <xref rid="ece38851-tbl-0001" ref-type="table">1</xref>).</p>
        <fig position="float" fig-type="FIGURE" id="ece38851-fig-0003">
          <label>FIGURE 3</label>
          <caption>
            <p>Summary of steps involved in face chipping. Step 1: Remove blurry and duplicate photos to create the raw photo dataset. Step 2: Run the automatic face detector to locate faces. Step 3: Manually locate the eye centers, nose, and mouth. Step 4: The GUI automatically aligns and chips all faces, saving output jpegs to a new folder. Step 5: Manually categorize chipped photos of the same seals into individual folders to be used for SealNet training</p>
          </caption>
          <graphic xlink:href="ECE3-12-e8851-g006" position="anchor" id="jats-graphic-7"/>
        </fig>
      </sec>
      <sec id="ece38851-sec-0006">
        <label>2.2.2</label>
        <title>Face detection</title>
        <p>We created the graphical user interface (GUI) in C++ by modifying <italic toggle="yes">imglab</italic> tools for image annotation (King, <xref rid="ece38851-bib-0019" ref-type="bibr">2009</xref>). We trained the interface to detect seal faces, allowing for automated detection of all seal faces in each photo. In addition, the GUI allows for the option to manually select seal faces by drawing boxes around valid faces in the application. A valid seal face is determined based on the quality and clarity of the image, as well as the angle of the complete seal face to the camera. Invalid faces are those that are too blurry, not facing the camera, or are partially obstructed; these can be marked by the user and will be ignored by the software. Variations in illuminations, lighting, and other conditions can introduce noise to the data and impede analysis. We next converted the photos to grayscale to help the model learn based on physical features of the face, which also serves to reduce overfitting during training. After all photos were aligned and chipped, we manually grouped photos of the same seals into folders by individual. To train our face detector, we selected 516 photos (10–20 seals faces per photo) from all locations in the 2020 dataset.</p>
        <p>Our imglab based face detection software is a CNN network which uses Max‐Margin Object Detection (King, <xref rid="ece38851-bib-0020" ref-type="bibr">2015</xref>) loss function. The first three layers of the network downsample the input images by 8 and output a feature map of 32 channels. This feature map will go through 4 more convolutional layers with batch normalization and Rectified Linear Unit (ReLU) as nonlinearity. The final output will only have 1 channel; a large value will indicate that the network has found an object at that location and vice versa.</p>
        <p>Using the full 2020 dataset, we measured the accuracy of the model using 5‐fold stratified cross‐validation. Each strata (i.e., a single location and date) was split into 5 sections. For each fold, 4 of the 5 sections of each strata were combined as a training set while the remaining section of each strata were combined and used as a validation set. For each fold, the training set contained ~413 photos from all 5 locations, and the validation set contained ~103 photos from the same 5 locations. The accuracy of the face detector is measured by two metrics: precision (the percentage of predictions that are seal face) and recall (the percentage of total seal faces that are correctly predicted; Figure <xref rid="ece38851-fig-0004" ref-type="fig">4</xref>).</p>
        <fig position="float" fig-type="FIGURE" id="ece38851-fig-0004">
          <label>FIGURE 4</label>
          <caption>
            <p>Precision‐recall curve of the seal face detector. The figure above shows the precision and recall at various thresholds of acceptance of the face detector. The precision‐recall was calculated from running 5‐fold cross‐validation on our dataset</p>
          </caption>
          <graphic xlink:href="ECE3-12-e8851-g005" position="anchor" id="jats-graphic-9"/>
        </fig>
      </sec>
      <sec id="ece38851-sec-0007">
        <label>2.2.3</label>
        <title>Landmark location</title>
        <p>Face alignment is critical for the accuracy of our face recognition software. As a result, prior to chipping the individual seal faces, we aligned them using the manually tagged eye (landmark) locations in each photo by performing in‐plane rotation to align the eyes along the x‐axis. Once the eyes are manually located in each photo, the GUI automatically aligns and chips the faces to the desired size (e.g., 112 × 112 pixels). We followed an approach similar to that used by the developers of LemurFaceID (Crouse et al., <xref rid="ece38851-bib-0007" ref-type="bibr">2017</xref>) to align faces: Given <mml:math id="jats-math-1"><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:math> and <mml:math id="jats-math-2"><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:math> to be the center of the left and right eyes respectively, one can calculate the rotation matrix <italic toggle="yes">M</italic> to be used in an affine transformation of the image. Let <mml:math id="jats-math-3"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math> and <mml:math id="jats-math-4"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math> and <mml:math id="jats-math-5"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>‐</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>‐</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mfenced></mml:mrow></mml:math>, so <mml:math id="jats-math-6"><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced></mml:math> will be the location of the midpoint between the centers of the two eyes and <mml:math id="jats-math-7"><mml:mi>θ</mml:mi></mml:math> be the rotational angle. Then <italic toggle="yes">M</italic> will be calculated as:<disp-formula id="ece38851-disp-0001"><mml:math id="jats-math-8" display="block"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>cos</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>‐</mml:mo><mml:mi>sin</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>‐</mml:mo><mml:mi>cos</mml:mi><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>y</mml:mi><mml:mi>sin</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mi>sin</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>cos</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>‐</mml:mo><mml:mi>cos</mml:mi><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>‐</mml:mo><mml:mi>x</mml:mi><mml:mi>sin</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula>
</p>
      </sec>
      <sec id="ece38851-sec-0008">
        <label>2.2.4</label>
        <title>Face alignment and chipping</title>
        <p>Inter‐pupil distance (IPD) is the distance between the center of the two eyes, or <mml:math id="jats-math-9"><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>‐</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>‐</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:math> We scaled each image automatically so that each eye would be <mml:math id="jats-math-10"><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>×</mml:mo><mml:mi>I</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:math> away from the closest side edge and <mml:math id="jats-math-11"><mml:mrow><mml:mn>0.6</mml:mn><mml:mo>×</mml:mo><mml:mi>I</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:math> away from the top edge of the cropped face image. We chose these values by sampling 30 seal images and determining the optimum face to background ratio for facial recognition.</p>
        <p>Thus, at the end of this step, each face image was rotated and resized to 112 × 112 pixels in preparation for facial recognition. The image label will contain information about its original image and the location within the original image from which it was chipped. Chips from multiple photographs of the same seal are clustered manually as a set that can act as probe images (if they are unknown) or gallery images (once they have been labeled with a name and ID number).</p>
      </sec>
    </sec>
    <sec id="ece38851-sec-0009">
      <label>2.3</label>
      <title>Development of SealNet</title>
      <sec id="ece38851-sec-0010">
        <label>2.3.1</label>
        <title>SealNet architecture</title>
        <p>The CNN‐based face recognition classifier is the main component of our software package. We train this classifier with photos that have been aligned, chipped, and normalized. Each input image underwent four convolutional blocks and a final bottleneck layer to output an embedded vector of length 512 that contained learned features of the input image (Figure <xref rid="ece38851-fig-0005" ref-type="fig">5</xref>). See Appendix <xref rid="ece38851-sup-0003" ref-type="supplementary-material">S1</xref> for additional details on the methodology involved in the development of SealNet.</p>
        <fig position="float" fig-type="FIGURE" id="ece38851-fig-0005">
          <label>FIGURE 5</label>
          <caption>
            <p>The architecture of SealNet’s recognition convolutional neural network. (a) The full operation of a single convolutional layer. Group Convolution is performed on the layer input. Shuffle operations are then performed on the Group Convolution’s output. Then, the output of this becomes the input to the next layer. (b) The architecture of SealNet’s layers and their respective parameters are pictured in the lower part of the illustration. The input is a photo of a seal and the output is a vector of length 512 representing the features extracted. *Image credit: Xu et al. (<xref rid="ece38851-bib-0032" ref-type="bibr">2020</xref>). **Image credit: Zhang et al. (<xref rid="ece38851-bib-0034" ref-type="bibr">2018</xref>)</p>
          </caption>
          <graphic xlink:href="ECE3-12-e8851-g004" position="anchor" id="jats-graphic-11"/>
        </fig>
      </sec>
      <sec id="ece38851-sec-0011">
        <label>2.3.2</label>
        <title>Validation of SealNet</title>
        <p>In this biometric system, the probe set refers to the collection of biometric identities to be recognized, while the gallery set refers to identities that have been previously enrolled into the system. The gallery set acts as a database from which each probe identity will be searched. We measured the accuracy of SealNet with two standard recognition tasks: closed‐set and open‐set identification. In closed‐set identification, it is guaranteed that the identity in the probe is present in the gallery; whereas in open‐set identification, it is uncertain whether that is the case. Both closed‐set and open‐set refer to 1:N matching scenarios where each identity in the probe set will be searched against multiple identities in the gallery. The SealNet face recognition software produces a similarity score for each probe‐gallery pair and the result will be sorted in descending order so that the identity with the highest score will be the most likely matched candidate. We trained the model on an average of 485 chips of the same resolution for each fold of the closed‐set and on 533 chips with dimensions (112, 112) for the open‐set. We validated SealNet's face recognition capabilities using 5‐fold cross‐validation, with seals that have more photos than the number of folds.</p>
        <p>To see how well our software performed compared to a previously developed facial recognition software, PrimNet, we trained and tested it and SealNet models using the same data and parameters We also measured how SealNet performs as we increase the size of our seal database (gallery set), for instance, by adding seals from new locations or new dates. Each time new data were added, we evaluated the model's closed‐set accuracy by running 5‐fold cross‐validation and calculating its average true identification rate.</p>
      </sec>
      <sec id="ece38851-sec-0012">
        <label>2.3.3</label>
        <title>Developing the database of known individuals</title>
        <p>Using a single folder of manually clustered chips from one site/location, we were able to create a gallery (A) of known individuals as each seal chip cluster was guaranteed to be a separate individual. We probed the (A) gallery with (A) images using the RecognitionGUI software, allowing for predictions of individuals. The RecognitionGUI software provides scores for each individual in the gallery, assigning them a number based on their facial biometric similarities to the individual in the probe. Similarity scores are provided for the top five ranked matches; top matches are confirmed by visual check in the output graphic. We added each known match to the database with a novel name and ID number. This (A) gallery was the foundation of our harbor seal ID database.</p>
      </sec>
      <sec id="ece38851-sec-0013">
        <label>2.3.4</label>
        <title>Probing the database with new individuals</title>
        <p>We subsequently processed folders of clustered chips (individual seals) from additional sites or days (B, C, D…) as probes to our original (A) gallery, one at a time. To add new individuals to the gallery database, a “match” probe photo (a photo from a new site or day that is already in the ID database) can be automatically merged to its matching file name and ID number. If the new individual does not match previous individuals in the database, a novel ID is created, and the seal photos and ID are added to the existing database. To speed up this process, we created a quick Python program, Tkinter GUI (Lundh, <xref rid="ece38851-bib-0023" ref-type="bibr">1999</xref>) and added this program to the SealNet package.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="results" id="ece38851-sec-0014">
    <label>3</label>
    <title>RESULTS</title>
    <sec id="ece38851-sec-0015">
      <label>3.1</label>
      <title>Automatic face detection</title>
      <p>We found that SealNet's face detector has a precision value (the percentage of predictions that are seal face) of 85.43% and a recall value (the percentage of total seal faces that are correctly predicted) of 86.94% after being trained on a dataset of 516 photos from one haul‐out site on a single day that contained 1178 valid seal faces. Figure <xref rid="ece38851-fig-0004" ref-type="fig">4</xref> shows the accuracy of our model across different classification threshold levels for detecting a seal face. As the value of threshold decreases, the precision decreases to 0 while the recall approaches to 1. On the other hand, if threshold increases, the precision increases to 1 but the recall will decrease to 0. We chose threshold 0 for our face detector because it gives the best precision‐recall trade‐off.</p>
      <p>We detected 49 false positives, that is, faces detected by SealNet that were not faces. Most were caused by vegetation or other parts of the seal that had face‐like shapes (Figure <xref rid="ece38851-sup-0001" ref-type="supplementary-material">S1</xref>). SealNet missed on average 43 faces, mostly ones that were angled away from the camera (false negatives, Figure <xref rid="ece38851-sup-0002" ref-type="supplementary-material">S2</xref>). We detected a total 408 unique seals, with an average of 2.9 photos per seal. Among these, 74 seals appeared in at least 5 photos.</p>
    </sec>
    <sec id="ece38851-sec-0016">
      <label>3.2</label>
      <title>Accuracy in seal identification</title>
      <p>Our closed set data contained the 74 seals (same day/same location) that had at least 5 photos (607 photos in total). For each fold, the testing set contains one‐fifth of the number of photos of each of the 74 seals, and the training set contains the remaining photos of those “known” seals. We trained and tested both PrimNet and SealNet on the same data for each fold. Our average rank‐1 accuracy was 88(±0.03)% and our average rank‐5 accuracy was 96(±0.01)% across the 5‐folds (Table <xref rid="ece38851-tbl-0002" ref-type="table">2</xref>). PrimNet yielded 70% rank‐1 accuracy and 91% rank‐5 accuracy on the same dataset.</p>
      <table-wrap position="float" id="ece38851-tbl-0002" content-type="TABLE">
        <label>TABLE 2</label>
        <caption>
          <p>Comparison of open‐set performance between SealNet and PrimNet for key metrics of model evaluation</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <thead valign="top">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="top" rowspan="1" colspan="1">F‐SCORE</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Rank</th>
              <th align="left" valign="top" rowspan="1" colspan="1"/>
              <th align="left" valign="top" rowspan="1" colspan="1">TPR</th>
              <th align="left" valign="top" rowspan="1" colspan="1">FPR</th>
              <th align="left" valign="top" rowspan="1" colspan="1">FNR</th>
              <th align="left" valign="top" rowspan="1" colspan="1">TNR</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Baseline</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Accuracy</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Precision</th>
              <th align="left" valign="top" rowspan="1" colspan="1">F‐Score</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="4" colspan="1">SealNet</td>
              <td align="left" rowspan="2" colspan="1">R1</td>
              <td align="left" rowspan="1" colspan="1">MEAN</td>
              <td align="char" char="." rowspan="1" colspan="1">0.678</td>
              <td align="char" char="." rowspan="1" colspan="1">0.032</td>
              <td align="char" char="." rowspan="1" colspan="1">0.322</td>
              <td align="char" char="." rowspan="1" colspan="1">0.968</td>
              <td align="char" char="." rowspan="1" colspan="1">0.892</td>
              <td align="char" char="." rowspan="1" colspan="1">0.945</td>
              <td align="char" char="." rowspan="1" colspan="1">0.656</td>
              <td align="char" char="." rowspan="1" colspan="1">0.663</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">SD</td>
              <td align="char" char="." rowspan="1" colspan="1">0.019</td>
              <td align="char" char="." rowspan="1" colspan="1">0.012</td>
              <td align="char" char="." rowspan="1" colspan="1">0.019</td>
              <td align="char" char="." rowspan="1" colspan="1">0.012</td>
              <td align="char" char="." rowspan="1" colspan="1">0.011</td>
              <td align="char" char="." rowspan="1" colspan="1">0.011</td>
              <td align="char" char="." rowspan="1" colspan="1">0.094</td>
              <td align="char" char="." rowspan="1" colspan="1">0.048</td>
            </tr>
            <tr>
              <td align="left" rowspan="2" colspan="1">R5</td>
              <td align="left" rowspan="1" colspan="1">MEAN</td>
              <td align="char" char="." rowspan="1" colspan="1">0.705</td>
              <td align="char" char="." rowspan="1" colspan="1">0.032</td>
              <td align="char" char="." rowspan="1" colspan="1">0.295</td>
              <td align="char" char="." rowspan="1" colspan="1">0.968</td>
              <td align="char" char="." rowspan="1" colspan="1">0.892</td>
              <td align="char" char="." rowspan="1" colspan="1">0.947</td>
              <td align="char" char="." rowspan="1" colspan="1">0.664</td>
              <td align="char" char="." rowspan="1" colspan="1">0.681</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">SD</td>
              <td align="char" char="." rowspan="1" colspan="1">0.023</td>
              <td align="char" char="." rowspan="1" colspan="1">0.012</td>
              <td align="char" char="." rowspan="1" colspan="1">0.023</td>
              <td align="char" char="." rowspan="1" colspan="1">0.012</td>
              <td align="char" char="." rowspan="1" colspan="1">0.011</td>
              <td align="char" char="." rowspan="1" colspan="1">0.011</td>
              <td align="char" char="." rowspan="1" colspan="1">0.093</td>
              <td align="char" char="." rowspan="1" colspan="1">0.049</td>
            </tr>
            <tr>
              <td align="left" rowspan="4" colspan="1">PrimNet</td>
              <td align="left" rowspan="2" colspan="1">R1</td>
              <td align="left" rowspan="1" colspan="1">MEAN</td>
              <td align="char" char="." rowspan="1" colspan="1">0.251</td>
              <td align="char" char="." rowspan="1" colspan="1">0.057</td>
              <td align="char" char="." rowspan="1" colspan="1">0.749</td>
              <td align="char" char="." rowspan="1" colspan="1">0.943</td>
              <td align="char" char="." rowspan="1" colspan="1">0.868</td>
              <td align="char" char="." rowspan="1" colspan="1">0.888</td>
              <td align="char" char="." rowspan="1" colspan="1">0.285</td>
              <td align="char" char="." rowspan="1" colspan="1">0.259</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">SD</td>
              <td align="char" char="." rowspan="1" colspan="1">0.060</td>
              <td align="char" char="." rowspan="1" colspan="1">0.019</td>
              <td align="char" char="." rowspan="1" colspan="1">0.060</td>
              <td align="char" char="." rowspan="1" colspan="1">0.019</td>
              <td align="char" char="." rowspan="1" colspan="1">0.017</td>
              <td align="char" char="." rowspan="1" colspan="1">0.014</td>
              <td align="char" char="." rowspan="1" colspan="1">0.059</td>
              <td align="char" char="." rowspan="1" colspan="1">0.038</td>
            </tr>
            <tr>
              <td align="left" rowspan="2" colspan="1">R5</td>
              <td align="left" rowspan="1" colspan="1">MEAN</td>
              <td align="char" char="." rowspan="1" colspan="1">0.281</td>
              <td align="char" char="." rowspan="1" colspan="1">0.057</td>
              <td align="char" char="." rowspan="1" colspan="1">0.719</td>
              <td align="char" char="." rowspan="1" colspan="1">0.943</td>
              <td align="char" char="." rowspan="1" colspan="1">0.868</td>
              <td align="char" char="." rowspan="1" colspan="1">0.891</td>
              <td align="char" char="." rowspan="1" colspan="1">0.307</td>
              <td align="char" char="." rowspan="1" colspan="1">0.285</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">SD</td>
              <td align="char" char="." rowspan="1" colspan="1">0.063</td>
              <td align="char" char="." rowspan="1" colspan="1">0.019</td>
              <td align="char" char="." rowspan="1" colspan="1">0.063</td>
              <td align="char" char="." rowspan="1" colspan="1">0.019</td>
              <td align="char" char="." rowspan="1" colspan="1">0.017</td>
              <td align="char" char="." rowspan="1" colspan="1">0.014</td>
              <td align="char" char="." rowspan="1" colspan="1">0.047</td>
              <td align="char" char="." rowspan="1" colspan="1">0.036</td>
            </tr>
            <tr>
              <td align="left" rowspan="2" colspan="1">Difference</td>
              <td align="left" rowspan="1" colspan="1">R1</td>
              <td align="left" rowspan="1" colspan="1">MEAN</td>
              <td align="char" char="." rowspan="1" colspan="1">0.427</td>
              <td align="char" char="." rowspan="1" colspan="1">−0.025</td>
              <td align="char" char="." rowspan="1" colspan="1">−0.427</td>
              <td align="char" char="." rowspan="1" colspan="1">0.025</td>
              <td align="char" char="." rowspan="1" colspan="1">0.023</td>
              <td align="char" char="." rowspan="1" colspan="1">0.057</td>
              <td align="char" char="." rowspan="1" colspan="1">0.371</td>
              <td align="char" char="." rowspan="1" colspan="1">0.405</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">R5</td>
              <td align="left" rowspan="1" colspan="1">MEAN</td>
              <td align="char" char="." rowspan="1" colspan="1">0.424</td>
              <td align="char" char="." rowspan="1" colspan="1">−0.025</td>
              <td align="char" char="." rowspan="1" colspan="1">−0.424</td>
              <td align="char" char="." rowspan="1" colspan="1">0.025</td>
              <td align="char" char="." rowspan="1" colspan="1">0.023</td>
              <td align="char" char="." rowspan="1" colspan="1">0.057</td>
              <td align="char" char="." rowspan="1" colspan="1">0.357</td>
              <td align="char" char="." rowspan="1" colspan="1">0.396</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot id="ece38851-ntgp-0003">
          <title>Note</title>
          <fn id="ece38851-note-0003">
            <p>In open‐set evaluation, any probe with a similarity score for its best match in the gallery less than the value of the threshold was rejected as an “imposter”. True Positives scored above the threshold and correct match was predicted within top “Rank” similarity scores (TPR). False Positives scored above the threshold but had no true match in gallery (FPR). False Negatives contained a match in gallery but had a top similarity score below the threshold, or the correct prediction for gallery member was not within the top “Rank” similarity scores (FNR). True Negatives had no match in the gallery and top predicted match had a similarity score below the threshold (TNR). Baseline accuracy is the accuracy score of the model assuming all probes were rejected. F1‐Score provides a better measure of propensity for incorrect classifications than accuracy, suited to unbalanced datasets.</p>
          </fn>
        </table-wrap-foot>
        <permissions>
          <copyright-holder>John Wiley &amp; Sons, Ltd</copyright-holder>
        </permissions>
      </table-wrap>
      <p>Our open set data also included 74 seals with at least 5 photos and 571 photos from seals with fewer than 5 photos. Both PrimNet and SealNet models were trained and tested utilizing the same splits of data and equivalent parameters for number of epochs and batches per epoch to ensure fairness. F1 scores (defined as the harmonic mean of precision and recall), a measure of model performance for unbalanced datasets, showed a similar result with SealNet performing 39.6%–40.5% better than PrimNet (Table <xref rid="ece38851-tbl-0002" ref-type="table">2</xref> and Table <xref rid="ece38851-sup-0003" ref-type="supplementary-material">S1</xref>).</p>
    </sec>
    <sec id="ece38851-sec-0017">
      <label>3.3</label>
      <title>SealNet's performance on a growing dataset</title>
      <p>We expanded the size of our closed set data five times, adding a new folder of seals and retraining the software each time, so our database increased from 194 to 406 unique seals. We calculated the average accuracy of both rank‐1 and rank‐5 training runs as shown in Table <xref rid="ece38851-tbl-0003" ref-type="table">3</xref>. Our data suggest that our model performs consistently (at the same accuracy level) as the size of our dataset increases.</p>
      <table-wrap position="float" id="ece38851-tbl-0003" content-type="TABLE">
        <label>TABLE 3</label>
        <caption>
          <p>Iterative training accuracy for closed‐set data</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <thead valign="top">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="top" rowspan="1" colspan="1"># Seals</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Rank 1 Accuracy</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Rank 5 Accuracy</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">197</td>
              <td align="char" char="." rowspan="1" colspan="1">0.850</td>
              <td align="char" char="." rowspan="1" colspan="1">0.950</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">324</td>
              <td align="char" char="." rowspan="1" colspan="1">0.871</td>
              <td align="char" char="." rowspan="1" colspan="1">0.966</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">389</td>
              <td align="char" char="." rowspan="1" colspan="1">0.865</td>
              <td align="char" char="." rowspan="1" colspan="1">0.968</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">397</td>
              <td align="char" char="." rowspan="1" colspan="1">0.856</td>
              <td align="char" char="." rowspan="1" colspan="1">0.956</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">405</td>
              <td align="char" char="." rowspan="1" colspan="1">0.871</td>
              <td align="char" char="." rowspan="1" colspan="1">0.958</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot id="ece38851-ntgp-0004">
          <title>Note</title>
          <fn id="ece38851-note-0004">
            <p>The average rank‐1 and rank‐5 accuracy levels for each iterative training run following the probing of individuals from each date during 2020; accuracies are relatively robust to numbers of individuals.</p>
          </fn>
        </table-wrap-foot>
        <permissions>
          <copyright-holder>John Wiley &amp; Sons, Ltd</copyright-holder>
        </permissions>
      </table-wrap>
      <p>Our Tkinter GUI allows for direct comparison of our existing seal database with new probed individuals, supported by similarity scores of the individuals in the gallery to the current probe (Figure <xref rid="ece38851-fig-0006" ref-type="fig">6</xref>). These similarity scores are based on facial biometrics of each individual, and they are determined after each image within a seal individual folder is compared to each image found in a gallery seal individual folder. On average, the similarity scores for individuals that are a match with the probe individual are approximately 0.65 with a SD of 0.1 (range 0.55–0.85). Similarity scores with individuals without a match range in similarity scores from 0.2 to 0.5.</p>
      <fig position="float" fig-type="FIGURE" id="ece38851-fig-0006">
        <label>FIGURE 6</label>
        <caption>
          <p>SealNet prediction of individual seals. The image above shows the probing process in which our recognition software matches individuals according to facial biometrics. Scores for each set of individuals in the training dataset are determined based on similarity in facial features to the probe individual, with the highest score indicating the most likely individual to be a match</p>
        </caption>
        <graphic xlink:href="ECE3-12-e8851-g003" position="anchor" id="jats-graphic-13"/>
      </fig>
    </sec>
    <sec id="ece38851-sec-0018">
      <label>3.4</label>
      <title>Ecological results</title>
      <p>SealNet identified four individual seals that were photographed in both 2019 and 2020: 015_Armani, 198_Petal, 211_Clove, and 393_Cystine. All four seals were originally photographed on Brandt Ledges in 2019 and were re‐photographed on Mitchell Fields (198_Petal and 211_Clove), Whaleboat (393_Cystine), or Branning Ledges (015_Armani) during the 2020 season. These preliminary findings suggest that some harbor seals exhibit site fidelity within local bays across years, and that there may be evidence of spatial connectivity among haul‐out sites.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="ece38851-sec-0019">
    <label>4</label>
    <title>DISCUSSION</title>
    <p>Here we present the utility of a new software package, SealNet, an automated pipeline to non‐invasively identify individual seals in photographic images. We describe a novel face detector GUI trained to detect harbor seal faces and the development of a new neural network to classify individual seal faces. Following validation of the technique, we use SealNet in a preliminary study to explore site‐fidelity of harbor seals in the Casco Bay, Maine region of the northwestern Atlantic coast. Our initial validation analyses confirm the efficiency and accuracy of our facial recognition technology in the photo identification of an economically important coastal marine mammal.</p>
    <sec id="ece38851-sec-0020">
      <label>4.1</label>
      <title>Performance of automated SealNet pipeline</title>
      <p>Our trained face detector had a precision of 85%, and a recall of 87% despite having very little restrictions on the position of the seals within the photo with the only limitation that both eyes of the seals are visible. This feature enables the successful use of SealNet software to conduct studies on animals in the wild without having the animals looking directly at the camera. The precision and recall of our detector could be increased by restricting the possible angle and pose of the seal, but this would limit the number of photos that meet such requirements in field studies.</p>
      <p>For our recognition software, we have achieved high accuracy in both close‐set (rank‐1: 88% and rank‐5: 96%) and open‐set (rank‐1 and rank‐5: 93%) analyses, but there is still room for improvement. CNN‐based facial recognition software achieves identification accuracies of 93.8% with lemurs, 92.5% with chimpanzees (Schofield et al., <xref rid="ece38851-bib-0027" ref-type="bibr">2019</xref>), and 97.27% with pandas (Chen et al., <xref rid="ece38851-bib-0004" ref-type="bibr">2020</xref>). Another software, BearID, recently achieved close to 100% face chipping accuracy (number of faces detected in an unprocessed photo) despite an overall pipeline identification accuracy of 82.4% (Clapham et al., <xref rid="ece38851-bib-0005" ref-type="bibr">2020</xref>). FaceNet (Schroff et al., <xref rid="ece38851-bib-0028" ref-type="bibr">2015</xref>) which was trained on more than 3 million images of almost 10,000 unique human individuals, achieved an accuracy of almost 100%. Therefore, with a larger dataset with more photos per seal, it is possible that we can further improve our accuracy. Accuracy in studies utilizing pelage markings in seals is generally lower than facial recognition studies, with the rank‐1 accuracies of 59% and rank‐10 accuracies of 67% (Cunningham, <xref rid="ece38851-bib-0008" ref-type="bibr">2009</xref>).</p>
      <p>In a direct performance comparison of the classification task, SealNet performs better than PrimNet on average at all ranks with improved classification accuracy of up to 18% improvement at rank‐1 for closed‐set and 6% improvement for open‐set. It is also important to note that our model performs consistently well as our database increases in size. The consistent performance of our model demonstrates that SealNet generalizes well (i.e., overfitting is not an issue).</p>
    </sec>
    <sec id="ece38851-sec-0021">
      <label>4.2</label>
      <title>Preliminary ecological results</title>
      <p>Using the SealNet facial recognition software package and a small, initial dataset sampled across two years (2019 and 2020) in Casco Bay, we identified four individuals in the datasets from both years, indicating a small degree of local site fidelity across years during the months of June and July. All four seals were found on the haul‐out site, Brandt Ledges, in 2019. In 2020, the four seals were photographed again within 1–3 nautical miles of Brandt Ledges: one was photographed on Branning Ledges, two were photographed at the Mitchell Field site, and one was photographed on the Whaleboat Island site. This result supports previous results suggesting site fidelity among harbor seals off the coast of NE Scotland (Cordes &amp; Thompson, <xref rid="ece38851-bib-0006" ref-type="bibr">2015</xref>). It is also interesting to note that two of the individuals found in the dataset from both years, Clove and Petal, were found together initially on Brandt Ledges on one day in 2019, and then found together again in 2020 at the Mitchell Field site. These results suggest that SealNet software may be useful in future long‐term studies of social relationships or aggregations in harbor seals. Previous studies have examined competitive relationships among harbor seals (Honeywell &amp; Maher, <xref rid="ece38851-bib-0017" ref-type="bibr">2017</xref>). However, further research is needed to examine other behavior‐related questions, including social fidelity, persistence of family groups, and other social dynamics.</p>
      <p>Our preliminary ecological results suggest some site‐fidelity of harbor seals in Middle Bay as well as site‐fidelity to neighboring haul‐out sites within the bay. However, the initial photographic study was designed to provide the optimal photographic data for the development and training of SealNet. A more extensive ecological study is underway to determine the degree of site fidelity and spatial connectivity of haul‐out sites in this region. In addition, more extensive photographic data will help refine a population estimate for the number of seals utilizing Middle Bay. Current estimates of harbor seal abundance are outdated, suggesting a population of 38,014 individuals in the whole of Maine in 2001 (Gilbert et al., <xref rid="ece38851-bib-0012" ref-type="bibr">2005</xref>), followed by an aerial survey done in 2012 determining a population of 75,834 individuals (Waring et al., <xref rid="ece38851-bib-0030" ref-type="bibr">2015</xref>). Accurate local and regional population estimates are imperative to understanding the dynamics of seal abundance in relationship to anthropomorphic and climate changes to coastal marine environments, as well as the impact of an increasing great white shark population.</p>
      <p>The use of facial recognition software to identify individuals in wild populations is a relatively new area of research and is primarily utilized in studies of land mammals such as lemurs and brown bears (Clapham et al., <xref rid="ece38851-bib-0005" ref-type="bibr">2020</xref>; Crouse et al., <xref rid="ece38851-bib-0007" ref-type="bibr">2017</xref>). Our research extends the use of such methods to marine mammal species. Facial biometrics are not the only measure that can be used for automated identification of seals. For example, a recent, groundbreaking study utilized pelage markings found on the seals coat to identify grey seal individuals near Wales (Langley et al., <xref rid="ece38851-bib-0022" ref-type="bibr">2020</xref>). Given that coat patterns change across seasons during molting or over time in harbor seals, facial biometrics may offer an additional and/or more reliable method of identification. Thus, the development of facial recognition techniques for harbor seals allows for a rapid, non‐invasive means for detailed study of an economically and ecologically important species. Importantly, researchers can customize the software and the GUI to suit their own needs at each step of data collection—training the face detector for additional species, modifying the alignment procedure, or preprocessing images for face recognition.</p>
    </sec>
    <sec id="ece38851-sec-0022">
      <label>4.3</label>
      <title>Limitations</title>
      <p>Although SealNet produced promising results, there are still limitations that need to be addressed. First, our SealNet software still requires some manual work during the data collection process—after running the automatic face detector, researchers are still required to manually locate the eyes, nose and mouth in order for the program to automatically align and chip the seal faces. Thus, one possible improvement that we can implement in the future is to add a landmark detector to be used in conjunction with the face detector. Secondly, to generate training data, researchers must manually group multiple face chips belonging to the same individual. Not only is this process laborious, it may be also error prone. A more sustainable approach would be to implement a classifier; however, researchers would still be required to manually check if the classification is accurate.</p>
      <p>Although SealNet does well in closed‐set classification, open‐set verification performance could be improved by reducing the similarity scores between such seals. This success could be achieved with changes to our model architecture. However, the inherent complexity in any attempt to leverage specificity, while simultaneously avoiding overfitting, presents a difficult balance which all recognition models struggle to strike. Thus, the best approach to this problem would be to maximize the quantity and quality of information available to the model through preprocessing improvements prior to making changes to the CNN architecture itself.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="ece38851-sec-0023">
    <label>5</label>
    <title>CONCLUSION</title>
    <p>We describe the development of SealNet, a novel facial recognition software package that includes an automated pipeline to detect individual seals from field photographs with high accuracy. The use of SealNet to identify individual harbor seals has multiple future applications to aid in decision‐making for conservation efforts, including assessments of seal abundance, evaluation of site fidelity within and across coastal regions, determination of trends in migration patterns, and the exploration of patterns in social behavior among harbor seals at haul‐out sites. The ease and wealth of data that can be collected with non‐invasive photography, coupled with the predictive ability of the SealNet to identify individuals, provides researchers with a robust toolkit that has the potential to transform ecological studies of wild populations of harbor seals. SealNet's ability to retrain and recognize additional marine mammal species provides a vital tool for ecological and behavioral studies of marine mammals in the developing field of conservation technology.</p>
  </sec>
  <sec sec-type="COI-statement" id="ece38851-sec-0025">
    <title>CONFLICT OF INTEREST</title>
    <p>The author(s) declare no competing interests.</p>
  </sec>
  <sec id="ece38851-sec-0026">
    <title>AUTHOR CONTRIBUTIONS</title>
    <p><bold>Zach Birenbaum:</bold> Data curation (equal); Investigation; Methodology; Software (equal); Validation (equal); Visualization (equal). <bold>Hieu Do:</bold> Data curation (equal); Investigation; Methodology; Software (equal); Validation (equal); Visualization (equal); Writing – original draft. <bold>Lauren Horstmyer:</bold> Investigation (equal); Methodology (equal); Visualization (equal); Writing – original draft (equal). <bold>Hailey Orff:</bold> Data curation (equal); Investigation. <bold>Krista Ingram:</bold> Conceptualization (lead); Investigation (equal); Methodology (equal); Project administration (lead); Resources (equal); Supervision (equal); Writing – original draft (equal); Writing – review &amp; editing (lead). <bold>Ahmet Ay:</bold> Conceptualization (equal); Data curation (equal); Formal analysis (equal); Investigation; Methodology; Project administration (equal); Resources; Software (equal); Supervision; Validation (equal); Writing – original draft.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material id="ece38851-sup-0001" position="float" content-type="local-data">
      <caption>
        <p>Figure S1</p>
      </caption>
      <media xlink:href="ECE3-12-e8851-s002.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="ece38851-sup-0002" position="float" content-type="local-data">
      <caption>
        <p>Figure S2</p>
      </caption>
      <media xlink:href="ECE3-12-e8851-s001.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="ece38851-sup-0003" position="float" content-type="local-data">
      <caption>
        <p>Appendix S1</p>
      </caption>
      <media xlink:href="ECE3-12-e8851-s003.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ece38851-sec-0024">
    <title>ACKNOWLEDGMENTS</title>
    <p>We would like to acknowledge the assistance of Rebecca Gowen, Daniel Jaris, and Nick Knight in the collection of seal images in Casco Bay, and Allan Filipowicz for piloting the boat.</p>
  </ack>
  <sec sec-type="data-availability" id="ece38851-sec-0028">
    <title>DATA AVAILABILITY STATEMENT</title>
    <p><italic toggle="yes">SealNet</italic>is an open‐source application available on GitHub at <ext-link xlink:href="https://github.com/zbirenbaum/SealFaceRecognition" ext-link-type="uri" specific-use="software is-supplemented-by">https://github.com/zbirenbaum/SealFaceRecognition</ext-link>. The code and models are both also archived at Zenodo (Accession number: <ext-link xlink:href="https://doi.org/10.5281/zenodo.6415595" ext-link-type="uri" specific-use="dataset is-supplemented-by">https://doi.org/10.5281/zenodo.6415595</ext-link>). Owing to file sizes, raw images will only be available upon request to the authors.</p>
  </sec>
  <ref-list content-type="cited-references" id="ece38851-bibl-0001">
    <title>REFERENCES</title>
    <ref id="ece38851-bib-0001">
      <mixed-citation publication-type="journal" id="ece38851-cit-0001"><string-name><surname>Aarts</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Brasseur</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Poos</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Schop</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kirkwood</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>van Kooten</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Mul</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Reijnders</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Rijnsdorp</surname>, <given-names>A. D.</given-names></string-name>, &amp; <string-name><surname>Tulp</surname>, <given-names>I.</given-names></string-name> (<year>2019</year>). <article-title>Top‐down pressure on a coastal ecosystem by harbor seals</article-title>. <source>Ecosphere</source>, <volume>10</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1002/ecs2.2538</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0002">
      <mixed-citation publication-type="journal" id="ece38851-cit-0002"><string-name><surname>Allen</surname>, <given-names>S. G.</given-names></string-name>, <string-name><surname>Ainley</surname>, <given-names>D. G.</given-names></string-name>, <string-name><surname>Page</surname>, <given-names>G. W.</given-names></string-name>, &amp; <string-name><surname>Ribic</surname>, <given-names>C. A.</given-names></string-name> (<year>1984</year>). <article-title>The effect of disturbance on harbor seal haul out patterns at Bolinas Lagoon</article-title>. <source>California. Fishery Bulletin</source>, <volume>82</volume>(<issue>3</issue>), <fpage>493</fpage>–<lpage>500</lpage>.</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0003">
      <mixed-citation publication-type="journal" id="ece38851-cit-0003"><string-name><surname>Balmer</surname>, <given-names>B. C.</given-names></string-name>, <string-name><surname>Wells</surname>, <given-names>R. S.</given-names></string-name>, <string-name><surname>Nowacek</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Nowacek</surname>, <given-names>D. P.</given-names></string-name>, <string-name><surname>Schwacke</surname>, <given-names>L. H.</given-names></string-name>, <string-name><surname>McLellan</surname>, <given-names>W. A.</given-names></string-name>, &amp; <string-name><surname>Scharf</surname>, <given-names>F.</given-names></string-name> (<year>2008</year>). <article-title>157 Seasonal abundance and distribution patterns of common bottlenose dolphins (<italic toggle="yes">Tursiops truncatus</italic>) near St. Joseph Bay, Florida, USA</article-title>. <source>Journal of Cetacean Research and Management</source>, <volume>10</volume>, <fpage>157</fpage>–<lpage>167</lpage>.</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0004">
      <mixed-citation publication-type="journal" id="ece38851-cit-0004"><string-name><surname>Chen</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Swarup</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Matkowski</surname>, <given-names>W. M.</given-names></string-name>, <string-name><surname>Kong</surname>, <given-names>A. W. K.</given-names></string-name>, <string-name><surname>Han</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Rong</surname>, <given-names>H.</given-names></string-name> (<year>2020</year>). <article-title>A study on giant panda recognition based on images of a large proportion of captive pandas</article-title>. <source>Ecology and Evolution</source>, <volume>10</volume>(<issue>7</issue>), <fpage>3561</fpage>–<lpage>3573</lpage>. <pub-id pub-id-type="doi">10.1002/ece3.6152</pub-id>
<pub-id pub-id-type="pmid">32274009</pub-id></mixed-citation>
    </ref>
    <ref id="ece38851-bib-0005">
      <mixed-citation publication-type="journal" id="ece38851-cit-0005"><string-name><surname>Clapham</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Darimont</surname>, <given-names>C. T.</given-names></string-name> (<year>2020</year>). <article-title>Automated facial recognition for wildlife that lack unique markings: A deep learning approach for brown bears</article-title>. <source>Ecology and Evolution</source>, <volume>10</volume>(<issue>23</issue>), <fpage>12883</fpage>–<lpage>12892</lpage>. <pub-id pub-id-type="doi">10.1002/ece3.6840</pub-id>
<pub-id pub-id-type="pmid">33304501</pub-id></mixed-citation>
    </ref>
    <ref id="ece38851-bib-0006">
      <mixed-citation publication-type="journal" id="ece38851-cit-0006"><string-name><surname>Cordes</surname>, <given-names>L. S.</given-names></string-name>, &amp; <string-name><surname>Thompson</surname>, <given-names>P. M.</given-names></string-name> (<year>2015</year>). <article-title>Mark‐resight estimates of seasonal variation in harbor seal abundance and site fidelity</article-title>. <source>Population Ecology</source>, <volume>57</volume>(<issue>3</issue>), <fpage>467</fpage>–<lpage>472</lpage>. <pub-id pub-id-type="doi">10.1007/s10144-015-0496-z</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0007">
      <mixed-citation publication-type="journal" id="ece38851-cit-0007"><string-name><surname>Crouse</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Jacobs</surname>, <given-names>R. L.</given-names></string-name>, <string-name><surname>Richardson</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Klum</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Jain</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Baden</surname>, <given-names>A. L.</given-names></string-name>, &amp; <string-name><surname>Tecot</surname>, <given-names>S. R.</given-names></string-name> (<year>2017</year>). <article-title>LemurFaceID: A face recognition system to facilitate individual identification of lemurs</article-title>. <source>BMC Zoology</source>, <volume>2</volume>(2), <fpage>1</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1186/s40850-016-0011-9</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0008">
      <mixed-citation publication-type="journal" id="ece38851-cit-0008"><string-name><surname>Cunningham</surname>, <given-names>L.</given-names></string-name> (<year>2009</year>). <article-title>Using computer‐assisted photo‐identification and capture‐recapture techniques to monitor the conservation status of harbour seals (<italic toggle="yes">Phoca vitulina</italic>)</article-title>. <source>Aquatic Mammals</source>, <volume>35</volume>(<issue>3</issue>), <fpage>319</fpage>–<lpage>329</lpage>. <pub-id pub-id-type="doi">10.1578/AM.35.3.2009.319</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0009">
      <mixed-citation publication-type="book" id="ece38851-cit-0009"><string-name><surname>DeArment</surname>, <given-names>R.</given-names></string-name> (<year>1971</year>). <source>Reaction and adaptability of introduced aoudad sheep</source> (pp. <fpage>1</fpage>–<lpage>20</lpage>). <publisher-name>Texas Parks and Wildlife Department</publisher-name>. Federal Aid Wildlife Restoration Project W‐45‐R‐21.</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0011">
      <mixed-citation publication-type="journal" id="ece38851-cit-0011"><string-name><surname>Elwen</surname>, <given-names>S. H.</given-names></string-name>, <string-name><surname>Reeb</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Thornton</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Best</surname>, <given-names>P. B.</given-names></string-name> (<year>2009</year>). <article-title>A population estimate of Heaviside's dolphins, <italic toggle="yes">Cephalorhynchus heavisidii</italic>, at the southern end of their range</article-title>. <source>Marine Mammal Science</source>, <volume>25</volume>(<issue>1</issue>), <fpage>107</fpage>–<lpage>124</lpage>.</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0012">
      <mixed-citation publication-type="journal" id="ece38851-cit-0012"><string-name><surname>Gilbert</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Waring</surname>, <given-names>G. T.</given-names></string-name>, <string-name><surname>Wynne</surname>, <given-names>K. M.</given-names></string-name>, &amp; <string-name><surname>Guldager</surname>, <given-names>N.</given-names></string-name> (<year>2005</year>). <article-title>Changes in abundance of harbor seals in maine, 1981–2001</article-title>. <source>Marine Mammal Science</source>, <volume>21</volume>(<issue>3</issue>), <fpage>519</fpage>–<lpage>535</lpage>. <pub-id pub-id-type="doi">10.1111/j.1748-7692.2005.tb01246.x</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0013">
      <mixed-citation publication-type="miscellaneous" id="ece38851-cit-0013"><string-name><surname>Glennie</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Thomas</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Speakman</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Garrison</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Takeshita</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Schwacke</surname>, <given-names>L.</given-names></string-name> (<year>2021</year>). <article-title>Estimating spatially‐varying density and time‐varying demographics with open population spatial capture‐recapture: a photo‐ID case study on bottlenose dolphins in Barataria Bay, Louisana, USA</article-title>. arXiv preprint arXiv:2106.09579.</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0014">
      <mixed-citation publication-type="miscellaneous" id="ece38851-cit-0014"><collab collab-type="authors">GPS and VHF Tracking Collars Used for Wildlife Monitoring</collab>
. (<year>2017</year>). Wildlife ACT.</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0015">
      <mixed-citation publication-type="journal" id="ece38851-cit-0015"><string-name><surname>Heithaus</surname>, <given-names>M. R.</given-names></string-name>, <string-name><surname>Frid</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Wirsing</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name><surname>Worm</surname>, <given-names>B.</given-names></string-name> (<year>2008</year>). <article-title>Predicting ecological consequences of marine top predator declines</article-title>. <source>Trends in Ecology &amp; Evolution</source>, <volume>23</volume>(<issue>4</issue>), <fpage>202</fpage>–<lpage>210</lpage>. <pub-id pub-id-type="doi">10.1016/j.tree.2008.01.003</pub-id>
<pub-id pub-id-type="pmid">18308421</pub-id></mixed-citation>
    </ref>
    <ref id="ece38851-bib-0016">
      <mixed-citation publication-type="book" id="ece38851-cit-0016"><string-name><surname>Hindell</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Bradshaw</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Harcourt</surname>, <given-names>C. J.</given-names></string-name>, &amp; <string-name><surname>Guinet</surname>, <given-names>R. C.</given-names></string-name> (<year>2003</year>). <source>Ecosystem monitoring: are seals a potential tool for monitoring change in marine systems? Demography and spatial patterns of Neanderthal disappearance in Eurasia View project SAFS pups View project</source>. <ext-link xlink:href="https://www.researchgate.net/publication/235784405" ext-link-type="uri">https://www.researchgate.net/publication/235784405</ext-link>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0017">
      <mixed-citation publication-type="journal" id="ece38851-cit-0017"><string-name><surname>Honeywell</surname>, <given-names>A. F.</given-names></string-name>, &amp; <string-name><surname>Maher</surname>, <given-names>C. R.</given-names></string-name> (<year>2017</year>). <article-title>Intensity, rate, and outcome of agonistic interactions in harbor seals (<italic toggle="yes">Phoca vitulina</italic> concolor) vary with density on haul‐out ledges</article-title>. <source>Journal of Mammalogy</source>, <volume>98</volume>(<issue>1</issue>), <fpage>135</fpage>–<lpage>142</lpage>. <pub-id pub-id-type="doi">10.1093/jmammal/gyw155</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0018">
      <mixed-citation publication-type="miscellaneous" id="ece38851-cit-0018"><string-name><surname>Hu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Shen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Albanie</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Wu</surname>, <given-names>E.</given-names></string-name> (<year>2017</year>). <article-title>Squeeze‐and‐Excitation Networks</article-title>. <ext-link xlink:href="http://arxiv.org/abs/1709.01507" ext-link-type="uri">http://arxiv.org/abs/1709.01507</ext-link>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0019">
      <mixed-citation publication-type="journal" id="ece38851-cit-0019"><string-name><surname>King</surname>, <given-names>D. E.</given-names></string-name> (<year>2009</year>). <article-title>Dlib‐ml: A Machine Learning Toolkit. In</article-title>. <source>Journal of Machine Learning Research</source>, <volume>10</volume>, <fpage>1755</fpage>–<lpage>1758</lpage>.</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0020">
      <mixed-citation publication-type="miscellaneous" id="ece38851-cit-0020"><string-name><surname>King</surname>, <given-names>D. E.</given-names></string-name> (<year>2015</year>). <article-title>Max‐Margin Object Detection</article-title>. <ext-link xlink:href="http://arxiv.org/abs/1502.00046" ext-link-type="uri">http://arxiv.org/abs/1502.00046</ext-link>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0021">
      <mixed-citation publication-type="book" id="ece38851-cit-0021"><person-group person-group-type="editor"><string-name><surname>Li</surname>, <given-names>S. Z.</given-names></string-name></person-group>, &amp; <person-group person-group-type="editor"><string-name><given-names>A. K.</given-names><surname>Jain</surname></string-name></person-group> (Eds.) (<year>2011</year>). <source>Handbook of Face Recognition</source>, <edition>2</edition>nd ed. <publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0022">
      <mixed-citation publication-type="journal" id="ece38851-cit-0022"><string-name><surname>Langley</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Rosas da Costa Oliver</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hiby</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Stringell</surname>, <given-names>T. B.</given-names></string-name>, <string-name><surname>Morris</surname>, <given-names>C. W.</given-names></string-name>, <string-name><surname>O’Cadhla</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Morgan</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Lock</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Perry</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Westcott</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Boyle</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Büche</surname>, <given-names>B. I.</given-names></string-name>, <string-name><surname>Stubbings</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Boys</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Self</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Lindenbaum</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Strong</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Baines</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Pomeroy</surname>, <given-names>P. P.</given-names></string-name> (<year>2020</year>). <article-title>Site use and connectivity of female grey seals (Halichoerus grypus) around Wales</article-title>. <source>Marine Biology</source>, <volume>167</volume>(<issue>6</issue>), <fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0023">
      <mixed-citation publication-type="book" id="ece38851-cit-0023"><string-name><surname>Lundh</surname>, <given-names>F.</given-names></string-name> (<year>1999</year>). <source>An introduction to tkinter</source>.</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0024">
      <mixed-citation publication-type="book" id="ece38851-cit-0024"><string-name><surname>O’Toole</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name><surname>Gallucci</surname>, <given-names>V. F.</given-names></string-name> (<year>2020</year>). <source>Anthrozoology and public perception: Humans and great white sharks (<italic toggle="yes">Carchardon carcharias</italic>) on cape cod</source>. [Master's thesis, University of Washington]. ResearchWorks Archive. <ext-link xlink:href="http://hdl.handle.net/1773/46087" ext-link-type="uri">http://hdl.handle.net/1773/46087</ext-link>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0025">
      <mixed-citation publication-type="journal" id="ece38851-cit-0025"><string-name><surname>Rayment</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Dawson</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Slooten</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Bräger</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fresne</surname>, <given-names>S. D.</given-names></string-name>, &amp; <string-name><surname>Webster</surname>, <given-names>T.</given-names></string-name> (<year>2009</year>). <article-title>Kernel density estimates of alongshore home range of Hector's dolphins at Banks Peninsula, New Zealand</article-title>. <source>Marine Mammal Science</source>, <volume>25</volume>(<issue>3</issue>), <fpage>537</fpage>–<lpage>556</lpage>. <pub-id pub-id-type="doi">10.1111/j.1748-7692.2008.00271.x</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0026">
      <mixed-citation publication-type="journal" id="ece38851-cit-0026"><string-name><surname>Rosen</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Gerlinsky</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Trites</surname>, <given-names>A.</given-names></string-name> (<year>2017</year>). <article-title>Telemetry tags increase the costs of swimming in northern fur seals, <italic toggle="yes">Callorhinus ursinus</italic>
</article-title>. <source>Marine Mammal Science</source>, <volume>34</volume>, <fpage>385</fpage>–<lpage>402</lpage>. <pub-id pub-id-type="doi">10.1111/mms.12460</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0027">
      <mixed-citation publication-type="journal" id="ece38851-cit-0027"><string-name><surname>Schofield</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Nagrani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zisserman</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hayashi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Matsuzawa</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Biro</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Carvalho</surname>, <given-names>S.</given-names></string-name> (<year>2019</year>). <article-title>Chimpanzee face recognition from videos in the wild using deep learning</article-title>. <source>Science Advances</source>, <volume>5</volume>(<issue>9</issue>), <fpage>1</fpage>–<lpage>9</lpage> eaaw0736. <pub-id pub-id-type="doi">10.1126/sciadv.aaw0736</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0028">
      <mixed-citation publication-type="book" id="ece38851-cit-0028"><string-name><surname>Schroff</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Kalenichenko</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Philbin</surname>, <given-names>J.</given-names></string-name> (<year>2015</year>). <source>FaceNet: A Unified Embedding for Face Recognition and Clustering</source>. <pub-id pub-id-type="doi">10.1109/CVPR.2015.7298682</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0030">
      <mixed-citation publication-type="miscellaneous" id="ece38851-cit-0030"><string-name><surname>Waring</surname>, <given-names>G. T.</given-names></string-name>, <string-name><surname>DiGiovanni</surname>, <given-names>R. A.</given-names><suffix>Jr</suffix></string-name>, <string-name><surname>Josephson</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Wood</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Gilbert</surname>, <given-names>J. R.</given-names></string-name> (<year>2015</year>). <article-title>2012 population estimate for the harbor seal (<italic toggle="yes">Phoca vitulina</italic> concolor) in New England waters</article-title>. NOAA Tech. Memo. NMFS NE‐235. <fpage>15</fpage> pp.</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0031">
      <mixed-citation publication-type="journal" id="ece38851-cit-0031"><string-name><surname>Weinstein</surname>, <given-names>B. G.</given-names></string-name> (<year>2018</year>). <article-title>A computer vision for animal ecology</article-title>. <source>Journal of Animal Ecology</source>, <volume>87</volume>(<issue>3</issue>), <fpage>533</fpage>–<lpage>545</lpage>. <pub-id pub-id-type="doi">10.1111/1365-2656.12780</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0032">
      <mixed-citation publication-type="miscellaneous" id="ece38851-cit-0032"><string-name><surname>Xu</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Ma</surname>, <given-names>C.</given-names></string-name> (<year>2020</year>). <article-title>Research and Verification of Convolutional Neural Network Lightweight in BCI</article-title>. <italic toggle="yes">Computational and mathematical methods in medicine</italic>, 2020, 5916818. <pub-id pub-id-type="doi">10.1155/2020/5916818</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0033">
      <mixed-citation publication-type="book" id="ece38851-cit-0033"><string-name><surname>Zeiler</surname>, <given-names>M. D.</given-names></string-name> (<year>2012</year>). <source>ADADELTA: An Adaptive Learning Rate Method</source>. <ext-link xlink:href="http://arxiv.org/abs/1212.5701" ext-link-type="uri">http://arxiv.org/abs/1212.5701</ext-link>
</mixed-citation>
    </ref>
    <ref id="ece38851-bib-0034">
      <mixed-citation publication-type="book" id="ece38851-cit-0034"><string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Lin</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name> (<year>2018</year>). <part-title>Shufflenet: An extremely efficient convolutional neural network for mobile devices</part-title>. In <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source> (pp. <fpage>6848</fpage>–<lpage>6856</lpage>).</mixed-citation>
    </ref>
  </ref-list>
</back>
