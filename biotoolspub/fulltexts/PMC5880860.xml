<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Behav Res Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Behav Res Methods</journal-id>
    <journal-title-group>
      <journal-title>Behavior Research Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1554-351X</issn>
    <issn pub-type="epub">1554-3528</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5880860</article-id>
    <article-id pub-id-type="publisher-id">909</article-id>
    <article-id pub-id-type="doi">10.3758/s13428-017-0909-3</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Gazepath: An eye-tracking analysis tool that accounts for individual differences and data quality</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>van Renswoude</surname>
          <given-names>Daan R.</given-names>
        </name>
        <address>
          <phone>+31 20 525 6908</phone>
          <email>D.R.vanRenswoude@uva.nl</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Raijmakers</surname>
          <given-names>Maartje E. J.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Koornneef</surname>
          <given-names>Arnout</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Johnson</surname>
          <given-names>Scott P.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hunnius</surname>
          <given-names>Sabine</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Visser</surname>
          <given-names>Ingmar</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000000084992262</institution-id><institution-id institution-id-type="GRID">grid.7177.6</institution-id><institution>Department of Psychology, </institution><institution>University of Amsterdam, </institution></institution-wrap>Amsterdam, The Netherlands </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2312 1970</institution-id><institution-id institution-id-type="GRID">grid.5132.5</institution-id><institution>Department of Education and Child Studies, </institution><institution>Leiden University, </institution></institution-wrap>Leiden, The Netherlands </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9632 6718</institution-id><institution-id institution-id-type="GRID">grid.19006.3e</institution-id><institution>Department of Psychology, </institution><institution>University of California, </institution></institution-wrap>Los Angeles, CA USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000000122931605</institution-id><institution-id institution-id-type="GRID">grid.5590.9</institution-id><institution>Department of Psychology, </institution><institution>Radboud University, </institution></institution-wrap>Nijmegen, The Netherlands </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ISNI">0000000084992262</institution-id><institution-id institution-id-type="GRID">grid.7177.6</institution-id><institution>Research Priority Area Yield, </institution><institution>University of Amsterdam, </institution></institution-wrap>Amsterdam, The Netherlands </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ISNI">0000000084992262</institution-id><institution-id institution-id-type="GRID">grid.7177.6</institution-id><institution>Amsterdam Brain and Cognition, </institution><institution>University of Amsterdam, </institution></institution-wrap>Amsterdam, The Netherlands </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>7</day>
      <month>6</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>7</day>
      <month>6</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2018</year>
    </pub-date>
    <volume>50</volume>
    <issue>2</issue>
    <fpage>834</fpage>
    <lpage>852</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2017</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p>Eye-trackers are a popular tool for studying cognitive, emotional, and attentional processes in different populations (e.g., clinical and typically developing) and participants of all ages, ranging from infants to the elderly. This broad range of processes and populations implies that there are many inter- and intra-individual differences that need to be taken into account when analyzing eye-tracking data. Standard parsing algorithms supplied by the eye-tracker manufacturers are typically optimized for adults and do not account for these individual differences. This paper presents gazepath, an easy-to-use R-package that comes with a graphical user interface (GUI) implemented in Shiny (RStudio Inc <xref ref-type="bibr" rid="CR27">2015</xref>). The gazepath R-package combines solutions from the adult and infant literature to provide an eye-tracking parsing method that accounts for individual differences and differences in data quality. We illustrate the usefulness of gazepath with three examples of different data sets. The first example shows how gazepath performs on free-viewing data of infants and adults, compared to standard EyeLink parsing. We show that gazepath controls for spurious correlations between fixation durations and data quality in infant data. The second example shows that gazepath performs well in high-quality reading data of adults. The third and last example shows that gazepath can also be used on noisy infant data collected with a Tobii eye-tracker and low (60 Hz) sampling rate.</p>
      <sec>
        <title>Electronic supplementary material</title>
        <p> The online version of this article (doi:10.3758/s13428-017-0909-3) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Infant eye movements</kwd>
      <kwd>Eye-tracking methodology</kwd>
      <kwd>Fixation duration</kwd>
      <kwd>Attention</kwd>
      <kwd>Event detection</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Psychonomic Society, Inc. 2018</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p>Eye-tracking has become a popular tool in many psychological disciplines. For instance, eye-tracking is used to study reading abilities (Rayner, Castelhano, &amp; Yang, <xref ref-type="bibr" rid="CR22">2009</xref>) and real-world scene perception (Henderson <xref ref-type="bibr" rid="CR10">2003</xref>) in different types of populations and age groups. For example, eye-trackers enable researchers to quantify differences between clinical populations and healthy controls in disorders such as schizophrenia, attention-deficit hyperactivity disorder (ADHD) and Williams syndrome (e.g., Riby &amp; Hancock, <xref ref-type="bibr" rid="CR26">2008</xref>; Karatekin &amp; Asarnow, <xref ref-type="bibr" rid="CR14">1999</xref>). Even in infants, looking measures have been suggested to predict infants at risk of developing autism (Wass et al. <xref ref-type="bibr" rid="CR37">2015</xref>). In reading research, eye-tracking can provide insights into reading behavior differences between children with and without dyslexia (e.g., Hutzler &amp; Wimmer, <xref ref-type="bibr" rid="CR12">2004</xref>), or between children, adults, and the elderly (Paterson, McGowan, &amp; Jordan, <xref ref-type="bibr" rid="CR20">2013</xref>; Reichle et al., <xref ref-type="bibr" rid="CR25">2013</xref>; Rayner, Reichle, Stroud, Williams, &amp; Pollatsek, <xref ref-type="bibr" rid="CR23">2006</xref>; Rayner et al., <xref ref-type="bibr" rid="CR22">2009</xref>).</p>
    <p>The fact that eye-tracking can be used in such a broad range of populations is one of its main advantages (Karatekin <xref ref-type="bibr" rid="CR13">2007</xref>). However, this also implies that there are most likely individual differences that should be taken into account, especially when comparing different populations. This paper presents gazepath: an R-package developed to detect fixations in eye-tracking data while accounting for individual differences.</p>
    <p>Fixations and saccades are the main elements of gaze patterns. During fixations, visual processing takes place and encoding information in memory is possible, whereas saccades are the rapid eye movements during which visual sensitivity is suppressed (Matin <xref ref-type="bibr" rid="CR17">1974</xref>). In order to analyze gaze patterns, eye-tracking data must be parsed into fixations and saccades. This is commonly accomplished by using dispersion, velocity, and/or acceleration-based algorithms supplied by the eye-tracker manufacturer. For example, EyeLink (SR Research Ltd., Ontario, Canada) uses a velocity threshold of 35 deg/s and an acceleration threshold of 8000 deg/s <sup>2</sup> as default values, although these thresholds can be altered manually. When both speed and acceleration of the eye exceed these thresholds, it is assumed that a saccade took place. Dispersion thresholds, on the other hand, assume that a saccade takes place when a distance threshold is crossed. For instance, the Tobii Clearview 2.7 Tobii Eye Tracker User Manual (<xref ref-type="bibr" rid="CR31">2006</xref>) defines the end of a fixations when the eye has moved .9 <sup>∘</sup> of visual angle, although this threshold can also be set to different values.</p>
    <p>In our eye-tracking studies with infants (Van Renswoude, Johnson, Raijmakers, &amp; Visser, <xref ref-type="bibr" rid="CR33">2016</xref>), we noticed that these standard algorithms with fixed thresholds were often unable to correctly identify fixations and saccades. This is a well-known problem in infant eye-tracking research (e.g., Wass, Forssman, &amp; Leppänen, <xref ref-type="bibr" rid="CR36">2014</xref>; Hessels, Andersson, Hooge, Nyström, &amp; Kemner, <xref ref-type="bibr" rid="CR11">2015</xref>; Gredebäck, Johnson, &amp; von Hofsten, <xref ref-type="bibr" rid="CR8">2009</xref>), as well as in adult eye-tracking research (e.g., Shic, Scassellati, &amp; Chawarska, <xref ref-type="bibr" rid="CR28">2008</xref>; Nyström &amp; Holmqvist, <xref ref-type="bibr" rid="CR19">2010</xref>). The aim of this work is to combine solutions from the fields of adult and infant eye-tracking and develop a tool that can be used to parse eye-tracking data of different populations and data quality into fixations.</p>
    <sec id="Sec2">
      <title>Individual differences</title>
      <p>Standard velocity and dispersion thresholds provided by eye-tracker manufacturers are not always optimal. Sometimes small saccades are missed because the threshold was not crossed, and it also happens that a speed and/or dispersion threshold is crossed, while no actual saccade took place. Optimizing the detection of fixations requires the use of different thresholds for different participants. Even in different blocks or trials, stimuli, tasks, or the mood of the participant can elicit different eye movements that are best classified by different thresholds. Standard algorithms supplied by eye-tracker manufacturers assume one threshold for everyone at every time during the experiment.</p>
      <p>Setting individual thresholds can possibly improve fixation detection, although there are some drawbacks. For instance, in a study it could become difficult to tell whether observed individual differences on the task reflect real underlying differences, or an artifact of the different threshold choices. Study results can depend on these threshold choices. Shic et al. (<xref ref-type="bibr" rid="CR28">2008</xref>) showed that using a different threshold, but the same within groups, can result in the (dis)appearance of an effect between these groups. The use of individual thresholds also complicates the replication and comparison of these studies (Nyström &amp; Holmqvist, <xref ref-type="bibr" rid="CR19">2010</xref>). Therefore, statistical criteria are needed to define threshold values.</p>
      <p>The literature offers several data-driven algorithms for defining thresholds (e.g., Blignaut, <xref ref-type="bibr" rid="CR3">2009</xref>; Shic et al., <xref ref-type="bibr" rid="CR28">2008</xref>; Nyström &amp; Holmqvist, <xref ref-type="bibr" rid="CR19">2010</xref>). In a recent paper, Andersson, Larsson, Holmqvist, Stridh, and Nyström (<xref ref-type="bibr" rid="CR1">2016</xref>) compared ten (mostly data-driven) algorithms with classification by humans. The aim of their study was to find the best performing algorithm, but they found large differences in performance, making it difficult to determine the best. Applied to static stimuli, the adaptive velocity-based algorithm of Nyström and Holmqvist (<xref ref-type="bibr" rid="CR19">2010</xref>) produced similar fixation durations as trained human coders. On a sample-to-sample basis, however, other algorithms performed well. For instance, algorithms that use hidden Markov models (Komogortsev, Gobert, Jayarathna, Koh, &amp; Gowda, <xref ref-type="bibr" rid="CR15">2010</xref>), a binocular-individual threshold (van der Lans, Wedel, &amp; Pieters, <xref ref-type="bibr" rid="CR32">2011</xref>) or a simple velocity threshold had also a close match to the human coders. An algorithm that Andersson et al. (<xref ref-type="bibr" rid="CR1">2016</xref>) did not take into account is the algorithm developed by Mould, Foster, Amano, and Oakley (<xref ref-type="bibr" rid="CR18">2012</xref>). This velocity-based algorithm is completely data-driven, meaning there is no need for initial starting values as in most data-driven algorithms. The Mould et al. (<xref ref-type="bibr" rid="CR18">2012</xref>) algorithm is able to adapt itself to the quality of the data by increasing velocity thresholds in low-quality data and lowering velocity thresholds in high-quality data. This algorithm makes it possible to apply the same method to the data of all participants, yet allowing for individual threshold estimation. This algorithm is developed for use in adult studies and not yet tested with infant data. Moreover, additional processing of the data is needed to deal with specific data-quality issues often observed in infants. As noise is a major issue in infant eye-tracking, we used the Mould et al. (<xref ref-type="bibr" rid="CR18">2012</xref>) algorithm as a starting point for gazepath because this algorithm is explicitly designed to adjust thresholds to noise in the data without specifying an initial starting threshold.</p>
    </sec>
    <sec id="Sec3">
      <title>Data quality</title>
      <p>A typical case of infant eye-tracking data is much noisier than adult eye-tracking data. Sampling point fluctuations are larger in infants than adults and there are much more missing sampling points. This is caused by multiple factors, for example, infants tend to make more head movements than adults, causing instances of missing data as the eye-tracker needs to re-identify the position of the head (Hessels et al. <xref ref-type="bibr" rid="CR11">2015</xref>). Head movements may also make it difficult for the eye-tracker to identify the eyes; for instance, the nostril may be mistaken for the pupil, resulting in a signal moving between the eye and the nostril. Furthermore, infants’ eyes can be watery, resulting in flicker in the data where the signal rapidly switches between on and off (Wass et al. <xref ref-type="bibr" rid="CR36">2014</xref>).</p>
      <p>Figure <xref rid="Fig1" ref-type="fig">1</xref> shows 8 s of raw eye-tracking data measured with a Tobii eye-tracker (Tobii 1750, Tobii Technology, Stockholm, Sweden). Time is plotted on the x-axis and the x- and y-positions of the left and right eyes are plotted on the y-axis. Data quality is characterized by precision and robustness (Wass, Smith, &amp; Johnson, <xref ref-type="bibr" rid="CR38">2013</xref>). Precision refers to the sampling point fluctuations. In Fig. <xref rid="Fig1" ref-type="fig">1</xref> the signal in the purple circle shows large fluctuations, thus low precision. Robustness refers to sequences of missing data. When there is a constant signal, robustness is high, but when the signal flickers on and off, such as in the yellow circle in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, robustness is low. The horizontal colored sequences below the left and right eye signals are the fixations that are classified by the standard Tobii event-detection algorithm. Each color change indicates a new fixation. In the purple circle, where precision is low, four fixations are classified; however by looking at the data, it seems more likely that one long fixation took place. Because of the low precision, the dispersion threshold of the Tobii algorithm is crossed several times and new fixations are classified. This shows how data quality can influence dependent variables such as fixation durations. In line with this example, Wass et al. (<xref ref-type="bibr" rid="CR36">2014</xref>) found that data quality correlates with key dependent variables, such as fixation durations. Lower data quality goes hand in hand with shorter fixation durations. Furthermore, data quality is also affected by other variables, such as age. Older infants have better data quality than younger infants (Wass et al. <xref ref-type="bibr" rid="CR36">2014</xref>). This makes it hard to assess, for instance, the relationship between fixation duration and age, as it is also influenced by data quality (see Wass and colleagues, 2013, 2014 and Hessels and colleagues, 2015 for a more detailed discussion on data quality in infant eye-tracking.)
<fig id="Fig1"><label>Fig. 1</label><caption><p>Example of low robustness and low precision in eye-tracking data collected with a Tobii 60 Hz eye-tracker. The <italic>colored horizontal line</italic> at y = 380, represents the fixations classified by the Tobii. When the color switches, a new fixation is identified, it can be seen that low data quality leads to identification of many short fixations. Also note the puzzling instances around 100 ms and 7000 ms, where Tobii detects fixations without any gaze data</p></caption><graphic xlink:href="13428_2017_909_Fig1_HTML" id="MO1"/></fig>
</p>
      <p>The relationship between data quality and dependent variables has been identified as a problem in infant eye-tracking studies, and several solutions have been offered. Wass et al. (<xref ref-type="bibr" rid="CR38">2013</xref>), for example, developed a parsing algorithm that performs post hoc checks on the data. Fixations are only kept if they have incoming and outgoing saccades. This is done to make sure fixation durations are not affected by missing data instances. These algorithms were used as the basis of GraFIX, a semiautomatic approach for parsing eye-tracking data (de Urabain, Johnson, &amp; Smith, <xref ref-type="bibr" rid="CR5">2015</xref>). A major advantage of GraFIX over most other algorithms is that GraFIX comes with a graphical user interface (GUI). This makes GraFIX also usable for researchers who lack MATLAB skills. A downside, however, is that GraFIX needs considerable user input. Fixations are initially parsed automatically and can then be manually adjusted. Despite these possible solutions, infant eye-tracking studies reporting data quality and/or taking measures to overcome the issues described here remain scarce.</p>
    </sec>
    <sec id="Sec4">
      <title>Current study</title>
      <p>To summarize, standard eye-tracker manufacturer classification methods provide no satisfactory solution to reliably parse eye-tracking data of different populations, because they do not allow individual threshold estimation. The algorithms that use individual thresholds are not yet suited to analyze infant eye-tracking data and the algorithms developed by Wass et al. (<xref ref-type="bibr" rid="CR38">2013</xref>) and de Urabain et al. (<xref ref-type="bibr" rid="CR5">2015</xref>) to analyze infant data do not allow individual threshold estimation. Furthermore, most of these approaches (except GraFIX) are implemented in MATLAB, which is expensive and requires advanced programming skills to use. In this paper, we attempt to combine the best of both worlds into a new eye-tracking parsing tool called gazepath. Gazepath is an easy-to-use open-source software tool, implemented in R (R Core Team <xref ref-type="bibr" rid="CR21">2014</xref>). It comes with a GUI implemented in the R-package shiny (RStudio Inc <xref ref-type="bibr" rid="CR27">2015</xref>). Gazepath is capable of dealing with low-quality eye-tracking data in terms of robustness and precision, but is also well suited for high-quality data. We show this by examining correlations between data quality and outcome measures and assessing the distribution of fixation durations when the gazepath method is used, compared to the standard classification methods. The functionality of gazepath will be illustrated on different data sets; first, we show how gazepath performs compared to the standard EyeLink classification on a free-viewing data set of infants and adults. Second, we compare gazepath performance with EyeLink performance on an adult reading data set. Third, we illustrate how gazepath performs on low sampled (60 Hz) infant experimental data collected with a Tobii. These data sets are chosen to reflect the data extremes obtained with eye-trackers. On the one end of the spectrum, there is infant free-viewing, which can be highly variable without any predictable patterns to expect. On the other end, there is adult reading, a highly automatic process with a very predictable pattern.</p>
    </sec>
  </sec>
  <sec id="Sec5">
    <title>Gazepath method</title>
    <p>The algorithm of Mould et al. (<xref ref-type="bibr" rid="CR18">2012</xref>) is taken as basis for the gazepath package. This algorithm is able to account for individual differences by estimating a velocity threshold for every individual and every trial in a data-driven manner, thereby providing a perfect starting point to develop an algorithm that can be used for different populations. The algorithm also has some limitations, one of which concerns the estimation of the duration threshold. Although the algorithm is capable of doing this in a data-driven manner based on initial fixation durations, the duration threshold is too unreliable. We estimated the duration thresholds, leaving out one data point for every estimation. What we observed were threshold differences up to 50 ms. These are very large differences that cannot be justified with only a single data point difference. Another limitation is the ability to deal with low robustness in the data. Consequently, instances of missing data signal the end of a fixation, even if data is only missing for a few milliseconds. In order to overcome these limitations, we combined the Mould et al. (<xref ref-type="bibr" rid="CR18">2012</xref>) algorithm with the methods described by Wass et al. (<xref ref-type="bibr" rid="CR38">2013</xref>) into the R-package gazepath.</p>
    <sec id="Sec6">
      <title>Gazepath pre-processing</title>
      <p>The gazepath method uses a six-step procedure to preprocess the data from raw samples into fixations and saccades. These six steps are described below and visualized in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. First, raw data of the left and right eye are combined when two eyes were tracked. This is done by calculating the mean of the x- and y-coordinates. Missing data points from one eye are interpolated with data points of the other eye when possible. This is done to maximize the available data.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Example of all steps of the gazepath method. First, gaze coordinates are combined when two eyes where tracked. Second, the speed threshold is derived using the Mould et al. (<xref ref-type="bibr" rid="CR18">2012</xref>) algorithm. Data sequences that fall below the threshold, are marked as initial fixations (see panel 3). Panel 3 also clearly shows the bad performance of initial parsing, as can be seen from the many short fixations that occur as a result of data quality. The fourth step is to interpolate sequences of missing data. Panel 4 shows this improves the classification a lot, but there are still instances (<italic>blue circle</italic>), where fixations should be combined. This is done in the fifth step, by combining successive fixations that overlap in space. The sixth and final step involves the selection of fixations that pass the duration threshold, which is often set to 100 ms</p></caption><graphic xlink:href="13428_2017_909_Fig2_HTML" id="MO2"/></fig>
</p>
      <p>Second, the velocity threshold is estimated using exactly the same method as the Mould et al. (<xref ref-type="bibr" rid="CR18">2012</xref>) algorithm to account for individual and trial-by-trial differences in precision. The velocity of the eye is calculated as the Euclidean distance between preceding and succeeding points divided by the time elapsed between them. Then, sampling points with velocities higher than the preceding and succeeding sampling point are classified as local maxima. The second panel of Fig. <xref rid="Fig2" ref-type="fig">2</xref> shows the distribution of local speed maxima exceeding the threshold (gray histogram), compared to a uniform null distribution (Tibshirani, Walther, &amp; Hastie, <xref ref-type="bibr" rid="CR30">2001</xref>) of local maxima exceeding the threshold (dotted line). The difference between these two distributions is given by the gap statistic (red line). This gap statistic is smoothed with a locally weighted quadratic regression (loess, Cleveland, <xref ref-type="bibr" rid="CR4">1979</xref>; Fan &amp; Gijbels, <xref ref-type="bibr" rid="CR6">1996</xref>) with increasing bandwidths until the gap statistic reaches one maximum. This maximum is the velocity threshold.</p>
      <p>Third, to account for low robustness, missing data sequences shorter than a given threshold (default = 250 ms) are interpolated. The default value is choosing so it is unlikely a saccade took place, as saccades take approximately 200 ms to program (Nyström &amp; Holmqvist, <xref ref-type="bibr" rid="CR19">2010</xref>). This is only done when the velocity difference between the last measured sample before the missing data and the first measured sample after the missing data, does not exceed the velocity threshold. This is done to make sure no saccade took place during the loss of signal.</p>
      <p>Fourth, data sequences of the interpolated data that are below the velocity threshold are marked as possible fixations and data sequences above the velocity threshold are marked as possible saccades. At this moment, it is still possible that there are fixations that are too short, because the velocity threshold was crossed without an actual saccade taking place.</p>
      <p>Fifth, to correct these instances, a check is made for successive fixations overlapping in space. This is done by drawing a polygon around the fixations, and when two successive fixations have overlapping polygons, the fixations are merged into one fixation.</p>
      <p>The sixth and final step is to remove short fixations. This is done by setting the duration threshold, the default value for which is 100 ms. Although the Mould et al. (<xref ref-type="bibr" rid="CR18">2012</xref>) algorithm offers a possibility to do this in a data-driven manner, this requires a lot of data. In practice, especially in infant studies, there are rarely enough data to reliably estimate the duration threshold. For the final classification, the effect of the duration threshold is also limited, since relatively few fixations fall in the interval of 50–150 ms. Given these considerations, we decided to set the duration threshold manually.</p>
    </sec>
    <sec id="Sec7">
      <title>Using gazepath</title>
      <p>This section describes the procedure to use gazepath. Gazepath is implemented in R (R Core Team <xref ref-type="bibr" rid="CR21">2014</xref>) and therefore requires the installation of R before gazepath can be used. In R, gazepath can be installed by running the commands:
<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\texttt{install.packages(`gazepath', dependencies = TRUE)} $$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mtext mathvariant="monospace">install.packages(‘gazepath', dependencies = TRUE)</mml:mtext></mml:mrow></mml:math><graphic xlink:href="13428_2017_909_Article_Equa.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equb"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\texttt{library(`gazepath')} $$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mtext mathvariant="monospace">library(‘gazepath')</mml:mtext></mml:mrow></mml:math><graphic xlink:href="13428_2017_909_Article_Equb.gif" position="anchor"/></alternatives></disp-formula>
</p>
      <p>Gazepath can be used from the R command line, but there is also a Shiny (RStudio Inc <xref ref-type="bibr" rid="CR27">2015</xref>) application that provides gazepath with a GUI, which can be opened in a web browser with the command: 
<disp-formula id="Equc"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\texttt{GUI()} $$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mtext mathvariant="monospace">GUI()</mml:mtext></mml:mrow></mml:math><graphic xlink:href="13428_2017_909_Article_Equc.gif" position="anchor"/></alternatives></disp-formula> Here we use the Shiny app to illustrate the use of gazepath. First, the data are loaded; second, parsing takes place using the procedure described above; third, the data can be visualized; and fourth, the fixations can be downloaded (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>).
<fig id="Fig3"><label>Fig. 3</label><caption><p>Illustration of the four-step procedure to parse fixations and saccades via the gazepath Shiny app</p></caption><graphic xlink:href="13428_2017_909_Fig3_HTML" id="MO3"/></fig>
</p>
      <p>After opening the application, the data must be loaded. Typically, eye-trackers generate text files with the raw data for every individual, and gazepath uses these files as input. As these text files can be formatted differently, there are several options to make sure the data are loaded correctly, such as different missing data strings and separation operators. On the right side of the screen, the top and bottom rows of the data file appear, and it is easy to check if the data are loaded correctly, i.e., if every point has its own cell in the data-frame. It is possible to load data of multiple participants, so the whole analysis can be conducted at once. However, loading multiple data sets requires all data sets to be formatted exactly the same way, i.e., having the same variable names, separation operators, etc.</p>
      <p>Once the data are loaded, the next step is to provide gazepath with the information needed to run the analyses. From the uploaded data, gazepath needs at least the variable names of the x- and y-coordinates, distance to the screen and trial index. When two eyes are tracked, as is common with many trackers, the x- and y-coordinates and the distance to the screen of the other eye can also be specified. Furthermore, gazepath needs information about the screen dimensions in pixels and the stimulus dimensions in both pixels and mm (when stimuli presentation is not full screen, it is assumed that stimuli are presented in the middle). Finally, it is mandatory to specify the sampling rate and choose a parsing method. The best available methods are the gazepath and Mould methods, as described above. It is also possible to select the MouldDur method, which uses a fixed-duration threshold (default = 100 ms), the dispersion method, which is an implementation of the Tobii algorithm described in the Clearview 2.7 manual (Tobii Eye Tracker User Manual <xref ref-type="bibr" rid="CR31">2006</xref>), and the velocity method, which fixes the velocity threshold at 35 deg/s and the duration threshold at 100 ms. It is not recommended to use the last two methods. These methods are only implemented to ease comparison with simple parsing methods. Apart from the mandatory input, gazepath can keep other variables from the raw data, such as condition, age, stimuli, etc. These extra variables can only have a single value per trial, i.e., if different stimuli appear during one trial, the stimuli variable cannot be kept.</p>
      <p>When all input parameters are set, the <italic>go</italic> button can be clicked to start the analysis. When there are multiple data sets loaded, this can take some time, and in the top right corner progress is displayed. It takes approximately 3 s to parse 1 min of 500-Hz data.<xref ref-type="fn" rid="Fn1">1</xref> After running the analyses, gazepath displays the top of the output file next to the input parameters. Now the data can be visualized. Fixations per participant per trial are displayed under <italic>visualize parsing</italic>, as seen in the middle of Fig. <xref rid="Fig3" ref-type="fig">3</xref>. The left screen plots the raw x- and y-coordinate overlaid with the order and position of fixations indicated by letters, the top right screen displays the raw x- and y-coordinates as the function of time and shows the fixations in green. The bottom right screen shows the speed in deg/s as a function of time with the velocity threshold in red. By clicking <italic>visualize threshold</italic> the velocity thresholds obtained for each individual on every trial are displayed. As estimation of the velocity threshold requires at least some data, some trials cannot be selected to inspect. This implies that there were not enough data to estimate a threshold in that trial. Finally, the fixations can also be visualized on the stimuli. Under the <italic>visualize stimuli</italic> tab, it is possible to upload the stimuli and plot fixations per participant per trial to inspect individual scanning patterns.</p>
      <p>The final data can be downloaded as a .csv file, which can be used to further analyze the data. The data can be obtained in four forms, (1) all parsed fixations and saccades, (2) fixations only, (3) only complete fixations and saccades, i.e., fixations that have in- and outgoing saccades and saccades that are between two fixations, and (4) only complete fixations. The last two options can be selected to make sure all fixations and saccades are ‘true’ fixations and saccades; however, in noisy data this could result in much fewer data points. The fixations-only option can be useful, as most researchers are only interested in fixations. To close gazepath, simply close the browser and press esc in R to close the R process. The columns of the output data frame are ordered as follows:
<def-list><def-item><term>Participant</term><def><p>the participant by the name of the data file.</p></def></def-item><def-item><term>Value</term><def><p>whether a fixation (f) or saccade (s) is classified.</p></def></def-item><def-item><term>Duration</term><def><p>the duration of the fixation or saccade in milliseconds.</p></def></def-item><def-item><term>Start and End</term><def><p>the start and end time in milliseconds of the fixations and saccades from the start of that trial.</p></def></def-item><def-item><term>mean_x and mean_y</term><def><p>the mean x- and y-coordinates in pixels of fixations and saccades (note that this measure is only meaningful for fixations).</p></def></def-item><def-item><term>sdPOGsacAMP</term><def><p>the standard deviation in point of gaze (for fixations) and the saccade amplitude in degrees of visual angle (for saccades).</p></def></def-item><def-item><term>RMS</term><def><p>the root mean square (RMS) within each fixation.</p></def></def-item><def-item><term>Order</term><def><p>the order of fixations and saccades within trials</p></def></def-item><def-item><term>Trial</term><def><p>the trial index.</p></def></def-item><def-item><term>*</term><def><p>When additional variables are kept from the original data, these variables appear after the last variable.</p></def></def-item></def-list>
</p>
    </sec>
  </sec>
  <sec id="Sec8">
    <title>Free-viewing data example</title>
    <p>The performance of the gazepath method is examined in a free-viewing data set of infants and adults. This is an existing data set that is published elsewhere (Van Renswoude et al. <xref ref-type="bibr" rid="CR33">2016</xref>).</p>
    <sec id="Sec9">
      <title>Participants</title>
      <p>Infant participants were recruited from Los Angeles County birth records. Adult participants were recruited through the University of California, Los Angeles subject pool and were given course credit for participating. Sixty-two infants (<italic>M</italic>
<sub><italic>a</italic><italic>g</italic><italic>e</italic></sub> = 9 months, range = 3–15) and 47 adults saw 28 real-world scenes for 4s each on a 17-inch computer monitor, which subtended an approximate 27<sup>∘</sup>× 34<sup>∘</sup> visual angle. Eye movements were recorded with an EyeLink eye-tracker (SR Research Ltd., Ontario, Canada) that sampled at 500 Hz. Prior to data collection, a five-point calibration scheme was used to calibrate each participant’s point of gaze. The calibration procedure was repeated if necessary until the recorded point of gaze was within 1 <sup>∘</sup> of the center of the target.</p>
    </sec>
    <sec id="Sec10">
      <title>Descriptives</title>
      <p>Fixations were detected by the gazepath method of the gazepath R-package and using the default settings of the EyeLink. Fixation durations typically show a right-skewed distribution, therefore the median fixation duration provides a more reliable measure than the mean (Helo, Pannasch, Sirri, &amp; Raemae, <xref ref-type="bibr" rid="CR9">2014</xref>; Velichkovsky, Dornhoefer, Pannasch, &amp; Unema, <xref ref-type="bibr" rid="CR34">2000</xref>). Figure <xref rid="Fig4" ref-type="fig">4</xref> shows the distributions of the infant and adult free-viewing data parsed with the standard EyeLink and gazepath methods. Although the distributions look similar, there are some differences. The most striking difference is that fixations parsed by the standard EyeLink method are longer than the fixations parsed by gazepath. Another difference is the number of fixations. In adults, the gazepath method results in approximately 10% more fixations than the EyeLink method, whereas in infants the difference is only 1% and in the opposite direction.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Distribution of fixation durations classified with the gazepath and EyeLink methods for free-viewing data of infants and adults. The distributions are plotted over a 100–1000-ms interval, whereas there are also some longer fixations classified</p></caption><graphic xlink:href="13428_2017_909_Fig4_HTML" id="MO4"/></fig>
</p>
      <p>In order to get a better understanding of these differences and to test the significance of these observations, the mean number of fixations and the median fixation durations were calculated for each participant. Figure <xref rid="Fig5" ref-type="fig">5</xref> shows the boxplots of these means and medians for the infants and adults parsed by the gazepath and EyeLink methods. A factorial mixed ANOVA revealed an interaction effect between group (infant or adult) and method (gazepath or EyeLink) on the mean number of fixations, <italic>F</italic>(1,107) = 29.23,<italic>p</italic> &lt; 0.001. For infants, there was no difference in the mean number of fixations classified by the EyeLink and gazepath method, whereas for adults the gazepath method classified more fixations than the EyeLink method. The median fixation duration differed between methods, <italic>F</italic>(1,107) = 108.75,<italic>p</italic> &lt; 0.001. Fixations parsed using the gazepath method were shorter than fixations parsed with the EyeLink method. This difference was similar for infants and adults as there was no interaction effect between group and method for the median fixation durations, <italic>F</italic>(1,107) = 0.73,<italic>p</italic> = 0.396.
<fig id="Fig5"><label>Fig. 5</label><caption><p>Boxplots of number of fixations (<italic>left panel</italic>) and median fixation durations (<italic>right panel</italic>) per participant, classified with the gazepath and EyeLink method for free-viewing data of infants and adults</p></caption><graphic xlink:href="13428_2017_909_Fig5_HTML" id="MO5"/></fig>
</p>
    </sec>
    <sec id="Sec11">
      <title>Performance in adult data</title>
      <p>For adults, these findings make sense, when fixations are shorter, more fixations can be made in the same time frame. This would imply that some fixations that are classified using the EyeLink method are split into two or more fixations using the gazepath method. This is likely, as gazepath sets the velocity threshold for every individual and every trial separately and lower thresholds would result in more fixations. To see if this is indeed what happened, we checked, for every fixation, for the possibility that the other method split that fixation.</p>
      <p>Figure <xref rid="Fig6" ref-type="fig">6</xref> provides a real data example wherein the letter S denotes saccades that led to splits. Here the gazepath fixations are identified as not being split, because every fixation also has one fixation classified by the EyeLink method. The first two EyeLink fixations are identified as being split because the gazepath method identified two fixations during the time frame of these fixations. Of the 10,764 gazepath fixations, only nine were split and only one fixation was not classified in the EyeLink method. Of the 10,867 EyeLink fixations, 1417 were split into 1738 extra fixations and 332 were not classified in the gazepath method. This explains the differences in the number of fixations between the two methods.
<fig id="Fig6"><label>Fig. 6</label><caption><p>Example of gazepath and EyeLink classification of fixation. S denotes instances where a small saccade took place that is missed by EyeLink, but picked up by gazepath</p></caption><graphic xlink:href="13428_2017_909_Fig6_HTML" id="MO6"/></fig>
</p>
      <p>Answering the question of which method provides the best classification method is difficult, because it is impossible to establish a clear ground truth from the eye-tracking signal alone. Often classification by human experts is taken as the best available benchmark (e.g., Andersson et al., <xref ref-type="bibr" rid="CR1">2016</xref>). In order to get some insight into this question, we examined all trials in which there were one or more splits. Figure <xref rid="Fig6" ref-type="fig">6</xref> shows two of these trials that are typical for what we observed. It can be seen that the gazepath method is more sensitive to small saccades (highlighted with S), which leads to more and shorter fixations being classified. Inspection of these trials also showed that most of the time the splits made in the gazepath method are easily observable by looking at the data, as is the case in these examples. However, we also observed trials where the splits were less prominent.</p>
    </sec>
    <sec id="Sec12">
      <title>Performance on infant data</title>
      <p>For infants, the relationship between the number of fixations and the fixation duration is less clear than in adults. Infants also showed shorter median fixation durations when gazepath was used to parse the data compared to EyeLink, but the two methods produced a similar number of fixations. However, Fig. <xref rid="Fig5" ref-type="fig">5</xref> also shows that there is more variance in the number of fixations classified using the gazepath method than the EyeLink method. This implies that for some infants, gazepath classified fewer fixations than EyeLink, but for others more. This is in line with the findings of the split fixations. Of the 15,368 gazepath fixations, 100 were split and 27 fixations were not classified in the EyeLink method. Of the 13,972 EyeLink fixations, 842 were split into 1005 extra fixations and 1017 were not classified in the gazepath method.</p>
      <p>Ideally, the fixations that are split are the fixations in higher-quality data, whereas the fixations that are not classified with the gazepath method are mostly found in low-quality data. In order to see if this is indeed the case, data quality were quantified in terms of robustness and precision. Robustness was calculated as the mean length of raw data segments per trial. Infants who stay focused, have long data segments, providing a robust measure, whereas infants who look away and move a lot have many more missing data points and therefore short data segments, providing a less robust measure. To obtain the precision measure, the signal was smoothed by calculating mean x- and y-coordinates over 100-ms time windows. Precision of a trial is the mean of the mean difference between the smoothed and raw data in each time window. Low values indicate high precision and vice versa.</p>
      <p>Correlations between data quality and fixation durations can give an indication of parsing performance. These correlations are often observed in infant data (Wass et al., <xref ref-type="bibr" rid="CR38">2013</xref>, <xref ref-type="bibr" rid="CR36">2014</xref>) and are considered problematic. As described in the introduction, these correlations can occur because poor data quality can lead to spurious short fixations. The top left panel of Fig. <xref rid="Fig7" ref-type="fig">7</xref> shows the correlation (<italic>r</italic> = −.52, <italic>r</italic> = −.31 without the outlier) between precision and median fixation durations classified with EyeLink. The top right panel shows the correlation (<italic>r</italic> = .36, <italic>r</italic> = .31 without the outlier) between robustness and median fixation duration of the EyeLink classification. These correlations are significant and in the expected direction. Poor data quality is associated with shorter fixation durations when the standard EyeLink is used. The bottom left and bottom right panel show that the fixations classified with the gazepath method have correlations that are non-significant and are closer to zero. To test if these dependent correlations do indeed differ, we used a Williams test (Steiger <xref ref-type="bibr" rid="CR29">1980</xref>), as implemented in R-package <italic>psych</italic>. The Williams test showed that the correlations between median fixation duration and precision for the gazepath and EyeLink classification differed significantly with the outlier (<italic>t</italic>(59) = 6.52, <italic>p</italic> &lt; 0.001) and without the outlier (<italic>t</italic>(58) = 5.84,<italic>p</italic> &lt; 0.001). For robustness, similar results were obtained; the correlations between median fixation duration and robustness for the gazepath and EyeLink classification differed significantly with the outlier (<italic>t</italic>(59) = −3.96,<italic>p</italic> &lt; 0.001) and without the outlier (<italic>t</italic>(58) = −3.82,<italic>p</italic> &lt; 0.001). These results imply that the individual threshold estimation and post hoc checks that are implemented in the gazepath method work well.
<fig id="Fig7"><label>Fig. 7</label><caption><p>Example of data-quality measures robustness and precision and their correlations with median fixation durations classified using the EyeLink (EL) and gazepath (GP) method. Fixation durations classified by gazepath have no correlation with data quality, whereas these correlations are present with the EyeLink classification. In <italic>red</italic>, the correlations without the outlier are shown. The outlier is marked in <italic>black</italic>, which is the same data point in all plots</p></caption><graphic xlink:href="13428_2017_909_Fig7_HTML" id="MO7"/></fig>
</p>
      <p>To illustrate the performance of gazepath, Fig. <xref rid="Fig8" ref-type="fig">8</xref> shows two trials that are typical for what we observed in the trials with split fixations. The top panel shows instances of interpolated short missing data sequences when the X and Y position did not change (highlighted with M). The bottom panel shows a trial with very noisy data, and it can be seen that EyeLink identified multiple short fixations, whereas gazepath combined these into one larger fixation (highlighted with N). Although the bottom panel illustrates the working of gazepath, the data are extremely noisy and should probably be excluded from further data analyses.
<fig id="Fig8"><label>Fig. 8</label><caption><p>Example of gazepath and EyeLink classification for infant data. The <italic>top panel</italic> shows instances where data are interpolated (M). The <italic>bottom panel</italic> shows extremely noisy data where gazepath combines multiple EyeLink fixations into one fixation (N)</p></caption><graphic xlink:href="13428_2017_909_Fig8_HTML" id="MO8"/></fig>
</p>
    </sec>
    <sec id="Sec13">
      <title>Conclusion free-viewing data</title>
      <p>In this section, we showed that gazepath performs well for both infant and adult data. In high-quality adult data, gazepath lowers its thresholds and is able to pick up more fixations than the standard EyeLink method. In infant data, gazepath does the same when the infant data are of good quality, but it can also combine fixations, when low data quality or signal loss results in spuriously short fixations. Despite the good performance of gazepath, there is reason to be cautious. That is, the data sets analyzed here are the same data sets that were used to develop gazepath. It is therefore important to also examine the performance on new data sets. We selected an adult reading data set and a experimental infant data set to further examine the performance of gazepath.</p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Adult reading data</title>
    <p>To test the performance of gazepath on a data set with very different characteristics, we selected a data set of an adult reading study. A part of this data is published in experiment 2 of Koornneef, Dotlacil, van den Broek, and Sanders (<xref ref-type="bibr" rid="CR16">2016</xref>). Reading is a highly automatic process, with predictable fixation and saccade patterns, which may make it easier to set a fixed velocity threshold. In line with what we observed in the free-viewing data, we expected gazepath to classify more and shorter fixations than the standard EyeLink method, as the individual threshold estimation allows gazepath to be more sensitive to detect short fixations.</p>
    <sec id="Sec15">
      <title>Participants</title>
      <p>Sixty-five adults (<italic>M</italic>
<sub><italic>a</italic><italic>g</italic><italic>e</italic></sub> = 25.0 years, range = 18–68) participated in a reading study at Utrecht University and were paid for participating. They read 88 short texts that were 4–5 lines long. Their eye movements were measured with a EyeLink (SR Research Ltd., Ontario, Canada) eye-tracker that sampled at 500 Hz.</p>
    </sec>
    <sec id="Sec16">
      <title>Results</title>
      <p>Figure <xref rid="Fig9" ref-type="fig">9</xref> shows the distributions of the adult reading data parsed with the standard EyeLink and gazepath methods in the upper panels. The lower panels show boxplots of the mean number of fixations and median fixation durations per participant. As expected, paired-samples <italic>t</italic> tests showed that the gazepath method classified more (<italic>t</italic>(64) = −96.58,<italic>p</italic> &lt; 0.001) and shorter (<italic>t</italic>(64) = 14.37,<italic>p</italic> &lt; 0.001) fixations than the EyeLink method.
<fig id="Fig9"><label>Fig. 9</label><caption><p>The <italic>top panels</italic> show the distribution of fixation durations classified with the EyeLink (EL) and gazepath (GP) method for reading data of adults. The distributions are plotted over the 100–1000-ms interval, whereas there are also some longer fixations classified. The <italic>bottom panels</italic> show boxplots of mean number of fixations per participant (<italic>left</italic>) and median fixation duration (<italic>right</italic>)</p></caption><graphic xlink:href="13428_2017_909_Fig9_HTML" id="MO9"/></fig>
</p>
      <p>These results imply that some fixations that are classified using the EyeLink method are split into two or more fixations using the gazepath method, as was the case in the free-viewing data. To check if this is indeed what happened, we again verified for every fixation if the other method split that fixation.</p>
      <p>Of the 188,372 gazepath fixations, only 63 were split and only 41 fixations were not classified in the EyeLink method. Of the 182,094 EyeLink fixations, 8926 were split into 9518 extra fixations and 3215 were not classified in the gazepath method. The shorter median fixation durations of the gazepath method compared to the EyeLink method can partly be explained by these splits. That is, gazepath classifies more fixations, leading to shorter fixation durations on average. However, less than 5% of the EyeLink fixations were split and therefore these splits cannot fully account for the difference. This means that there may be another difference between the two methods that also accounts for the difference in median fixation durations. For instance, there may be a difference in onset and offset times of fixations between the gazepath and EyeLink method.</p>
      <p>To test for these differences, we selected trials (14%, <italic>N</italic> = 29499) that had no splits for both methods and had the exact same number of fixations. In these trials, all classified fixations are very similar and the only difference can occur in onset and offset times. In this subset of the data, we also found that gazepath had shorter median fixation durations (182) than EyeLink (194). This difference is primarily driven by later onset times of the fixations classified with gazepath compared to EyeLink. Figure <xref rid="Fig10" ref-type="fig">10</xref> shows the distribution of the differences between the start (left panel) and end times (right panel) of fixations classified using the EyeLink and gazepath method. The EyeLink fixations start earlier, whereas the end times are very similar.
<fig id="Fig10"><label>Fig. 10</label><caption><p>Differences (EyeLink - gazepath) between the start and end times of the same fixations, classified with EyeLink and gazepath. Both histograms are zoomed in to highlight differences around zero and show 99% of the data</p></caption><graphic xlink:href="13428_2017_909_Fig10_HTML" id="MO10"/></fig>
</p>
    </sec>
    <sec id="Sec17">
      <title>Gazepath performance</title>
      <p>To get a better understanding of the overall performance of gazepath, we again inspected the trials that had split fixations. We observed similar patterns as in the free-viewing data of adults; gazepath is more sensitive than EyeLink to small saccades. For eye-tracking data related to reading, this can be a very useful property because saccades opposite to the reading direction are often studied. These saccades are called regressive saccades (Bicknell &amp; Levy, <xref ref-type="bibr" rid="CR2">2011</xref>) and can have different interpretations. For instance, readers may miss the optimal viewing position of a word and correct with a regressive saccade (Rayner, Schotter, Masson, Potter, &amp; Treiman, <xref ref-type="bibr" rid="CR24">2016</xref>). Regressions can also indicate difficulty to process a word (Vitu, McConkie, &amp; Zola, <xref ref-type="bibr" rid="CR35">1998</xref>), or indicate failure to integrate a word within the context of a sentence (Frazier &amp; Rayner, <xref ref-type="bibr" rid="CR7">1982</xref>). Figure <xref rid="Fig11" ref-type="fig">11</xref> shows three of these instances (highlighted with R) where EyeLink missed a small regressive saccade that was picked up by gazepath.
<fig id="Fig11"><label>Fig. 11</label><caption><p>Two examples of gazepath and EyeLink classification for adult reading data. The reading pattern is clearly visible as the eye is stable on the y-axis and moves progressively higher (<italic>to the right</italic>) over the x-axis until the end of a sentence where a large saccade to the start of a new sentence is made. Overall, classification is very similar, although gazepath is more sensitive to detect small saccades. This can be useful for reading data as saccades in the opposite direction (R) of the reading direction (regressive saccades) are often studied</p></caption><graphic xlink:href="13428_2017_909_Fig11_HTML" id="MO11"/></fig>
</p>
    </sec>
    <sec id="Sec18">
      <title>Conclusion reading data</title>
      <p>EyeLink and gazepath produce very similar results when parsing adult reading data. The main difference lies in gazepath’s ability to pick up small saccades, something that can be very useful in reading studies. Another difference is that the fixations classified with gazepath are a bit shorter than fixations classified with EyeLink. This is caused by later onset times of gazepath fixations, although it is difficult to draw conclusions about one method being better than the other, as it is impossible to decide which is the ‘correct’ classification based on the eye-tracking signal alone. Overall, gazepath and EyeLink work well and produce similar results. An advantage of gazepath over EyeLink is when researchers are interested in small regressive saccades.</p>
    </sec>
  </sec>
  <sec id="Sec19">
    <title>Infant experimental data</title>
    <p>To test the performance of gazepath on data of a different eye-tracker with a lower sample rate (60 Hz) and dynamic instead of static stimuli, we selected a data set of an infant experimental study using a Tobii eye-tracker. The combination of infants, a low sample rate and dynamic stimuli makes it likely that data is noisy. In line with what we observed in the infant free-viewing data, we expected gazepath to classify shorter fixations than the standard Tobii method. Given the expected noise in the data, we also expected gazepath to classify fewer fixations than the standard Tobii method, since the individual threshold estimation and post hoc checks allow gazepath to be more conservative to classify fixations in noisy data. For the same reason, we also expected to see correlations between data quality and median fixation durations classified with the Tobii method, but not with the gazepath method.</p>
    <sec id="Sec20">
      <title>Participants and design</title>
      <p>The Tobii data were provided by 127 infants (<italic>M</italic>
<sub><italic>a</italic><italic>g</italic><italic>e</italic></sub> = 11 months, range = 10–12) who participated in a categorical learning study at Radboud University Nijmegen. They saw dynamic stimuli<xref ref-type="fn" rid="Fn2">2</xref> of a red ball moving to the left, or a blue ball moving to the right. The ball ended up in a cup and a reward (a small flickering chick making a whistling sound) was shown. All infants saw 20 trials of 8 s each, on a 17-inch computer monitor, which subtended an approximate 27<sup>∘</sup>× 34<sup>∘</sup> visual angle. Eye movements were recorded with a Tobii eye-tracker (Tobii 1750, Tobii Technology, Stockholm, Sweden) that sampled at 60 Hz. Prior to data collection, a nine-point calibration scheme was used to calibrate each participant’s point of gaze.</p>
    </sec>
    <sec id="Sec21">
      <title>Results</title>
      <p>Figure <xref rid="Fig12" ref-type="fig">12</xref> shows the distributions of the infant experimental data parsed with the standard Tobii and gazepath methods in the upper panels. The lower panels show the boxplots with the mean number of fixations and median fixation durations per participant. Paired-samples <italic>t</italic> tests showed that the gazepath method classified fewer (<italic>t</italic>(126) = 13.41,<italic>p</italic> &lt; 0.001), but not shorter (<italic>t</italic>(126) = −0.93,<italic>p</italic> = 0.356) fixations than the Tobii method. Of the 17,700 gazepath fixations, 902 were split into 1245 extra fixations and 133 fixations were not classified in the Tobii method. Of the 26,691 Tobii fixations, 1406 were split into 1647 extra fixations and 9600 were not classified in the gazepath method. The distribution of the Tobii fixations (Fig. <xref rid="Fig12" ref-type="fig">12</xref>) is oddly shaped, with many very short fixations compared to the distribution of gazepath fixations.
<fig id="Fig12"><label>Fig. 12</label><caption><p>Distribution of fixation durations classified with the gazepath (GP) and Tobii (TB) method for experimental data of infants. The distributions are plotted over a 100–1000-ms interval, whereas there are also some longer fixations classified</p></caption><graphic xlink:href="13428_2017_909_Fig12_HTML" id="MO12"/></fig>
</p>
    </sec>
    <sec id="Sec22">
      <title>Gazepath performance</title>
      <p>In the infant free-viewing data, we observed correlations between data quality and median fixation duration using the standard EyeLink classification method. These problematic correlations (Wass et al. <xref ref-type="bibr" rid="CR36">2014</xref>) were also found with the standard Tobii classification method. The upper panels of Fig. <xref rid="Fig13" ref-type="fig">13</xref> show that lower precision and robustness are strongly correlated with fixation durations (<italic>r</italic> = −.44,<italic>p</italic> &lt; 0.001 &amp; <italic>r</italic> = .55,<italic>p</italic> &lt; 0.001, respectively). The lower panels of Fig. <xref rid="Fig13" ref-type="fig">13</xref> show that the correlation between median fixation duration and precision becomes non-significant (<italic>r</italic> = −.13,<italic>p</italic> = 0.143) and that the correlation between fixation duration and robustness becomes smaller (<italic>r</italic> = .24,<italic>p</italic> = 0.007) when gazepath is used to detect fixations. A Williams test confirmed that these correlations between median fixation duration of the gazepath and Tobii classification differed significantly for both precision (<italic>t</italic>(124) = 3.67,<italic>p</italic> &lt; 0.001) and robustness (<italic>t</italic>(124) = −3.91,<italic>p</italic> &lt; 0.001). This is a strong indication that the gazepath method is able to detect fixations with higher accuracy than the standard Tobii method.
<fig id="Fig13"><label>Fig. 13</label><caption><p>Example of data-quality measures precision and robustness and their correlations with median fixation durations classified using the gazepath (GP) method and Tobii (TB) method. Gazepath classifies more fixations in higher quality data and has lower correlations between data quality and median fixation duration than the Tobii classification</p></caption><graphic xlink:href="13428_2017_909_Fig13_HTML" id="MO13"/></fig>
</p>
      <p>To verify that the correlations between data quality and fixation durations disappeared because gazepath (1) combined fixations that should not be split and (2) correctly did not classify the 9000 fixations that were classified by Tobii, we inspected the trials that had split fixations. Figure <xref rid="Fig14" ref-type="fig">14</xref> shows two trials that were typical for what we observed. There were instances where gazepath classified longer fixations, whereas Tobii classified multiple short fixations (A, B, C, &amp; D). It is difficult to tell what classification is better, given the noise in the data. Sometimes gazepath seems too conservative; for instance B is likely two multiple fixations, instead of the one gazepath classified. The lower panel of Fig. <xref rid="Fig14" ref-type="fig">14</xref> shows that gazepath does a much better job than Tobii in not classifying fixations when there is a loss of signal (E) and extreme noise in the data (F).
<fig id="Fig14"><label>Fig. 14</label><caption><p>Two examples of gazepath and Tobii classification for infant data. The <italic>upper panel</italic> shows instances where gazepath identifies longer fixations, whereas Tobii identifies multiple short fixations (A, B, C &amp; D). It is difficult to tell which method performed better, due to the large amount of noise. It seems that A, C, and D could be a single fixation as classified by gazepath, but they could also be multiple fixations as classified by Tobii. B is likely a double fixation that is correctly identified by Tobii, but not by gazepath. The <italic>lower panel</italic> shows instances (E &amp; F) where Tobii identified fixations while there is a loss of signal from one eye (E) and extreme noise (F), and here it is clear gazepath outperforms Tobii by not classifying any fixations</p></caption><graphic xlink:href="13428_2017_909_Fig14_HTML" id="MO14"/></fig>
</p>
    </sec>
    <sec id="Sec23">
      <title>Conclusion infant experimental data</title>
      <p>In this section, we showed that gazepath also performs well in low-sampled (60 Hz), noisy infant data. The main benefit of using the gazepath method over the standard Tobii method lies in the fact that gazepath classifies far fewer fixations than Tobii. Tobii misclassified around 9000 fixations, leading to spurious correlations between fixation durations and data quality. Gazepath lowered these correlations, but could not fully account for them, as was the case in the infant free-viewing data. Finally, it seems that gazepath might still be too conservative in classifying fixations, as it remains unclear whether most long fixations classified with gazepath reflect one real underlying fixations or are actually multiple fixations.</p>
    </sec>
  </sec>
  <sec id="Sec24">
    <title>General conclusion</title>
    <p>The aim of this project was to develop an easy-to-use eye-tracking data parsing tool that can be used to parse both low- and high-quality data into fixations and saccades. With the infant free-viewing data we showed how gazepath controlled for low-quality data in infants by reducing spurious correlations between fixation durations and data quality. The adult free-viewing data showed that gazepath is more sensitive than the standard EyeLink method in picking up small fixations. This finding was corroborated in the reading data set, for which we showed that gazepath can identify small fixations that are left undetected by the EyeLink method. This can be useful because small regressive saccades might be of interest in linguistic studies. Finally, we showed that gazepath also works well when parsing noisy infant data measured with a low sample rate eye-tracker and dynamic instead of static stimuli. Although gazepath seems conservative in setting its threshold, leading to (possibly too) long fixations, gazepath classified fixations better than the standard Tobii method. The largest benefit of gazepath is leaving out fixations that the Tobii method classified during loss of signal and extreme noise.</p>
    <p>The analyses show that gazepath provides a useful tool for parsing both low- and high-quality eye-tracking data. However, it is important to note that gazepath cannot turn low-quality data into a sequence of fixations and saccades that can be interpreted perfectly. It is important that researchers inspect the data and make sensible choices about whether data can be interpreted, or data quality is too low. Gazepath’s GUI provides the user with an interface to inspect the data of all participants and trials. This makes it easy to inspect the trials with abnormally high velocity thresholds or low robustness and precision. Moreover, by providing these data-quality measures directly, gazepath makes it also easier to report such measures, something rarely seen in the literature (Hessels et al. <xref ref-type="bibr" rid="CR11">2015</xref>).</p>
    <p>The gazepath method presented in this paper combines the best of several methods into one R-package. The data-driven non-parametric Mould et al. (<xref ref-type="bibr" rid="CR18">2012</xref>) algorithm is taken as a basis to account for individual differences in data quality and looking behavior. Furthermore, modified versions of the algorithms developed by Wass et al. (<xref ref-type="bibr" rid="CR38">2013</xref>) are used to make gazepath capable of dealing with noise typical in infant data. Finally, gazepath is implemented in R (R Core Team <xref ref-type="bibr" rid="CR21">2014</xref>), which is open-source software. Since gazepath comes with a Shiny app to provide a GUI, researchers can decide for themselves whether they like scripting or clicking.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Electronic supplementary material</title>
    <sec id="Sec25">
      <p>Below is the link to the electronic supplementary material.
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="13428_2017_909_MOESM1_ESM.pdf"><caption><p>(PDF 98.3 KB)</p></caption></media></supplementary-material>
</p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p>laptop: SONY VAIO VPCEH3N1E, Intel Core i5-2450M Processor, 2.50 GHz, 4GB</p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p>The use of dynamic stimuli may have introduced smooth pursuit eye movements, rather than fixations and saccades only. To assess the magnitude of this possible confound, the <xref ref-type="sec" rid="Sec25">Supplemental Materials</xref> provide the same analysis described here, without data points obtained during the dynamic part of the stimuli. In general, the analyses show similar results and overall conclusions remain the same.</p>
    </fn>
    <fn>
      <p>
        <bold>Electronic supplementary material</bold>
      </p>
      <p> The online version of this article (doi:10.3758/s13428-017-0909-3) contains supplementary material, which is available to authorized users.</p>
    </fn>
  </fn-group>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <mixed-citation publication-type="other">Andersson, R., Larsson, L., Holmqvist, K., Stridh, M., &amp; Nyström, M. (2016). One algorithm to rule them all? An evaluation and discussion of ten eye movement event-detection algorithms. <italic>Behavior Research Methods</italic>, 1–22.</mixed-citation>
    </ref>
    <ref id="CR2">
      <mixed-citation publication-type="other">Bicknell, K., &amp; Levy, R. (2011). Why readers regress to previous words: A statistical analysis. In <italic>Proceedings of the 33rd annual meeting of the Cognitive Science Society</italic> (pp. 931–936).</mixed-citation>
    </ref>
    <ref id="CR3">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blignaut</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Fixation identification: The optimum threshold for a dispersion algorithm</article-title>
        <source>Attention, Perception, and Psychophysics</source>
        <year>2009</year>
        <volume>71</volume>
        <issue>4</issue>
        <fpage>881</fpage>
        <lpage>895</lpage>
        <pub-id pub-id-type="doi">10.3758/APP.71.4.881</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cleveland</surname>
            <given-names>WS</given-names>
          </name>
        </person-group>
        <article-title>Robust locally weighted regression and smoothing scatterplots</article-title>
        <source>Journal of the American Statistical Association</source>
        <year>1979</year>
        <volume>74</volume>
        <issue>368</issue>
        <fpage>829</fpage>
        <lpage>836</lpage>
        <pub-id pub-id-type="doi">10.1080/01621459.1979.10481038</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>de Urabain</surname>
            <given-names>IRS</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>MH</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>TJ</given-names>
          </name>
        </person-group>
        <article-title>Grafix: A semiautomatic approach for parsing low-and high-quality eye-tracking data</article-title>
        <source>Behavior Research Methods</source>
        <year>2015</year>
        <volume>47</volume>
        <issue>1</issue>
        <fpage>53</fpage>
        <lpage>72</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-014-0456-0</pub-id>
        <pub-id pub-id-type="pmid">24671827</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <mixed-citation publication-type="other">Fan, J., &amp; Gijbels, I. (1996). <italic>Local polynomial modelling and its applications: Monographs on statistics and applied probability 66</italic> (vol. 66). CRC Press.</mixed-citation>
    </ref>
    <ref id="CR7">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Frazier</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Rayner</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences</article-title>
        <source>Cognitive Psychology</source>
        <year>1982</year>
        <volume>14</volume>
        <issue>2</issue>
        <fpage>178</fpage>
        <lpage>210</lpage>
        <pub-id pub-id-type="doi">10.1016/0010-0285(82)90008-1</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gredebäck</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>von Hofsten</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Eye tracking in infancy research</article-title>
        <source>Developmental Neuropsychology</source>
        <year>2009</year>
        <volume>35</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>19</lpage>
        <pub-id pub-id-type="doi">10.1080/87565640903325758</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Helo</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pannasch</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sirri</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Raemae</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>The maturation of eye movement behavior: Scene viewing characteristics in children and adults</article-title>
        <source>Vision Research</source>
        <year>2014</year>
        <volume>103</volume>
        <fpage>83</fpage>
        <lpage>91</lpage>
        <pub-id pub-id-type="doi">10.1016/j.visres.2014.08.006</pub-id>
        <?supplied-pmid 25152319?>
        <pub-id pub-id-type="pmid">25152319</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Henderson</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>Human gaze control during real-world scene perception</article-title>
        <source>Trends in Cognitive Sciences</source>
        <year>2003</year>
        <volume>7</volume>
        <issue>11</issue>
        <fpage>498</fpage>
        <lpage>504</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tics.2003.09.006</pub-id>
        <?supplied-pmid 14585447?>
        <pub-id pub-id-type="pmid">14585447</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hessels</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Andersson</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hooge</surname>
            <given-names>IT</given-names>
          </name>
          <name>
            <surname>Nyström</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kemner</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Consequences of eye color, positioning, and head movement for eye-tracking data quality in infant research</article-title>
        <source>Infancy</source>
        <year>2015</year>
        <volume>20</volume>
        <issue>6</issue>
        <fpage>601</fpage>
        <lpage>633</lpage>
        <pub-id pub-id-type="doi">10.1111/infa.12093</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hutzler</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Wimmer</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Eye movements of dyslexic children when reading in a regular orthography</article-title>
        <source>Brain and Language</source>
        <year>2004</year>
        <volume>89</volume>
        <issue>1</issue>
        <fpage>235</fpage>
        <lpage>242</lpage>
        <pub-id pub-id-type="doi">10.1016/S0093-934X(03)00401-2</pub-id>
        <?supplied-pmid 15010255?>
        <pub-id pub-id-type="pmid">15010255</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Karatekin</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Eye-tracking studies of normative and atypical development</article-title>
        <source>Developmental Review</source>
        <year>2007</year>
        <volume>27</volume>
        <issue>3</issue>
        <fpage>283</fpage>
        <lpage>348</lpage>
        <pub-id pub-id-type="doi">10.1016/j.dr.2007.06.006</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Karatekin</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Asarnow</surname>
            <given-names>RF</given-names>
          </name>
        </person-group>
        <article-title>Exploratory eye movements to pictures in childhood-onset schizophrenia and attention-deficit/hyperactivity disorder (ADHD)</article-title>
        <source>Journal of Abnormal Child Psychology</source>
        <year>1999</year>
        <volume>27</volume>
        <issue>1</issue>
        <fpage>35</fpage>
        <lpage>49</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1022662323823</pub-id>
        <?supplied-pmid 10197405?>
        <pub-id pub-id-type="pmid">10197405</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Komogortsev</surname>
            <given-names>OV</given-names>
          </name>
          <name>
            <surname>Gobert</surname>
            <given-names>DV</given-names>
          </name>
          <name>
            <surname>Jayarathna</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Koh</surname>
            <given-names>DH</given-names>
          </name>
          <name>
            <surname>Gowda</surname>
            <given-names>SM</given-names>
          </name>
        </person-group>
        <article-title>Standardization of automated analyses of oculomotor fixation and saccadic behaviors</article-title>
        <source>IEEE Transactions on Biomedical Engineering</source>
        <year>2010</year>
        <volume>57</volume>
        <issue>11</issue>
        <fpage>2635?</fpage>
        <lpage>2645</lpage>
        <pub-id pub-id-type="doi">10.1109/TBME.2010.2057429</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koornneef</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dotlacil</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>van den Broek</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Sanders</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>The influence of linguistic and cognitive factors on the time course of verb-based implicit causality</article-title>
        <source>The Quarterly Journal of Experimental Psychology</source>
        <year>2016</year>
        <volume>69</volume>
        <issue>3</issue>
        <fpage>455</fpage>
        <lpage>481</lpage>
        <pub-id pub-id-type="doi">10.1080/17470218.2015.1055282</pub-id>
        <?supplied-pmid 26031765?>
        <pub-id pub-id-type="pmid">26031765</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Matin</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Saccadic suppression: a review and an analysis</article-title>
        <source>Psychological Bulletin</source>
        <year>1974</year>
        <volume>81</volume>
        <issue>12</issue>
        <fpage>899</fpage>
        <pub-id pub-id-type="doi">10.1037/h0037368</pub-id>
        <?supplied-pmid 4612577?>
        <pub-id pub-id-type="pmid">4612577</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mould</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Foster</surname>
            <given-names>DH</given-names>
          </name>
          <name>
            <surname>Amano</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Oakley</surname>
            <given-names>JP</given-names>
          </name>
        </person-group>
        <article-title>A simple nonparametric method for classifying eye fixations</article-title>
        <source>Vision Research</source>
        <year>2012</year>
        <volume>57</volume>
        <fpage>18</fpage>
        <lpage>25</lpage>
        <pub-id pub-id-type="doi">10.1016/j.visres.2011.12.006</pub-id>
        <?supplied-pmid 22227608?>
        <pub-id pub-id-type="pmid">22227608</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nyström</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Holmqvist</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>An adaptive algorithm for fixation, saccade, and glissade detection in eyetracking data</article-title>
        <source>Behavior Research Methods</source>
        <year>2010</year>
        <volume>42</volume>
        <issue>1</issue>
        <fpage>188</fpage>
        <lpage>204</lpage>
        <pub-id pub-id-type="doi">10.3758/BRM.42.1.188</pub-id>
        <?supplied-pmid 20160299?>
        <pub-id pub-id-type="pmid">20160299</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paterson</surname>
            <given-names>KB</given-names>
          </name>
          <name>
            <surname>McGowan</surname>
            <given-names>VA</given-names>
          </name>
          <name>
            <surname>Jordan</surname>
            <given-names>TR</given-names>
          </name>
        </person-group>
        <article-title>Filtered text reveals adult age differences in reading: Evidence from eye movements</article-title>
        <source>Psychology and Aging</source>
        <year>2013</year>
        <volume>28</volume>
        <issue>2</issue>
        <fpage>352</fpage>
        <pub-id pub-id-type="doi">10.1037/a0030350</pub-id>
        <?supplied-pmid 23066801?>
        <pub-id pub-id-type="pmid">23066801</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <mixed-citation publication-type="other">R Core Team (2014). R: a language and environment for statistical computing. Vienna, Austria. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.R-project.org/">http://www.R-project.org/</ext-link></mixed-citation>
    </ref>
    <ref id="CR23">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rayner</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Reichle</surname>
            <given-names>ED</given-names>
          </name>
          <name>
            <surname>Stroud</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Williams</surname>
            <given-names>CC</given-names>
          </name>
          <name>
            <surname>Pollatsek</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>The effect of word frequency, word predictability, and font difficulty on the eye movements of young and older readers</article-title>
        <source>Psychology and Aging</source>
        <year>2006</year>
        <volume>21</volume>
        <issue>3</issue>
        <fpage>448</fpage>
        <pub-id pub-id-type="doi">10.1037/0882-7974.21.3.448</pub-id>
        <?supplied-pmid 16953709?>
        <pub-id pub-id-type="pmid">16953709</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rayner</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Castelhano</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Eye movements and the perceptual span in older and younger readers</article-title>
        <source>Psychology and Aging</source>
        <year>2009</year>
        <volume>24</volume>
        <issue>3</issue>
        <fpage>755</fpage>
        <pub-id pub-id-type="doi">10.1037/a0014300</pub-id>
        <?supplied-pmid 19739933?>
        <pub-id pub-id-type="pmid">19739933</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rayner</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Schotter</surname>
            <given-names>ER</given-names>
          </name>
          <name>
            <surname>Masson</surname>
            <given-names>ME</given-names>
          </name>
          <name>
            <surname>Potter</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Treiman</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>So much to read, so little time how do we read, and can speed reading help?</article-title>
        <source>Psychological Science in the Public Interest</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>1</issue>
        <fpage>4</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="doi">10.1177/1529100615623267</pub-id>
        <?supplied-pmid 26769745?>
        <pub-id pub-id-type="pmid">26769745</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reichle</surname>
            <given-names>ED</given-names>
          </name>
          <name>
            <surname>Liversedge</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Drieghe</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Blythe</surname>
            <given-names>HI</given-names>
          </name>
          <name>
            <surname>Joseph</surname>
            <given-names>HS</given-names>
          </name>
          <name>
            <surname>White</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Rayner</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Using E-Z Reader to examine the concurrent development of eye-movement control and reading skill</article-title>
        <source>Developmental Review</source>
        <year>2013</year>
        <volume>33</volume>
        <issue>2</issue>
        <fpage>110</fpage>
        <lpage>149</lpage>
        <pub-id pub-id-type="doi">10.1016/j.dr.2013.03.001</pub-id>
        <?supplied-pmid 24058229?>
        <pub-id pub-id-type="pmid">24058229</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Riby</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Hancock</surname>
            <given-names>PJ</given-names>
          </name>
        </person-group>
        <article-title>Viewing it differently: Social scene perception in Williams syndrome and autism</article-title>
        <source>Neuropsychologia</source>
        <year>2008</year>
        <volume>46</volume>
        <issue>11</issue>
        <fpage>2855</fpage>
        <lpage>2860</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2008.05.003</pub-id>
        <?supplied-pmid 18561959?>
        <pub-id pub-id-type="pmid">18561959</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <mixed-citation publication-type="other">RStudio Inc (2015). Easy web applications in R. [computer software manual]. (<ext-link ext-link-type="uri" xlink:href="http://www.rstudio.com/shiny/">http://www.rstudio.com/shiny/</ext-link>).</mixed-citation>
    </ref>
    <ref id="CR28">
      <mixed-citation publication-type="other">Shic, F., Scassellati, B., &amp; Chawarska, K. (2008). The incomplete fixation measure. In <italic>Proceedings of the 2008 symposium on eye tracking research &amp; applications</italic> (pp. 111–114).</mixed-citation>
    </ref>
    <ref id="CR29">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Steiger</surname>
            <given-names>JH</given-names>
          </name>
        </person-group>
        <article-title>Tests for comparing elements of a correlation matrix</article-title>
        <source>Psychological Bulletin</source>
        <year>1980</year>
        <volume>87</volume>
        <issue>2</issue>
        <fpage>245</fpage>
        <pub-id pub-id-type="doi">10.1037/0033-2909.87.2.245</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Walther</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Estimating the number of clusters in a data set via the gap statistic</article-title>
        <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>
        <year>2001</year>
        <volume>63</volume>
        <issue>2</issue>
        <fpage>411</fpage>
        <lpage>423</lpage>
        <pub-id pub-id-type="doi">10.1111/1467-9868.00293</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <mixed-citation publication-type="other">Tobii Eye Tracker User Manual (2006). Clearview analysis software. Tobii technology AB.</mixed-citation>
    </ref>
    <ref id="CR32">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van der Lans</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wedel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pieters</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Defining eye-fixation sequences across individuals and tasks: the binocular-individual threshold (bit) algorithm</article-title>
        <source>Behavior Research Methods</source>
        <year>2011</year>
        <volume>43</volume>
        <issue>1</issue>
        <fpage>239</fpage>
        <lpage>257</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-010-0031-2</pub-id>
        <?supplied-pmid 21287116?>
        <pub-id pub-id-type="pmid">21287116</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van Renswoude</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Raijmakers</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Visser</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Do infants have the horizontal bias?</article-title>
        <source>Infant Behavior and Development</source>
        <year>2016</year>
        <volume>44</volume>
        <fpage>38</fpage>
        <lpage>48</lpage>
        <pub-id pub-id-type="doi">10.1016/j.infbeh.2016.05.005</pub-id>
        <?supplied-pmid 27281348?>
        <pub-id pub-id-type="pmid">27281348</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <mixed-citation publication-type="other">Velichkovsky, B. M., Dornhoefer, S. M., Pannasch, S., &amp; Unema, P. J. (2000). Visual fixations and level of attentional processing. In <italic>Proceedings of the 2000 symposium on eye tracking research &amp; applications</italic> (pp. 79–85).</mixed-citation>
    </ref>
    <ref id="CR35">
      <mixed-citation publication-type="other">Vitu, F., McConkie, G., &amp; Zola, D. (1998). About regressive saccades in reading and their relation to word identification. <italic>Eye Guidance in Reading and Scene Perception</italic>, pp. 101–124.</mixed-citation>
    </ref>
    <ref id="CR38">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wass</surname>
            <given-names>SV</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>MH</given-names>
          </name>
        </person-group>
        <article-title>Parsing eye-tracking data of variable quality to provide accurate fixation duration estimates in infants and adults</article-title>
        <source>Behavior Research Methods</source>
        <year>2013</year>
        <volume>45</volume>
        <issue>1</issue>
        <fpage>229</fpage>
        <lpage>250</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-012-0245-6</pub-id>
        <?supplied-pmid 22956360?>
        <pub-id pub-id-type="pmid">22956360</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wass</surname>
            <given-names>SV</given-names>
          </name>
          <name>
            <surname>Forssman</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Leppänen</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Robustness and precision: How data quality may influence key dependent variables in infant eye-tracker analyses</article-title>
        <source>Infancy</source>
        <year>2014</year>
        <volume>19</volume>
        <issue>5</issue>
        <fpage>427</fpage>
        <lpage>460</lpage>
        <pub-id pub-id-type="doi">10.1111/infa.12055</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <mixed-citation publication-type="other">Wass, S. V., Jones, E. J., Gliga, T., Smith, T. J., Charman, T., &amp; Johnson, M. H. (2015). Shorter spontaneous fixation durations in infants with later emerging autism. <italic>Scientific Reports</italic>, 5.</mixed-citation>
    </ref>
  </ref-list>
</back>
