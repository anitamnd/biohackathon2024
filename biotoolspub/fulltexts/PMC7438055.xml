<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">eNeuro</journal-id>
    <journal-id journal-id-type="iso-abbrev">eNeuro</journal-id>
    <journal-id journal-id-type="hwp">eneuro</journal-id>
    <journal-id journal-id-type="pmc">eneuro</journal-id>
    <journal-id journal-id-type="publisher-id">eNeuro</journal-id>
    <journal-title-group>
      <journal-title>eNeuro</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2373-2822</issn>
    <publisher>
      <publisher-name>Society for Neuroscience</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7438055</article-id>
    <article-id pub-id-type="pmid">32699072</article-id>
    <article-id pub-id-type="doi">10.1523/ENEURO.0038-20.2020</article-id>
    <article-id pub-id-type="publisher-id">eN-OTM-0038-20</article-id>
    <article-categories>
      <subj-group subj-group-type="hwp-journal-coll">
        <subject>0200</subject>
        <subject>7</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Open Source Tools and Methods</subject>
        <subj-group>
          <subject>Novel Tools and Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepCINAC: A Deep-Learning-Based Python Toolbox for Inferring Calcium Imaging Neuronal Activity Based on Movie Visualization</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Denis</surname>
          <given-names>Julien</given-names>
        </name>
        <xref ref-type="author-notes" rid="FN4">
          <sup>*</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dard</surname>
          <given-names>Robin F.</given-names>
        </name>
        <xref ref-type="author-notes" rid="FN4">
          <sup>*</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Quiroli</surname>
          <given-names>Eleonora</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cossart</surname>
          <given-names>Rosa</given-names>
        </name>
        <xref ref-type="aff" rid="aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Picardo</surname>
          <given-names>Michel A.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1"/>
      </contrib>
      <aff id="aff1">Aix Marseille Univ, INSERM, INMED, Marseille 13273, France</aff>
    </contrib-group>
    <author-notes>
      <fn fn-type="other">
        <p>The authors declare no competing financial interests.</p>
      </fn>
      <fn fn-type="con">
        <p>Author contributions: J.D., R.F.D., R.C., and M.A.P. Designed Research; R.F.D. and J.D. Performed Research; J.D. Wrote code; M.P., J.D., R.F.D., and E.Q., Labeled data; R.F.D., J.D., M.A.P., and R.C. Wrote the paper.</p>
      </fn>
      <fn fn-type="supported-by">
        <p>This work was supported by the European Research Council under the European Union’s FP7 and Horizon 2020 research and innovation program Grants 242842 and 646925. J.D. was supported by the Fondation pour la Recherche Médicale Grant FDM20170638339. M.P was supported by the Fondation pour la Recherche Médicale Grant ARF20160936186.</p>
      </fn>
      <fn id="FN4" fn-type="equal">
        <p><sup>*</sup>J.D. and R.F.D. contributed equally to this work.</p>
      </fn>
      <corresp id="cor1">Correspondence should be addressed to Julien Denis at <email>julien.denis@inserm.fr</email>.</corresp>
    </author-notes>
    <pub-date pub-type="epreprint">
      <day>22</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>12</day>
      <month>8</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <season>Jul-Aug</season>
      <year>2020</year>
    </pub-date>
    <volume>7</volume>
    <issue>4</issue>
    <elocation-id>ENEURO.0038-20.2020</elocation-id>
    <history>
      <date date-type="received">
        <day>3</day>
        <month>2</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>6</day>
        <month>7</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>10</day>
        <month>7</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2020 Denis et al.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Denis et al.</copyright-holder>
      <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International license</ext-link>, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="ENEURO.0038-20.2020.pdf"/>
    <self-uri xlink:role="icon" xlink:href="ENEURO.0038-20.2020g1.jpg"/>
    <abstract>
      <title>Abstract</title>
      <p>Two-photon calcium imaging is now widely used to infer neuronal dynamics from changes in fluorescence of an indicator. However, state-of-the-art computational tools are not optimized for the reliable detection of fluorescence transients from highly synchronous neurons located in densely packed regions such as the CA1 pyramidal layer of the hippocampus during early postnatal stages of development. Indeed, the latest analytical tools often lack proper benchmark measurements. To meet this challenge, we first developed a graphical user interface (GUI) allowing for a precise manual detection of all calcium transients from imaged neurons based on the visualization of the calcium imaging movie. Then, we analyzed movies from mouse pups using a convolutional neural network (CNN) with an attention process and a bidirectional long-short term memory (LSTM) network. This method is able to reach human performance and offers a better F1 score (harmonic mean of sensitivity and precision) than CaImAn to infer neural activity in the developing CA1 without any user intervention. It also enables automatically identifying activity originating from GABAergic neurons. Overall, DeepCINAC offers a simple, fast and flexible open-source toolbox for processing a wide variety of calcium imaging datasets while providing the tools to evaluate its performance.</p>
    </abstract>
    <kwd-group>
      <kwd>calcium imaging</kwd>
      <kwd>CNN</kwd>
      <kwd>deep learning</kwd>
      <kwd>hippocampus</kwd>
      <kwd>LSTM</kwd>
      <kwd>neuronal activity</kwd>
    </kwd-group>
    <counts>
      <fig-count count="10"/>
      <table-count count="2"/>
      <equation-count count="0"/>
      <ref-count count="39"/>
      <page-count count="15"/>
      <word-count count="00"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>cover-date</meta-name>
        <meta-value>July/August 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Significance Statement</title>
    <p>Inferring neuronal activity from calcium imaging data remains a challenge because of the difficulty in obtaining a ground truth using patch clamp recordings and the problem of finding optimal tuning parameters of inference algorithms. DeepCINAC offers a flexible, fast and easy-to-use toolbox to infer neuronal activity from any kind of calcium imaging dataset through visual inspection.</p>
  </sec>
  <sec sec-type="intro" id="s2">
    <title>Introduction</title>
    <p><italic>In vivo</italic> calcium imaging is widely used to study activity in neuronal microcircuits. Advances in imaging now allows for the simultaneous recording of several thousands of neurons (<xref rid="B36" ref-type="bibr">Stringer et al., 2019</xref>). One difficulty resides in how to infer single neuron activation dynamics from changes in fluorescence of a calcium indicator. A challenge is therefore to offer an analytical tool that would be scalable to the wide variety of calcium imaging datasets while providing reliable analysis.</p>
    <p>State-of-the-art computational tools to infer neuronal activity (such as CaImAn; <xref rid="B30" ref-type="bibr">Pnevmatikakis et al., 2016</xref>; <xref rid="B17" ref-type="bibr">Giovannucci et al., 2019</xref>) are based on the deconvolution and demixing of fluorescence traces from segmented cells. However, to optimize the deconvolution parameters, a ground truth based on simultaneous targeted patch-clamp recordings and two-photon imaging is necessary (<xref rid="B7" ref-type="bibr">Chen et al., 2013</xref>; <xref rid="B11" ref-type="bibr">Evans et al., 2019</xref>).</p>
    <p>Moreover, an analysis based on the fluorescence traces even after a demixing process can still be biased by overlapping cells (<xref rid="B14" ref-type="bibr">Gauthier et al., 2018</xref>). In a recent study from Gauthier and collaborators (<xref rid="B14" ref-type="bibr">Gauthier et al., 2018</xref>) analyzing calcium imaging data recorded in the region CA1 in adult rodents (<xref rid="B13" ref-type="bibr">Gauthier and Tank, 2018</xref>), 66% of the cells were reported as having at least one false transient, and overall, among 33,090 transients (from 1325 sources), 67% were considered as true, 13% as false, and 20% were unclassified. Those contaminations increase the risk of misinterpretation of the data. Inferring neuronal activity from the developing hippocampus <italic>in vivo</italic> is even more challenging because of several factors: (1) recurring network synchronizations are a hallmark of developing neuronal networks (<xref rid="B31" ref-type="bibr">Provine, 1972</xref>; <xref rid="B12" ref-type="bibr">Galli and Maffei, 1988</xref>; <xref rid="B3" ref-type="bibr">Ben-Ari et al., 1989</xref>; <xref rid="B26" ref-type="bibr">O’Donovan, 1989</xref>), which results in frequent cell co-activations, (2) the somata of pyramidal neurons are densely packed which results in spatial overlap, (3) different calcium kinetics are observed in the same field of view (because of different cell types and different stages of neuronal maturation; <xref rid="B1" ref-type="bibr">Allene et al., 2012</xref>). All these points are illustrated in <xref ref-type="supplementary-material" rid="vid1">Movie 1</xref>, <xref ref-type="supplementary-material" rid="vid2">Movie 2</xref> (region CA1 of the hippocampus from mouse pups). In addition, most methods do not offer solutions to evaluate the performance of neuronal activity inference on user datasets. To meet those challenges, we have developed a graphical user interface (GUI) that allows for such evaluation through data exploration and a method based on deep learning to infer neuronal activity. Even if several deep-learning-based methods to infer neuronal activity from fluorescence signals have already been developed (<xref rid="B4" ref-type="bibr">Berens et al., 2018</xref>), none proposes a method directly based on raw two-photon imaging signals.</p>
    <supplementary-material content-type="local-data" id="vid1">
      <label>Movie 1.</label>
      <caption>
        <p><italic>In vivo</italic> two-photon imaging in the CA1 region of the hippocampus in a 12-d-old mouse pup. Field of view (FOV) is 80 × 80 µm, frame rate is 8 Hz, and video is speeded up 10 times. The video shows recurrent periods of neuronal activations recruiting a large number of adjacent neurons leading to spatial and temporal overlaps.</p>
      </caption>
      <media id="sv1" content-type="play-in-place" xlink:href="enu-eN-OTM-0038-20-s01.mp4" xlink:role="external-host-filename" orientation="portrait" position="anchor" mimetype="video">
        <object-id pub-id-type="doi">10.1523/ENEURO.0038-20.2020.video.1</object-id>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="vid2">
      <label>Movie 2.</label>
      <caption>
        <p><italic>In vivo</italic> two-photon imaging in the CA1 region of the hippocampus in a 7-d-old mouse pup. Field of view (FOV) is 100 × 100 µm, frame rate is 8 Hz, and video is speeded up 10 times. The Video shows different cell types (i.e., interneurons and pyramidal cells) with different calcium dynamics.</p>
      </caption>
      <media id="sv2" content-type="play-in-place" xlink:href="enu-eN-OTM-0038-20-s08.mp4" xlink:role="external-host-filename" orientation="portrait" position="anchor" mimetype="video">
        <object-id pub-id-type="doi">10.1523/ENEURO.0038-20.2020.video.2</object-id>
      </media>
    </supplementary-material>
    <p>Our goal was to train a classifier to recognize cell activation directly from a movie which falls into the domain of action recognition. Action recognition from videos has seen recent important progress thanks to deep learning (<xref rid="B5" ref-type="bibr">Bin et al., 2019</xref>). Using a similar approach, we have trained a binary classifier on calcium imaging movies (allowing us to explore both the forward and backward temporal information among the whole sequence of video frames) to capture the fluorescence dynamics in the field of view and then predict the activity of all identified cells. It gave us the opportunity to take full advantage of the information contained in the movie in terms of dynamics and potential overlaps or other sources of contamination that might not be accessible when working only on fluorescence time courses.</p>
    <p>To train the classifier, a ground truth was needed. To our knowledge, no calcium imaging datasets from the developing hippocampus <italic>in vivo</italic> with simultaneous electrophysiological ground truth measurements are available. The most accurate ground truth would require targeted patch-clamp recordings with two-photon imaging on all the different hippocampal cell types with different calcium dynamics. This is technically difficult, time consuming, and even more during development as the ground truth must be obtained from cells at various stages of maturation. As a result, we decided to base the ground truth on the visual inspection of raw movies using a custom-made GUI. It gives the advantages to work on any kind of calcium imaging dataset and to offer an easy tool to benchmark methods that infer neuronal activity.</p>
    <p>The GUI offers a tool to precisely and manually detect all calcium transients (from onset to peak, which is the time when cells are active). We collected and combined a corpus of manual annotations from four human experts representing 37 h of two-photon calcium imaging from 11 mouse pups aged between five and 16 postnatal days in the CA1 region using GCaMP6s. Almost 80% of the labeled data were used to train the model, while the rest was kept to benchmark the performance. Then, movies were processed using a convolutional neural network (CNN) with an attention mechanism and a bidirectional long-short term memory (LSTM) network (<xref rid="B22" ref-type="bibr">LeCun and Bengio, 1995</xref>; <xref rid="B18" ref-type="bibr">Hochreiter and Schmidhuber, 1997</xref>; <xref rid="B38" ref-type="bibr">Vaswani et al., 2017</xref>).</p>
    <p>To evaluate the method, we used the ground truth as a benchmark. We found that this method reached human level performance and offered a better sensitivity and F1 score than CaImAn to infer neuronal activity in the developing hippocampus without any user intervention. Overall, DeepCINAC (Calcium Imaging Neuronal Activity Classifier) offers a simple, ergonomic, fast, and flexible open-source toolbox for processing a wide variety of calcium imaging data while providing the tools to evaluate its performance.</p>
  </sec>
  <sec sec-type="materials|methods" id="s3">
    <title>Materials and Methods</title>
    <p>In this section, we will describe all the necessary steps to build a deep learning neural network “DeepCINAC.” This toolbox was developed to analyze <italic>in vivo</italic> two-photon calcium imaging data acquired in the developing hippocampus (see below, Experimental procedure and data acquisition). As a first step, we needed to set a ground truth that was established on the visualization of the recorded movie by three to four human experts (see below, Ground truth). Then, data are preprocessed (see below, Data preprocessing, feature engineering, and model description) and used to train the network (see below, Computational performance). As a final step, we used labeled data to evaluate the performance of DeepCINAC (see below, Performance evaluation). Tutorials and the source code are freely available online (see below, Toolbox and data availability).</p>
    <sec id="s3A">
      <title>Experimental procedure and data acquisition</title>
      <p>All experiments were performed under the guidelines of the French National Ethic Committee for Sciences and Health report on Ethical Principles for Animal Experimentation in agreement with the European Community Directive 86/609/EEC.</p>
      <sec id="s3A1">
        <title>Viral injection</title>
        <p>To induce widespread, rapid and stable expression of the calcium indicator GCaMP6s in hippocampal neurons at early postnatal stages, we intraventricularly injected a viral solution (pAAV.Syn.GCaMP6s.WPRE.SV40, Addgene #100843-AAV1) at P0 (Postnatal day zero) in mouse pups of either sex (<xref ref-type="fig" rid="F1">Fig. 1<italic>A</italic>,<italic>B</italic>
</xref>). This injection protocol was adapted from already published methods (<xref rid="B20" ref-type="bibr">Kim et al., 2013</xref>, <xref rid="B21" ref-type="bibr">2014</xref>). Mouse pups were anesthetized on ice for 3–4 min, and 2 µl of the viral solution were injected in the left lateral ventricle which coordinates were estimated at the 2/5 of the imaginary line between the λ and the eye at a depth of 400 µm. Expression of GCaMP was checked on slices and was sufficient for <italic>in vivo</italic> imaging as early as P5, which is consistent with already published data (<xref rid="B21" ref-type="bibr">Kim et al., 2014</xref>). In addition, GCaMP expression, brightness, and kinetics of the reporter was then stable throughout all developmental stages used (data not shown).</p>
        <fig id="F1" fig-type="figure" orientation="portrait" position="float">
          <label>Figure 1.</label>
          <caption>
            <p> Experimental paradigm. <bold><italic>A</italic></bold>, Experimental timeline. <bold><italic>B</italic></bold>, Intraventricular injection of GCaMP6s on pups (drawing) done at P0. <bold><italic>C</italic></bold>, Schematic representing the cranial window surgery. <bold><italic>D</italic></bold>, top left, Imaged field of view. Scale bar: 100 µm. Top right, Activity of five random neurons in the field of view (variation of fluorescence is expressed as Δf/f). Scale bar: 50 s. Bottom, Drawing of a head fixed pup under the microscope.</p>
          </caption>
          <graphic xlink:href="SN-ENUJ200193F001"/>
        </fig>
      </sec>
      <sec id="s3A2">
        <title>Surgery</title>
        <p>The surgery to implant a 3-mm-large cranial window above corpus callosum was adapted from described methods (<xref rid="B9" ref-type="bibr">Dombeck et al., 2010</xref>; <xref rid="B39" ref-type="bibr">Villette et al., 2015</xref>). Anesthesia was induced using 3% isoflurane in a mix of 90% O<sub>2</sub>-10% air and maintained during the whole surgery (∼1:30 h) between 1% and 2.5% isoflurane. Body temperature was controlled and maintained at 36°C. Analgesia was controlled using Buprenorphine (0.025 mg/kg). Coordinates of the window implant were estimated by eyes. The skull was removed and the cortex was gently aspirated until the external capsule/alveus that appears as a plexus of fibers was visible. Surface of the corpus callosum was protected with QuickSil (WPI) then the cannula with the window was implanted and fixed to the heaplate of the animal.</p>
      </sec>
      <sec id="s3A3">
        <title>Imaging</title>
        <p>Two-photon calcium imaging experiments were performed on the day of the surgery (<xref ref-type="fig" rid="F1">Fig. 1<italic>C</italic>,<italic>D</italic>
</xref>) at least 1 h after the end of the surgery. A total of 12,500-frames-long image series from a 400 × 400 μm field of view with a resolution of 200 × 200 pixels were acquired at a frame rate of 10.6 Hz (<xref ref-type="fig" rid="F1">Fig. 1<italic>D</italic>
</xref>). We then motion corrected the acquired images by finding the center of mass of the correlations across frames relative to a set of reference frames (<xref rid="B24" ref-type="bibr">Miri et al., 2011</xref>) .</p>
      </sec>
      <sec id="s3A4">
        <title>Cell segmentation</title>
        <p>To detect cell contours, we used either the segmentation method implemented in suite2p (<xref rid="B42" ref-type="bibr">Pachitariu et al., 2017</xref>) or the Constrained Nonnegative Matrix Factorization (CNMF) implemented in CaImAn.</p>
      </sec>
      <sec id="s3A5">
        <title>Activity inference</title>
        <p>To infer activity, we used the Markov chain Monte Carlo (MCMC) implemented in CaImAn on cell contours obtained from the CNMF of the toolbox. The MCMC spike inference was done as described (<xref rid="B30" ref-type="bibr">Pnevmatikakis et al., 2016</xref>). We used DeepCINAC predictions on both contours from suite2p and CaImAn.</p>
      </sec>
    </sec>
    <sec id="s3B">
      <title>Data visualization: GUI</title>
      <p>To visualize our data and explore the results from any spike inference method, we designed a GUI that provides a visual inspection of each cell’s activity (<xref ref-type="fig" rid="F2">Fig. 2</xref>). The GUI offers a set of functionalities allowing visualization of (1) calcium imaging movies centered and zoomed on the cell of interest during a time window that includes a given transient; (2) sources, transient profiles, and their correlations (as developed by Gauthier and collaborators); and (3) transient fluorescence signal shape.</p>
      <fig id="F2" fig-type="figure" orientation="portrait" position="float">
        <label>Figure 2.</label>
        <caption>
          <p>Examples of different uses of the GUI. The GUI can be used for data exploration (<bold><italic>A1</italic></bold>, <bold><italic>A2</italic></bold>), to establish the ground truth (<bold><italic>B</italic></bold>) and to evaluate DeepCINAC predictions (<bold><italic>C</italic></bold>). <bold><italic>A</italic></bold>, The GUI can be used to explore the activity inference from any methods. The spikes inferred from CaImAn are represented by the green marks at the bottom. The GUI allows the user to play the movie at the time of the selected transient and visualize the transients and source profile of the cell of interest. <bold><italic>A1</italic></bold>, Movie visualization and correlation between transient and source profiles allow the classification of the first selected transient as true positive (TP) and the second selected transient as false positive (FP). <bold><italic>A2</italic>,</bold> Movie visualization and correlation between transient and source profiles allow the classification of the selected transient as false negative (FN). <bold><italic>B</italic>,</bold> The GUI can be used to establish a ground truth. In this condition, it offers the user the possibility to manually annotate onset and peak of calcium transient. Onsets are represented by vertical dashed blue lines, peaks by green dots. <bold><italic>C</italic>,</bold> When the activity inference is done using DeepCINAC, the GUI allows the display of the classifier predictions. The prediction is represented by the red line. The dashed horizontal red line is a probability of one. The blue area represents time periods during which the probability is above a given threshold, in this example 0.5. T: transient profile, S: source profile, Corr: correlation, FOV: field of view.</p>
        </caption>
        <graphic xlink:href="SN-ENUJ200193F002"/>
      </fig>
      <p>Additionally, the GUI can be used to (1) display the spike times from an inference method (<xref ref-type="fig" rid="F2">Fig. 2<italic>A1</italic>,<italic>A2</italic>
</xref>), (2) establish a ground truth (<xref ref-type="fig" rid="F2">Fig. 2<italic>B</italic>
</xref>), and (3) visualize DeepCINAC predictions (<xref ref-type="fig" rid="F2">Fig. 2<italic>C</italic>
</xref>).</p>
      <p>The GUI was developed using Python and Tkinter package. It can read data from several formats including neurodata without borders files (<xref rid="B37" ref-type="bibr">Teeters et al., 2015</xref>; <xref rid="B34" ref-type="bibr">Rübel et al., 2019</xref>). More details on the GUI and a complete tutorial are available on GitLab (<ext-link ext-link-type="uri" xlink:href="https://gitlab.com/cossartlab/deepcinac">https://gitlab.com/cossartlab/deepcinac</ext-link>).</p>
    </sec>
    <sec id="s3C">
      <title>Ground truth</title>
      <sec id="s3C6">
        <title>Electrophysiological ground truth</title>
        <p>Ground truth data from experiments previously described were taken from crcns.org. (<xref rid="B7" ref-type="bibr">Chen et al., 2013</xref>; <xref rid="B15" ref-type="bibr">GENIE Project, 2015</xref>). Briefly, visual cortex neurons expressing the calcium indicator GCaMP6s were imaged while mice were presented with visual stimuli; 60-Hz two-photon imaging and loose cell-attached recordings at 10 kHz were performed simultaneously. Using ImageJ software, we downsampled imaging data to 10 Hz by averaging every six frames and rescaled it to 1.2 µm/pixel. We considered a cell active during a rise time if a spike was detected during that time and used the previously described GUI to convert those data in the cinac format so we could produce benchmarks and train a classifier using those data (for more details, see <xref rid="T1" ref-type="table">Table 1</xref>; Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>).</p>
        <table-wrap id="T1" orientation="portrait" position="float">
          <label>Table 1</label>
          <caption>
            <p>Data used to train the classifiers</p>
          </caption>
          <table frame="hsides" rules="none">
            <col align="left" valign="top" span="1"/>
            <col align="left" valign="top" span="1"/>
            <col align="left" valign="top" span="1"/>
            <col align="left" valign="top" span="1"/>
            <col align="left" valign="top" span="1"/>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1"/>
                <th align="left" rowspan="1" colspan="1">CINAC version*</th>
                <th align="left" rowspan="1" colspan="1"><italic>n</italic> cells</th>
                <th align="left" rowspan="1" colspan="1"><italic>n</italic> animals</th>
                <th align="left" rowspan="1" colspan="1"><italic>n</italic> frames</th>
              </tr>
            </thead>
            <tbody valign="top">
              <tr>
                <td align="left" rowspan="1" colspan="1">Hippo-dvt</td>
                <td align="left" rowspan="1" colspan="1">v1 v4 v6</td>
                <td align="left" rowspan="1" colspan="1">104<xref ref-type="table-fn" rid="TF5"><sup>1</sup></xref>
</td>
                <td align="left" rowspan="1" colspan="1">13<xref ref-type="table-fn" rid="TF6"><sup>2</sup></xref>
</td>
                <td rowspan="1" colspan="1">689,272</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Hippo-GECO</td>
                <td align="left" rowspan="1" colspan="1">v3</td>
                <td rowspan="1" colspan="1">5</td>
                <td rowspan="1" colspan="1">2</td>
                <td rowspan="1" colspan="1">45,000</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Hippo-6m</td>
                <td align="left" rowspan="1" colspan="1">v4</td>
                <td rowspan="1" colspan="1">3</td>
                <td rowspan="1" colspan="1">1</td>
                <td rowspan="1" colspan="1">42,000</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Barrel-ctx-6s</td>
                <td align="left" rowspan="1" colspan="1">v4</td>
                <td rowspan="1" colspan="1">20</td>
                <td rowspan="1" colspan="1">2</td>
                <td rowspan="1" colspan="1">36,000</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Visual-ctx-6s</td>
                <td align="left" rowspan="1" colspan="1">v5 v6</td>
                <td rowspan="1" colspan="1">7</td>
                <td align="left" rowspan="1" colspan="1">NA</td>
                <td rowspan="1" colspan="1">33,800</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Hippo-dvt-INs</td>
                <td align="left" rowspan="1" colspan="1">v7</td>
                <td rowspan="1" colspan="1">29</td>
                <td rowspan="1" colspan="1">9</td>
                <td rowspan="1" colspan="1">362,500</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="TF1">
              <p>Training dataset include validation dataset (see Materials and Methods).</p>
            </fn>
            <fn id="TF2">
              <p>Description of the datasets precising the number of frames, number of animals and field of views included, as well as the classifiers that used these datasets.</p>
            </fn>
            <fn id="TF3">
              <p><italic>n</italic>: number of.</p>
            </fn>
            <fn id="TF4">
              <p>* version that used at least part of those dataset.</p>
            </fn>
            <fn id="TF5">
              <label>1</label>
              <p>including two simulated movies, representing 32 cells and 80,000 frames.</p>
            </fn>
            <fn id="TF6">
              <label>2</label>
              <p>including two simulated movies.</p>
            </fn>
            <fn id="TF7">
              <p><xref rid="T1" ref-type="table">Table 1</xref> is supported by Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <supplementary-material content-type="local-data" id="tab1-1">
          <object-id pub-id-type="doi">10.1523/ENEURO.0038-20.2020.t1-1</object-id>
          <label>Extended Data Table 1-1</label>
          <caption>
            <p>Detailed data used to train and test the classifiers. Detailed content of training and test datasets used for all CINAC versions (v1 to v7) used in the analysis. Download <inline-supplementary-material id="t1-1" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="enu-eN-OTM-0038-20-s02.docx" content-type="local-data">Table 1-1, DOCX file</inline-supplementary-material>.</p>
          </caption>
        </supplementary-material>
      </sec>
      <sec id="s3C7">
        <title>Visual ground truth</title>
        <p>All functionalities of the GUI were used as criteria by each human expert to label the data. The ground truth was established based on two-photon calcium imaging from pups from 5 to 16 d old (see <xref rid="T1" ref-type="table">Table 1</xref>) in a four-step workflow as described in <xref ref-type="fig" rid="F3">Figure 3</xref>. Data were selected and labeled at least by two independent human experts (<xref ref-type="fig" rid="F3">Fig. 3</xref>, steps 1 and 2). We then combined those labels (<xref ref-type="fig" rid="F3">Fig. 3</xref>, step 3), and a final agreement was decided by three to four human experts (<xref ref-type="fig" rid="F3">Fig. 3</xref>, step 4). In addition, we trained another classifier for interneurons using transgenic pups in which only interneurons express the indicator (<xref rid="B23" ref-type="bibr">Melzer et al., 2012</xref>). As previously described, interneurons’ activity was labeled by three or four human experts and used to train an interneuron specific classifier (CINAC_v7; see <xref rid="T1" ref-type="table">Table 1</xref>). After training our classifier on a first set of cells, we used the predictions obtained on new data to establish additional ground truth based on the mistakes made on those data. At least two human experts labeled segments of 200 frames containing the wrong predictions. Additional visual ground truth was established by one human-expert (R.F.D.) on three other datasets from our lab using the GUI: (1) GCAMP6s calcium imaging movies from the developing barrel cortex (“Barrel-ctx-6s,” 1.5 Hz, 1.2 µm/pixel; <xref rid="B25" ref-type="bibr">Modol et al., 2020</xref>), (2) GCaMP6m imaging movies (“Hippo-6m,” 10 Hz, 2 µm/pixel), and (3) GECO imaging movies (“Hippo-GECO,” 5 Hz, 2 µm/pixel) both from the adult hippocampus (for details, see Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>). For Barrel-ctx-6s and Hippo-GECO, the CaImAn spike inference had already been performed by the original experimenter. We performed CaImAn spike inference on Hippo-6m.</p>
        <fig id="F3" fig-type="figure" orientation="portrait" position="float">
          <label>Figure 3.</label>
          <caption>
            <p>Workflow to establish the ground truth. First, a cell was randomly chosen in the imaged field of view. <bold>1</bold>, All putative transients of the segment to label were identified for the onset to the peak of each calcium event. <bold>2</bold>, Three human experts [“expert” (<italic>A</italic>), “expert” (<italic>B</italic>), “expert” (<italic>C</italic>)] independently annotated the segment. Among all putative transients, each human expert had to decide whether it was in his opinion a true transient. <bold>3</bold>, The combination of the labeling lead to “consensual transients” (i.e., true transient for each human expert; black square) and to “non-consensual transients” (i.e., true transient for at least one human expert but not all of them; open square). <bold>4</bold>, All non-consensual transients were discussed and ground truth was established.</p>
          </caption>
          <graphic xlink:href="SN-ENUJ200193F003"/>
        </fig>
        <sec id="s3C7c">
          <title>Cell type ground truth</title>
          <p>We used calcium imaging movies from GadCre (<xref rid="B23" ref-type="bibr">Melzer et al., 2012</xref>) positive animals injected with both h-SynGCaMP6s and Cre-dependent TdTomato to identify interneurons by the overlap of GCaMP6s and TdTomato signals. Using the GUI, we manually categorized 743 cells from 85 recordings among three categories: interneuron, pyramidal cell, and noisy cell. A total of 283 TdTomato-expressing cells were categorized as interneurons; 296 cells were categorized as putative pyramidal cells based on their localization in the pyramidal layer, their shape and their activity. Finally, 164 cells were categorized as noisy cells, determined by visually estimating their signal-to-noise ratio. We used a total of 643 cells (245 interneurons, 245 putative pyramidal cells, and 153 noisy ones) to train the cell type classifier and 100 cells (38 interneurons, 51 putative pyramidal cells, and 11 noisy ones, not included in the training dataset) were used to evaluate it.</p>
        </sec>
      </sec>
    </sec>
    <sec id="s3D">
      <title>Data preprocessing, feature engineering, and model description</title>
      <sec id="s3D8">
        <title>Definition of training, validation, and test datasets</title>
        <p>Our main dataset was split between a test dataset and a dataset used to train the classifier (referred to as training dataset; see <xref rid="T1" ref-type="table">Table 1</xref>; Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>). The training dataset used as the input of the classifier was randomly split with a ratio of 80–20% on a training and validation dataset. Validation data are used at the end of each epoch of the training to update the weights of the classifier.</p>
      </sec>
      <sec id="s3D9">
        <title>Data preprocessing and feature engineering</title>
        <p>Calcium movies in tiff format were split into individual tiff frames to be efficiently loaded in real time during the data generation for each batch of data fed to the classifier. For any given cell, a batch was composed of a sequence of 100 frames of 25 × 25 pixels window centered on the cell body. The length of the batch was chosen to fit for interneurons activity (rise and decay time). The window size was adapted to capture the activity of cells overlapping the target cell. In a recording of 12,500 frames, the number of transients ranges from 10 to 200, approximately. Thus, the frames during which the cell is active (from onset to peak), represents a low percentage of the total data. Because manual labeling is time consuming, the data used as ground truth were limited in size. To overcome the issue of the imbalanced data and to enlarge the dataset, we used the following three approaches.</p>
        <sec id="s3D9d">
          <title>Number 1: data augmentation (<xref rid="B29" ref-type="bibr">Perez and Wang, 2017</xref>)</title>
          <p>Temporal and spatial data augmentation was used. Temporal augmentation was used in that each block of 100 frames was overlapping with each other using a sliding window of 10 frames of length. Spatial augmentation took the form of transformations such as flip, rotation, or translations of the images. The data augmentation was done online, meaning that the transformations were done on the mini-batches that the model was processing. This allowed avoiding memory consumption and generating a dataset on multiple cores in real time.</p>
        </sec>
        <sec id="s3D9e">
          <title>Number 2: simulated data</title>
          <p>To balance our dataset, and increase the ability of the network to predict a fake transient as false, we have simulated calcium imaging movies with a higher rate of overlapping activity than our dataset (an example of artificial movie is available online on the GitLab page, alongside the source code: <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/cossartlab/deepcinac">https://gitlab.com/cossartlab/deepcinac</ext-link>). We started by collecting &gt;2000 cell contours from several movies that were segmented using suite2p. We randomly picked contours to build a cell map, with 16 cells for which one to four cells are overlapping it. We then generated for each cell an activity pattern, with a randomly chosen number of transients (from 2 to 16 for 1000 frames, 1.2–9.6 transients/min for a 10-Hz sampling rate) and duration of rise time (from one to eight frames, 100–800 ms for a 10-Hz sampling rate), following a random distribution. To simulate the fluorescence signal, on the rise time, we use a linear fit from the onset to peak, for the decay, we use an exponential decay with a decay from 10 to 12 frames of duration. To generate the calcium imaging movie, we decided on a basal level of activity, and then we adjusted the intensity of pixels in the cell for each frame according to the amplitude of the cell fluorescence, pixels in the cell have a different weight depending on whether they are in the soma or not, their intensity being lower in the nucleus. We finally added some Gaussian noise (µ = 0, σ<sup>2</sup> = 0.1) on every frame.</p>
        </sec>
        <sec id="s3D9f">
          <title>Number 3: data stratification</title>
          <p>In order to balance the data, we used data augmentation on selected movie segments (underrepresented segments) and excluded others (overrepresented segments) from the training dataset. After data stratification, we obtained ∼60% of the movie segments containing at least one real transient, 30% at least one fake transient without real ones, and 10% without transients. We were then able to be more precise over the proportion of segments with multiple transients or cropped transients. We gave higher weights to segments containing fake transients in order for the network to adjust the accuracy accordingly.</p>
          <p>The data augmentation, simulated data and stratification were applied to the part of the training dataset not used for validation.</p>
        </sec>
      </sec>
      <sec id="s3D10">
        <title>Model description</title>
        <p>To perform action recognition, we designed a joint model combining a forward-pass LSTM, a backward-pass LSTM and CNN features. In order for the bidirectional LSTM to focus on relevant information, we reinforced it by an attention process at the stage of encoding similar to previous work (<xref rid="B5" ref-type="bibr">Bin et al., 2019</xref>; <xref rid="B33" ref-type="bibr">Rémy, 2019</xref>). The model was designed using Python and Keras library (<xref rid="B8" ref-type="bibr">Chollet, 2015</xref>; see <xref ref-type="fig" rid="F4">Fig. 4</xref>).</p>
        <fig id="F4" fig-type="figure" orientation="portrait" position="float">
          <label>Figure 4.</label>
          <caption>
            <p>Architecture of DeepCINAC neural network. As a first step, for each set of inputs of the same cell, we extract CNNs features of video frames that we pass to an attention mechanism and feed the outputs into a forward pass network (FU, green units) and a backward pass network (BU, orange units), representing a bidirectional LSTM. Another bidirectional LSTM is fed from the attention mechanism and previous bidirectional LSTM outputs. A LSTM (MU, blue units) then integrates the outputs from the process of the three types of inputs to generate a final video representation. A sigmoid activation function is finally used to produce a probability for the cell to be active at each given frame given as input.</p>
          </caption>
          <graphic xlink:href="SN-ENUJ200193F004"/>
        </fig>
        <p>The model used to predict the cell activity takes three inputs, each representing the same sequence of 100 frames (around 10 s of activity). Each frame had dimensions of 25 × 25 pixels, centered around the cell of interest, whose activity we want to classify. The first input has all its pixels set to zero except for the mask of the cell of interest (cell activity). The second input has all its pixels set to zero except for the mask of the cells that intersect the cell of interest (overlapping activity). The final input has the cell of interest and the one intersecting its pixels set to zeros (neuropil activity). That way, the model has all the information necessary to learn to classify the cell’s activity according to its fluorescence variation.</p>
        <p>The model used to predict the cell type takes two inputs, each representing the same sequence of 500 frames (around 50 s of activity). Each frame had dimensions of 20 × 20 pixels, centered around the cell of interest, whose cell type we want to classify. The first input has all its pixels set to zero except for the mask of the cell of interest (cell activity). The second input has all its pixels.</p>
        <p>We used dropout (<xref rid="B35" ref-type="bibr">Srivastava et al., 2014</xref>) to avoid overfitting, but no batch normalization. The activation function was swish (<xref rid="B32" ref-type="bibr">Ramachandran et al., 2017</xref>). The loss function was binary cross-entropy and the optimizer was RMSprop. To classify cell activity, the output of the model was a vector of length 100 with values between 0 and 1 representing the probability for the cell to be active at a given frame of the sequence. To classify the cell type (interneuron, pyramidal cell, or noisy cell), the output was three values ranging from 0 to 1 and whose sum is equal to 1, representing the probability for a cell to be one of those three cell types.</p>
      </sec>
    </sec>
    <sec id="s3E">
      <title>Computational performance</title>
      <sec id="s3E11">
        <title>Classifier training</title>
        <p>We trained the classifier on a Linux-based HPC cluster where 4 CPUs (Intel(R) Xeon(R) CPU EV-2680 v3), 320 GB of RAM and 2 bi-GPU NVIDIA Tesla V100 were allocated for the processing task. To give an estimation of the time required to complete the training, the general classifier (CINAC_v1) was trained over 14 epochs. Training took around 40 h (&lt;3 h by epoch).</p>
      </sec>
      <sec id="s3E12">
        <title>Classifier prediction</title>
        <p>Using Linux-based workstation with one GPU (NVIDIA GeForce GTX 1080), 12 CPUs (Intel Xeon CPU W-2135 at 3.70 GHz), and 64 GB of RAM, the time to predict the cell activity on a movie of 12,500 frames was on average 13 s, ∼3.5 h for 1000 cells. The time to predict the cell type on a movie of 12,500 frames was on average 2 s, ∼33 min for 1000 cells. Similar performance was achieved using Google Colab.</p>
      </sec>
    </sec>
    <sec id="s3F">
      <title>Performance evaluation</title>
      <sec id="s3F13">
        <title>Descriptive metrics for activity classifier: sensitivity, precision, F1 score</title>
        <p>We evaluated the performance of the activity classifiers which predict for each frame if a cell is active or not. We chose to measure the sensitivity and precision values, as well as the F1 score that combines precision and sensitivity into a single metric defined as the harmonic mean of precision and sensitivity (<xref rid="B16" ref-type="bibr">Géron, 2019</xref>). Because we have a skewed dataset (cells being mostly inactive), we chose not to use the accuracy. The output of the binary classifier being the probability for a cell to be active at a given frame, we considered that a transient was predicted as true if at least during one of its frames the cell was predicted as active. On this basis, we were then able to compute the sensitivity (defined as the proportion of real transients that were detected) and the precision (defined as the proportion of detected transients that are real transients). We used these metrics to base the choice of the “best” epoch on the classifier performance on the test dataset rather than the performance on validation dataset. However, we stopped the training when the validation dataset metrics reached a plateau.</p>
      </sec>
      <sec id="s3F14">
        <title>Descriptive metrics for cell type classifier: sensitivity, precision, F1 score</title>
        <p>We evaluated the performance of the cell type classifier which predicts the type of a cell. We chose to measure the sensitivity and precision values, as well as the F1 score. To do so we used the metrics module of the Python package scikit-learn (<xref rid="B28" ref-type="bibr">Pedregosa et al., 2011</xref>) that returns the confusion matrix and a classification report containing those metrics.</p>
      </sec>
      <sec id="s3F15">
        <title>Statistical analysis</title>
        <p>The distribution of F1 score values on the datasets for each inference method were compared using Wilcoxon signed-rank test with an a priori significance level of <italic>p </italic>=<italic> </italic>0.05 using scipy Python package (<xref rid="B27" ref-type="bibr">Virtanen et al., 2020</xref>). This test was performed only on distribution with &gt;15 samples. Significance level: we used * for 0.01 ≤ <italic>p</italic> &lt; 0.05, ** for 0.001 ≤ <italic>p</italic> &lt; 0.01, and *** for <italic>p</italic> &lt; 0.001.</p>
      </sec>
      <sec id="s3F16">
        <title>Detection of overlap activity</title>
        <p>Based on empirical research we found that 15% of overlap was the minimal size above which a true transient in the cell is sufficient to trigger a false transient in the overlapped cell. For all pairs of overlapping cells (with an intersected area of at least 15% of the highest area of the two cells), we computed their transient profiles over all putative activations (all rise time over the full recording) and then calculated the Pearson correlation with their respective cell source profile. To assure to attribute the correct transient to the truly active cell we used a high correlation threshold 0.7 for the first cells and low threshold for the second cell of 0.2. We considered that the transient was a true activation of the first cell leading to a false transient in the second one. Indeed, we observed that the correlation method such as the one used in <xref rid="B14" ref-type="bibr">Gauthier et al. (2018)</xref> is not always sufficient to classify correctly the transient activity. However, by using the combination of a very low and high threshold, we assure (in most of the cases) that one cell is having a false transient while the other one is truly active. Finally, we evaluated whether the classifier could classify the putative transient of the second cell as false (with a prediction &lt;0.5).</p>
      </sec>
      <sec id="s3F17">
        <title>Comparison with CaImAn</title>
        <p>We compared the classifier performance against a state-of-the-art computational tool, namely, CaImAn. To fairly compare CaImAn and DeepCINAC to the ground truth, we used the cell contours obtained from the CNMF. The spike inference from the MCMC as well as DeepCINAC predictions and the ground truth were established on these contours. A transient was considered as detected by CaImAn if at least one spike was inferred during the rise time of the transient.</p>
      </sec>
    </sec>
    <sec id="s3G">
      <title>DeepCINAC workflow</title>
      <p>To summarize, DeepCINAC uses .cinac files built using the GUI. To train a classifier, those files are given as inputs to the neuronal network, providing time series data representing the calcium fluorescence dynamics of the cells. The same files can be used to benchmark the performance of a classifier and using the GUI, it is possible to add new data for training based on the errors of previous classifier outputs (<xref ref-type="fig" rid="F5">Fig. 5</xref>).</p>
      <fig id="F5" fig-type="figure" orientation="portrait" position="float">
        <label>Figure 5.</label>
        <caption>
          <p>DeepCINAC step by step workflow. <bold><italic>A</italic></bold>, Schematic of two-photon imaging experiment. <bold><italic>B</italic></bold>, Screenshot of DeepCINAC GUI used to explore and annotate data. <bold><italic>C</italic></bold>, The GUI produces .cinac files that contain the necessary data to train or benchmark a classifier. <bold><italic>D</italic></bold>, Schematic representation of the architecture of the model that will be used to train the classifier and predict neuronal activity. <bold><italic>E</italic></bold>, Training of the classifier using the previously defined model. <bold><italic>F</italic></bold>, Schematic of a raster plot resulting from the inference of the neuronal activity using the trained classifier. <bold><italic>G</italic></bold>, Evaluation of the classifier performance using precision, sensitivity and F1 score. <bold><italic>H</italic></bold>, Active learning pipeline: screenshots of the GUI used to identify edge cases where the classifier wrongly infers the neuronal activity and annotate new data on similar situations to add data for a new classifier training.</p>
        </caption>
        <graphic xlink:href="SN-ENUJ200193F005"/>
      </fig>
    </sec>
    <sec id="s3H">
      <title>Toolbox and data availability</title>
      <p>The source code is available on GitLab (<ext-link ext-link-type="uri" xlink:href="https://gitlab.com/cossartlab/deepcinac">https://gitlab.com/cossartlab/deepcinac</ext-link>). The page includes a full description of the method, a user manual, tutorials and test data, as well as the settings used. A notebook configured to work on Google Colab is also provided, allowing for the classifier to run online, thus avoiding installing the necessary environment and providing a free GPU. The toolbox has been tested on windows (v7 Pro), Mac Os X (MacOS Mojave), and Linux Ubuntu (v.18.04.1).</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s4">
    <title>Results</title>
    <sec id="s4A">
      <title>Validation of visual ground truth</title>
      <p>As a first step, we asked whether the visualization of fluorescent transients was a good estimation of spiking activity present in a neuron. To do so, we used previously published data combining loose seal cell attached recordings with two-photon calcium imaging (<xref rid="B7" ref-type="bibr">Chen et al., 2013</xref>; <xref rid="B15" ref-type="bibr">GENIE Project, 2015</xref>). We compared the visual ground truth to the “true” spiking of the cell. We found that visual inspection of calcium imaging movies allows the detection of 87.1%, 79.1%, and 80.7% true transients (i.e., spike associated transient) for each human expert respectively (median sensitivity; <xref ref-type="fig" rid="F6">Fig. 6<italic>A</italic>
</xref>). Among visually detected transients, 98.7%, 98.6%, and 98.6% were true transients for each human expert respectively (median precision; <xref ref-type="fig" rid="F6">Fig. 6<italic>B</italic>
</xref>). The F1 scores that combine these two previous metrics were 84.1%, 81.5%, and 85.9% for each human expert, respectively (median value; <xref ref-type="fig" rid="F6">Fig. 6<italic>C</italic>
</xref>). We evaluated the classifier CINAC_v6 trained with some recordings of “Visual-ctx-6s” and “Hippo-dvt” (Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>). We found that it allows the detection of 94% of the true transients (median sensitivity; <xref ref-type="fig" rid="F6">Fig. 6<italic>A</italic>
</xref>). Among predicted transients, 94.2% were true transients (median precision; <xref ref-type="fig" rid="F6">Fig. 6<italic>B</italic>
</xref>). F1 score was 94.7% (median value; <xref ref-type="fig" rid="F6">Fig. 6<italic>C</italic>
</xref>). Overall, we conclude that in absence of patch-clamp-based ground truth, the visual inspection of the movie provides a good estimation of neuronal activity and that deep learning approach based on movie visualization can reach the human level in estimating cell activations.</p>
      <fig id="F6" fig-type="figure" orientation="portrait" position="float">
        <label>Figure 6.</label>
        <caption>
          <p>Validation of visual ground truth and deep learning approach. <bold><italic>A</italic></bold>, Boxplots showing sensitivity for the three human experts (R.F.D., J.D., M.A.P.) and CINAC_v6 evaluated against the known ground truth from four cells from the GENIE project. <bold><italic>B</italic></bold>, Boxplots showing precision for the three human experts (R.F.D., J.D., M.A.P.) and CINAC_v6 evaluated against the known ground truth from four cells from the GENIE project. <bold><italic>C</italic></bold>, Boxplots showing F1 score for the three human experts (R.F.D., J.D., M.A.P.) and CINAC_v6 evaluated against the known ground truth from four cells from the GENIE project. Each colored dot represents a cell. Cell labels in the legend correspond to session identifiers from the dataset. CINAC_v6 is a classifier trained on data from the GENIE project and the Hippo-dvt dataset (<xref rid="T1" ref-type="table">Table 1</xref>; Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>).</p>
        </caption>
        <graphic xlink:href="SN-ENUJ200193F006"/>
      </fig>
    </sec>
    <sec id="s4B">
      <title>DeepCINAC performance evaluation on developing hippocampus dataset</title>
      <sec id="s4B1">
        <title>Comparing DeepCINAC against CaImAn and human level</title>
        <p>We compared the performance of DeepCINAC and CaImAn (<xref rid="B30" ref-type="bibr">Pnevmatikakis et al., 2016</xref>), a well-established algorithm to infer neuronal activity, against the visual ground truth on CA1 hippocampus data during development (Hippo-dvt). We first evaluated DeepCINAC (CINAC_v1) on 20 putative pyramidal neurons and five interneurons (<xref ref-type="fig" rid="F7">Fig. 7</xref>). The median sensitivity was 80.3% (interquartile range 75–94.5; <xref ref-type="fig" rid="F7">Fig. 7<italic>A</italic>
</xref>), the median precision was 90.8% (interquartile range 81.2–95.5; <xref ref-type="fig" rid="F7">Fig. 7<italic>B</italic>
</xref>), and the median F1 score was 86.3% (interquartile range 78.9–91.3; <xref ref-type="fig" rid="F7">Fig. 7<italic>C</italic>
</xref>).</p>
        <fig id="F7" fig-type="figure" orientation="portrait" position="float">
          <label>Figure 7.</label>
          <caption>
            <p>Evaluation of CINAC_v1 performance on Hippo-dvt dataset. <bold><italic>A</italic></bold>, Boxplots showing sensitivity for the three human experts (R.F.D., J.D., M.A.P.), CaImAn and CINAC_v1 evaluated against the visual ground truth of 25 cells. A total of 15 cells were annotated by J.D. and R.F.D., six by M.A.P. <bold><italic>B</italic></bold>, Boxplots showing precision for the three human experts (R.F.D., J.D., M.A.P.), CaImAn and CINAC_v1 evaluated against the visual ground truth of 25 cells. A total of 15 cells were annotated by J.D. and R.F.D., six by M.A.P. <bold><italic>C</italic></bold>, Boxplots showing F1 score for the three human experts (R.F.D., J.D., M.A.P.), CaImAn and CINAC_v1 evaluated against the visual ground truth of 25 cells. A total of 15 cells were annotated by J.D. and R.F.D., six by M.A.P. Each colored dot represents a cell, the number inside indicates the cell’s id and each color represents a session as identified in the legend. CINAC_v1 is a classifier trained on data from the Hippo-dvt dataset (<xref rid="T1" ref-type="table">Table 1</xref>; Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>). <xref ref-type="fig" rid="F7">Figure 7</xref> is supported by Extended Data <xref ref-type="supplementary-material" rid="fig7-1">Figures 7-1</xref>, <xref ref-type="supplementary-material" rid="fig7-2">7-2</xref>. *<italic>p</italic> &lt; 0.05.</p>
          </caption>
          <graphic xlink:href="SN-ENUJ200193F007"/>
        </fig>
        <supplementary-material content-type="local-data" id="fig7-1">
          <object-id pub-id-type="doi">10.1523/ENEURO.0038-20.2020.f7-1</object-id>
          <label>Extended Data Figure 7-1</label>
          <caption>
            <p>Comparison of CINAC performance to human experts. <bold><italic>A</italic></bold>, Boxplot displaying F1 score of two human experts (R.F.D. and J.D.) and CINAC_v1. Here are shown 15 cells annotate by both experts. <bold><italic>B</italic></bold>, Boxplot displaying F1 score of one human expert (M.A.P.) and CINAC_v1. Here are shown six cells annotated by M.A.P. Each colored dot represents a cell, the number inside indicates the cell’s id and each color represents a session as identified in the legend. CINAC_v1 is a classifier trained on data from the Hippo-dvt dataset (<xref rid="T1" ref-type="table">Table 1</xref>; Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>). Download <inline-supplementary-material id="f7-1" mimetype="image" mime-subtype="tiff" xlink:href="enu-eN-OTM-0038-20-s03.tif" content-type="local-data">Figure 7-1, TIF file</inline-supplementary-material>.</p>
          </caption>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="fig7-2">
          <object-id pub-id-type="doi">10.1523/ENEURO.0038-20.2020.f7-2</object-id>
          <label>Extended Data Figure 7-2</label>
          <caption>
            <p>Onset to peak detection of calcium transient. Boxplot showing the proportion of frames predicted as active during the transient rise time. CINAC_v1 is a classifier trained on data from the Hippo-dvt dataset (<xref rid="T1" ref-type="table">Table 1</xref>; Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>). Each colored dot represents a transient and each color represents a session as identified in the legend. Download <inline-supplementary-material id="f7-2" mimetype="image" mime-subtype="tiff" xlink:href="enu-eN-OTM-0038-20-s04.tif" content-type="local-data">Figure 7-2, TIF file</inline-supplementary-material>.</p>
          </caption>
        </supplementary-material>
        <p>We next evaluated CaImAn on the same cells using the same metrics. The median sensitivity was 60.6% (interquartile range 45.6–76; <xref ref-type="fig" rid="F7">Fig. 7<italic>A</italic>
</xref>), the median precision was 100% (interquartile range 93.8–100; <xref ref-type="fig" rid="F7">Fig. 7<italic>B</italic>
</xref>). The median F1 score was 70.1% (interquartile range 62.6–81.6; <xref ref-type="fig" rid="F7">Fig. 7<italic>C</italic>
</xref>), which was significantly lower than CINAC_v1 F1 score (Wilcoxson signed-rank test, <italic>T</italic> = 50 and <italic>p </italic>=<italic> </italic>0.002). Finally, we asked whether DeepCINAC could perform as well as human “experts.” The median CINAC_v1 F1 score on the 15 cells annotated by the two human experts (J.D and R.F.D.) was 88.2% (interquartile range 78.3–92), which was significantly lower than R.F.D. and J.D F1 scores (F1 = 95.2%, <italic>T</italic> = 4, <italic>p </italic>=<italic> </italic>0.002 and F1 = 96.8%, <italic>T</italic> = 22, <italic>p </italic>=<italic> </italic>0.031, respectively; Extended Data <xref ref-type="supplementary-material" rid="fig7-1">Fig. 7-1<italic>A</italic>
</xref>). However, on six cells annotated by M.A.P., CINAC_v1 and M.A.P. F1 scores were close (F1 = 84.3% and F1 = 86.4%, respectively; Extended Data <xref ref-type="supplementary-material" rid="fig7-1">Fig. 7-1<italic>B</italic>
</xref>). Although DeepCINAC is still not at the ground truth level (combination of triple human labeling), it approximates human level.</p>
      </sec>
      <sec id="s4B2">
        <title>Specific handling of overlap</title>
        <p>One important characteristic of data from the developing CA1 region of the hippocampus is the high density of active neurons that can lead to overlap. This overlap between cells leading to false transients was pointed out as a specific issue in the analysis of calcium traces from a demixing (<xref rid="B14" ref-type="bibr">Gauthier et al., 2018</xref>). We asked whether the classifier would be able to distinguish real transients from increases in fluorescence because of the activity of an overlapping cell. Based on the visual inspection of imaged fields of view with numerous overlaps, we chose to specifically test the algorithm on calcium imaging data containing 391 cells segmented using CaImAn. Among those cells, we detected a total of 426 transients (fluorescence rise time) from 23 cells that were likely because of overlapping activity from a neighboring cell (see method for overlap activity detection). Among those transients, 98.6% were correctly classified as false by CINAC_v1 (general classifier), 93.2% were correctly classified as false by the CINAC_v7 (interneuron specific classifier), and 93.2% were correctly classified as false by CaImAn. We next asked whether the results could be improved by the use of another segmentation method. To do so, we performed the same analysis on the exact same field of view using the classifier prediction on the segmented cells obtained from suite2p (<xref rid="B42" ref-type="bibr">Pachitariu et al., 2017</xref>). Among a total of 480 cells, a total of 2718 transients from 101 cells were likely because of the activation of an overlapping cell, 99.1% of them were correctly classified as false by CINAC_v1.</p>
      </sec>
      <sec id="s4B3">
        <title>Onset to peak prediction</title>
        <p>Since we aimed at predicting as active all the frames included in the full rise time of the calcium transient (from onset to peak), we looked at the proportion of frames predicted as active in real transients. Using the general classifier (CINAC_v1), the median ratio of frames predicted among each real transient was 85.7% (interquartile range 70–100) for the 20 putative pyramidal cells and the five putative interneurons (Extended Data <xref ref-type="supplementary-material" rid="fig7-2">Fig. 7-2</xref>). We demonstrated that CINAC_v1 allows the detection of cell activation all along the rise time, giving us both the onset of cell activation and the duration of the rise time (Extended Data Fig. <xref ref-type="supplementary-material" rid="fig7-2">7-2</xref>).</p>
      </sec>
    </sec>
    <sec id="s4C">
      <title>Classifier generalization and specialization</title>
      <sec id="s4C4">
        <title>DeepCINAC performances on other datasets</title>
        <p>A major aspect to consider in the development of algorithms to infer neuronal activity from calcium imaging data is the ability to be easily scalable to the wide variety of datasets (i.e., different indicators, different brain regions, …).</p>
        <p>We investigated the extent to which DeepCINAC (CINAC_v1) that was trained on data from the developing hippocampus would perform on other datasets (Extended Data <xref ref-type="supplementary-material" rid="fig8-1">Fig. 8-1</xref>). To answer that question, we used (1) GECO imaging movies (Hippo-GECO, 5 Hz, 2 µm/pixel; Extended Data <xref ref-type="supplementary-material" rid="fig8-1">Fig. 8-1<italic>A</italic></xref>), (2) GCaMP6m imaging movies (Hippo-6m, 10 Hz, 2 µm/pixel; Extended Data <xref ref-type="supplementary-material" rid="fig8-1">Fig. 8-1<italic>B</italic></xref>) both from the adult hippocampus, (3) GCAMP6s calcium imaging movies from the developing barrel cortex (Barrel-ctx-6s, 1.5 Hz, 1.2 µm/pixel; Extended Data <xref ref-type="supplementary-material" rid="fig8-1">Fig. 8-1<italic>C</italic></xref>; <xref rid="B25" ref-type="bibr">Modol et al., 2020</xref>), (4) GCAMP6s calcium imaging movies of interneurons from the developing hippocampus (“Hippo-dvt-INs,” 10 Hz, 2 µm/pixel; Extended Data <xref ref-type="supplementary-material" rid="fig8-1">Fig. 8-1<italic>D</italic></xref>) and GCaMP6s recordings from the adult visual cortex (Visual-ctx-6s, downsampled 10 Hz, rescaled 1.2 µm/pixel, see Materials and Methods; Extended Data <xref ref-type="supplementary-material" rid="fig8-2">Fig. 8-2</xref>). We show that DeepCINAC performs well on Hippo-6m and Barrel-ctx-6s data. On Hippo-6m, F1 scores were 66.7% and 70.9% for CaImAn and CINAC_v1, respectively (Extended Data <xref ref-type="supplementary-material" rid="fig8-1">Fig. 8-1<italic>B</italic></xref>, bottom panel). On Barrel-ctx-6s, F1 scores were 54.3% and 76.4% for CaImAn and CINAC_v1, respectively (Extended Data <xref ref-type="supplementary-material" rid="fig8-1">Fig. 8-1<italic>C</italic></xref>, bottom panel). However, CINAC_v1 does not generalize well enough to infer activity on the Hippo-GECO recordings (F1 score = 44.2%; Extended Data <xref ref-type="supplementary-material" rid="fig8-1">Fig. 8-1<italic>A</italic></xref>) neither on Visual-ctx-6s (F1 score = 69.9%; Extended Data <xref ref-type="supplementary-material" rid="fig8-1">Fig. 8-1<italic>C</italic></xref>).</p>
        <supplementary-material content-type="local-data" id="fig8-1">
          <object-id pub-id-type="doi">10.1523/ENEURO.0038-20.2020.f8-1</object-id>
          <label>Extended Data Figure 8-1</label>
          <caption>
            <p>Comparison of CaImAn and CINAC_v1 performances on various dataset. <bold><italic>A</italic></bold>, Boxplot displaying the sensitivity (top panel), precision (middle panel), and F1 score (bottom panel) for Hippo-GECO dataset. For each panel, we evaluated CaImAn performance as well as CINAC_v1. <bold><italic>B</italic></bold>, Boxplot displaying the sensitivity (top panel), precision (middle panel), and F1 score (bottom panel) for Hippo-6m dataset. <bold><italic>C</italic></bold>, Boxplot displaying the sensitivity (top panel), precision (middle panel), and F1 score (bottom panel) for Barrel-ctx-6s. <bold><italic>D</italic></bold>, Boxplot displaying the sensitivity (top panel), precision (middle panel), and F1 score (bottom panel) for Hippo-dvt-INs dataset. Each colored dot represents a cell, the number inside indicates the cell’s id and each color represents a session as identified in the legend. CINAC_v1 is a classifier trained on data from the Hippo-dvt dataset (<xref rid="T1" ref-type="table">Table 1</xref>; Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>). Download <inline-supplementary-material id="f8-1" mimetype="image" mime-subtype="tiff" xlink:href="enu-eN-OTM-0038-20-s05.tif" content-type="local-data">Figure 8-1, TIF file</inline-supplementary-material>.</p>
          </caption>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="fig8-2">
          <object-id pub-id-type="doi">10.1523/ENEURO.0038-20.2020.f8-2</object-id>
          <label>Extended Data Figure 8-2</label>
          <caption>
            <p>Use of DeepCINAC classifiers to optimize performances on Visual-ctx-6s dataset. <bold><italic>A</italic></bold>, Boxplots showing sensitivity for CINAC_v1, CINAC_v5 and CINAC_v6 evaluated against the known ground truth of four cells from the GENIE project. <bold><italic>B</italic></bold>, Boxplots showing precision for CINAC_v1, CINAC_v5, and CINAC_v6 evaluated against the known ground truth of four cells from the GENIE project. <bold><italic>C</italic></bold>, Boxplots showing F1 score for CINAC_v1, CINAC_v5, and CINAC_v6 evaluated against the known ground truth of four cells from the GENIE project. CINAV_v1 is a classifier trained on data from the Hippo-dvt dataset, CINAC_v5 is a classifier trained on data from Visual-ctx-6s dataset, CINAC_v6 is a classifier trained on data from Visual-ctx-6s dataset and four cells from the Hippo-dvt dataset (<xref rid="T1" ref-type="table">Table 1</xref>; Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>). Each colored dot represents a cell. Cell labels in the legend correspond to session identifiers from the dataset. Download <inline-supplementary-material id="f8-2" mimetype="image" mime-subtype="tiff" xlink:href="enu-eN-OTM-0038-20-s06.tif" content-type="local-data">Figure 8-2, TIF file</inline-supplementary-material>.</p>
          </caption>
        </supplementary-material>
        <p>To overcome these poor performances on Hippo-GECO and Visual-ctx-6s and to improve performances on Barrel-ctx-6s and Hippo-6m datasets, we considered two strategies. The first one consists in training a classifier specific to the data. The second one consists in adding part of the new data to the large database to improve the classifier ability to generalize (<xref ref-type="fig" rid="F8">Fig. 8</xref>; Extended Data <xref ref-type="supplementary-material" rid="fig8-1">Fig. 8-1</xref>).</p>
        <fig id="F8" fig-type="figure" orientation="portrait" position="float">
          <label>Figure 8.</label>
          <caption>
            <p>Use of DeepCINAC classifiers to optimize performances on various dataset. <bold><italic>A</italic></bold>, Boxplot displaying the sensitivity (top panel), precision (middle panel) and F1 score (bottom panel) for Hippo-GECO dataset. For each panel, we evaluated CaImAn performance as well as two different versions of CINAC (v1 and v3). CINAC_v1 is a classifier trained on data from the Hippo-dvt dataset and CINAC_v3 is a classifier trained on data from the Hippo-GECO dataset (<xref rid="T1" ref-type="table">Table 1</xref>; Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>). <bold><italic>B</italic></bold>, Boxplot displaying the sensitivity (top panel), precision (middle panel) and F1 score (bottom panel) for Hippo-6m dataset. For each panel, we evaluated CaImAn performance as well as two different versions of CINAC (v1 and v4). CINAC_v1 is a classifier trained on data from the Hippo-dvt dataset and CINAC_v4 is a classifier trained on data from the Hippo-dvt, Hippo-6m, and Barrel-ctx-6s dataset (<xref rid="T1" ref-type="table">Table 1</xref>; Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>). <bold><italic>C</italic></bold>, Boxplot displaying the sensitivity (top panel), precision (middle panel) and F1 score (bottom panel) for Barrel-ctx-6s dataset. For each panel, we evaluated CaImAn performance as well as two different versions of CINAC (v1 and v4). CINAC_v1 is a classifier trained on data from the Hippo-dvt dataset and CINAC_v4 is a classifier trained on data from the Hippo-dvt, Hippo-6m, and Barrel-ctx-6s dataset (<xref rid="T1" ref-type="table">Table 1</xref>; Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>). <bold><italic>D</italic></bold>, Boxplot displaying the sensitivity (top panel), precision (middle panel) and F1 score (bottom panel) for Hippo-dvt-INs dataset. For each panel, we evaluated CaImAn performance as well as two different versions of CINAC (v1 and v7). CINAC_v1 is a classifier trained on data from the Hippo-dvt dataset and CINAC_v7 is a classifier trained on interneurons from the Hippo-dvt dataset (<xref rid="T1" ref-type="table">Table 1</xref>; Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>). Each colored dot represents a cell, the number inside indicates the cell’s id and each color represents a session as identified in the legend. <xref ref-type="fig" rid="F8">Figure 8</xref> is supported by Extended Data <xref ref-type="supplementary-material" rid="fig8-1">Figures 8-1</xref>, <xref ref-type="supplementary-material" rid="fig8-2">8-2</xref>, <xref ref-type="supplementary-material" rid="fig8-3">8-3</xref>.</p>
          </caption>
          <graphic xlink:href="SN-ENUJ200193F008"/>
        </fig>
        <p>Since our performances were low using CINAC_v1, we adopted the first strategy to improve the classifier on Hippo-GECO and Visual-ctx-6s datasets. We used part of these datasets to train specific classifiers and evaluate them on the remaining data (Extended Data <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>). First, we observed that the Hippo-GECO-specific classifier (i.e., CINAC_v3) performed better than CINAC_v1 and CaImAn (CINAC_v3 median F1 score = 69.6%, CINAC_ v1 median F1 score = 12.9%, and CaImAn median F1 score = 63.5%; <xref ref-type="fig" rid="F8">Fig. 8<italic>A</italic>
</xref>, bottom panel). This increase in F1 score from CINAC_v1 to CINAC_v3 was because of an increase in the sensitivity of the classifier (CINAC_v1 median sensitivity = 10%, CINAC_v3 median sensitivity = 70.3%; <xref ref-type="fig" rid="F8">Fig. 8<italic>A</italic>
</xref>, top panel) with a moderate loss in precision (CINAC_v1 median precision = 95%, CINAC_v3 median precision = 81.2%; <xref ref-type="fig" rid="F8">Fig. 8<italic>A</italic>
</xref>, middle panel). Second, we showed that the Visual-ctx-6s datasets specific classifiers (i.e., CINAC_V5 and CINAC_v6) performed better than CINAC_v1 and CaImAn (CINAC_v1 median F1 score = 69.9%, CINAC_v5 median F1 score = 73.9%, and CINAC_v6 median F1 score = 94.7%; Extended Data <xref ref-type="supplementary-material" rid="fig8-2">Fig. 8-2<italic>C</italic></xref>). Because CINAC_v5 was trained exclusively on labeled data from the Visual-ctx-6s dataset, it allows an increase in the classifier sensitivity (CINAC_v1 median sensitivity = 59.2%, CINAC_v5 median sensitivity = 100%; Extended Data <xref ref-type="supplementary-material" rid="fig8-2">Fig. 8-2<italic>A</italic>
</xref>). However, this increase was achieved at the cost of a reduced precision (CINAC_v1 median precision = 96.2%, CINAC_v5 median precision = 59%; Extended Data <xref ref-type="supplementary-material" rid="fig8-2">Fig. 8-2<italic>B</italic>
</xref>), and overall a slight increase in the F1 score (Extended Data <xref ref-type="supplementary-material" rid="fig8-2">Fig. 8-2<italic>C</italic>
</xref>). To improve the performance on this dataset, we extended the CINAC_v5 training set with four cells from the Hippo-dvt to train CINAC_v6. This allows us to increase both sensitivity (CINAC_v6 median sensitivity = 94%) and precision (CINAC_v6 median precision = 94.2%) of the classifier, leading to a large improvement of the F1 score (CINAC_v6 median F1 score = 94.7%; Extended Data <xref ref-type="supplementary-material" rid="fig8-2">Fig. 8-2<italic>C</italic>
</xref>). In a nutshell, when the dataset to analyze has different calcium dynamics, a new classifier specifically trained on this dataset would reach higher performance than CINAC_v1.</p>
        <p>On the datasets from the adult hippocampus (Hippo-6m) and developing barrel cortex (Barrel-ctx-6s), since CINAC_v1 performances were close to the performance reached on Hippo-dvt dataset, we considered the second strategy. We extended the CINAC_v1 training dataset with labeled data from Hippo-6m and Barrel-ctx-6s datasets and trained a new classifier (CINAC_v4). First, we observed that on the Hippo-6m dataset, CINAC_v4 classifier has better sensitivity and precision than CINAC_v1 and CaImAN (CINAC_v1 median sensitivity = 53.8%, CINAC_ v4 median sensitivity = 58.7%, CaImAn median sensitivity = 37.3%, CINAC_v1 median precision = 95.3%, CINAC_v4 median precision = 96.2%, CaImAn median precision = 95.7%, CINAC_v1 median F1 score = 68%, CINAC_v4 median F1 score = 71.6%, CaImAn median F1 score = 51.4%; <xref ref-type="fig" rid="F8">Fig. 8<italic>B</italic>
</xref>). Second, we found that on Barrel-ctx-6s dataset, CINAC_v4 classifier performed better than CINAC_v1 and CaImAn (CINAC_v4 F1 score = 87.3%, CINAC_v1 F1 score = 79.7%, CaImAn F1 score = 53.3%; <xref ref-type="fig" rid="F8">Fig. 8<italic>C</italic>
</xref>, bottom panel). This increase in F1 score from CINAC_v1 to CINAC_V4 was because of an increase in the sensitivity of the classifier (CINAC_v4 median sensitivity = 92.6%, CINAC_v1 median sensitivity = 67.1%, CaImAn median sensitivity = 43.5%; <xref ref-type="fig" rid="F8">Fig. 8<italic>C</italic>
</xref>, top panel) with a moderate loss in precision (CINAC_v4 median precision = 84.3%, CINAC_v1 median precision = 95.9%, CaImAn median precision = 92.9%; <xref ref-type="fig" rid="F8">Fig. 8<italic>C</italic>
</xref>, middle panel). Overall, we confirm here that adding part of a new dataset to the training set of CINAC_v1 classifier allows us to improve the performance on this new dataset. Thus, one could use the training dataset of CINAC_v1 and add part of a new dataset to train a classifier that would achieve better performance than CINAC_v1 on this new data.</p>
        <p>Because we benefited from already published data from our group (Barrel-ctx-6s), we next asked whether we could arrive at the same conclusion using CINAC_v1. We used the activity inferred by CINAC_v1 on this data and performed assemblies detection analysis as described previously (<xref rid="B25" ref-type="bibr">Modol et al., 2020</xref>). We found the same number of assemblies as the original analysis (Extended Data <xref ref-type="supplementary-material" rid="fig8-3">Fig. 8-3<italic>A</italic>,<italic>B</italic></xref>, first two panels), as well as the same topographic organization (Extended Data <xref ref-type="supplementary-material" rid="fig8-3">Fig. 8-3<italic>A</italic>,<italic>B</italic></xref>, bottom panels). We confirmed that the assemblies detected by either CaImAn or CINAC_v1 were composed of similar cells (Extended Data <xref ref-type="supplementary-material" rid="fig8-3">Fig. 8-3<italic>C</italic></xref>).</p>
        <supplementary-material content-type="local-data" id="fig8-3">
          <object-id pub-id-type="doi">10.1523/ENEURO.0038-20.2020.f8-3</object-id>
          <label>Extended Data Figure 8-3</label>
          <caption>
            <p>Cell assemblies detection and organization using CaImAn and CINAC_v1 on published data. <bold><italic>A</italic></bold>, <bold><italic>B</italic></bold>, The top panel represents the clustered covariance matrix of synchronous calcium events (SCE). The middle panel represents neurons active in SCE organized by cluster (cell assembly). The bottom panel represents the cell’s map, each color represents a cell assembly. <bold><italic>A</italic></bold>, Cell assemblies detection results using CaImAn. <bold><italic>B</italic></bold>, Cell assemblies detection results using CINAC_v1. <bold><italic>C</italic></bold>, Individual cells composing assemblies in each method. an a represents the number of neurons detected by <xref rid="B25" ref-type="bibr">Modol et al. (2020)</xref>, using CaImAn; an b represents the number of neurons detected using CINAC_v1. Each color represents a cell assembly, color coded as in the maps. Download <inline-supplementary-material id="f8-3" mimetype="image" mime-subtype="tiff" xlink:href="enu-eN-OTM-0038-20-s07.tif" content-type="local-data">Figure 8-3, TIF file</inline-supplementary-material>.</p>
          </caption>
        </supplementary-material>
      </sec>
      <sec id="s4C5">
        <title>DeepCINAC performances on different cell types</title>
        <p>A second important aspect to infer neuronal activity from calcium imaging movies is the variety of cell types recorded in the same field of view (e.g., interneuron and pyramidal cells). In recordings from the hippocampus, we observed that most interneurons have very different calcium dynamics than pyramidal cells (higher fluorescence signal followed by a plateau). Because CINAC_v1 was mainly trained on the activity of pyramidal cells, we suspected that it would not provide accurate inference on interneurons. Using the GUI, we verified its prediction on interneurons and concluded that they were not always optimal (Extended Data <xref ref-type="supplementary-material" rid="fig8-1">Fig. 8-1<italic>D</italic>
</xref>). To improve activity inference on interneurons, we trained an interneuron specific classifier. In more detail, the precision of the inference was similar for CINAC_v1 and CINAC_v7 (CINAC_v1 median precision = 79%, CINAC_v7 median precision = 78.9%; <xref ref-type="fig" rid="F8">Fig. 8<italic>D</italic>
</xref>, top panel). However, CINAC_v7 provides more sensitive inference (CINAC_v1 median sensitivity = 88.6%, CINAC_v7 median sensitivity = 92.6%; <xref ref-type="fig" rid="F8">Fig. 8<italic>D</italic>
</xref>, middle panel). As a result, the specific classifier performed better than the general one on interneurons (CINAC_v1 median F1 score = 81.9%, CINAC_v7 median F1 score = 85.1%; <xref ref-type="fig" rid="F8">Fig. 8<italic>D</italic>
</xref>, bottom panel).</p>
      </sec>
    </sec>
    <sec id="s4D">
      <title>Cell type inference using DeepCINAC</title>
      <p>Recently, a deep-learning method using a similar model to DeepCINAC was proposed to differentiate cell types (<xref rid="B43" ref-type="bibr">Troullinou et al., 2019</xref>). This model was based on the analysis of fluorescence traces from various cell types and automatically classified imaged cells in different types. We asked whether DeepCINAC would be able to distinguish interneurons from pyramidal cells using as input the calcium imaging movie rather than the fluorescence trace. Additionally, we added a noise category in the training dataset allowing us to automatically discard cells. We achieved a general F1 score of 86%. We had a sensitivity of 90.2%, 81.6%, and 81.8% and a precision of 90.2%, 91.2%, and 60% for pyramidal, interneuron, and noisy cells, respectively (<xref rid="T2" ref-type="table">Table 2</xref>).</p>
      <table-wrap id="T2" orientation="portrait" position="float">
        <label>Table 2</label>
        <caption>
          <p>Cell type prediction confusion matrix</p>
        </caption>
        <table frame="hsides" rules="none">
          <col align="left" valign="top" span="1"/>
          <col align="left" valign="top" span="1"/>
          <col align="left" valign="top" span="1"/>
          <col align="left" valign="top" span="1"/>
          <col align="left" valign="top" span="1"/>
          <thead>
            <tr>
              <th align="left" rowspan="1" colspan="1"/>
              <th align="left" rowspan="1" colspan="1"/>
              <th align="center" colspan="3" rowspan="1">Ground truth</th>
            </tr>
            <tr>
              <th align="left" rowspan="1" colspan="1"/>
              <th align="left" rowspan="1" colspan="1"/>
              <th align="left" rowspan="1" colspan="1">Pyramidal cell</th>
              <th align="left" rowspan="1" colspan="1">Interneuron</th>
              <th align="left" rowspan="1" colspan="1">Noise</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" rowspan="3" colspan="1">
                <bold>Prediction</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">Pyramidal cell</td>
              <td align="char" char="." rowspan="1" colspan="1">46</td>
              <td align="char" char="." rowspan="1" colspan="1">5</td>
              <td align="char" char="." rowspan="1" colspan="1">0</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Interneuron</td>
              <td align="char" char="." rowspan="1" colspan="1">1</td>
              <td align="char" char="." rowspan="1" colspan="1">31</td>
              <td align="char" char="." rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Noise</td>
              <td align="char" char="." rowspan="1" colspan="1">4</td>
              <td align="char" char="." rowspan="1" colspan="1">2</td>
              <td align="char" char="." rowspan="1" colspan="1">9</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="TF8">
            <p>Confusion matrix, representing the number of true positives, true negatives, false positives, and false negatives. Ground truth refers to the manually detected interneurons and pyramidal cells. Prediction refers to the type predicted by the classifier for the same cells.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Since activity inference performance using DeepCINAC depends on the cell type, we perform this cell type prediction before activity inference. During the activity inference of a movie DeepCINAC can be configured to switch between different activity classifiers depending on the type of the cell to predict.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s5">
    <title>Discussion</title>
    <p>Deep learning-based method(s) to infer neuronal activity from two-photon calcium imaging datasets use cellular fluorescence signals as inputs. Here, we propose a method based on the visual inspection of the recordings. We will discuss the advantages and limitations of this approach.</p>
    <p>Using the movie dynamics, we benefited from all the information available in the calcium imaging movie. This approach allowed us to not rely on a demixing algorithm to produce the neuron’s traces. Instead, by working directly on the raw calcium imaging movie, the algorithm has learned to identify a transient and distinguish overlap activity from a real transient. DeepCINAC achieves better performance than CaImAn and is able to achieve human performance level on some fields of view and cells.</p>
    <p>Additionally, we show that a classifier trained on a specific dataset (“Hippo-dvlt-6s”) is able to generalize to other datasets (Hippo-6m and Barrel-ctx-6s). DeepCINAC allows training of flexible classifiers whose generalization on new datasets can be improved by adding part of this new dataset to the training (at the cost of slightly reduced performance of the classifier on original data). However, we show that generalization is not always achieved such as in the case of a classifier trained on Hippo-dvt data and used to predict activity on some very different datasets (Hippo-GECO and Visual-ctx-6s). This is likely explained by the difference in calcium indicator, imaging rate, and imaging resolution. We demonstrated that this limitation can be circumvented by training specific classifiers. Overall, this approach allowed us to create classifiers that scale to different developmental stages (from P5 to adult), different types of neurons (pyramidal cells and interneurons), as well as different indicators (GCaMP6s, GCaMP6m, GECO).</p>
    <p>Analysis of calcium imaging data may be impacted by some factors: (1) small amplitude transients, (2) transients occurring during the decay of another one, (3) summations, and (4) X and Y movement or neuropil activation. Users can evaluate the impact of those factors through visual inspection of the inferred activity using the deepCINAC GUI.</p>
    <p>Finally, we explored the range of values of hyperparameters to optimize the accuracy of the classifier. Labeling data are time consuming, but the training does not need any parameters tuning and the prediction is straight forward. Neither tedious manual tuning of parameters is required, nor a GPU on a local device because we provide a notebook to run predictions on Google Colab (see Materials and Methods). Predictions are fast, with a run-time of around 10 s by cell for 12,500 frames, meaning &lt;3 h for 1000 cells. However, a GPU would be necessary to train the network on a big dataset.</p>
    <p>Already widely used by many calcium imaging labs (<xref rid="B10" ref-type="bibr">Driscoll et al., 2017</xref>; <xref rid="B13" ref-type="bibr">Gauthier and Tank, 2018</xref>; <xref rid="B19" ref-type="bibr">Katlowitz et al., 2018</xref>; <xref rid="B2" ref-type="bibr">Andalman et al., 2019</xref>), CaImAn offers a performing and functional analysis pipeline. Although the complex fine tuning of CaImAn parameters on the dataset might lead to a suboptimal spike inference from the model, we decided to compare CaImAn against our ground truth.</p>
    <p>The benchmarks remain limited to a small number of cells for which we established a ground truth and may be extended to more cells. Notably, a future approach could be to use more realistic simulated data such as done in a recent work (<xref rid="B6" ref-type="bibr">Charles et al., 2019</xref>).</p>
    <p>In the model we used, each cell was represented by a segment of the field of view, in our case a 25 × 25 pixel (50 × 50 µm) window that allows complete coverage of the cell fluorescence and potential overlapping cells. Consequently, the network is able to generalize its prediction to recordings acquired with this resolution (2 µm/pixel). However, to be efficient on another calcium imaging dataset with a different resolution it would be necessary to train a new classifier adjusting the window size accordingly. Importantly, we trained the model on a selection of cells with valid segmentations; meaning that a cell is not represented by several contours. The inference performance of the classifier might decrease on cells whose segmentation was not properly achieved.</p>
    <p>Since precise spike inference cannot be experimentally assessed on the data, we chose to infer the activity of the cell defined by the fluorescence rise time instead of inferring the spikes. However, with a ground truth based on patch-clamp recordings, we could adapt this method to switch from a binary classification task to a regression task, predicting the number of spikes at each frame.</p>
  </sec>
  <sec id="s6">
    <title>Conclusion</title>
    <p>We built DeepCINAC basing the ground truth on visual inspection of the movie and training the classifier on movie segments. DeepCINAC offers a flexible, fast, and easy-to-use toolbox to infer neuronal activity from a variety of two-photon calcium imaging dataset, reaching human level performance. It provides the tools to measure its performance based on human evaluation. Currently, DeepCINAC provides several trained classifiers on CA1 two-photon calcium imaging at early postnatal stages; its performance might still be improved with more labeled data. In the future, we believe that a variety of classifiers collaboratively trained for specific datasets should be available to open access.</p>
  </sec>
</body>
<back>
  <ack>
    <p>Acknowledgements: We thank the Centre de Calcul Intensif d’Aix-Marseille for granting access to its high-performance computing resources. We would like to thank Marco Bocchio, Laura Modol, Yannick Bollmann, Susanne Reichinnek and Vincent Villette for providing us calcium imaging data.</p>
  </ack>
  <ref-list content-type="nameDate">
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><string-name><surname>Allene</surname><given-names>C</given-names></string-name>, <string-name><surname>Picardo</surname><given-names>MA</given-names></string-name>, <string-name><surname>Becq</surname><given-names>H</given-names></string-name>, <string-name><surname>Miyoshi</surname><given-names>G</given-names></string-name>, <string-name><surname>Fishell</surname><given-names>G</given-names></string-name>, <string-name><surname>Cossart</surname><given-names>R</given-names></string-name> (<year>2012</year>) <article-title>Dynamic changes in interneuron morphophysiological properties mark the maturation of hippocampal network activity</article-title>. <source>J Neurosci</source>
<volume>32</volume>:<fpage>6688</fpage>–<lpage>6698</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0081-12.2012</pub-id>
<?supplied-pmid 22573691?><pub-id pub-id-type="pmid">22573691</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><string-name><surname>Andalman</surname><given-names>AS</given-names></string-name>, <string-name><surname>Burns</surname><given-names>VM</given-names></string-name>, <string-name><surname>Lovett-Barron</surname><given-names>M</given-names></string-name>, <string-name><surname>Broxton</surname><given-names>M</given-names></string-name>, <string-name><surname>Poole</surname><given-names>B</given-names></string-name>, <string-name><surname>Yang</surname><given-names>SJ</given-names></string-name>, <string-name><surname>Grosenick</surname><given-names>L</given-names></string-name>, <string-name><surname>Lerner</surname><given-names>TN</given-names></string-name>, <string-name><surname>Chen</surname><given-names>R</given-names></string-name>, <string-name><surname>Benster</surname><given-names>T</given-names></string-name>, <string-name><surname>Mourrain</surname><given-names>P</given-names></string-name>, <string-name><surname>Levoy</surname><given-names>M</given-names></string-name>, <string-name><surname>Rajan</surname><given-names>K</given-names></string-name>, <string-name><surname>Deisseroth</surname><given-names>K</given-names></string-name> (<year>2019</year>) <article-title>Neuronal dynamics regulating brain and behavioral state transitions</article-title>. <source>Cell</source>
<volume>177</volume>:<fpage>970</fpage>–<lpage>985.e20</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2019.02.037</pub-id>
<?supplied-pmid 31031000?><pub-id pub-id-type="pmid">31031000</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><string-name><surname>Ben-Ari</surname><given-names>Y</given-names></string-name>, <string-name><surname>Cherubini</surname><given-names>E</given-names></string-name>, <string-name><surname>Corradetti</surname><given-names>R</given-names></string-name>, <string-name><surname>Gaiarsa</surname><given-names>JL</given-names></string-name> (<year>1989</year>) <article-title>Giant synaptic potentials in immature rat CA3 hippocampal neurones</article-title>. <source>J Physiol</source>
<volume>416</volume>:<fpage>303</fpage>–<lpage>325</lpage>. <pub-id pub-id-type="doi">10.1113/jphysiol.1989.sp017762</pub-id>
<?supplied-pmid 2575165?><pub-id pub-id-type="pmid">2575165</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><string-name><surname>Berens</surname><given-names>P</given-names></string-name>, <string-name><surname>Freeman</surname><given-names>J</given-names></string-name>, <string-name><surname>Deneux</surname><given-names>T</given-names></string-name>, <string-name><surname>Chenkov</surname><given-names>N</given-names></string-name>, <string-name><surname>McColgan</surname><given-names>T</given-names></string-name>, <string-name><surname>Speiser</surname><given-names>A</given-names></string-name>, <string-name><surname>Macke</surname><given-names>JH</given-names></string-name>, <string-name><surname>Turaga</surname><given-names>SC</given-names></string-name>, <string-name><surname>Mineault</surname><given-names>P</given-names></string-name>, <string-name><surname>Rupprecht</surname><given-names>P</given-names></string-name>, <string-name><surname>Gerhard</surname><given-names>S</given-names></string-name>, <string-name><surname>Friedrich</surname><given-names>RW</given-names></string-name>, <string-name><surname>Friedrich</surname><given-names>J</given-names></string-name>, <string-name><surname>Paninski</surname><given-names>L</given-names></string-name>, <string-name><surname>Pachitariu</surname><given-names>M</given-names></string-name>, <string-name><surname>Harris</surname><given-names>KD</given-names></string-name>, <string-name><surname>Bolte</surname><given-names>B</given-names></string-name>, <string-name><surname>Machado</surname><given-names>TA</given-names></string-name>, <string-name><surname>Ringach</surname><given-names>D</given-names></string-name>, <string-name><surname>Stone</surname><given-names>J</given-names></string-name>, <etal>et al.</etal> (<year>2018</year>) <article-title>Community-based benchmarking improves spike rate inference from two-photon calcium imaging data</article-title>. <source>PLoS Comput Biol</source>
<volume>14</volume>:<fpage>e1006157</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006157</pub-id>
<?supplied-pmid 29782491?><pub-id pub-id-type="pmid">29782491</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><string-name><surname>Bin</surname><given-names>Y</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Shen</surname><given-names>F</given-names></string-name>, <string-name><surname>Xie</surname><given-names>N</given-names></string-name>, <string-name><surname>Shen</surname><given-names>HT</given-names></string-name>, <string-name><surname>Li</surname><given-names>X</given-names></string-name> (<year>2019</year>) <article-title>Describing video with attention-based bidirectional LSTM</article-title>. <source>IEEE Trans Cybern</source>
<volume>49</volume>:<fpage>2631</fpage>–<lpage>2641</lpage>.<pub-id pub-id-type="pmid">29993730</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><string-name><surname>Charles</surname><given-names>AS</given-names></string-name>, <string-name><surname>Song</surname><given-names>A</given-names></string-name>, <string-name><surname>Gauthier</surname><given-names>JL</given-names></string-name>, <string-name><surname>Pillow</surname><given-names>JW</given-names></string-name>, <string-name><surname>Tank</surname><given-names>DW</given-names></string-name> (<year>2019</year>) <article-title>Neural anatomy and optical microscopy (NAOMi) simulation for evaluating calcium imaging methods</article-title>. <source>bioRxiv</source>
<volume>726174</volume>
<comment>doi: https://doi.org/10.1101/726174</comment>.</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><string-name><surname>Chen</surname><given-names>TW</given-names></string-name>, <string-name><surname>Wardill</surname><given-names>TJ</given-names></string-name>, <string-name><surname>Sun</surname><given-names>Y</given-names></string-name>, <string-name><surname>Pulver</surname><given-names>SR</given-names></string-name>, <string-name><surname>Renninger</surname><given-names>SL</given-names></string-name>, <string-name><surname>Baohan</surname><given-names>A</given-names></string-name>, <string-name><surname>Schreiter</surname><given-names>ER</given-names></string-name>, <string-name><surname>Kerr</surname><given-names>RA</given-names></string-name>, <string-name><surname>Orger</surname><given-names>MB</given-names></string-name>, <string-name><surname>Jayaraman</surname><given-names>V</given-names></string-name>, <string-name><surname>Looger</surname><given-names>LL</given-names></string-name>, <string-name><surname>Svoboda</surname><given-names>K</given-names></string-name>, <string-name><surname>Kim</surname><given-names>DS</given-names></string-name> (<year>2013</year>) <article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title>. <source>Nature</source>
<volume>499</volume>:<fpage>295</fpage>–<lpage>300</lpage>. <pub-id pub-id-type="doi">10.1038/nature12354</pub-id>
<?supplied-pmid 23868258?><pub-id pub-id-type="pmid">23868258</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="other"><string-name><surname>Chollet</surname><given-names>F</given-names></string-name> (<year>2015</year>) <article-title>Keras</article-title>. Available at <ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link>.</mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><string-name><surname>Dombeck</surname><given-names>DA</given-names></string-name>, <string-name><surname>Harvey</surname><given-names>CD</given-names></string-name>, <string-name><surname>Tian</surname><given-names>L</given-names></string-name>, <string-name><surname>Looger</surname><given-names>LL</given-names></string-name>, <string-name><surname>Tank</surname><given-names>DW</given-names></string-name> (<year>2010</year>) <article-title>Functional imaging of hippocampal place cells at cellular resolution during virtual navigation</article-title>. <source>Nat Neurosci</source>
<volume>13</volume>:<fpage>1433</fpage>–<lpage>1440</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2648</pub-id>
<?supplied-pmid 20890294?><pub-id pub-id-type="pmid">20890294</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><string-name><surname>Driscoll</surname><given-names>LN</given-names></string-name>, <string-name><surname>Pettit</surname><given-names>NL</given-names></string-name>, <string-name><surname>Minderer</surname><given-names>M</given-names></string-name>, <string-name><surname>Chettih</surname><given-names>SN</given-names></string-name>, <string-name><surname>Harvey</surname><given-names>CD</given-names></string-name> (<year>2017</year>) <article-title>Dynamic reorganization of neuronal activity patterns in parietal cortex</article-title>. <source>Cell</source>
<volume>170</volume>:<fpage>986</fpage>–<lpage>999.e16</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2017.07.021</pub-id>
<?supplied-pmid 28823559?><pub-id pub-id-type="pmid">28823559</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><string-name><surname>Evans</surname><given-names>MH</given-names></string-name>, <string-name><surname>Petersen</surname><given-names>RS</given-names></string-name>, <string-name><surname>Humphries</surname><given-names>MD</given-names></string-name> (<year>2019</year>) <article-title>On the use of calcium deconvolution algorithms in practical contexts</article-title>. <source>bioRxiv</source>
<volume>871137</volume>
<comment>doi: https://doi.org/10.1101/871137</comment>.</mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><string-name><surname>Galli</surname><given-names>L</given-names></string-name>, <string-name><surname>Maffei</surname><given-names>L</given-names></string-name> (<year>1988</year>) <article-title>Spontaneous impulse activity of rat retinal ganglion cells in prenatal life</article-title>. <source>Science</source>
<volume>242</volume>:<fpage>90</fpage>–<lpage>91</lpage>. <pub-id pub-id-type="doi">10.1126/science.3175637</pub-id>
<?supplied-pmid 3175637?><pub-id pub-id-type="pmid">3175637</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><string-name><surname>Gauthier</surname><given-names>JL</given-names></string-name>, <string-name><surname>Tank</surname><given-names>DW</given-names></string-name> (<year>2018</year>) <article-title>A dedicated population for reward coding in the hippocampus</article-title>. <source>Neuron</source>
<volume>99</volume>:<fpage>179</fpage>–<lpage>193</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2018.06.008</pub-id>
<?supplied-pmid 30008297?><pub-id pub-id-type="pmid">30008297</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><string-name><surname>Gauthier</surname><given-names>JL</given-names></string-name>, <string-name><surname>Koay</surname><given-names>SA</given-names></string-name>, <string-name><surname>Nieh</surname><given-names>EH</given-names></string-name>, <string-name><surname>Tank</surname><given-names>DW</given-names></string-name>, <string-name><surname>Pillow</surname><given-names>JW</given-names></string-name>, <string-name><surname>Charles</surname><given-names>AS</given-names></string-name> (<year>2018</year>) <article-title>Detecting and correcting false transients in calcium imaging</article-title>. <source>bioRxiv</source>
<volume>473470</volume>. doi: https://doi.org/10.1101/473470.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="other"><collab>GENIE Project</collab> (<year>2015</year>) <article-title>Simultaneous imaging and loose-seal cell-attached electrical recordings from neurons expressing a variety of genetically encoded calcium indicators</article-title>. Available at <ext-link ext-link-type="uri" xlink:href="http://CRCNS.org">http://CRCNS.org</ext-link>.</mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="book"><string-name><surname>Géron</surname><given-names>A</given-names></string-name> (<year>2019</year>) <source>Hands-on machine learning with scikit-learn, Keras, and TensorFlow: concepts, tools, and techniques to build intelligent systems</source>. <publisher-loc>Sebastopol</publisher-loc>: <publisher-name>O’Reilly Media</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><string-name><surname>Giovannucci</surname><given-names>A</given-names></string-name>, <string-name><surname>Friedrich</surname><given-names>J</given-names></string-name>, <string-name><surname>Gunn</surname><given-names>P</given-names></string-name>, <string-name><surname>Kalfon</surname><given-names>J</given-names></string-name>, <string-name><surname>Brown</surname><given-names>BL</given-names></string-name>, <string-name><surname>Koay</surname><given-names>SA</given-names></string-name>, <string-name><surname>Taxidis</surname><given-names>J</given-names></string-name>, <string-name><surname>Najafi</surname><given-names>F</given-names></string-name>, <string-name><surname>Gauthier</surname><given-names>JL</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>P</given-names></string-name>, <string-name><surname>Khakh</surname><given-names>BS</given-names></string-name>, <string-name><surname>Tank</surname><given-names>DW</given-names></string-name>, <string-name><surname>Chklovskii</surname><given-names>DB</given-names></string-name>, <string-name><surname>Pnevmatikakis</surname><given-names>EA</given-names></string-name> (<year>2019</year>) <article-title>CaImAn an open source tool for scalable calcium imaging data analysis</article-title>. <source>Elife</source>
<volume>8</volume>:<fpage>e38173</fpage>
<pub-id pub-id-type="doi">10.7554/eLife.38173</pub-id>
<pub-id pub-id-type="pmid">30652683</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><string-name><surname>Hochreiter</surname><given-names>S</given-names></string-name>, <string-name><surname>Schmidhuber</surname><given-names>J</given-names></string-name> (<year>1997</year>) <article-title>Long short-term memory</article-title>. <source>Neural Comput</source>
<volume>9</volume>:<fpage>1735</fpage>–<lpage>1780</lpage>. <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
<?supplied-pmid 9377276?><pub-id pub-id-type="pmid">9377276</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><string-name><surname>Katlowitz</surname><given-names>KA</given-names></string-name>, <string-name><surname>Picardo</surname><given-names>MA</given-names></string-name>, <string-name><surname>Long</surname><given-names>MA</given-names></string-name> (<year>2018</year>) <article-title>Stable sequential activity underlying the maintenance of a precisely executed skilled behavior</article-title>. <source>Neuron</source>
<volume>98</volume>:<fpage>1133</fpage>–<lpage>1140.e3</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2018.05.017</pub-id>
<?supplied-pmid 29861283?><pub-id pub-id-type="pmid">29861283</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><string-name><surname>Kim</surname><given-names>JY</given-names></string-name>, <string-name><surname>Ash</surname><given-names>RT</given-names></string-name>, <string-name><surname>Ceballos‐Diaz</surname><given-names>C</given-names></string-name>, <string-name><surname>Levites</surname><given-names>Y</given-names></string-name>, <string-name><surname>Golde</surname><given-names>TE</given-names></string-name>, <string-name><surname>Smirnakis</surname><given-names>SM</given-names></string-name>, <string-name><surname>Jankowsky</surname><given-names>JL</given-names></string-name> (<year>2013</year>) <article-title>Viral transduction of the neonatal brain delivers controllable genetic mosaicism for visualising and manipulating neuronal circuits in vivo</article-title>. <source>Eur J Neurosci</source>
<volume>37</volume>:<fpage>1203</fpage>–<lpage>1220</lpage>. <pub-id pub-id-type="doi">10.1111/ejn.12126</pub-id>
<?supplied-pmid 23347239?><pub-id pub-id-type="pmid">23347239</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><string-name><surname>Kim</surname><given-names>JY</given-names></string-name>, <string-name><surname>Grunke</surname><given-names>SD</given-names></string-name>, <string-name><surname>Levites</surname><given-names>Y</given-names></string-name>, <string-name><surname>Golde</surname><given-names>TE</given-names></string-name>, <string-name><surname>Jankowsky</surname><given-names>JL</given-names></string-name> (<year>2014</year>) <article-title>Intracerebroventricular viral injection of the neonatal mouse brain for persistent and widespread neuronal transduction</article-title>. <source>J Vis Exp</source>
<volume>(91)</volume>:<fpage>51863</fpage>
<pub-id pub-id-type="doi">10.3791/51863</pub-id>
</mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="book"><string-name><surname>LeCun</surname><given-names>Y</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y</given-names></string-name> (<year>1995</year>) <article-title>Convolutional networks for images, speech, and time series</article-title>. <source>Handb Brain Theory Neural Netw</source> (Arbib MA ed). <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><string-name><surname>Melzer</surname><given-names>S</given-names></string-name>, <string-name><surname>Michael</surname><given-names>M</given-names></string-name>, <string-name><surname>Caputi</surname><given-names>A</given-names></string-name>, <string-name><surname>Eliava</surname><given-names>M</given-names></string-name>, <string-name><surname>Fuchs</surname><given-names>EC</given-names></string-name>, <string-name><surname>Whittington</surname><given-names>MA</given-names></string-name>, <string-name><surname>Monyer</surname><given-names>H</given-names></string-name> (<year>2012</year>) <article-title>Long-range–projecting GABAergic neurons modulate inhibition in hippocampus and entorhinal cortex</article-title>. <source>Science</source>
<volume>335</volume>:<fpage>1506</fpage>–<lpage>1510</lpage>. <pub-id pub-id-type="doi">10.1126/science.1217139</pub-id>
<?supplied-pmid 22442486?><pub-id pub-id-type="pmid">22442486</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><string-name><surname>Miri</surname><given-names>A</given-names></string-name>, <string-name><surname>Daie</surname><given-names>K</given-names></string-name>, <string-name><surname>Arrenberg</surname><given-names>AB</given-names></string-name>, <string-name><surname>Baier</surname><given-names>H</given-names></string-name>, <string-name><surname>Aksay</surname><given-names>E</given-names></string-name>, <string-name><surname>Tank</surname><given-names>DW</given-names></string-name> (<year>2011</year>) <article-title>Spatial gradients and multidimensional dynamics in a neural integrator circuit</article-title>. <source>Nat Neurosci</source>
<volume>14</volume>:<fpage>1150</fpage>–<lpage>1159</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2888</pub-id>
<?supplied-pmid 21857656?><pub-id pub-id-type="pmid">21857656</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><string-name><surname>Modol</surname><given-names>L</given-names></string-name>, <string-name><surname>Bollmann</surname><given-names>Y</given-names></string-name>, <string-name><surname>Tressard</surname><given-names>T</given-names></string-name>, <string-name><surname>Baude</surname><given-names>A</given-names></string-name>, <string-name><surname>Che</surname><given-names>A</given-names></string-name>, <string-name><surname>Duan</surname><given-names>ZRS</given-names></string-name>, <string-name><surname>Babij</surname><given-names>R</given-names></string-name>, <string-name><surname>García</surname><given-names>NVDM</given-names></string-name>, <string-name><surname>Cossart</surname><given-names>R</given-names></string-name> (<year>2020</year>) <article-title>Assemblies of perisomatic GABAergic neurons in the developing barrel cortex</article-title>. <source>Neuron</source>
<volume>105</volume>:<fpage>93</fpage>–<lpage>105.e4</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.007</pub-id>
<pub-id pub-id-type="pmid">31780328</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><string-name><surname>O’Donovan</surname><given-names>MJ</given-names></string-name> (<year>1989</year>) <article-title>Motor activity in the isolated spinal cord of the chick embryo: synaptic drive and firing pattern of single motoneurons</article-title>. <source>J Neurosci</source>
<volume>9</volume>:<fpage>943</fpage>–<lpage>958</lpage>.<pub-id pub-id-type="pmid">2926486</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><string-name><surname>Pachitariu</surname><given-names>M</given-names></string-name>, <string-name><surname>Stringer</surname><given-names>C</given-names></string-name>, <string-name><surname>Dipoppa</surname><given-names>M</given-names></string-name>, <string-name><surname>Schröder</surname><given-names>S</given-names></string-name>, <string-name><surname>Rossi</surname><given-names>LF</given-names></string-name>, <string-name><surname>Dalgleish</surname><given-names>H</given-names></string-name>, <string-name><surname>Carandini</surname><given-names>M</given-names></string-name>, <string-name><surname>Harris</surname><given-names>KD</given-names></string-name> (<year>2017</year>) <article-title>Suite2p: beyond 10,000 neurons with standard two-photon microscopy</article-title>. <source>bioRxiv.</source>
<comment>doi: https://doi.org/10.1101/061507</comment>.</mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><string-name><surname>Pedregosa</surname><given-names>F</given-names></string-name>, <string-name><surname>Varoquaux</surname><given-names>G</given-names></string-name>, <string-name><surname>Gramfort</surname><given-names>A</given-names></string-name>, <string-name><surname>Michel</surname><given-names>V</given-names></string-name>, <string-name><surname>Thirion</surname><given-names>B</given-names></string-name>, <string-name><surname>Grisel</surname><given-names>O</given-names></string-name>, <string-name><surname>Blondel</surname><given-names>M</given-names></string-name>, <string-name><surname>Prettenhofer</surname><given-names>P</given-names></string-name>, <string-name><surname>Weiss</surname><given-names>R</given-names></string-name>, <string-name><surname>Dubourg</surname><given-names>V</given-names></string-name>, <string-name><surname>Vanderplas</surname><given-names>J</given-names></string-name>, <string-name><surname>Passos</surname><given-names>A</given-names></string-name>, <string-name><surname>Cournapeau</surname><given-names>D</given-names></string-name>, <string-name><surname>Brucher</surname><given-names>M</given-names></string-name>, <string-name><surname>Perrot</surname><given-names>M</given-names></string-name>, <string-name><surname>Duchesnay</surname><given-names>É</given-names></string-name> (<year>2011</year>) <article-title>Scikit-learn: machine learning in Python</article-title>. <source>J Mach Learn Res</source>
<volume>12</volume>:<fpage>2825</fpage>–<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><string-name><surname>Perez</surname><given-names>L</given-names></string-name>, <string-name><surname>Wang</surname><given-names>J</given-names></string-name> (<year>2017</year>) <article-title>The effectiveness of data augmentation in image classification using deep learning</article-title>. <source>ArXiv</source>
<volume>1712.04621</volume>.</mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><string-name><surname>Pnevmatikakis</surname><given-names>EA</given-names></string-name>, <string-name><surname>Soudry</surname><given-names>D</given-names></string-name>, <string-name><surname>Gao</surname><given-names>Y</given-names></string-name>, <string-name><surname>Machado</surname><given-names>TA</given-names></string-name>, <string-name><surname>Merel</surname><given-names>J</given-names></string-name>, <string-name><surname>Pfau</surname><given-names>D</given-names></string-name>, <string-name><surname>Reardon</surname><given-names>T</given-names></string-name>, <string-name><surname>Mu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Lacefield</surname><given-names>C</given-names></string-name>, <string-name><surname>Yang</surname><given-names>W</given-names></string-name>, <string-name><surname>Ahrens</surname><given-names>M</given-names></string-name>, <string-name><surname>Bruno</surname><given-names>R</given-names></string-name>, <string-name><surname>Jessell</surname><given-names>TM</given-names></string-name>, <string-name><surname>Peterka</surname><given-names>DS</given-names></string-name>, <string-name><surname>Yuste</surname><given-names>R</given-names></string-name>, <string-name><surname>Paninski</surname><given-names>L</given-names></string-name> (<year>2016</year>) <article-title>Simultaneous denoising, deconvolution, and demixing of calcium imaging data</article-title>. <source>Neuron</source>
<volume>89</volume>:<fpage>285</fpage>–<lpage>299</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.037</pub-id>
<?supplied-pmid 26774160?><pub-id pub-id-type="pmid">26774160</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><string-name><surname>Provine</surname><given-names>RR</given-names></string-name> (<year>1972</year>) <article-title>Ontogeny of bioelectric activity in the spinal cord of the chick embryo and its behavioral implications</article-title>. <source>Brain Res</source>
<volume>41</volume>:<fpage>365</fpage>–<lpage>378</lpage>. <pub-id pub-id-type="doi">10.1016/0006-8993(72)90508-2</pub-id>
<?supplied-pmid 4338887?><pub-id pub-id-type="pmid">4338887</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><string-name><surname>Ramachandran</surname><given-names>P</given-names></string-name>, <string-name><surname>Zoph</surname><given-names>B</given-names></string-name>, <string-name><surname>Le</surname><given-names>QV</given-names></string-name> (<year>2017</year>) <article-title>Searching for activation functions</article-title>. <source>ArXiv</source>
<volume>171005941</volume>.</mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="other"><string-name><surname>Rémy</surname><given-names>P</given-names></string-name> (<year>2019</year>) <article-title>philipperemy/keras-attention-mechanism</article-title>. Available at <ext-link ext-link-type="uri" xlink:href="https://github.com/philipperemy/keras-attention-mechanism">https://github.com/philipperemy/keras-attention-mechanism</ext-link>.</mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><string-name><surname>Rübel</surname><given-names>O</given-names></string-name>, <string-name><surname>Tritt</surname><given-names>A</given-names></string-name>, <string-name><surname>Dichter</surname><given-names>B</given-names></string-name>, <string-name><surname>Braun</surname><given-names>T</given-names></string-name>, <string-name><surname>Cain</surname><given-names>N</given-names></string-name>, <string-name><surname>Clack</surname><given-names>N</given-names></string-name>, <string-name><surname> Davidson</surname><given-names>TJ</given-names></string-name>, <string-name><surname>Dougherty</surname><given-names>M</given-names></string-name>, <string-name><surname>Fillion-Robin</surname><given-names>J-N</given-names></string-name>, <string-name><surname>Graddis</surname><given-names>N</given-names></string-name>, <string-name><surname>Grauer</surname><given-names>M</given-names></string-name>, <string-name><surname>Kiggins</surname><given-names>JT</given-names></string-name>, <string-name><surname>Niu</surname><given-names>L</given-names></string-name>, <string-name><surname>Ozturk</surname><given-names>D</given-names></string-name>, <string-name><surname>Schroeder</surname><given-names>W</given-names></string-name>, <string-name><surname>Soltesz</surname><given-names>I</given-names></string-name>, <string-name><surname>Sommer</surname><given-names>FT</given-names></string-name>, <string-name><surname>Svoboda</surname><given-names>K</given-names></string-name>, <string-name><surname>Lydia</surname><given-names>N</given-names></string-name>, <string-name><surname>Frank</surname><given-names>LM</given-names></string-name><etal>et al.</etal> (<year>2019</year>) <article-title>NWB: n 2.0: an accessible data standard for neurophysiology</article-title>. <source>bioRxiv</source>
<volume>523035</volume>
<comment>doi: https://doi.org/10.1101/523035</comment>.</mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><string-name><surname>Srivastava</surname><given-names>N</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>G</given-names></string-name>, <string-name><surname>Krizhevsky</surname><given-names>A</given-names></string-name>, <string-name><surname>Sutskever</surname><given-names>I</given-names></string-name>, <string-name><surname>Salakhutdinov</surname><given-names>R</given-names></string-name> (<year>2014</year>) <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>. <source>J Mach Learn Res</source>
<volume>15</volume>:<fpage>1929</fpage>–<lpage>1958</lpage>.</mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><string-name><surname>Stringer</surname><given-names>C</given-names></string-name>, <string-name><surname>Pachitariu</surname><given-names>M</given-names></string-name>, <string-name><surname>Steinmetz</surname><given-names>N</given-names></string-name>, <string-name><surname>Reddy</surname><given-names>CB</given-names></string-name>, <string-name><surname>Carandini</surname><given-names>M</given-names></string-name>, <string-name><surname>Harris</surname><given-names>KD</given-names></string-name> (<year>2019</year>) <article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title>. <source>Science</source>
<volume>364</volume>:<fpage>255</fpage>–<lpage>255</lpage>. <pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id>
<?supplied-pmid 31000656?><pub-id pub-id-type="pmid">31000656</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><string-name><surname>Teeters</surname><given-names>JL</given-names></string-name>, <string-name><surname>Godfrey</surname><given-names>K</given-names></string-name>, <string-name><surname>Young</surname><given-names>R</given-names></string-name>, <string-name><surname>Dang</surname><given-names>C</given-names></string-name>, <string-name><surname>Friedsam</surname><given-names>C</given-names></string-name>, <string-name><surname>Wark</surname><given-names>B</given-names></string-name>, <string-name><surname>Asari</surname><given-names>H</given-names></string-name>, <string-name><surname>Peron</surname><given-names>S</given-names></string-name>, <string-name><surname>Li</surname><given-names>N</given-names></string-name>, <string-name><surname>Peyrache</surname><given-names>A</given-names></string-name>, <string-name><surname>Denisov</surname><given-names>G</given-names></string-name>, <string-name><surname>Siegle</surname><given-names>JH</given-names></string-name>, <string-name><surname>Olsen</surname><given-names>SR</given-names></string-name>, <string-name><surname>Martin</surname><given-names>C</given-names></string-name>, <string-name><surname>Chun</surname><given-names>M</given-names></string-name>, <string-name><surname>Tripathy</surname><given-names>S</given-names></string-name>, <string-name><surname>Blanche</surname><given-names>TJ</given-names></string-name>, <string-name><surname>Harris</surname><given-names>K</given-names></string-name>, <string-name><surname>Buzsáki</surname><given-names>G</given-names></string-name>, <string-name><surname>Koch</surname><given-names>C</given-names></string-name>, <etal>et al.</etal> (<year>2015</year>) <article-title>Neurodata without borders: creating a common data format for neurophysiology</article-title>. <source>Neuron</source>
<volume>88</volume>:<fpage>629</fpage>–<lpage>634</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.10.025</pub-id>
<?supplied-pmid 26590340?><pub-id pub-id-type="pmid">26590340</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><string-name><surname>Troullinou</surname><given-names>E</given-names></string-name>, <string-name><surname>Tsagkatakis</surname><given-names>G</given-names></string-name>, <string-name><surname>Chavlis</surname><given-names>S</given-names></string-name>, <string-name><surname>Turi</surname><given-names>G</given-names></string-name>, <string-name><surname>Li</surname><given-names>W-K</given-names></string-name>, <string-name><surname>Losonczy</surname><given-names>A</given-names></string-name>, <string-name><surname>Tsakalides</surname><given-names>P</given-names></string-name>, <string-name><surname>Poirazi</surname><given-names>P</given-names></string-name> (<year>2019</year>) <article-title>Artificial neural networks in action for an automated cell-type classification of biological neural networks</article-title>. <source>arXiv</source>:<volume>191109977</volume> [cs, q-bio].</mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="other"><string-name><surname>Vaswani</surname><given-names>A</given-names></string-name>, <string-name><surname>Shazeer</surname><given-names>N</given-names></string-name>, <string-name><surname>Parmar</surname><given-names>N</given-names></string-name>, <string-name><surname>Uszkoreit</surname><given-names>J</given-names></string-name>, <string-name><surname>Jones</surname><given-names>L</given-names></string-name>, <string-name><surname>Gomez</surname><given-names>AN</given-names></string-name>, <string-name><surname>Kaiser</surname><given-names>L</given-names></string-name>, <string-name><surname>Polosukhin</surname><given-names>I</given-names></string-name> (<year>2017</year>) <article-title>Attention is all you need. In Advances in Neural Information Processing Systems</article-title>, pp. <fpage>6000</fpage>–<lpage>6010</lpage>.</mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><string-name><surname>Villette</surname><given-names>V</given-names></string-name>, <string-name><surname>Malvache</surname><given-names>A</given-names></string-name>, <string-name><surname>Tressard</surname><given-names>T</given-names></string-name>, <string-name><surname>Dupuy</surname><given-names>N</given-names></string-name>, <string-name><surname>Cossart</surname><given-names>R</given-names></string-name> (<year>2015</year>) <article-title>Internally recurring hippocampal sequences as a population template of spatiotemporal information</article-title>. <source>Neuron</source>
<volume>88</volume>:<fpage>357</fpage>–<lpage>366</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.052</pub-id>
<?supplied-pmid 26494280?><pub-id pub-id-type="pmid">26494280</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><string-name><surname>Virtanen</surname><given-names>P</given-names></string-name>, <string-name><surname>Gommers</surname><given-names>R</given-names></string-name>, <string-name><surname>Oliphant</surname><given-names>TE</given-names></string-name>, <string-name><surname>Haberland</surname><given-names>M</given-names></string-name>, <string-name><surname>Reddy</surname><given-names>T</given-names></string-name>, <string-name><surname>Cournapeau</surname><given-names>D</given-names></string-name>, <string-name><surname>Burovski</surname><given-names>E</given-names></string-name>, <string-name><surname>Peterson</surname><given-names>P</given-names></string-name>, <string-name><surname>Weckesser</surname><given-names>W</given-names></string-name>, <string-name><surname>Bright</surname><given-names>J</given-names></string-name>, <string-name><surname>van der Walt</surname><given-names>SJ</given-names></string-name>, <string-name><surname>Brett</surname><given-names>M</given-names></string-name>, <string-name><surname>Wilson</surname><given-names>J</given-names></string-name>, <string-name><surname>Millman</surname><given-names>KJ</given-names></string-name>, <string-name><surname>Mayorov</surname><given-names>N</given-names></string-name>, <string-name><surname>Nelson</surname><given-names>ARJ</given-names></string-name>, <string-name><surname>Jones</surname><given-names>E</given-names></string-name>, <string-name><surname>Kern</surname><given-names>R</given-names></string-name>, <string-name><surname>Larson</surname><given-names>E</given-names></string-name>, <string-name><surname>Carey</surname><given-names>CJ</given-names></string-name>, <etal>et al.</etal> (<year>2020</year>) <article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title>. <source>Nat Methods</source>
<volume>17</volume>:<fpage>261</fpage>–<lpage>272</lpage>.<pub-id pub-id-type="pmid">32015543</pub-id></mixed-citation>
    </ref>
  </ref-list>
  <sec sec-type="synthesis-author-response" id="s7">
    <title>Synthesis</title>
    <boxed-text position="float" orientation="portrait">
      <p>Reviewing Editor: Thomas McHugh, RIKEN Center for Brain Science</p>
      <p>Decisions are customarily a result of the Reviewing Editor and the peer reviewers coming together and discussing their recommendations until a consensus is reached. When revisions are invited, a fact-based synthesis statement explaining their decision and outlining what is needed to prepare a revision will be listed below. The following reviewer(s) agreed to reveal their identity: Jayeeta Basu.</p>
    </boxed-text>
    <p>The reviewers found the manuscript to be much improved, commenting, “... methods (both surgical and computational) are more clearly described, and the figures are better organized and the authors do a good job of presenting the tool. The authors have also included a number of new datasets from different brain regions, even using different indicators. To be sure, the method performs better on some datasets than others, but that will be true of any algorithm. Evaluating this algorithm as well as CaImAn for these additional diverse datasets greatly improves the impact of the paper.”</p>
    <p>Following discussion, the referees highlighted a few outstanding issues that should be relatively easily addressed in a minor revision:</p>
    <p>1. Remaining concerns about the discussion section (line 609 to 616). X-Y movements, neuropil activation, summating transients, and changes in baseline are all important issues in correctly analyzing and interpreting calcium imaging data. Using softened messaging here does not really address the elephant in the room: either the data analysis package can address these above issues or it cannot. If the authors have indeed tested how the package handles artifacts of X-Y movements, etc - and have data or analyses to concisely demonstrate that these issues can be handled, they should be presented in the results section. However, if the authors have not tested how the software handles these issues -then they should refrain from presumptive claims that it may be able to handle it. If inclusion of these would be complicated or distractive, or such tests have not been made, then I feel the claims should be removed, while still acknowledging the importance of these factors and that further testing of how the software handles these is required but beyond the scope of this submission.</p>
    <p>2. Regarding the use of the LSTM neural network classifier, the explanation of the motivation behind using this model the authors presented in their rebuttal letter is helpful, and the sentence “Action recognition from videos...” starting on line 72 helps to frame the use of this model. Some expansion on these topics in the introduction and methods (line 280) would help the reader understand the need for such a model.</p>
    <p>3. Training Procedure and Evaluation: Figure 5 is very helpful to understand the workflow. However, the procedure used and conditions to be met to train the classifier can be made clearer. In the rebuttal letter, the authors state that “we based the choice of the ‘best’ epoch on the classifier performance on the test dataset more than the validation dataset.” This information is important and should be included in the manuscript, as well as a clear definition of training, validation, and test datasets.</p>
    <p>4. The extended <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref> gives detailed information about which datasets are used for training and testing, but I cannot find information describing the validation dataset or its delineation from the training and test datasets. In lines 277-278 of the manuscript, it is stated that “data augmentation, simulated data and stratification were used only to produce the training dataset and not the validation dataset,” which implies that there is a validation dataset being used, but no further details about the validation dataset are given. It is possible I am used to a different terminology, but evaluating performance on the test set is not typically used to define a stopping point for training; that is the purpose of the validation set. Evaluation of the model against the test set should only be done to report final performance statistics. Regardless of which sets were used at which stage of training, these decisions should be clearly stated.</p>
    <p>5. If I read figure R1 correctly, it seems like there is only a small improvement in any metric with increasing epoch number. In Section 2.5, lines 314-315, the manuscript states “the general classifier (CINAC_v1) was trained over 14 epochs.” What I am trying to understand is the stopping criteria used for training; why 14 epochs?</p>
    <p>6. Performance evaluation: It is still unclear what the performance evaluations use as their ground truth data: Is it consensus labeling from the human experts, or is it the output of the simple labelling algorithm? The authors’ rebuttal letter (6a) implies it is consensus labeling from human experts: “Figure 3 (Ground Truth workflow) precises how this ‘Ground Truth’ is established. All benchmarks are done against it and not against all the putative transients obtained with the simple labelling algorithm.” But lines 333-336 of the current version of the manuscript suggest evaluation was done against the simpler algorithm: “In order to compute the metrics we needed to evaluate the number of true negative and false negative transients. To do this evaluation, we defined all putative transients. We identified the putative transient using the change of the sign...” In the context of performance evaluation, “putative transients” and “ground truth” seem to be synonymous. Is that not the case?</p>
    <p>7. The authors’ explanation of determining the parameters for overlap activity in the rebuttal is complete and well-written. I suggest some of that information be included in the manuscript.</p>
    <p>8. Line 267-268: “We finally added some Gaussian noise on every frame"</p>
    <p>The standard deviation (or that relative to the signal amplitude) should be defined here.</p>
  </sec>
  <sec sec-type="synthesis-author-response" id="s8">
    <title>Author Response</title>
    <p>Dear Editor and reviewers,</p>
    <p>We would like to thank you for all constructive comments. In this letter, we have addressed each of the minor points raised by reviewers, by providing additional information in the manuscript.</p>
    <p>Please find below a point by point response to reviewers’ comments. Manuscript Instructions</p>
    <p>- Extended Data should be labeled as Figure 1-1, Figure 1-2, <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref>, etc., so they indicate which figure they are supporting (i.e. Extended Data table supporting Figure 5 labeled as Figure 5-1).</p>
    <p>We carefully checked extended data labels in the manuscript. 2 figures support figure</p>
    <p>7 ( <xref ref-type="supplementary-material" rid="fig7-1">figure 7-1</xref> and 7-2), 3 figures support figure 8 (<xref ref-type="supplementary-material" rid="fig8-1">figure 8-1</xref>, 8-2 and 8-3), <xref ref-type="supplementary-material" rid="tab1-1">table 1-1</xref> support table 1. We changed the tiff names of the figures from “figure 8_1” to “figure</p>
    <p>8-1"</p>
    <p>- Extended Data is not referenced in the legend for the figure/table it is supporting.</p>
    <p>We added extended data references in the legend for the figure/table it is supporting.</p>
    <p>- The species studied is not mentioned in the abstract. Please make sure to update both the abstract in the article file and on the submission form.</p>
    <p>We updated both the abstract in the article file and on the submission form accordingly (see line 11).</p>
    <p>- The sex of the species studied is not mentioned in the materials and methods section. Please mention “male", “female", “x males and x females", “of either sex”.</p>
    <p>We updated the Method section to mention “either sex” (line 126).</p>
    <p>Synthesis of Reviews:</p>
    <p>Comments on the Visual Abstract for Author (Required):</p>
    <p>The visual abstract is still a little complicated - simplifying it will be better.</p>
    <p>We agree with reviewers and have difficulties simplifying the visual abstract, so we prefer to not include a visual abstract.</p>
    <p>Synthesis Statement for Author (Required):</p>
    <p>The reviewers found the manuscript to be much improved, commenting, “... methods (both surgical and computational) are more clearly described, and the figures are better organized and the authors do a good job of presenting the tool. The authors have also included a number of new datasets from different brain regions, even using different indicators. To be sure, the method performs better on some datasets than others, but that will be true of any algorithm. Evaluating this algorithm as well as CaImAn for these additional diverse datasets greatly improves the impact of the paper.”</p>
    <p>Following discussion, the referees highlighted a few outstanding issues that if addressed in a minor revision will not require further review by the referees.</p>
    <p>1. Remaining concerns about the discussion section (line 609 to 616). X-Y movements, neuropil activation, summating transients, and changes in baseline are all important issues in correctly analyzing and interpreting calcium imaging data. Using softened messaging here does not really address the elephant in the room: either the data analysis package can address these above issues or it cannot. If the authors have indeed tested how the package handles artifacts of X-Y movements, etc - and have data or analyses to concisely demonstrate that these issues can be handled, they should be presented in the results section. However, if the authors have not tested how the software handles these issues</p>
    <p>-then they should refrain from presumptive claims that it may be able to handle it. If inclusion of these would be complicated or distractive, or such tests have not been made, then I feel the claims should be removed, while still acknowledging the importance of these factors and that further testing of how the software handles these is required but beyond the scope of this submission.</p>
    <p>Our claims were based on empirical experience but are complicated to properly demonstrate. Thus, we decided to remove the claims from the discussion while still acknowledging the importance of these factors and noting the use of visual inspection through the GUI to manually evaluate the effect of those factors.</p>
    <p>We replaced : “On a side note, we observed during the visual inspection of the prediction through the GUI that the network might be able to handle fake transients due to X and Y movements or neuropil activation. By avoiding the use of any threshold to select transients over fluorescence time-course, the classifier might also be able to detect: i) small amplitude transients, ii) transients occurring during the decay of another one, iii) summations. Importantly, because activity is split into segments of 100 frames (around 10 sec), DeepCINAC classifiers would still perform well and deal with changes in fluorescence baseline. “ by : “Analysis of calcium imaging data may be impacted by some factors: i) small amplitude transients, ii) transients occurring during the decay of another one, iii) summations, iv) X and Y movement or neuropil activation. Users can evaluate the impact of those factors through visual inspection of the inferred activity using the deepCINAC GUI”. (lines 620-630)</p>
    <p>2. Regarding the use of the LSTM neural network classifier, the explanation of the motivation behind using this model the authors presented in their rebuttal letter is helpful, and the</p>
    <p>sentence “Action recognition from videos...” starting on line 72 helps to frame the use of this model. Some expansion on these topics in the introduction and methods (line 280) would help the reader understand the need for such a model.</p>
    <p>We agreed with reviewers and added a new sentence in the introduction (lines 71-72): “Our goal was to train a classifier to recognize cell activation directly from a movie which falls into the domain of action recognition", as well as in the method (line 285): “To perform action recognition"</p>
    <p>3. Training Procedure and Evaluation: Figure 5 is very helpful to understand the workflow. However, the procedure used and conditions to be met to train the classifier can be made clearer. In the rebuttal letter, the authors state that “we based the choice of the ‘best’ epoch on the classifier performance on the test dataset more than the validation dataset.” This information is important and should be included in the manuscript, as well as a clear definition of training, validation, and test datasets.</p>
    <p>We agreed with reviewers and added a new paragraph in the manuscript (lines</p>
    <p>347-349): “We used these metrics in order to base the choice of the ‘best’ epoch on the classifier performance on the test dataset rather than the performance on validation dataset. However, we stopped the training when the validation dataset metrics reached a plateau.”</p>
    <p>Additionally, regarding the definition of training, validation and datasets we added a new paragraph, meanwhile we clarify the condition and procedure used in the manuscript as followed (lines 233-237): “Definition of training, validation and test datasets. Our main dataset was split between a test dataset and a dataset used to train the classifier (referred to as training dataset, see Table 1 and 1-1). The training dataset used as the input of the classifier was randomly split with a ratio of 80-20% on a training and validation dataset. Validation data are used at the end of each epoch of the training to update the weights of the classifier.”</p>
    <p>In the methods section, we replaced the sentence (lines 281-283): “The data augmentation, simulated data and stratification were used only to produce the training dataset and not the validation dataset.” by: “The data augmentation, simulated data and stratification were applied to the part of the training dataset not used for validation. “</p>
    <p>4. The extended <xref ref-type="supplementary-material" rid="tab1-1">Table 1-1</xref> gives detailed information about which datasets are used for training and testing, but I cannot find information describing the validation dataset or its delineation from the training and test datasets. In lines 277-278 of the manuscript, it is stated that “data augmentation, simulated data and stratification were used only to produce the training dataset and not the validation dataset,” which implies that there is a validation dataset being used, but no further details about the validation dataset are given. It is possible I am used to a different terminology, but evaluating performance on the test set is not typically used to define a stopping point for training; that is the purpose of the validation set. Evaluation of the model against the test set should only be done to report final</p>
    <p>performance statistics. Regardless of which sets were used at which stage of training, these decisions should be clearly stated.</p>
    <p>We definitely agree with reviewers comments and have already addressed this in the third point that raises the similar question. In addition, we think we clarified the message by adding one sentence in the legend of table 1: “Training dataset include validation dataset (see methods) “ (line 862). We also modified the title of the 6th column in <xref ref-type="supplementary-material" rid="tab1-1">table 1-1</xref> to clarify that the so-called “training dataset” includes a validation dataset.</p>
    <p>5. If I read figure R1 correctly, it seems like there is only a small improvement in any metric with increasing epoch number. In Section 2.5, lines 314-315, the manuscript states “the general classifier (CINAC_v1) was trained over 14 epochs.” What I am trying to understand is the stopping criteria used for training; why 14 epochs?</p>
    <p>As presented in the response to the third point (line 349) “we stopped the training when the validation dataset metrics reached a plateau”.</p>
    <p>In this particular case, we stopped the training at epoch 14 because it reaches a plateau after epoch 10. While epoch 9 had the best accuracy on validation data, the epoch 6 was the one performing the best in our test dataset and chosen for CINAC_v1. You can see the evaluation of the epochs for this classifier on this google spreadsheet: https://docs.google.com/spreadsheets/d/1PWCHWAI13TR1TIBQ5q2UyeGWkt-vnpcJPL07w</p>
    <p>5J3nXo/edit?usp=sharing</p>
    <p>6. Performance evaluation: It is still unclear what the performance evaluations use as their ground truth data: Is it consensus labeling from the human experts, or is it the output of the simple labelling algorithm? The authors’ rebuttal letter (6a) implies it is consensus labeling from human experts: “Figure 3 (Ground Truth workflow) precises how this ‘Ground Truth’ is established. All benchmarks are done against it and not against all the putative transients obtained with the simple labelling algorithm.” But lines 333-336 of the current version of the manuscript suggest evaluation was done against the simpler algorithm: “In order to compute the metrics we needed to evaluate the number of true negative and false negative transients. To do this evaluation, we defined all putative transients. We identified the putative transient using the change of the sign...” In the context of performance evaluation, “putative transients” and “ground truth” seem to be synonymous. Is that not the case?</p>
    <p>We apologize for the confusion created by the use of the “simple algorithm” to detect “putative transients”. We actually did not need this method to evaluate the number of False Negative transients, but only to evaluate the number of True Negative transients. However, as we have a skewed dataset, we evaluated the performance of the classifier using the sensitivity and the precision that do not need the True Negative transients (see figure below). Thus, the performance evaluations only use as their ground truth data the consensus labeling from the human experts. We removed this part in the methods that was addressing the simple algorithm (line 336-342).</p>
    <p>7. The authors’ explanation of determining the parameters for overlap activity in the rebuttal is complete and well-written. I suggest some of that information be included in the manuscript.</p>
    <p>We agreed with reviewers and added some information from the rebuttal letter into the manuscript as followed (lines 363-377): “Based on empirical research we found that</p>
    <p>15% of overlap was the minimal size above which a true transient in the cell is sufficient to trigger a false transient in the overlapped cell [...] To assure to attribute the correct transient to the truly active cell we used a high correlation threshold 0.7 for the first cells and low threshold for the second cell of 0.2. We considered that the transient was a true activation of the first cell leading to a false transient in the second one. Indeed, we observed that the correlation method such as the one used in Gauthier et al. (Gauthier et al., 2018) is not always sufficient to classify correctly the transient activity. However, by using the combination of a very low and high threshold, we assure (in most of the cases) that one cell is having a false transient while the other one is truly active.”</p>
    <p>8. Line 267-268: “We finally added some Gaussian noise on every frame”. The standard deviation (or that relative to the signal amplitude) should be defined here.</p>
    <p>We agreed and added this information in the methods part (line 272). Mean was 0 and variance of this gaussian noise was 0.1.</p>
  </sec>
</back>
